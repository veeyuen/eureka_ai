# YUREEKA AI RESEARCH ASSISTANT v7.41
# Streamlit-safe single-file build (refactor series).
# NOTE: Long historical banner comments removed in REFACTOR159 (no behavior change).

# =============================================================================
# MODULE INDEX (v1) — single-file today, easy split tomorrow
#
# This file is intentionally structured as "virtual modules" to enable clean
# modularization once the NLP/LLM roadmap reaches end-state.
#
# Conventions:
# - Each module is a contiguous block marked by a banner and a [MOD:...] anchor.
# - Modules should avoid import-time side effects; runtime init happens in main().
# - Flags/config are resolved via helpers (prefer "read once, pass down").
# - All LLM calls must go through the sidecar boundary (cache + strict JSON + health).
#
# Suggested future split (approx):
#   yureeka/config_flags.py            [MOD:CONFIG_FLAGS]
#   yureeka/meta/versioning.py         [MOD:VERSION_STAMP]
#   yureeka/meta/patch_tracker.py      [MOD:PATCH_TRACKER]
#   yureeka/llm/sidecar.py             [MOD:LLM_SIDECAR]
#   yureeka/llm/evidence_snippets.py   [MOD:LLM01_EVIDENCE]
#   yureeka/llm/query_structure.py     [MOD:LLM00_QSTRUCT]
#   yureeka/freshness/source_freshness.py  [MOD:FRESH01]
#   yureeka/core/pipeline.py           [MOD:PIPELINE_CORE]
#   yureeka/core/diffing.py            [MOD:DIFF_CORE]
#   yureeka/ui/app.py                  [MOD:UI_APP]
#
# Search anchors:
#   [MOD:CONFIG_FLAGS] [MOD:VERSION_STAMP] [MOD:PATCH_TRACKER]
#   [MOD:LLM_SIDECAR] [MOD:LLM01_EVIDENCE] [MOD:LLM00_QSTRUCT]
#   [MOD:PIPELINE_CORE] [MOD:DIFF_CORE] [MOD:UI_APP]
# =============================================================================

import io
import os
import re
import json
import time
import streamlit as st


# [MOD:HYPERPARAMS_V1]
# HYPERPARAMETERS (v1)
# Centralized "policy knobs" for deterministic + LLM/NLP layers.
#
# Design rules:
# - Defaults are frozen to preserve REFACTOR206 / LLM36 deterministic outcomes.
# - Overrides are OPTIONAL and must be explicit (env JSON or legacy env vars).
# - LLM cache keys may be salted with an LLM-only hyperparam fingerprint while
#   remaining backward-compatible (fallback to unsalted key).
#
# Override (optional):
#   - YUREEKA_HYPERPARAMS_JSON: JSON object (nested) merged into defaults.
#     Example: {"freshness":{"tiebreak":{"min_score_delta":5}}}
#
# Audit (attached additively into wrapper.debug):
#   - hyperparams_snapshot_v1
#   - hyperparams_fingerprint_v1
#   - hyperparams_fingerprint_llm_v1
#   - hyperparams_source_v1
import copy as _hp_copy
import hashlib as _hp_hashlib

def _hp__stable_json_v1(obj):
    try:
        return json.dumps(obj, sort_keys=True, separators=(",", ":"), ensure_ascii=False)
    except Exception:
        try:
            return json.dumps(str(obj), sort_keys=True, separators=(",", ":"), ensure_ascii=False)
        except Exception:
            return ""

def _hp__sha256_hex_v1(s: str) -> str:
    try:
        return _hp_hashlib.sha256((s or "").encode("utf-8", errors="ignore")).hexdigest()
    except Exception:
        return ""

def _hp__deep_update_v1(dst: dict, src: dict) -> dict:
    """Deep-merge src into dst (dict-only), returning dst."""
    try:
        if not isinstance(dst, dict) or not isinstance(src, dict):
            return dst
        for k, v in src.items():
            if isinstance(v, dict) and isinstance(dst.get(k), dict):
                _hp__deep_update_v1(dst[k], v)
            else:
                dst[k] = v
    except Exception:
        return dst
    return dst

_HYPERPARAMS_V1_DEFAULTS = {
    "llm": {
        # LLM01 acceptance gates (deterministic rules decide)
        "evidence_confidence_threshold": 0.75,
        "evidence_score_tie_delta": 2.5,
        "qstruct_confidence_threshold": 0.75,
        "strict_mode": True,
        "cache": {
            "max_mem_entries": 128,
            "dir": None,  # if None, fallback to YUREEKA_LLM_CACHE_DIR env or default
            "salt_with_hyperparams": True,   # salt cache keys (with fallback to unsalted)
            "salt_scope": "llm_only",        # reserved: llm_only / full
        },
    },
    "freshness": {
        # Deterministic freshness curve (age days -> base score)
        "score_cutoffs_days": [30, 180, 365, 730],
        "score_values": [100.0, 85.0, 70.0, 55.0, 40.0],
        "bucket_labels": ["≤30d", "≤6mo", "≤1y", "≤2y", ">2y"],
        "tiebreak": {
            # Only affects behavior when ENABLE_SOURCE_FRESHNESS_TIEBREAK is enabled.
            # min_score_delta=0.0 preserves existing LLM36 behavior (no quantization).
            "min_score_delta": 0.0,
            # Reserved for future: prefer explicit published_at over inferred date.
            "prefer_published_at": True,
            # Reserved for future: url_sort_v1 / source_index_v1
            "stable_fallback": "url_sort_v1",
        },
    },

    "nlp": {
        "query_frame": {
            # Enable deterministic enrichment of metric_families (proposal-only; does NOT change winners/values).
            "enable_enrichment": True,
            # Maximum number of boost terms shown in preview (actual boosting gated by ENABLE_LLM_QUERY_FRAME and/or ENABLE_NLP_QUERY_BOOST).
            "max_boost_terms": 8,
            # Confidence threshold for including derived families (0..1).
            "min_family_confidence": 0.25,
        },
        "query_boost": {
            "max_terms": 10,
            "max_chars": 500
        },
    },

    "ops": {
        "sheets_read_cache_ttl_sec": 45,
        "search_cache_ttl_hours": 24,
        "max_history_items": 50,
    },
}

def _hp__coerce_number_v1(v, default=None):
    try:
        if v is None:
            return default
        if isinstance(v, (int, float)):
            return v
        s = str(v).strip()
        if not s:
            return default
        return float(s) if ("." in s or "e" in s.lower()) else int(s)
    except Exception:
        return default

def _hp__coerce_bool_v1(v, default=None):
    try:
        if v is None:
            return default
        if isinstance(v, bool):
            return v
        s = str(v).strip().lower()
        if s in ("1", "true", "yes", "y", "on"):
            return True
        if s in ("0", "false", "no", "n", "off"):
            return False
    except Exception:
        return default
    return default

def _hp__load_overrides_env_json_v1() -> tuple:
    """Return (overrides_dict, source_str)."""
    try:
        raw = os.environ.get("YUREEKA_HYPERPARAMS_JSON") or os.environ.get("YUREEKA_HYPERPARAMS_V1_JSON") or ""
    except Exception:
        raw = ""
    raw = str(raw or "").strip()
    if not raw:
        return ({}, "defaults")
    try:
        obj = json.loads(raw)
        if isinstance(obj, dict):
            return (obj, "env:YUREEKA_HYPERPARAMS_JSON")
    except Exception:
        pass
    return ({}, "env_json_invalid")

def _hp__load_hyperparams_v1() -> tuple:
    """Resolve hyperparams (defaults + env overrides + legacy env vars)."""
    hp = _hp_copy.deepcopy(_HYPERPARAMS_V1_DEFAULTS)
    src = "defaults"
    # 1) env JSON merge
    env_obj, env_src = _hp__load_overrides_env_json_v1()
    if isinstance(env_obj, dict) and env_obj:
        _hp__deep_update_v1(hp, env_obj)
        src = env_src
    elif env_src == "env_json_invalid":
        src = env_src

    # 2) legacy env vars (backward-compatible)
    try:
        v = os.environ.get("YUREEKA_LLM01_EVIDENCE_CONFIDENCE_THRESHOLD")
        nv = _hp__coerce_number_v1(v, None)
        if nv is not None:
            hp["llm"]["evidence_confidence_threshold"] = float(nv)
            src = (src + "+env:LLM01_CONF") if src else "env:LLM01_CONF"
    except Exception:
        pass
    try:
        v = os.environ.get("YUREEKA_LLM01_EVIDENCE_SCORE_TIE_DELTA")
        nv = _hp__coerce_number_v1(v, None)
        if nv is not None:
            hp["llm"]["evidence_score_tie_delta"] = float(nv)
            src = (src + "+env:LLM01_TIE") if src else "env:LLM01_TIE"
    except Exception:
        pass
    try:
        v = os.environ.get("YUREEKA_LLM00_QSTRUCT_CONFIDENCE_THRESHOLD")
        nv = _hp__coerce_number_v1(v, None)
        if nv is not None:
            hp["llm"]["qstruct_confidence_threshold"] = float(nv)
            src = (src + "+env:QSTRUCT") if src else "env:QSTRUCT"
    except Exception:
        pass

    # 3) sanitize known shapes
    try:
        cut = hp.get("freshness", {}).get("score_cutoffs_days")
        vals = hp.get("freshness", {}).get("score_values")
        labels = hp.get("freshness", {}).get("bucket_labels")
        if not (isinstance(cut, list) and isinstance(vals, list) and isinstance(labels, list)):
            raise ValueError("freshness curve invalid shape")
        # expect labels == values length and values == cutoffs+1
        if len(vals) != (len(cut) + 1) or len(labels) != len(vals):
            raise ValueError("freshness curve length mismatch")
    except Exception:
        # fallback to defaults on invalid override
        try:
            hp["freshness"] = _hp_copy.deepcopy(_HYPERPARAMS_V1_DEFAULTS["freshness"])
            src = (src + "+sanitize:freshness_defaults") if src else "sanitize:freshness_defaults"
        except Exception:
            pass

    return (hp, src)

_HYPERPARAMS_V1, _HYPERPARAMS_SOURCE_V1 = _hp__load_hyperparams_v1()

def _yureeka_get_hyperparams_v1() -> dict:
    return _HYPERPARAMS_V1

def _yureeka_hp_get_v1(path: str, default=None):
    """Get a nested hyperparam by dot-path (e.g. 'freshness.tiebreak.min_score_delta')."""
    try:
        parts = [p for p in str(path or "").split(".") if p]
        cur = _HYPERPARAMS_V1
        for p in parts:
            if not isinstance(cur, dict):
                return default
            cur = cur.get(p)
        return default if cur is None else cur
    except Exception:
        return default

def _yureeka_hyperparams_fingerprint_v1(scope: str = "full") -> str:
    try:
        if str(scope or "").strip().lower() == "llm_only":
            sub = {
                "llm": _HYPERPARAMS_V1.get("llm", {}),
                "freshness": {"tiebreak": (_HYPERPARAMS_V1.get("freshness", {}) or {}).get("tiebreak", {})},
            }
        else:
            sub = _HYPERPARAMS_V1
        return _hp__sha256_hex_v1(_hp__stable_json_v1(sub))
    except Exception:
        return ""

HYPERPARAMS_V1 = _HYPERPARAMS_V1
HYPERPARAMS_FINGERPRINT_V1 = _yureeka_hyperparams_fingerprint_v1("full")
HYPERPARAMS_FINGERPRINT_LLM_V1 = _yureeka_hyperparams_fingerprint_v1("llm_only")

# Derived knobs (validated) used in hot paths
try:
    _FRESH01_SCORE_CUTOFFS_DAYS_V1 = list((_HYPERPARAMS_V1.get("freshness", {}) or {}).get("score_cutoffs_days") or [30, 180, 365, 730])
    _FRESH01_SCORE_VALUES_V1 = list((_HYPERPARAMS_V1.get("freshness", {}) or {}).get("score_values") or [100.0, 85.0, 70.0, 55.0, 40.0])
    _FRESH01_BUCKET_LABELS_V1 = list((_HYPERPARAMS_V1.get("freshness", {}) or {}).get("bucket_labels") or ["≤30d", "≤6mo", "≤1y", "≤2y", ">2y"])
except Exception:
    _FRESH01_SCORE_CUTOFFS_DAYS_V1 = [30, 180, 365, 730]
    _FRESH01_SCORE_VALUES_V1 = [100.0, 85.0, 70.0, 55.0, 40.0]
    _FRESH01_BUCKET_LABELS_V1 = ["≤30d", "≤6mo", "≤1y", "≤2y", ">2y"]

# REFACTOR155: Downsizing (low-risk): prune obsolete legacy diff-binding fallback ladders (no behavior change).
# REFACTOR177: Downsizing (low-risk): remove legacy diff_metrics_by_name authority/binding scaffolding; Diff Panel V2 remains authoritative.

try:
    import requests  # type: ignore
except Exception:  # pragma: no cover
    requests = None  # type: ignore

# FRESH00: global URL normalizer used by freshness scoring + tie-break beacons.
# Deterministic, side-effect free.
# Note: This does not perform any network IO; it only normalizes strings.
def _normalize_url(s: str) -> str:
    try:
        t = (s or "").strip()
        if not t:
            return ""
        # Add scheme if missing.
        if not re.match(r"^https?://", t, flags=re.I):
            if re.match(r"^[a-z0-9.-]+\.[a-z]{2,}(/.*)?$", t, flags=re.I):
                t = "https://" + t
            else:
                return t.lower()
        # Strip fragment.
        t = re.sub(r"#.*$", "", t)
        # Best-effort parse to normalize scheme/host casing and trailing slash.
        try:
            import urllib.parse as _up
            p = _up.urlsplit(t)
            scheme = (p.scheme or "https").lower()
            netloc = (p.netloc or "").lower()
            path = p.path or ""
            if path.endswith("/") and len(path) > 1:
                path = path[:-1]
            query = p.query or ""
            return _up.urlunsplit((scheme, netloc, path, query, ""))
        except Exception:
            return t.strip().lower()
    except Exception:
        try:
            return (s or "").strip().lower()
        except Exception:
            return ""


# REFACTOR108: Shared Sheets read-cache (used by sheets_get_all_values_cached). Keep TTL short to prevent stale baseline selection.
_SHEETS_READ_CACHE = {}
_SHEETS_READ_CACHE_TTL_SEC = int(_yureeka_hp_get_v1('ops.sheets_read_cache_ttl_sec', 45) or 45)
_SHEETS_LAST_READ_ERROR = None

# REFACTOR195: Sheets rate-limit cooldown (avoid repeated 429 loops; prefer cached/local fallbacks).
_SHEETS_RATE_LIMIT_COOLDOWN_UNTIL_TS_V1 = 0.0

_YUREEKA_SHEETS_SCOPES_DEBUG_V1 = {}  # REFACTOR100

def _yureeka_sheets_in_rate_limit_cooldown_v1() -> bool:
    try:
        return _sheets_now_ts() < float(_SHEETS_RATE_LIMIT_COOLDOWN_UNTIL_TS_V1 or 0.0)
    except Exception:
        return False

def _yureeka_sheets_mark_rate_limited_v1(err: Exception, cooldown_sec: int = 65):
    """Mark the Sheets API as rate-limited and activate a short cooldown to avoid repeated 429 loops.

    REFACTOR197: prune unused last-error / last-mark debug globals (they were never read).
    """
    global _SHEETS_RATE_LIMIT_COOLDOWN_UNTIL_TS_V1
    try:
        now = _sheets_now_ts()
        _SHEETS_RATE_LIMIT_COOLDOWN_UNTIL_TS_V1 = now + float(cooldown_sec or 60)
    except Exception:
        pass

def _yureeka_sheets_rate_limit_notice_once_v1(msg: str):
    """Emit a Streamlit warning at most once every ~20s (per session)."""
    try:
        k = "_yureeka_sheets_rate_limit_notice_ts_v1"
        now = _sheets_now_ts()
        last = float(st.session_state.get(k) or 0.0)
        if (now - last) > 20.0:
            st.session_state[k] = now
            st.warning(msg)
    except Exception:
        pass



def _coerce_google_oauth_scopes(scopes) -> list:
    """Return a de-duplicated list of string OAuth scopes (drops non-strings).

    This is a defensive guard for Google Sheets auth. If any non-string value
    (e.g., a dict) contaminates the scopes list, Google auth may crash with:
      'sequence item N: expected str instance, dict found'
    """
    raw = []
    try:
        raw = list(scopes) if scopes is not None else []
    except Exception:
        raw = []

    out = []
    dropped = 0
    for s in raw:
        if isinstance(s, str):
            out.append(s)
        else:
            dropped += 1

    # de-dup while preserving order
    seen = set()
    deduped = []
    for s in out:
        if s in seen:
            continue
        seen.add(s)
        deduped.append(s)

    if not deduped:
        # sensible fallback
        deduped = [
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive",
        ]

        # REFACTOR100: capture a structured debug block once when scopes are contaminated.
    try:
        if dropped and (not _YUREEKA_SHEETS_SCOPES_DEBUG_V1.get('sheets_scopes_v1')):
            _YUREEKA_SHEETS_SCOPES_DEBUG_V1['sheets_scopes_v1'] = {
                'raw_types': [type(x).__name__ for x in (raw or [])],
                'sanitized': list(deduped or []),
            }
            try:
                st.session_state.setdefault('debug', {})
                st.session_state['debug']['sheets_scopes_v1'] = dict(_YUREEKA_SHEETS_SCOPES_DEBUG_V1['sheets_scopes_v1'])
            except Exception:
                pass
    except Exception:
        pass

    # warn once per session (non-fatal)
    try:
        if dropped:
            _k = "_coerce_google_oauth_scopes_warned_v1"
            if hasattr(st, "session_state") and not st.session_state.get(_k):
                st.session_state[_k] = True
                st.warning(f"⚠️ Dropped {dropped} invalid Google OAuth scope(s) from configuration.")
    except Exception:
        pass

    return deduped

import base64
import hashlib
import gspread
from google.oauth2.service_account import Credentials
from typing import Dict, List, Optional, Any, Union, Tuple
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup
from pydantic import BaseModel, Field, ValidationError, ConfigDict

# [MOD:VERSION_STAMP]
# VERSION STAMP (LOCKED)
# REFACTOR12: single-source-of-truth version lock.
# - All JSON outputs must stamp using _yureeka_get_code_version().
# - The getter is intentionally "frozen" via a default arg to prevent late overrides.
_YUREEKA_CODE_VERSION_LOCK = "NLP08"
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
# REFACTOR206: Release Candidate freeze (no pipeline behavior change).
YUREEKA_RELEASE_CANDIDATE_V1 = False
YUREEKA_RELEASE_TAG_V1 = "LLM33"
YUREEKA_FREEZE_MODE_V1 = True
# Small default regression set (optional; used for manual triad smoke tests).
YUREEKA_REGRESSION_QUESTIONS_V1 = [
    "what is the size of the global ev market?",
    "what are global EV sales YTD 2025?",
    "how many global EV chargers will there be in 2040?",
    "what is global EV charging investment in 2040?",
    "what is the CAGR of global EV chargers from 2026 to 2040?",
]

# [MOD:LLM_SIDECAR]
# === LLM SIDECAR (LLM01) ======================================================
# Hybrid NLP/LLM "sidecar assist" layer scaffolding.
# Design rule: LLM assists; deterministic rules decide.
# All feature flags are OFF by default to preserve REFACTOR206 behavior.

ENABLE_LLM_EVIDENCE_SNIPPETS = False
ENABLE_LLM_SOURCE_CLUSTERING = False
ENABLE_LLM_QUERY_FRAME = False
ENABLE_LLM_QUERY_STRUCTURE_FALLBACK = False
ENABLE_LLM_ANOMALY_FLAGS = False

# NLP03: Deterministic NLP assist flags (default OFF; determinism preserved)
ENABLE_NLP_QUERY_BOOST = False


# LLM12: Optional diagnostics/testing toggles (default OFF)
# - LLM_BYPASS_CACHE: ignore disk/mem cache to verify live calls (may change snippet ranking output).
# - ENABLE_LLM_SMOKE_TEST: run a one-shot provider connectivity check and attach non-sensitive diag.
LLM_BYPASS_CACHE = False
ENABLE_LLM_SMOKE_TEST = False

# LLM29/LLM30: One-run-only cache refresh (default OFF)
# - If enabled, bypass cache for the FIRST eligible cached key only, then self-consume.
# - LLM30 safety: if the key already exists on disk, do NOT overwrite the disk cache file.
LLM_FORCE_REFRESH_ONCE = False



# LLM18: Evidence snippet assist acceptance policy (deterministic rules decide)
# These thresholds gate whether an LLM proposal is *accepted* (i.e., actually used) vs. rejected.
# Env overrides (optional):
#   YUREEKA_LLM01_EVIDENCE_CONFIDENCE_THRESHOLD
#   YUREEKA_LLM01_EVIDENCE_SCORE_TIE_DELTA
LLM01_EVIDENCE_CONFIDENCE_THRESHOLD = float(_yureeka_hp_get_v1('llm.evidence_confidence_threshold', 0.75) or 0.75)
LLM01_EVIDENCE_SCORE_TIE_DELTA = float(_yureeka_hp_get_v1('llm.evidence_score_tie_delta', 2.5) or 2.5)
LLM01_EVIDENCE_FORCE_CALL = False  # If True, call LLM rank even without a tie-set (proposal still accepted only on ties).

# LLM22: Query-structure LLM fallback (opt-in) acceptance policy.
# Env override (optional): YUREEKA_LLM00_QSTRUCT_CONFIDENCE_THRESHOLD
LLM00_QUERY_STRUCTURE_CONFIDENCE_THRESHOLD = float(_yureeka_hp_get_v1('llm.qstruct_confidence_threshold', 0.75) or 0.75)


def _yureeka__parse_boolish_v1(v: Any) -> Optional[bool]:
    """Parse bool-ish env strings safely. Returns None if unparseable."""
    try:
        if v is None:
            return None
        if isinstance(v, bool):
            return bool(v)
        s = str(v).strip().lower()
        if s in ("1", "true", "yes", "y", "on", "enable", "enabled"):
            return True
        if s in ("0", "false", "no", "n", "off", "disable", "disabled"):
            return False
        return None
    except Exception:
        return None


def _yureeka__parse_floatish_v1(v: Any) -> Optional[float]:
    """Parse float-ish env/secrets strings safely. Returns None if unparseable."""
    try:
        if v is None:
            return None
        if isinstance(v, (int, float)):
            return float(v)
        s = str(v).strip()
        if not s:
            return None
        return float(s)
    except Exception:
        return None



def _yureeka_llm_flag_effective_v1(flag_name: str) -> Tuple[bool, str]:
    """Resolve effective flag value with optional UI/secrets/env override.

    Precedence (highest → lowest):
      0) Streamlit session_state override (if present) under:
         - st.session_state['YUREEKA_LLM_FLAGS'][FLAG_NAME]
         - st.session_state['YUREEKA_FLAGS'][FLAG_NAME]
         - st.session_state['LLM_FLAGS'][FLAG_NAME]
      1) Streamlit secrets override (if present), supports:
         - st.secrets[FLAG_NAME]
         - st.secrets['general'][FLAG_NAME] (common Streamlit pattern)
         - st.secrets['YUREEKA_LLM_FLAGS'][FLAG_NAME] (also supports 'YUREEKA_FLAGS' / 'LLM_FLAGS')
      2) Environment variable: YUREEKA_<FLAG_NAME>
      3) Code default: globals()[FLAG_NAME]

    Design note: Streamlit's session_state and secrets sub-sections are often
    "mapping-like" proxies rather than plain dicts. We therefore treat any
    object with a working .get(...) as eligible.
    """
    _fname = str(flag_name or '').strip()
    _sentinel = object()

    def _get_map_val(mobj: Any, key: str):
        try:
            if mobj is None or not key:
                return _sentinel
            getf = getattr(mobj, "get", None)
            if not callable(getf):
                return _sentinel
            return getf(key, _sentinel)
        except Exception:
            return _sentinel

    # 0) Streamlit UI/session_state override (highest precedence)
    try:
        _st0 = globals().get('st')
        _ss0 = getattr(_st0, 'session_state', None) if _st0 is not None else None
        if _ss0 is not None and _fname:
            for root in ('YUREEKA_LLM_FLAGS', 'YUREEKA_FLAGS', 'LLM_FLAGS'):
                try:
                    sub = _get_map_val(_ss0, root)
                    if sub is _sentinel:
                        continue
                    raw = _get_map_val(sub, _fname)
                    if raw is _sentinel:
                        continue
                    pv = _yureeka__parse_boolish_v1(raw)
                    if pv is not None:
                        return (bool(pv), f"ui:{root}.{_fname}")
                except Exception:
                    continue
    except Exception:
        pass

    # 1) Streamlit secrets override (safe: we only record which key provided the value).
    try:
        _st = globals().get('st')
        _secrets = getattr(_st, 'secrets', None) if _st is not None else None
        if _secrets is not None and _fname:
            # Nested locations (include [general] / [GENERAL])
            for root in ('YUREEKA_LLM_FLAGS', 'YUREEKA_FLAGS', 'LLM_FLAGS', 'general', 'GENERAL'):
                try:
                    sub = _get_map_val(_secrets, root)
                    if sub is _sentinel:
                        continue
                    raw = _get_map_val(sub, _fname)
                    if raw is _sentinel:
                        continue
                    pv = _yureeka__parse_boolish_v1(raw)
                    if pv is not None:
                        return (bool(pv), f"secrets:{root}.{_fname}")
                except Exception:
                    continue
            # Direct key
            try:
                raw = _get_map_val(_secrets, _fname)
                if raw is not _sentinel:
                    pv = _yureeka__parse_boolish_v1(raw)
                    if pv is not None:
                        return (bool(pv), f"secrets:{_fname}")
            except Exception:
                pass
    except Exception:
        pass

    # 2) Env override
    try:
        env_name = "YUREEKA_" + _fname
        raw = os.environ.get(env_name)
        if raw is not None:
            pv = _yureeka__parse_boolish_v1(raw)
            if pv is not None:
                return (bool(pv), "env:" + env_name)
    except Exception:
        pass

    # 3) Code default
    try:
        return (bool(globals().get(_fname)), "code:" + _fname)
    except Exception:
        return (False, "default")



def _yureeka_llm_flag_bool_v1(flag_name: str) -> bool:
    try:
        return bool(_yureeka_llm_flag_effective_v1(flag_name)[0])
    except Exception:
        return False


def _yureeka_llm_flags_snapshot_v1() -> dict:
    """Non-sensitive effective flag snapshot (helps diagnose why a feature didn't run)."""
    out = {}
    for k in (
        "ENABLE_LLM_EVIDENCE_SNIPPETS",
        "ENABLE_LLM_SOURCE_CLUSTERING",
        "ENABLE_LLM_QUERY_FRAME",
        "ENABLE_LLM_QUERY_STRUCTURE_FALLBACK",
        "ENABLE_NLP_QUERY_BOOST",
        "ENABLE_LLM_ANOMALY_FLAGS",
        "ENABLE_LLM_DATASET_LOGGING",
        "LLM_BYPASS_CACHE",
        "ENABLE_LLM_SMOKE_TEST",
        "LLM_FORCE_REFRESH_ONCE",
    ):
        try:
            v, src = _yureeka_llm_flag_effective_v1(k)
            out[k] = {"value": bool(v), "source": str(src)[:80]}
        except Exception:
            out[k] = {"value": False, "source": "error"}
    # LLM12: surface cache-bypass state (non-sensitive).
    try:
        _bypass, _src = _yureeka_llm_flag_effective_v1("LLM_BYPASS_CACHE")
        out["cache_bypass"] = {"enabled": bool(_bypass), "source": str(_src)[:80]}
    except Exception:
        pass

    return out


def _llm02__tokenize_v1(s: str) -> List[str]:
    try:
        t = (s or "").lower()
        t = re.sub(r"[^a-z0-9]+", " ", t)
        toks = [w for w in t.split() if len(w) > 2]
        return toks[:160]
    except Exception:
        return []


def _llm02__jaccard_v1(a: set, b: set) -> float:
    try:
        if not a or not b:
            return 0.0
        inter = len(a.intersection(b))
        union = len(a.union(b))
        return float(inter) / float(union) if union else 0.0
    except Exception:
        return 0.0


def _llm02_cluster_urls_v1(
    urls: List[str],
    search_results: List[dict],
    *,
    jaccard_threshold: float = 0.78,
) -> Tuple[List[str], dict]:
    """Cluster near-duplicate search results and reorder URLs so cluster primaries come first.

    This does NOT drop URLs; it only reorders to reduce duplicate scraping when `num_sources` is small.
    Deterministic, NLP-first. Optional LLM tiebreakers are deferred to a later patch.
    """
    dbg = {
        "v": "llm02_source_cluster_v1",
        "enabled": True,
        "jaccard_threshold": float(jaccard_threshold),
        "urls_before": int(len(urls or [])),
        "urls_after": int(len(urls or [])),
        "clusters_n": 0,
        "moved_to_front": 0,
        "primaries": [],
    }
    try:
        if not isinstance(urls, list) or not urls:
            dbg["enabled"] = False
            dbg["reason"] = "no_urls"
            return (urls or [], dbg)
        if not isinstance(search_results, list) or not search_results:
            dbg["enabled"] = False
            dbg["reason"] = "no_search_results"
            return (urls or [], dbg)

        # Map normalized URL -> (title, snippet)
        meta = {}
        for r in (search_results or []):
            if not isinstance(r, dict):
                continue
            u = str((r.get("link") or r.get("url") or "")).strip()
            if not u:
                continue
            try:
                # Keep consistent with fetch_web_context normalization
                u = u.strip()
            except Exception:
                pass
            meta[u] = {
                "title": str(r.get("title") or "")[:220],
                "snippet": str(r.get("snippet") or "")[:420],
                "source": str(r.get("source") or "")[:80],
            }

        # Token sets for similarity; if missing, fall back to URL tokens.
        token_sets = []
        for u in urls:
            m = meta.get(u) or {}
            t = (m.get("title") or "") + " " + (m.get("snippet") or "")
            toks = set(_llm02__tokenize_v1(t))
            if not toks:
                toks = set(_llm02__tokenize_v1(u))
            token_sets.append(toks)

        clusters = []  # list of lists of indices into urls
        for i, u in enumerate(urls):
            placed = False
            for c in clusters:
                try:
                    rep_i = c[0]
                    sim = _llm02__jaccard_v1(token_sets[i], token_sets[rep_i])
                    if sim >= float(jaccard_threshold):
                        c.append(i)
                        placed = True
                        break
                except Exception:
                    continue
            if not placed:
                clusters.append([i])

        dbg["clusters_n"] = int(len(clusters))

        # Choose primary per cluster using deterministic heuristics (reliability label if available).
        fn_rel = globals().get("classify_source_reliability")
        def _score(u: str) -> int:
            s = 0
            try:
                if callable(fn_rel):
                    lab = str(fn_rel(u) or "")
                    if "✅" in lab:
                        s += 300
                    elif "⚠️" in lab:
                        s += 200
                    elif "❌" in lab:
                        s += 0
            except Exception:
                pass
            try:
                # Prefer shorter, cleaner URLs
                s += max(0, 60 - min(60, len(str(u or ""))))
            except Exception:
                pass
            try:
                # Tiny domain heuristic
                from urllib.parse import urlsplit
                host = (urlsplit(u).netloc or "").lower()
                if host.endswith(".gov") or host.endswith(".edu") or host.endswith(".org"):
                    s += 20
            except Exception:
                pass
            return int(s)

        primaries = []
        for c in clusters:
            best_i = c[0]
            best_s = -10**9
            for i in c:
                sc = _score(urls[i])
                if sc > best_s:
                    best_s = sc
                    best_i = i
            primaries.append(best_i)

        # Cluster order is by first appearance (stable).
        seen = set()
        reordered = []
        for best_i in primaries:
            u = urls[best_i]
            if u not in seen:
                reordered.append(u)
                seen.add(u)
                dbg["primaries"].append(str(u)[:180])
        for u in urls:
            if u not in seen:
                reordered.append(u)
                seen.add(u)

        try:
            # Count how many URLs changed position into the primary prefix
            prefix = set(reordered[: len(primaries)])
            moved = 0
            for j, u in enumerate(urls):
                if u in prefix and j >= len(primaries):
                    moved += 1
            dbg["moved_to_front"] = int(moved)
        except Exception:
            dbg["moved_to_front"] = 0

        return (reordered, dbg)
    except Exception as e:
        dbg["enabled"] = False
        dbg["reason"] = "exception:" + str(type(e).__name__)
        return (urls or [], dbg)

# Strict mode (future patches): enforce schema-checked JSON responses and hard validators.
LLM_STRICT_MODE = bool(_yureeka_hp_get_v1('llm.strict_mode', True))

# Debug-only dataset logging (default OFF; never required for correctness).
ENABLE_LLM_DATASET_LOGGING = False

# Optional deterministic cache (disk + tiny in-memory LRU). Never required for correctness.
LLM_CACHE_DIR_V1 = str((_yureeka_hp_get_v1("llm.cache.dir", None) or os.environ.get("YUREEKA_LLM_CACHE_DIR") or ".yureeka_llm_cache"))
LLM_CACHE_MAX_MEM_ENTRIES_V1 = int(_yureeka_hp_get_v1('llm.cache.max_mem_entries', 128) or 128)
_LLM_CACHE_MEM_V1: dict = {}
_LLM_CACHE_MEM_ORDER_V1: list = []


# -------------------------------
# LLM03 — Query framing / intent decomposition (proposal-only, strict validation)
# -------------------------------

_LLM03_ALLOWED_METRIC_FAMILIES_V1 = [
    # Mirror classify_question_signals() intent buckets (stable, deterministic)
    "market_size",
    "growth_forecast",
    "competitive_landscape",
    "pricing",
    "consumer_demand",
    "supply_chain",
    "regulation",
    "investment",
    "macro_outlook",
]

_LLM03_ALLOWED_GEO_SCOPES_V1 = ["global", "regional", "country", "unknown"]
_LLM03_ALLOWED_TIME_SCOPES_V1 = ["current", "historical", "forecast", "ytd", "unknown"]

_LLM03_ALLOWED_UNIT_PREFS_V1 = [
    "usd", "eur", "sgd", "cny", "jpy", "inr",
    "percent", "percentage",
    "vehicles", "units", "chargers",
    "billion", "million",
]


# NLP01: Deterministic query-frame enrichment (regex + expected_metric_ids)
# - Purpose: surface metric_family hints (e.g., "market_size", "growth_forecast") in query_frame_v1
# - Safety: proposal-only; does NOT affect metric winners/values unless downstream flags explicitly use it
def _nlp01_derive_metric_families_v1(
    q: str,
    base_signals: Optional[Dict[str, Any]] = None,
) -> Tuple[List[str], Dict[str, Any]]:
    base_signals = base_signals if isinstance(base_signals, dict) else {}
    q0 = (q or "").strip().lower()

    max_terms = int(((HYPERPARAMS_V1.get("nlp") or {}).get("query_frame") or {}).get("max_boost_terms", 8) or 8)
    min_conf = float(((HYPERPARAMS_V1.get("nlp") or {}).get("query_frame") or {}).get("min_family_confidence", 0.25) or 0.25)

    mids = base_signals.get("expected_metric_ids") or []
    if not isinstance(mids, list):
        mids = []
    mids_l = [str(x).lower().strip() for x in mids if str(x).strip()]

    kw_hits: List[str] = []
    fam_hits: List[str] = []

    # From expected_metric_ids (category-derived)
    def _add_fam(f: str, why: str):
        if f and f not in fam_hits:
            fam_hits.append(f)
        if why and why not in kw_hits:
            kw_hits.append(why)

    for mid in mids_l:
        if "market_size" in mid or "market value" in mid or "market_value" in mid:
            _add_fam("market_size", f"mid:{mid}")
        if "cagr" in mid or "growth" in mid or "forecast" in mid:
            _add_fam("growth_forecast", f"mid:{mid}")
        if "share" in mid or "top_players" in mid or "competitive" in mid:
            _add_fam("competitive_landscape", f"mid:{mid}")
        if "price" in mid or "asp" in mid or "pricing" in mid:
            _add_fam("pricing", f"mid:{mid}")
        if "users" in mid or "penetration" in mid or "demand" in mid:
            _add_fam("consumer_demand", f"mid:{mid}")
        if "capacity" in mid or "shipments" in mid or "supply" in mid:
            _add_fam("supply_chain", f"mid:{mid}")
        if "capex" in mid or "investment" in mid or "spend" in mid or "expenditure" in mid:
            _add_fam("investment", f"mid:{mid}")
        if "gdp" in mid or "inflation" in mid or "interest" in mid or "exchange" in mid:
            _add_fam("macro_outlook", f"mid:{mid}")

    # From query text (lightweight patterns)
    if "market size" in q0 or ("market" in q0 and "size" in q0) or ("market" in q0 and "value" in q0):
        _add_fam("market_size", "kw:market_size")
    if "cagr" in q0 or "growth rate" in q0 or ("growth" in q0 and "forecast" in q0):
        _add_fam("growth_forecast", "kw:growth")
    if "market share" in q0 or ("share" in q0 and "market" in q0) or "top players" in q0:
        _add_fam("competitive_landscape", "kw:share")
    if "price" in q0 or "pricing" in q0 or "asp" in q0:
        _add_fam("pricing", "kw:price")
    if "investment" in q0 or "capex" in q0 or "spend" in q0 or "expenditure" in q0:
        _add_fam("investment", "kw:investment")

    # Simple confidence: proportion of evidence signals; deterministic and bounded
    evidence_n = float(len(fam_hits) + len(kw_hits))
    conf = 0.0
    try:
        conf = min(1.0, evidence_n / 6.0)  # saturate at ~6 evidence points
    except Exception:
        conf = 0.0

    families_out = fam_hits if conf >= min_conf else []

    dbg = {
        "min_family_confidence": min_conf,
        "confidence": conf,
        "expected_metric_ids_in": mids_l,
        "evidence": kw_hits[:max_terms],
        "families": list(families_out),
    }
    return list(families_out), dbg

def _llm03__deterministic_query_frame_v1(query: str, base_signals: Optional[Dict[str, Any]] = None, nlp01_families: Optional[List[str]] = None) -> dict:
    """Deterministically derive a query-frame from existing rule-based classifiers."""
    q_raw = (query or "").strip()
    q = q_raw.lower()
    base = base_signals if isinstance(base_signals, dict) else (classify_question_signals(q_raw) or {})

    years = base.get("years", []) if isinstance(base.get("years", []), list) else []
    regions = base.get("regions", []) if isinstance(base.get("regions", []), list) else []
    intents = base.get("intents", []) if isinstance(base.get("intents", []), list) else []

    # Metric families: clamp to allowed list
    metric_families = []
    for it in intents:
        s = str(it or "").strip()
        if s and s in _LLM03_ALLOWED_METRIC_FAMILIES_V1 and s not in metric_families:
            metric_families.append(s)

    # NLP01 enrichment (proposal-only)
    try:
        for f in (nlp01_families or []):
            ff = str(f or "").strip()
            if ff and ff in _LLM03_ALLOWED_METRIC_FAMILIES_V1 and ff not in metric_families:
                metric_families.append(ff)
    except Exception:
        pass

    # Geo scope
    geo_scope = "unknown"
    geo_name = ""
    try:
        if any(tok in q for tok in ("global", "worldwide", "world", "international")):
            geo_scope = "global"
        elif regions:
            geo_scope = "country" if len(regions) == 1 else "regional"
            geo_name = str(regions[0] or "")[:64]
    except Exception:
        pass

    # Time scope
    time_scope = "unknown"
    try:
        if "ytd" in q or "year to date" in q:
            time_scope = "ytd"
        elif "growth_forecast" in metric_families:
            time_scope = "forecast"
        elif years:
            if any(int(y) >= 2025 for y in years if str(y).isdigit()):
                time_scope = "forecast"
            else:
                time_scope = "historical"
        else:
            time_scope = "current"
    except Exception:
        time_scope = "unknown"

    # Unit preferences (very light)
    unit_prefs = []
    try:
        if "$" in q_raw or "usd" in q:
            unit_prefs.append("usd")
        if "eur" in q:
            unit_prefs.append("eur")
        if "sgd" in q:
            unit_prefs.append("sgd")
        if "%" in q_raw or "percent" in q or "percentage" in q:
            unit_prefs.append("percent")
        if "vehicle" in q:
            unit_prefs.append("vehicles")
        if "charger" in q:
            unit_prefs.append("chargers")
        if "billion" in q or "bn" in q:
            unit_prefs.append("billion")
        if "million" in q or "mn" in q:
            unit_prefs.append("million")
    except Exception:
        pass
    unit_prefs = [u for u in unit_prefs if u in _LLM03_ALLOWED_UNIT_PREFS_V1]

    # Normalize years
    years_out = []
    try:
        for y in years or []:
            try:
                yi = int(y)
            except Exception:
                continue
            if 1900 <= yi <= 2100 and yi not in years_out:
                years_out.append(yi)
        years_out = sorted(years_out)
    except Exception:
        years_out = []

    return {
        "metric_families": metric_families,
        "geo_scope": geo_scope if geo_scope in _LLM03_ALLOWED_GEO_SCOPES_V1 else "unknown",
        "geo_name": geo_name,
        "time_scope": time_scope if time_scope in _LLM03_ALLOWED_TIME_SCOPES_V1 else "unknown",
        "years": years_out,
        "unit_preferences": unit_prefs,
        "confidence": 0.55,
        "source": "deterministic",
    }

def _llm03__validate_query_frame_v1(frame: Any) -> Tuple[bool, str]:
    """Strict validator for LLM query frame proposals."""
    if not isinstance(frame, dict) or not frame:
        return (False, "not_dict")

    mf = frame.get("metric_families", [])
    if not isinstance(mf, list):
        return (False, "metric_families_not_list")
    for x in mf:
        s = str(x or "").strip()
        if not s or s not in _LLM03_ALLOWED_METRIC_FAMILIES_V1:
            return (False, "metric_family_invalid")

    geo_scope = str(frame.get("geo_scope") or "").strip()
    if geo_scope not in _LLM03_ALLOWED_GEO_SCOPES_V1:
        return (False, "geo_scope_invalid")

    time_scope = str(frame.get("time_scope") or "").strip()
    if time_scope not in _LLM03_ALLOWED_TIME_SCOPES_V1:
        return (False, "time_scope_invalid")

    years = frame.get("years", [])
    if not isinstance(years, list):
        return (False, "years_not_list")
    for y in years:
        try:
            yi = int(y)
        except Exception:
            return (False, "year_not_int")
        if yi < 1900 or yi > 2100:
            return (False, "year_out_of_range")

    up = frame.get("unit_preferences", [])
    if up is not None:
        if not isinstance(up, list):
            return (False, "unit_prefs_not_list")
        for u in up:
            s = str(u or "").strip().lower()
            if s and s not in _LLM03_ALLOWED_UNIT_PREFS_V1:
                return (False, "unit_pref_invalid")

    if "confidence" in frame:
        try:
            c = float(frame.get("confidence"))
            if c < 0.0 or c > 1.0:
                return (False, "confidence_out_of_range")
        except Exception:
            return (False, "confidence_not_number")

    try:
        if len(str(frame.get("geo_name") or "")) > 128:
            return (False, "geo_name_too_long")
    except Exception:
        pass

    return (True, "ok")

def _llm03_build_boost_terms_v1(query: str, frame: Optional[dict]) -> List[str]:
    """Deterministic term expansion based on query frame.

    Used by:
      - LLM query framing when ENABLE_LLM_QUERY_FRAME is enabled
      - NLP03 deterministic query boosting when ENABLE_NLP_QUERY_BOOST is enabled

    Determinism: stable ordering + stable truncation (hyperparam-capped).
    """
    q = (query or "").lower()
    terms: List[str] = []
    if not isinstance(frame, dict):
        return terms

    mfs = frame.get("metric_families") if isinstance(frame.get("metric_families"), list) else []
    geo_scope = str(frame.get("geo_scope") or "").strip()
    time_scope = str(frame.get("time_scope") or "").strip()
    years = frame.get("years") if isinstance(frame.get("years"), list) else []

    if "market_size" in mfs:
        for t in ["market size", "market value", "market revenue"]:
            if t not in terms and t not in q:
                terms.append(t)
    if "growth_forecast" in mfs:
        for t in ["forecast", "cagr", "projection"]:
            if t not in terms and t not in q:
                terms.append(t)
    if "investment" in mfs:
        for t in ["investment", "capex", "spending"]:
            if t not in terms and t not in q:
                terms.append(t)

    if geo_scope == "global" and "global" not in q:
        terms.append("global")
    if time_scope == "ytd" and "ytd" not in q:
        terms.append("ytd")

    try:
        if years:
            y = max(int(x) for x in years)
            ys = str(y)
            if ys not in q:
                terms.append(ys)
    except Exception:
        pass

    _lim = 10
    try:
        _lim = int(_yureeka_hp_get_v1("nlp.query_boost.max_terms", 10) or 10)
    except Exception:
        _lim = 10
    if _lim < 0:
        _lim = 0
    if _lim > 50:
        _lim = 50
    return terms[:_lim]

def _llm03_boost_web_query_v1(query: str, frame: Optional[dict], *, _force: bool = False) -> str:
    """Return boosted web-search query string when enabled; otherwise original query.

    Enabled when:
      - ENABLE_LLM_QUERY_FRAME is True, OR
      - NLP03: ENABLE_NLP_QUERY_BOOST is True (deterministic; no LLM call)

    Determinism: stable term selection + stable truncation.
    """
    q_raw = (query or "")

    try:
        _qf_on, _qf_src = _yureeka_llm_flag_effective_v1("ENABLE_LLM_QUERY_FRAME")
    except Exception:
        _qf_on, _qf_src = (bool(globals().get("ENABLE_LLM_QUERY_FRAME")), "code:ENABLE_LLM_QUERY_FRAME")

    try:
        _nb_on, _nb_src = _yureeka_llm_flag_effective_v1("ENABLE_NLP_QUERY_BOOST")
    except Exception:
        _nb_on, _nb_src = (bool(globals().get("ENABLE_NLP_QUERY_BOOST")), "code:ENABLE_NLP_QUERY_BOOST")

    if not _force and not (bool(_qf_on) or bool(_nb_on)):
        return q_raw
    if not isinstance(frame, dict):
        return q_raw

    try:
        terms = _llm03_build_boost_terms_v1(q_raw, frame)
        if not terms:
            return q_raw
        out = (q_raw.strip() + " " + " ".join([t.strip() for t in terms if str(t).strip()])).strip()
        _maxc = 500
        try:
            _maxc = int(_yureeka_hp_get_v1("nlp.query_boost.max_chars", 500) or 500)
        except Exception:
            _maxc = 500
        if _maxc < 50:
            _maxc = 50
        if _maxc > 2000:
            _maxc = 2000
        return out[:_maxc]
    except Exception:
        return q_raw

def _llm03_get_query_frame_v1(query: str, base_signals: Optional[Dict[str, Any]] = None) -> Tuple[dict, dict]:
    """Return (effective_frame, debug). LLM output is proposal-only and must pass strict validation."""
    q_raw = (query or "").strip()

    _qf_on, _qf_src = _yureeka_llm_flag_effective_v1("ENABLE_LLM_QUERY_FRAME")

    nlp01_fams: List[str] = []
    nlp01_dbg: Dict[str, Any] = {}
    try:
        nlp01_fams, nlp01_dbg = _nlp01_derive_metric_families_v1(q_raw, base_signals=base_signals)
    except Exception:
        nlp01_fams, nlp01_dbg = [], {}

    det = _llm03__deterministic_query_frame_v1(q_raw, base_signals=base_signals, nlp01_families=nlp01_fams)

    dbg: Dict[str, Any] = {
        "v": "llm03_query_frame_v1",
        "flag_enable_llm": bool(_qf_on),
        "flag_source": str(_qf_src)[:80],
        "provider": {},
        "deterministic_frame": det,
        "nlp01_enrichment_v1": nlp01_dbg,
        "llm_used": False,
        "llm_cache_hit": False,
        "llm_ok": False,
        "llm_reason": "",
        "llm_call_diag": {},
        "llm_candidate_frame": None,
        "effective_frame": det,
        "boost_preview_terms": [],
        "boosted_query_preview": "",
    }

    # NLP03: report deterministic query-boost flag state (even when LLM query framing is OFF).
    try:
        _nb_on, _nb_src = _yureeka_llm_flag_effective_v1("ENABLE_NLP_QUERY_BOOST")
    except Exception:
        _nb_on, _nb_src = (bool(globals().get("ENABLE_NLP_QUERY_BOOST")), "code:ENABLE_NLP_QUERY_BOOST")
    dbg["flag_enable_nlp_query_boost"] = bool(_nb_on)
    dbg["flag_source_nlp_query_boost"] = str(_nb_src)[:80]
    dbg["boost_effective_enabled"] = bool(bool(_qf_on) or bool(_nb_on))

    # Preview boosted query (deterministic)
    try:
        dbg["boost_preview_terms"] = _llm03_build_boost_terms_v1(q_raw, det)
        dbg["boosted_query_preview"] = _llm03_boost_web_query_v1(q_raw, det, _force=True)
    except Exception:
        pass

    if not bool(_qf_on):
        dbg["llm_reason"] = "flag_off"
        return (det, dbg)

    # Provider snapshot (non-sensitive)
    try:
        dbg["provider"] = _llm01_provider_status_v1()
    except Exception:
        dbg["provider"] = {"ok": False}

    try:
        model = os.environ.get("YUREEKA_LLM_MODEL") or os.environ.get("OPENAI_MODEL") or "gpt-4o-mini"
    except Exception:
        model = "gpt-4o-mini"

    prompt_version = "llm03_query_frame_v1"
    schema_version = "schema_frozen_v1"

    try:
        input_obj = {"query": q_raw, "deterministic_frame": det}
        input_hash = _yureeka_hash_text_v1(_yureeka__stable_json_dumps_v1(input_obj))
        q_hash = _yureeka_hash_text_v1(q_raw)
        key, key_fallback = get_llm_cache_key_salted_v1(str(model), prompt_version, schema_version, input_hash, "", q_hash)
    except Exception:
        key = ""

    candidate = None
    if key:
        cached = get_cached_llm_response(key)
        if cached is None and key_fallback:
            cached = get_cached_llm_response(key_fallback)
            if cached is not None:
                dbg["llm_cache_fallback_hit"] = True
        if cached is not None:
            dbg["llm_cache_hit"] = True
            try:
                _yureeka_llm_global_agg_update_v1("llm03_query_frame", {"ok": True, "status": None, "reason": "cache_hit", "model": str(model)}, cache_hit=True)
            except Exception:
                pass
            if isinstance(cached, dict) and "frame" in cached and isinstance(cached.get("frame"), dict):
                candidate = cached.get("frame")
            elif isinstance(cached, dict):
                candidate = cached

    if candidate is None:
        system_prompt = (
            "You are a query-framing assistant. "
            "Return ONLY a strict JSON object with keys:\n"
            "  metric_families: array of strings from allowed list\n"
            "  geo_scope: one of [global, regional, country, unknown]\n"
            "  geo_name: short string or empty\n"
            "  time_scope: one of [current, historical, forecast, ytd, unknown]\n"
            "  years: array of integers\n"
            "  unit_preferences: array of strings from allowed list\n"
            "  confidence: number 0-1\n"
            "No extra keys. No commentary."
        )
        user_payload = {
            "query": q_raw,
            "deterministic_frame": det,
            "allowed_metric_families": _LLM03_ALLOWED_METRIC_FAMILIES_V1,
            "allowed_geo_scopes": _LLM03_ALLOWED_GEO_SCOPES_V1,
            "allowed_time_scopes": _LLM03_ALLOWED_TIME_SCOPES_V1,
            "allowed_unit_preferences": _LLM03_ALLOWED_UNIT_PREFS_V1,
        }

        obj, call_diag = _yureeka_call_openai_chat_json_v1(
            model=str(model),
            system_prompt=system_prompt,
            user_payload=user_payload,
            timeout_sec=30,
            max_tokens=260,
        )
        dbg["llm_call_diag"] = call_diag or {}
        try:
            _yureeka_llm_global_agg_update_v1("llm03_query_frame", (call_diag or {}), cache_hit=False)
        except Exception:
            pass
        if isinstance(obj, dict) and obj:
            candidate = obj

        if key and candidate is not None:
            try:
                cache_llm_response(key, candidate)
            except Exception:
                pass
    else:
        dbg["llm_call_diag"] = {"ok": True, "reason": "cache_hit"}

    dbg["llm_used"] = True

    ok, reason = _llm03__validate_query_frame_v1(candidate)
    if ok:
        eff = {
            "metric_families": [str(x).strip() for x in (candidate.get("metric_families") or []) if str(x).strip()],
            "geo_scope": str(candidate.get("geo_scope") or "unknown").strip(),
            "geo_name": str(candidate.get("geo_name") or "")[:64],
            "time_scope": str(candidate.get("time_scope") or "unknown").strip(),
            "years": sorted({int(y) for y in (candidate.get("years") or []) if str(y).strip().lstrip("-").isdigit() and 1900 <= int(y) <= 2100}),
            "unit_preferences": [str(u).strip().lower() for u in (candidate.get("unit_preferences") or []) if str(u).strip()],
            "confidence": float(candidate.get("confidence")) if candidate.get("confidence") is not None else 0.6,
            "source": "llm",
        }
        dbg["llm_ok"] = True
        dbg["llm_reason"] = "ok"
        dbg["llm_candidate_frame"] = eff
        dbg["effective_frame"] = eff
        try:
            dbg["boost_preview_terms"] = _llm03_build_boost_terms_v1(q_raw, eff)
            dbg["boosted_query_preview"] = _llm03_boost_web_query_v1(q_raw, eff, _force=True)
        except Exception:
            pass
        return (eff, dbg)

    dbg["llm_ok"] = False
    dbg["llm_reason"] = str(reason or "invalid")
    dbg["llm_candidate_frame"] = candidate if isinstance(candidate, dict) else None
    dbg["effective_frame"] = det
    return (det, dbg)



def _yureeka__stable_json_dumps_v1(obj: Any) -> str:
    """Stable JSON string (sorted keys, compact separators)."""
    try:
        return json.dumps(obj, sort_keys=True, separators=(",", ":"), ensure_ascii=False)
    except Exception:
        try:
            return json.dumps(str(obj), sort_keys=True, separators=(",", ":"), ensure_ascii=False)
        except Exception:
            return ""


def get_llm_cache_key(
    model: str,
    prompt_version: str,
    schema_version: str,
    input_hash: str,
    url: str,
    question_hash: str,
) -> str:
    """Deterministic cache key for LLM proposals.

    NOTE: This key must remain stable across reruns for replayability.
    """
    payload = {
        "v": "llm_cache_key_v1",
        "model": str(model or ""),
        "prompt_version": str(prompt_version or ""),
        "schema_version": str(schema_version or ""),
        "input_hash": str(input_hash or ""),
        "url": str(url or ""),
        "question_hash": str(question_hash or ""),
    }
    s = _yureeka__stable_json_dumps_v1(payload)
    try:
        h = hashlib.sha256((s or "").encode("utf-8", errors="ignore")).hexdigest()
        return f"llm_v1_{h}"
    except Exception:
        return "llm_v1_unknown"


def get_llm_cache_key_salted_v1(
    model: str,
    prompt_version: str,
    schema_version: str,
    input_hash: str,
    url: str,
    question_hash: str,
) -> tuple:
    """Return (primary_key, fallback_key) for LLM cache lookups.

    LLM37: optionally salt the primary key with a hyperparam fingerprint (LLM-only)
    while preserving replayability via fallback to the legacy unsalted key.
    """
    legacy = get_llm_cache_key(model, prompt_version, schema_version, input_hash, url, question_hash)
    try:
        salt_on = bool(_yureeka_hp_get_v1("llm.cache.salt_with_hyperparams", True))
    except Exception:
        salt_on = True
    if not salt_on:
        return (legacy, "")
    try:
        scope = str(_yureeka_hp_get_v1("llm.cache.salt_scope", "llm_only") or "llm_only").strip().lower()
    except Exception:
        scope = "llm_only"
    hp_fp = ""
    try:
        if scope == "full":
            hp_fp = str(globals().get("HYPERPARAMS_FINGERPRINT_V1") or "")
        else:
            hp_fp = str(globals().get("HYPERPARAMS_FINGERPRINT_LLM_V1") or "")
    except Exception:
        hp_fp = ""
    if not hp_fp:
        return (legacy, "")
    payload = {
        "v": "llm_cache_key_v2",
        "hp_fp": hp_fp,
        "salt_scope": scope,
        "model": str(model or ""),
        "prompt_version": str(prompt_version or ""),
        "schema_version": str(schema_version or ""),
        "input_hash": str(input_hash or ""),
        "url": str(url or ""),
        "question_hash": str(question_hash or ""),
    }
    try:
        s = _yureeka__stable_json_dumps_v1(payload)
        h = hashlib.sha256((s or "").encode("utf-8", errors="ignore")).hexdigest()
        return (f"llm_v2_{h}", legacy)
    except Exception:
        return (legacy, "")



def _llm_cache_path_v1(key: str) -> str:
    try:
        safe = re.sub(r"[^a-zA-Z0-9_.-]+", "_", str(key or ""))
    except Exception:
        safe = "llm_v1_unknown"
    try:
        return os.path.join(str(LLM_CACHE_DIR_V1 or ".yureeka_llm_cache"), f"{safe}.json")
    except Exception:
        return f"{safe}.json"


def _llm_cache__mem_touch_v1(key: str):
    """Update tiny in-memory LRU order."""
    try:
        if key in _LLM_CACHE_MEM_ORDER_V1:
            _LLM_CACHE_MEM_ORDER_V1.remove(key)
        _LLM_CACHE_MEM_ORDER_V1.append(key)
        # prune
        max_n = int(LLM_CACHE_MAX_MEM_ENTRIES_V1 or 0) if str(LLM_CACHE_MAX_MEM_ENTRIES_V1 or "").isdigit() else 128
        max_n = max(0, max_n)
        while max_n and len(_LLM_CACHE_MEM_ORDER_V1) > max_n:
            old = _LLM_CACHE_MEM_ORDER_V1.pop(0)
            try:
                _LLM_CACHE_MEM_V1.pop(old, None)
            except Exception:
                pass
    except Exception:
        return


# LLM29/LLM30: One-run-only cache refresh state (non-sensitive).
def _yureeka_llm_force_refresh_once_state_v1() -> dict:
    """Return mutable state for LLM_FORCE_REFRESH_ONCE (non-sensitive)."""
    try:
        st = globals().get("_YUREEKA_LLM_FORCE_REFRESH_ONCE_STATE_V1")
        if not isinstance(st, dict):
            st = {
                "enabled": False,
                "source": "",
                "consumed": False,
                "key": "",
                "consumed_at": 0.0,
                "disk_existed": False,
                "disk_write_skipped": False,
                "backup_done": False,
            }
            globals()["_YUREEKA_LLM_FORCE_REFRESH_ONCE_STATE_V1"] = st
        try:
            on, src = _yureeka_llm_flag_effective_v1("LLM_FORCE_REFRESH_ONCE")
            st["enabled"] = bool(on)
            st["source"] = str(src)[:80]
        except Exception:
            pass
        return st
    except Exception:
        return {"enabled": False, "source": "error", "consumed": False, "key": "", "consumed_at": 0.0, "disk_existed": False, "disk_write_skipped": False, "backup_done": False}


def _yureeka_llm_force_refresh_once_should_bypass_v1(key: str) -> bool:
    """Return True if we should bypass cache for this key (and consume the one-shot)."""
    try:
        st = _yureeka_llm_force_refresh_once_state_v1()
        if not bool(st.get("enabled")) or bool(st.get("consumed")):
            return False
        if not key:
            return False

        mem_hit = False
        try:
            mem_hit = bool(key in _LLM_CACHE_MEM_V1)
        except Exception:
            mem_hit = False

        disk_hit = False
        try:
            path = _llm_cache_path_v1(key)
            disk_hit = bool(path) and os.path.exists(path)
        except Exception:
            disk_hit = False

        # Only consume when we would have served from cache (guarantees ≥1 live call).
        if not (mem_hit or disk_hit):
            return False

        st["consumed"] = True
        st["key"] = str(key)
        try:
            st["consumed_at"] = float(time.time())
        except Exception:
            st["consumed_at"] = 0.0

        st["disk_existed"] = bool(disk_hit)

        # LLM30 safety: if disk cache already exists, do NOT overwrite it on this forced refresh.
        st["disk_write_skipped"] = bool(disk_hit)
        st["backup_done"] = False
        return True
    except Exception:
        return False

def get_cached_llm_response(key: str) -> Any:
    """Return cached LLM payload for key, or None if missing/unreadable."""
    if not key:
        return None

    # LLM12: optional cache bypass (helps verify live provider calls).
    try:
        _bypass, _src = _yureeka_llm_flag_effective_v1("LLM_BYPASS_CACHE")
        if bool(_bypass):
            try:
                globals()["_YUREEKA_LLM_CACHE_BYPASS_STATUS_V1"] = {"enabled": True, "source": str(_src)[:80]}
            except Exception:
                pass
            return None
    except Exception:
        pass

    # LLM29/LLM30: one-run-only force refresh (consume once on first eligible cache-hit key).
    try:
        if _yureeka_llm_force_refresh_once_should_bypass_v1(key):
            return None
    except Exception:
        pass

    try:
        if key in _LLM_CACHE_MEM_V1:
            _llm_cache__mem_touch_v1(key)
            return _LLM_CACHE_MEM_V1.get(key)
    except Exception:
        pass

    path = _llm_cache_path_v1(key)
    try:
        if not path or not os.path.exists(path):
            return None
    except Exception:
        return None

    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        # allow either raw payload or wrapped {"payload": ...}
        payload = data.get("payload") if isinstance(data, dict) and "payload" in data else data
        _LLM_CACHE_MEM_V1[key] = payload
        _llm_cache__mem_touch_v1(key)
        return payload
    except Exception:
        return None

def cache_llm_response(key: str, payload: Any) -> bool:
    """Persist payload to cache. Best-effort; returns True on success."""
    if not key:
        return False

    try:
        _LLM_CACHE_MEM_V1[key] = payload
        _llm_cache__mem_touch_v1(key)
    except Exception:
        pass

    # LLM30: If LLM_FORCE_REFRESH_ONCE consumed a key that already existed on disk,
    # do NOT overwrite the disk cache file (preserves replay determinism/auditability).
    try:
        st = globals().get("_YUREEKA_LLM_FORCE_REFRESH_ONCE_STATE_V1")
        if isinstance(st, dict) and bool(st.get("disk_write_skipped")) and str(st.get("key") or "") == str(key or ""):
            return True
    except Exception:
        pass

    path = _llm_cache_path_v1(key)
    try:
        d = os.path.dirname(path)
        if d:
            os.makedirs(d, exist_ok=True)
    except Exception:
        # disk cache is optional
        return False

    try:
        tmp = f"{path}.tmp.{os.getpid()}"
        with open(tmp, "w", encoding="utf-8") as f:
            json.dump({"payload": payload}, f, ensure_ascii=False, sort_keys=True)
        os.replace(tmp, path)
        return True
    except Exception:
        try:
            # cleanup temp file if any
            if os.path.exists(tmp):
                os.remove(tmp)
        except Exception:
            pass
        return False

def _yureeka_llm_text_hash_v1(text: str) -> str:
    """Stable short hash for large text (do NOT store raw text in logs)."""
    try:
        h = hashlib.sha256((text or "").encode("utf-8", errors="ignore")).hexdigest()
        return h[:16]
    except Exception:
        return ""


def debug_llm_dataset_v1(
    *,
    url: str = "",
    scraped_text: str = "",
    snippet_windows: Any = None,
    rule_candidates: Any = None,
    chosen_winners: Any = None,
    schema_keys: Any = None,
    stage: str = "",
):
    """Debug-only dataset logger (OFF by default).

    Captures only hashes + small structured windows to enable later offline replay/testing.
    """
    _dl_on, _dl_src = _yureeka_llm_flag_effective_v1("ENABLE_LLM_DATASET_LOGGING")
    if not bool(_dl_on):
        return
    try:
        rec = {
            "url": str(url or ""),
            "text_hash": _yureeka_llm_text_hash_v1(scraped_text or ""),
            "snippet_windows": snippet_windows if snippet_windows is not None else [],
            "rule_candidates": rule_candidates if rule_candidates is not None else [],
            "chosen_winners": chosen_winners if chosen_winners is not None else {},
            "schema_keys": schema_keys if schema_keys is not None else [],
            "stage": str(stage or ""),
        }
        st.session_state.setdefault("debug", {})
        dbg = st.session_state.get("debug")
        if isinstance(dbg, dict):
            dbg.setdefault("llm_dataset_v1", [])
            if isinstance(dbg.get("llm_dataset_v1"), list):
                dbg["llm_dataset_v1"].append(rec)
                # cap
                if len(dbg["llm_dataset_v1"]) > 200:
                    dbg["llm_dataset_v1"] = dbg["llm_dataset_v1"][-200:]
    except Exception:
        return


# --- LLM01: Evidence snippet selection + offsets (auditability; no numeric changes) ---

def _yureeka_question_hash_v1(question: str) -> str:
    try:
        q = (question or "").strip().lower()
        h = hashlib.sha256(q.encode("utf-8", errors="ignore")).hexdigest()
        return h[:16]
    except Exception:
        return ""


# ================================
# LLM04 — Anomaly flags + confidence penalties (proposal-only; rules decide)
# ================================

_LLM04_IRRELEVANCE_TOKENS_V1 = [
    "page ", "pages ", "table ", "figure ", "fig ", "appendix", "copyright",
    "all rights reserved", "isbn", "doi", "cookie", "privacy policy",
    "terms of service", "subscribe", "newsletter", "advertisement", "sponsored",
]

def _llm04__coerce_float_v1(x, default=None):
    try:
        if isinstance(x, bool):
            return default
        if isinstance(x, (int, float)) and not isinstance(x, bool):
            return float(x)
        if isinstance(x, str) and x.strip():
            # tolerate commas
            s = x.strip().replace(",", "")
            return float(s)
    except Exception:
        pass
    return default

def _llm04__extract_years_v1(text: str) -> List[int]:
    try:
        ys = re.findall(r"(19\d{2}|20\d{2})", str(text or ""))
        out = []
        for y in ys:
            try:
                yi = int(y)
                if 1900 <= yi <= 2100:
                    out.append(yi)
            except Exception:
                continue
        # deterministic unique
        return sorted(set(out))
    except Exception:
        return []

def _llm04_rule_anomaly_flags_v1(
    *,
    candidate: dict,
    spec: dict,
    canonical_key: str,
    pool_stats: Optional[dict] = None,
    question_text: str = "",
) -> dict:
    """Return rule-based anomaly flags + a small integer penalty for tie-breaking.
    This never forces acceptance; it only discourages suspicious candidates when enabled.
    """
    out = {"penalty_points": 0, "flags": [], "detail": {}}
    if not isinstance(candidate, dict):
        return out

    try:
        raw = str(candidate.get("raw") or "")
        ctx = str(candidate.get("context_snippet") or candidate.get("context") or candidate.get("context_window") or "")
        blob = (raw + " " + ctx).lower()
        vn = _llm04__coerce_float_v1(candidate.get("value_norm"), default=None)
        unit_family = str(candidate.get("unit_family") or "").strip().lower()
        unit_tag = str(candidate.get("unit_tag") or candidate.get("unit") or "").strip().lower()

        # Required years (from canonical_key)
        req_years = _llm04__extract_years_v1(canonical_key)
        ctx_years = _llm04__extract_years_v1(blob)

        # Year mismatch (mild)
        if req_years:
            if not any(y in ctx_years for y in req_years):
                out["flags"].append("year_anchor_missing")
                out["penalty_points"] += 1
            # If it mentions other years but none of the required ones
            if ctx_years and (set(ctx_years) - set(req_years)) and not (set(ctx_years) & set(req_years)):
                out["flags"].append("year_mismatch")
                out["penalty_points"] += 1

        # Percent out-of-range (strong)
        expects_percent = False
        try:
            ut_spec = str(spec.get("unit_tag") or spec.get("unit") or "").strip().lower()
            expects_percent = (unit_family == "percent") or (ut_spec in ("%", "percent", "percentage")) or ("cagr" in str(canonical_key or "").lower())
        except Exception:
            expects_percent = (unit_family == "percent")

        if expects_percent and vn is not None:
            if vn > 250.0 or vn < -1.0:
                out["flags"].append("percent_out_of_range")
                out["penalty_points"] += 3
            elif vn > 120.0:
                out["flags"].append("percent_high")
                out["penalty_points"] += 1

        # Negative or zero where unlikely (mild)
        if vn is not None and vn < 0:
            out["flags"].append("negative_value")
            out["penalty_points"] += 1

        # Irrelevance tokens (mild)
        if any(t in blob for t in _LLM04_IRRELEVANCE_TOKENS_V1):
            out["flags"].append("context_irrelevant_token")
            out["penalty_points"] += 1

        # Outlier vs pool median (mild)
        try:
            if isinstance(pool_stats, dict) and vn is not None:
                med = _llm04__coerce_float_v1(pool_stats.get("median"), default=None)
                if med is not None and abs(med) > 1e-12 and vn > 0 and med > 0:
                    ratio = vn / med
                    if ratio > 60.0 or ratio < (1.0 / 60.0):
                        out["flags"].append("outlier_vs_pool")
                        out["penalty_points"] += 1
                        out["detail"]["pool_ratio"] = float(ratio)
        except Exception:
            pass

        # Store light details (no prompts)
        out["detail"]["required_years"] = req_years
        out["detail"]["ctx_years"] = ctx_years
        out["detail"]["unit_family"] = unit_family
        out["detail"]["unit_tag"] = unit_tag
        try:
            out["detail"]["value_norm"] = float(vn) if vn is not None else None
        except Exception:
            out["detail"]["value_norm"] = None
        out["detail"]["question_hash"] = _yureeka_hash_text_v1(str(question_text or "")) if question_text else ""

        # Cap penalty to keep tie-break gentle
        try:
            out["penalty_points"] = int(max(0, min(6, int(out.get("penalty_points") or 0))))
        except Exception:
            out["penalty_points"] = 0

        # Deterministic flags order
        try:
            out["flags"] = sorted({str(f) for f in (out.get("flags") or []) if str(f)})
        except Exception:
            pass

        return out
    except Exception:
        return out

def _llm04__validate_relevance_obj_v1(obj: Any) -> Tuple[bool, str]:
    try:
        if not isinstance(obj, dict):
            return (False, "not_dict")
        if "relevant" not in obj:
            return (False, "missing_relevant")
        rel = obj.get("relevant")
        if not isinstance(rel, bool):
            return (False, "relevant_not_bool")
        conf = obj.get("confidence")
        if conf is None:
            return (False, "missing_confidence")
        try:
            cf = float(conf)
        except Exception:
            return (False, "confidence_not_float")
        if not (0.0 <= cf <= 1.0):
            return (False, "confidence_range")
        rat = obj.get("rationale")
        if rat is not None and not isinstance(rat, str):
            return (False, "rationale_not_str")
        return (True, "ok")
    except Exception:
        return (False, "exception")

def _llm04_llm_relevance_check_v1(
    *,
    question_text: str,
    canonical_key: str,
    spec: dict,
    candidate: dict,
) -> Tuple[Optional[dict], dict]:
    """Optional OpenAI relevance probe. Returns (obj, diag). Obj is strict JSON:
    {relevant: bool, confidence: 0-1, rationale: short str}
    """
    diag = {"ok": False, "reason": "", "llm_used": False}

    # LLM05: run-scope circuit breaker + call budget guard (non-behavioral; only affects failing/disabled calls).
    try:
        _is_open, _cb = _yureeka_llm_circuit_is_open_v1()
        if bool(_is_open):
            diag["reason"] = "circuit_open"
            try:
                diag["status"] = _cb.get("status")
            except Exception:
                pass
            try:
                diag["hint"] = str(_cb.get("hint") or "")[:200]
            except Exception:
                pass
            return (None, diag)
    except Exception:
        pass

    try:
        _max_calls = 0
        try:
            _max_calls = int(os.environ.get("YUREEKA_LLM_MAX_CALLS_PER_RUN") or os.environ.get("LLM_MAX_CALLS_PER_RUN") or 0)
        except Exception:
            _max_calls = 0
        if _max_calls and _max_calls > 0:
            _agg = globals().get("_YUREEKA_LLM_RUN_AGG_V1")
            if isinstance(_agg, dict) and int(_agg.get("attempts") or 0) >= int(_max_calls):
                diag["reason"] = "call_budget_exhausted"
                diag["hint"] = "increase YUREEKA_LLM_MAX_CALLS_PER_RUN or reduce enabled LLM flags"
                return (None, diag)
    except Exception:
        pass

    try:
        _af_on, _af_src = _yureeka_llm_flag_effective_v1("ENABLE_LLM_ANOMALY_FLAGS")
        diag["flag_source"] = str(_af_src)[:80]
        if not bool(_af_on):
            diag["reason"] = "disabled"
            return (None, diag)

        # Avoid calling if requests/key missing (diag will explain).
        try:
            model = os.environ.get("YUREEKA_LLM_MODEL") or os.environ.get("OPENAI_MODEL") or "gpt-4o-mini"
        except Exception:
            model = "gpt-4o-mini"

        prompt_version = "llm04_anomaly_relevance_v1"
        schema_version = "schema_frozen_v1"

        # Cache key (replayable)
        try:
            input_obj = {
                "q": str(question_text or "")[:400],
                "canonical_key": str(canonical_key or "")[:160],
                "spec_name": str((spec or {}).get("name") or "")[:120],
                "spec_unit": str((spec or {}).get("unit_tag") or (spec or {}).get("unit") or "")[:16],
                "raw": str((candidate or {}).get("raw") or "")[:80],
                "ctx": str((candidate or {}).get("context_snippet") or "")[:260],
            }
            input_hash = _yureeka_hash_text_v1(_yureeka__stable_json_dumps_v1(input_obj))
            q_hash = _yureeka_hash_text_v1(str(question_text or ""))
            key, key_fallback = get_llm_cache_key_salted_v1(str(model), prompt_version, schema_version, input_hash, "", q_hash)
        except Exception:
            key = ""
            key_fallback = ""

        if key:
            cached = get_cached_llm_response(key)
            if cached is None and key_fallback:
                cached = get_cached_llm_response(key_fallback)
                if cached is not None:
                    diag["cache_fallback_hit"] = True
            if cached is not None:
                diag["llm_used"] = True
                diag["ok"] = True
                diag["reason"] = "cache_hit"
                try:
                    _yureeka_llm_global_agg_update_v1("llm04_anomaly_relevance", {"ok": True, "status": None, "reason": "cache_hit", "model": str(model)}, cache_hit=True)
                except Exception:
                    pass
                if isinstance(cached, dict):
                    return (cached, diag)
                return (None, diag)

        system_prompt = (
            "You are a metric relevance checker. "
            "Given a user question, a canonical metric spec, and an evidence snippet with a number, "
            "decide if the number is actually describing the target metric. "
            "Return ONLY strict JSON: {\"relevant\": true/false, \"confidence\": 0..1, \"rationale\": \"<max 120 chars>\"}."
        )
        user_payload = {
            "question": str(question_text or "")[:420],
            "canonical_key": str(canonical_key or "")[:180],
            "metric_name": str((spec or {}).get("name") or canonical_key)[:140],
            "expected_unit": str((spec or {}).get("unit_tag") or (spec or {}).get("unit") or "")[:18],
            "required_years": _llm04__extract_years_v1(canonical_key),
            "evidence_raw": str((candidate or {}).get("raw") or "")[:120],
            "evidence_snippet": str((candidate or {}).get("context_snippet") or (candidate or {}).get("context") or "")[:320],
        }

        obj, call_diag = _yureeka_call_openai_chat_json_v1(
            model=str(model),
            system_prompt=system_prompt,
            user_payload=user_payload,
            timeout_sec=30,
            max_tokens=180,
        )
        diag.update(call_diag or {})
        diag["llm_used"] = True
        try:
            _yureeka_llm_global_agg_update_v1("llm04_anomaly_relevance", (call_diag or {}), cache_hit=False)
        except Exception:
            pass

        ok, reason = _llm04__validate_relevance_obj_v1(obj)
        if ok:
            # sanitize
            out = {
                "relevant": bool(obj.get("relevant")),
                "confidence": float(obj.get("confidence") or 0.0),
                "rationale": str(obj.get("rationale") or "")[:120],
            }
            diag["ok"] = True
            diag["reason"] = "ok"
            if key:
                try:
                    cache_llm_response(key, out)
                except Exception:
                    pass
            return (out, diag)

        diag["ok"] = False
        diag["reason"] = "invalid:" + str(reason or "")
        return (None, diag)
    except Exception as e:
        diag["ok"] = False
        diag["reason"] = "exception:" + str(type(e).__name__)
        try:
            diag["hint"] = _yureeka_llm_hint_from_diag_v1(diag)
        except Exception:
            pass
        try:
            diag["exception_snip"] = str(e)[:160]
        except Exception:
            pass
        return (None, diag)

def _llm01__coerce_int_v1(x, default=None):
    try:
        if isinstance(x, bool):
            return default
        if isinstance(x, (int, float)) and not isinstance(x, bool):
            return int(x)
        if isinstance(x, str) and x.strip():
            return int(float(x.strip()))
    except Exception:
        pass
    return default

def _llm01_safe_get_source_text_v1(baseline_sources_cache: Any, url: str) -> Tuple[str, str]:
    """Return (text, basis_key) for url from baseline_sources_cache. basis_key indicates the field used."""
    if not url or not isinstance(baseline_sources_cache, list):
        return ("", "")
    try:
        for s in baseline_sources_cache:
            if not isinstance(s, dict):
                continue
            u = s.get("url") or s.get("source_url") or ""
            if str(u) == str(url):
                # Prefer full snapshot_text if present, else excerpt.
                for k in ("snapshot_text", "text", "content", "snapshot_text_excerpt"):
                    v = s.get(k)
                    if isinstance(v, str) and v.strip():
                        return (v, k)
                return ("", "")
    except Exception:
        pass
    return ("", "")

def _llm01_find_line_bounds_v1(text: str, start_idx: int, end_idx: int) -> Tuple[int, int]:
    try:
        start_idx = max(0, int(start_idx))
        end_idx = max(start_idx, int(end_idx))
        ls = text.rfind("\n", 0, start_idx)
        if ls < 0:
            ls = 0
        else:
            ls = ls + 1
        le = text.find("\n", end_idx)
        if le < 0:
            le = len(text)
        return (ls, le)
    except Exception:
        return (0, min(len(text or ""), 0))

def _llm01_find_sentence_bounds_v1(text: str, start_idx: int, end_idx: int, max_span: int = 420) -> Tuple[int, int]:
    """Heuristic sentence bounds around [start_idx,end_idx] within max_span chars each direction."""
    try:
        n = len(text)
        start_idx = max(0, int(start_idx))
        end_idx = max(start_idx, int(end_idx))
        # Backward to boundary
        ws = max(0, start_idx - int(max_span))
        i = start_idx
        while i > ws:
            ch = text[i-1]
            if ch in ".!?\n":
                break
            i -= 1
        ss = i
        # Forward to boundary
        we = min(n, end_idx + int(max_span))
        j = end_idx
        while j < we:
            ch = text[j]
            if ch in ".!?\n":
                j += 1
                break
            j += 1
        se = j
        # Clamp
        ss = max(0, min(ss, n))
        se = max(ss, min(se, n))
        return (ss, se)
    except Exception:
        return (0, len(text or ""))

def _llm01_extract_number_tokens_v1(raw: str) -> List[str]:
    try:
        toks = re.findall(r"\d+(?:\.\d+)?", str(raw or ""))
        # De-dup preserving order
        out = []
        seen = set()
        for t in toks:
            if t in seen:
                continue
            seen.add(t)
            out.append(t)
        return out
    except Exception:
        return []

def _llm01_unit_tokens_v1(metric: dict, best_candidate: dict) -> List[str]:
    toks = []
    try:
        # Metric fields
        for k in ("unit", "unit_tag", "unit_family", "currency_code", "currency", "unit_norm", "unit_raw", "base_unit"):
            v = (metric or {}).get(k)
            if isinstance(v, str) and v.strip():
                toks.append(v.strip())
        # Candidate fields
        for k in ("unit_tag", "unit", "unit_norm", "currency", "currency_code", "base_unit"):
            v = (best_candidate or {}).get(k)
            if isinstance(v, str) and v.strip():
                toks.append(v.strip())
        # Heuristics
        fam = str((metric or {}).get("unit_family") or (best_candidate or {}).get("unit_family") or "").lower()
        if fam in ("pct", "percent", "%"):
            toks.extend(["%", "percent"])
        # Common magnitude words that often anchor evidence snippets
        toks.extend(["million", "billion", "trillion", "usd", "sgd", "eur"])
    except Exception:
        pass
    # Normalize + de-dup
    out = []
    seen = set()
    for t in toks:
        t2 = str(t or "").strip()
        if not t2:
            continue
        t2l = t2.lower()
        if t2l in seen:
            continue
        seen.add(t2l)
        out.append(t2)
    return out

def _llm01_make_snippet_windows_v1(
    text: str,
    start_idx: int,
    end_idx: int,
    *,
    char_windows: Tuple[int, ...] = (140, 240),
) -> List[dict]:
    """Return deterministic candidate snippet windows (line, sentence, char windows)."""
    windows = []
    if not isinstance(text, str) or not text:
        return windows
    try:
        n = len(text)
        s = max(0, int(start_idx))
        e = max(s, int(end_idx))

        # Line window
        ls, le = _llm01_find_line_bounds_v1(text, s, e)
        if le > ls:
            windows.append({
                "kind": "line",
                "window_start": int(ls),
                "window_end": int(le),
                "text": text[ls:le],
            })

        # Sentence window
        ss, se = _llm01_find_sentence_bounds_v1(text, s, e)
        if se > ss:
            windows.append({
                "kind": "sentence",
                "window_start": int(ss),
                "window_end": int(se),
                "text": text[ss:se],
            })

        # Char windows
        for w in char_windows:
            w = int(w)
            ws = max(0, s - w)
            we = min(n, e + w)
            if we > ws:
                windows.append({
                    "kind": f"char_{w}",
                    "window_start": int(ws),
                    "window_end": int(we),
                    "text": text[ws:we],
                })

        # De-dup by (start,end)
        dedup = []
        seen = set()
        for w in windows:
            key = (int(w.get("window_start") or 0), int(w.get("window_end") or 0))
            if key in seen:
                continue
            seen.add(key)
            dedup.append(w)
        return dedup
    except Exception:
        return windows

def _llm01_score_window_v1(window_text: str, num_tokens: List[str], unit_tokens: List[str]) -> float:
    try:
        t = str(window_text or "")
        tl = t.lower()
        score = 0.0
        # Must contain at least one numeric token (soft requirement; but scoring reflects)
        for nt in num_tokens or []:
            if nt and nt in t:
                score += 10.0
        for ut in unit_tokens or []:
            utl = str(ut or "").lower()
            if not utl:
                continue
            if utl in tl:
                score += 2.5
        # Prefer shorter windows
        score -= (len(t) / 600.0)
        # Penalize if window is all whitespace
        if not t.strip():
            score -= 5.0
        return float(score)
    except Exception:
        return 0.0


def _llm01_evidence_policy_snapshot_v1() -> dict:
    """Deterministic acceptance policy for LLM01 evidence-snippet assist (non-sensitive).
    Returns: {confidence_threshold, score_tie_delta, sources:{...}}
    """
    try:
        thr = float(globals().get("LLM01_EVIDENCE_CONFIDENCE_THRESHOLD") or 0.75)
    except Exception:
        thr = 0.75
    try:
        tie = float(globals().get("LLM01_EVIDENCE_SCORE_TIE_DELTA") or 2.5)
    except Exception:
        tie = 2.5

    src_thr = "code"
    src_tie = "code"

    # 0) Streamlit UI/session_state overrides (highest precedence)
    try:
        _st0 = globals().get('st')
        _ss0 = getattr(_st0, 'session_state', None) if _st0 is not None else None
        if (_ss0 is not None) and hasattr(_ss0, 'get'):
            pol = _ss0.get('YUREEKA_LLM_POLICY')
            if (pol is not None) and hasattr(pol, 'get'):
                vv = _yureeka__parse_floatish_v1(pol.get('LLM01_EVIDENCE_CONFIDENCE_THRESHOLD'))
                if vv is not None:
                    thr = float(vv)
                    src_thr = 'ui:YUREEKA_LLM_POLICY.LLM01_EVIDENCE_CONFIDENCE_THRESHOLD'
                vv = _yureeka__parse_floatish_v1(pol.get('LLM01_EVIDENCE_SCORE_TIE_DELTA'))
                if vv is not None:
                    tie = float(vv)
                    src_tie = 'ui:YUREEKA_LLM_POLICY.LLM01_EVIDENCE_SCORE_TIE_DELTA'
    except Exception:
        pass

    # 1) Streamlit secrets overrides
    try:
        _st1 = globals().get('st')
        _secrets = getattr(_st1, 'secrets', None) if _st1 is not None else None
        if _secrets is not None:
            for root in ('YUREEKA_LLM_POLICY', 'YUREEKA_POLICY', 'LLM_POLICY'):
                try:
                    sub = _secrets.get(root)  # type: ignore[attr-defined]
                    if (sub is not None) and hasattr(sub, 'get'):
                        vv = _yureeka__parse_floatish_v1(sub.get('LLM01_EVIDENCE_CONFIDENCE_THRESHOLD'))
                        if vv is not None and src_thr == 'code':
                            thr = float(vv)
                            src_thr = f"secrets:{root}.LLM01_EVIDENCE_CONFIDENCE_THRESHOLD"
                        vv = _yureeka__parse_floatish_v1(sub.get('LLM01_EVIDENCE_SCORE_TIE_DELTA'))
                        if vv is not None and src_tie == 'code':
                            tie = float(vv)
                            src_tie = f"secrets:{root}.LLM01_EVIDENCE_SCORE_TIE_DELTA"
                except Exception:
                    pass
    except Exception:
        pass


    # Environment overrides (best-effort)
    try:
        v = os.environ.get("YUREEKA_LLM01_EVIDENCE_CONFIDENCE_THRESHOLD")
        vv = _yureeka__parse_floatish_v1(v)
        if vv is not None:
            thr = float(vv)
            src_thr = "env:YUREEKA_LLM01_EVIDENCE_CONFIDENCE_THRESHOLD"
    except Exception:
        pass
    try:
        v = os.environ.get("YUREEKA_LLM01_EVIDENCE_SCORE_TIE_DELTA")
        vv = _yureeka__parse_floatish_v1(v)
        if vv is not None:
            tie = float(vv)
            src_tie = "env:YUREEKA_LLM01_EVIDENCE_SCORE_TIE_DELTA"
    except Exception:
        pass

    # Clamp to sensible bounds
    try:
        thr = max(0.0, min(1.0, float(thr)))
    except Exception:
        thr = 0.75
    try:
        tie = max(0.05, min(12.0, float(tie)))
    except Exception:
        tie = 2.5


    # Optional policy toggle: force a rank call even when there is no deterministic tie-set.
    force_call = False
    src_force = "code"
    try:
        force_call = bool(globals().get("LLM01_EVIDENCE_FORCE_CALL") or False)
    except Exception:
        force_call = False
    try:
        _fc, _src = _yureeka_llm_flag_effective_v1("LLM01_EVIDENCE_FORCE_CALL")
        force_call = bool(_fc)
        src_force = str(_src)[:80]
    except Exception:
        pass

    return {
        "confidence_threshold": float(thr),
        "score_tie_delta": float(tie),
        "force_call": bool(force_call),
        "sources": {"confidence_threshold": str(src_thr), "score_tie_delta": str(src_tie), "force_call": str(src_force)},
    }


def _llm01_window_kind_priority_v1(kind: str) -> int:
    """Deterministic tie-break priority for snippet windows (lower is better)."""
    try:
        k = str(kind or "").strip().lower()
    except Exception:
        k = ""
    # Prefer readable excerpts when scores tie.
    order = {
        "sentence": 0,
        "line": 1,
        "char_240": 2,
        "char_140": 3,
    }
    return int(order.get(k, 9))


def _yureeka_parse_json_object_from_text_v1(text: str) -> Optional[dict]:
    """Best-effort JSON object parse from model text (extracts first {...})."""
    if not isinstance(text, str) or not text.strip():
        return None
    try:
        s = text.strip()
        # Fast path: direct
        if s.startswith("{") and s.endswith("}"):
            return json.loads(s)
        # Extract first object
        m = re.search(r"\{.*\}", s, flags=re.DOTALL)
        if m:
            return json.loads(m.group(0))
    except Exception:
        return None
    return None


def _yureeka_llm_is_perplexity_base_v1(base_url: str) -> bool:
    try:
        return "perplexity.ai" in str(base_url or "").lower()
    except Exception:
        return False

def _yureeka_llm_pick_api_key_v1(base_url: str) -> Tuple[str, str]:
    """Pick an API key based on provider base_url. Returns (key, source_label)."""
    try:
        b = str(base_url or "")
    except Exception:
        b = ""
    is_pplx = _yureeka_llm_is_perplexity_base_v1(b)

    candidates: List[Tuple[str, Any]] = []
    if bool(is_pplx):
        candidates = [
            ("PERPLEXITY_API_KEY", os.environ.get("PERPLEXITY_API_KEY") or ""),
            ("YUREEKA_PERPLEXITY_API_KEY", os.environ.get("YUREEKA_PERPLEXITY_API_KEY") or ""),
        ]
        try:
            _k = globals().get("PERPLEXITY_KEY")
            if isinstance(_k, str) and _k.strip():
                candidates.append(("PERPLEXITY_KEY", _k))
        except Exception:
            pass
        candidates.extend([
            ("OPENAI_API_KEY", os.environ.get("OPENAI_API_KEY") or ""),
            ("YUREEKA_OPENAI_API_KEY", os.environ.get("YUREEKA_OPENAI_API_KEY") or ""),
        ])
    else:
        candidates = [
            ("OPENAI_API_KEY", os.environ.get("OPENAI_API_KEY") or ""),
            ("YUREEKA_OPENAI_API_KEY", os.environ.get("YUREEKA_OPENAI_API_KEY") or ""),
            ("YUREEKA_LLM_API_KEY", os.environ.get("YUREEKA_LLM_API_KEY") or ""),
        ]

    for label, val in candidates:
        try:
            if isinstance(val, str) and val.strip():
                return (val.strip(), str(label))
        except Exception:
            continue
    try:
        return ("", str(candidates[0][0]) if candidates else "")
    except Exception:
        return ("", "")

def _yureeka_call_openai_chat_json_v1(
    *,
    model: str,
    system_prompt: str,
    user_payload: dict,
    timeout_sec: int = 30,
    max_tokens: int = 220,
) -> Tuple[Optional[dict], dict]:
    """Call OpenAI Chat Completions for a strict JSON object response (best-effort)."""
    diag = {"ok": False, "status": None, "reason": "", "model": str(model or ""), "requests_present": bool(requests is not None), "api_key_present": False, "api_key_source": "", "base_url": "", "strict_mode": bool(globals().get("LLM_STRICT_MODE")), "response_format": False, "timeout_sec": int(timeout_sec or 30)}

    # LLM05: run-scope circuit breaker + call budget guard (non-behavioral; only affects failing/disabled calls).
    try:
        _is_open, _cb = _yureeka_llm_circuit_is_open_v1()
        if bool(_is_open):
            diag["reason"] = "circuit_open"
            try:
                diag["status"] = _cb.get("status")
            except Exception:
                pass
            try:
                diag["hint"] = str(_cb.get("hint") or "")[:200]
            except Exception:
                pass
            return (None, diag)
    except Exception:
        pass

    try:
        _max_calls = 0
        try:
            _max_calls = int(os.environ.get("YUREEKA_LLM_MAX_CALLS_PER_RUN") or os.environ.get("LLM_MAX_CALLS_PER_RUN") or 0)
        except Exception:
            _max_calls = 0
        if _max_calls and _max_calls > 0:
            _agg = globals().get("_YUREEKA_LLM_RUN_AGG_V1")
            if isinstance(_agg, dict) and int(_agg.get("attempts") or 0) >= int(_max_calls):
                diag["reason"] = "call_budget_exhausted"
                diag["hint"] = "increase YUREEKA_LLM_MAX_CALLS_PER_RUN or reduce enabled LLM flags"
                return (None, diag)
    except Exception:
        pass

    try:
        if requests is None:
            diag["reason"] = "requests_missing"
            try:
                diag["hint"] = _yureeka_llm_hint_from_diag_v1(diag)
            except Exception:
                pass
            return (None, diag)

        base = os.environ.get("OPENAI_BASE_URL") or "https://api.openai.com/v1"
        try:
            diag["base_url"] = str(base or "")[:200]
        except Exception:
            pass

        api_key, api_key_source = _yureeka_llm_pick_api_key_v1(base)
        try:
            diag["api_key_present"] = bool(api_key)
        except Exception:
            pass
        try:
            diag["api_key_source"] = str(api_key_source or "")[:60]
        except Exception:
            pass
        if not api_key:
            diag["reason"] = "missing_api_key"
            try:
                diag["hint"] = _yureeka_llm_hint_from_diag_v1(diag)
            except Exception:
                pass
            try:
                _yureeka_llm_circuit_open_from_diag_v1(diag)
            except Exception:
                pass
            return (None, diag)

        # Allow OPENAI_BASE_URL to be either a base (…/v1) or a full endpoint (…/chat/completions)
        try:
            _b = str(base or "").rstrip("/")
        except Exception:
            _b = str(base or "").rstrip("/")
        if _b.lower().endswith("/chat/completions"):
            url = _b
        else:
            url = _b + "/chat/completions"

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": _yureeka__stable_json_dumps_v1(user_payload)},
        ]

        payload = {
            "model": str(model or ""),
            "messages": messages,
            "temperature": 0,
            "max_tokens": int(max_tokens or 220),
        }
        # Perplexity Sonar (OpenAI-compatible): disable search for determinism unless explicitly enabled.
        try:
            _base_l = str(base or "").lower()
            if "perplexity.ai" in _base_l:
                payload["disable_search"] = True
                try:
                    diag["disable_search"] = True
                except Exception:
                    pass
        except Exception:
            pass
        # Best-effort: request JSON output
        # - OpenAI: legacy json_object mode
        # - Perplexity Sonar: supports JSON Schema structured outputs (json_schema) but not json_object
        try:
            if bool(globals().get("LLM_STRICT_MODE")):
                try:
                    _is_pplx = _yureeka_llm_is_perplexity_base_v1(base)
                except Exception:
                    _is_pplx = False

                if bool(_is_pplx):
                    # Minimal schema: enforce "a JSON object" without constraining fields.
                    # Perplexity requires: response_format.type="json_schema" + json_schema.name + json_schema.schema
                    payload["response_format"] = {
                        "type": "json_schema",
                        "json_schema": {
                            "name": "yureekaSidecar1",
                            "schema": {"type": "object", "additionalProperties": True},
                        },
                    }
                    try:
                        diag["response_format"] = True
                        diag["response_format_type"] = "json_schema"
                    except Exception:
                        pass
                else:
                    payload["response_format"] = {"type": "json_object"}
                    try:
                        diag["response_format"] = True
                        diag["response_format_type"] = "json_object"
                    except Exception:
                        pass
        except Exception:
            pass

        resp = requests.post(url, headers=headers, json=payload, timeout=int(timeout_sec or 30))
        diag["status"] = int(resp.status_code)
        if int(resp.status_code) != 200:
            diag["reason"] = "http_" + str(resp.status_code)
            try:
                diag["body_snip"] = str(getattr(resp, "text", "") or "")[:180]
            except Exception:
                pass
            try:
                diag["hint"] = _yureeka_llm_hint_from_diag_v1(diag)
            except Exception:
                pass
            try:
                _yureeka_llm_circuit_open_from_diag_v1(diag)
            except Exception:
                pass
            return (None, diag)

        j = resp.json()
        content = None
        try:
            content = (((j or {}).get("choices") or [])[0] or {}).get("message", {}).get("content")
        except Exception:
            content = None
        obj = _yureeka_parse_json_object_from_text_v1(content or "")
        if isinstance(obj, dict) and obj:
            diag["ok"] = True
            diag["reason"] = "ok"
            return (obj, diag)

        diag["reason"] = "parse_failed"
        return (None, diag)
    except Exception as e:
        diag["reason"] = "exception:" + str(type(e).__name__)
        try:
            diag["exception_snip"] = str(e)[:180]
        except Exception:
            pass
        return (None, diag)


def _llm01_provider_status_v1() -> dict:
    """Non-sensitive provider readiness snapshot (no secrets)."""
    try:
        model = os.environ.get("YUREEKA_LLM_MODEL") or os.environ.get("OPENAI_MODEL") or "gpt-4o-mini"
    except Exception:
        model = "gpt-4o-mini"
    try:
        base = os.environ.get("OPENAI_BASE_URL") or "https://api.openai.com/v1"
    except Exception:
        base = "https://api.openai.com/v1"
    try:
        api_key, api_key_source = _yureeka_llm_pick_api_key_v1(base)
        api_key_present = bool(api_key)
    except Exception:
        api_key_present = False
        api_key_source = ""
    try:
        cache_dir = str(globals().get("LLM_CACHE_DIR_V1") or ".yureeka_llm_cache")
    except Exception:
        cache_dir = ".yureeka_llm_cache"
    return {
        "requests_present": bool(requests is not None),
        "api_key_present": bool(api_key_present),
        "api_key_source": str(api_key_source or "")[:60],
        "base_url": str(base or "")[:200],
        "model": str(model or "")[:80],
        "strict_mode": bool(globals().get("LLM_STRICT_MODE")),
        "cache_dir": str(cache_dir or "")[:180],
        "evidence_policy": _llm01_evidence_policy_snapshot_v1(),
        "flags": _yureeka_llm_flags_snapshot_v1(),
    }


def _yureeka_llm_hint_from_diag_v1(diag: dict) -> str:
    """Return a short non-sensitive hint for common LLM failure reasons."""
    try:
        if not isinstance(diag, dict):
            return ""
        r = str(diag.get("reason") or "")
        s = diag.get("status")
        body = str(diag.get("body_snip") or "")[:300].lower()

        base_url = str(diag.get("base_url") or "")
        src = str(diag.get("api_key_source") or "")
        if not src:
            try:
                src = "PERPLEXITY_API_KEY" if "perplexity.ai" in base_url.lower() else "OPENAI_API_KEY"
            except Exception:
                src = "OPENAI_API_KEY"

        if "missing_api_key" in r:
            if "perplexity.ai" in base_url.lower():
                return "missing_api_key:set PERPLEXITY_API_KEY (or Streamlit secrets PERPLEXITY_API_KEY)"
            return "missing_api_key:set OPENAI_API_KEY (or YUREEKA_OPENAI_API_KEY)"
        if "requests_missing" in r:
            return "requests_missing:install 'requests' in your environment"
        # HTTP status hints
        try:
            sc = int(s) if s is not None else None
        except Exception:
            sc = None
        if sc == 401 or sc == 403:
            if "perplexity.ai" in base_url.lower():
                return f"auth_error:check {src} (Perplexity)"
            return f"auth_error:check {src} and permissions"
        if sc == 400:
            if "perplexity.ai" in base_url.lower() and ("response_format" in body or "json_schema" in body or "responseformat" in body):
                return "bad_request:Perplexity needs response_format.type='json_schema' (json_object unsupported)"
        if sc == 429:
            if "exceeded your current quota" in body or "billing" in body or "quota" in body:
                return "quota_exceeded:ChatGPT subscription ≠ API quota; check provider billing/credits"
            return "rate_limited:slow down or raise rate limits"
        if sc and sc >= 500:
            return "server_error:temporary upstream issue; retry later"
        if "parse_failed" in r:
            return "parse_failed:model did not return JSON; try strict_mode or different model"
        if r.startswith("exception:"):
            return "exception:see exception_snip/body_snip"
    except Exception:
        pass
    return ""

def _yureeka_llm_reset_run_state_v1(stage: str = "") -> None:
    """Reset per-run LLM diagnostics + circuit breaker state (best-effort)."""
    try:
        globals()["_YUREEKA_LLM_RUN_AGG_V1"] = {
            "stage": str(stage or "")[:40],
            "started_at": float(time.time()),
            "attempts": 0,
            "ok": 0,
            "cache_hits": 0,
            "features": {},
            "reasons": {},
            "status": {},
            "last": {},
        }
    except Exception:
        pass
    try:
        globals()["_YUREEKA_LLM_CIRCUIT_V1"] = {"open_until": 0.0, "status": None, "reason": "", "hint": ""}
    except Exception:
        pass

def _yureeka_llm_global_agg_update_v1(feature: str, call_diag: dict, *, cache_hit: bool = False) -> None:
    """Global (run-scope) call aggregation; does not store prompts/outputs."""
    try:
        agg = globals().get("_YUREEKA_LLM_RUN_AGG_V1")
        if not isinstance(agg, dict):
            return
        try:
            agg["attempts"] = int(agg.get("attempts") or 0) + 1
        except Exception:
            pass
        if bool(cache_hit):
            try:
                agg["cache_hits"] = int(agg.get("cache_hits") or 0) + 1
            except Exception:
                pass
        if isinstance(call_diag, dict) and bool(call_diag.get("ok")):
            try:
                agg["ok"] = int(agg.get("ok") or 0) + 1
            except Exception:
                pass

        f = str(feature or "unknown")[:80]
        try:
            fs = agg.get("features")
            if not isinstance(fs, dict):
                fs = {}
                agg["features"] = fs
            fe = fs.get(f)
            if not isinstance(fe, dict):
                fe = {"attempts": 0, "ok": 0, "cache_hits": 0}
                fs[f] = fe
            fe["attempts"] = int(fe.get("attempts") or 0) + 1
            if bool(cache_hit):
                fe["cache_hits"] = int(fe.get("cache_hits") or 0) + 1
            if isinstance(call_diag, dict) and bool(call_diag.get("ok")):
                fe["ok"] = int(fe.get("ok") or 0) + 1
        except Exception:
            pass

        if not isinstance(call_diag, dict):
            return

        reason = str(call_diag.get("reason") or "")[:120]
        status = call_diag.get("status")
        status_s = ""
        try:
            status_s = str(int(status))
        except Exception:
            status_s = str(status) if status is not None else ""

        try:
            rs = agg.get("reasons")
            if not isinstance(rs, dict):
                rs = {}
                agg["reasons"] = rs
            rs[reason] = int(rs.get(reason) or 0) + 1
        except Exception:
            pass

        try:
            ss = agg.get("status")
            if not isinstance(ss, dict):
                ss = {}
                agg["status"] = ss
            if status_s:
                ss[status_s] = int(ss.get(status_s) or 0) + 1
        except Exception:
            pass

        try:
            agg["last"] = {
                "feature": f,
                "reason": reason,
                "status": status if status is None else int(status),
                "model": str(call_diag.get("model") or "")[:80],
                "hint": str(call_diag.get("hint") or _yureeka_llm_hint_from_diag_v1(call_diag))[:200],
            }
        except Exception:
            pass
    except Exception:
        pass

def _yureeka_llm_circuit_is_open_v1() -> Tuple[bool, dict]:
    try:
        cb = globals().get("_YUREEKA_LLM_CIRCUIT_V1")
        if not isinstance(cb, dict):
            return (False, {})
        until = float(cb.get("open_until") or 0.0)
        if until and until > float(time.time()):
            return (True, dict(cb))
    except Exception:
        pass
    return (False, {})

def _yureeka_llm_circuit_open_from_diag_v1(call_diag: dict) -> None:
    """Open circuit breaker on persistent-failure signals (e.g., quota/auth)."""
    try:
        if not isinstance(call_diag, dict):
            return
        st = call_diag.get("status")
        try:
            sc = int(st) if st is not None else None
        except Exception:
            sc = None
        # Only open circuit on likely-persistent or very noisy failure modes
        if sc not in (401, 403, 429) and not (sc and sc >= 500):
            return
        sec = 180
        try:
            sec = int(os.environ.get("YUREEKA_LLM_CIRCUIT_SECONDS") or os.environ.get("LLM_CIRCUIT_SECONDS") or 180)
        except Exception:
            sec = 180
        cb = globals().get("_YUREEKA_LLM_CIRCUIT_V1")
        if not isinstance(cb, dict):
            cb = {}
            globals()["_YUREEKA_LLM_CIRCUIT_V1"] = cb
        cb["open_until"] = float(time.time()) + float(max(15, sec))
        cb["status"] = sc
        cb["reason"] = str(call_diag.get("reason") or "")[:120]
        cb["hint"] = str(call_diag.get("hint") or _yureeka_llm_hint_from_diag_v1(call_diag))[:200]
    except Exception:
        pass

def _yureeka_llm_health_snapshot_v1(stage: str = "") -> dict:
    """Compact per-run LLM health snapshot for JSON reports (non-sensitive)."""
    out = {"v": "llm_health_v1", "stage": str(stage or "")[:40]}
    try:
        out["provider"] = _llm01_provider_status_v1()
    except Exception:
        pass
    try:
        agg = globals().get("_YUREEKA_LLM_RUN_AGG_V1")
        if isinstance(agg, dict):
            out["agg"] = {
                "attempts": int(agg.get("attempts") or 0),
                "ok": int(agg.get("ok") or 0),
                "cache_hits": int(agg.get("cache_hits") or 0),
                "live_calls": max(0, int(agg.get("attempts") or 0) - int(agg.get("cache_hits") or 0)),
                "status": dict(agg.get("status") or {}),
                "reasons": dict(agg.get("reasons") or {}),
                "last": dict(agg.get("last") or {}),
                "features": dict(agg.get("features") or {}),
            }
    except Exception:
        pass
    try:
        is_open, cb = _yureeka_llm_circuit_is_open_v1()
        out["circuit_open"] = bool(is_open)
        if is_open and isinstance(cb, dict):
            out["circuit"] = {
                "open_until": float(cb.get("open_until") or 0.0),
                "status": cb.get("status"),
                "reason": str(cb.get("reason") or "")[:120],
                "hint": str(cb.get("hint") or "")[:200],
            }
    except Exception:
        pass

    # LLM29/LLM30: include one-shot refresh state (non-sensitive).
    try:
        out["force_refresh_once"] = _yureeka_llm_force_refresh_once_state_v1()
    except Exception:
        pass

    return out

def _yureeka_llm_smoke_test_v1(stage: str = "") -> dict:
    """One-shot provider connectivity check (non-sensitive; does not store prompts/outputs).

    Runs only when ENABLE_LLM_SMOKE_TEST is enabled.
    Intended for diagnosing why LLM sidecar calls are not happening (e.g., missing key, auth, 429).
    """
    out = {"v": "llm_smoke_test_v1", "stage": str(stage or "")[:40], "attempted": False, "ok": False, "reason": "flag_off", "diag": {}}

    _on, _src = _yureeka_llm_flag_effective_v1("ENABLE_LLM_SMOKE_TEST")
    if not bool(_on):
        out["reason"] = "flag_off"
        out["flag_source"] = str(_src)[:80]
        return out

    # Ensure one live attempt per run (reused across stages).
    try:
        cached = globals().get("_YUREEKA_LLM_SMOKE_TEST_RESULT_V1")
        if isinstance(cached, dict) and cached.get("attempted"):
            # reuse result; stamp stage only
            out.update({k: cached.get(k) for k in ("attempted", "ok", "reason", "diag", "model", "base_url", "api_key_source")})
            out["attempted"] = bool(out.get("attempted"))
            out["ok"] = bool(out.get("ok"))
            out["stage"] = str(stage or "")[:40]
            return out
    except Exception:
        pass

    out["attempted"] = True
    out["reason"] = "attempting"
    out["flag_source"] = str(_src)[:80]

    try:
        model = os.environ.get("YUREEKA_LLM_MODEL") or os.environ.get("OPENAI_MODEL") or "gpt-4o-mini"
    except Exception:
        model = "gpt-4o-mini"

    system_prompt = "You are a health-check endpoint. Return ONLY the strict JSON object: {\"pong\":\"ok\"}."
    user_payload = {"ping": 1}

    t0 = time.time()
    obj, diag = _yureeka_call_openai_chat_json_v1(
        model=str(model),
        system_prompt=system_prompt,
        user_payload=user_payload,
        timeout_sec=12,
        max_tokens=60,
    )
    t1 = time.time()
    try:
        if isinstance(diag, dict):
            diag = dict(diag)
            diag["latency_ms"] = int(max(0.0, (t1 - t0) * 1000.0))
    except Exception:
        pass

    ok = bool(isinstance(obj, dict) and str((obj or {}).get("pong") or "").strip().lower() == "ok")
    out["ok"] = bool(ok)
    out["reason"] = "ok" if ok else str((diag or {}).get("reason") or "failed")[:120]
    out["diag"] = diag if isinstance(diag, dict) else {}
    out["model"] = str(model or "")[:80]
    try:
        out["base_url"] = str((diag or {}).get("base_url") or "")[:200]
    except Exception:
        out["base_url"] = ""
    try:
        out["api_key_source"] = str((diag or {}).get("api_key_source") or "")[:60]
    except Exception:
        out["api_key_source"] = ""

    # Count it in global aggregation (best-effort) so llm_sidecar_health reflects the attempt.
    try:
        _yureeka_llm_global_agg_update_v1("llm_smoke_test", (diag or {}), cache_hit=False)
    except Exception:
        pass

    try:
        globals()["_YUREEKA_LLM_SMOKE_TEST_RESULT_V1"] = dict(out)
    except Exception:
        pass

    return out



def _llm01_update_llm_diag_agg_v1(out_debug: dict, call_diag: dict, *, cache_hit: bool = False, feature: str = "llm01") -> None:
    """Aggregate call outcomes without storing prompts or raw model outputs."""
    if not isinstance(out_debug, dict) or not isinstance(call_diag, dict):
        return

    agg = out_debug.get("llm01_llm_diag_agg_v1")
    if not isinstance(agg, dict):
        agg = {"attempts": 0, "ok": 0, "cache_hits": 0, "reasons": {}, "status": {}, "last": {}}
        out_debug["llm01_llm_diag_agg_v1"] = agg

    # LLM15: global run-scope aggregation (non-sensitive) — skip flag_off so health reflects real LLM usage.
    try:
        if str(call_diag.get("reason") or "") != "flag_off":
            _yureeka_llm_global_agg_update_v1(str(feature or "llm01"), call_diag, cache_hit=bool(cache_hit))
    except Exception:
        pass

    try:
        agg["attempts"] = int(agg.get("attempts") or 0) + 1
    except Exception:
        pass

    if bool(cache_hit):
        try:
            agg["cache_hits"] = int(agg.get("cache_hits") or 0) + 1
        except Exception:
            pass

    if bool(call_diag.get("ok")):
        try:
            agg["ok"] = int(agg.get("ok") or 0) + 1
        except Exception:
            pass

    reason = str(call_diag.get("reason") or "")[:120]
    status = call_diag.get("status")
    status_s = ""
    try:
        status_s = str(int(status))
    except Exception:
        status_s = str(status) if status is not None else ""

    # Counts
    try:
        rs = agg.get("reasons")
        if not isinstance(rs, dict):
            rs = {}
            agg["reasons"] = rs
        rs[reason] = int(rs.get(reason) or 0) + 1
    except Exception:
        pass

    try:
        ss = agg.get("status")
        if not isinstance(ss, dict):
            ss = {}
            agg["status"] = ss
        if status_s:
            ss[status_s] = int(ss.get(status_s) or 0) + 1
    except Exception:
        pass

    # Last
    try:
        last = agg.get("last")
        if not isinstance(last, dict):
            last = {}
            agg["last"] = last
        last.update({
            "reason": reason,
            "status": status if status is None else int(status),
            "model": str(call_diag.get("model") or "")[:80],
            "requests_present": bool(call_diag.get("requests_present")) if "requests_present" in call_diag else bool(requests is not None),
            "api_key_present": bool(call_diag.get("api_key_present")) if "api_key_present" in call_diag else bool(os.environ.get("OPENAI_API_KEY") or os.environ.get("YUREEKA_OPENAI_API_KEY")),
            "base_url": str(call_diag.get("base_url") or "")[:200],
            "response_format": bool(call_diag.get("response_format")) if "response_format" in call_diag else None,
        })
        # Optional tiny error hints
        if isinstance(call_diag.get("exception_snip"), str):
            last["exception_snip"] = str(call_diag.get("exception_snip") or "")[:180]
        if isinstance(call_diag.get("body_snip"), str):
            last["body_snip"] = str(call_diag.get("body_snip") or "")[:180]
    except Exception:
        pass

    # Provider status snapshot (non-sensitive)
    try:
        out_debug["llm01_llm_provider_status_v1"] = _llm01_provider_status_v1()
    except Exception:
        pass

def _llm01_llm_rank_windows_v1(
    *,
    windows: List[dict],
    metric_label: str,
    num_tokens: List[str],
    unit_tokens: List[str],
    url: str,
    question: str,
    out_debug: Optional[dict] = None,
) -> Tuple[Optional[int], Optional[float], dict]:
    """Return (best_index, confidence, diag). Uses cache; calls LLM only if needed."""
    diag = {"used": False, "cache_hit": False, "cache_key": "", "reason": "", "model": ""}
    call_diag = None  # populated only on real network calls

    # Resolve model early (for diagnostics)
    try:
        model = os.environ.get("YUREEKA_LLM_MODEL") or os.environ.get("OPENAI_MODEL") or "gpt-4o-mini"
        diag["model"] = str(model)
    except Exception:
        model = "gpt-4o-mini"
        diag["model"] = model

    _flag_on, _flag_src = _yureeka_llm_flag_effective_v1("ENABLE_LLM_EVIDENCE_SNIPPETS")
    if not bool(_flag_on):
        diag["reason"] = "flag_off"
        try:
            diag["flag_source"] = str(_flag_src)[:80]
        except Exception:
            pass
        return (None, None, diag)

    # Prepare minimal prompt payload
    safe_windows = []
    try:
        for w in windows or []:
            if not isinstance(w, dict):
                continue
            t = str(w.get("text") or "")
            # Truncate to reduce cost + avoid accidental large leaks
            if len(t) > 520:
                t = t[:520] + "…"
            safe_windows.append({"kind": str(w.get("kind") or ""), "text": t})
    except Exception:
        safe_windows = []

    prompt_version = "llm01_evidence_rank_v1"
    schema_version = "llm01_rank_schema_v1"
    question_hash = _yureeka_question_hash_v1(question or "")
    input_payload = {
        "task": "pick_best_evidence_snippet",
        "metric": str(metric_label or ""),
        "num_tokens": list(num_tokens or [])[:6],
        "unit_tokens": list(unit_tokens or [])[:10],
        "windows": safe_windows[:8],
        "output_schema": {"best_index": "int", "confidence": "float_0_to_1"},
    }
    try:
        input_hash = hashlib.sha256(_yureeka__stable_json_dumps_v1(input_payload).encode("utf-8", errors="ignore")).hexdigest()
    except Exception:
        input_hash = ""

    try:
        cache_key, cache_key_fallback = get_llm_cache_key_salted_v1(
            str(model), prompt_version, schema_version, str(input_hash), str(url or ""), str(question_hash or "")
        )
    except Exception as e:
        cache_key_fallback = ""
        diag["reason"] = "cache_key_exception:" + str(type(e).__name__)
        try:
            if isinstance(out_debug, dict):
                _llm01_update_llm_diag_agg_v1(out_debug, {"ok": False, "status": None, "reason": diag["reason"], "model": str(model)}, cache_hit=False)
                out_debug.setdefault("llm01_evidence_snippet_call_v1", [])
                if isinstance(out_debug.get("llm01_evidence_snippet_call_v1"), list) and len(out_debug["llm01_evidence_snippet_call_v1"]) < 12:
                    out_debug["llm01_evidence_snippet_call_v1"].append({
                        "url": str(url or "")[:160],
                        "ok": False,
                        "status": None,
                        "reason": diag["reason"],
                        "model": str(model)[:80],
                    })
                out_debug.setdefault("llm01_rank_exceptions_v1", [])
                if isinstance(out_debug.get("llm01_rank_exceptions_v1"), list) and len(out_debug["llm01_rank_exceptions_v1"]) < 12:
                    out_debug["llm01_rank_exceptions_v1"].append({
                        "where": "cache_key",
                        "exc": str(type(e).__name__),
                        "msg": str(e)[:180],
                    })
        except Exception:
            pass
        return (None, None, diag)

    diag["cache_key"] = cache_key
    try:
        if cache_key_fallback:
            diag["cache_key_fallback"] = cache_key_fallback
    except Exception:
        pass

    # Cache lookup (always)
    cached = None
    try:
        cached = get_cached_llm_response(cache_key)
        if cached is None and cache_key_fallback:
            cached = get_cached_llm_response(cache_key_fallback)
            if cached is not None:
                diag["cache_fallback_hit"] = True
    except Exception:
        cached = None

    if isinstance(cached, dict) and cached:
        diag["used"] = True
        diag["cache_hit"] = True
        obj = cached
    else:
        # Call model (best-effort)
        system_prompt = (
            "You select the best evidence snippet window for a metric. "
            "Choose the window that most clearly supports the metric value with number+unit context. "
            "Return ONLY a JSON object with keys: best_index (0-based int), confidence (0..1)."
        )
        obj, call_diag = _yureeka_call_openai_chat_json_v1(
            model=str(model),
            system_prompt=system_prompt,
            user_payload=input_payload,
        )
        diag.update({"used": bool(call_diag.get("ok")), "cache_hit": False, "reason": call_diag.get("reason")})
        if isinstance(obj, dict) and obj:
            try:
                cache_llm_response(cache_key, obj)
            except Exception:
                pass

    if not isinstance(obj, dict) or not obj:
        diag["reason"] = diag.get("reason") or "no_response"
        try:
            if isinstance(out_debug, dict):
                _llm01_update_llm_diag_agg_v1(out_debug, {"ok": False, "status": (call_diag.get("status") if isinstance(call_diag, dict) else None), "reason": str(diag.get("reason") or "no_response"), "model": str(model), "base_url": (call_diag.get("base_url") if isinstance(call_diag, dict) else str(os.environ.get("OPENAI_BASE_URL") or ""))}, cache_hit=bool(diag.get("cache_hit")))
                out_debug.setdefault("llm01_evidence_snippet_call_v1", [])
                if isinstance(out_debug.get("llm01_evidence_snippet_call_v1"), list) and len(out_debug["llm01_evidence_snippet_call_v1"]) < 12:
                    out_debug["llm01_evidence_snippet_call_v1"].append({
                        "url": str(url or "")[:160],
                        "ok": False,
                        "status": (call_diag.get("status") if isinstance(call_diag, dict) else None),
                        "reason": str(diag.get("reason") or "no_response")[:80],
                        "model": str(model)[:80],
                        "cache_hit": bool(diag.get("cache_hit")),
                    })
        except Exception:
            pass
        return (None, None, diag)


    # Validate
    best_index = _llm01__coerce_int_v1(obj.get("best_index"), default=None)
    try:
        conf = obj.get("confidence")
        conf = float(conf) if conf is not None else None
    except Exception:
        conf = None

    if best_index is None or not isinstance(best_index, int):
        diag["reason"] = "invalid_best_index"
        try:
            if isinstance(out_debug, dict):
                _llm01_update_llm_diag_agg_v1(out_debug, {"ok": False, "status": None, "reason": "invalid_best_index", "model": str(model)}, cache_hit=False)
                out_debug.setdefault("llm01_evidence_snippet_call_v1", [])
                if isinstance(out_debug.get("llm01_evidence_snippet_call_v1"), list) and len(out_debug["llm01_evidence_snippet_call_v1"]) < 12:
                    out_debug["llm01_evidence_snippet_call_v1"].append({
                        "url": str(url or "")[:160],
                        "ok": False,
                        "status": None,
                        "reason": "invalid_best_index",
                        "model": str(model)[:80],
                    })
        except Exception:
            pass
        return (None, None, diag)
    if best_index < 0 or best_index >= len(windows or []):
        diag["reason"] = "best_index_oob"
        try:
            if isinstance(out_debug, dict):
                _llm01_update_llm_diag_agg_v1(out_debug, {"ok": False, "status": None, "reason": "best_index_oob", "model": str(model)}, cache_hit=False)
                out_debug.setdefault("llm01_evidence_snippet_call_v1", [])
                if isinstance(out_debug.get("llm01_evidence_snippet_call_v1"), list) and len(out_debug["llm01_evidence_snippet_call_v1"]) < 12:
                    out_debug["llm01_evidence_snippet_call_v1"].append({
                        "url": str(url or "")[:160],
                        "ok": False,
                        "status": None,
                        "reason": "best_index_oob",
                        "model": str(model)[:80],
                    })
        except Exception:
            pass
        return (None, None, diag)
    if conf is None or (conf < 0.0) or (conf > 1.0):
        # clamp to [0,1]
        try:
            conf = max(0.0, min(1.0, float(conf or 0.0)))
        except Exception:
            conf = 0.0

    # Hard validator: chosen window must contain at least one numeric token (or any digit as fallback).
    try:
        chosen_text = str((windows[best_index] or {}).get("text") or "")
        _has_digit = bool(re.search(r"\d", chosen_text))
        _has_token = False
        try:
            if num_tokens:
                _has_token = any((str(nt) in chosen_text) for nt in (num_tokens or []))
        except Exception:
            _has_token = False
        if (not _has_digit) and (not _has_token):
            diag["reason"] = "validator_no_number_token"
    except Exception:
        pass

    if str(diag.get("reason") or "") == "validator_no_number_token":
        try:
            if isinstance(out_debug, dict):
                _llm01_update_llm_diag_agg_v1(out_debug, {"ok": False, "status": (call_diag.get("status") if isinstance(call_diag, dict) else None), "reason": "validator_no_number_token", "model": str(model), "base_url": (call_diag.get("base_url") if isinstance(call_diag, dict) else str(os.environ.get("OPENAI_BASE_URL") or ""))}, cache_hit=bool(diag.get("cache_hit")))
                out_debug.setdefault("llm01_evidence_snippet_call_v1", [])
                if isinstance(out_debug.get("llm01_evidence_snippet_call_v1"), list) and len(out_debug["llm01_evidence_snippet_call_v1"]) < 12:
                    out_debug["llm01_evidence_snippet_call_v1"].append({
                        "url": str(url or "")[:160],
                        "ok": False,
                        "status": (call_diag.get("status") if isinstance(call_diag, dict) else None),
                        "reason": "validator_no_number_token",
                        "model": str(model)[:80],
                        "cache_hit": bool(diag.get("cache_hit")),
                    })
        except Exception:
            pass
        return (None, None, diag)

    diag["reason"] = diag.get("reason") or ("cache_hit" if diag.get("cache_hit") else "ok")
    # Aggregate once per attempt (post-validation).
    try:
        if isinstance(out_debug, dict):
            _agg_diag = {}
            try:
                if isinstance(call_diag, dict):
                    _agg_diag.update(call_diag)
            except Exception:
                pass
            _agg_diag.update({
                "ok": True,
                "status": (_agg_diag.get("status") if "status" in _agg_diag else (call_diag.get("status") if isinstance(call_diag, dict) else None)),
                "reason": str(diag.get("reason") or "ok"),
                "model": str(model or ""),
                "base_url": str((_agg_diag.get("base_url") if isinstance(_agg_diag, dict) else "") or (os.environ.get("OPENAI_BASE_URL") or ""))[:200],
            })
            _llm01_update_llm_diag_agg_v1(out_debug, _agg_diag, cache_hit=bool(diag.get("cache_hit")))
            out_debug.setdefault("llm01_evidence_snippet_call_v1", [])
            if isinstance(out_debug.get("llm01_evidence_snippet_call_v1"), list) and len(out_debug["llm01_evidence_snippet_call_v1"]) < 12:
                out_debug["llm01_evidence_snippet_call_v1"].append({
                    "url": str(url or "")[:160],
                    "ok": True,
                    "status": (call_diag.get("status") if isinstance(call_diag, dict) else None),
                    "reason": str(diag.get("reason") or "ok")[:80],
                    "model": str(model)[:80],
                    "cache_hit": bool(diag.get("cache_hit")),
                })
    except Exception:
        pass

    # LLM20: sync successful evidence-snippet calls into run-scope health aggregation
    # so llm_sidecar_health_v1.agg reflects real feature calls (not only smoke tests).
    try:
        _yureeka_llm_global_agg_update_v1(
            "llm01_evidence_snippets",
            {
                "ok": True,
                "status": (call_diag.get("status") if isinstance(call_diag, dict) else None),
                "reason": str(diag.get("reason") or "ok")[:120],
                "model": str(model or "")[:80],
            },
            cache_hit=bool(diag.get("cache_hit")),
        )
    except Exception:
        pass
    return (best_index, conf, diag)


def _llm01_pmc_numeric_signature_v1(pmc: Any) -> dict:
    """Return a stable signature map for PMC numeric identity (used for safety beacons).

    The signature intentionally ignores additive evidence-snippet fields; it focuses on the
    metric winner/value identity so we can assert 'no numeric deltas' when LLM01 runs.
    Returns: {"sig": <hex>, "per_key": {ckey: <shorthex>}}
    """
    out = {"sig": "", "per_key": {}}
    if not isinstance(pmc, dict) or not pmc:
        return out
    try:
        per = {}
        rows = []
        for ckey in sorted([str(k) for k in pmc.keys()]):
            metric = pmc.get(ckey)
            if not isinstance(metric, dict):
                continue
            rec = {"ckey": str(ckey)}
            # Core numeric identity fields (winner/value should not change).
            for f in ("value", "year", "unit", "value_range", "value_min", "value_max"):
                if f in metric:
                    rec[f] = metric.get(f)
            # Shallow provenance (winner url/raw) if present.
            try:
                if metric.get("source_url") is not None:
                    rec["source_url"] = metric.get("source_url")
            except Exception:
                pass
            prov = metric.get("provenance") if isinstance(metric.get("provenance"), dict) else None
            if isinstance(prov, dict):
                bc = prov.get("best_candidate") if isinstance(prov.get("best_candidate"), dict) else None
                if isinstance(bc, dict):
                    rec["best_url"] = bc.get("source_url") or bc.get("url")
                    if bc.get("raw") is not None:
                        rec["best_raw"] = str(bc.get("raw"))[:120]
            # Evolution wrappers may only have evidence list.
            ev = metric.get("evidence")
            if isinstance(ev, list) and ev and isinstance(ev[0], dict):
                rec["ev0_url"] = ev[0].get("source_url") or ev[0].get("url")
                if ev[0].get("raw") is not None:
                    rec["ev0_raw"] = str(ev[0].get("raw"))[:120]
            rows.append(rec)
            try:
                s = _yureeka__stable_json_dumps_v1(rec)
                per[str(ckey)] = hashlib.sha256((s or "").encode("utf-8", errors="ignore")).hexdigest()[:16]
            except Exception:
                per[str(ckey)] = ""
        out["per_key"] = per
        try:
            s_all = _yureeka__stable_json_dumps_v1(rows)
            out["sig"] = hashlib.sha256((s_all or "").encode("utf-8", errors="ignore")).hexdigest()
        except Exception:
            out["sig"] = ""
    except Exception:
        return {"sig": "", "per_key": {}}
    return out

def _llm01_attach_evidence_snippets_to_pmc_v1(
    *,
    pmc: Any,
    baseline_sources_cache: Any,
    metric_schema: Any = None,
    question: str = "",
    stage: str = "",
    out_debug: Optional[dict] = None,
) -> dict:
    """Mutate pmc entries to add evidence_best_snippet + evidence_offsets (additive only)."""
    try:
        _sig_before_map = _llm01_pmc_numeric_signature_v1(pmc)
    except Exception:
        _sig_before_map = {"sig": "", "per_key": {}}
    if not isinstance(pmc, dict) or not pmc:
        return {"applied": 0, "llm_used": 0, "skipped": 0, "cache_hits": 0, "llm_calls": 0, "llm_accepts": 0, "llm_agrees": 0, "llm_rejects": 0}
    pool_llm01 = baseline_sources_cache if isinstance(baseline_sources_cache, list) else []

    applied = 0
    llm_used = 0  # accepted overrides (back-compat)
    skipped = 0
    cache_hits = 0

    llm_calls = 0
    llm_accepts = 0
    llm_agrees = 0
    llm_rejects = 0

    _llm01_policy = _llm01_evidence_policy_snapshot_v1()
    _llm01_conf_thr = float(_llm01_policy.get("confidence_threshold") or 0.75)
    _llm01_tie_delta = float(_llm01_policy.get("score_tie_delta") or 2.5)

    try:
        schema_keys = list(metric_schema.keys()) if isinstance(metric_schema, dict) else list(pmc.keys())
    except Exception:
        schema_keys = list(pmc.keys())

    for ckey, metric in list(pmc.items()):
        if not isinstance(metric, dict):
            continue

        prov = metric.get("provenance") if isinstance(metric.get("provenance"), dict) else {}
        best = prov.get("best_candidate") if isinstance(prov.get("best_candidate"), dict) else None
        if best is None and isinstance(metric.get("best_candidate"), dict):
            best = metric.get("best_candidate")
        if best is None:
            # Evolution wrapper PMC often carries minimal `evidence` list instead of full provenance.
            try:
                ev = metric.get("evidence")
                if isinstance(ev, list) and ev and isinstance(ev[0], dict):
                    best = ev[0]
            except Exception:
                pass
        if best is None:
            # Last-resort: synthesize a minimal best candidate from shallow fields.
            try:
                best = {}
                u = metric.get("source_url") or ""
                if u:
                    best["source_url"] = u
                r = metric.get("raw") or ""
                if r:
                    best["raw"] = r
                cs = metric.get("context_snippet") or metric.get("context") or ""
                if cs:
                    best["context_snippet"] = cs
                if not best:
                    best = None
            except Exception:
                best = None
        if best is None:
            skipped += 1
            continue
        url = str(best.get("source_url") or best.get("url") or metric.get("source_url") or "")
        start_idx = _llm01__coerce_int_v1(best.get("start_idx"), default=None)
        end_idx = _llm01__coerce_int_v1(best.get("end_idx"), default=None)
        raw = str(best.get("raw") or best.get("value_raw") or metric.get("raw") or "")
        if (not raw) and (metric.get("value") is not None):
            try:
                raw = str(metric.get("value")) + (str(metric.get("unit") or "") if (metric.get("unit") or "") else "")
            except Exception:
                pass

        if not url:
            skipped += 1
            continue

        # Retrieve a stable text basis if possible; otherwise fall back to candidate context snippet.
        text, basis_key = _llm01_safe_get_source_text_v1(pool_llm01, url)
        if not text:
            ctx = best.get("context_snippet")
            if isinstance(ctx, str) and ctx.strip():
                text = ctx.strip()
                basis_key = "context_snippet"

        if not text:
            skipped += 1
            continue

        # If indices are missing (e.g., evolution wrappers), derive within the chosen text basis.
        idx_method = "candidate_indices"
        if start_idx is None or end_idx is None:
            idx_method = "derived_within_" + str(basis_key or "text")
            try:
                if raw and isinstance(raw, str) and raw and (raw in text):
                    start_idx = int(text.find(raw))
                    end_idx = int(start_idx + len(raw))
                else:
                    mnum = re.search(r"\d[\d,\.]*", text)
                    if mnum:
                        start_idx = int(mnum.start())
                        end_idx = int(mnum.end())
                    else:
                        start_idx = 0
                        end_idx = min(len(text), 1)
            except Exception:
                start_idx = 0
                end_idx = min(len(text), 1)

        # Build snippet windows (deterministic)
        windows = _llm01_make_snippet_windows_v1(text, int(start_idx or 0), int(end_idx or 0))
        if not windows:
            skipped += 1
            continue

        num_tokens = _llm01_extract_number_tokens_v1(raw)
        unit_tokens = _llm01_unit_tokens_v1(metric, best)

        # Score deterministically (with explicit tie-break)
        scored: List[Tuple[float, int]] = []
        score_map: Dict[int, float] = {}
        for i, w in enumerate(windows):
            wt = str(w.get("text") or "")
            sc = float(_llm01_score_window_v1(wt, num_tokens, unit_tokens))
            scored.append((sc, i))
            score_map[int(i)] = float(sc)

        def _det_key(tup: Tuple[float, int]) -> Tuple[float, int, int, int]:
            sc, i = tup
            try:
                kind = windows[int(i)].get("kind")
            except Exception:
                kind = ""
            pri = _llm01_window_kind_priority_v1(str(kind))
            try:
                ln = len(str(windows[int(i)].get("text") or ""))
            except Exception:
                ln = 0
            # Sort: higher score first, then preferred kind, then shorter, then stable index.
            return (-float(sc), int(pri), int(ln), int(i))

        scored.sort(key=_det_key)
        best_i = scored[0][1] if scored else 0
        best_score = float(scored[0][0]) if scored else 0.0
        second_score = float(scored[1][0]) if len(scored) > 1 else None

        # Define the deterministic "tie set" as all windows within _llm01_tie_delta of the best score.
        tie_set: List[int] = []
        try:
            for sc, i in scored:
                if (best_score - float(sc)) <= float(_llm01_tie_delta):
                    tie_set.append(int(i))
        except Exception:
            tie_set = [int(best_i)]
# Optional LLM rank (proposal only)
        llm_i = None
        llm_conf = None
        llm_diag = {}

        # LLM18/LLM19: call the sidecar only when it can matter (tie-set) unless force_call is enabled.
        _llm_flag_on = False
        _force_call = False
        _tie_ok = False
        try:
            _llm_flag_on = bool(_yureeka_llm_flag_bool_v1("ENABLE_LLM_EVIDENCE_SNIPPETS"))
        except Exception:
            _llm_flag_on = False
        try:
            _force_call = bool(_yureeka_llm_flag_bool_v1("LLM01_EVIDENCE_FORCE_CALL"))
        except Exception:
            _force_call = False
        try:
            _tie_ok = bool(isinstance(tie_set, list) and len(tie_set) >= 2)
        except Exception:
            _tie_ok = False

        if not _llm_flag_on:
            # Feature flag OFF: do not call the LLM rank function (avoids flag_off attempt noise).
            llm_diag = {"reason": "flag_off"}
        elif (not _tie_ok) and (not _force_call):
            # Skip the LLM call entirely; keep deterministic winner.
            llm_diag = {"reason": "pre_gate_no_tie"}
            try:
                if isinstance(out_debug, dict):
                    out_debug.setdefault("llm01_evidence_pre_gate_v1", [])
                    if isinstance(out_debug.get("llm01_evidence_pre_gate_v1"), list) and len(out_debug["llm01_evidence_pre_gate_v1"]) < 30:
                        out_debug["llm01_evidence_pre_gate_v1"].append({
                            "ckey": str(ckey)[:120],
                            "url": str(url or "")[:160],
                            "tie_set_len": int(len(tie_set) if isinstance(tie_set, list) else 0),
                            "best_i": int(best_i),
                            "best_score": float(best_score),
                            "second_score": float(second_score) if second_score is not None else None,
                            "reason": "pre_gate_no_tie",
                        })
            except Exception:
                pass
        else:
            try:
                llm_i, llm_conf, llm_diag = _llm01_llm_rank_windows_v1(
                    windows=windows,
                    metric_label=str(ckey),
                    num_tokens=list(num_tokens),
                    unit_tokens=list(unit_tokens),
                    url=str(url or ""),
                    question=str(question or ""),
                    out_debug=out_debug,
                )
            except Exception as e:
                llm_i, llm_conf, llm_diag = (None, None, {"reason": "exception:" + str(type(e).__name__)})
                try:
                    if isinstance(out_debug, dict):
                        out_debug.setdefault("llm01_rank_call_exceptions_v1", [])
                        if isinstance(out_debug.get("llm01_rank_call_exceptions_v1"), list) and len(out_debug["llm01_rank_call_exceptions_v1"]) < 12:
                            out_debug["llm01_rank_call_exceptions_v1"].append({
                                "ckey": str(ckey)[:120],
                                "url": str(url or "")[:160],
                                "exc": str(type(e).__name__),
                                "msg": str(e)[:180],
                            })
                        _llm01_update_llm_diag_agg_v1(out_debug, {"ok": False, "status": None, "reason": "exception:" + str(type(e).__name__), "model": str(os.environ.get("YUREEKA_LLM_MODEL") or os.environ.get("OPENAI_MODEL") or "gpt-4o-mini")}, cache_hit=False)
                except Exception:
                    pass
        try:
            if isinstance(llm_diag, dict) and bool(llm_diag.get("cache_hit")):
                cache_hits += 1
        except Exception:
            pass

        chosen_i = best_i
        method = "deterministic"

        llm_called = bool(llm_i is not None and isinstance(llm_i, int))
        llm_accepted = False
        llm_decision_reason = ""

        if llm_called:
            llm_calls += 1

            # Confidence gate
            try:
                conf_v = float(llm_conf) if llm_conf is not None else 0.0
            except Exception:
                conf_v = 0.0

            if conf_v < float(_llm01_conf_thr):
                llm_decision_reason = "conf_below_threshold"
                llm_rejects += 1
            else:
                # Tie-break rule: only accept LLM when there is genuine ambiguity (>=2 windows in tie_set),
                # and the proposed index is inside the tie_set.
                if not isinstance(tie_set, list) or len(tie_set) < 2:
                    llm_decision_reason = "no_tie_set"
                    # If LLM agrees with the deterministic winner, count as 'agree' not 'reject'.
                    if int(llm_i) == int(best_i):
                        llm_agrees += 1
                        llm_decision_reason = "llm_agrees_det"
                    else:
                        llm_rejects += 1
                elif int(llm_i) not in tie_set:
                    llm_decision_reason = "llm_outside_tie_set"
                    llm_rejects += 1
                elif int(llm_i) == int(best_i):
                    llm_decision_reason = "llm_agrees_det"
                    llm_agrees += 1
                else:
                    chosen_i = int(llm_i)
                    method = "llm_ranked"
                    llm_used += 1
                    llm_accepts += 1
                    llm_accepted = True
                    llm_decision_reason = "accepted_tiebreak"

        chosen = windows[chosen_i] if 0 <= chosen_i < len(windows) else windows[best_i]
        snippet = str(chosen.get("text") or "").strip()
        if len(snippet) > 720:
            snippet = snippet[:720] + "…"

        # Top windows (up to 3)
        top3 = []
        try:
            for sc, i in scored[:3]:
                w = windows[i]
                wt = str(w.get("text") or "").strip()
                if len(wt) > 320:
                    wt = wt[:320] + "…"
                top3.append({
                    "kind": str(w.get("kind") or ""),
                    "window_start": int(w.get("window_start") or 0),
                    "window_end": int(w.get("window_end") or 0),
                    "score": float(sc),
                    "text": wt,
                })
        except Exception:
            top3 = []

        metric["evidence_best_snippet"] = snippet
        metric["evidence_offsets"] = {
            "start_idx": int(start_idx),
            "end_idx": int(end_idx),
            "window_start": int(chosen.get("window_start") or 0) if chosen.get("window_start") is not None else None,
            "window_end": int(chosen.get("window_end") or 0) if chosen.get("window_end") is not None else None,
            "text_basis": str(basis_key or ""),
                    "idx_method": str(idx_method or ""),
            "text_len": int(len(text) if isinstance(text, str) else 0),
            "source_url": url,
            "anchor_hash": str(best.get("anchor_hash") or ""),
        }
        if top3:
            metric["evidence_snippets_top"] = top3

        metric["evidence_snippet_method"] = method

        # Non-leaky LLM proposal/decision markers (safe for triad JSONs)
        if llm_called:
            try:
                metric["evidence_snippet_llm_proposed_index"] = int(llm_i) if llm_i is not None else None
            except Exception:
                metric["evidence_snippet_llm_proposed_index"] = None
            try:
                metric["evidence_snippet_llm_proposed_confidence"] = float(llm_conf) if llm_conf is not None else None
            except Exception:
                metric["evidence_snippet_llm_proposed_confidence"] = None
            metric["evidence_snippet_llm_accepted"] = bool(llm_accepted)
            metric["evidence_snippet_llm_decision_reason"] = str(llm_decision_reason or "")[:80]
            metric["evidence_snippet_llm_policy"] = {
                "confidence_threshold": float(_llm01_conf_thr),
                "score_tie_delta": float(_llm01_tie_delta),
            }

        if method == "llm_ranked":
            # Minimal non-leaky markers (no raw prompts/outputs)
            metric["evidence_snippet_llm_used"] = True
            if llm_conf is not None:
                try:
                    metric["evidence_snippet_llm_confidence"] = float(llm_conf)
                except Exception:
                    pass

        # Add minimal provenance marker (safe)
        try:
            if isinstance(prov, dict):
                prov.setdefault("evidence_snippet_v1", {})
                if isinstance(prov.get("evidence_snippet_v1"), dict):
                    prov["evidence_snippet_v1"].update({
                        "method": method,
                        "basis": str(basis_key or ""),
                        "llm_flag": bool(_yureeka_llm_flag_bool_v1("ENABLE_LLM_EVIDENCE_SNIPPETS")),
                        "llm_used": bool(method == "llm_ranked"),
                        "llm_called": bool(llm_called),
                        "llm_accepted": bool(llm_accepted),
                        "llm_decision_reason": str(llm_decision_reason or "")[:80],
                        "llm_policy": {"confidence_threshold": float(_llm01_conf_thr), "score_tie_delta": float(_llm01_tie_delta)},
                        "llm_proposed_index": int(llm_i) if (llm_i is not None and isinstance(llm_i, int)) else None,

                        # Provide a bounded reason string even when falling back to deterministic.
                        "llm_reason": str((llm_diag or {}).get("reason") or "")[:120],
                        "llm_model": str((llm_diag or {}).get("model") or "")[:80],
                        "llm_cache_key": str((llm_diag or {}).get("cache_key") or "") if (method == "llm_ranked") else "",
                        "llm_cache_hit": bool((llm_diag or {}).get("cache_hit")) if (method == "llm_ranked") else False,
                    })
                metric["provenance"] = prov
        except Exception:
            pass

        # Debug-only dataset record (hash only)
        try:
            debug_llm_dataset_v1(
                url=url,
                scraped_text=text,
                snippet_windows=[{"kind": w.get("kind"), "window_start": w.get("window_start"), "window_end": w.get("window_end")} for w in windows[:8]],
                rule_candidates=[{"score": sc, "i": i} for sc, i in scored[:8]],
                chosen_winners={str(ckey): {"method": method, "chosen_i": int(chosen_i)}},
                schema_keys=schema_keys,
                stage=str(stage or ""),
            )
        except Exception:
            pass

        applied += 1

    # Debug summary
    try:
        if out_debug is not None and isinstance(out_debug, dict):
            out_debug.setdefault("llm01_evidence_snippets_v1", {})
            if isinstance(out_debug.get("llm01_evidence_snippets_v1"), dict):
                out_debug["llm01_evidence_snippets_v1"].update({
                    "applied": int(applied),
                    "llm_used": int(llm_used),
                    "skipped": int(skipped),
                    "cache_hits": int(cache_hits),
                    "flag_enable_llm": bool(_yureeka_llm_flag_bool_v1("ENABLE_LLM_EVIDENCE_SNIPPETS")),
                    "llm_calls": int(llm_calls),
                    "llm_accepts": int(llm_accepts),
                    "llm_agrees": int(llm_agrees),
                    "llm_rejects": int(llm_rejects),
                    "llm_confidence_threshold": float(_llm01_conf_thr),
                    "llm_score_tie_delta": float(_llm01_tie_delta),

                })
    except Exception:
        pass


    # LLM30: Safety beacon — assert evidence-snippet assist did NOT change metric numeric identity.
    try:
        _sig_after_map = _llm01_pmc_numeric_signature_v1(pmc)
        ok_sig = bool((_sig_before_map or {}).get("sig")) and ((_sig_before_map.get("sig") == (_sig_after_map or {}).get("sig")))
        diff_n = 0
        try:
            bpk = (_sig_before_map or {}).get("per_key") or {}
            apk = (_sig_after_map or {}).get("per_key") or {}
            if isinstance(bpk, dict) and isinstance(apk, dict):
                keys = set(bpk.keys()).union(set(apk.keys()))
                diff_n = sum(1 for k in keys if str(bpk.get(k) or "") != str(apk.get(k) or ""))
        except Exception:
            diff_n = 0
        if isinstance(out_debug, dict):
            out_debug["llm01_evidence_snippets_safety_v1"] = {
                "v": "llm01_evidence_snippets_safety_v1",
                "stage": str(stage or "")[:40],
                "ok": bool(ok_sig),
                "sig_before": str((_sig_before_map or {}).get("sig") or "")[:16],
                "sig_after": str((_sig_after_map or {}).get("sig") or "")[:16],
                "diff_metric_keys": int(diff_n),
            }
    except Exception:
        pass
    return {
        "applied": int(applied),
        "llm_used": int(llm_used),
        "skipped": int(skipped),
        "cache_hits": int(cache_hits),
        "llm_calls": int(llm_calls),
        "llm_accepts": int(llm_accepts),
        "llm_agrees": int(llm_agrees),
        "llm_rejects": int(llm_rejects),
    }


# --- End LLM01 evidence snippet helpers ---

def _llm01_hotfix_apply_evidence_snippets_final_v1(wrapper: dict, *, stage: str = "", question: str = "") -> dict:
    """LLM01H hotfix: apply evidence snippet fields on FINAL wrapper(s).

    The LLM01 attach hook may run before PMC materialisation in some wrapper paths (e.g., FIX2D75 add_to_history
    rebuild). This helper re-attaches deterministically right before JSON export so keys appear in triad JSONs.

    Returns a compact stats dict; never raises.
    """
    try:
        if not isinstance(wrapper, dict):
            return {}
        stage_s = str(stage or "")
        stage_l = stage_s.strip().lower()

        # Choose debug bucket location (analysis vs evolution)
        dbg = _yureeka_get_debug_bucket_v1(wrapper, default_path=("evolution" if "evol" in stage_l else "analysis"))

        # Collect schema keys (optional)
        schema = None
        try:
            pr = wrapper.get("primary_response")
            if isinstance(pr, dict) and isinstance(pr.get("metric_schema_frozen"), dict):
                schema = pr.get("metric_schema_frozen")
        except Exception:
            schema = None
        try:
            rr = wrapper.get("results")
            if schema is None and isinstance(rr, dict):
                pr2 = rr.get("primary_response")
                if isinstance(pr2, dict) and isinstance(pr2.get("metric_schema_frozen"), dict):
                    schema = pr2.get("metric_schema_frozen")
        except Exception:
            pass

        # Collect baseline sources cache (optional)
        bsc = None
        try:
            bsc = wrapper.get("baseline_sources_cache") or wrapper.get("baseline_sources_cache_current")
        except Exception:
            bsc = None
        try:
            rr = wrapper.get("results")
            if bsc is None and isinstance(rr, dict):
                bsc = rr.get("baseline_sources_cache") or rr.get("baseline_sources_cache_current")
        except Exception:
            pass
        # Collect PMC dict references (wrapper shapes vary across stages)
        # NOTE: In some wrapper shapes (analysis runs), the same canonical metrics may exist as
        # multiple distinct PMC dict objects (different identities). To avoid triple-work (and
        # triple LLM calls when ENABLE_LLM_EVIDENCE_SNIPPETS is ON), we:
        #   1) pick a "master" PMC dict,
        #   2) run attach once on the master,
        #   3) sync only the evidence-snippet fields + provenance to other PMC dicts.
        pmcs = []  # list of (label, pmc_dict)

        try:
            pr = wrapper.get("primary_response")
            if isinstance(pr, dict) and isinstance(pr.get("primary_metrics_canonical"), dict) and pr.get("primary_metrics_canonical"):
                pmcs.append(("primary_response_pmc", pr.get("primary_metrics_canonical")))
        except Exception:
            pass
        try:
            if isinstance(wrapper.get("primary_metrics_canonical"), dict) and wrapper.get("primary_metrics_canonical"):
                pmcs.append(("wrapper_pmc", wrapper.get("primary_metrics_canonical")))
        except Exception:
            pass
        try:
            rr = wrapper.get("results")
            if isinstance(rr, dict) and isinstance(rr.get("primary_metrics_canonical"), dict) and rr.get("primary_metrics_canonical"):
                pmcs.append(("results_pmc", rr.get("primary_metrics_canonical")))
            pr2 = rr.get("primary_response") if isinstance(rr, dict) else None
            if isinstance(pr2, dict) and isinstance(pr2.get("primary_metrics_canonical"), dict) and pr2.get("primary_metrics_canonical"):
                pmcs.append(("results_primary_response_pmc", pr2.get("primary_metrics_canonical")))
        except Exception:
            pass

        # De-dup pmc objects by identity (keep first label)
        seen = set()
        pmc_unique = []
        for lbl, pmc in pmcs:
            if not isinstance(pmc, dict) or not pmc:
                continue
            pid = id(pmc)
            if pid in seen:
                continue
            seen.add(pid)
            pmc_unique.append((str(lbl or ""), pmc))

        # Choose a master PMC dict (prefer wrapper-level PMC when present)
        pmc_master = None
        pmc_master_label = ""
        try:
            for lbl, pmc in pmc_unique:
                if lbl == "wrapper_pmc":
                    pmc_master = pmc
                    pmc_master_label = lbl
                    break
        except Exception:
            pass
        if pmc_master is None and pmc_unique:
            try:
                pmc_master_label, pmc_master = pmc_unique[0]
            except Exception:
                pmc_master = None
                pmc_master_label = ""

        # Aggregate stats (counts are based on a single attach run + optional sync)
        agg = {
            "applied": 0,
            "llm_used": 0,
            "skipped": 0,
            "cache_hits": 0,
            "llm_calls": 0,
            "llm_accepts": 0,
            "llm_agrees": 0,
            "llm_rejects": 0,
            "pmc_targets": int(len(pmc_unique)),
            "pmc_targets_processed": 0,
            "pmc_targets_synced": 0,
            "pmc_master_label": str(pmc_master_label or ""),
            "pmc_dedup_mode": "master_sync_v1",
            "pmc_metric_count_total": 0,
            "pmc_dicts_with_work": 0,
        }
        _pmc_metric_keys_unique = set()

        for lbl, pmc in pmc_unique:
            try:
                agg["pmc_metric_count_total"] += int(len(pmc))
                try:
                    _pmc_metric_keys_unique.update([str(_k) for _k in list(pmc.keys())])
                except Exception:
                    pass
            except Exception:
                pass

        # Attach evidence snippets once to the master PMC dict
        if isinstance(pmc_master, dict) and pmc_master:
            stt = _llm01_attach_evidence_snippets_to_pmc_v1(
                pmc=pmc_master,
                baseline_sources_cache=bsc,
                metric_schema=schema,
                question=str(question or wrapper.get("question") or ""),
                stage=stage_s or "final",
                out_debug=dbg,
            ) or {}
            agg["pmc_targets_processed"] = 1

            try:
                if int(stt.get("applied") or 0) > 0 or int(stt.get("llm_used") or 0) > 0:
                    agg["pmc_dicts_with_work"] = 1
            except Exception:
                pass
            try:
                agg["applied"] += int(stt.get("applied") or 0)
                agg["llm_used"] += int(stt.get("llm_used") or 0)
                agg["skipped"] += int(stt.get("skipped") or 0)
                agg["cache_hits"] += int(stt.get("cache_hits") or 0)
                agg["llm_calls"] += int(stt.get("llm_calls") or 0)
                agg["llm_accepts"] += int(stt.get("llm_accepts") or 0)
                agg["llm_agrees"] += int(stt.get("llm_agrees") or 0)
                agg["llm_rejects"] += int(stt.get("llm_rejects") or 0)
            except Exception:
                pass

        # Sync evidence fields from master -> other PMC dicts (audit-only; winners/values untouched)
        if isinstance(pmc_master, dict) and pmc_master and len(pmc_unique) > 1:
            _sync_fields = ["evidence_best_snippet", "evidence_offsets", "evidence_snippets_top", "evidence_snippet_method"]
            for lbl, pmc in pmc_unique:
                if pmc is pmc_master:
                    continue
                try:
                    agg["pmc_targets_synced"] += 1
                except Exception:
                    pass
                try:
                    for _k, _m in pmc_master.items():
                        if _k not in pmc:
                            continue
                        _t = pmc.get(_k)
                        if not isinstance(_t, dict):
                            continue
                        if isinstance(_m, dict):
                            for _f in _sync_fields:
                                if _f in _m:
                                    _t[_f] = _m.get(_f)
                            # provenance sync (optional)
                            _mp = _m.get("provenance")
                            if isinstance(_mp, dict) and ("evidence_snippet_v1" in _mp):
                                _tp = _t.get("provenance")
                                if not isinstance(_tp, dict):
                                    _tp = {}
                                    _t["provenance"] = _tp
                                try:
                                    _tp["evidence_snippet_v1"] = _mp.get("evidence_snippet_v1")
                                except Exception:
                                    pass
                except Exception:
                    pass


        # Durable summary marker (grep-friendly)
        try:
            if isinstance(dbg, dict):
                dbg.setdefault("llm01_evidence_snippets_summary", {})
                if isinstance(dbg.get("llm01_evidence_snippets_summary"), dict):
                    dbg["llm01_evidence_snippets_summary"].update({
                        "stage": stage_s,
                        "pmc_targets": int(agg.get("pmc_targets") or 0),
                        "pmc_targets_processed": int(agg.get("pmc_targets_processed") or 0),
                        "pmc_targets_synced": int(agg.get("pmc_targets_synced") or 0),
                        "pmc_master_label": str(agg.get("pmc_master_label") or ""),
                        "pmc_dedup_mode": str(agg.get("pmc_dedup_mode") or ""),
                        "applied": int(agg.get("applied") or 0),
                        "llm_used": int(agg.get("llm_used") or 0),
                        "llm_calls": int(agg.get("llm_calls") or 0),
                        "llm_accepts": int(agg.get("llm_accepts") or 0),
                        "llm_agrees": int(agg.get("llm_agrees") or 0),
                        "llm_rejects": int(agg.get("llm_rejects") or 0),
                        "llm_policy": _llm01_evidence_policy_snapshot_v1(),

                        "skipped": int(agg.get("skipped") or 0),
                        "cache_hits": int(agg.get("cache_hits") or 0),
                        "pmc_metric_count_total": int(agg.get("pmc_metric_count_total") or 0),
                        "pmc_metric_keys_unique": int(len(_pmc_metric_keys_unique) if isinstance(_pmc_metric_keys_unique, set) else 0),
                        "pmc_dicts_with_work": int(agg.get("pmc_dicts_with_work") or 0),
                        "flag_enable_llm": bool(_yureeka_llm_flag_bool_v1("ENABLE_LLM_EVIDENCE_SNIPPETS")),
                    })
        except Exception:
            pass

        # Provider readiness snapshot (non-sensitive; helps diagnose why llm_used==0)
        try:
            if isinstance(dbg, dict):
                dbg["llm01_llm_provider_status_v1"] = _llm01_provider_status_v1()
        except Exception:
            pass

        return agg
    except Exception:
        return {}


# === END LLM SIDECAR (LLM01) ==================================================




# REFACTOR129: run-level beacons (reset per evolution run)
_REFACTOR129_DECIMAL_OVERLAP_SUPPRESSION_V1 = {"total_numbers": 0, "overlaps_detected": 0, "overlaps_dropped": 0, "sample_drops": []}
_REFACTOR129_PRECISION_TIEBREAK_V1 = {}

# REFACTOR131: injected semantic override beacon (selection rescue for injection runs)
_REFACTOR131_INJECTED_SEMANTIC_OVERRIDE_V1 = {"overrides": 0, "samples": [], "strict_injected_year_tokens_filtered": 0}
# REFACTOR132: injected CAGR rescue beacon (when binding fallback is applied in injection runs)
_REFACTOR132_INJECTED_CAGR_RESCUE_V1 = {"applied": 0, "samples": []}

# REFACTOR111 escape hatch + selector gate
DISABLE_FASTPATH_FOR_NOW = True
FORCE_LATEST_PREV_SNAPSHOT_V1 = True

# REFACTOR176: shared tiny helpers (module-scope) to eliminate duplicated nested defs
def _yureeka_now_iso_v1() -> str:
    """UTC ISO timestamp (stable helper)."""
    try:
        return datetime.now(timezone.utc).isoformat()
    except Exception:
        return ""

def _yureeka_sha1_v1(s: str) -> str:
    """SHA1 hex digest for stable anchor hashing."""
    try:
        return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()
    except Exception:
        return ""

# - Downsizing step 1: remove accumulated per-patch try/append scaffolding.
# - Registers a canonical entries list idempotently at import time.

_PATCH_TRACKER_B64_ZLIB_V1 = "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"

def _yureeka__decode_patch_tracker_v1(blob: str) -> list:
    """Decode the compressed patch tracker blob into a list[dict] (Streamlit-safe, no IO).
    Fallback: returns [] on any decode/parse error.
    """
    try:
        import zlib as _zlib
        import base64 as _base64
        raw = _zlib.decompress(_base64.b64decode((blob or "").encode("ascii"))).decode("utf-8")
        out = json.loads(raw)
        return out if isinstance(out, list) else []
    except Exception:
        return []

_PATCH_TRACKER_CANONICAL_ENTRIES_V1 = _yureeka__decode_patch_tracker_v1(_PATCH_TRACKER_B64_ZLIB_V1)

# [MOD:PATCH_TRACKER]
# Patch tracker registry (latest-first).
# Streamlit reruns start from a clean module state, so we can bind directly.
try:
    PATCH_TRACKER_V1 = list(_PATCH_TRACKER_CANONICAL_ENTRIES_V1 or [])
except Exception:
    PATCH_TRACKER_V1 = []



# REFACTOR194: patch tracker overlay (avoid touching the compressed canonical blob)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "REFACTOR194" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "REFACTOR194", "scope": "downsizing", "summary": "Prune redundant local stdlib imports (os/json/re) + drop import aliases; no behavior changes.", "risk": "low"})
except Exception:
    pass

# REFACTOR195: patch tracker overlay (avoid touching the compressed canonical blob)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "REFACTOR195" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "REFACTOR195", "scope": "sheets-robustness", "summary": "Harden Google Sheets snapshot loading against 429 rate limits (cooldown + retry + reduced header reads) and improve graceful fallback to local history.", "risk": "low"})
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "REFACTOR196" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "REFACTOR196", "scope": "sheets-robustness", "summary": "Restore missing generate_analysis_id() used by add_to_history/HistoryFull writes; fix NameError during Google Sheets save.", "risk": "low"})
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "REFACTOR197" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "REFACTOR197", "scope": "downsizing", "summary": "Prune unused Sheets rate-limit debug globals + remove redundant local streamlit/Credentials imports in Sheets helpers; no behavior changes.", "risk": "low"})
except Exception:
    pass




# LLM01F: patch tracker overlay (hotfix: avoid legacy helper name collisions overriding sidecar cache utilities).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM01F" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM01F", "scope": "llm-sidecar", "summary": "Hotfix: prevent legacy LLM cache helper name collisions (get_llm_cache_key/cache_llm_response) from overriding sidecar cache utilities; LLM assist calls now proceed and cache works. Metric winners/values unchanged.", "risk": "low"})
except Exception:
    pass






# LLM04: patch tracker overlay (avoid touching the compressed canonical blob)
try:

    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM04H" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM04H", "scope": "llm-sidecar", "summary": "Hotfix: make all LLM sidecar feature-flag gates use the env-overridable resolver (not globals().get), so ENABLE_LLM_QUERY_FRAME / ENABLE_LLM_ANOMALY_FLAGS / ENABLE_LLM_DATASET_LOGGING behave consistently. Add flag_source to LLM03/LLM04 debug beacons. Also fix harness version mismatch banner to compare against patch-tracker head (supports LLM series) while still recording latest REFACTOR id.", "risk": "low"})
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM04" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM04", "scope": "llm-sidecar", "summary": "Add anomaly flags + confidence-penalty sidecar (rule-based outlier/irrelevance checks + optional OpenAI relevance probe) behind ENABLE_LLM_ANOMALY_FLAGS (default OFF).", "risk": "medium"})
except Exception:
    pass

# LLM03: patch tracker overlay (avoid touching the compressed canonical blob)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM03" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM03", "scope": "llm-sidecar", "summary": "Add strict query framing / intent decomposition sidecar (deterministic frame + optional OpenAI JSON proposal + validator); enables safe web-query boosting behind ENABLE_LLM_QUERY_FRAME (default OFF).", "risk": "medium"})
except Exception:
    pass

# LLM02: patch tracker overlay (source clustering behind flag + better LLM flag diagnostics).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM02" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM02", "scope": "llm-sidecar", "summary": "Add env-overridable effective flag resolver + non-sensitive flag snapshot diagnostics; LLM01 evidence-snippet ranking now records flag_source. Introduce NLP-first source clustering/reordering behind ENABLE_LLM_SOURCE_CLUSTERING (OFF by default) to reduce duplicate scraping. No behavior changes unless flags enabled.", "risk": "low"})
except Exception:
    pass

# REFACTOR198: patch tracker overlay (avoid touching the compressed canonical blob)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "REFACTOR198" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "REFACTOR198", "scope": "downsizing", "summary": "Remove redundant local imports (hashlib/json/re/time/os aliases) and unify time usage via module-level import; no behavior changes.", "risk": "low"})
except Exception:
    pass


# REFACTOR199: patch tracker overlay (avoid touching the compressed canonical blob)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "REFACTOR199" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "REFACTOR199", "scope": "endstate", "summary": "Add debug.endstate_check_v1 (analysis + evolution) for machine-readable invariants; additive only.", "risk": "low"})
except Exception:
    pass

# REFACTOR200: patch tracker entry (downsizing)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "REFACTOR200" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "REFACTOR200", "scope": "downsizing", "summary": "Prune redundant Sheets timestamp helper (dedupe _yureeka_sheets_now_ts_v1 vs _sheets_now_ts) + remove unused key-overlap debug emitter; no pipeline behavior change.", "risk": "low"})
except Exception:
    pass





# REFACTOR201: patch tracker overlay (fastpath validation gate + injection safety)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "REFACTOR201" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "REFACTOR201", "scope": "endstate", "summary": "Add fastpath enable override (opt-in), force-disable fastpath in injection mode, and surface fastpath/injection flags in endstate_check_v1; default behavior unchanged.", "risk": "low"})
except Exception:
    pass



# REFACTOR202: patch tracker overlay (downsizing)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "REFACTOR202" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "REFACTOR202", "scope": "downsizing", "summary": "Prune redundant local imports (datetime/_dt, requests, gspread, BeautifulSoup, Counter, traceback) by using module-scope imports; no behavior changes.", "risk": "low"})
except Exception:
    pass


# REFACTOR203: endstate_check_v1 correctness + wrapper-shape awareness (diagnostic-only)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "REFACTOR203" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "REFACTOR203", "scope": "diagnostics", "summary": "Make debug.endstate_check_v1 wrapper-shape aware and stage-aware; attach only at final wrapper points (analysis/evolution) to avoid misleading pmc_count=0 reports; no changes to diff/selection logic.", "risk": "low"})
except Exception:
    pass

# REFACTOR204: endstate_check_v1 evolution final-wrapper attach + debug-bucket preference (diagnostic-only)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "REFACTOR204" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "REFACTOR204", "scope": "diagnostics", "summary": "Attach debug.endstate_check_v1 only on the FINAL evolution output wrapper (after Δt stamping + timestamps), prefer results.debug over results.results.debug, and remove inner endstate_check emission from run_source_anchored_evolution; also surface fastpath_gate_v2 into results.debug for audit. Diagnostic-only; diff/selection unchanged.", "risk": "low"})
except Exception:
    pass

# REFACTOR205: endstate_check_v1 evolution placement hardening + false-negative avoidance (diagnostic-only)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "REFACTOR205" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "REFACTOR205", "scope": "diagnostics", "summary": "Harden debug-bucket selection so endstate_check_v1 lands in results.debug even when wrapper depth varies; avoid false negatives when evolution wrapper lacks timestamps/Δt rows; diagnostic-only (diff/selection unchanged).", "risk": "low"})
except Exception:
    pass


# REFACTOR206: Release Candidate label + freeze (no pipeline behavior change).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "REFACTOR206" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "REFACTOR206", "scope": "release", "summary": "Release Candidate freeze: add build_meta (release_tag/freeze_mode) + small optional regression question set; also harden injected_url hint passed into endstate_check_v1 (diagnostic-only). No changes to selection/diff engines.", "risk": "low"})
except Exception:
    pass



# LLM01: patch tracker overlay (evidence snippet selection + optional LLM ranking; additive only).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM01" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM01", "scope": "llm-sidecar", "summary": "Add evidence snippet selection + offsets for each chosen metric (auditability only), plus optional LLM-assisted ranking of snippet windows behind feature flag + deterministic cache. Winners/values unchanged.", "risk": "low"})
except Exception:
    pass

# LLM00: patch tracker overlay (LLM sidecar scaffolding; flags default OFF).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM00" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM00", "scope": "llm-sidecar", "summary": "Add LLM sidecar scaffolding (feature flags default OFF), deterministic cache key + optional disk cache helpers, and a debug-only dataset logger. No selection/diff behavior changes.", "risk": "low"})
except Exception:
    pass


# LLM01D: diagnostics overlay (explain why LLM calls are not used; additive only).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM01D" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM01D", "scope": "llm-sidecar", "summary": "Diagnostics: add provider readiness + aggregated call failure reasons for LLM01 evidence-snippet ranking; also support evolution wrapper metrics lacking provenance by deriving indices from context_snippet. Additive only.", "risk": "low"})
except Exception:
    pass

# LLM01H: hotfix patch tracker overlay (ensure evidence snippet fields land in final JSON wrappers; additive only).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM01H" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM01H", "scope": "llm-sidecar", "summary": "Hotfix: ensure LLM01 evidence snippet fields + debug summary are attached on final analysis/evolution wrappers (post materialisation) so they appear in exported JSON; add cache-hit counters. Winners/values unchanged.", "risk": "low"})
except Exception:
    pass



# LLM10: patch tracker head normalization + ensure evolution outputs always carry evidence snippet fields.
# Additive only; no winner/value changes.

def _yureeka_patch_tracker_ensure_head_v1(patch_id: str, entry: dict) -> None:
    """Ensure PATCH_TRACKER_V1 has `patch_id` at index 0 (move if already present).

    LLM13: Be conservative when merging metadata: do not overwrite non-empty existing fields
    (e.g., keep a human-written summary) when later "head normalization" runs.
    """
    try:
        pt = globals().get('PATCH_TRACKER_V1')
        if not isinstance(pt, list):
            return
        pid = str(patch_id or '')
        if not pid:
            return
        found_i = None
        for i, e in enumerate(list(pt)):
            if isinstance(e, dict) and str(e.get('patch_id') or '') == pid:
                found_i = i
                break
        if found_i is not None:
            existing = pt.pop(found_i)
            if isinstance(existing, dict):
                try:
                    ent = dict(entry or {})
                    ent.pop('patch_id', None)
                    for k, v in ent.items():
                        # Only fill blanks / missing keys; don't stomp existing content
                        if k not in existing or existing.get(k) in (None, '', [], {}, ()):
                            existing[k] = v
                except Exception:
                    pass
                pt.insert(0, existing)
            else:
                pt.insert(0, dict(entry or {'patch_id': pid}))
            return
        pt.insert(0, dict(entry or {'patch_id': pid}))
    except Exception:
        return


try:
    _yureeka_patch_tracker_ensure_head_v1('LLM14', {
        'patch_id': 'LLM14',
        'scope': 'llm-sidecar',
        'summary': 'LLM14: Patch hygiene fix (code_version + patch tracker head alignment) and safer defaults (LLM_BYPASS_CACHE default OFF; assist flags remain OFF by default). No selection/diff behavior changes.',
        'risk': 'low'
    })
except Exception:
    pass

try:
    _yureeka_patch_tracker_ensure_head_v1('LLM15', {
        'patch_id': 'LLM15',
        'scope': 'llm-sidecar',
        'summary': 'LLM15: Fix patch tracker reorder bug that forced head to LLM00; extend reorder list through LLM15. Also tighten LLM health aggregation so flag_off does not count as an attempt (global agg reflects real use). No selection/diff behavior changes.',
        'risk': 'low'
    })
except Exception:
    pass

try:
    _yureeka_patch_tracker_ensure_head_v1('LLM16', {
        'patch_id': 'LLM16',
        'scope': 'llm-sidecar',
        'summary': 'LLM16: Add LLM01 evidence snippet assist instrumentation to both analysis + evolution runs, plus run-delta display and health snapshots. Flags remain OFF by default. No selection/diff behavior changes.',
        'risk': 'low'
    })
except Exception:
    pass

try:
    _yureeka_patch_tracker_ensure_head_v1('LLM17', {
        'patch_id': 'LLM17',
        'scope': 'llm-sidecar',
        'summary': 'LLM17: Add Streamlit UI toggles for LLM sidecar flags (session_state override) so evidence-snippet assist can be enabled without code edits. Fix release_tag mismatch. Defaults remain OFF (REFACTOR206 behavior preserved).',
        'risk': 'low'
    })
except Exception:
    pass

def _yureeka_get_code_version(_lock=_YUREEKA_CODE_VERSION_LOCK):
    try:
        return str(_lock)
    except Exception:
        return "UNKNOWN"


# LLM10: ensure patch tracker head matches current code version lock
try:
    _yureeka_patch_tracker_ensure_head_v1(_yureeka_get_code_version(), {"patch_id": _yureeka_get_code_version(), "scope": "llm-sidecar", "summary": "(auto) head normalized to code version", "risk": "low"})
except Exception:
    pass

def _yureeka_is_injection_mode_v1(web_context: dict) -> bool:
    """Heuristic: determine whether this run should be treated as an injection run.

    Injection runs must not use fastpath reuse, because injected URLs are expected to override winners
    even when question/source hashes would otherwise match.
    """
    try:
        wc = web_context if isinstance(web_context, dict) else {}
        # explicit markers (future-proof)
        if str(wc.get("mode") or "").strip().lower() in ("inject", "injection", "injected"):
            return True
        inj = str(wc.get("injected_url") or "").strip()
        if inj:
            return True

        extra = wc.get("extra_urls")
        urls = []
        if isinstance(extra, (list, tuple)):
            for u in extra:
                u = str(u or "").strip()
                if u:
                    urls.append(u)
        elif isinstance(extra, str) and extra.strip():
            for line in extra.splitlines():
                line = (line or "").strip()
                if not line:
                    continue
                for p in line.split(","):
                    p = (p or "").strip()
                    if p:
                        urls.append(p)

        for u in urls:
            ul = u.lower()
            if "injected-content" in ul:
                return True
            if "veeyuen.github.io" in ul and "injected" in ul:
                return True
        return False
    except Exception:
        return False



def _yureeka_get_pmc_v1(wrapper) -> dict:
    """Wrapper-shape aware getter for primary_metrics_canonical.
    Tries multiple known wrapper shapes (analysis/evolution) and returns {} if missing.
    """
    if not isinstance(wrapper, dict):
        return {}
    try:
        candidates = []
        # Direct
        candidates.append(wrapper.get("primary_metrics_canonical"))
        # Common nested shapes
        rr = wrapper.get("results")
        if isinstance(rr, dict):
            candidates.append(rr.get("primary_metrics_canonical"))
            rr2 = rr.get("results")
            if isinstance(rr2, dict):
                candidates.append(rr2.get("primary_metrics_canonical"))
        # Analysis wrapper often keeps PMC under primary_response
        pr = wrapper.get("primary_response")
        if isinstance(pr, dict):
            candidates.append(pr.get("primary_metrics_canonical"))
        # (Optional) some wrappers use primary_response nested under results
        if isinstance(rr, dict) and isinstance(rr.get("primary_response"), dict):
            candidates.append((rr.get("primary_response") or {}).get("primary_metrics_canonical"))
        for c in candidates:
            if isinstance(c, dict) and c:
                return c
        # If we found an empty dict at least, return it (preserves count==0 correctly).
        for c in candidates:
            if isinstance(c, dict):
                return c
    except Exception:
        return {}
    return {}


def _yureeka_get_metric_changes_v1(wrapper) -> list:
    """Wrapper-shape aware getter for metric_changes rows list."""
    if not isinstance(wrapper, dict):
        return []
    try:
        candidates = []
        candidates.append(wrapper.get("metric_changes"))
        rr = wrapper.get("results")
        if isinstance(rr, dict):
            candidates.append(rr.get("metric_changes"))
            rr2 = rr.get("results")
            if isinstance(rr2, dict):
                candidates.append(rr2.get("metric_changes"))
        for c in candidates:
            if isinstance(c, list):
                return c
        # tolerate tuple
        for c in candidates:
            if isinstance(c, tuple):
                return list(c)
    except Exception:
        return []
    return []


def _yureeka_get_debug_bucket_v1(wrapper: dict, default_path: str = "analysis") -> dict:
    """Ensure and return a durable debug dict for the given wrapper.

    default_path:
      - "analysis": attach to wrapper["debug"]
      - "evolution": attach to wrapper["results"]["debug"] (or wrapper["debug"] if wrapper already *is* the results dict)

    REFACTOR205: harden against wrapper-depth mismatches so diagnostics do not land at results.results.debug.
    """
    if not isinstance(wrapper, dict):
        return {}
    mode = str(default_path or "").strip().lower()

    try:
        if mode.startswith("evol"):
            # If the caller already passed the evolution "results" dict, attach directly.
            # This prevents accidental nesting at results.results.debug.
            if ("metric_changes" in wrapper) or ("stability_score" in wrapper):
                if not isinstance(wrapper.get("debug"), dict):
                    wrapper["debug"] = {}
                return wrapper["debug"]

            rr = wrapper.get("results")
            if isinstance(rr, dict):
                # Force debug to be a dict (setdefault does not fix an existing non-dict).
                if not isinstance(rr.get("debug"), dict):
                    rr["debug"] = {}
                return rr["debug"]

            # Last resort: top-level debug.
            if not isinstance(wrapper.get("debug"), dict):
                wrapper["debug"] = {}
            return wrapper["debug"]

        # Analysis default
        if not isinstance(wrapper.get("debug"), dict):
            wrapper["debug"] = {}
        return wrapper["debug"]
    except Exception:
        return {}
    return {}




def _fresh02_summarize_tiebreaks_from_pmc_v1(pmc: dict) -> dict:
    """Summarize per-metric fresh_tiebreak_v1 beacons into a run-scope block.

    NLP07: Some wrapper shapes / persistence paths can drop the earlier run-scope beacon attachment.
    This helper derives a stable summary directly from the authoritative PMC (additive; no behavior changes).
    """
    if not isinstance(pmc, dict):
        return {}
    total = 0
    used = 0
    changed = 0
    flag_any = False
    changed_keys = []
    examples = []
    try:
        for ckey, m in pmc.items():
            if not isinstance(ckey, str) or not ckey:
                continue
            if not isinstance(m, dict):
                continue
            prov = m.get("provenance")
            if not isinstance(prov, dict):
                continue
            ft = prov.get("fresh_tiebreak_v1")
            if not isinstance(ft, dict):
                continue
            total += 1
            try:
                if bool(ft.get("flag_enabled")):
                    flag_any = True
            except Exception:
                pass
            if ft.get("used") is True:
                used += 1
            if ft.get("changed_winner") is True:
                changed += 1
                changed_keys.append(ckey)
                if len(examples) < 6:
                    examples.append({
                        "canonical_key": ckey,
                        "winner_url": str(ft.get("winner_url") or ""),
                        "base_url": str(ft.get("base_url") or ""),
                        "a_score": ft.get("a_score"),
                        "b_score": ft.get("b_score"),
                        # Both legacy + current field names are tolerated
                        "a_age_days": ft.get("a_age_days") if ft.get("a_age_days") is not None else ft.get("a_freshness_age_days"),
                        "b_age_days": ft.get("b_age_days") if ft.get("b_age_days") is not None else ft.get("b_freshness_age_days"),
                        "a_freshness": ft.get("a_freshness"),
                        "b_freshness": ft.get("b_freshness"),
                        "reason": str(ft.get("reason") or ""),
                    })
    except Exception:
        return {}

    return {
        "metrics_with_beacon_count": int(total),
        "used_count": int(used),
        "changed_winner_count": int(changed),
        "changed_winner_keys": list(changed_keys[:24]),
        "flag_enabled_any": bool(flag_any),
        "examples": list(examples),
        "derived_from": "primary_metrics_canonical.provenance.fresh_tiebreak_v1",
    }


def _fresh02_attach_run_tiebreak_summary_v1(wrapper: dict, default_path: str = "analysis") -> None:
    """Attach run-scope FRESH02 summary into the durable debug bucket (analysis/evolution)."""
    try:
        if not isinstance(wrapper, dict):
            return
        pmc = _yureeka_get_pmc_v1(wrapper)
        if not isinstance(pmc, dict) or not pmc:
            return
        summary = _fresh02_summarize_tiebreaks_from_pmc_v1(pmc)
        if not isinstance(summary, dict) or not summary:
            return
        dbg = _yureeka_get_debug_bucket_v1(wrapper, default_path=default_path)
        if not isinstance(dbg, dict):
            return

        # Only overwrite if missing/non-dict (additive + stable)
        if not isinstance(dbg.get("fresh02_freshness_tiebreak_summary_v1"), dict):
            dbg["fresh02_freshness_tiebreak_summary_v1"] = summary
        if not isinstance(dbg.get("fresh02_freshness_tiebreak_v1"), dict):
            dbg["fresh02_freshness_tiebreak_v1"] = {
                "enabled": bool(summary.get("flag_enabled_any")),
                "metrics_with_beacon_count": int(summary.get("metrics_with_beacon_count") or 0),
                "used_count": int(summary.get("used_count") or 0),
                "changed_winner_count": int(summary.get("changed_winner_count") or 0),
                "note": "derived_from_primary_metrics_canonical",
            }
    except Exception:
        return


def _yureeka_endstate_check_v1(stage: str, analysis_wrapper: dict = None, evolution_wrapper: dict = None, injected_url: str = "") -> dict:
    """Build a compact, machine-readable end-state invariants check block.

    REFACTOR199+: additive-only diagnostics. Never raises; callers should guard anyway.
    REFACTOR203: wrapper-shape aware + stage-aware (avoid misleading pmc_count=0 from early/inner wrappers).
    """
    stage_raw = str(stage or "")
    stage_l = stage_raw.strip().lower()
    checks: dict = {}
    failures: list = []

    # --- Fastpath flags (global + best-effort debug extraction) ---
    try:
        checks["disable_fastpath_for_now"] = bool(globals().get("DISABLE_FASTPATH_FOR_NOW"))
    except Exception:
        pass

    # Determine stage kind (analysis vs evolution) in a tolerant way
    stage_kind = ""
    if "evolution" in stage_l:
        stage_kind = "evolution"
    elif "analysis" in stage_l:
        stage_kind = "analysis"
    else:
        # Heuristic: if we can see metric_changes rows, it's evolution
        try:
            if _yureeka_get_metric_changes_v1(evolution_wrapper if isinstance(evolution_wrapper, dict) else (analysis_wrapper if isinstance(analysis_wrapper, dict) else {})):
                stage_kind = "evolution"
            else:
                stage_kind = "analysis"
        except Exception:
            stage_kind = "analysis"
    checks["stage_kind"] = stage_kind

    # --- Analysis-side checks ---
    if stage_kind == "analysis":
        try:
            aw = analysis_wrapper if isinstance(analysis_wrapper, dict) else (evolution_wrapper if isinstance(evolution_wrapper, dict) else {})
            pmc = _yureeka_get_pmc_v1(aw)
            pmc_count = len(pmc) if isinstance(pmc, dict) else 0
            checks["pmc_count"] = pmc_count
            checks["pmc_top_n"] = min(pmc_count, 10)
            try:
                checks["pmc_sample_keys"] = list(pmc.keys())[:4] if isinstance(pmc, dict) else []
            except Exception:
                pass
            checks["pmc_expected_4"] = bool(pmc_count == 4)
            if pmc_count != 4:
                failures.append("analysis:pmc_count!=4")
        except Exception:
            pass

    # --- Evolution-side checks ---
    if stage_kind == "evolution":
        try:
            ew = evolution_wrapper if isinstance(evolution_wrapper, dict) else (analysis_wrapper if isinstance(analysis_wrapper, dict) else {})
            rows = _yureeka_get_metric_changes_v1(ew)
            if not isinstance(rows, list):
                rows = []
            row_count = len(rows)
            checks["row_count"] = row_count
            checks["metric_rows"] = row_count
            if row_count != 4:
                failures.append("evolution:row_count!=4")

            # Timestamps (avoid false negatives when the caller passes an inner wrapper)
            _has_prev_ts_key = "previous_timestamp" in (ew or {})
            _has_ts_key = "timestamp" in (ew or {})
            prev_ts = str((ew or {}).get("previous_timestamp") or "") if _has_prev_ts_key else ""
            cur_ts = str((ew or {}).get("timestamp") or "") if _has_ts_key else ""
            checks["previous_timestamp_present"] = (bool(prev_ts) if _has_prev_ts_key else None)
            checks["timestamp_present"] = (bool(cur_ts) if _has_ts_key else None)

            # Baseline wiring: previous_timestamp should match baseline analysis timestamp (if available).
            base_ts = ""
            try:
                if isinstance(analysis_wrapper, dict):
                    base_ts = str(analysis_wrapper.get("timestamp") or "")
                    if not base_ts:
                        base_ts = str(analysis_wrapper.get("analysis_timestamp_effective") or "")
            except Exception:
                base_ts = ""
            checks["analysis_timestamp_available"] = bool(base_ts)
            checks["prev_equals_analysis_ts"] = ((prev_ts == base_ts) if (base_ts and _has_prev_ts_key and prev_ts) else None)
            if base_ts and _has_prev_ts_key and prev_ts and prev_ts != base_ts:
                failures.append("evolution:prev_ts!=analysis_ts")

            # Δt gating diagnostics (prod populated, injected blank/None)
            delta_vals = []
            urls = []
            for r in rows:
                if isinstance(r, dict):
                    delta_vals.append(r.get("analysis_evolution_delta_seconds"))
                    urls.append(str(r.get("source_url") or "").strip())

            present = 0
            blank = 0
            for v in delta_vals:
                try:
                    if v is None or v == "" or v == "None":
                        blank += 1
                    elif isinstance(v, (int, float)) and float(v) >= 0:
                        present += 1
                    else:
                        # non-numeric -> treat as blank-ish
                        blank += 1
                except Exception:
                    blank += 1

            denom = len(delta_vals) if delta_vals else 0
            checks["delta_present_count"] = present
            checks["delta_blank_count"] = blank
            checks["delta_present_ratio"] = ((present / denom) if denom else None)

            inj = str(injected_url or "").strip()
            checks["injected_url_hint_present"] = bool(inj)
            _ratio = checks.get("delta_present_ratio")
            assumed_injected = (bool(inj) if denom == 0 else (bool(inj) or ((isinstance(_ratio, (int, float)) and _ratio < 0.5))))
            checks["assumed_injected_mode"] = assumed_injected

            # Expected gating by mode (soft-check; never fails if denom==0)
            if denom:
                if assumed_injected:
                    checks["delta_gating_expected"] = "blank"
                    checks["delta_gating_ok"] = bool(present == 0)
                else:
                    checks["delta_gating_expected"] = "present"
                    checks["delta_gating_ok"] = bool(present == denom)
            else:
                checks["delta_gating_expected"] = "unknown"
                checks["delta_gating_ok"] = None

            # If we have an injected_url hint, verify winners are from it (informational)
            if inj:
                inj_hits = sum(1 for u in urls if u == inj)
                checks["injected_url"] = inj
                checks["injected_url_hit_count"] = inj_hits
                checks["injected_url_hit_ratio"] = (inj_hits / len(urls)) if urls else 0.0

            # Stability (if present)
            try:
                if isinstance((ew or {}).get("stability_score"), (int, float)):
                    checks["stability_score"] = float((ew or {}).get("stability_score"))
                else:
                    rr = (ew or {}).get("results")
                    if isinstance(rr, dict) and isinstance(rr.get("stability_score"), (int, float)):
                        checks["stability_score"] = float(rr.get("stability_score"))
            except Exception:
                pass

            # Optional: baseline PMC visibility in evolution (informational only)
            try:
                pmc = _yureeka_get_pmc_v1(analysis_wrapper if isinstance(analysis_wrapper, dict) else ew)
                if isinstance(pmc, dict):
                    checks["pmc_count"] = len(pmc)
            except Exception:
                pass
        except Exception:
            pass

    return {
        "stage": stage_raw,
        "pass": (len(failures) == 0),
        "failures": failures,
        "checks": checks,
    }


def _yureeka_attach_build_meta_v1(wrapper: dict, stage: str = "", injected_url: str = ""):
    """Additive: attach a small build_meta block for auditability (RC/freeze), without affecting selection/diff."""
    if not isinstance(wrapper, dict):
        return
    try:
        meta = {
            "code_version": _yureeka_get_code_version(),
            "release_candidate": bool(globals().get("YUREEKA_RELEASE_CANDIDATE_V1")),
            "release_tag": str(globals().get("YUREEKA_RELEASE_TAG_V1") or ""),
            "freeze_mode": bool(globals().get("YUREEKA_FREEZE_MODE_V1")),
            "stage": str(stage or ""),
        }
        inj = str(injected_url or "").strip()
        if inj:
            meta["injected_url"] = inj
        wrapper.setdefault("build_meta", {})
        if isinstance(wrapper.get("build_meta"), dict):
            wrapper["build_meta"].update(meta)
        try:
            _yureeka_attach_hyperparams_debug_v1(wrapper, stage=str(stage or ""))
        except Exception:
            pass
    except Exception:
        return

def _yureeka_attach_hyperparams_debug_v1(wrapper: dict, stage: str = ""):
    """Additive: attach hyperparam snapshot + fingerprints for auditability (no behavior change).

    Stage-aware placement:
      - Always attach to wrapper["debug"] when available
      - If wrapper has nested results dicts, also attach to results["debug"] for Evolution UI panels
    """
    if not isinstance(wrapper, dict):
        return
    try:
        targets = [wrapper]
        r1 = wrapper.get("results")
        if isinstance(r1, dict):
            targets.append(r1)
            r2 = r1.get("results")
            if isinstance(r2, dict):
                targets.append(r2)

        hp = _yureeka_get_hyperparams_v1()
        hp_dict = hp if isinstance(hp, dict) else {}
        fp_full = str(globals().get("HYPERPARAMS_FINGERPRINT_V1") or _yureeka_hyperparams_fingerprint_v1("full"))
        fp_llm = str(globals().get("HYPERPARAMS_FINGERPRINT_LLM_V1") or _yureeka_hyperparams_fingerprint_v1("llm_only"))
        src = str(globals().get("_HYPERPARAMS_SOURCE_V1") or "")

        for t in targets:
            try:
                dbg = t.setdefault("debug", {})
                if not isinstance(dbg, dict):
                    continue
                # Attach only once per debug dict
                if isinstance(dbg.get("hyperparams_snapshot_v1"), dict):
                    continue
                dbg["hyperparams_snapshot_v1"] = hp_dict
                dbg["hyperparams_fingerprint_v1"] = fp_full
                dbg["hyperparams_fingerprint_llm_v1"] = fp_llm
                dbg["hyperparams_source_v1"] = src
                dbg["hyperparams_stage_v1"] = str(stage or "")
            except Exception:
                continue
    except Exception:
        return


def _yureeka_attach_endstate_check_v1(wrapper: dict, stage: str, analysis_wrapper: dict = None, injected_url: str = ""):
    """Attach endstate_check_v1 into a durable debug location for the given wrapper.

    REFACTOR203: stage-aware placement:
      - Analysis: wrapper["debug"]
      - Evolution: wrapper["results"]["debug"] (or wrapper["results"]["results"]["debug"])
    """
    if not isinstance(wrapper, dict):
        return

    stage_l = str(stage or "").strip().lower()
    mode = "evolution" if "evolution" in stage_l else ("analysis" if "analysis" in stage_l else "analysis")
    try:
        if mode == "analysis":
            block = _yureeka_endstate_check_v1(stage=stage, analysis_wrapper=wrapper, evolution_wrapper=None, injected_url=injected_url)
        else:
            block = _yureeka_endstate_check_v1(stage=stage, analysis_wrapper=analysis_wrapper, evolution_wrapper=wrapper, injected_url=injected_url)
    except Exception:
        return

    dbg = _yureeka_get_debug_bucket_v1(wrapper, default_path=("evolution" if mode == "evolution" else "analysis"))
    if isinstance(dbg, dict):
        try:
            dbg["endstate_check_v1"] = block
        except Exception:
            pass

def _yureeka_authority_manifest_v1() -> dict:
    """Additive debug helper: capture which 'last-wins' definitions are actually active at runtime.

    This is used during the downsizing phase to ensure deletions don't accidentally swap authority.
    Streamlit-safe: pure introspection (no IO/network).
    """
    def _fn_meta(name: str):
        try:
            fn = globals().get(name)
            if fn is None:
                return {"present": False}
            meta = {"present": True, "type": str(type(fn))}
            if callable(fn):
                try:
                    meta.update({
                        "name": str(getattr(fn, "__name__", "") or ""),
                        "qualname": str(getattr(fn, "__qualname__", "") or ""),
                        "module": str(getattr(fn, "__module__", "") or ""),
                        "id": str(id(fn)),
                    })
                except Exception:
                    pass
                try:
                    c = getattr(fn, "__code__", None)
                    if c is not None:
                        meta["firstlineno"] = int(getattr(c, "co_firstlineno", -1) or -1)
                        meta["filename"] = str(getattr(c, "co_filename", "") or "")
                except Exception:
                    pass
            return meta
        except Exception as e:
            return {"present": False, "error": f"{e}"}

    try:
        keys = [
            # evolution / snapshots
            "run_source_anchored_evolution",
            "compute_source_anchored_diff",
            "attach_source_snapshots_to_analysis",
            # diff panel + join engine
            "build_diff_metrics_panel_v2__rows_refactor47",

        ]
    except Exception:
        keys = []

    out = {"code_version": str(globals().get("_YUREEKA_CODE_VERSION_LOCK") or ""), "targets": {}}
    for k in keys:
        out["targets"][k] = _fn_meta(k)
    return out
def _yureeka_runtime_identity_v1():
    """Additive debug helper: identify the running script reliably (helps diagnose stale-version runs)."""
    try:
        import sys, platform, datetime
        out = {
            "code_version": _yureeka_get_code_version(),
            "authority_manifest_v1": _yureeka_authority_manifest_v1(),
            "code_version_lock": globals().get("_YUREEKA_CODE_VERSION_LOCK"),
        }
        try:
            out["__file__"] = __file__
        except Exception:
            out["__file__"] = None
        try:
            out["cwd"] = os.getcwd()
        except Exception:
            out["cwd"] = None
        try:
            out["pid"] = os.getpid()
        except Exception:
            out["pid"] = None
        try:
            out["python"] = sys.version.split()[0]
        except Exception:
            out["python"] = None
        try:
            out["platform"] = platform.platform()
        except Exception:
            out["platform"] = None
        try:
            out["now_utc"] = datetime.datetime.now(datetime.timezone.utc).isoformat()
        except Exception:
            out["now_utc"] = None

        # File signature (best-effort; safe in Streamlit)
        try:
            p = out.get("__file__")
            if isinstance(p, str) and p and os.path.exists(p):
                with open(p, "rb") as f:
                    b = f.read()
                out["file_sha1_12"] = hashlib.sha1(b).hexdigest()[:12]
                out["file_bytes"] = int(len(b))
        except Exception:
            pass

        return out
    except Exception:
        return {"code_version": _yureeka_get_code_version(),
            "authority_manifest_v1": _yureeka_authority_manifest_v1(), "code_version_lock": globals().get("_YUREEKA_CODE_VERSION_LOCK")}

def _yureeka_lock_version_globals_v1():
    """Re-assert global version vars for observability (does not affect the frozen getter)."""
    try:
        v = _yureeka_get_code_version()
        globals()["_YUREEKA_CODE_VERSION_LOCK"] = v
        globals()["CODE_VERSION"] = v
    except Exception:
        pass

def _fix2d73_promote_rehydrated_prevdata_v1(prev_full: dict) -> dict:
    diag = {
        "applied": False,
        "pmc_before": 0,
        "pmc_after": 0,
        "pmc_source": None,
        "notes": [],
    }
    try:
        if not isinstance(prev_full, dict):
            return prev_full

        # Count before
        try:
            if isinstance(prev_full.get("primary_metrics_canonical"), dict):
                diag["pmc_before"] = int(len(prev_full.get("primary_metrics_canonical") or {}))
        except Exception:
            pass

        # Find candidate pmc in common nested locations
        pmc = None
        src = None
        if isinstance(prev_full.get("primary_metrics_canonical"), dict) and prev_full.get("primary_metrics_canonical"):
            pmc = prev_full.get("primary_metrics_canonical")
            src = "prev_full.primary_metrics_canonical"
        elif isinstance(prev_full.get("primary_response"), dict) and isinstance(prev_full["primary_response"].get("primary_metrics_canonical"), dict) and prev_full["primary_response"].get("primary_metrics_canonical"):
            pmc = prev_full["primary_response"].get("primary_metrics_canonical")
            src = "prev_full.primary_response.primary_metrics_canonical"
        elif isinstance(prev_full.get("results"), dict) and isinstance(prev_full["results"].get("primary_metrics_canonical"), dict) and prev_full["results"].get("primary_metrics_canonical"):
            pmc = prev_full["results"].get("primary_metrics_canonical")
            src = "prev_full.results.primary_metrics_canonical"
        elif isinstance(prev_full.get("results"), dict) and isinstance(prev_full["results"].get("primary_response"), dict) and isinstance(prev_full["results"]["primary_response"].get("primary_metrics_canonical"), dict) and prev_full["results"]["primary_response"].get("primary_metrics_canonical"):
            pmc = prev_full["results"]["primary_response"].get("primary_metrics_canonical")
            src = "prev_full.results.primary_response.primary_metrics_canonical"
        elif isinstance(prev_full.get("results"), dict) and isinstance(prev_full["results"].get("results"), dict) and isinstance(prev_full["results"]["results"].get("primary_metrics_canonical"), dict) and prev_full["results"]["results"].get("primary_metrics_canonical"):
            pmc = prev_full["results"]["results"].get("primary_metrics_canonical")
            src = "prev_full.results.results.primary_metrics_canonical"

        # Promote to top-level + mirror into primary_response
        if isinstance(pmc, dict) and pmc:
            if not (isinstance(prev_full.get("primary_metrics_canonical"), dict) and prev_full.get("primary_metrics_canonical")):
                prev_full["primary_metrics_canonical"] = dict(pmc)
                diag["notes"].append("promoted_top_level_primary_metrics_canonical")
                diag["applied"] = True
            if not isinstance(prev_full.get("primary_response"), dict):
                prev_full["primary_response"] = {}
                diag["notes"].append("created_primary_response")
            if isinstance(prev_full.get("primary_response"), dict):
                if not (isinstance(prev_full["primary_response"].get("primary_metrics_canonical"), dict) and prev_full["primary_response"].get("primary_metrics_canonical")):
                    prev_full["primary_response"]["primary_metrics_canonical"] = dict(prev_full.get("primary_metrics_canonical") or {})
                    diag["notes"].append("filled_primary_response.primary_metrics_canonical")
                    diag["applied"] = True

            diag["pmc_source"] = src

        # Count after
        try:
            if isinstance(prev_full.get("primary_metrics_canonical"), dict):
                diag["pmc_after"] = int(len(prev_full.get("primary_metrics_canonical") or {}))
        except Exception:
            pass

        # Attach diag
        try:
            prev_full.setdefault("debug", {})
            if isinstance(prev_full.get("debug"), dict):
                prev_full["debug"]["fix2d73_historyfull_load_counts"] = dict(diag)
        except Exception:
            pass

        return prev_full
    except Exception:
        return prev_full

# Purpose:
#   Force current-side canonical rebuild to be schema-anchored
#   to the Analysis (prev_response) schema universe, so keys
#   overlap and "Current" can populate.
#
# Strategy:
#   Prefer rebuild_metrics_from_snapshots_schema_only_fix16 if
#   callable; fallback to rebuild_metrics_from_snapshots_analysis_canonical_v1.
#
#   Render/diff-facing only; does not change hashing/snapshots.

def _fix2d9_schema_anchored_rebuild_current_metrics_v1(prev_response, pool, web_context=None):
    diag = {
        "applied": False,
        "fn": None,
        "count": 0,
        "keys_sample": [],
        "reason": None,
    }
    try:
        if pool is None:
            diag["reason"] = "pool_none"
            return None, diag
        if not isinstance(pool, list) or not pool:
            diag["reason"] = "pool_empty"
            return None, diag

        fn = globals().get("rebuild_metrics_from_snapshots_schema_only_fix16")
        fn_name = "rebuild_metrics_from_snapshots_schema_only_fix16"
        if not callable(fn):
            fn = globals().get("rebuild_metrics_from_snapshots_analysis_canonical_v1")
            fn_name = "rebuild_metrics_from_snapshots_analysis_canonical_v1"

        if not callable(fn):
            diag["reason"] = "fn_missing"
            return None, diag

        diag["fn"] = fn_name

        try:
            rebuilt = fn(prev_response, pool, web_context=web_context)
        except TypeError:
            rebuilt = fn(prev_response, pool)

        # REFACTOR04: enrich rebuilt PMC metrics with unit_tag/unit_family/multiplier_to_base for parity + diffing.
        try:
            if isinstance(rebuilt, dict) and rebuilt:
                rebuilt = _refactor04_enrich_pmc_units_v1(rebuilt, prev_response=prev_response)
        except Exception:
            pass

        if isinstance(rebuilt, dict) and rebuilt:
            diag["applied"] = True
            diag["count"] = len(rebuilt)
            try:
                diag["keys_sample"] = list(rebuilt.keys())[:10]
            except Exception:
                pass
                diag["keys_sample"] = []
            return dict(rebuilt), diag

        diag["reason"] = "rebuilt_empty_or_non_dict"
        return None, diag
    except Exception as _e:
        diag["reason"] = "exception:" + str(type(_e).__name__)
        return None, diag

# Purpose:
#   Allow a hardcoded override for diff join mode (demo/debug).
#   If FORCE_DIFF_JOIN_MODE is set (e.g. "union"), it overrides
#   EVO_DIFF_JOIN_MODE environment variable.
FORCE_DIFF_JOIN_MODE = "union"   # set to None to restore env-based behavior

def _fix2d6_get_diff_join_mode_v1():
    try:
        if FORCE_DIFF_JOIN_MODE:
            return str(FORCE_DIFF_JOIN_MODE).strip().lower()
    except Exception:
        pass
    try:
        return str(os.getenv("EVO_DIFF_JOIN_MODE", "strict")).strip().lower()
    except Exception:
        return "strict"

# Purpose:
#   Emit explicit canonical key overlap diagnostics between previous and
#   current canonical metrics to make diff feasibility observable.
#   (Additive, no behavior change)

def _fix2af_norm_url(u: str) -> str:
    try:
        s = str(u or "").strip()
        if not s:
            return ""
        _norm = globals().get("_inj_diag_norm_url_list")
        if callable(_norm):
            try:
                out = _norm([s])
                if out and isinstance(out, list):
                    return str(out[0] or "")
            except Exception:
                pass
        if s.startswith("http://"):
            s = "https://" + s[len("http://"):]
        if "#" in s:
            s = s.split("#", 1)[0]
        return s.rstrip("/")
    except Exception:
        return ""

def _fix2af_normalize_url_items(urls):
    diag = {
        "input_type": type(urls).__name__,
        "input_len": 0,
        "flattened_len": 0,
        "string_urls": 0,
        "dict_urls": 0,
        "dropped_non_url": 0,
        "mixed_shape": False,
        "nested_lists": 0,
        "samples_dropped": [],
    }
    out = []
    def _emit(u):
        nu = _fix2af_norm_url(u)
        if nu:
            out.append(nu)
        else:
            diag["dropped_non_url"] += 1
            if len(diag["samples_dropped"]) < 10:
                diag["samples_dropped"].append(str(u)[:200])
    def _walk(x):
        if x is None:
            return
        if isinstance(x, (list, tuple)):
            diag["nested_lists"] += 1
            for y in x:
                _walk(y)
            return
        if isinstance(x, dict):
            diag["dict_urls"] += 1
            for k in ("url", "href", "link"):
                if k in x and x.get(k):
                    _emit(x.get(k))
                    return
            diag["dropped_non_url"] += 1
            if len(diag["samples_dropped"]) < 10:
                diag["samples_dropped"].append(str(x)[:200])
            return
        diag["string_urls"] += 1
        _emit(x)

    try:
        if urls is None:
            diag["input_len"] = 0
        elif isinstance(urls, (list, tuple)):
            diag["input_len"] = len(urls)
            for it in urls:
                _walk(it)
        else:
            diag["input_len"] = 1
            _walk(urls)
    except Exception:
        pass

    diag["flattened_len"] = len(out)
    diag["mixed_shape"] = (diag["dict_urls"] > 0 and diag["string_urls"] > 0)

    seen = set()
    dedup = []
    for u in out:
        if u in seen:
            continue
        seen.add(u)
        dedup.append(u)
    return dedup, diag

def _fix2af_scraped_text_accessor(x):
    try:
        if x is None:
            return ""
        if isinstance(x, str):
            return x
        if isinstance(x, bytes):
            try:
                return x.decode("utf-8", errors="ignore")
            except Exception:
                return ""
        if isinstance(x, dict):
            for k in ("text", "clean_text", "content", "body", "html"):
                v = x.get(k)
                if isinstance(v, str) and v.strip():
                    return v
                if isinstance(v, bytes):
                    try:
                        return v.decode("utf-8", errors="ignore")
                    except Exception:
                        pass
            if "data" in x and isinstance(x["data"], dict):
                return _fix2af_scraped_text_accessor(x["data"])
            return ""
        return str(x)
    except Exception:
        return ""

def _fix2af_classify_fetch_failure(status, txt):
    try:
        s = str(status or "").lower()
        tlen = len(txt or "")
        if (not s or s == "success_direct") and tlen > 0:
            return "ok"
        if "timeout" in s:
            return "timeout"
        if "captcha" in s or "forbidden" in s or "blocked" in s or "403" in s:
            return "blocked"
        if "paywall" in s:
            return "paywall"
        if "pdf" in s and (tlen == 0 or "no_text" in s):
            return "pdf_no_text"
        if "no_text" in s or tlen == 0 or "empty" in s:
            return "no_text"
        if "redirect" in s:
            return "redirect"
        if "error" in s or "exception" in s or "fail" in s:
            return "error"

        # - Some sources label unit counts as "million units" while metric_name is "... Sales ...".
        # - If we see sales language AND a magnitude-like unit, bind to unit_sales.
        try:
            if ("sales" in n or "sold" in n) and any(tok in u for tok in ("million", "billion", "thousand", "units", "unit", "vehicles", "pcs", "pieces")):
                return "unit_sales"
        except Exception:
            return "unknown"
    except Exception:
        return "unknown"

def _fix2af_ledger_put(ledger: dict, url_raw: str, stage: str, reason: str = "", extra: dict = None):
    try:
        if ledger is None:
            return
        u = _fix2af_norm_url(url_raw)
        if not u:
            return
        rec = ledger.get(u) or {"url_norm": u, "stages": []}
        rec["stages"].append({
            "stage": str(stage or ""),
            "reason": str(reason or ""),
            "extra": extra if isinstance(extra, dict) else {}, })
        ledger[u] = rec
    except Exception:
        pass

_fix2af_last_scrape_ledger = {}

# - Deterministic sorting / tie-breaking helpers
# - Deterministic candidate index builder (anchor_hash -> best candidate)
# - Lightweight schema + universe hashing for convergence checks
# - One-button end-state validation harness (callable)
# NOTE: Additive only; existing logic remains intact.

# GOOGLE SHEETS HISTORY STORAGE
# Max number of history rows to load from Sheets/session when hydrating baselines.
# Restored in REFACTOR170 (was accidentally removed in REFACTOR169).
MAX_HISTORY_ITEMS = int(_yureeka_hp_get_v1('ops.max_history_items', 50) or 50)

# Google Sheets OAuth scopes (module-level; used by get_google_sheet/get_google_spreadsheet)
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",
]


def get_google_sheet():
    """Connect to Google Sheets worksheet (History). Returns a gspread Worksheet or None.

    REFACTOR195:
      - Avoid per-connect header reads (row_values) which cost extra quota.
      - Add cooldown handling for 429/RESOURCE_EXHAUSTED to prevent repeated failures.
      - Cache gspread client/spreadsheet/worksheet in Streamlit session_state to reduce repeated API calls.
    """
    try:
        spreadsheet_name = st.secrets.get("google_sheets", {}).get("spreadsheet_name", "Yureeka_JSON")
        worksheet_title = st.secrets.get("google_sheets", {}).get("worksheet_title", "History")

        # Short-circuit during cooldown: prefer cached handles or local fallbacks.
        if _yureeka_sheets_in_rate_limit_cooldown_v1():
            ws_key = f"_yureeka_gsheet_ws_v1::{spreadsheet_name}::{worksheet_title}"
            cached_ws = st.session_state.get(ws_key)
            if cached_ws is not None:
                return cached_ws
            _yureeka_sheets_rate_limit_notice_once_v1("⚠️ Google Sheets rate limit hit (429). Using local snapshot history for now.")
            return None

        scopes = _coerce_google_oauth_scopes(SCOPES)
        try:
            _YUREEKA_SHEETS_SCOPES_DEBUG_V1.clear()
            _YUREEKA_SHEETS_SCOPES_DEBUG_V1.update({"scopes": list(scopes), "n": int(len(scopes)), "ts": _yureeka_now_iso_v1()})
        except Exception:
            pass

        creds = Credentials.from_service_account_info(dict(st.secrets["gcp_service_account"]), scopes=scopes)

        # Cache the gspread client & opened spreadsheet per Streamlit session.
        client_key = "_yureeka_gspread_client_v1"
        ss_key = f"_yureeka_gspread_ss_v1::{spreadsheet_name}"
        ws_key = f"_yureeka_gsheet_ws_v1::{spreadsheet_name}::{worksheet_title}"

        client = st.session_state.get(client_key)
        if client is None:
            client = gspread.authorize(creds)
            st.session_state[client_key] = client

        ss = st.session_state.get(ss_key)
        if ss is None:
            ss = client.open(spreadsheet_name)
            st.session_state[ss_key] = ss

        ws = st.session_state.get(ws_key)
        if ws is None:
            try:
                ws = ss.worksheet(worksheet_title)
            except Exception:
                ws = ss.sheet1
            st.session_state[ws_key] = ws

        return ws
    except Exception as e:
        try:
            if _is_sheets_rate_limit_error(e):
                _yureeka_sheets_mark_rate_limited_v1(e)
                _yureeka_sheets_rate_limit_notice_once_v1("⚠️ Google Sheets rate limit hit (429). Using local snapshot history for now.")
                # best-effort: return cached worksheet handle if present
                try:
                    spreadsheet_name = st.secrets.get("google_sheets", {}).get("spreadsheet_name", "Yureeka_JSON")
                    worksheet_title = st.secrets.get("google_sheets", {}).get("worksheet_title", "History")
                    ws_key = f"_yureeka_gsheet_ws_v1::{spreadsheet_name}::{worksheet_title}"
                    cached_ws = st.session_state.get(ws_key)
                    if cached_ws is not None:
                        return cached_ws
                except Exception:
                    pass
                return None
            st.error(f"❌ Failed to connect to Google Sheets: {e}")
        except Exception:
            pass
        return None



def generate_analysis_id() -> str:
    """Generate an analysis_id like YYYYMMDD_HHMMSS_xxxxxx (UTC).

    This format is relied upon by _refactor112_parse_ts_from_analysis_id_v1 for
    timestamp inference when created_at fields are missing or non-ISO.
    """
    try:
        ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        suf = os.urandom(3).hex()  # 6 hex chars
        return f"{ts}_{suf}"
    except Exception:
        try:
            return str(int(time.time()))
        except Exception:
            return "0"

def add_to_history(analysis: dict) -> bool:
    """
    Save analysis to Google Sheet (or session fallback).

    ADDITIVE end-state wiring:
      - If a baseline source cache exists, build & store:
          * evidence_records (structured, cached)
          * metric_anchors (baseline metrics anchored to evidence)
      - Prevent Google Sheets 50,000-char single-cell limit errors by shrinking only
        the JSON payload written into the single "analysis json" cell when necessary.

    Backward compatible:
      - Only adds keys; does not remove existing fields.
      - Never blocks saving if enrichment fails.
      - If Sheets unavailable, falls back to session_state.
    """

    # REFACTOR36: harden against None callers (prevents NoneType.get crashes)
    if not isinstance(analysis, dict):
        return False

    SHEETS_CELL_LIMIT = 50000

    # - Added primary_response.baseline_sources_cache as extra fallback
    baseline_cache = (
        analysis.get("baseline_sources_cache")
        or (analysis.get("primary_response", {}) or {}).get("baseline_sources_cache")
        or (analysis.get("results", {}) or {}).get("baseline_sources_cache")
        or (analysis.get("results", {}) or {}).get("source_results")
    )

    # REFACTOR168: Evidence Layer / Metric Anchors enrichment removed (downsize-only; backward-compatible readers remain).

    # Why:
    # - HistoryFull replay/diff requires Analysis baseline canonical values (not just schema).
    # - Prior runs persisted metric_schema_frozen but had no primary_metrics_canonical map.
    # - Option B: compute once during Analysis and persist the decided map.
    # Behavior:
    # - If primary_metrics_canonical is missing/empty, rebuild deterministically from
    #   metric_schema_frozen + baseline_sources_cache using the schema-anchored rebuild.
    # - Mirror into analysis.primary_response.primary_metrics_canonical so Sheets minimal
    #   fallback still carries it.
    # - Emit debug counts for verification.
    try:
        if isinstance(analysis, dict):
            _already = None
            if isinstance(analysis.get('primary_metrics_canonical'), dict) and analysis.get('primary_metrics_canonical'):
                _already = 'analysis.primary_metrics_canonical'
            elif isinstance(analysis.get('primary_response'), dict) and isinstance(analysis['primary_response'].get('primary_metrics_canonical'), dict) and analysis['primary_response'].get('primary_metrics_canonical'):
                _already = 'analysis.primary_response.primary_metrics_canonical'
            elif isinstance(analysis.get('results'), dict) and isinstance(analysis['results'].get('primary_metrics_canonical'), dict) and analysis['results'].get('primary_metrics_canonical'):
                _already = 'analysis.results.primary_metrics_canonical'

            if not _already:
                _pool = None
                try:
                    if isinstance(analysis.get('baseline_sources_cache'), list):
                        _pool = analysis.get('baseline_sources_cache')
                    elif isinstance(analysis.get('results'), dict) and isinstance(analysis['results'].get('baseline_sources_cache'), list):
                        _pool = analysis['results'].get('baseline_sources_cache')
                except Exception:
                    _pool = None

                _schema_ok = isinstance(analysis.get('metric_schema_frozen'), dict) and bool(analysis.get('metric_schema_frozen'))

                _rebuilt = None
                _diag = None
                if _schema_ok and isinstance(_pool, list) and _pool:
                    try:
                        _rebuilt, _diag = _fix2d9_schema_anchored_rebuild_current_metrics_v1(
                            analysis,
                            _pool,
                            web_context=analysis.get('web_context') if isinstance(analysis.get('web_context'), dict) else None,
                        )

                        # FIX2D86: sanitize rebuilt baseline PMC so __percent keys cannot bind bare year tokens (e.g., 2040)
                        try:
                            if isinstance(_rebuilt, dict) and _rebuilt:
                                _schema_fix2d86 = analysis.get("metric_schema_frozen") if isinstance(analysis.get("metric_schema_frozen"), dict) else {}
                                _rebuilt2, _dbg_fix2d86 = _fix2d86_sanitize_pmc_percent_year_tokens_v1(
                                    pmc=_rebuilt,
                                    metric_schema_frozen=_schema_fix2d86,
                                    label="fix2d75_materialize_pmc",
                                )
                                _rebuilt = _rebuilt2
                                try:
                                    analysis.setdefault("debug", {})
                                    if isinstance(analysis.get("debug"), dict):
                                        analysis["debug"]["fix2d86_percent_year_token_sanitize_materialize"] = _dbg_fix2d86
                                except Exception:
                                    pass
                        except Exception:
                            pass

                    except Exception:
                        _rebuilt, _diag = (None, None)

                # REFACTOR35: guard against schema-only rebuilds leaking debug keys into PMC
                # Keep only keys that exist in the frozen schema.
                try:
                    _schema_keys = set((analysis.get('metric_schema_frozen') or {}).keys()) if isinstance(analysis.get('metric_schema_frozen'), dict) else set()
                    if isinstance(_rebuilt, dict) and _schema_keys:
                        _rebuilt = {k: v for (k, v) in _rebuilt.items() if isinstance(k, str) and k in _schema_keys and isinstance(v, dict)}
                except Exception:
                    pass

                if isinstance(_rebuilt, dict) and _rebuilt:
                    try:
                        analysis['primary_metrics_canonical'] = dict(_rebuilt)
                    except Exception:
                        pass
                    try:
                        analysis.setdefault('primary_response', {})
                        if isinstance(analysis.get('primary_response'), dict):
                            analysis['primary_response']['primary_metrics_canonical'] = dict(_rebuilt)
                    except Exception:
                        pass
                    try:
                        analysis.setdefault('results', {})
                        if isinstance(analysis.get('results'), dict):
                            analysis['results']['primary_metrics_canonical'] = dict(_rebuilt)
                    except Exception:
                        pass

                # Debug
                try:
                    analysis.setdefault('debug', {})
                    if isinstance(analysis.get('debug'), dict):
                        analysis['debug']['fix2d75_materialize_pmc'] = {
                            'had_existing': bool(_already),
                            'existing_source': str(_already or ''),
                            'schema_present': bool(_schema_ok),
                            'pool_count': int(len(_pool)) if isinstance(_pool, list) else 0,
                            'rebuilt_count': int(len(_rebuilt)) if isinstance(_rebuilt, dict) else 0,
                            'rebuilt_diag': _diag if isinstance(_diag, dict) else {},
                        }
                except Exception:
                    pass
            else:
                try:
                    analysis.setdefault('debug', {})
                    if isinstance(analysis.get('debug'), dict):
                        analysis['debug']['fix2d75_materialize_pmc'] = {
                            'had_existing': True,
                            'existing_source': str(_already),
                        }
                except Exception:
                    pass
    except Exception:
        pass

    # REFACTOR188: ensure analysis_wrapper_shape_v1 reflects the final persisted wrapper shape
    # (must run AFTER FIX2D75 materializes primary_metrics_canonical inside add_to_history).
    try:
        _pmc188 = analysis.get("primary_metrics_canonical")
        _pr188 = analysis.get("primary_response")
        _pmc_keys188 = list(_pmc188.keys()) if isinstance(_pmc188, dict) else []
        _pr_keys188 = list(_pr188.keys()) if isinstance(_pr188, dict) else []
        analysis.setdefault("debug", {})
        if isinstance(analysis.get("debug"), dict):
            analysis["debug"]["analysis_wrapper_shape_v1"] = {
                "pmc_present": bool(_pmc_keys188),
                "pmc_count": int(len(_pmc_keys188)),
                "pmc_top_n": int(min(4, len(_pmc_keys188))),
                "pmc_sample_keys": sorted(_pmc_keys188)[:4] if _pmc_keys188 else [],
                "primary_response_present": bool(_pr_keys188),
                "primary_response_count": int(len(_pr_keys188)),
                "primary_response_top_n": int(min(4, len(_pr_keys188))),
                "primary_response_sample_keys": sorted(_pr_keys188)[:4] if _pr_keys188 else [],
            }
    except Exception:
        pass

    # NLP07: ensure run-scope FRESH02 tiebreak summary lands in the durable analysis debug bucket
    try:
        _fresh02_attach_run_tiebreak_summary_v1(analysis, default_path="analysis")
    except Exception:
        pass


    # Why:
    # - Drift=0 depends on analysis and evolution sharing the SAME anchor IDs.
    # - Some downstream code paths expect anchor_hash on the metric row itself
    #   and/or inside evidence entries (not only in analysis["metric_anchors"]).
    # - This patch copies existing anchor metadata only (no fabrication, no refetch).
    try:
        def _norm_ctx(s: str) -> str:
            try:
                return re.sub(r"\s+", " ", (s or "").strip())
            except Exception:
                return (s or "").strip()

        def _compute_anchor_hash(url: str, ctx: str) -> str:
            """Deterministic anchor hash used for audit/debug joins.

            Legacy optional helpers (e.g. compute_anchor_hash) were removed to keep
            this codepath single-sourced and stable.
            """
            try:
                u = (url or "").strip()
                c = _norm_ctx(ctx or "")
                if not u:
                    return ""
                payload = (u + "\n" + c).encode("utf-8", errors="ignore")
                return hashlib.sha1(payload).hexdigest()[:16]
            except Exception:
                return ""

        # Locate canonical metrics dict (prefer primary_response)
        _pmc = None
        _pr0 = analysis.get("primary_response") if isinstance(analysis, dict) else None
        if isinstance(_pr0, dict) and isinstance(_pr0.get("primary_metrics_canonical"), dict):
            _pmc = _pr0.get("primary_metrics_canonical")
        if _pmc is None and isinstance(analysis, dict) and isinstance(analysis.get("primary_metrics_canonical"), dict):
            _pmc = analysis.get("primary_metrics_canonical")

        if isinstance(metric_anchors, dict) and isinstance(_pmc, dict):
            for _ckey, _a in metric_anchors.items():
                if not isinstance(_ckey, str) or not _ckey:
                    continue
                if not isinstance(_a, dict) or not _a:
                    continue

                _m = _pmc.get(_ckey)
                if not isinstance(_m, dict):
                    continue

                _ah = str(_a.get("anchor_hash") or _a.get("anchor") or "").strip()
                _src = str(_a.get("source_url") or _a.get("url") or "").strip()
                _ctx = _a.get("context_snippet") or _a.get("context") or ""
                _ctx = _ctx.strip() if isinstance(_ctx, str) else ""

                # Copy onto metric row (only if missing)
                if _ah and not _m.get("anchor_hash"):
                    _m["anchor_hash"] = _ah
                if _src and not (_m.get("source_url") or _m.get("url")):
                    _m["source_url"] = _src
                if _ctx and not (_m.get("context_snippet") or _m.get("context")):
                    _m["context_snippet"] = _ctx[:220]

                # Pass through extra metadata if present (additive)
                if _a.get("anchor_confidence") is not None and _m.get("anchor_confidence") is None:
                    _m["anchor_confidence"] = _a.get("anchor_confidence")
                if _a.get("candidate_id") and not _m.get("candidate_id"):
                    _m["candidate_id"] = _a.get("candidate_id")
                if _a.get("fingerprint") and not _m.get("fingerprint"):
                    _m["fingerprint"] = _a.get("fingerprint")

                # Ensure evidence entries carry anchor_hash (deterministic; no new evidence)
                _ev = _m.get("evidence")
                if isinstance(_ev, list) and _ev:
                    for _e in _ev:
                        if not isinstance(_e, dict):
                            continue
                        if _e.get("anchor_hash"):
                            continue
                        _e_url = str(_e.get("url") or _e.get("source_url") or _src or "").strip()
                        _e_ctx = _e.get("context_snippet") or _e.get("context") or _ctx or ""
                        _e_ctx = _e_ctx.strip() if isinstance(_e_ctx, str) else ""
                        _eh = _compute_anchor_hash(_e_url, _e_ctx)
                        if _eh:
                            _e["anchor_hash"] = _eh
    except Exception:
        pass

    def _try_make_sheet_json(obj: dict) -> str:
        try:
            # Summarize heavy fields first (keeps payload JSON valid and smaller).
            compact = _summarize_heavy_fields_for_sheets(obj if isinstance(obj, dict) else {'value': obj})
            if isinstance(compact, dict):
                compact['_sheets_safe'] = True
            s = json.dumps(compact, ensure_ascii=False, default=str)
            if isinstance(s, str) and len(s) <= SHEETS_CELL_LIMIT:
                return s
            # Always return valid JSON under the cell limit (no mid-string truncation).
            preview = s[:2000] if isinstance(s, str) else ''
            wrapper = {'_sheets_safe': True, '_truncated': True, 'preview': preview, 'meta': {'orig_len': len(s) if isinstance(s, str) else None, 'max_chars': SHEETS_CELL_LIMIT}}
            ws = json.dumps(wrapper, ensure_ascii=False, default=str)
            if isinstance(ws, str) and len(ws) <= SHEETS_CELL_LIMIT:
                return ws
            # Tighten preview further if needed.
            wrapper['preview'] = preview[: max(0, SHEETS_CELL_LIMIT - 300)]
            ws = json.dumps(wrapper, ensure_ascii=False, default=str)
            if isinstance(ws, str) and len(ws) <= SHEETS_CELL_LIMIT:
                return ws
            # Final fallback: omit preview entirely.
            wrapper.pop('preview', None)
            return json.dumps(wrapper, ensure_ascii=False, default=str)
        except Exception:
            return json.dumps(obj if isinstance(obj, dict) else {'value': obj}, ensure_ascii=False, default=str)

    def _shrink_for_sheets(original: dict) -> dict:
        base_copy = dict(original)
        s = _try_make_sheet_json(base_copy)
        if isinstance(s, str) and len(s) <= SHEETS_CELL_LIMIT:
            return base_copy

        reduced = dict(base_copy)
        removed = []

        for k in [
            "evidence_records",
            "baseline_sources_cache",
            "metric_anchors",
            "source_results",
            "web_context",
            "scraped_meta",
            "raw_sources",
            "raw_text",
            "debug",
        ]:
            if k in reduced:
                reduced.pop(k, None)
                removed.append(k)

        reduced.setdefault("_sheet_write", {})
        if isinstance(reduced["_sheet_write"], dict):
            reduced["_sheet_write"]["truncated"] = True
            reduced["_sheet_write"]["removed_keys"] = removed[:50]

        s2 = _try_make_sheet_json(reduced)
        if isinstance(s2, str) and len(s2) <= SHEETS_CELL_LIMIT:
            return reduced

        return {
            "question": original.get("question"),
            "timestamp": original.get("timestamp"),
            "final_confidence": original.get("final_confidence"),
            "question_profile": original.get("question_profile"),
            "primary_response": original.get("primary_response") or {},
            "_sheet_write": {
                "truncated": True,
                "mode": "minimal_fallback",
                "note": "Full analysis too large for single Google Sheets cell (50k limit).",
            },
        }

    # Try Sheets
    try:
        sheet = get_google_sheet()
    except Exception:
        pass
        sheet = None

    if not sheet:
        # Sheets unavailable: fall back to in-session history and exit early
        try:
            if "analysis_history" not in st.session_state:
                st.session_state.analysis_history = []
            st.session_state.analysis_history.append(analysis)
        except Exception:
            pass
        try:
            st.session_state["last_analysis"] = analysis
        except Exception:
            pass
        return True

    # REFACTOR195: ensure History worksheet headers once per session (avoids per-connect reads).
    try:
        if not st.session_state.get("_yureeka_history_headers_ok_v1"):
            if _yureeka_sheets_in_rate_limit_cooldown_v1():
                raise RuntimeError("sheets_rate_limit_cooldown")
            expected_headers = ["id", "timestamp", "question", "final_confidence", "analysis_json"]
            headers = []
            try:
                headers = sheet.row_values(1) if sheet else []
            except Exception as e:
                if _is_sheets_rate_limit_error(e):
                    raise
            norm = [str(h or "").strip() for h in (headers or [])[:5]]
            if (not norm) or (norm != expected_headers):
                sheet.update("A1:E1", [expected_headers])
            st.session_state["_yureeka_history_headers_ok_v1"] = True
    except Exception as e:
        if _is_sheets_rate_limit_error(e):
            _yureeka_sheets_mark_rate_limited_v1(e)
            _yureeka_sheets_rate_limit_notice_once_v1("⚠️ Google Sheets rate limit hit (429). Saving to local history only for now.")
            try:
                if "analysis_history" not in st.session_state:
                    st.session_state.analysis_history = []
                st.session_state.analysis_history.append(analysis)
                st.session_state["last_analysis"] = analysis
            except Exception:
                pass
            return True

    try:
        analysis_id = generate_analysis_id()

        # - If full baseline_sources_cache exists (list-shaped), store it outside
        #   Sheets keyed by source_snapshot_hash, and attach pointer fields into
        #   analysis/results for deterministic evolution rehydration.
        # - Pure enrichment only (no refetch, no heuristics).
        try:
            _bsc = None
            if isinstance(analysis, dict):
                _bsc = analysis.get("results", {}).get("baseline_sources_cache") or analysis.get("baseline_sources_cache")

            # rebuild minimal snapshot shape from evidence_records (deterministic).
            # This enables snapshot persistence even when baseline_sources_cache
            # is a summary dict in the main analysis object.
            try:
                if (not isinstance(_bsc, list)) and isinstance(analysis, dict):
                    _er = None
                    # prefer nested results evidence_records first
                    if isinstance(analysis.get("results"), dict):
                        _er = analysis["results"].get("evidence_records")
                    if _er is None:
                        _er = analysis.get("evidence_records")
                    _rebuilt = build_baseline_sources_cache_from_evidence_records(_er)
                    if isinstance(_rebuilt, list) and _rebuilt:
                        _bsc = _rebuilt
            except Exception:
                pass

            if isinstance(_bsc, list) and _bsc:
                _ssh = compute_source_snapshot_hash(_bsc)

                _ssh_v2 = None
                try:
                    _ssh_v2 = compute_source_snapshot_hash_v2(_bsc)
                except Exception:
                    pass
                    _ssh_v2 = None
                if _ssh:
                    # - Persists full baseline_sources_cache in a dedicated worksheet tab.
                    # - Falls back to local snapshot_store file if Sheets snapshot store unavailable.
                    # - Pointer ref stored as 'gsheet:Snapshots:<hash>' when successful.
                    _gs_ref = ""
                    _gs_ref_v2 = ""
                    try:
                        _gs_ref = store_full_snapshots_to_sheet(_bsc, _ssh, worksheet_title="Snapshots")
                        if _ssh_v2 and isinstance(_ssh_v2, str) and _ssh_v2 != _ssh:
                            try:
                                _gs_ref_v2 = store_full_snapshots_to_sheet(_bsc, _ssh_v2, worksheet_title="Snapshots")
                            except Exception:
                                _gs_ref_v2 = ""
                    except Exception:
                        _gs_ref = ""
                        _gs_ref_v2 = ""

                    _ref = ""
                    try:
                        _ref = store_full_snapshots_local(_bsc, _ssh)
                    except Exception:
                        _ref = ""

                    analysis["source_snapshot_hash"] = analysis.get("source_snapshot_hash") or _ssh
                    analysis.setdefault("results", {})
                    if isinstance(analysis["results"], dict):
                        analysis["results"]["source_snapshot_hash"] = analysis["results"].get("source_snapshot_hash") or _ssh
                        try:
                            if _ssh_v2:
                                analysis["results"]["source_snapshot_hash_v2"] = analysis["results"].get("source_snapshot_hash_v2") or _ssh_v2
                                # - Prefer v2 (stable) when present; fall back to legacy v1.
                                try:
                                    _ssh_stable = _ssh_v2 or _ssh
                                    if _ssh_stable:
                                        analysis["source_snapshot_hash_stable"] = analysis.get("source_snapshot_hash_stable") or _ssh_stable
                                        analysis["results"]["source_snapshot_hash_stable"] = analysis["results"].get("source_snapshot_hash_stable") or _ssh_stable
                                except Exception:
                                    pass
                        except Exception:
                            pass

                    if _ref:
                        analysis["snapshot_store_ref"] = analysis.get("snapshot_store_ref") or _ref
                        if isinstance(analysis["results"], dict):
                            analysis["results"]["snapshot_store_ref"] = analysis["results"].get("snapshot_store_ref") or _ref
                            try:
                                if _ssh_v2 and _gs_ref_v2:
                                    analysis["results"]["snapshot_store_ref_v2"] = analysis["results"].get("snapshot_store_ref_v2") or _gs_ref_v2
                            except Exception:
                                pass
                    try:
                        if _gs_ref:
                            analysis["snapshot_store_ref"] = _gs_ref
                            if isinstance(analysis.get("results"), dict):
                                analysis["results"]["snapshot_store_ref"] = _gs_ref
                    except Exception:
                        pass

                    # - Avoid advertising a v2 gsheet ref unless it was actually written successfully.
                    # - Provide a stable ref that always points to a verified store (v2 sheet > v1 sheet > local).
                    # - Emit a compact debug manifest for diagnosing snapshot write failures.
                    try:
                        _stable_ref = (_gs_ref_v2 or _gs_ref or (analysis.get("snapshot_store_ref") if isinstance(analysis, dict) else "") or (_ref if "_ref" in locals() else "") or "")
                        if _stable_ref and isinstance(analysis, dict):
                            analysis["snapshot_store_ref_stable"] = analysis.get("snapshot_store_ref_stable") or _stable_ref
                            if isinstance(analysis.get("results"), dict):
                                analysis["results"]["snapshot_store_ref_stable"] = analysis["results"].get("snapshot_store_ref_stable") or _stable_ref
                    except Exception:
                        pass

                    try:
                        if isinstance(analysis, dict) and isinstance(analysis.get("results"), dict):
                            _dbg = analysis["results"].get("debug")
                            if not isinstance(_dbg, dict):
                                _dbg = {}
                                analysis["results"]["debug"] = _dbg
                            _dbg["snapshot_store_write_v1"] = {
                                "ssh_v1": str(_ssh or ""),
                                "ssh_v2": str(_ssh_v2 or ""),
                                "gs_ref_v1": str(_gs_ref or ""),
                                "gs_ref_v2": str(_gs_ref_v2 or ""),
                                "local_ref_v1": str(_ref or ""),
                                "final_snapshot_store_ref": str((analysis.get("snapshot_store_ref") or "") if isinstance(analysis, dict) else ""),
                                "final_snapshot_store_ref_stable": str((analysis.get("snapshot_store_ref_stable") or "") if isinstance(analysis, dict) else ""),
                            }
                            # - After writing snapshot_store_ref_stable, attempt to load it back
                            #   (sheet or local path) and record basic success/failure stats.
                            # - Best-effort only; never blocks persistence.
                            try:
                                _stable_ref_rt = str(analysis.get("snapshot_store_ref_stable") or "")
                                _rt = {
                                    "stable_ref": _stable_ref_rt,
                                    "origin": "",
                                    "expected_count": 0,
                                    "loaded_count": 0,
                                    "ok": False,
                                    "note": "",
                                }
                                try:
                                    _rt["expected_count"] = int(len(_bsc)) if isinstance(_bsc, list) else 0
                                except Exception:
                                    _rt["expected_count"] = 0

                                _loaded = None
                                try:
                                    if _stable_ref_rt.startswith("gsheet:Snapshots:"):
                                        _h = ""
                                        try:
                                            _h = _stable_ref_rt.split(":", 2)[-1].strip()
                                        except Exception:
                                            _h = ""
                                        if _h:
                                            _loaded = load_full_snapshots_from_sheet(_h, worksheet_title="Snapshots")
                                            _rt["origin"] = "sheet"
                                except Exception:
                                    _loaded = None

                                try:
                                    if _loaded is None and _stable_ref_rt and os.path.exists(_stable_ref_rt):
                                        with open(_stable_ref_rt, "r", encoding="utf-8") as _f:
                                            _loaded = json.load(_f)
                                        _rt["origin"] = "path"
                                except Exception:
                                    _loaded = None

                                try:
                                    _rt["loaded_count"] = int(len(_loaded)) if isinstance(_loaded, list) else 0
                                except Exception:
                                    _rt["loaded_count"] = 0

                                try:
                                    if _rt["loaded_count"] > 0:
                                        if _rt["expected_count"] > 0:
                                            _rt["ok"] = bool(_rt["loaded_count"] == _rt["expected_count"])
                                            if not _rt["ok"]:
                                                _rt["note"] = "count_mismatch"
                                        else:
                                            _rt["ok"] = True
                                    else:
                                        _rt["ok"] = False
                                        _rt["note"] = "empty_or_unreadable"
                                except Exception:
                                    pass

                                _dbg["snapshot_roundtrip_v1"] = _rt
                            except Exception:
                                pass
                    except Exception:
                        pass

        except Exception:
            pass

        payload_for_sheets = _shrink_for_sheets(analysis)
        payload_json = _try_make_sheet_json(payload_for_sheets)

        # - Previous hard truncation produced non-JSON (prefix + random suffix),
        #   causing history loaders (json.loads) to skip the row entirely.
        # - This wrapper guarantees valid JSON even when we must truncate.
        if isinstance(payload_json, str) and len(payload_json) > SHEETS_CELL_LIMIT:
            try:
                payload_json = json.dumps(
                    {
                        "_sheet_write": {
                            "truncated": True,
                            "mode": "hard_truncation_wrapper",
                            "note": "Payload exceeded Google Sheets single-cell limit; stored preview only.",
                        },
                        # keep a preview for debugging/UI; still parseable JSON
                        "preview": payload_json[: max(0, SHEETS_CELL_LIMIT - 600)],
                        "analysis_id": analysis_id,
                        "timestamp": analysis.get("timestamp", _yureeka_now_iso_utc()),
                        "question": (analysis.get("question", "") or "")[:200],
                    },
                    ensure_ascii=False,
                    default=str,
                )
            except Exception:
                pass
                # ultra-safe fallback: still valid JSON
                payload_json = '{"_sheet_write":{"truncated":true,"mode":"hard_truncation_wrapper","note":"json.dumps failed"}}'
        # Why:
        # - Evolution rebuild requires schema/anchors which may be lost in a sheets-safe wrapper
        # - HistoryFull stores the full JSON keyed by analysis_id for later rehydration
        # Behavior:
        # - If payload_json indicates truncation/wrapper OR is very large, write full payload to HistoryFull
        # - Attach a pointer full_store_ref to both analysis and the wrapper object (when possible)
        try:
            is_truncated = False
            try:
                if isinstance(payload_json, str) and ('"_sheet_write"' in payload_json or '"_sheets_safe"' in payload_json):
                    # quick signal; parse if possible
                    try:
                        _pj = json.loads(payload_json)
                        sw = _pj.get("_sheet_write") if isinstance(_pj, dict) else None
                        if isinstance(sw, dict) and sw.get("truncated") is True:
                            is_truncated = True
                        if _pj.get("_sheets_safe") is True:
                            is_truncated = True
                    except Exception:
                        pass
                        # if we can't parse and it's huge, treat as truncated risk
                        if len(payload_json) > 45000:
                            is_truncated = True
                elif isinstance(payload_json, str) and len(payload_json) > 45000:
                    is_truncated = True
            except Exception:
                pass

            if is_truncated:
                full_payload_json = ""
                # FIX2D73: save-side debug counts for baseline canonical metrics persistence
                try:
                    _fix2d73_pmc_count = 0
                    _fix2d73_pmc_src = None
                    if isinstance(analysis, dict):
                        if isinstance(analysis.get("primary_metrics_canonical"), dict) and analysis.get("primary_metrics_canonical"):
                            _fix2d73_pmc_count = len(analysis.get("primary_metrics_canonical") or {})
                            _fix2d73_pmc_src = "analysis.primary_metrics_canonical"
                        elif isinstance(analysis.get("primary_response"), dict) and isinstance(analysis["primary_response"].get("primary_metrics_canonical"), dict) and analysis["primary_response"].get("primary_metrics_canonical"):
                            _fix2d73_pmc_count = len(analysis["primary_response"].get("primary_metrics_canonical") or {})
                            _fix2d73_pmc_src = "analysis.primary_response.primary_metrics_canonical"
                        elif isinstance(analysis.get("results"), dict) and isinstance(analysis["results"].get("primary_metrics_canonical"), dict) and analysis["results"].get("primary_metrics_canonical"):
                            _fix2d73_pmc_count = len(analysis["results"].get("primary_metrics_canonical") or {})
                            _fix2d73_pmc_src = "analysis.results.primary_metrics_canonical"
                    analysis.setdefault("debug", {})
                    if isinstance(analysis.get("debug"), dict):
                        analysis["debug"]["fix2d73_historyfull_save_counts"] = {
                            "primary_metrics_canonical_count": int(_fix2d73_pmc_count),
                            "primary_metrics_canonical_source": str(_fix2d73_pmc_src or ""),
                        }
                except Exception:
                    pass
                try:
                    full_payload_json = json.dumps(analysis, ensure_ascii=False, default=str)
                except Exception:
                    pass
                    full_payload_json = ""

                if full_payload_json:
                    ok_full = write_full_history_payload_to_sheet(analysis_id, full_payload_json, worksheet_title="HistoryFull")
                    if ok_full:
                        ref = f"gsheet:HistoryFull:{analysis_id}"
                        try:
                            analysis["full_store_ref"] = ref
                        except Exception:
                            pass
                        # If payload_json is a wrapper dict, embed ref too
                        try:
                            _pj2 = json.loads(payload_json)
                            if isinstance(_pj2, dict):
                                _pj2["full_store_ref"] = ref
                                sw2 = _pj2.get("_sheet_write")
                                if isinstance(sw2, dict):
                                    sw2["full_store_ref"] = ref
                                    _pj2["_sheet_write"] = sw2
                                payload_json = json.dumps(_pj2, ensure_ascii=False, default=str)
                        except Exception:
                            pass
        except Exception:
            pass

        row = [
            analysis_id,
            analysis.get("timestamp", _yureeka_now_iso_utc()),
            (analysis.get("question", "") or "")[:100],
            str(analysis.get("final_confidence", "")),
            payload_json,
        ]
        sheet.append_row(row, value_input_option="RAW")

        # REFACTOR102: invalidate History get_all_values cache so get_history() sees the newly appended row immediately.
        try:
            _cache = globals().get("_SHEETS_READ_CACHE")
            if isinstance(_cache, dict):
                _ws_title = getattr(sheet, "title", "") or "Sheet1"
                _cache.pop(f"get_all_values:History::{_ws_title}", None)
                # Defensive: drop any other cached History::* reads (worksheet rename / prior cache keys).
                for _k in list(_cache.keys()):
                    if isinstance(_k, str) and _k.startswith("get_all_values:History::"):
                        _cache.pop(_k, None)
        except Exception:
            pass

        # REFACTOR105: mark History as dirty after a successful Sheets append so the next get_history() bypasses cached reads.
        try:
            st.session_state["_history_dirty_v1"] = float(time.time())
            st.session_state["_history_dirty_reason_v1"] = "append_row"
        except Exception:
            pass

        # - This prevents Evolution from being blocked when a Sheets write succeeds/fails intermittently.
        try:
            if "analysis_history" not in st.session_state:
                st.session_state.analysis_history = []
            st.session_state.analysis_history.append(analysis)
            # If Sheets succeeded, clear any prior write-failure forcing.
            st.session_state.pop("fix2d66_force_session_history", None)
        except Exception:
            pass

        try:
            st.session_state["last_analysis"] = analysis
        except Exception:
            pass

        return True

    except Exception as e:
        st.warning(f"⚠️ Failed to save to Google Sheets: {e}")
        try:
            globals()["_SHEETS_LAST_WRITE_ERROR"] = str(e)
        except Exception:
            pass
        try:
            st.session_state["fix2d66_force_session_history"] = True
        except Exception:
            pass
        if "analysis_history" not in st.session_state:
            st.session_state.analysis_history = []
        st.session_state.analysis_history.append(analysis)
        try:
            st.session_state["last_analysis"] = analysis
        except Exception:
            pass

        return False

def normalize_unit_tag(unit_str: str) -> str:
    """
    Canonical unit tags used for drift=0 comparisons.
    """
    u = (unit_str or "").strip()
    if not u:
        return ""
    ul = u.lower().replace(" ", "")

    # energy units
    if ul == "twh":
        return "TWh"
    if ul == "gwh":
        return "GWh"
    if ul == "mwh":
        return "MWh"
    if ul == "kwh":
        return "kWh"
    if ul == "wh":
        return "Wh"

    # magnitudes
    if ul in ("t", "trillion", "tn"):
        return "T"
    if ul in ("b", "bn", "billion"):
        return "B"
    if ul in ("m", "mn", "mio", "million"):
        return "M"
    if ul in ("k", "thousand", "000"):
        return "K"

    # composite phrases (e.g. "million units")
    # Some extractors pass unit strings like "million units" as a single tag.
    # Normalize these into the same magnitude tags used elsewhere (M/B/T/K) so
    # unit_family can be typed deterministically.
    if ("unit" in ul) or ("units" in ul):
        if ("trillion" in ul) or ul.startswith("tn") or ul.startswith("t") and ul.endswith("units"):
            return "T"
        if ("billion" in ul) or ul.startswith("bn"):
            return "B"
        if ("million" in ul) or ul.startswith("mn") or ul.startswith("mio"):
            return "M"
        if ("thousand" in ul) or ul.startswith("k"):
            return "K"

    # percent
    if ul in ("%", "pct", "percent"):
        return "%"

    return u

def unit_family(unit_tag: str) -> str:
    """
    Unit family classifier for gating.
    """
    ut = (unit_tag or "").strip()

    if ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
        return "energy"
    if ut == "%":
        return "percent"
    if ut in ("T", "B", "M", "K"):
        return "magnitude"

    return ""

# - Many extracted candidates arrive with unit_family='' due to legacy drift.
# - Provide a stable, analysis/evolution-shared unit_family normalizer.
# - Currency requires context evidence; caller may pass ctx/raw for upgrade.

# - Some sources yield numbers without an attached unit token (unit_tag="").
# - We conservatively infer unit_tag/unit_family from nearby context text.
# - This does NOT weaken FIX2D24 year-blocking; it only restores missing unit metadata.
# REFACTOR20 (BUGFIX): boundary-aware currency evidence detector
# - Prevent false positives like 'eur'/'euro' inside 'Europe' from upgrading unit_family to currency.
# - Treat currency codes/words as tokens (word-boundary), while allowing symbol markers ($, €, £, ¥).
def _yureeka_has_currency_evidence_v1(text: str) -> bool:
    try:
        t = (text or "").lower()
    except Exception:
        t = str(text or "")
        t = t.lower()

    # strong symbol markers
    if any(sym in t for sym in ("$", "€", "£", "¥")):
        return True

    # common composite tokens
    if "us$" in t or "s$" in t:
        return True

    # currency codes as tokens (avoid substrings inside other words)
    try:
        if re.search(r"\b(usd|sgd|eur|gbp|jpy|cny|rmb|aud|cad|inr|krw|chf|hkd|nzd)\b", t):
            return True
        if re.search(r"\b(dollar|dollars|euro|euros|pound|pounds|yen|yuan|rupee|rupees)\b", t):
            return True
    except Exception:
        pass

    return False

def infer_unit_tag_from_context(ctx: str, raw: str = ""):
    """Return (unit_tag, unit_family, matched_phrase, excerpt)."""
    c = (ctx or "")
    r = (raw or "")
    cl = (c + " " + r).lower()

    # percent signals
    if "%" in cl or "percent" in cl or "pct" in cl or "market share" in cl or "share" in cl:
        return "%", "percent", "percent/market_share", (c[:160] if c else r[:160])

    # currency signals
    if _yureeka_has_currency_evidence_v1(cl):
        return "USD", "currency", "currency_marker", (c[:160] if c else r[:160])
    if any(k in cl for k in ["revenue", "market size", "market value", "valuation", "valued", "worth", "price"]):
        # keyword-only currency is weaker; require a magnitude word to reduce false positives
        if any(w in cl for w in ["billion", "bn", "million", "mn", "trillion", "tn"]):
            return "USD", "currency", "currency_keyword", (c[:160] if c else r[:160])

    # magnitude / unit-sales style signals
    # detect explicit magnitude words, and also "units".
    if "million" in cl or " mn" in cl or "mio" in cl:
        if "unit" in cl or "vehicle" in cl or "sales" in cl:
            return "M", "magnitude", "million_units", (c[:160] if c else r[:160])
        return "M", "magnitude", "million", (c[:160] if c else r[:160])
    if "billion" in cl or " bn" in cl:
        return "B", "magnitude", "billion", (c[:160] if c else r[:160])
    if "trillion" in cl or " tn" in cl:
        return "T", "magnitude", "trillion", (c[:160] if c else r[:160])
    if "thousand" in cl or "k " in cl:
        return "K", "magnitude", "thousand", (c[:160] if c else r[:160])

    return "", "", "", (c[:160] if c else r[:160])
def normalize_unit_family(unit_tag: str, ctx: str = "", raw: str = "") -> str:
    """
    Deterministic unit-family normalization.

    Goals (FIX2D30):
    - Keep 'M'/'million' candidates in "million units / units sold / chargers" contexts as *magnitude* (or unit-count/sales downstream),
      preventing false 'currency' upgrades that block baseline comparability.
    - Only label a candidate as 'currency' when explicit currency evidence exists (symbols/codes/words), not just generic keywords like "market".

    Notes:
    - This helper is intentionally conservative.
    - Hard unit-family rejection (Diff Panel inference) remains the enforcement point; this just fixes upstream family typing.
    """
    ut = (unit_tag or "").strip()
    fam = unit_family(ut)

    # REFACTOR22: Do not infer unit family from surrounding context for plain yearlike tokens
    # when unit_tag is missing. Year/range endpoints (e.g., '2026–2040') commonly sit next to
    # '%' or currency symbols and can be mis-typed as percent/currency, creating noisy
    # unit inconsistencies in baseline_sources_cache. This is behavior-preserving for binding,
    # since yearlike tokens are not legitimate metric values for __percent/__currency keys.
    if fam == "" and ut == "":
        try:
            _rs = (raw or "").strip()
            if re.fullmatch(r"(19|20)\d{2}", _rs or ""):
                # Only allow inference if the raw token itself contains explicit unit evidence.
                if not re.search(r"[%$€£¥]", _rs):
                    return ""
        except Exception:
            pass

    if fam == "" and ut == "":
        try:
            _itag, ifam, _phr, _ex = infer_unit_tag_from_context(ctx or "", raw or "")
        except Exception:
            pass
            _itag, ifam, _phr, _ex = "", "", "", ""
        if ifam:
            return ifam

    if fam == "magnitude":
        c = ((ctx or "") + " " + (raw or "")).lower()

        # Strong unit-count / unit-sales evidence: keep as magnitude.
        # This blocks the legacy false-positive path where 'M' + 'market' upgraded to currency even when the phrase is "million units".
        unit_evidence = [
            "million units",
            "units sold",
            "unit sales",
            "vehicles sold",
            "ev sales",
            "sales ytd",
            "ytd",
            "chargers",
            "charger",
            "charging points",
            "charging stations",
            "stations",
            "units",
        ]
        has_unit_evidence = any(u in c for u in unit_evidence)

        # Explicit currency markers only (symbols/codes/words)
        # REFACTOR20: boundary-aware currency detection (avoid "Europe" → "euro" false positives)
        has_currency_markers = _yureeka_has_currency_evidence_v1(c)

        if has_unit_evidence and not has_currency_markers:
            return "magnitude"

        if has_currency_markers:
            return "currency"

        # FIX2D30: Remove keyword-only currency upgrades (e.g., 'market', 'valuation') because they cause false positives
        # for phrases like "million units" that also mention "market".

    return fam
def infer_currency_code_from_text_v1(text: str) -> str:
    """
    Best-effort, deterministic currency code inference from raw/context strings.
    Returns an ISO-ish code (e.g., USD/EUR/GBP/JPY/SGD/AUD/CAD/HKD/CNY/KRW/INR) or "".
    """
    try:
        s = (text or "").strip().lower()
        if not s:
            return ""
        # Explicit codes first
        for code in ("usd","eur","gbp","jpy","cny","rmb","aud","cad","sgd","hkd","krw","inr","chf","sek","nok","dkk","nzd"):
            if re.search(r"\b" + re.escape(code) + r"\b", s):
                return "CNY" if code == "rmb" else code.upper()
        # Prefixed symbols
        if "us$" in s or "u.s.$" in s:
            return "USD"
        if "s$" in s:
            return "SGD"
        if "a$" in s:
            return "AUD"
        if "c$" in s:
            return "CAD"
        if "hk$" in s:
            return "HKD"
        # Unicode currency symbols
        if "€" in s:
            return "EUR"
        if "£" in s:
            return "GBP"
        if "¥" in s:
            return "JPY"
        if "₹" in s:
            return "INR"
        if "₩" in s:
            return "KRW"
        # Plain "$" is ambiguous; assume USD only if US markers exist; otherwise leave blank.
        if "$" in s and ("united states" in s or "u.s." in s or " us " in s or " usa" in s):
            return "USD"
    except Exception:
        pass
    return ""

def canonicalize_numeric_candidate(candidate: dict) -> dict:

    """

    Additive: attach canonical numeric fields to a candidate dict.

    Safe to call multiple times.

    PATCH AI4 (ADDITIVE): anchor integrity

    - Ensures anchor_hash + candidate_id are present when possible (derived if missing).

    - Does not change extraction behavior; only enriches fields.

    """
    if not isinstance(candidate, dict):

        return {}

    v_raw = candidate.get("value_norm")

    v = None

    if v_raw is not None:

        try:

            v = float(v_raw)

        except Exception:
            pass

            v = None

    if v is None:

        try:

            v0 = candidate.get("value")

            if v0 is None:

                return candidate

            v = float(v0)

        except Exception:
            return candidate

    try:

        ut = normalize_unit_tag(candidate.get("unit_tag") or candidate.get("unit") or "")

    except Exception:
        pass

        ut = str(candidate.get("unit_tag") or candidate.get("unit") or "").strip()

    # - Some sources yield numbers without an attached unit token (unit_tag="").
    # - Infer unit_tag/unit_family from nearby context_snippet/raw without weakening FIX2D24.
    # - Attach per-candidate trace: context_unit_backfill_v1.
    ctx_s = (candidate.get("context") or candidate.get("context_snippet") or "")
    raw_s = (candidate.get("raw") or candidate.get("display_value") or "")
    context_unit_backfill_v1 = {"applied": False}
    if (ut or "").strip() == "":
        # Guard: do not infer %/currency units from surrounding context for plain year tokens.
        _skip_backfill_yearlike = False
        try:
            _vi = int(float(v)) if v is not None else None
            if _vi is not None and float(_vi) == float(v) and 1900 <= _vi <= 2100:
                _rs = (raw_s or "").strip().lower()
                if not re.search(r"[%$€£¥]|\b(us\$|s\$|usd|sgd|eur|gbp|jpy|aud|cad|chf)\b", _rs):
                    _skip_backfill_yearlike = True
        except Exception:
            _skip_backfill_yearlike = False

        if _skip_backfill_yearlike:
            context_unit_backfill_v1 = {"applied": False, "skipped": True, "reason": "yearlike_no_backfill"}
        else:
            itag, ifam, phr, ex = ("", "", "", "")
            try:
                itag, ifam, phr, ex = infer_unit_tag_from_context(ctx_s, raw_s)
            except Exception:
                pass
                itag, ifam, phr, ex = ("", "", "", "")
            if itag or ifam:
                if itag:
                    ut = itag
                    candidate["unit_tag"] = itag
                context_unit_backfill_v1 = {
                    "applied": True,
                    "matched_phrase": phr,
                    "inferred_unit_family": ifam,
                    "inferred_unit_tag": itag,
                    "window_excerpt": ex,
                }
    try:
        candidate["context_unit_backfill_v1"] = context_unit_backfill_v1
    except Exception:
        pass

    # - Backfill unit_family using unit_tag and currency evidence in context
    # - Correct measure_kind/measure_assoc for currency-like candidates
    # - Attach small per-candidate trace fields for audit
    ctx_s = (candidate.get("context") or candidate.get("context_snippet") or "")
    raw_s = (candidate.get("raw") or candidate.get("display_value") or "")
    existing_fam = (candidate.get("unit_family") or "")

    try:
        fam = normalize_unit_family(ut, ctx=ctx_s, raw=raw_s)
    except Exception:
        pass
        fam = ""

    unit_family_backfilled = False
    if (not existing_fam) and fam:
        candidate["unit_family"] = fam
        unit_family_backfilled = True
    elif fam:
        # keep existing if set, but normalize obvious empties/whitespace
        if str(existing_fam).strip() == "":
            candidate["unit_family"] = fam
            unit_family_backfilled = True

    # REFACTOR24: infer/carry currency_code for currency candidates (used later for unit comparability)
    try:
        if fam == "currency" and not str(candidate.get("currency_code") or "").strip():
            _cc = infer_currency_code_from_text_v1((raw_s or "") + " " + (ctx_s or ""))
            if _cc:
                candidate["currency_code"] = _cc
    except Exception:
        pass

    # currency kind correction (only when evidence exists)
    measure_kind_corrected = False
    measure_assoc_corrected = False
    classifier_reason = ""
    if fam == "currency":
        classifier_reason = "currency_evidence"
        mk0 = str(candidate.get("measure_kind") or "").strip()
        if mk0 in ("", "count_units"):
            candidate["measure_kind"] = "currency"
            measure_kind_corrected = True
        ma0 = str(candidate.get("measure_assoc") or "").strip()
        cxl = (ctx_s or "").lower()
        assoc = None
        if "revenue" in cxl:
            assoc = "revenue"
        elif "market" in cxl and any(k in cxl for k in ["size", "value", "valuation"]):
            assoc = "market_value"
        elif "price" in cxl:
            assoc = "price"
        if assoc and (ma0 in ("", "units")):
            candidate["measure_assoc"] = assoc
            measure_assoc_corrected = True
    elif fam == "percent":
        classifier_reason = "percent_tag"
        mk0 = str(candidate.get("measure_kind") or "").strip()
        if mk0 == "":
            candidate["measure_kind"] = "percent"
            measure_kind_corrected = True

    # attach trace (compact)
    try:
        candidate["unit_measure_classifier_trace_v1"] = {
            "unit_tag": ut,
            "unit_family": candidate.get("unit_family") or fam,
            "context_unit_backfill_applied": bool((candidate.get("context_unit_backfill_v1") or {}).get("applied")),
            "unit_family_backfilled": bool(unit_family_backfilled),
            "measure_kind": candidate.get("measure_kind"),
            "measure_kind_corrected": bool(measure_kind_corrected),
            "measure_assoc": candidate.get("measure_assoc"),
            "measure_assoc_corrected": bool(measure_assoc_corrected),
            "reason": classifier_reason or ("from_unit_tag" if fam else "unknown"),
        }
    except Exception:
        pass

    # If candidate already has base_unit/multiplier_to_base, respect them

    base_unit = candidate.get("base_unit")

    mult = candidate.get("multiplier_to_base")

    try:

        mult = float(mult) if mult is not None else None

    except Exception:
        pass

        mult = None

    # Minimal deterministic mapping (extend as needed)

    if (not base_unit) or (mult is None):

        base_unit = ""

        mult = 1.0

        # percents

        if ut in ("%", "pct"):

            base_unit, mult = "%", 1.0

        # energy

        elif ut == "MWh":

            base_unit, mult = "Wh", 1e6

        elif ut == "kWh":

            base_unit, mult = "Wh", 1e3

        elif ut == "Wh":

            base_unit, mult = "Wh", 1.0

        # power

        elif ut == "GW":

            base_unit, mult = "W", 1e9

        elif ut == "MW":

            base_unit, mult = "W", 1e6

        elif ut == "kW":

            base_unit, mult = "W", 1e3

        elif ut == "W":

            base_unit, mult = "W", 1.0

        # mass

        elif ut in ("Mt", "million_tonnes", "million_tons"):

            base_unit, mult = "t", 1e6

        elif ut in ("kt", "kilo_tonnes", "kilo_tons"):

            base_unit, mult = "t", 1e3

        elif ut in ("t", "tonne", "tonnes", "ton", "tons"):

            base_unit, mult = "t", 1.0

        # count-ish

        elif ut in ("vehicles", "units", "count"):

            base_unit, mult = ut, 1.0

        else:

            # unknown unit: treat as-is

            base_unit, mult = (ut or str(candidate.get("unit") or "").strip()), 1.0

    # Only set defaults to avoid overriding existing enriched fields

    candidate.setdefault("unit_tag", ut)

    candidate.setdefault("unit_family", fam)

    candidate.setdefault("base_unit", base_unit)

    candidate.setdefault("multiplier_to_base", mult)

    # value_norm: if already present, do not overwrite

    if candidate.get("value_norm") is None:

        try:

            candidate["value_norm"] = float(v) * float(mult)

        except Exception:

            pass

    ah = candidate.get("anchor_hash") or candidate.get("anchor")

    if not ah:

        # attempt deterministic derive if fields exist

        src = candidate.get("source_url") or candidate.get("url") or ""

        ctx = candidate.get("context_snippet") or candidate.get("context") or ""

        if isinstance(ctx, str):

            ctx = ctx.strip()[:240]

        else:

            ctx = ""

        raw = candidate.get("raw")

        if raw is None:

            raw = f"{candidate.get('value')}{candidate.get('unit') or ''}"

        ah = _yureeka_sha1_v1(f"{src}|{str(raw)[:120]}|{ctx}") if (src or ctx) else ""

        if ah:

            candidate["anchor_hash"] = ah

    if not candidate.get("candidate_id") and ah:

        candidate["candidate_id"] = str(ah)[:16]

    return candidate

# Goal:
#   - Provide a deterministic, evolution-safe metric rebuild that uses ONLY:
#       (a) baseline_sources_cache snapshots (and their extracted_numbers)
#       (b) frozen metric schema (metric_schema_frozen)
#   - No re-fetch, no LLM inference, no heuristic "best guess" beyond schema fields.
#
# Contract:
#   - Returns a dict shaped like primary_metrics_canonical:
#       { canonical_key: { ...metric fields... } }
#   - Deterministic tie-break ordering.

#   - Enforce that ANY candidate flagged as junk is excluded from:
#       * candidate indexing
#       * candidate scoring
#       * final metric assignment
#   - Additionally, suppress "year-like" unitless tokens (e.g., 2024/2025) for
#     non-year metrics (currency/percent/rate/ratio/growth/etc.) to prevent
#     year fixation during evolution.
#   - Purely deterministic: no LLM, no refetch, no heuristics outside schema cues.

def _candidate_disallowed_for_metric(_cand: dict, _spec: dict = None) -> bool:
    """Return True if a snapshot candidate must not be used to assign a metric value."""
    if not isinstance(_cand, dict):
        return True

    # 1) Hard exclusion: explicit junk flags / reasons from extraction phase
    if _cand.get("is_junk") is True:
        return True
    jr = str(_cand.get("junk_reason") or "").strip().lower()
    if jr:
        # If a junk_reason exists, treat it as non-selectable deterministically.
        return True

    # 2) Deterministic anti-year-fixation: unitless year-like tokens are disallowed
    #    for most numeric metrics (unless schema clearly indicates a "year" metric).
    try:
        v = _cand.get("value_norm", _cand.get("value"))
        unitish = str(_cand.get("base_unit") or _cand.get("unit_tag") or _cand.get("unit") or "").strip()
        if unitish == "" and isinstance(v, (int, float)):
            if abs(float(v) - round(float(v))) < 1e-9:
                vi = int(round(float(v)))
                if 1900 <= vi <= 2100:
                    if isinstance(_spec, dict):
                        nm = str(_spec.get("name") or "").lower()
                        cid = str(_spec.get("canonical_id") or _spec.get("canonical_key") or "").lower()
                        kws = _spec.get("keywords") or []
                        kws_s = " ".join([str(k).lower() for k in kws]) if isinstance(kws, list) else str(kws).lower()

                        # Allow explicit year metrics
                        if ("year" in nm) or ("year" in cid) or ("founded" in nm) or ("since" in nm) or ("year" in kws_s):
                            return False

                        uf = str(_spec.get("unit_family") or "").lower().strip()
                        ut = str(_spec.get("unit_tag") or _spec.get("unit") or "").lower().strip()

                        # For common non-year metric families, exclude year-like tokens.
                        if uf in ("currency", "percent", "rate", "ratio", "growth", "share"):
                            return True
                        if "%" in ut:
                            return True
                        if any(w in nm for w in ("cagr", "revenue", "growth", "market", "sales", "profit", "margin", "volume")):
                            return True

                    # Default: unitless year-like token is not a valid metric value.
                    return True
    except Exception:
        return False

# Unit-family + scale eligibility guardrails for schema-only rebuild
# and unit-mismatch detection for Diff Panel V2.
#
# Motivation (from REFACTOR02 JSONs):
# - A currency token like "US$ 996.3bn" was being selected for a magnitude/count schema key
#   (e.g., global_ev_chargers_2040__unit_count), causing nonsensical diffs (B vs M).
# - We fix this *at selection time* and also *at diff time* (so any future regressions are
#   surfaced as unit_mismatch rather than as a bogus increased/decreased classification).
# Determinism:
# - Pure filtering + stable logic; no refetch; no randomness.

def _fix17_candidate_allowed_with_reason(candidate: dict, metric_spec: dict = None, canonical_key: str = None) -> tuple:
    """
    FIX17 helper used by rebuild_metrics_from_snapshots_analysis_canonical_v1.

    Returns (allowed: bool, reason: str).
    Must remain deterministic and lightweight.

    Why:
      - REFACTOR121 shows fix41afc19 rebuild fails with:
        NameError: _fix17_candidate_allowed_with_reason not defined
      - This blocks post-seed canonical rebuild and keeps prod all-null.
    """
    try:
        if not isinstance(candidate, dict):
            return False, "not_dict"

        # Hard junk flags
        if candidate.get("is_junk") is True:
            jr = str(candidate.get("junk_reason") or "junk").strip()
            return False, ("junk:" + jr)[:120]

        jr = str(candidate.get("junk_reason") or "").strip()
        if jr:
            return False, ("junk_reason:" + jr)[:120]

        # Generic candidate disallow rules (years-as-values, etc.)
        try:
            if callable(globals().get("_candidate_disallowed_for_metric")) and isinstance(metric_spec, dict):
                if _candidate_disallowed_for_metric(candidate, metric_spec):
                    return False, "candidate_disallowed_for_metric"
        except Exception:
            pass

        # Unit-family eligibility (currency/percent poisoning guard)
        try:
            if callable(globals().get("_refactor03_candidate_rejected_by_unit_family_v1")) and isinstance(metric_spec, dict):
                if _refactor03_candidate_rejected_by_unit_family_v1(candidate, metric_spec):
                    return False, "unit_family_gate"
        except Exception:
            pass

        # Currency date-fragment filter (day-of-month tokens)
        try:
            if callable(globals().get("_refactor27_candidate_rejected_currency_date_fragment_v1")) and isinstance(metric_spec, dict):
                if _refactor27_candidate_rejected_currency_date_fragment_v1(candidate, metric_spec):
                    return False, "currency_date_fragment"
        except Exception:
            pass

        return True, "ok"

    except Exception as e:
        return False, ("exception:" + str(type(e).__name__))[:120]

def _refactor03_has_currency_evidence_v1(text: str) -> bool:
    try:
        t = (text or "").lower()
        if not t:
            return False
        # Common currency markers; keep conservative to avoid false positives.
        markers = ["us$", "usd", "eur", "€", "gbp", "£", "sgd", "s$", "aud", "cad", "jpy", "¥", "$"]
        return any(m in t for m in markers)
    except Exception:
        return False

def _refactor03_has_percent_evidence_v1(text: str) -> bool:
    try:
        t = (text or "").lower()
        if not t:
            return False
        return ("%" in t) or ("percent" in t) or ("pct" in t)
    except Exception:
        return False

def _refactor04_scale_multiplier_from_unit_tag_v1(unit_tag: str) -> float:
    """Convert common magnitude tags to a scale multiplier (K/M/B/T)."""
    try:
        t = str(unit_tag or "").upper().strip()
        if t == "K":
            return 1e3
        if t == "M":
            return 1e6
        if t == "B":
            return 1e9
        if t == "T":
            return 1e12
    except Exception:
        pass
    return 1.0

def _refactor04_get_metric_schema_frozen_v1(obj: dict) -> dict:
    """Best-effort retrieval of metric_schema_frozen from common nesting patterns."""
    try:
        if not isinstance(obj, dict):
            return {}
        if isinstance(obj.get("metric_schema_frozen"), dict):
            return obj.get("metric_schema_frozen") or {}
        pr = obj.get("primary_response")
        if isinstance(pr, dict) and isinstance(pr.get("metric_schema_frozen"), dict):
            return pr.get("metric_schema_frozen") or {}
        res = obj.get("results")
        if isinstance(res, dict):
            if isinstance(res.get("metric_schema_frozen"), dict):
                return res.get("metric_schema_frozen") or {}
            pr2 = res.get("primary_response")
            if isinstance(pr2, dict) and isinstance(pr2.get("metric_schema_frozen"), dict):
                return pr2.get("metric_schema_frozen") or {}
        return {}
    except Exception:
        return {}

def _refactor04_enrich_pmc_units_v1(pmc: dict, prev_response: dict = None) -> dict:
    """Ensure PMC rows carry unit_tag/unit_family/multiplier_to_base (and base_unit) for parity + diffing."""
    try:
        if not isinstance(pmc, dict) or not pmc:
            return pmc
        schema = _refactor04_get_metric_schema_frozen_v1(prev_response) if isinstance(prev_response, dict) else {}
        for ckey, m in list(pmc.items()):
            if not isinstance(m, dict):
                continue
            spec = schema.get(ckey) if isinstance(schema, dict) else None
            spec = spec if isinstance(spec, dict) else {}
            # unit_tag
            ut = m.get("unit_tag") or m.get("base_unit") or m.get("unit") or spec.get("unit_tag") or spec.get("unit") or ""
            ut = str(ut or "").strip()
            # unit_family
            uf = m.get("unit_family") or spec.get("unit_family") or _refactor03_unit_family_from_ckey_v1(ckey)
            uf = str(uf or "").strip()
            # multiplier_to_base (scale)
            mult = m.get("multiplier_to_base")
            if mult is None:
                mult = _refactor04_scale_multiplier_from_unit_tag_v1(ut)
            try:
                mult = float(mult)
            except Exception:
                mult = _refactor04_scale_multiplier_from_unit_tag_v1(ut)

            # write back (do not delete existing fields)
            if ut and not m.get("unit"):
                m["unit"] = ut
            if ut and not m.get("unit_tag"):
                m["unit_tag"] = ut
            if ut and not m.get("base_unit"):
                m["base_unit"] = ut
            if uf and not m.get("unit_family"):
                m["unit_family"] = uf
            if m.get("multiplier_to_base") is None and mult is not None:
                m["multiplier_to_base"] = mult
    except Exception:
        return pmc
    return pmc

def _refactor03_unit_family_from_ckey_v1(canonical_key: str) -> str:
    try:
        ck = str(canonical_key or "").lower()
        if "__currency" in ck:
            return "currency"
        if "__percent" in ck:
            return "percent"
        # treat all "__unit_*" as magnitude/count family
        if "__unit_" in ck:
            return "magnitude"
        return "unknown"
    except Exception:
        return "unknown"

def _refactor03_candidate_rejected_by_unit_family_v1(cand: dict, spec: dict = None) -> bool:
    """Return True if candidate is incompatible with schema's unit family / tag."""
    if not isinstance(cand, dict):
        return True
    if not isinstance(spec, dict):
        return False  # no schema => don't over-filter

    try:
        uf = str(spec.get("unit_family") or "").lower().strip()
        ut = str(spec.get("unit_tag") or spec.get("unit") or "").strip()
        unit_tag = str(cand.get("unit_tag") or cand.get("unit") or cand.get("base_unit") or "").strip()

        raw_core = str(cand.get("raw") or "")

        raw_ctx = " ".join([

            raw_core,

            str(cand.get("context_snippet") or ""),

            str(cand.get("context") or ""),

        ])

        # For magnitude/count metrics, avoid broad context leakage (%/currency nearby).

        raw_for = raw_ctx

        try:

            if uf not in ("currency", "money", "percent", "rate", "ratio", "growth", "share"):

                raw_for = raw_core.strip() or raw_ctx

        except Exception:

            raw_for = raw_core.strip() or raw_ctx

        is_cur = _refactor03_has_currency_evidence_v1(raw_for)

        is_pct = _refactor03_has_percent_evidence_v1(raw_for)

        # 1) unit-family gating
        if uf in ("currency", "money"):
            if not is_cur:
                return True
        elif uf in ("percent", "rate", "ratio", "growth", "share"):
            if not is_pct:
                return True
        else:
            # magnitude/count: reject obvious currency/percent
            if is_cur or is_pct:
                return True

        # 2) unit-tag scale gating for magnitude/count metrics (K/M/B)
        try:
            ut_up = ut.upper().strip()
            unit_up = unit_tag.upper().strip()
            if ut_up in ("K", "M", "B", "T") and unit_up in ("K", "M", "B", "T") and ut_up != unit_up:
                return True
        except Exception:
            pass

        return False
    except Exception:
        return False

def _refactor27_candidate_rejected_currency_date_fragment_v1(cand: dict, spec: dict = None) -> bool:
    """Reject date-fragment candidates like '01' in contexts such as 'July 01, 2025' for currency-ish metrics.

    Rationale:
      - Some news pages include datelines (e.g., 'July 01, 2025') near genuine currency values.
      - Weak context-based currency evidence can cause day-of-month tokens to outscore real values.
    Determinism:
      - Pure filter; does not invent candidates or refetch content.
    """
    try:
        if not isinstance(cand, dict) or not isinstance(spec, dict):
            return False
        uf = str(spec.get("unit_family") or "").lower().strip()
        if uf not in ("currency", "money"):
            return False

        raw = str(cand.get("raw") or "").strip()
        if not raw:
            return False

        # Only target tiny integer tokens that look like day-of-month (01..31)
        try:
            v = cand.get("value_norm")
            if v is None:
                v = cand.get("value")
            iv = int(float(v))
        except Exception:
            return False

        if iv < 1 or iv > 31:
            return False

        if not re.fullmatch(r"0?\d{1,2}", raw):
            return False

        ctx = " ".join([
            str(cand.get("context_snippet") or ""),
            str(cand.get("context") or ""),
        ]).lower()

        if not ctx:
            return False

        # Month + year pattern indicates this is very likely a dateline token
        months = (
            "jan", "january", "feb", "february", "mar", "march", "apr", "april",
            "may", "jun", "june", "jul", "july", "aug", "august", "sep", "sept", "september",
            "oct", "october", "nov", "november", "dec", "december",
        )
        if any(m in ctx for m in months) and re.search(r"\b(19|20)\d{2}\b", ctx):
            # If the raw itself directly carries currency markers, do not reject
            raw_l = raw.lower()
            if any(sym in raw_l for sym in ("$", "usd", "eur", "gbp", "sgd", "aud", "cad", "hk$", "us$")):
                return False
            return True

        return False
    except Exception:
        return False

def get_history(limit: int = MAX_HISTORY_ITEMS) -> List[Dict]:
    """Load analysis history from Google Sheet"""
    sheet = get_google_sheet()
    # This prevents Evolution from being blocked by transient Sheets failures.
    try:
        if st.session_state.get("fix2d66_force_session_history"):
            return st.session_state.get('analysis_history', [])
    except Exception:
        pass
    if not sheet:
        # Fallback to session state
        return st.session_state.get('analysis_history', [])

    try:
        # Why:
        # - Your sheet names are: 'Sheet1', 'Snapshots', 'HistoryFull'
        # - There is no worksheet called 'History'
        # - Using cache_key='History' can cache empty reads under the wrong key.
        _ws_title = getattr(sheet, "title", "") or "Sheet1"
        _cache_key = f"History::{_ws_title}"

        # Get all rows (skip header)
        values = []
        _r105_used_direct = False

        # REFACTOR105: If we just wrote to Sheets, bypass cached reads once to avoid stale History reads.
        try:
            _dirty = st.session_state.get("_history_dirty_v1")
        except Exception:
            _dirty = None

        try:
            if isinstance(_dirty, (int, float)) and (time.time() - float(_dirty) < 120):
                direct = sheet.get_all_values()
                if direct and len(direct) >= 2:
                    values = direct
                    _r105_used_direct = True
                    try:
                        _cache = globals().get("_SHEETS_READ_CACHE")
                        if isinstance(_cache, dict):
                            _cache.pop(f"get_all_values:{_cache_key}", None)
                            # Defensive: drop any cached History::* reads.
                            for _k in list(_cache.keys()):
                                if isinstance(_k, str) and _k.startswith("get_all_values:History::"):
                                    _cache.pop(_k, None)
                    except Exception:
                        pass
                    try:
                        st.session_state.pop("_history_dirty_v1", None)
                        st.session_state.pop("_history_dirty_reason_v1", None)
                    except Exception:
                        pass
        except Exception:
            pass

        if not _r105_used_direct:
            try:
                values = sheets_get_all_values_cached(sheet, cache_key=_cache_key)
            except Exception:
                pass
                values = []

        # Why:
        # - If a prior transient read/429 produced an empty cached value,
        #   evolution may temporarily see no history even though rows exist.
        if not values or len(values) < 2:
            try:
                direct = sheet.get_all_values()
                if direct and len(direct) >= 2:
                    values = direct
            except Exception:
                pass

        all_rows = values[1:] if values and len(values) >= 2 else []

        try:
            if (not all_rows) and st.session_state.get('analysis_history'):
                return st.session_state.get('analysis_history', [])
        except Exception:
            pass

        try:
            if (not all_rows) and globals().get("_SHEETS_LAST_READ_ERROR"):
                if ("RESOURCE_EXHAUSTED" in str(_SHEETS_LAST_READ_ERROR)
                    or "Quota exceeded" in str(_SHEETS_LAST_READ_ERROR)
                    or "429" in str(_SHEETS_LAST_READ_ERROR)):
                    return st.session_state.get('analysis_history', [])
        except Exception:
            pass

        # Parse and return most recent
        history = []
        for row in all_rows[-limit:]:
            if len(row) >= 5:
                raw_cell = row[4]
                try:
                    data = json.loads(raw_cell)
                    data['_sheet_id'] = row[0]  # Keep track of sheet row ID

                    # (your existing GH2 / ES1G / GH1 / GH3 logic unchanged)
                    # ...
                    history.append(data)

                except json.JSONDecodeError:
                    # (your existing GH1 rescue logic unchanged)
                    continue

        # REFACTOR108: Ensure history is newest-first (Sheet rows are chronological).
        try:
            history = list(reversed(history))
        except Exception:
            pass

# REFACTOR105: Merge in-session history (analysis_history + last_analysis) to avoid stale Sheets reads.
        try:
            _sess_hist = st.session_state.get("analysis_history") or []
        except Exception:
            _sess_hist = []
        try:
            _sess_last = st.session_state.get("last_analysis")
        except Exception:
            _sess_last = None

        def _r105_hist_key(_h):
            try:
                if not isinstance(_h, dict):
                    return None
                _t = _h.get("timestamp") or (_h.get("results") or {}).get("timestamp") or ""
                _q = (_h.get("question") or (_h.get("results") or {}).get("question") or "").strip()
                if not _t and not _q:
                    return None
                return (_t, _q)
            except Exception:
                return None

        try:
            _merged = []
            _seen = set()

            for _h in (history or []):
                _k = _r105_hist_key(_h)
                if _k and _k not in _seen:
                    _seen.add(_k)
                    _merged.append(_h)

            if isinstance(_sess_hist, list):
                for _h in _sess_hist:
                    _k = _r105_hist_key(_h)
                    if _k and _k not in _seen:
                        _seen.add(_k)
                        _merged.append(_h)

            if isinstance(_sess_last, dict):
                _k = _r105_hist_key(_sess_last)
                if _k and _k not in _seen:
                    _seen.add(_k)
                    _merged.append(_sess_last)

            try:
                def _ts(_h):
                    try:
                        _t = (
                            (_h.get("timestamp") if isinstance(_h, dict) else "")
                            or ((_h.get("results") or {}).get("timestamp") if isinstance(_h.get("results"), dict) else "")
                            or ((_h.get("primary_response") or {}).get("timestamp") if isinstance(_h.get("primary_response"), dict) else "")
                            or ""
                        )
                        _dt = _parse_iso_dt(_t) if _t else None
                        return _dt.timestamp() if _dt else 0.0
                    except Exception:
                        return 0.0
                _merged.sort(key=_ts, reverse=True)
                if isinstance(limit, int) and limit > 0 and len(_merged) > limit:
                    _merged = _merged[:limit]
            except Exception:
                pass

            history = _merged
        except Exception:
            pass

        # (your existing GH3 sort unchanged)
        return history

    except Exception as e:
        st.warning(f"⚠️ Failed to load from Google Sheets: {e}")
        return st.session_state.get('analysis_history', [])

# 1. CONFIGURATION & API KEY VALIDATION

def load_api_keys():
    """Load and validate API keys from secrets or environment"""

    try:
        PERPLEXITY_KEY = st.secrets.get("PERPLEXITY_API_KEY") or os.getenv("PERPLEXITY_API_KEY", "")
        GEMINI_KEY = st.secrets.get("GEMINI_API_KEY") or os.getenv("GEMINI_API_KEY", "")
        SERPAPI_KEY = st.secrets.get("SERPAPI_KEY") or os.getenv("SERPAPI_KEY", "")
        SCRAPINGDOG_KEY = st.secrets.get("SCRAPINGDOG_KEY") or os.getenv("SCRAPINGDOG_KEY", "")
    except Exception:
        pass
        PERPLEXITY_KEY = os.getenv("PERPLEXITY_API_KEY", "")
        GEMINI_KEY = os.getenv("GEMINI_API_KEY", "")
        SERPAPI_KEY = os.getenv("SERPAPI_KEY", "")
        SCRAPINGDOG_KEY = os.getenv("SCRAPINGDOG_KEY", "")

    # Validate critical keys
    if not PERPLEXITY_KEY or len(PERPLEXITY_KEY) < 10:
        st.error("❌ PERPLEXITY_API_KEY is missing or invalid")
        st.stop()

    if not GEMINI_KEY or len(GEMINI_KEY) < 10:
        st.error("❌ GEMINI_API_KEY is missing or invalid")
        st.stop()

    return PERPLEXITY_KEY, GEMINI_KEY, SERPAPI_KEY, SCRAPINGDOG_KEY

PERPLEXITY_KEY, GEMINI_KEY, SERPAPI_KEY, SCRAPINGDOG_KEY = load_api_keys()
PERPLEXITY_URL = "https://api.perplexity.ai/chat/completions"

# Configure Gemini
#genai.configure(api_key=GEMINI_KEY)
#gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')

# 2. PYDANTIC MODELS

class MetricDetail(BaseModel):
    """Individual metric with name, value, and unit"""
    name: str = Field(..., description="Metric name")
    value: Union[float, int, str] = Field(..., description="Metric value")
    unit: str = Field(default="", description="Unit of measurement")
    model_config = ConfigDict(extra='ignore')

class LLMResponse(BaseModel):
    """Complete LLM response schema"""
    executive_summary: str = Field(..., description="High-level summary")
    primary_metrics: Dict[str, MetricDetail] = Field(default_factory=dict)
    key_findings: List[str] = Field(default_factory=list)
    top_entities: List[Dict[str, Any]] = Field(default_factory=list)
    trends_forecast: List[Dict[str, Any]] = Field(default_factory=list)
    visualization_data: Optional[Dict[str, Any]] = None
    comparison_bars: Optional[Dict[str, Any]] = None
    benchmark_table: Optional[List[Dict[str, Any]]] = None
    sources: List[str] = Field(default_factory=list)
    confidence: Union[float, int] = Field(default=75)
    freshness: Optional[str] = Field(None)
    model_config = ConfigDict(extra='ignore')

# 3. PROMPTS

RESPONSE_TEMPLATE = """
{
  "executive_summary": "3-4 sentence high-level answer",
  "primary_metrics": {
    "metric_1": {"name": "Key Metric 1", "value": 25.5, "unit": "%"},
    "metric_2": {"name": "Key Metric 2", "value": 623, "unit": "$B"}
  },
  "key_findings": [
    "Finding 1 with quantified impact",
    "Finding 2 explaining drivers"
  ],
  "top_entities": [
    {"name": "Entity 1", "share": "25%", "growth": "15%"}
  ],
  "trends_forecast": [
    {"trend": "Trend description", "direction": "↑", "timeline": "2025-2027"}
  ],
  "visualization_data": {
    "chart_labels": ["2023", "2024", "2025"],
    "chart_values": [100, 120, 145],
    "chart_title": "Market Growth",
    "chart_type": "line"
  },
  "comparison_bars": {
    "title": "Market Share",
    "categories": ["A", "B", "C"],
    "values": [45, 30, 25]
  },
  "benchmark_table": [
    {"category": "Company A", "value_1": 25.5, "value_2": 623}
  ],
  "sources": ["source1.com"],
  "confidence": 87,
  "freshness": "Dec 2024"
}
"""

SYSTEM_PROMPT = f"""You are a professional market research analyst.

CRITICAL RULES:
1. Return ONLY valid JSON. NO markdown, NO code blocks, NO extra text.
2. NO citation references like [1][2] inside strings.
3. Use double quotes for all keys and string values.
4. NO trailing commas in arrays or objects.
5. Escape internal quotes with backslash.
6. If the prompt includes "Query Structure", you MUST follow it:
   - Treat "MAIN QUESTION" as the primary topic and address it FIRST.
   - Treat "SIDE QUESTIONS" as secondary topics and address them AFTER the main topic.
   - Do NOT let a side question replace the main question just because it is more specific.
   - In executive_summary, clearly separate: "Main:" then "Side:" when side questions exist.

NUMERIC FIELD RULES (IMPORTANT):
- In benchmark_table: value_1 and value_2 MUST be numbers (never "N/A", "null", or text)
- If data unavailable, use 0 for benchmark_table values
- In primary_metrics: values can be numbers or strings with units (e.g., "25.5" or "25.5 billion")
- In top_entities: share and growth can be strings (e.g., "25%")

REQUIRED FIELDS (provide substantive data):

**executive_summary** - MUST be 4-6 complete sentences covering:
  • Sentence 1: Direct answer with specific quantitative data (market size, revenue, units, etc.)
  • Sentence 2: Major players or regional breakdown with percentages/numbers
  • Sentence 3: Key growth drivers or market dynamics
  • Sentence 4: Future outlook with projected CAGR, timeline, or target values
  • Sentence 5 (optional): Challenge, risk, or competitive dynamic

  BAD (too short): "The EV market is growing rapidly due to government policies."

  GOOD: "The global electric vehicle market reached 14.2 million units sold in 2023, representing 18% of total auto sales. China dominates with 60% market share, followed by Europe (25%) and North America (10%). Growth is driven by battery cost reductions (down 89% since 2010), expanding charging infrastructure, and stricter emission regulations in over 20 countries. The market is projected to grow at 21% CAGR through 2030, reaching 40 million units annually. However, supply chain constraints for lithium and cobalt remain key challenges."

- primary_metrics (3+ metrics with numbers)
- key_findings (3+ findings with quantitative details)
- top_entities (3+ companies/countries with market share %)
- trends_forecast (2+ trends with timelines)
- visualization_data (MUST have chart_labels and chart_values)
- benchmark_table (if included, value_1 and value_2 must be NUMBERS, not "N/A")

Even if web data is sparse, use your knowledge to provide complete, detailed analysis.

Output ONLY this JSON structure:
{RESPONSE_TEMPLATE}
"""
# 5. JSON REPAIR FUNCTIONS

def repair_llm_response(data: dict) -> dict:
    """
    Repair common LLM JSON structure issues:

    - Convert primary_metrics from list -> dict (stable keys)
    - Normalize MetricDetail fields so currency+unit do NOT get lost:
        "29.8 S$B" / "S$29.8B" / "S$29.8 billion" -> value=29.8, unit="S$B"
        "$204.7B" -> value=204.7, unit="$B"
        "9.8%" -> value=9.8, unit="%"
    - Ensure top_entities and trends_forecast are lists
    - Fix visualization_data legacy keys (labels/values)
    - Fix benchmark_table numeric values
    - Remove 'action' block entirely (no longer used)
    - Add minimal required fields if missing

    NOTE: This function is intentionally conservative: it normalizes obvious formatting
    without trying to "invent" missing values.
    """
    if not isinstance(data, dict):
        return {}

    def _to_list(x):
        if x is None:
            return []
        if isinstance(x, list):
            return x
        if isinstance(x, dict):
            return list(x.values())
        return []

    def _coerce_number(s: str):
        try:
            return float(str(s).replace(",", "").strip())
        except Exception:
            return None

    def _normalize_metric_item(item: dict) -> dict:
        """
        Normalize a single metric dict in-place-ish and return it.

        Goal: preserve currency + magnitude in `unit`, keep `value` numeric when possible.
        """
        if not isinstance(item, dict):
            return {"name": "N/A", "value": "N/A", "unit": ""}

        name = item.get("name")
        if not isinstance(name, str) or not name.strip():
            name = "N/A"
        item["name"] = name

        raw_val = item.get("value")
        raw_unit = item.get("unit")

        unit = (raw_unit or "")
        if not isinstance(unit, str):
            unit = str(unit)

        # If already numeric and unit looks okay, keep as-is
        if isinstance(raw_val, (int, float)) and isinstance(unit, str):
            item["unit"] = unit.strip()
            return item

        # Try to parse string value forms like:
        # "S$29.8B", "29.8 S$B", "$ 204.7 billion", "9.8%", "12 percent"
        if isinstance(raw_val, str):
            txt = raw_val.strip()

            # Also allow unit to carry the number sometimes (rare but happens)
            # e.g. value="29.8", unit="S$B" is already fine.
            # But if unit is empty and txt contains unit, we extract.
            # Percent detection
            if re.search(r'(%|\bpercent\b)', txt, flags=re.I):
                num = _coerce_number(re.sub(r'[^0-9\.\-\,]+', '', txt))
                if num is not None:
                    item["value"] = num
                    item["unit"] = "%"
                    return item

            # Currency detection
            currency = ""
            # Normalize currency tokens in either value or unit
            combo = f"{txt} {unit}".strip()

            if re.search(r'\bSGD\b', combo, flags=re.I) or "S$" in combo.upper():
                currency = "S$"
            elif re.search(r'\bUSD\b', combo, flags=re.I) or "$" in combo:
                currency = "$"

            # Magnitude detection
            # Accept: T/B/M/K, or words
            mag = ""
            if re.search(r'\btrillion\b', combo, flags=re.I):
                mag = "T"
            elif re.search(r'\bbillion\b', combo, flags=re.I):
                mag = "B"
            elif re.search(r'\bmillion\b', combo, flags=re.I):
                mag = "M"
            elif re.search(r'\bthousand\b', combo, flags=re.I):
                mag = "K"
            else:
                m = re.search(r'([TBMK])\b', combo.replace(" ", ""), flags=re.I)
                if m:
                    mag = m.group(1).upper()

            # Extract numeric
            num = _coerce_number(re.sub(r'[^0-9\.\-\,]+', '', txt))
            if num is not None:
                # If unit was present and meaningful (and already includes %), keep it
                if unit.strip() == "%":
                    item["value"] = num
                    item["unit"] = "%"
                    return item

                # Build unit as currency+magnitude when any found
                # If neither found, keep existing unit (may be e.g. "years", "points")
                if currency or mag:
                    item["value"] = num
                    item["unit"] = f"{currency}{mag}".strip()
                    return item

                # No currency/mag detected: keep unit if provided; else blank
                item["value"] = num
                item["unit"] = unit.strip()
                return item

            # If we can’t parse into a number, at least preserve the original text
            item["value"] = txt
            item["unit"] = unit.strip()
            return item

        # Non-string, non-numeric (None, dict, list, etc.)
        if raw_val is None or raw_val == "":
            item["value"] = "N/A"
        else:
            item["value"] = str(raw_val)

        item["unit"] = unit.strip()
        return item

    # primary_metrics normalization
    metrics = data.get("primary_metrics")

    # list -> dict
    if isinstance(metrics, list):
        new_metrics = {}
        for i, item in enumerate(metrics):
            if not isinstance(item, dict):
                continue
            item = _normalize_metric_item(item)

            raw_name = item.get("name", f"metric_{i+1}")
            key = re.sub(r'[^a-z0-9_]', '', str(raw_name).lower().replace(" ", "_")).strip("_")
            if not key:
                key = f"metric_{i+1}"

            original_key = key
            j = 1
            while key in new_metrics:
                key = f"{original_key}_{j}"
                j += 1

            new_metrics[key] = item

        data["primary_metrics"] = new_metrics

    elif isinstance(metrics, dict):
        # Normalize each metric dict entry
        cleaned = {}
        for k, v in metrics.items():
            if isinstance(v, dict):
                cleaned[str(k)] = _normalize_metric_item(v)
            else:
                # If someone stored a scalar, wrap it
                cleaned[str(k)] = _normalize_metric_item({"name": str(k), "value": v, "unit": ""})
        data["primary_metrics"] = cleaned

    else:
        data["primary_metrics"] = {}

    # list-like fields
    data["top_entities"] = _to_list(data.get("top_entities"))
    data["trends_forecast"] = _to_list(data.get("trends_forecast"))
    data["key_findings"] = _to_list(data.get("key_findings"))

    # Ensure strings in key_findings
    data["key_findings"] = [str(x) for x in data["key_findings"] if x is not None and str(x).strip()]

    # visualization_data legacy keys
    if isinstance(data.get("visualization_data"), dict):
        viz = data["visualization_data"]
        if "labels" in viz and "chart_labels" not in viz:
            viz["chart_labels"] = viz.pop("labels")
        if "values" in viz and "chart_values" not in viz:
            viz["chart_values"] = viz.pop("values")

        # Coerce chart_labels/values types gently
        if "chart_labels" in viz and not isinstance(viz["chart_labels"], list):
            viz["chart_labels"] = [str(viz["chart_labels"])]
        if "chart_values" in viz and not isinstance(viz["chart_values"], list):
            viz["chart_values"] = [viz["chart_values"]]

    # benchmark_table numeric cleaning
    if isinstance(data.get("benchmark_table"), list):
        cleaned_table = []
        for row in data["benchmark_table"]:
            if not isinstance(row, dict):
                continue

            if "category" not in row:
                row["category"] = "Unknown"

            for key in ["value_1", "value_2"]:
                if key not in row:
                    row[key] = 0
                    continue

                val = row.get(key)
                if isinstance(val, str):
                    val_upper = val.upper().strip()
                    if val_upper in ["N/A", "NA", "NULL", "NONE", "", "-", "—"]:
                        row[key] = 0
                    else:
                        try:
                            cleaned = re.sub(r'[^\d.-]', '', val)
                            row[key] = float(cleaned) if '.' in cleaned else int(cleaned) if cleaned else 0
                        except Exception:
                            pass
                            row[key] = 0
                elif isinstance(val, (int, float)):
                    pass
                else:
                    row[key] = 0

            cleaned_table.append(row)

        data["benchmark_table"] = cleaned_table

    # Remove action block entirely
    data.pop("action", None)

    # Minimal required top-level fields
    if not isinstance(data.get("executive_summary"), str) or not data.get("executive_summary", "").strip():
        data["executive_summary"] = "No executive summary provided."

    if not isinstance(data.get("sources"), list):
        data["sources"] = []

    if "confidence" not in data:
        data["confidence"] = 60

    if not isinstance(data.get("freshness"), str) or not data.get("freshness", "").strip():
        data["freshness"] = "Current"

    return data

def validate_numeric_fields(data: dict, context: str = "LLM Response") -> None:
    """
    Guardrail logger (and gentle coercer) for numeric lists used in charts/tables.

    We keep this lightweight: warn when strings appear where numbers are expected,
    and attempt to coerce when safe.
    """
    if not isinstance(data, dict):
        return

    # Check benchmark_table
    if "benchmark_table" in data and isinstance(data["benchmark_table"], list):
        for i, row in enumerate(data["benchmark_table"]):
            if isinstance(row, dict):
                for key in ["value_1", "value_2"]:
                    val = row.get(key)
                    if isinstance(val, str):
                        st.warning(
                            f"⚠️ {context}: benchmark_table[{i}].{key} is string: '{val}' (coercing to 0 if invalid)"
                        )
                        try:
                            cleaned = re.sub(r"[^\d\.\-]", "", val)
                            row[key] = float(cleaned) if cleaned else 0
                        except Exception:
                            pass
                            row[key] = 0

    # Check visualization_data chart_values
    viz = data.get("visualization_data")
    if isinstance(viz, dict):
        vals = viz.get("chart_values")
        if isinstance(vals, list):
            new_vals = []
            for j, v in enumerate(vals):
                if isinstance(v, (int, float)):
                    new_vals.append(v)
                elif isinstance(v, str):
                    try:
                        cleaned = re.sub(r"[^\d\.\-]", "", v)
                        new_vals.append(float(cleaned) if cleaned else 0.0)
                        st.warning(f"⚠️ {context}: visualization_data.chart_values[{j}] is string: '{v}' (coerced)")
                    except Exception:
                        pass
                        new_vals.append(0.0)
                else:
                    new_vals.append(0.0)
            viz["chart_values"] = new_vals

def preclean_json(raw: str) -> str:
    """
    Remove markdown fences and common citation markers before JSON parsing.
    Conservative: tries not to destroy legitimate JSON content.
    """
    if not raw or not isinstance(raw, str):
        return ""

    text = raw.strip()

    # Remove leading/trailing code fences (```json ... ```)
    text = re.sub(r'^\s*```(?:json)?\s*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\s*```\s*$', '', text)

    text = text.strip()

    # Remove common citation formats the model may append
    # [web:1], [1], (1) etc. (but avoid killing array syntax by being specific)
    text = re.sub(r'\[web:\d+\]', '', text, flags=re.IGNORECASE)
    text = re.sub(r'(?<!")\[\d+\](?!")', '', text)   # not inside quotes
    text = re.sub(r'(?<!")\(\d+\)(?!")', '', text)   # not inside quotes

    return text.strip()

def parse_json_safely(json_str: str, context: str = "LLM") -> dict:
    """
    Parse JSON with aggressive error recovery:
    1) Pre-clean markdown/citations
    2) Extract the *first* JSON object
    3) Repair common issues (unquoted keys, trailing commas, True/False/Null)
    4) Try parsing; if it fails, attempt a small set of pragmatic fixes
    """
    if json_str is None:
        return {}
    if not isinstance(json_str, str):
        json_str = str(json_str)

    if not json_str.strip():
        return {}

    cleaned = preclean_json(json_str)

    # Extract first JSON object (most LLM outputs are one object)
    match = re.search(r'\{.*\}', cleaned, flags=re.DOTALL)
    if not match:
        st.warning(f"⚠️ No JSON object found in {context} response")
        return {}

    json_content = match.group(0)

    # Structural repairs
    try:
        # Fix unquoted keys: {key: -> {"key":
        json_content = re.sub(
            r'([\{\,]\s*)([a-zA-Z_][a-zA-Z0-9_\-]*)(\s*):',
            r'\1"\2"\3:',
            json_content
        )

        # Remove trailing commas
        json_content = re.sub(r',\s*([\]\}])', r'\1', json_content)

        # Fix boolean/null capitalization
        json_content = re.sub(r':\s*True\b', ': true', json_content)
        json_content = re.sub(r':\s*False\b', ': false', json_content)
        json_content = re.sub(r':\s*Null\b', ': null', json_content)

    except Exception as e:
        st.warning(f"⚠️ {context}: Regex repair failed: {e}")

    # Attempt parse with a few passes
    attempts = 0
    last_err = None

    while attempts < 6:
        try:
            return json.loads(json_content)
        except json.JSONDecodeError as e:
            last_err = e
            msg = (e.msg or "").lower()

            # Pass 1: replace smart quotes
            if attempts == 0:
                json_content = (
                    json_content.replace(""", '"')
                                .replace(""", '"')
                                .replace("’", "'")
                )

            # Pass 2: single-quote keys/strings -> double quotes (limited)
            elif attempts == 1:
                # Only do this if it looks like single quotes dominate
                if json_content.count("'") > json_content.count('"'):
                    json_content = re.sub(r"\'", '"', json_content)

            # Pass 3: try removing control characters
            elif attempts == 2:
                json_content = re.sub(r"[\x00-\x1F\x7F]", "", json_content)

            # Pass 4: if unterminated string, try escaping a quote near the error
            elif "unterminated string" in msg or "unterminated" in msg:
                pos = e.pos
                # Try escaping a quote a bit before pos
                for i in range(pos - 1, max(0, pos - 200), -1):
                    if i < len(json_content) and json_content[i] == '"':
                        if i == 0 or json_content[i - 1] != "\\":
                            json_content = json_content[:i] + '\\"' + json_content[i + 1:]
                            break

            # Pass 5+: give up
            attempts += 1
            continue

    st.error(f"❌ Failed to parse JSON from {context}: {str(last_err)[:180] if last_err else 'unknown error'}")
    return {}

# 6. WEB SEARCH FUNCTIONS
#   SERPAPI STABILITY CONFIGURATION

# Fixed parameters to prevent geo/personalization variance

SERPAPI_STABILITY_CONFIG = {
    "gl": "us",                    # Fixed country
    "hl": "en",                    # Fixed language
    "google_domain": "google.com", # Fixed domain
    "nfpr": "1",                   # No auto-query correction
    "safe": "active",              # Consistent safe search
    "device": "desktop",           # Fixed device type
    "no_cache": "false",           # Allow Google caching (more stable)
}

# Preferred domains for consistent sourcing (sorted by priority)
PREFERRED_SOURCE_DOMAINS = [
    "statista.com", "reuters.com", "bloomberg.com", "imf.org", "wsj.com", "bcg.com", "opec.org",
    "worldbank.org", "mckinsey.com", "deloitte.com", "spglobal.com", "ft.com", "pwc.com", "semiconductors.org",
    "ft.com", "economist.com", "wsj.com", "forbes.com", "cnbc.com", "kpmg.com", "eia.org"
]

# Search results cache
_search_cache: Dict[str, Tuple[List[Dict], datetime]] = {}
SEARCH_CACHE_TTL_HOURS = int(_yureeka_hp_get_v1('ops.search_cache_ttl_hours', 24) or 24)

def get_search_cache_key(query: str) -> str:
    """Generate stable cache key for search query"""
    normalized = re.sub(r'\s+', ' ', query.lower().strip())
    normalized = re.sub(r'\b(today|current|latest|now|recent)\b', '', normalized)
    return hashlib.md5(normalized.encode()).hexdigest()[:16]

def get_cached_search_results(query: str) -> Optional[List[Dict]]:
    """
    Get cached search results if still valid.

    IMPORTANT:
    - Never treat cached empty results as valid.
      Returning [] here "poisons" the pipeline for hours and makes SerpAPI look broken.
    """
    try:
        cache_key = get_search_cache_key(query)
        if cache_key in _search_cache:
            cached_results, cached_time = _search_cache[cache_key]
            if datetime.now() - cached_time < timedelta(hours=SEARCH_CACHE_TTL_HOURS):
                # ✅ Do not reuse empty cache entries
                if isinstance(cached_results, list) and len(cached_results) == 0:
                    return None
                return cached_results
            # expired
            del _search_cache[cache_key]
    except Exception:
        return None
    return None

def cache_search_results(query: str, results: List[Dict]):
    """
    Cache search results.

    IMPORTANT:
    - Do NOT cache empty lists
    - Do NOT cache lists that contain no usable URLs
      (prevents "poisoned cache" that makes SerpAPI appear broken)
    """
    try:
        if not isinstance(query, str) or not query.strip():
            return
        if not isinstance(results, list) or not results:
            return

        # Require at least one usable url/link
        has_url = False
        for r in results:
            if isinstance(r, dict):
                u = (r.get("link") or r.get("url") or "").strip()
                if u:
                    has_url = True
                    break
            elif isinstance(r, str) and r.strip():
                has_url = True
                break

        if not has_url:
            return

        cache_key = get_search_cache_key(query)
        _search_cache[cache_key] = (results, datetime.now())
    except Exception:
        return

# LLM RESPONSE CACHE - Prevents variance on identical inputs
_llm_cache: Dict[str, Tuple[str, datetime]] = {}

def _legacy_get_llm_cache_key_v0(query: str, web_context: Dict) -> str:
    """Generate cache key from query + source URLs"""
    # Include source URLs so cache invalidates if sources change
    source_urls = sorted(web_context.get("sources", [])[:5])
    cache_input = f"{query.lower().strip()}|{'|'.join(source_urls)}"
    return hashlib.md5(cache_input.encode()).hexdigest()[:20]

def _legacy_cache_llm_response_v0(query: str, web_context: Dict, response: str):
    """Cache LLM response"""
    cache_key = _legacy_get_llm_cache_key_v0(query, web_context)
    _llm_cache[cache_key] = (response, datetime.now())

def sort_results_deterministically(results: List[Dict]) -> List[Dict]:
    """Sort results for consistent ordering"""
    def sort_key(r):
        link = r.get("link", "").lower()
        # Priority: preferred domains first, then alphabetical
        priority = 999
        for i, domain in enumerate(PREFERRED_SOURCE_DOMAINS):
            if domain in link:
                priority = i
                break
        return (priority, link)
    return sorted(results, key=sort_key)

def classify_source_reliability(source: str) -> str:
    """Classify source as High/Medium/Low quality"""
    source = source.lower() if isinstance(source, str) else ""

    high = ["gov", "imf", "worldbank", "central bank", "fed", "ecb", "reuters", "spglobal", "economist", "mckinsey", "bcg", "cognitive market research",
            "financial times", "wsj", "oecd", "bloomberg", "tradingeconomics", "deloitte", "hsbc", "imarc", "booz allen", "bakerinstitute.org", "wef",
           "kpmg", "semiconductors.org", "eu", "iea", "world bank", "opec", "jpmorgan", "citibank", "goldmansachs", "j.p. morgan", "oecd",
           "world bank", "sec", "federalreserve", "bls", "bea"]
    medium = ["wikipedia", "forbes", "cnbc", "yahoo", "ceic", "statista", "trendforce", "digitimes", "idc", "gartner", "marketwatch", "fortune", "investopedia"]
    low = ["blog", "medium.com", "wordpress", "ad", "promo"]

    for h in high:
        if h in source:
            return "✅ High"
    for m in medium:
        if m in source:
            return "⚠️ Medium"
    for l in low:
        if l in source:
            return "❌ Low"

    return "⚠️ Medium"

def source_quality_score(sources: List[str]) -> float:
    """Calculate average source quality (0-100)"""
    if not sources:
        return 50.0  # Lower default when no sources

    weights = {"✅ High": 100, "⚠️ Medium": 60, "❌ Low": 30}
    scores = [weights.get(classify_source_reliability(s), 60) for s in sources]
    return sum(scores) / len(scores) if scores else 50.0

@st.cache_data(ttl=3600, show_spinner=False)
def search_serpapi(query: str, num_results: int = 10) -> List[Dict]:
    """Search Google via SerpAPI with stability controls"""
    if not SERPAPI_KEY:
        return []

    # Check cache first (this is the ONLY cache we use - removed @st.cache_data to avoid conflicts)
    cached = get_cached_search_results(query)
    if cached:
        st.info("📦 Using cached search results")
        return cached

    # Aggressive query normalization for consistent searches
    query_normalized = query.lower().strip()

    # Remove temporal words that cause variance
    query_normalized = re.sub(r'\b(latest|current|today|now|recent|new|upcoming|this year|this month)\b', '', query_normalized)

    # Normalize whitespace
    query_normalized = re.sub(r'\s+', ' ', query_normalized).strip()

    # Add year for consistency
    if not re.search(r'\b20\d{2}\b', query_normalized):
        query_normalized = f"{query_normalized} 2024"

    # Build search terms
    query_lower = query_normalized
    industry_kw = ["industry", "market", "sector", "size", "growth", "players"]

    if any(kw in query_lower for kw in industry_kw):
        search_terms = f"{query_normalized} market size growth statistics"
        tbm, tbs = "", ""  # Organic results (more stable than news)
    else:
        search_terms = f"{query_normalized} finance economics data"
        tbm, tbs = "", ""  # Use organic for stability

    params = {
        "engine": "google",
        "q": search_terms,
        "api_key": SERPAPI_KEY,
        "num": num_results,
        "tbm": tbm,
        "tbs": tbs,
        **SERPAPI_STABILITY_CONFIG  # Add fixed location params
    }

    try:
        # REFACTOR153: requests may be absent in minimal envs; handle gracefully.
        if requests is None:
            raise RuntimeError('requests library is not available')
        resp = requests.get("https://serpapi.com/search", params=params, timeout=10)
        resp.raise_for_status()
        data = resp.json()

        results = []

        # Prefer organic results (more stable than news)
        for item in data.get("organic_results", [])[:num_results]:
            results.append({
                "title": item.get("title", ""),
                "link": item.get("link", ""),
                "snippet": item.get("snippet", ""),
                "date": item.get("date", ""),
                "source": item.get("source", "")
            })

        # Fall back to news only if no organic results
        if not results:
            for item in data.get("news_results", [])[:num_results]:
                src = item.get("source", {})
                source_name = src.get("name", "") if isinstance(src, dict) else str(src)
                results.append({
                    "title": item.get("title", ""),
                    "link": item.get("link", ""),
                    "snippet": item.get("snippet", ""),
                    "date": item.get("date", ""),
                    "source": source_name
                })

        # Sort deterministically
        results = sort_results_deterministically(results)
        results = results[:num_results]

        # Cache results
        if results:
            cache_search_results(query, results)

        return results

    except Exception as e:
        st.warning(f"⚠️ SerpAPI error: {e}")
        return []

# - Pure helpers (no control-flow changes)
# - Used to trace injected extra URLs across: UI -> intake -> scrape -> snapshots -> hashing -> rebuild
def _inj_diag_make_run_id(prefix: str = "run") -> str:
    """Short correlation id for a single analysis/evolution run."""
    try:
        seed = f"{prefix}|{time.time()}|{os.getpid()}|{os.urandom(8).hex()}"
        return hashlib.sha256(seed.encode("utf-8")).hexdigest()[:12]
    except Exception:
        pass
        try:
            import random
            return f"{prefix}_{random.randint(100000,999999)}"
        except Exception:
            return f"{prefix}_unknown"

# - Strips common tracking/query parameters from injected URLs ONLY
# - Keeps scheme/host/path; preserves non-tracking query params (sorted)
# - Adds deterministic canonical form for stable admission/dedupe/hashing
def _canonicalize_injected_url(url: str) -> str:
    """Canonicalize injected URLs by stripping known tracking params.

    This is intentionally conservative and applied only to user-injected URLs
    (extra URLs), not to SERP-derived URLs.
    """
    try:
        from urllib.parse import urlsplit, urlunsplit, parse_qsl, urlencode
        u = str(url or "").strip()
        if not u:
            return ""
        if not (u.startswith("http://") or u.startswith("https://")):
            return u

        parts = urlsplit(u)
        # Normalize scheme/host case
        scheme = (parts.scheme or "").lower()
        netloc = (parts.netloc or "").lower()
        path = parts.path or ""
        fragment = ""  # drop fragments for stability

        # Tracking params to drop (exact match)
        drop_exact = {
            "guccounter", "guce_referrer", "guce_referrer_sig",
            "gclid", "fbclid", "msclkid", "mc_cid", "mc_eid",
            "ref", "ref_src",
        }
        # Drop prefixes (utm_*, etc.)
        drop_prefixes = ("utm_",)

        qs = []
        for k, v in parse_qsl(parts.query or "", keep_blank_values=True):
            kk = (k or "").strip()
            if not kk:
                continue
            k_lower = kk.lower()
            if k_lower in drop_exact:
                continue
            if any(k_lower.startswith(p) for p in drop_prefixes):
                continue
            qs.append((kk, v))

        # Sort query params for determinism
        qs_sorted = sorted(qs, key=lambda kv: (kv[0].lower(), str(kv[1])))

        query = urlencode(qs_sorted, doseq=True) if qs_sorted else ""
        return urlunsplit((scheme, netloc, path, query, fragment))
    except Exception:
        pass
        try:
            return str(url or "").strip()
        except Exception:
            return ""

def _inj_diag_norm_url_list(extra_urls: Any) -> list:
    """Normalize/dedupe injected URL list (http/https only) with canonicalization.

    NOTE: This is used for injected/extra URL diagnostics and admission wiring only.
    It canonicalizes by stripping known tracking params for stability.
    """
    out = []
    try:
        if extra_urls is None:
            return []
        items = extra_urls
        if isinstance(items, str):
            items = [u.strip() for u in items.splitlines()]
        if not isinstance(items, (list, tuple, set)):
            items = [str(items)]
        seen = set()
        for u in items:
            uu = str(u or "").strip()
            if not uu:
                continue
            if not (uu.startswith("http://") or uu.startswith("https://")):
                continue
            cu = _canonicalize_injected_url(uu) or uu
            if cu in seen:
                continue
            seen.add(cu)
            out.append(cu)
    except Exception:
        return []
    return out

# =========================
# REFACTOR115 helpers (additive; deterministic; schema/key grammar unchanged)
# =========================

_REFACTOR115_SCHEMA_KEYS_V1 = [
    "global_ev_chargers_2040__unit_count",
    "global_ev_charging_investment_2040__currency",
    "global_ev_chargers_cagr_2026_2040__percent",
    "global_ev_sales_ytd_2025__unit_sales",
]

# Deterministic "known-good" seed URLs to stabilize year-anchor gating in production pools.
# These are only unioned into production (non-injection) runs.
_REFACTOR115_SCHEMA_SEED_URLS_V1 = [
    # Wood Mackenzie press release contains 2026–2040 CAGR, 2040 port totals, and 2040 spend ($).
    "https://www.woodmac.com/press-releases/global-ev-charging-ports-to-increase-cagr-of-12.3-from-2026-2040-reaching-206.6m-total-ports",
    # Secondary syndications of the same Wood Mackenzie facts (often easier to parse / less scripting).
    "https://www.aa.com.tr/en/energy/electricity/global-ev-charging-ports-forecast-to-surpass-206-million-by-2040/51226",
    "https://www.evinfrastructurenews.com/ev-networks/wood-mackenzie-global-ev-chargers-206-6m-2040",
    # YTD sales anchors for 2025 (Rho Motion).
    "https://rhomotion.com/news/global-ev-sales-reach-18-5-million-units-growing-by-21-ytd-in-november-2025/",
    # IEA HTML (non-PDF) for broader charging context.
    "https://www.iea.org/reports/global-ev-outlook-2024/trends-in-electric-vehicle-charging",

    # Additional syndicated/secondary pages with explicit 'investment' / '$... by 2040' phrasing (improves investment_2040 binding).
    "https://pv-magazine-usa.com/2025/08/18/global-ev-charging-ports-to-grow-12-annually-through-2040-says-wood-mackenzie/",
    "https://www.pv-magazine-india.com/2024/07/12/the-ev-charging-boom-a-1-trillion-opportunity-by-2040/",
    "https://www.sustainabletimes.co.uk/post/global-ev-charging-network-on-track-to-surpass-200-million-by-2040",
    "https://electrek.co/2025/08/18/home-charging-rules-as-global-ev-ports-soar-to-206-million-by-2040/",
    "https://energynews.pro/en/global-ev-charging-points-to-reach-206-6-million-by-2040/",
    "https://bolt.earth/blog/how-india-s-ev-infrastructure-rise-will-reshape-mobility-mindsets-and-market-dynamics-by-2040",
]

def _refactor115_collect_injection_urls_v1(results: Any, web_context: Any) -> list:
    """Collect injected URLs robustly from web_context + debug fields.

    This is intentionally defensive because multiple historical debug fields exist.
    Returned URLs are normalized/deduped via _inj_diag_norm_url_list.
    """
    urls = []
    try:
        wc = web_context if isinstance(web_context, dict) else {}
        # Common web_context locations
        for k in ("extra_urls", "injected_urls", "injected_urls_ui_norm", "ui_extra_urls"):
            v = wc.get(k)
            if isinstance(v, str):
                urls.extend([s.strip() for s in v.splitlines() if s.strip()])
            elif isinstance(v, (list, tuple, set)):
                urls.extend([str(x).strip() for x in v if isinstance(x, str) and str(x).strip()])
        # Diag containers
        dv = wc.get("diag_injected_urls") or wc.get("debug_injected_urls") or {}
        if isinstance(dv, dict):
            for kk in ("ui_norm", "intake_norm", "all_norm"):
                vv = dv.get(kk)
                if isinstance(vv, (list, tuple, set)):
                    urls.extend([str(x).strip() for x in vv if isinstance(x, str) and str(x).strip()])
        # Results/debug legacy fields
        if isinstance(results, dict):
            dbg = results.get("debug") if isinstance(results.get("debug"), dict) else {}
            # inj_trace variants
            for trace_k in ("inj_trace_v1", "inj_trace_v2_postfetch", "inj_trace_v2"):
                it = dbg.get(trace_k)
                if isinstance(it, dict):
                    for kk in ("ui_norm", "intake_norm"):
                        vv = it.get(kk)
                        if isinstance(vv, (list, tuple, set)):
                            urls.extend([str(x).strip() for x in vv if isinstance(x, str) and str(x).strip()])
            # Legacy lists
            for kk in ("injected_urls_v1", "fix2d65b_injected_urls", "fix2d66_injected_urls", "injected_urls"):
                vv = dbg.get(kk) or results.get(kk)
                if isinstance(vv, (list, tuple, set)):
                    urls.extend([str(x).strip() for x in vv if isinstance(x, str) and str(x).strip()])
    except Exception:
        pass

    try:
        return _inj_diag_norm_url_list(urls)
    except Exception:
        # best-effort fallback
        out = []
        seen = set()
        for u in urls:
            uu = str(u or "").strip()
            if uu and uu not in seen:
                seen.add(uu)
                out.append(uu)
        return out

# REFACTOR116: Robust post-hoc timing + per-row Δt stamping.
# Some runs pass a stale previous_data.timestamp into evolution while the evolution
# engine correctly selects a newer baseline snapshot (see debug.prev_snapshot_pick_v1).
# This helper recomputes run_timing_v1 and per-row analysis_evolution_delta_* from the
# *effective* baseline timestamp actually used, and blanks row-level Δt for injected rows.

def _refactor116_locate_evolution_results_dict_v1(evo_obj: Any) -> tuple:
    """Return (results_dict, wrapper_dict) for an evolution payload.

    - If evo_obj already looks like the results dict (has metric_changes/source_results), return it.
    - Else if evo_obj['results'] looks like the results dict, return that plus the wrapper.
    """
    if not isinstance(evo_obj, dict):
        return ({}, {})
    # Direct results dict
    if ("metric_changes" in evo_obj) and ("source_results" in evo_obj):
        return (evo_obj, evo_obj)
    inner = evo_obj.get("results")
    if isinstance(inner, dict) and (("metric_changes" in inner) or ("source_results" in inner)):
        return (inner, evo_obj)
    return (evo_obj, evo_obj)

def _refactor116_effective_baseline_timestamp_v1(results_dict: dict, wrapper_dict: dict, previous_data: Any) -> str:
    """Choose the effective baseline timestamp actually used for diffing."""
    ts = None
    try:
        if isinstance(wrapper_dict, dict):
            ts = wrapper_dict.get("previous_timestamp") or wrapper_dict.get("baseline_timestamp")
    except Exception:
        ts = None
    if not ts:
        try:
            ts = _first_present(
                results_dict or {},
                [
                    ["debug", "prev_snapshot_pick_v1", "selected_timestamp"],
                    ["debug", "prev_snapshot_pick_v2", "selected_timestamp"],
                    ["debug", "baseline_selector_v1", "selected_timestamp"],
                ],
                default=None,
            )
        except Exception:
            ts = None
    if not ts:
        try:
            if isinstance(previous_data, dict):
                ts = previous_data.get("timestamp")
        except Exception:
            ts = None
    return ts or ""

def _refactor116_compute_run_timing_v1(results_dict: dict, wrapper_dict: dict, previous_data: Any) -> tuple:
    """Return (analysis_ts_norm, evolution_ts_norm, delta_seconds, delta_human, warnings)."""
    warnings = []
    evo_ts_raw = None
    try:
        evo_ts_raw = None
        if isinstance(wrapper_dict, dict):
            evo_ts_raw = wrapper_dict.get("timestamp") or wrapper_dict.get("generated_at")
        if not evo_ts_raw and isinstance(results_dict, dict):
            evo_ts_raw = results_dict.get("generated_at") or results_dict.get("timestamp")
    except Exception:
        evo_ts_raw = None
    evo_ts_raw = evo_ts_raw or _yureeka_now_iso_utc()

    base_ts_raw = _refactor116_effective_baseline_timestamp_v1(results_dict, wrapper_dict, previous_data)
    dt_a = _parse_iso_dt(base_ts_raw) if base_ts_raw else None
    dt_e = _parse_iso_dt(evo_ts_raw) if evo_ts_raw else None

    analysis_ts_norm = dt_a.isoformat() if dt_a else (base_ts_raw or None)
    evo_ts_norm = dt_e.isoformat() if dt_e else (evo_ts_raw or None)

    delta_seconds = None
    delta_human = ""
    try:
        if dt_a and dt_e:
            ds = (dt_e - dt_a).total_seconds()
            if ds < 0:
                warnings.append("delta_negative_clamped_to_zero")
                ds = 0.0
            delta_seconds = float(ds)
            delta_human = _yureeka_humanize_seconds_v1(delta_seconds)
        else:
            warnings.append("delta_uncomputed_missing_timestamp")
    except Exception:
        warnings.append("delta_uncomputed_exception")
    return (analysis_ts_norm, evo_ts_norm, delta_seconds, delta_human, warnings)

def _refactor116_apply_effective_timing_and_row_deltas_v1(evo_obj: Any, previous_data: Any, web_context: Any = None) -> None:
    """Mutate the evolution payload to ensure Δt + per-row delta fields are correct."""
    try:
        res, wrap = _refactor116_locate_evolution_results_dict_v1(evo_obj)
        if not isinstance(res, dict):
            return
        wc = web_context if isinstance(web_context, dict) else {}

        analysis_ts_norm, evo_ts_norm, delta_seconds, delta_human, warnings = _refactor116_compute_run_timing_v1(res, wrap, previous_data)

        # Ensure debug container
        dbg = res.get("debug")
        if not isinstance(dbg, dict):
            dbg = {}
            res["debug"] = dbg

        # Beacon: effective timing (v4) (authoritative)
        _rt_eff = {
            "raw_previous_data_timestamp": (previous_data or {}).get("timestamp") if isinstance(previous_data, dict) else None,
            "previous_timestamp_field": (wrap or {}).get("previous_timestamp") if isinstance(wrap, dict) else None,
            "analysis_timestamp_effective": analysis_ts_norm,
            "evolution_timestamp": evo_ts_norm,
            "delta_seconds_effective": delta_seconds,
            "delta_human_effective": delta_human,
            "baseline_ts_source": (
                "wrapper.previous_timestamp" if ((wrap or {}).get("previous_timestamp") if isinstance(wrap, dict) else None)
                else "debug.prev_snapshot_pick_v1_or_previous_data"
            ),
        }
        dbg["run_timing_effective_v4"] = _rt_eff
        # Backward compatibility: alias v2 -> v4 (do not duplicate dict build)
        dbg["run_timing_effective_v2"] = _rt_eff

        # Overwrite run_timing_v1 with effective values
        rt = dbg.get("run_timing_v1") if isinstance(dbg.get("run_timing_v1"), dict) else {}
        rt.update({
            "analysis_timestamp": analysis_ts_norm,
            "evolution_timestamp": evo_ts_norm,
            "delta_seconds": delta_seconds,
            "delta_human": delta_human,
            "warnings": list(warnings or []),
        })
        # Record injection presence (but do not suppress run-level Δt)
        try:
            inj_urls_probe = _refactor115_collect_injection_urls_v1(res, wc)
            rt["injection_present_v3"] = bool(inj_urls_probe)
        except Exception:
            rt["injection_present_v3"] = False
        dbg["run_timing_v1"] = rt

        # Copy run delta fields
        res["run_delta_seconds"] = delta_seconds
        res["run_delta_human"] = delta_human
        # Copy into nested analysis payload (res["results"]) if present
        try:
            if isinstance(res.get("results"), dict):
                res["results"]["run_delta_seconds"] = delta_seconds
                res["results"]["run_delta_human"] = delta_human
                dbg2 = res["results"].get("debug")
                if not isinstance(dbg2, dict):
                    dbg2 = {}
                    res["results"]["debug"] = dbg2
                dbg2["run_timing_v1"] = dict(rt)
        except Exception:
            pass

        # Row-level delta stamping
        inj_urls = []
        try:
            inj_urls = _refactor115_collect_injection_urls_v1(res, wc)
        except Exception:
            inj_urls = []
        inj_norm = None
        try:
            inj_norm = _inj_diag_norm_url_list(inj_urls)
        except Exception:
            inj_norm = None
        inj_set = set([str(u).strip() for u in (inj_norm or inj_urls) if isinstance(u, str) and str(u).strip()])

        gating = {
            "inj_set_size": int(len(inj_set)),
            "inj_urls_sample": list(list(inj_set)[:3]),
            "rows_total": 0,
            "injected_rows_total": 0,
            "injected_rows_with_delta": 0,
            "injected_rows_blank_delta": 0,
            "production_rows_total": 0,
            "production_rows_with_delta": 0,
            "rows_with_source_url": 0,
            "rows_missing_source_url": 0,
            "injection_run": bool(inj_urls or inj_set),
            "injection_run_force_blank_rows": 0,
        }

        def _stamp(rows: Any):
            if not isinstance(rows, list):
                return
            for r in rows:
                if not isinstance(r, dict):
                    continue
                gating["rows_total"] += 1
                su = r.get("source_url")
                if not su:
                    gating["rows_missing_source_url"] += 1
                else:
                    gating["rows_with_source_url"] += 1

                # LLM38: row-level injection delta blanking.
                # Only blank Δt when the *row* is actually sourced from an injected URL.
                _is_injected_row = False
                try:
                    if su:
                        try:
                            _su_n = _fix2af_norm_url(su)
                        except Exception:
                            _su_n = str(su).strip()
                        if _su_n and (_su_n in inj_set):
                            _is_injected_row = True
                except Exception:
                    _is_injected_row = False

                if _is_injected_row:
                    gating["injected_rows_total"] += 1
                    r["analysis_evolution_delta_human"] = delta_human or ""
                    r["analysis_evolution_delta_seconds"] = delta_seconds
                    if (delta_human or delta_seconds is not None):
                        gating["injected_rows_with_delta"] += 1
                    else:
                        gating["injected_rows_blank_delta"] += 1
                    continue

                gating["production_rows_total"] += 1
                r["analysis_evolution_delta_human"] = delta_human or ""
                r["analysis_evolution_delta_seconds"] = delta_seconds
                if (delta_human or delta_seconds is not None):
                    gating["production_rows_with_delta"] += 1

        _stamp(res.get("metric_changes"))

        # NLP05: Injection-aware stability display (exclude injected rows from stability/summary).
        # This is reporting-only: it does NOT change deterministic winners/values.
        try:
            if bool(gating.get("injection_run")) and isinstance(res.get("metric_changes"), list):
                _rows_all = [r for r in (res.get("metric_changes") or []) if isinstance(r, dict)]

                def _is_injected_row_v1(_r: dict) -> bool:
                    try:
                        su = _r.get("source_url")
                        if not su:
                            return False
                        try:
                            su_n = _fix2af_norm_url(su)
                        except Exception:
                            su_n = str(su).strip()
                        return bool(su_n and (su_n in inj_set))
                    except Exception:
                        return False

                _rows_prod = [r for r in _rows_all if not _is_injected_row_v1(r)]
                _tmp = {"metric_changes": _rows_prod}
                _refactor13_recompute_summary_and_stability_v1(_tmp)

                _eff_stab = _tmp.get("stability_score")
                if _eff_stab is None:
                    _eff_stab = 100.0 if not _rows_prod else 0.0

                res["stability_score_effective"] = round(float(_eff_stab), 1)

                _eff_summary = _tmp.get("summary") if isinstance(_tmp.get("summary"), dict) else {}
                # Annotate effective summary with injection counters (for UI + audit).
                _eff_summary["metrics_injected"] = int(gating.get("injected_rows_total") or 0)
                _eff_summary["metrics_production"] = int(gating.get("production_rows_total") or 0)
                _eff_summary["total_metrics"] = int(gating.get("rows_total") or len(_rows_all))
                _eff_summary["metrics_found"] = int(gating.get("rows_total") or len(_rows_all))

                res["summary_effective"] = _eff_summary

                try:
                    dbg["injection_stability_adjust_v1"] = {
                        "injection_run": True,
                        "raw_stability_score": res.get("stability_score"),
                        "effective_stability_score": res.get("stability_score_effective"),
                        "raw_summary": res.get("summary"),
                        "effective_summary": _eff_summary,
                        "injected_rows_total": int(gating.get("injected_rows_total") or 0),
                        "production_rows_total": int(gating.get("production_rows_total") or 0),
                    }
                except Exception:
                    pass

                # Mirror into nested results.debug if present (backward compatibility).
                try:
                    if isinstance(res.get("results"), dict):
                        _dbg2 = res["results"].get("debug")
                        if isinstance(_dbg2, dict):
                            _dbg2["injection_stability_adjust_v1"] = dbg.get("injection_stability_adjust_v1")
                except Exception:
                    pass
        except Exception:
            pass


        # LLM10: ensure evolution JSON always carries evidence snippet fields (auditability only)
        try:
            _pmc = res.get("primary_metrics_canonical")
            if isinstance(_pmc, dict) and _pmc:
                _pool = []
                if isinstance(res.get("baseline_sources_cache_current"), list) and res.get("baseline_sources_cache_current"):
                    _pool = res.get("baseline_sources_cache_current")
                elif isinstance(res.get("baseline_sources_cache"), list):
                    _pool = res.get("baseline_sources_cache")
                elif isinstance((wrap or {}).get("baseline_sources_cache"), list):
                    _pool = (wrap or {}).get("baseline_sources_cache")
                _stats = _llm01_attach_evidence_snippets_to_pmc_v1(
                    pmc=_pmc,
                    baseline_sources_cache=_pool,
                    metric_schema=res.get("metric_schema_frozen"),
                    question=str((wrap or {}).get("question") or ""),
                    stage="evolution_timing_pass",
                    out_debug=dbg,
                )
                dbg["llm01_evidence_snippets_evolution_timing_hook_v1"] = _stats
        except Exception:
            pass

        _gating_dbg = dict(gating)
        dbg["row_delta_gating_v4"] = _gating_dbg
        # Backward compatibility: alias v2 -> v4 (do not duplicate dict build)
        dbg["row_delta_gating_v2"] = _gating_dbg
        try:
            if isinstance(res.get("results"), dict):
                dbg2 = res["results"].get("debug")
                if not isinstance(dbg2, dict):
                    dbg2 = {}
                    res["results"]["debug"] = dbg2
                dbg2["row_delta_gating_v4"] = _gating_dbg
                dbg2["row_delta_gating_v2"] = _gating_dbg
        except Exception:
            pass
    except Exception:
        return

def _refactor115_all_schema_values_null_in_payload_v1(payload: Any) -> bool:
    """Return True if all schema keys are missing or null in primary_metrics_canonical/metric_schema_frozen."""
    try:
        if not isinstance(payload, dict):
            return False
        pmc = payload.get("primary_metrics_canonical")
        if not isinstance(pmc, dict) and isinstance(payload.get("results"), dict):
            pmc = payload["results"].get("primary_metrics_canonical")
        ms = payload.get("metric_schema_frozen")
        if not isinstance(ms, dict) and isinstance(payload.get("results"), dict):
            ms = payload["results"].get("metric_schema_frozen")
        # helper to read a value-ish field
        def _val_from_metric(m):
            if not isinstance(m, dict):
                return None
            for k in ("value", "point_estimate", "numeric_value", "raw_value"):
                if k in m:
                    return m.get(k)
            return m.get("value")
        any_non_null = False
        for ck in _REFACTOR115_SCHEMA_KEYS_V1:
            v = None
            if isinstance(pmc, dict) and ck in pmc:
                v = _val_from_metric(pmc.get(ck))
            if v is None and isinstance(ms, dict) and ck in ms:
                v = _val_from_metric(ms.get(ck))
            if v is None or v == "":
                continue
            any_non_null = True
            break
        return (not any_non_null)
    except Exception:
        return False

def _yureeka_extract_injected_urls_v1(web_context: Any) -> list:
    """Extract UI-injected URLs deterministically.

    Contract:
      - Prefer Streamlit/UI fields: diag_extra_urls_ui(_raw), extra_urls_ui(_raw), and list variants.
      - If Evolution wired injected URLs into web_context['extra_urls'], it MUST also set
        __yureeka_extra_urls_are_injection_v1 / __yureeka_injected_urls_v1 so we can safely
        treat extra_urls as injected without misclassifying production source lists.
    """
    out: list = []
    try:
        if not isinstance(web_context, dict):
            return []
        # list variants (UI)
        _cand = web_context.get("diag_extra_urls_ui") or web_context.get("extra_urls_ui") or []
        if isinstance(_cand, str):
            # allow simple newline/comma separated
            for part in _cand.replace(",", "\n").split():
                if part.startswith("http://") or part.startswith("https://"):
                    out.append(part.strip())
            _cand = []
        if isinstance(_cand, (list, tuple)):
            out.extend([u for u in _cand if isinstance(u, str)])

        # ui_raw string variants
        _ui_raw = web_context.get("diag_extra_urls_ui_raw") or web_context.get("extra_urls_ui_raw") or ""
        if isinstance(_ui_raw, str) and _ui_raw.strip():
            for part in _ui_raw.replace(",", "\n").split():
                if part.startswith("http://") or part.startswith("https://"):
                    out.append(part.strip())

        # explicit internal marker fallback (set by Evolution wiring)
        _marked = bool(web_context.get("__yureeka_extra_urls_are_injection_v1"))
        _marked_list = web_context.get("__yureeka_injected_urls_v1")
        if _marked and isinstance(_marked_list, (list, tuple)):
            out.extend([u for u in _marked_list if isinstance(u, str)])
        if _marked and isinstance(web_context.get("extra_urls"), (list, tuple)):
            out.extend([u for u in web_context.get("extra_urls") if isinstance(u, str)])

        # normalize / de-dup (keep original strings, but stable ordering)
        out = [u.strip() for u in out if isinstance(u, str) and u.strip()]
        _seen = set()
        _uniq = []
        for u in out:
            if u not in _seen:
                _seen.add(u)
                _uniq.append(u)
        out = _uniq

        # canonicalize for stability where possible (strip tracking params)
        try:
            _norm = _inj_diag_norm_url_list(out)
            if isinstance(_norm, list) and _norm:
                # keep normalized, but only if it doesn't erase all
                out = _norm
        except Exception:
            pass

        return out
    except Exception:
        return []

def _inj_diag_set_hash(urls: list) -> str:
    """Stable sha256 of sorted URL list (for compact logging)."""
    try:
        lst = [str(u or "").strip() for u in (urls or []) if str(u or "").strip()]
        lst = sorted(set(lst))
        payload = "|".join(lst)
        return hashlib.sha256(payload.encode("utf-8")).hexdigest()
    except Exception:
        return ""

def _inj_diag_hash_inputs_from_bsc(baseline_sources_cache: Any) -> list:
    """Extract deterministic URL inputs used by snapshot hashing (v1/v2 both include URL)."""
    urls = []
    try:
        if not isinstance(baseline_sources_cache, list):
            return []
        for sr in baseline_sources_cache:
            if not isinstance(sr, dict):
                continue
            u = (sr.get("source_url") or sr.get("url") or "").strip()
            if u:
                urls.append(u)
    except Exception:
        return []
    return sorted(set(urls))

# Default behavior is OFF to avoid disrupting locked fastpath.
#
# When enabled, injected URLs that were persisted (per diag_injected_urls.persisted*)
# but are missing from baseline_sources_cache will be added as *synthetic* source
# records (url-only) so that:
#   - source_snapshot_hash (v1/v2) reflects injected sources deterministically
#   - evolution rebuild sees the same snapshot pool and hash identity via persistence
#
# Safety:
#   - Does NOT modify fastpath logic.
#   - Does NOT change metric selection (synthetic records have no extracted_numbers).
#   - Only activates when INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH is True.

def _inj_hash_should_include() -> bool:
    """Single switch for inclusion; additive-only. Supports env override."""
    try:
        v = os.getenv("YUREEKA_INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH", "").strip().lower()
        if v in ("1", "true", "yes", "y", "on"):
            return True
        if v in ("0", "false", "no", "n", "off"):
            return False
    except Exception:
        return bool(globals().get("INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH", False))

# Goal:
#   - Align injected URL "new data" identity semantics with baseline sources:
#       If an injected URL is PERSISTED as a successful snapshot, it should
#       participate in snapshot hash inputs by default (unless explicitly disabled).
#   - Preserve existing safety switch INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH
#     and its env override for emergency forcing.
#
# Controls:
#   - Default behavior (policy-aligned): ON when persisted injected URLs exist.
#   - Explicit disable: env YUREEKA_EXCLUDE_INJECTED_URLS_FROM_SNAPSHOT_HASH=1
#   - Explicit force include: env YUREEKA_INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH=1
#
# Notes:
#   - Fastpath logic is NOT modified.
#   - This only affects hash identity input construction; metric selection remains unchanged.

def _inj_hash_policy_explicit_disable() -> bool:
    try:
        v = os.getenv("YUREEKA_EXCLUDE_INJECTED_URLS_FROM_SNAPSHOT_HASH", "").strip().lower()
        return v in ("1", "true", "yes", "y", "on")
    except Exception:
        return False

def _inj_hash_policy_should_include(persisted_injected_urls) -> bool:
    """Policy-aligned include decision for injected URLs in hash identity.

    - If explicitly disabled via env, returns False.
    - If explicitly forced via existing switch/env, returns True.
    - Otherwise, when policy-align is enabled and persisted injected URLs exist, returns True.
    - Else, falls back to legacy _inj_hash_should_include().
    """
    try:
        if _inj_hash_policy_explicit_disable():
            return False
        # Respect existing forcing mechanism first
        if _inj_hash_should_include():
            return True
        if bool(globals().get("INJECTED_URL_HASH_POLICY_ALIGN_WITH_BASELINE", True)) and (persisted_injected_urls or []):
            return True
    except Exception:
        return _inj_hash_should_include()

def _inj_hash_add_synthetic_sources(
    baseline_sources_cache: Any,
    injected_persisted_urls: list,
    now_iso: str = ""
) -> tuple:
    """
    Return (bsc_augmented, added_urls, reasons_by_url) without mutating the original list.
    Synthetic records are url-only, deterministic, and safe for selection logic.
    """
    reasons = {}
    added = []
    try:
        bsc = list(baseline_sources_cache) if isinstance(baseline_sources_cache, list) else []
        inj = _inj_diag_norm_url_list(injected_persisted_urls or [])
        if not inj:
            return (bsc, added, reasons)

        existing = set(_inj_diag_hash_inputs_from_bsc(bsc))
        for u in inj:
            if u in existing:
                reasons[u] = "present_in_bsc"
                continue
            # Add synthetic source record (no numbers) so hash includes the URL deterministically
            added.append(u)
            reasons[u] = "added_synthetic_for_hash"
            bsc.append({
                "url": u,
                "source_url": u,
                "status": "fetched",
                "status_detail": "synthetic_injected_for_hash",
                "numbers_found": 0,
                "fetched_at": now_iso or "",
                "fingerprint": "",
                "extracted_numbers": [],
                "__inj_synthetic": True,
            })

        # Keep deterministic ordering identical to existing conventions
        bsc = sorted(bsc, key=lambda x: str((x or {}).get("url") or ""))
        return (bsc, added, reasons)
    except Exception:
        pass
        try:
            return (list(baseline_sources_cache) if isinstance(baseline_sources_cache, list) else [], [], {})
        except Exception:
            return ([], [], {})

# Purpose:
# - Make injected URL admission deterministic & auditable across Analysis/Evolution.
# - Promote UI raw/diag fields into web_context['extra_urls'] (the admission input).
# - Synthesize a minimal web_context['diag_injected_urls'] when fetch_web_context was bypassed.
# - Pure wiring/diagnostics only: no scraping, no selection changes.

def _fix2d66_collect_injected_urls(web_context: dict, question_text: str = "") -> list:
    """Collect **only user-intended** injected URLs.

    Important: do NOT treat generic wc['extra_urls'] (often used for seed/forced source pools)
    as injection unless Evolution explicitly marked it with __yureeka_extra_urls_are_injection_v1.
    """
    try:
        wc = web_context if isinstance(web_context, dict) else {}
        found: list = []
        # Primary: dedicated injected-url extractor (REFACTOR32 semantics)
        try:
            found.extend(_yureeka_extract_injected_urls_v1(wc))
        except Exception:
            pass
        # Last resort: URL pasted into the prompt/question text
        if isinstance(question_text, str) and question_text.strip():
            try:
                found.extend(_fix2d66_extract_urls_from_text(question_text))
            except Exception:
                pass
        # Normalize + stable de-dup
        try:
            norm = _inj_diag_norm_url_list(found)
        except Exception:
            norm = [u for u in found if isinstance(u, str) and (u.startswith("http://") or u.startswith("https://"))]
        out: list = []
        seen = set()
        for u in norm:
            uu = str(u or "").strip()
            if not uu or uu in seen:
                continue
            seen.add(uu)
            out.append(uu)
        return out
    except Exception:
        return []

def _fix2d66_promote_injected_urls(web_context: dict, question_text: str = "", stage: str = "") -> dict:
    """Promote injected URLs into web_context in a way that cannot poison production runs.

    - Only runs when **true user injection URLs** are present (via UI fields / explicit marker / prompt URL).
    - Does NOT auto-fill diag_extra_urls_ui_raw (that field is UI-owned; writing to it can make
      production runs look like injection runs).
    """
    try:
        if not isinstance(web_context, dict):
            return web_context
        inj = _fix2d66_collect_injected_urls(web_context or {}, question_text=question_text)
        if not inj:
            return web_context

        # Mark for downstream semantics (REFACTOR32 contract)
        try:
            web_context["__yureeka_extra_urls_are_injection_v1"] = True
            web_context["__yureeka_injected_urls_v1"] = list(inj)
        except Exception:
            pass

        # Promote into extra_urls (merge, stable)
        try:
            cur = web_context.get("extra_urls")
            cur_list = list(cur) if isinstance(cur, (list, tuple)) else []
            merged = _inj_diag_norm_url_list(cur_list + list(inj))
            seen = set()
            out = []
            for u in merged:
                uu = str(u or "").strip()
                if not uu or uu in seen:
                    continue
                seen.add(uu)
                out.append(uu)
            web_context["extra_urls"] = out
        except Exception:
            try:
                web_context["extra_urls"] = list(inj)
            except Exception:
                pass

        # Ensure diag payload exists for traceability, WITHOUT mutating UI fields
        try:
            _d = web_context.get("diag_injected_urls")
            if not isinstance(_d, dict):
                _d = {}
                web_context["diag_injected_urls"] = _d
            if isinstance(_d, dict):
                try:
                    _d.setdefault("run_id", web_context.get("diag_run_id") or _inj_diag_make_run_id(stage or "run"))
                except Exception:
                    pass
                # prefer actual UI raw if present; otherwise synthesize
                _ui_raw = ""
                try:
                    _ui_raw = str(web_context.get("diag_extra_urls_ui_raw") or web_context.get("extra_urls_ui_raw") or "")
                except Exception:
                    _ui_raw = ""
                if not _ui_raw.strip():
                    _ui_raw = "\n".join(list(inj))
                _d.setdefault("ui_raw", _ui_raw)
                _d.setdefault("ui_norm", list(inj))
                _d.setdefault("intake_norm", list(inj))
                _d.setdefault("admitted", list(inj))
        except Exception:
            pass

        # lightweight debug
        try:
            web_context["__fix2d66_injected_urls_promoted_v1"] = True
            web_context["__fix2d66_injected_urls_count_v1"] = int(len(inj))
            web_context["__fix2d66_stage_v1"] = str(stage or "")
        except Exception:
            pass

        return web_context
    except Exception:
        return web_context

def _fix2d66_extract_urls_from_text(text: str) -> list:
    try:
        if not isinstance(text, str):
            return []
        t = text.strip()
        if not t:
            return []
        # conservative URL matcher
        urls = re.findall(r"https?://[^\s\]\)\}\>\"']+", t)
        return [u.strip() for u in urls if isinstance(u, str) and u.strip()]
    except Exception:
        return []

def _fix2d66_collect_injected_urls_from_wc(web_context: dict, question: str = "") -> dict:
    """Return {ui_raw, ui_norm, intake_norm} for **true injection URLs** only.

    NOTE: This MUST NOT treat seed/forced source pools (often carried in wc['extra_urls'])
    as injection unless Evolution explicitly marked them with __yureeka_extra_urls_are_injection_v1.
    """
    wc = web_context if isinstance(web_context, dict) else {}

    # UI-owned raw injection field (do not synthesize from other fields)
    ui_raw = ""
    for k in ("diag_extra_urls_ui_raw", "extra_urls_ui_raw"):
        v = wc.get(k)
        if isinstance(v, str) and v.strip():
            ui_raw = v
            break

    candidates: list = []
    try:
        candidates.extend(_yureeka_extract_injected_urls_v1(wc))
    except Exception:
        pass

    # Also allow URLs embedded in the question text (rare, but helpful for CLI/API use)
    if isinstance(question, str) and question.strip():
        try:
            candidates.extend(_fix2d66_extract_urls_from_text(question))
        except Exception:
            pass

    try:
        norm = _inj_diag_norm_url_list(candidates)
    except Exception:
        norm = [x for x in candidates if isinstance(x, str) and x.strip()]

    return {
        "ui_raw": ui_raw,
        "ui_norm": list(norm),
        "intake_norm": list(norm),
    }

def _fix2d66_promote_injection_in_web_context(web_context: dict, question: str = "") -> dict:
    """Mutate web_context in-place: ensure extra_urls + diag_injected_urls reflect UI/raw injection."""
    wc = web_context if isinstance(web_context, dict) else {}
    info = _fix2d66_collect_injected_urls_from_wc(wc, question=question)
    intake = info.get('intake_norm') or []

    # Mark for downstream semantics (REFACTOR32 contract)
    try:
        if intake:
            wc['__yureeka_extra_urls_are_injection_v1'] = True
            wc['__yureeka_injected_urls_v1'] = list(intake)
    except Exception:
        pass

    # Promote into extra_urls when missing/empty
    try:
        cur = wc.get('extra_urls')
        needs = (not isinstance(cur, (list, tuple)) or not cur)
        if needs and intake:
            wc['extra_urls'] = list(intake)
    except Exception:
        pass

    # Ensure diag_injected_urls exists even if fetch_web_context wasn't called
    try:
        wc.setdefault('diag_injected_urls', {})
        if isinstance(wc.get('diag_injected_urls'), dict):
            d = wc['diag_injected_urls']
            # Fill minimally if absent
            d.setdefault('run_id', wc.get('diag_run_id') or _inj_diag_make_run_id('inj'))
            if info.get('ui_raw') and not d.get('ui_raw'):
                d['ui_raw'] = info.get('ui_raw')
            d.setdefault('ui_norm', info.get('ui_norm') or [])
            d.setdefault('intake_norm', intake)
            # If no admitted present, treat intake as admitted universe (diagnostic only)
            if not isinstance(d.get('admitted'), list) or not d.get('admitted'):
                d['admitted'] = list(intake)
    except Exception:
        pass

    # breadcrumb
    try:
        wc.setdefault('debug', {})
        if isinstance(wc.get('debug'), dict):
            wc['debug'].setdefault('fix2d66_injection', {})
            if isinstance(wc['debug'].get('fix2d66_injection'), dict):
                wc['debug']['fix2d66_injection'].update({
                    'promoted': bool(intake),
                    'intake_count': int(len(intake)),
                    'intake_set_hash': _inj_diag_set_hash(intake) if intake else '',
                })
    except Exception:
        return wc

# Purpose:
# - Populate inj_trace_v1 attempted/persisted fields from *real* artifacts when
#   the upstream diag_injected_urls payload is partial (common in baseline/no-injection
#   or fastpath replay scenarios).
# - Pure diagnostics only: does NOT alter control flow, hashing, scraping, or selection.
#
# Artifacts supported:
#   - baseline_sources_cache (BSC): list of per-url snapshot dicts
#   - scraped_meta: dict keyed by url with status/status_detail/clean_text_len

def _inj_trace_v1_enrich_diag_from_bsc(diag: dict, baseline_sources_cache: list) -> dict:
    """Add attempted/persisted evidence into diag_injected_urls from baseline_sources_cache."""
    try:
        d = diag if isinstance(diag, dict) else {}
        bsc = baseline_sources_cache if isinstance(baseline_sources_cache, list) else []
        # If attempted already present, do not overwrite (avoid clobbering richer traces).
        if not isinstance(d.get("attempted"), list) or not d.get("attempted"):
            attempted = []
            for row in bsc:
                if not isinstance(row, dict):
                    continue
                u = str(row.get("url") or row.get("source_url") or "").strip()
                if not u:
                    continue
                st = str(row.get("status") or row.get("fetch_status") or "").strip() or "unknown"
                rs = str(row.get("status_detail") or row.get("fail_reason") or "").strip()
                clen = row.get("clean_text_len") or row.get("content_len") or 0
                try:
                    clen_i = int(clen)
                except Exception:
                    pass
                    clen_i = 0
                attempted.append({"url": u, "status": st, "reason": rs, "content_len": clen_i})
            if attempted:
                d["attempted"] = attempted

        # Persisted: if missing, derive from successful snapshot rows in BSC.
        if not isinstance(d.get("persisted"), (list, str)) or not d.get("persisted"):
            persisted = []
            for row in bsc:
                if not isinstance(row, dict):
                    continue
                u = str(row.get("url") or row.get("source_url") or "").strip()
                if not u:
                    continue
                st = str(row.get("status") or row.get("fetch_status") or "").lower().strip()
                if st in ("success", "ok", "fetched"):
                    persisted.append(u)
            if persisted:
                d["persisted"] = _inj_diag_norm_url_list(persisted)

        return d
    except Exception:
        return diag if isinstance(diag, dict) else {}

def _inj_trace_v1_enrich_diag_from_scraped_meta(diag: dict, scraped_meta: dict, extra_urls: list) -> dict:
    """Add attempted/persisted evidence into diag_injected_urls from scraped_meta (evolution-side)."""
    try:
        d = diag if isinstance(diag, dict) else {}
        sm = scraped_meta if isinstance(scraped_meta, dict) else {}
        xs = _inj_diag_norm_url_list(extra_urls or [])
        if not xs:
            return d

        # attempted rows for injected urls
        if not isinstance(d.get("attempted"), list) or not d.get("attempted"):
            attempted = []
            for u in xs:
                m = sm.get(u) if isinstance(sm.get(u), dict) else {}
                st = str(m.get("status") or m.get("fetch_status") or "").strip() or "unknown"
                rs = str(m.get("status_detail") or m.get("fail_reason") or "").strip()
                clen = m.get("clean_text_len") or m.get("content_len") or 0
                try:
                    clen_i = int(clen)
                except Exception:
                    pass
                    clen_i = 0
                attempted.append({"url": u, "status": st, "reason": rs, "content_len": clen_i})
            d["attempted"] = attempted

        # persisted success urls (only for injected)
        if not isinstance(d.get("persisted"), (list, str)) or not d.get("persisted"):
            persisted = []
            for a in (d.get("attempted") or []):
                if not isinstance(a, dict):
                    continue
                st = str(a.get("status") or "").lower().strip()
                if st in ("success", "ok", "fetched"):
                    u = str(a.get("url") or "").strip()
                    if u:
                        persisted.append(u)
            d["persisted"] = _inj_diag_norm_url_list(persisted)

        return d
    except Exception:
        return diag if isinstance(diag, dict) else {}

def scrape_url(url: str) -> Optional[str]:
    """
    Scrape webpage content.

    Priority:
      1) ScrapingDog (if SCRAPINGDOG_KEY is present)
      2) Safe fallback: direct requests + BeautifulSoup visible-text extraction

    Returns:
      - Clean visible text (<= 3000 chars) or None
    """

    url_s = (url or "").strip()
    if not url_s:
        return None

    def _clean_html_to_text(html: str) -> str:
        try:
            soup = BeautifulSoup(html or "", "html.parser")
            for tag in soup(["script", "style", "nav", "footer", "header", "aside", "noscript", "form"]):
                try:
                    tag.decompose()
                except Exception:
                    pass
            txt = soup.get_text(separator="\n")
        except Exception:
            pass
            # fallback: strip tags
            txt = re.sub(r"(?is)<(script|style|noscript).*?>.*?</\1>", " ", html or "")
            txt = re.sub(r"(?is)<[^>]+>", " ", txt)
        # normalize whitespace
        lines = [ln.strip() for ln in (txt or "").splitlines() if ln.strip()]
        out = "\n".join(lines)
        out = re.sub(r"\n{3,}", "\n\n", out)
        return out.strip()

    def _direct_fetch(u: str) -> Optional[str]:
        try:
            headers = {
                "User-Agent": (
                    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/120.0.0.0 Safari/537.36"
                ),
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9",
                "Cache-Control": "no-cache",
                "Pragma": "no-cache",
            }
            resp = requests.get(u, headers=headers, timeout=12, allow_redirects=True)
            if resp.status_code >= 400:
                return None

            ctype = (resp.headers.get("Content-Type") or "").lower()

            ctype = (resp.headers.get("Content-Type") or "").lower()
            # REFACTOR85: handle PDFs (IEA/others) instead of treating them as failed:no_text
            if ("application/pdf" in ctype) or u.lower().endswith(".pdf"):
                try:
                    _pdf_txt = _extract_pdf_text_from_bytes(resp.content, max_pages=8, max_chars=7000)
                    if isinstance(_pdf_txt, str) and _pdf_txt.strip():
                        return _pdf_txt.strip()[:7000]
                except Exception:
                    return None
                return None

            cleaned = _clean_html_to_text(resp.text or "")
            cleaned = cleaned.strip()
            if not cleaned:
                return None
            return cleaned[:3000]
        except Exception:
            return None

    # 1) ScrapingDog path (if configured)
    if globals().get("SCRAPINGDOG_KEY"):
        try:
            params = {"api_key": SCRAPINGDOG_KEY, "url": url_s, "dynamic": "false"}
            resp = requests.get("https://api.scrapingdog.com/scrape", params=params, timeout=15)
            if resp.status_code < 400:
                cleaned = _clean_html_to_text(resp.text or "").strip()
                if cleaned:
                    return cleaned[:3000]
        except Exception:
            pass  # fall through to direct fetch

    # 2) Safe fallback
    return _direct_fetch(url_s)

def fetch_web_context(
    query: str,
    num_sources: int = 3,
    *,
    fallback_mode: bool = False,
    fallback_urls: list = None,
    existing_snapshots: Any = None,   # <-- ADDITIVE
    extra_urls: Any = None,
    diag_run_id: str = "",
    diag_extra_urls_ui_raw: Any = None,
    identity_only: bool = False,
    # - Default False: no behavior change.
    # - When True: normalized extra URLs will be appended to admitted list for scraping.
    force_scrape_extra_urls: bool = False,
    # - Default False: no behavior change.
    # - When True: normalized extra URLs will be appended to admitted list (not just scrape list),
    #   enabling deterministic admission of injected URLs when delta exists.
    force_admit_extra_urls: bool = False,
) -> dict:

    """
    Web context collector used by BOTH analysis + evolution.

    Enhancements:
    - Dashboard telemetry (sources found / HQ / admitted / scraped / success)
    - Keeps snapshot-friendly scraped_meta (fingerprint + extracted_numbers + numbers_found)
    - Uses scrape_url() which now has ScrapingDog + safe fallback scraper
    - Restores legacy contract: web_context["sources"] AND ["web_sources"]
    """
    # REFACTOR129: reset run-level beacons for overlap suppression + precision tiebreak
    global _REFACTOR129_DECIMAL_OVERLAP_SUPPRESSION_V1, _REFACTOR129_PRECISION_TIEBREAK_V1, _REFACTOR131_INJECTED_SEMANTIC_OVERRIDE_V1, _REFACTOR132_INJECTED_CAGR_RESCUE_V1
    try:
        _REFACTOR129_DECIMAL_OVERLAP_SUPPRESSION_V1 = {"total_numbers": 0, "overlaps_detected": 0, "overlaps_dropped": 0, "sample_drops": []}
        _REFACTOR129_PRECISION_TIEBREAK_V1 = {}
        _REFACTOR131_INJECTED_SEMANTIC_OVERRIDE_V1 = {"overrides": 0, "samples": [], "strict_injected_year_tokens_filtered": 0}
        _REFACTOR132_INJECTED_CAGR_RESCUE_V1 = {"applied": 0, "samples": []}
    except Exception:
        pass

    try:
        _qtxt = str((analysis or {}).get('question') or (analysis or {}).get('query') or '')
        web_context = _fix2d66_promote_injected_urls(web_context or {}, question_text=_qtxt, stage='analysis_attach')
    except Exception:
        pass

    def _is_probably_url(s: str) -> bool:
        if not s or not isinstance(s, str):
            return False
        t = s.strip()
        if " " in t:
            return False
        if re.match(r"^https?://", t, flags=re.I):
            return True
        if re.match(r"^[a-z0-9.-]+\.[a-z]{2,}(/.*)?$", t, flags=re.I):
            return True
        return False

    def _normalize_url(s: str) -> str:
        t = (s or "").strip()
        if not t:
            return ""
        if re.match(r"^https?://", t, flags=re.I):
            return t
        if re.match(r"^[a-z0-9.-]+\.[a-z]{2,}(/.*)?$", t, flags=re.I):
            return "https://" + t
        return ""

    out = {
        "query": query,
        "sources": [],        # ✅ legacy key many downstream blocks expect
        "web_sources": [],    # ✅ newer key used by evolution/snapshots
        "search_results": [],
        "scraped_meta": {},
        "scraped_content": {},
        "errors": [],
        "status": "ok",
        "status_detail": "",
        "fetched_at": _yureeka_now_iso_v1(),
        "debug_counts": {},   # ✅ telemetry for dashboard + JSON debugging
    }

    # Record explicit last-good snapshot fallback usage (never silent).
    def _record_last_good_fallback(_url: str) -> None:
        try:
            if not isinstance(_url, str) or not _url.strip():
                return
            out.setdefault("debug_counts", {})
            out["debug_counts"]["fallback_last_good_snapshot_used"] = int(out["debug_counts"].get("fallback_last_good_snapshot_used") or 0) + 1
            out["debug_counts"].setdefault("fallback_last_good_snapshot_used_urls", [])
            if _url not in out["debug_counts"]["fallback_last_good_snapshot_used_urls"]:
                out["debug_counts"]["fallback_last_good_snapshot_used_urls"].append(_url)
        except Exception:
            pass

    snap_lookup = {}
    try:
        def _snap_variants(u: str) -> list:
            try:
                if not isinstance(u, str):
                    return []
                u0 = u.strip()
                if not u0:
                    return []
                nu = _normalize_url(u0) or u0
                cands = []
                for x in (u0, nu):
                    if isinstance(x, str) and x.strip():
                        cands.append(x.strip())

                outv = []
                for x in cands:
                    x2 = x.rstrip("/")
                    if x2 and x2 not in outv:
                        outv.append(x2)
                    if x2 and (x2 + "/") not in outv:
                        outv.append(x2 + "/")

                more = []
                for x in list(outv):
                    if x.lower().startswith("https://"):
                        more.append("http://" + x[8:])
                    elif x.lower().startswith("http://"):
                        more.append("https://" + x[7:])
                for x in more:
                    if x and x not in outv:
                        outv.append(x)

                final = []
                for x in outv:
                    final.append(x)
                    try:
                        if x.lower().startswith("https://www."):
                            final.append("https://" + x[len("https://www."):])
                        elif x.lower().startswith("http://www."):
                            final.append("http://" + x[len("http://www."):])
                    except Exception:
                        pass

                seen = set()
                uniq = []
                for x in final:
                    if not isinstance(x, str):
                        continue
                    t = x.strip()
                    if not t or t in seen:
                        continue
                    seen.add(t)
                    uniq.append(t)
                return uniq
            except Exception:
                return [u] if isinstance(u, str) else []

        def _choose_better(a: dict, b: dict) -> dict:
            """Pick the better snapshot (prefer more numbers; then newer fetched_at)."""
            try:
                if not isinstance(a, dict):
                    return b
                if not isinstance(b, dict):
                    return a
                an = int(a.get("numbers_found") or 0)
                bn = int(b.get("numbers_found") or 0)
                if bn != an:
                    return b if bn > an else a
                af = str(a.get("fetched_at") or "")
                bf = str(b.get("fetched_at") or "")
                if bf and af and bf != af:
                    return b if bf > af else a
            except Exception:
                pass
            return a

        _cands = []
        if isinstance(existing_snapshots, dict):
            for _k, _v in (existing_snapshots or {}).items():
                if isinstance(_v, dict):
                    if not _v.get("url") and isinstance(_k, str) and _k.strip():
                        _vv = dict(_v)
                        _vv["url"] = _k.strip()
                        _v = _vv
                    if _v.get("url"):
                        _cands.append(_v)
        elif isinstance(existing_snapshots, list):
            for _v in (existing_snapshots or []):
                if isinstance(_v, dict) and _v.get("url"):
                    _cands.append(_v)

        for s in _cands:
            try:
                u0 = str(s.get("url") or "").strip()
                if not u0:
                    continue
                for key in _snap_variants(u0):
                    if not key:
                        continue
                    if key not in snap_lookup:
                        snap_lookup[key] = s
                    else:
                        snap_lookup[key] = _choose_better(snap_lookup.get(key), s)
            except Exception:
                continue

        def _get_prev_snapshot(u: str):
            try:
                if not isinstance(u, str) or not u.strip():
                    return None
                if not isinstance(snap_lookup, dict):
                    return None
                for key in _snap_variants(u):
                    if key in snap_lookup:
                        return snap_lookup.get(key)
                return snap_lookup.get(u.strip())
            except Exception:
                return None
    except Exception:
        snap_lookup = snap_lookup if isinstance(snap_lookup, dict) else {}
        def _get_prev_snapshot(u: str):
            try:
                return snap_lookup.get(u) if isinstance(snap_lookup, dict) else None
            except Exception:
                return None

    extractor_fp = get_extractor_fingerprint()

    q = (query or "").strip()
    if not q:
        out["status"] = "no_query"
        out["status_detail"] = "empty_query"
        return out

    # 1) Search (SerpAPI) OR fallback_urls
    search_results = []
    urls_raw = []

    if not fallback_mode:
        try:
            sr = search_serpapi(q, num_results=10) or []
            if isinstance(sr, list):
                search_results = sr
        except Exception as e:
            out["errors"].append(f"search_failed:{type(e).__name__}")
            search_results = []

        out["search_results"] = search_results

        # Extract urls from results
        for r in (search_results or []):
            if isinstance(r, dict):
                u = (r.get("link") or r.get("url") or "").strip()
                if _is_probably_url(u):
                    urls_raw.append(u)
            elif isinstance(r, str):
                if _is_probably_url(r):
                    urls_raw.append(r.strip())

    else:
        # Evolution fallback: use provided URLs
        if isinstance(fallback_urls, list):
            for u in fallback_urls:
                if isinstance(u, str) and _is_probably_url(u.strip()):
                    urls_raw.append(u.strip())

    # 2) Compute "HQ" counts (like old version)
    total_found = len(search_results) if not fallback_mode else len(urls_raw)
    hq_count = 0

    try:
        fn_rel = globals().get("classify_source_reliability")
        if callable(fn_rel) and not fallback_mode:
            for r in (search_results or []):
                if not isinstance(r, dict):
                    continue
                u = (r.get("link") or "").strip()
                if not u:
                    continue
                label = fn_rel(u) or ""
                if "✅" in str(label):
                    hq_count += 1
    except Exception:
        pass
        hq_count = 0

    # 3) Sanitize + normalize + dedupe
    normed = []
    seen = set()
    for u in (urls_raw or []):
        nu = _normalize_url(u)
        if not nu:
            continue
        if nu in seen:
            continue
        seen.add(nu)
        normed.append(nu)


    # LLM02 (optional): Source triage / de-dup clustering (NLP-first; OFF by default).
    # When enabled, we reorder candidate URLs so cluster primaries appear first.
    try:
        _cl_on, _cl_src = _yureeka_llm_flag_effective_v1("ENABLE_LLM_SOURCE_CLUSTERING")
        if bool(_cl_on) and (not bool(fallback_mode)) and isinstance(normed, list) and normed and isinstance(search_results, list) and search_results:
            _normed2, _cl_dbg = _llm02_cluster_urls_v1(normed, search_results)
            if isinstance(_normed2, list) and _normed2:
                normed = _normed2
            try:
                out.setdefault("debug", {})
                if isinstance(out.get("debug"), dict):
                    _cl_dbg = dict(_cl_dbg or {})
                    _cl_dbg["enabled_source"] = str(_cl_src)[:80]
                    out["debug"]["source_cluster_v1"] = _cl_dbg
            except Exception:
                pass
    except Exception:
        pass

    # admitted for scraping (top N)
    try:
        n = int(num_sources or 3)
    except Exception:
        pass
        n = 3
    n = max(1, min(12, n))
    admitted = normed[:n] if not fallback_mode else normed  # fallback_mode typically wants all

    #
    # Why:
    # - In evolution injection scenarios, extra URLs may be deliberately outside the normal
    #   admitted universe (domain allowlists, heuristics, etc.), but the user's intent is
    #   to attempt a fetch so the run can either persist a snapshot or fail with a concrete reason.
    #
    # Behavior:
    # - When force_scrape_extra_urls=True and normalized extras exist, append them into the
    #   admitted list (deduped, stable order) so downstream scraping attempts occur.
    #
    # Safety:
    # - Default is False (no change for normal runs).
    # - Never raises.
    try:
        if bool(force_scrape_extra_urls):
            _fx8_extras = []
            if "_extras" in locals() and isinstance(_extras, list):
                _fx8_extras = [u for u in _extras if isinstance(u, str) and u.strip()]
            if _fx8_extras and isinstance(admitted, list):
                _seen = set([u for u in admitted if isinstance(u, str)])
                for _u in _fx8_extras:
                    if _u not in _seen:
                        admitted.append(_u)
                        _seen.add(_u)
                # breadcrumb for diagnostics
                try:
                    out.setdefault("debug_counts", {})
                    if isinstance(out.get("debug_counts"), dict):
                        out["debug_counts"].setdefault("fix41afc8", {})
                        if isinstance(out["debug_counts"].get("fix41afc8"), dict):
                            out["debug_counts"]["fix41afc8"].update({
                                "force_scrape_extra_urls": True,
                                "force_scrape_extra_urls_count": int(len(_fx8_extras)),
                            })
                except Exception:
                    pass
    except Exception:
        pass

    try:
        _extras_in = extra_urls or []
        _extras = []
        _canon_map = {}
        if isinstance(_extras_in, str):
            _extras_in = [u.strip() for u in _extras_in.splitlines()]
        if isinstance(_extras_in, (list, tuple)):
            for u in _extras_in:
                u = str(u or "").strip()
                if not u:
                    continue
                if not (u.startswith("http://") or u.startswith("https://")):
                    continue
                _canon = _canonicalize_injected_url(u) or u
                _extras.append(_canon)
                try:
                    _canon_map[u] = _canon
                except Exception:
                    pass
        _seen = set()
        merged = []
        for u in _extras + (admitted or []):
            if u in _seen:
                continue
            _seen.add(u)
            merged.append(u)
        admitted = merged
        out.setdefault("debug", {})
        if isinstance(out.get("debug"), dict):
            out["debug"].setdefault("fwc_extra_urls", {})
            out["debug"]["fwc_extra_urls"]["extra_urls_count"] = int(len(_extras))
            out["debug"]["fwc_extra_urls"]["admitted_count_after_merge"] = int(len(admitted or []))
            out["debug"]["fwc_extra_urls"]["extra_urls"] = _extras[:20]
    except Exception:
        pass

    # Records: UI->intake->admitted, and later enriches with scrape outcomes.
    try:
        _diag_run = str(diag_run_id or "") or _inj_diag_make_run_id("analysis")
        out["diag_run_id"] = out.get("diag_run_id") or _diag_run

        _ui_raw = diag_extra_urls_ui_raw if diag_extra_urls_ui_raw is not None else extra_urls
        _ui_norm = _inj_diag_norm_url_list(_ui_raw)
        _intake_norm = list(_extras or []) if "_extras" in locals() and isinstance(_extras, list) else _inj_diag_norm_url_list(extra_urls)

        out["diag_injected_urls"] = {
            "run_id": _diag_run,
            "ui_raw": _ui_raw if isinstance(_ui_raw, (str, list, tuple)) else str(_ui_raw or ""),
            "ui_norm": _ui_norm,
            "intake_norm": _intake_norm,
            "admitted": list(admitted or []),
            "attempted": [],
            "persisted": [],
            "hash_inputs": [],
            "rebuild_pool": [],
            "rebuild_selected": [],
            "set_hashes": {
                "ui_norm": _inj_diag_set_hash(_ui_norm),
                "intake_norm": _inj_diag_set_hash(_intake_norm),
                "admitted": _inj_diag_set_hash(list(admitted or [])),
            },
            "canon_map": dict(_canon_map) if "_canon_map" in locals() else {},
            "deltas": {
                "ui_minus_intake": sorted(list(set(_ui_norm) - set(_intake_norm))),
                "intake_minus_admitted": sorted(list(set(_intake_norm) - set(list(admitted or [])))),
            },
        }
    except Exception:
        pass

    #
    # Goal:
    # - When force_admit_extra_urls is True, ensure normalized extra_urls are INCLUDED in the
    #   admitted list itself (not only the scrape list). This prevents injected URLs from dying
    #   at "intake_minus_admitted" and allows deterministic fetch/persist behavior.
    #
    # Safety:
    # - Default flag False => no behavior change.
    # - Never raises.
    try:
        if force_admit_extra_urls:
            _fix41afc13_extra = _inj_diag_norm_url_list(extra_urls) if extra_urls else []
            if _fix41afc13_extra:
                _fix41afc13_before = list(admitted or [])
                _fix41afc13_set = set(_inj_diag_norm_url_list(_fix41afc13_before))
                _fix41afc13_added = []
                for _u in _fix41afc13_extra:
                    if _u and _u not in _fix41afc13_set:
                        _fix41afc13_before.append(_u)
                        _fix41afc13_set.add(_u)
                        _fix41afc13_added.append(_u)
                if _fix41afc13_added:
                    admitted = _fix41afc13_before
                    out.setdefault("debug_counts", {})
                    if isinstance(out.get("debug_counts"), dict):
                        out["debug_counts"].update({
                            "forced_admit_extra_urls_count": int(len(_fix41afc13_added)),
                        })
                    out.setdefault("debug", {})
                    if isinstance(out.get("debug"), dict):
                        out["debug"].setdefault("fix41afc13", {})
                        if isinstance(out["debug"].get("fix41afc13"), dict):
                            out["debug"]["fix41afc13"].update({
                                "forced_admit_applied": True,
                                "forced_admit_added": list(_fix41afc13_added),
                                "forced_admit_total_extra": int(len(_fix41afc13_extra)),
                            })
    except Exception:
        pass

    out["sources"] = admitted
    out["web_sources"] = admitted

    # Telemetry before scrape
    out["debug_counts"].update({
        "total_found": int(total_found),
        "high_quality": int(hq_count),
        "admitted_for_scraping": int(len(admitted)),
        "fallback_mode": bool(fallback_mode),
    })

    # Dashboard info (restored)
    try:
        if not fallback_mode:
            st.info(
                f"🔍 Sources Found: **{out['debug_counts']['total_found']} total** | "
                f"**{out['debug_counts']['high_quality']} high-quality** | "
                f"Scraping **{out['debug_counts']['admitted_for_scraping']}**"
            )
        else:
            st.info(
                f"🧩 Fallback Sources: **{out['debug_counts']['admitted_for_scraping']}** (no SerpAPI search)"
            )
    except Exception:
        pass

    if not admitted:
        out["status"] = "no_sources"
        out["status_detail"] = "empty_sources_after_filter"
        return out

    try:
        if bool(identity_only):
            out["status"] = out.get("status") or "ok"
            out["status_detail"] = out.get("status_detail") or "identity_only"
            return out
    except Exception:
        pass

    # 4) Scrape + extract numbers (snapshot-friendly scraped_meta)
    fn_fp = globals().get("fingerprint_text")
    fn_extract = globals().get("extract_numbers_with_context")

    scraped_attempted = 0
    scraped_ok_text = 0
    scraped_ok_numbers = 0
    scraped_failed = 0

    # optional progress bar
    progress = None
    try:
        progress = st.progress(0)
    except Exception:
        pass
        progress = None

    for i, url in enumerate(admitted):
        scraped_attempted += 1

        meta = {
            "url": url,
            "fetched_at": _yureeka_now_iso_v1(),
            "status": "failed",
            "status_detail": "",
            "content_type": "",
            "content_len": 0,
            "clean_text_len": 0,
            "fingerprint": None,
            "numbers_found": 0,
            "extracted_numbers": [],
            "content": "",
            "clean_text": "",
        }

        try:
            text = scrape_url(url)  # ✅ ScrapingDog + fallback inside scrape_url
            if not text or not str(text).strip():
                meta["status"] = "failed"
                meta["status_detail"] = "failed:no_text"

                # Why:
                # - External flakiness can yield failed:no_text even for stable sources.
                # - If existing_snapshots contains a last-good snapshot for this URL,
                #   reuse its extracted_numbers with explicit provenance (never silent).
                try:
                    _prev = _get_prev_snapshot(url) if isinstance(snap_lookup, dict) else None
                    _prev_nums = _prev.get("extracted_numbers") if isinstance(_prev, dict) else None
                    if isinstance(_prev_nums, list) and _prev_nums:
                        meta["status"] = "fetched"
                        meta["status_detail"] = "fallback:last_good_snapshot"
                        meta["fallback_used"] = True
                        meta["fallback_reason"] = "failed:no_text"
                        meta["fallback_source"] = "existing_snapshots"
                        meta["fallback_snapshot_fetched_at"] = _prev.get("fetched_at") if isinstance(_prev, dict) else None
                        meta["reused_snapshot"] = True

                        # Reuse snapshot identity so downstream snapshot hashes remain deterministic.
                        meta["fingerprint"] = _prev.get("fingerprint") if isinstance(_prev, dict) else meta.get("fingerprint")
                        meta["extractor_fingerprint"] = _prev.get("extractor_fingerprint") if isinstance(_prev, dict) else meta.get("extractor_fingerprint")
                        if not meta.get("extractor_fingerprint"):
                            meta["extractor_fingerprint"] = extractor_fp

                        meta["extracted_numbers"] = list(_prev_nums)
                        meta["numbers_found"] = len(_prev_nums)

                        # Best-effort: keep content fields non-empty if the snapshot had them.
                        _pc = (_prev.get("content") if isinstance(_prev, dict) else "") or ""
                        _pt = (_prev.get("clean_text") if isinstance(_prev, dict) else "") or ""
                        meta["content"] = _pc or _pt or ""
                        meta["clean_text"] = _pt or _pc or ""
                        meta["content_len"] = len(meta.get("content") or "")
                        meta["clean_text_len"] = len(meta.get("clean_text") or "")

                        out["scraped_meta"][url] = meta
                        out["scraped_content"][url] = meta.get("clean_text") or meta.get("content") or ""

                        scraped_ok_text += 1
                        if meta.get("numbers_found", 0) > 0:
                            scraped_ok_numbers += 1
                        # REFACTOR75: we recovered; do not count as scraped_failed.
                        try:
                            _record_last_good_fallback(url)
                        except Exception:
                            pass
                        continue
                except Exception:
                    pass
                scraped_failed += 1
                out["scraped_meta"][url] = meta
            else:
                cleaned = str(text).strip()
                meta["status"] = "fetched"
                meta["status_detail"] = "success"
                meta["content"] = cleaned
                meta["clean_text"] = cleaned
                meta["content_len"] = len(cleaned)
                meta["clean_text_len"] = len(cleaned)

                # fingerprint
                try:
                    if callable(fn_fp):
                        meta["fingerprint"] = fn_fp(cleaned)
                    else:
                        meta["fingerprint"] = fingerprint_text(cleaned) if callable(globals().get("fingerprint_text")) else None
                except Exception:
                    pass
                    meta["fingerprint"] = None

                meta["extractor_fingerprint"] = extractor_fp
                prev = _get_prev_snapshot(url) if isinstance(snap_lookup, dict) else None
                if isinstance(prev, dict):
                    if prev.get("fingerprint") == meta.get("fingerprint") and prev.get("extractor_fingerprint") == extractor_fp:
                        prev_nums = prev.get("extracted_numbers")
                        if isinstance(prev_nums, list) and prev_nums:
                            meta["extracted_numbers"] = prev_nums
                            meta["numbers_found"] = len(prev_nums)
                            meta["reused_snapshot"] = True

                            out["scraped_meta"][url] = meta
                            out["scraped_content"][url] = cleaned

                            scraped_ok_text += 1
                            if meta["numbers_found"] > 0:
                                scraped_ok_numbers += 1

                            if progress:
                                try:
                                    progress.progress((i + 1) / max(1, len(admitted)))
                                except Exception:
                                    pass

                            continue
                meta["reused_snapshot"] = False

                # numeric extraction (analysis-aligned if fn exists)

                nums = []
                meta["fix2d68_extract_attempted"] = bool(callable(fn_extract))
                meta["fix2d68_extract_input_len"] = int(len(cleaned) if isinstance(cleaned, str) else 0)
                try:
                    meta["fix2d68_extract_input_head"] = (cleaned[:200] if isinstance(cleaned, str) else "")
                except Exception:
                    meta["fix2d68_extract_input_head"] = ""

                _fix2d68_errors = []
                if callable(fn_extract):
                    # Robust dispatcher: try source_url, then url, then plain. Do not fail silently.
                    for _mode in ("source_url", "url", "plain"):
                        try:
                            _tmp = None
                            if _mode == "source_url":
                                _tmp = fn_extract(cleaned, source_url=url)
                            elif _mode == "url":
                                _tmp = fn_extract(cleaned, url=url)
                            else:
                                _tmp = fn_extract(cleaned)
                            # FIX2D69A: normalize extractor return (list | (list, meta) | dict | None)
                            if _tmp is None:
                                nums = []
                            elif isinstance(_tmp, list):
                                nums = _tmp
                            elif isinstance(_tmp, tuple) and len(_tmp) >= 1 and isinstance(_tmp[0], list):
                                nums = _tmp[0]
                            elif isinstance(_tmp, dict) and isinstance(_tmp.get("extracted_numbers"), list):
                                nums = _tmp.get("extracted_numbers") or []
                            else:
                                nums = []
                            meta["fix2d68_extract_call_mode"] = _mode
                            break
                        except Exception as _e:
                            _fix2d68_errors.append({"mode": _mode, "error": repr(_e)})
                            nums = []

                if _fix2d68_errors:
                    meta["fix2d68_extract_errors"] = _fix2d68_errors

                # FIX2D69A: normalize extractor return (list | (list, meta) | None)
                _nums_norm = []
                try:
                    if nums is None:
                        _nums_norm = []
                    elif isinstance(nums, list):
                        _nums_norm = nums
                    elif isinstance(nums, tuple) and len(nums) >= 1 and isinstance(nums[0], list):
                        _nums_norm = nums[0]
                    elif isinstance(nums, dict) and isinstance(nums.get("extracted_numbers"), list):
                        _nums_norm = nums.get("extracted_numbers") or []
                    else:
                        _nums_norm = []
                except Exception:
                    _nums_norm = []
                nums = _nums_norm

                if isinstance(nums, list):
                    meta["extracted_numbers"] = nums
                    meta["numbers_found"] = len(nums)
                    # REFACTOR85: last-good numbers fallback when fetch succeeded but extractor yields 0 numbers
                    # (common when a source returns a bot-wall / placeholder page that still has text).
                    if int(meta.get("numbers_found") or 0) == 0:
                        try:
                            _prev = _get_prev_snapshot(url, existing_snapshots)
                        except Exception:
                            _prev = None
                        try:
                            _prev_nums = (_prev.get("extracted_numbers") or []) if isinstance(_prev, dict) else []
                        except Exception:
                            _prev_nums = []
                        if _prev_nums:
                            try:
                                _low = (cleaned or "").lower()
                            except Exception:
                                _low = ""
                            _looks_blocked = False
                            try:
                                if len(cleaned or "") < 500:
                                    _looks_blocked = True
                            except Exception:
                                pass
                            if not _looks_blocked:
                                for _tok in ("captcha", "access denied", "enable javascript", "cloudflare", "unusual traffic", "verify you are", "blocked", "please wait"):
                                    if _tok in _low:
                                        _looks_blocked = True
                                        break
                            if _looks_blocked:
                                try:
                                    meta["status_detail_original"] = str(meta.get("status_detail") or "")
                                except Exception:
                                    pass
                                try:
                                    meta["status_detail"] = "fallback:last_good_snapshot_numbers"
                                except Exception:
                                    pass
                                try:
                                    meta["fallback_used"] = True
                                    meta["reused_snapshot"] = True
                                    meta["fallback_reason"] = "zero_numbers_blocked_or_short_text"
                                    meta["fallback_snapshot_fetched_at"] = (_prev.get("fetched_at") if isinstance(_prev, dict) else None)
                                except Exception:
                                    pass
                                try:
                                    meta["extracted_numbers"] = _prev_nums
                                    meta["numbers_found"] = len(_prev_nums)
                                except Exception:
                                    pass

                    urlv = meta.get("url") or url
                    fpv = meta.get("fingerprint") or ""

                    for n in (meta["extracted_numbers"] or []):
                        if isinstance(n, dict):
                            if "extracted_number_id" not in n:
                                n["extracted_number_id"] = make_extracted_number_id(urlv, fpv, n)
                            if not n.get("source_url"):
                                n["source_url"] = urlv

                    meta["extracted_numbers"] = sort_snapshot_numbers(meta["extracted_numbers"])
                    meta["numbers_found"] = len(meta["extracted_numbers"])

                # If extraction yields zero numbers but a last-good snapshot has numbers,
                # reuse them with explicit provenance (never silent).
                try:
                    if int(meta.get("numbers_found") or 0) <= 0:
                        _prev = _get_prev_snapshot(url) if callable(locals().get("_get_prev_snapshot")) else (snap_lookup.get(url) if isinstance(snap_lookup, dict) else None)
                        _prev_nums = _prev.get("extracted_numbers") if isinstance(_prev, dict) else None
                        if isinstance(_prev_nums, list) and _prev_nums:
                            meta["status"] = "fetched"
                            meta["status_detail"] = "fallback:last_good_snapshot"
                            meta["fallback_used"] = True
                            meta["fallback_reason"] = "numbers_found=0"
                            meta["fallback_source"] = "existing_snapshots"
                            meta["fallback_snapshot_fetched_at"] = _prev.get("fetched_at") if isinstance(_prev, dict) else None
                            meta["reused_snapshot"] = True

                            meta["fingerprint"] = _prev.get("fingerprint") if isinstance(_prev, dict) else meta.get("fingerprint")
                            meta["extractor_fingerprint"] = _prev.get("extractor_fingerprint") if isinstance(_prev, dict) else meta.get("extractor_fingerprint")
                            if not meta.get("extractor_fingerprint"):
                                meta["extractor_fingerprint"] = extractor_fp

                            meta["extracted_numbers"] = list(_prev_nums)
                            meta["numbers_found"] = len(_prev_nums)

                            _pc = (_prev.get("content") if isinstance(_prev, dict) else "") or ""
                            _pt = (_prev.get("clean_text") if isinstance(_prev, dict) else "") or ""
                            if not (meta.get("content") or "").strip():
                                meta["content"] = _pc or _pt or ""
                            if not (meta.get("clean_text") or "").strip():
                                meta["clean_text"] = _pt or _pc or ""
                            meta["content_len"] = len(meta.get("content") or "")
                            meta["clean_text_len"] = len(meta.get("clean_text") or "")

                            try:
                                _record_last_good_fallback(url)
                            except Exception:
                                pass
                except Exception:
                    pass

                out["scraped_meta"][url] = meta
                out["scraped_content"][url] = cleaned

                scraped_ok_text += 1
                if meta["numbers_found"] > 0:
                    scraped_ok_numbers += 1

        except Exception as e:
            meta["status"] = "failed"
            meta["status_detail"] = f"failed:exception:{type(e).__name__}"

            # Attempt snapshot fallback on scrape exceptions.
            try:
                _prev = _get_prev_snapshot(url) if isinstance(snap_lookup, dict) else None
                _prev_nums = _prev.get("extracted_numbers") if isinstance(_prev, dict) else None
                if isinstance(_prev_nums, list) and _prev_nums:
                    meta["status"] = "fetched"
                    meta["status_detail"] = "fallback:last_good_snapshot"
                    meta["fallback_used"] = True
                    meta["fallback_reason"] = meta.get("status_detail") or f"failed:exception:{type(e).__name__}"
                    meta["fallback_source"] = "existing_snapshots"
                    meta["fallback_snapshot_fetched_at"] = _prev.get("fetched_at") if isinstance(_prev, dict) else None
                    meta["reused_snapshot"] = True

                    meta["fingerprint"] = _prev.get("fingerprint") if isinstance(_prev, dict) else meta.get("fingerprint")
                    meta["extractor_fingerprint"] = _prev.get("extractor_fingerprint") if isinstance(_prev, dict) else meta.get("extractor_fingerprint")
                    if not meta.get("extractor_fingerprint"):
                        meta["extractor_fingerprint"] = extractor_fp

                    meta["extracted_numbers"] = list(_prev_nums)
                    meta["numbers_found"] = len(_prev_nums)

                    _pc = (_prev.get("content") if isinstance(_prev, dict) else "") or ""
                    _pt = (_prev.get("clean_text") if isinstance(_prev, dict) else "") or ""
                    meta["content"] = _pc or _pt or ""
                    meta["clean_text"] = _pt or _pc or ""
                    meta["content_len"] = len(meta.get("content") or "")
                    meta["clean_text_len"] = len(meta.get("clean_text") or "")

                    out["scraped_meta"][url] = meta
                    out["scraped_content"][url] = meta.get("clean_text") or meta.get("content") or ""

                    scraped_ok_text += 1
                    if meta.get("numbers_found", 0) > 0:
                        scraped_ok_numbers += 1
                    # recovered; do not count as scraped_failed.
                    try:
                        _record_last_good_fallback(url)
                    except Exception:
                        pass
                    continue
            except Exception:
                pass
            scraped_failed += 1
            out["scraped_meta"][url] = meta
            out["errors"].append(meta["status_detail"])

        if progress:
            try:
                progress.progress((i + 1) / max(1, len(admitted)))
            except Exception:
                pass

    try:
        d = out.get("diag_injected_urls")
        if isinstance(d, dict):
            _inj = set(d.get("intake_norm") or [])
            sm = out.get("scraped_meta") or {}
            attempted = []
            persisted = []
            if isinstance(sm, dict):
                for u in sorted(_inj):
                    meta = sm.get(u) or {}
                    status = (meta.get("status") or "")
                    status_detail = (meta.get("status_detail") or "")
                    content = meta.get("clean_text") or meta.get("content") or ""
                    attempted.append({
                        "url": u,
                        "attempted": bool(u in (admitted or [])),
                        "fetch_status": "success" if (str(status_detail).startswith("success") or status == "fetched") else ("failed" if meta else "skipped"),
                        "fail_reason": (str(status_detail) or str(status) or "")[:80],
                        "content_len": int(len(content) if isinstance(content, str) else 0),
                        "numbers_found": int(meta.get("numbers_found") or 0),
                    })
                    if str(status_detail).startswith("success") or status == "fetched":
                        persisted.append(u)
            d["attempted"] = attempted
            d["persisted"] = persisted
            d.setdefault("set_hashes", {})
            if isinstance(d["set_hashes"], dict):
                d["set_hashes"]["persisted"] = _inj_diag_set_hash(persisted)
    except Exception:
        pass

    out["debug_counts"].update({
        "scraped_attempted": int(scraped_attempted),
        "scraped_ok_text": int(scraped_ok_text),
        "scraped_ok_numbers": int(scraped_ok_numbers),
        "scraped_failed": int(scraped_failed),
    })

    # Dashboard scrape summary
    try:
        st.info(
            f"🧽 Scrape Results: **{out['debug_counts']['scraped_ok_text']} ok-text** | "
            f"**{out['debug_counts']['scraped_ok_numbers']} ok-numbers** | "
            f"**{out['debug_counts']['scraped_failed']} failed**"
        )
    except Exception:
        pass

    # status summarization

    # Why:
    # - When scenario B "extra URLs" are provided, it can be unclear whether they:
    #     (a) were normalized/deduped
    #     (b) were admitted into the scrape list
    #     (c) were successfully scraped
    #     (d) actually entered the snapshot-hash pool used by analysis/evolution
    # - This patch records a deterministic, non-invasive trace in web_context only.
    try:
        if not isinstance(out.get("debug_counts"), dict):
            out["debug_counts"] = {}
        _dbg_counts = out["debug_counts"]

        _extra_trace = {
            "extra_urls_requested": list(extra_urls or []) if isinstance(extra_urls, list) else [],
            "extra_urls_normalized": list(_extras or []) if "_extras" in locals() and isinstance(_extras, list) else [],
            "extra_urls_admitted": [],
            "extra_urls_scraped": [],
            "extra_urls_in_hash_pool": [],
            "notes": [],
        }

        # Which extras actually made it into the final admitted URL list?
        try:
            _admitted_urls = []
            if "admitted" in locals() and isinstance(admitted, list):
                _admitted_urls = [u for u in admitted if isinstance(u, str) and u.strip()]
            _extra_set = set(_extra_trace["extra_urls_normalized"])
            _extra_trace["extra_urls_admitted"] = [u for u in _admitted_urls if u in _extra_set]
        except Exception:
            pass

        # How did each extra URL scrape?
        try:
            sm = out.get("scraped_meta") or {}
            if isinstance(sm, dict):
                for u in _extra_trace["extra_urls_normalized"]:
                    meta = sm.get(u) or {}
                    if isinstance(meta, dict) and meta:
                        content = meta.get("clean_text") or meta.get("content") or ""
                        fp = meta.get("fingerprint")
                        _extra_trace["extra_urls_scraped"].append({
                            "url": u,
                            "status": meta.get("status"),
                            "status_detail": meta.get("status_detail"),
                            "fingerprint": (fp[:16] if isinstance(fp, str) else fp),
                            "numbers_found": meta.get("numbers_found"),
                            "content_len": (len(content) if isinstance(content, str) else 0),
                            "content_type": meta.get("content_type") or "",
                        })
        except Exception:
            pass

        # Approximate "hash pool" membership (non-invasive):
        # we mark extras whose scrape produced a non-empty fingerprint + some text.
        try:
            sm = out.get("scraped_meta") or {}
            if isinstance(sm, dict):
                for row in (_extra_trace.get("extra_urls_scraped") or []):
                    u = row.get("url")
                    meta = sm.get(u) or {}
                    content = meta.get("clean_text") or meta.get("content") or ""
                    fp = meta.get("fingerprint")
                    if isinstance(fp, str) and fp and isinstance(content, str) and len(content) >= 200:
                        _extra_trace["extra_urls_in_hash_pool"].append(u)
        except Exception:
            pass

        out["extra_urls_debug"] = _extra_trace
        _dbg_counts["extra_urls_trace"] = {
            "requested": len(_extra_trace.get("extra_urls_requested") or []),
            "normalized": len(_extra_trace.get("extra_urls_normalized") or []),
            "admitted": len(_extra_trace.get("extra_urls_admitted") or []),
            "scraped": len(_extra_trace.get("extra_urls_scraped") or []),
            "in_hash_pool": len(_extra_trace.get("extra_urls_in_hash_pool") or []),
        }
    except Exception:
        pass
    if scraped_ok_text == 0:
        out["status"] = "failed"
        out["status_detail"] = "no_usable_text"
    elif scraped_ok_numbers == 0:
        out["status"] = "partial"
        out["status_detail"] = "text_ok_numbers_empty"
    else:
        out["status"] = "success"
        out["status_detail"] = "ok"

    return out

# 7. LLM QUERY FUNCTIONS

def query_perplexity(query: str, web_context: Dict, query_structure: Optional[Dict[str, Any]] = None) -> Optional[str]:
    """
    Query Perplexity and return a validated JSON string (LLMResponse-compatible).
    Removes 'action' and excludes None fields from output JSON.
    """
    if not PERPLEXITY_KEY:
        st.error("❌ PERPLEXITY_KEY not set.")
        return None

    query_structure = query_structure or {}
    structure_txt = ""
    ordering_contract = ""

    try:
        structure_txt, ordering_contract = build_query_structure_prompt(query_structure)
    except Exception:
        pass
        structure_txt = ""
        ordering_contract = ""

    # Web context: show top sources + snippets
    sources = (web_context.get("sources", []) if isinstance(web_context, dict) else []) or []
    search_results = (web_context.get("search_results", []) if isinstance(web_context, dict) else []) or []
    search_count = int(web_context.get("search_count", len(search_results)) if isinstance(web_context, dict) else 0)

    context_section = "WEB CONTEXT:\n"
    for url in sources[:6]:
        content = (web_context.get("scraped_content", {}) or {}).get(url) if isinstance(web_context, dict) else None
        if content:
            context_section += f"\n{url}:\n{str(content)[:800]}...\n"
        else:
            context_section += f"\n{url}\n"

    enhanced_query = (
        f"{context_section}\n"
        f"{SYSTEM_PROMPT}\n\n"
        f"User Question: {query}\n\n"
        f"{structure_txt}\n\n"
        f"{ordering_contract}\n"
        f"Web search returned {search_count} results.\n"
        f"Return ONLY valid JSON matching the template and include all required fields."
    )

    headers = {
        "Authorization": f"Bearer {PERPLEXITY_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "sonar",
        "temperature": 0.0,
        "max_tokens": 2400,
        "top_p": 1.0,
        "messages": [{"role": "user", "content": enhanced_query}]
    }

    try:
        resp = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=45)
        resp.raise_for_status()
        data = resp.json()

        if "choices" not in data:
            raise Exception("No 'choices' in Perplexity response")

        content = data["choices"][0]["message"]["content"]
        if not content or not content.strip():
            raise Exception("Empty Perplexity response")

        parsed = parse_json_safely(content, "Perplexity")
        if not parsed:
            return create_fallback_response(query, search_count, web_context)

        repaired = repair_llm_response(parsed)

        # Ensure action is removed even if present
        repaired.pop("action", None)

        validate_numeric_fields(repaired, "Perplexity")

        try:
            llm_obj = LLMResponse.model_validate(repaired)
            # action is dropped in repair_llm_response

            # Merge web sources
            if isinstance(web_context, dict) and web_context.get("sources"):
                existing = llm_obj.sources or []
                merged = list(dict.fromkeys(existing + web_context["sources"]))
                llm_obj.sources = merged[:10]
                llm_obj.freshness = "Current (web-enhanced)"

            result = llm_obj.model_dump_json(exclude_none=True)
            _legacy_cache_llm_response_v0(query, web_context, result)
            return result

        except ValidationError as e:
            st.warning(f"⚠️ Pydantic validation failed: {e}")
            return create_fallback_response(query, search_count, web_context)

    except Exception as e:
        st.error(f"❌ Perplexity API error: {e}")
        return create_fallback_response(query, search_count, web_context)

def query_perplexity_raw(prompt: str, max_tokens: int = 400, timeout: int = 30) -> str:
    """
    Raw Perplexity call that returns text only.
    IMPORTANT: Does NOT attempt to validate as LLMResponse.
    """
    headers = {
        "Authorization": f"Bearer {PERPLEXITY_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "sonar",
        "temperature": 0.0,
        "top_p": 1.0,
        "max_tokens": max_tokens,
        "messages": [{"role": "user", "content": prompt}],
    }

    resp = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=timeout)
    resp.raise_for_status()
    data = resp.json()
    return (data.get("choices", [{}])[0].get("message", {}) or {}).get("content", "") or ""

def create_fallback_response(query: str, search_count: int, web_context: Dict) -> str:
    """Create fallback response matching schema, excluding None fields and removing action."""
    fallback = LLMResponse(
        executive_summary=f"Analysis of '{query}' completed with {search_count} web sources. Schema validation used fallback structure.",
        primary_metrics={
            "sources": MetricDetail(name="Web Sources", value=search_count, unit="sources"),
            "quality": MetricDetail(name="Data Quality", value=70, unit="%")
        },
        key_findings=[
            f"Web search found {search_count} relevant sources.",
            "Primary model output required fallback due to format issues.",
            "Manual review of raw data recommended for accuracy."
        ],
        top_entities=[
            {"name": "Source 1", "share": "N/A", "growth": "N/A"}
        ],
        trends_forecast=[
            {"trend": "Schema validation used fallback", "direction": "⚠️", "timeline": "Now"}
        ],
        visualization_data={
            "chart_labels": ["Attempt"],
            "chart_values": [search_count],
            "chart_title": "Search Results"
        },
        sources=web_context.get("sources", []),
        confidence=60,
        freshness="Current (fallback)",
    )

    return fallback.model_dump_json(exclude_none=True)

# 7B. ANCHORED EVOLUTION QUERY

# 8. VALIDATION & SCORING

def parse_number_with_unit(val_str: str) -> float:
    """
    Parse a numeric string into a comparable base scale.
    Returns a float in "millions" for currency/volume-like values.
    Percentages are returned as their numeric value (e.g., "9.8%" -> 9.8).

    Handles:
      - $58.3B, 58.3B, S$29.8B, 29.8 S$B, USD 21.18 B
      - 58.3 billion, 58.3 bn, 58.3 million, 58.3 mn, 570 thousand
      - 570,000 (interpreted as an absolute count -> converted to millions)
      - 9.8% (kept as 9.8)
    """
    if val_str is None:
        return 0.0

    s = str(val_str).strip()
    if not s:
        return 0.0

    s_low = s.lower()

    # If it's a percentage, return the raw percent number (not millions)
    if "%" in s_low:
        m = re.search(r'(-?\d+(?:\.\d+)?)', s_low)
        if not m:
            return 0.0
        try:
            return float(m.group(1))
        except Exception:
            return 0.0

    # Normalize: remove commas and common currency tokens/symbols
    # (keep letters because we need bn/mn/b/m/k detection)
    s_low = s_low.replace(",", " ")
    for token in ["s$", "usd", "sgd", "us$", "$", "€", "£", "aud", "cad"]:
        s_low = s_low.replace(token, " ")

    # Collapse whitespace
    s_low = re.sub(r"\s+", " ", s_low).strip()

    # Extract the first number
    m = re.search(r'(-?\d+(?:\.\d+)?)', s_low)
    if not m:
        return 0.0

    try:
        num = float(m.group(1))
    except Exception:
        return 0.0

    # Look at the remaining text after the number for unit words/suffix
    tail = s_low[m.end():].strip()

    # Decide multiplier (base = millions)
    # billions -> *1000, millions -> *1, thousands -> *0.001
    multiplier = 1.0

    # Word-based units
    if re.search(r'\b(trillion|tn)\b', tail):
        multiplier = 1_000_000.0  # trillion -> million
    elif re.search(r'\b(billion|bn)\b', tail):
        multiplier = 1000.0
    elif re.search(r'\b(million|mn)\b', tail):
        multiplier = 1.0
    elif re.search(r'\b(thousand|k)\b', tail):
        multiplier = 0.001
    else:
        # Suffix-style units (possibly with spaces), e.g. "29.8 b", "21.18 b", "58.3m"
        # We only look at the very first letter-ish token in tail.
        t0 = tail[:4].strip()  # enough to catch "b", "m", "k"
        if t0.startswith("b"):
            multiplier = 1000.0
        elif t0.startswith("m"):
            multiplier = 1.0
        elif t0.startswith("k"):
            multiplier = 0.001
        else:
            # No unit detected. If it's a big integer like 570000 (jobs, people),
            # interpret as an absolute count and convert to millions.
            # (570000 -> 0.57 million)
            if abs(num) >= 10000 and float(num).is_integer():
                multiplier = 1.0 / 1_000_000.0
            else:
                multiplier = 1.0

    return num * multiplier

def numeric_consistency_with_sources_v2(primary_data: dict, web_context: dict) -> float:
    """
    Stable numeric consistency (0-100):
    - Evidence text: search_results snippets + web_context summary + scraped_content
    - Unit-aware parsing via parse_number_with_unit()
    - Range-aware (supports min/max if metric has a 'range' dict)
    - Downweights proxy metrics (is_proxy=True) so they don't tank the score
    """

    try:
        # Prefer canonical metrics if available (has is_proxy, range, etc.)
        metrics = primary_data.get("primary_metrics_canonical") or primary_data.get("primary_metrics") or {}
        if not isinstance(metrics, dict) or not metrics:
            return 50.0

        # Build evidence text corpus
        texts = []

        # 1) snippets
        sr = (web_context or {}).get("search_results") or []
        if isinstance(sr, list):
            for r in sr:
                if isinstance(r, dict):
                    snip = r.get("snippet", "")
                    if isinstance(snip, str) and snip.strip():
                        texts.append(snip)

        # 2) summary
        summary = (web_context or {}).get("summary") or ""
        if isinstance(summary, str) and summary.strip():
            texts.append(summary)

        # 3) scraped_content
        scraped = (web_context or {}).get("scraped_content") or {}
        if isinstance(scraped, dict):
            for _, content in scraped.items():
                if isinstance(content, str) and content.strip():
                    texts.append(content)

        evidence_text = "\n".join(texts)
        if not evidence_text.strip():
            return 45.0  # no evidence stored

        # Extract numeric candidates from evidence text
        # Keep this broad; parse_number_with_unit will normalize.
        patterns = [
            r'\$?\s?\d{1,3}(?:,\d{3})*(?:\.\d+)?\s*[BbMmKk]\b',                 # 29.8B, 570K, 1.2M
            r'\$?\s?\d+(?:\.\d+)?\s*(?:billion|million|thousand|bn|mn)\b',      # 29.8 billion, 29.8 bn
            r'\b\d{1,3}(?:,\d{3})+(?:\.\d+)?\b',                               # 570,000
            r'\b\d+(?:\.\d+)?\s*%\b',                                          # 9.8%
        ]

        evidence_numbers = []
        lowered = evidence_text.lower()

        for pat in patterns:
            for m in re.findall(pat, lowered, flags=re.IGNORECASE):
                n = parse_number_with_unit(str(m))
                if n and n > 0:
                    evidence_numbers.append(n)

        # If nothing extracted, don’t penalize too hard
        if not evidence_numbers:
            return 50.0

        # Verify each metric against evidence numbers (tolerance match)
        def _metric_candidates(m: dict) -> list:
            """Return list of candidate numeric values for a metric (range-aware)."""
            out = []
            if not isinstance(m, dict):
                return out

            # Range support: check min/max if present
            rng = m.get("range") if isinstance(m.get("range"), dict) else None
            if rng:
                if rng.get("min") is not None:
                    out.append(rng.get("min"))
                if rng.get("max") is not None:
                    out.append(rng.get("max"))

            # Also check main value
            if m.get("value") is not None:
                out.append(m.get("value"))

            return out

        def _parse_metric_num(val, unit_hint: str = "") -> float:
            # build a value+unit string so parse_number_with_unit has a chance
            if val is None:
                return 0.0
            s = str(val)
            if unit_hint and unit_hint.lower() not in s.lower():
                s = f"{s} {unit_hint}"
            return parse_number_with_unit(s)

        def _is_supported(target: float, evidence_nums: list, rel_tol: float = 0.25) -> bool:
            # same tolerance approach as v1 (25%)
            if not target or target <= 0:
                return False
            for e in evidence_nums:
                if e <= 0:
                    continue
                if abs(target - e) / max(target, e, 1) < rel_tol:
                    return True
            return False

        supported_w = 0.0
        total_w = 0.0

        for _, m in metrics.items():
            if not isinstance(m, dict):
                continue

            unit = str(m.get("unit") or "").strip()

            # proxy weighting
            is_proxy = bool(m.get("is_proxy"))
            w = 0.5 if is_proxy else 1.0

            cands = _metric_candidates(m)
            if not cands:
                continue

            # parse candidates into numeric values
            parsed_targets = []
            for c in cands:
                n = _parse_metric_num(c, unit_hint=unit)
                if n and n > 0:
                    parsed_targets.append(n)

            if not parsed_targets:
                continue

            total_w += w

            # supported if ANY candidate matches evidence
            if any(_is_supported(t, evidence_numbers, rel_tol=0.25) for t in parsed_targets):
                supported_w += w

        if total_w <= 0:
            return 50.0

        ratio = supported_w / total_w
        # Map: keep a soft floor so one miss doesn't tank the whole run
        score = 30.0 + (ratio * 65.0)  # same scale as v1 (30..95)
        return min(max(score, 20.0), 95.0)

    except Exception:
        return 45.0

def source_consensus(web_context: dict) -> float:
    """
    Calculate source consensus based on proportion of high-quality sources.
    Returns continuous score 0-100 based on quality distribution.
    """
    reliabilities = web_context.get("source_reliability", [])

    if not reliabilities:
        return 50.0  # Neutral when no sources

    total = len(reliabilities)
    high_count = sum(1 for r in reliabilities if "✅" in str(r))
    medium_count = sum(1 for r in reliabilities if "⚠️" in str(r))
    low_count = sum(1 for r in reliabilities if "❌" in str(r))

    # Weighted score: High=100, Medium=60, Low=30
    weighted_sum = (high_count * 100) + (medium_count * 60) + (low_count * 30)
    consensus_score = weighted_sum / total

    # Bonus for having multiple high-quality sources
    if high_count >= 3:
        consensus_score = min(100, consensus_score + 10)
    elif high_count >= 2:
        consensus_score = min(100, consensus_score + 5)

    return round(consensus_score, 1)

def source_freshness_score_v1(sources: Any, web_context: dict) -> Optional[float]:
    """Compute a deterministic source freshness score (0-100) from cached metadata only.

    Preference order (best-effort, all deterministic; no network IO):
      1) baseline_sources_cache (if present on web_context) with freshness_score.
      2) baseline_sources_cache with freshness_age_days (+ optional freshness_date_confidence).
      3) scraped_meta (url->meta) with the same fields; if missing, compute once from scraped_content + fetched_at.

    Notes:
      - Returns the median freshness score across matched sources (robust to outliers).
      - If `sources` is provided, we filter to those URLs; otherwise we use all cached entries.
    """
    try:
        wc = web_context or {}
        bsc_list: list = []

        # 1) baseline_sources_cache variants (prefer any that already include freshness_*).
        try:
            for k in (
                "baseline_sources_cache",
                "baseline_sources",
                "sources_cache",
                "baseline_sources_cache_current",
                "current_baseline_sources_cache",
            ):
                _v = wc.get(k)
                if isinstance(_v, list) and _v:
                    bsc_list = list(_v)
                    break
        except Exception:
            bsc_list = []

        # 2) If baseline list missing, build a lightweight list from scraped_meta (and compute missing freshness deterministically).
        if not bsc_list:
            try:
                sm = wc.get("scraped_meta") or {}
                sc = wc.get("scraped_content") or {}
                if isinstance(sm, dict) and sm:
                    tmp = []
                    for url, meta in sm.items():
                        if not isinstance(meta, dict):
                            continue
                        # Compute missing freshness fields once (no refetch; uses scraped_content + fetched_at).
                        try:
                            need = (meta.get("freshness_age_days") is None) or (meta.get("published_at") in (None, ""))
                            if need:
                                content = ""
                                try:
                                    content = sc.get(url) or sc.get(_normalize_url(url)) or ""
                                except Exception:
                                    content = ""
                                fetched_at = meta.get("fetched_at") or meta.get("fetched_at_iso") or ""
                                fres = _fresh01_compute_source_freshness_v2(content or "", fetched_at=str(fetched_at or ""), url=str(url or ""))
                                if isinstance(fres, dict) and fres:
                                    for kk in (
                                        "published_at",
                                        "freshness_age_days",
                                        "freshness_bucket",
                                        "freshness_method",
                                        "freshness_date_confidence",
                                        "freshness_score",
                                        "freshness_score_method",
                                    ):
                                        if kk in fres:
                                            meta[kk] = fres.get(kk)
                        except Exception:
                            pass

                        e = {"url": url}
                        for kk in ("freshness_score", "freshness_age_days", "freshness_date_confidence", "published_at"):
                            if kk in meta:
                                e[kk] = meta.get(kk)
                        tmp.append(e)
                    if tmp:
                        bsc_list = tmp
            except Exception:
                pass

        if not isinstance(bsc_list, list) or not bsc_list:
            return None

        # Normalize source URL set
        src_set = set()
        try:
            for u in (sources or []):
                if isinstance(u, str) and u.strip():
                    src_set.add(_normalize_url(u))
        except Exception:
            src_set = set()

        def _compute_score_from_entry(ent: dict) -> Optional[float]:
            if not isinstance(ent, dict):
                return None
            scv = ent.get("freshness_score")
            try:
                return float(scv)
            except Exception:
                pass
            age = ent.get("freshness_age_days")
            try:
                age_i = int(float(age)) if age is not None else None
            except Exception:
                age_i = None
            if age_i is None:
                return None
            try:
                base = _fresh01_score_from_age_days_v1(age_i)
            except Exception:
                base = None
            if base is None:
                return None
            conf = ent.get("freshness_date_confidence")
            try:
                cconf = float(conf) if conf is not None else 1.0
            except Exception:
                cconf = 1.0
            cconf = max(0.0, min(1.0, cconf))
            out = float(base) * cconf
            if out < 0.0:
                out = 0.0
            if out > 100.0:
                out = 100.0
            return out

        vals = []
        try:
            for ent in bsc_list:
                if not isinstance(ent, dict):
                    continue
                u = ent.get("url") or ent.get("source_url") or ""
                try:
                    un = _normalize_url(u) if isinstance(u, str) and u else ""
                except Exception:
                    un = str(u or "").strip().lower()
                if src_set and un and un not in src_set:
                    continue
                scv = _compute_score_from_entry(ent)
                if scv is None:
                    continue
                vals.append(float(scv))
        except Exception:
            pass

        if not vals:
            try:
                for ent in bsc_list:
                    scv = _compute_score_from_entry(ent)
                    if scv is None:
                        continue
                    vals.append(float(scv))
            except Exception:
                pass

        if not vals:
            return None

        vals.sort()
        mid = len(vals) // 2
        if len(vals) % 2 == 1:
            return float(vals[mid])
        return float((vals[mid - 1] + vals[mid]) / 2.0)
    except Exception:
        return None

def evidence_based_veracity(primary_data: dict, web_context: dict) -> dict:
    """
    Evidence-driven veracity scoring.
    Returns breakdown of component scores and overall score (0-100).
    """
    breakdown = {}

    # 1. SOURCE QUALITY (35% weight)
    sources = primary_data.get("sources", [])
    src_score = source_quality_score(sources)
    breakdown["source_quality"] = round(src_score, 1)

    # 2. NUMERIC CONSISTENCY (30% weight)
    num_score = numeric_consistency_with_sources_v2(primary_data, web_context)
    breakdown["numeric_consistency"] = round(num_score, 1)

    # 3. CITATION DENSITY (20% weight)
    # FIXED: Higher score when sources support findings, not penalize detail
    sources_count = len(sources)
    findings_count = len(primary_data.get("key_findings", []))
    metrics_count = len(primary_data.get("primary_metrics", {}))

    # Total claims = findings + metrics
    total_claims = findings_count + metrics_count

    if total_claims == 0:
        citations_score = 40.0  # Low score for no claims
    else:
        # Ratio of sources to claims - ideal is ~0.5-1.0 sources per claim
        ratio = sources_count / total_claims
        if ratio >= 1.0:
            citations_score = 90.0  # Well-supported
        elif ratio >= 0.5:
            citations_score = 70.0 + (ratio - 0.5) * 40  # 70-90 range
        elif ratio >= 0.25:
            citations_score = 50.0 + (ratio - 0.25) * 80  # 50-70 range
        else:
            citations_score = ratio * 200  # 0-50 range

    breakdown["citation_density"] = round(min(citations_score, 95.0), 1)

    # 4. SOURCE CONSENSUS (15% weight)
    consensus_score = source_consensus(web_context)
    breakdown["source_consensus"] = round(consensus_score, 1)

    # 5. DATA FRESHNESS (diagnostic component; 0-100; not included in overall v1 weight)
    fresh_score = source_freshness_score_v1(sources, web_context)
    breakdown["data_freshness"] = round(float(fresh_score), 1) if (fresh_score is not None) else None


    # Calculate weighted total
    total_score = (
        breakdown["source_quality"] * 0.35 +
        breakdown["numeric_consistency"] * 0.30 +
        breakdown["citation_density"] * 0.20 +
        breakdown["source_consensus"] * 0.15
    )

    breakdown["overall"] = round(total_score, 1)

    return breakdown

def calculate_final_confidence(
    base_conf: float,
    evidence_score: float
) -> float:
    """
    Calculate final confidence score.

    Formula balances model confidence with evidence quality:
    - Evidence has higher weight (65%) as it's more objective
    - Model confidence (35%) is adjusted by evidence quality

    This ensures:
    - High model + High evidence → High final (~85-90%)
    - High model + Low evidence → Medium final (~55-65%)
    - Low model + High evidence → Medium-High final (~70-80%)
    - Low model + Low evidence → Low final (~40-50%)
    """

    # Normalize inputs to 0-100 range
    base_conf = max(0, min(100, base_conf))
    evidence_score = max(0, min(100, evidence_score))

    # 1. EVIDENCE COMPONENT (65% weight) - Primary driver
    evidence_component = evidence_score * 0.65

    # 2. MODEL COMPONENT (35% weight) - Adjusted by evidence quality
    # When evidence is weak, model confidence is discounted
    evidence_multiplier = 0.5 + (evidence_score / 200)  # Range: 0.5 to 1.0
    model_component = base_conf * evidence_multiplier * 0.35

    final = evidence_component + model_component

    # Ensure result is in valid range
    return round(max(0, min(100, final)), 1)

METRIC_REGISTRY = {
    # Market Size metrics
    "market_size": {
        "canonical_name": "Market Size",
        "aliases": [
            "market size", "market value", "market cap", "total market",
            "global market", "market valuation", "industry size",
            "total addressable market", "tam", "market worth"
        ],
        "unit_type": "currency",
        "category": "size"
    },
    "market_size_current": {
        "canonical_name": "Current Market Size",
        "aliases": [
            "2024 market size", "2025 market size", "current market",
            "present market size", "today market", "current year market",
            "market size 2024", "market size 2025"
        ],
        "unit_type": "currency",
        "category": "size"
    },
    "market_size_projected": {
        "canonical_name": "Projected Market Size",
        "aliases": [
            "projected market", "forecast market", "future market",
            "2026 market", "2027 market", "2028 market", "2029 market", "2030 market",
            "market projection", "expected market size", "estimated market"
        ],
        "unit_type": "currency",
        "category": "size"
    },

    # Growth metrics
    "cagr": {
        "canonical_name": "CAGR",
        "aliases": [
            "cagr", "compound annual growth", "compound growth rate",
            "annual growth rate", "growth rate", "yearly growth"
        ],
        "unit_type": "percentage",
        "category": "growth"
    },
    "yoy_growth": {
        "canonical_name": "YoY Growth",
        "aliases": [
            "yoy growth", "year over year", "year-over-year",
            "annual growth", "yearly growth rate", "growth percentage"
        ],
        "unit_type": "percentage",
        "category": "growth"
    },

    # Revenue metrics
    "revenue": {
        "canonical_name": "Revenue",
        "aliases": [
            "revenue",
            # "sales",
            "total revenue", "annual revenue",
            "yearly revenue", "gross revenue",

            "sales revenue",
            "revenue from sales",
            "sales value",
            "value of sales",
            "sales (value)",
            "turnover",  # common finance synonym
        ],
        "unit_type": "currency",
        "category": "financial"
    },

    # Market share
    "market_share": {
        "canonical_name": "Market Share",
        "aliases": [
            "market share", "share", "market portion", "market percentage",
            "share of market"
        ],
        "unit_type": "percentage",
        "category": "share"
    },

    # Volume metrics
    "units_sold": {
        "canonical_name": "Units Sold",
        "aliases": [
            "units sold", "unit sales", "volume", "units shipped",
            "shipments", "deliveries", "production volume",

            "sales volume",
            "volume sales",
        ],
        "unit_type": "count",
        "category": "volume"
    },

    # Pricing
    "average_price": {
        "canonical_name": "Average Price",
        "aliases": [
            "average price", "avg price", "mean price", "asp",
            "average selling price", "unit price"
        ],
        "unit_type": "currency",
        "category": "pricing"
    },

    # Country / Macro metrics
    "gdp": {
        "canonical_name": "GDP",
        "aliases": ["gdp", "gross domestic product", "economic output"],
        "unit_type": "currency",
        "category": "macro"
    },
    "gdp_per_capita": {
        "canonical_name": "GDP per Capita",
        "aliases": ["gdp per capita", "gdp/capita", "income per person", "per capita gdp"],
        "unit_type": "currency",
        "category": "macro"
    },
    "gdp_growth": {
        "canonical_name": "GDP Growth",
        "aliases": ["gdp growth", "economic growth", "growth rate of gdp", "real gdp growth"],
        "unit_type": "percentage",
        "category": "macro"
    },
    "population": {
        "canonical_name": "Population",
        "aliases": ["population", "population size", "number of people"],
        "unit_type": "count",
        "category": "macro"
    },
    "exports": {
        "canonical_name": "Exports",
        "aliases": ["exports", "export value", "total exports"],
        "unit_type": "currency",
        "category": "trade"
    },
    "imports": {
        "canonical_name": "Imports",
        "aliases": ["imports", "import value", "total imports"],
        "unit_type": "currency",
        "category": "trade"
    },
    "inflation": {
        "canonical_name": "Inflation",
        "aliases": ["inflation", "cpi", "consumer price index", "inflation rate"],
        "unit_type": "percentage",
        "category": "macro"
    },
    "interest_rate": {
        "canonical_name": "Interest Rate",
        "aliases": ["interest rate", "policy rate", "benchmark rate", "central bank rate"],
        "unit_type": "percentage",
        "category": "macro"
    }
}

# Year extraction pattern
YEAR_PATTERN = re.compile(r'(20\d{2})')

# DETERMINISTIC QUESTION SIGNALS
# Drives metric table templates (no LLM)

def get_expected_metric_ids_for_category(category: str) -> List[str]:
    """
    Domain-agnostic mapping from a template/category string to expected metric IDs.

    Backward compatible:
      - accepts legacy categories like 'country', 'industry', 'company', 'generic'
      - also accepts template IDs like 'ENTITY_OVERVIEW_MARKET_LIGHT_V1', etc.

    NOTE:
    - This function returns a *default* set for a given template/category.
    - The profiler (classify_question_signals) can override/compose expected_metric_ids dynamically.
    """
    c_raw = (category or "unknown").strip()
    c = c_raw.lower().strip()

    # New generalized templates
    if c in {"entity_overview_country_light_v1", "entity_overview_country_v1"}:
        return [
            "population",
            "gdp_nominal",
            "gdp_per_capita",
            "gdp_growth",
            "inflation",
            "currency",
            "unemployment",
            "exports",
            "imports",
            "top_industries",
        ]

    if c in {"entity_overview_market_light_v1"}:
        return [
            "market_size_current",
            "market_size_projected",
            "cagr",
            "key_trends",
            "top_players",
        ]

    if c in {"entity_overview_market_heavy_v1"}:
        return [
            "market_size_current",
            "market_size_projected",
            "cagr",
            "key_trends",
            "top_players",
            "key_regions",
            "segments",
            "market_share",
            "revenue",
            "units_sold",
            "average_price",
        ]

    if c in {"entity_overview_company_light_v1", "entity_overview_company_v1"}:
        return [
            "revenue",
            "growth",
            "gross_margin",
            "operating_margin",
            "net_income",
            "market_cap",
            "valuation_multiple",
        ]

    if c in {"entity_overview_product_light_v1", "entity_overview_product_v1"}:
        return [
            "average_price",
            "units_sold",
            "market_share",
            "growth",
            "key_trends",
        ]

    if c in {"entity_overview_topic_v1", "generic_v1"}:
        return []

    # Legacy categories (still supported)
    if c == "country":
        return get_expected_metric_ids_for_category("ENTITY_OVERVIEW_COUNTRY_LIGHT_V1")

    if c == "industry":
        # legacy industry defaults to light market
        return get_expected_metric_ids_for_category("ENTITY_OVERVIEW_MARKET_LIGHT_V1")

    if c == "company":
        return get_expected_metric_ids_for_category("ENTITY_OVERVIEW_COMPANY_LIGHT_V1")

    if c == "generic":
        return []

    # fallback
    return []

def classify_question_signals(query: str) -> Dict[str, Any]:
    """
    Deterministically classify query and return:
      - category: high-level bucket used for templates (country | industry | company | generic)
      - expected_metric_ids: list[str]
      - signals: list[str] (debuggable reasons)
      - years: list[int]
      - regions: list[str]
      - intents: list[str] (market_size, growth_forecast, competitive_landscape, pricing, regulation, consumer_demand, supply_chain, investment, macro_outlook)
    """
    q_raw = (query or "").strip()
    q = q_raw.lower().strip()
    signals: List[str] = []

    if not q:
        return {
            "category": "generic",
            "expected_metric_ids": [],
            "signals": ["empty_query"],
            "years": [],
            "regions": [],
            "intents": []
        }

    # 1) Extract years (deterministic)
    years: List[int] = []
    try:
        year_matches = re.findall(r"\b(19|20)\d{2}\b", q_raw)
        # The regex above returns the first group; re-run with a non-capturing group to capture full year strings.
        year_matches_full = re.findall(r"\b(?:19|20)\d{2}\b", q_raw)
        years = sorted({int(y) for y in year_matches_full})
        if years:
            signals.append(f"years:{','.join(map(str, years[:8]))}")
    except Exception:
        pass
        years = []

    # 2) Extract regions/countries (best-effort deterministic; spaCy if available)
    regions: List[str] = []
    try:
        nlp = _try_spacy_nlp()
        if nlp:
            doc = nlp(q_raw)
            gpes = [ent.text.strip() for ent in getattr(doc, "ents", []) if ent.label_ in ("GPE", "LOC")]
            regions = []
            for g in gpes:
                if g and g.lower() not in [x.lower() for x in regions]:
                    regions.append(g)
            if regions:
                signals.append(f"regions_spacy:{','.join(regions[:6])}")
    except Exception:
        pass

    # Fallback: very lightweight region tokens
    if not regions:
        region_tokens = [
            "singapore", "malaysia", "indonesia", "thailand", "vietnam", "philippines",
            "china", "india", "japan", "korea", "australia",
            "usa", "united states", "europe", "uk", "united kingdom",
            "asean", "southeast asia", "sea", "global", "worldwide"
        ]
        hits = [t for t in region_tokens if t in q]
        if hits:
            # Keep original casing loosely (title-case single words)
            regions = [h.title() if " " not in h else h.upper() if h in ("usa", "uk") else h.title() for h in hits[:6]]
            signals.append(f"regions_kw:{','.join(hits[:6])}")

    # 3) Intent detection (domain-agnostic)
    intent_patterns: Dict[str, List[str]] = {
        "market_size": ["market size", "tam", "total addressable market", "how big", "size of the market", "market value"],
        "growth_forecast": ["cagr", "forecast", "projection", "by 20", "growth rate", "expected to", "outlook", "trend"],
        "competitive_landscape": ["key players", "competitors", "market share", "top companies", "leading players", "who are the players"],
        "pricing": ["pricing", "price", "asp", "average selling price", "cost", "margins"],
        "consumer_demand": ["demand", "users", "penetration", "adoption", "consumer", "customer", "behavior"],
        "supply_chain": ["supply", "capacity", "production", "manufacturing", "inventory", "shipment", "lead time"],
        "regulation": ["regulation", "policy", "law", "compliance", "tax", "tariff", "subsidy"],
        "investment": ["investment", "capex", "funding", "valuation", "roi", "profit", "ebitda"],
        "macro_outlook": ["gdp", "inflation", "interest rate", "policy rate", "exports", "imports", "currency", "exchange rate", "per capita"],
    }

    intents: List[str] = []
    for intent, pats in intent_patterns.items():
        if any(p in q for p in pats):
            intents.append(intent)

    # Small disambiguation: "by 2030" etc. strongly suggests forecast if years exist
    if years and "growth_forecast" not in intents and any(yr >= 2025 for yr in years):
        intents.append("growth_forecast")

    if intents:
        signals.append(f"intents:{','.join(intents[:10])}")

    # 4) Category decision (template driver)
    # Keep it coarse: country vs industry vs company vs generic
    country_kw = [
        "gdp", "per capita", "population", "exports", "imports",
        "inflation", "cpi", "interest rate", "policy rate", "central bank",
        "currency", "exchange rate"
    ]
    company_kw = ["revenue", "earnings", "profit", "ebitda", "guidance", "quarter", "fy", "10-k", "10q", "balance sheet"]
    industry_kw = [
        "market", "industry", "sector", "tam", "cagr", "market size", "market share",
        "key players", "competitors", "pricing", "forecast", "outlook"
    ]

    country_hits = [k for k in country_kw if k in q]
    company_hits = [k for k in company_kw if k in q]
    industry_hits = [k for k in industry_kw if k in q]

    # If macro intent is present, strongly bias to country
    if "macro_outlook" in intents and (regions or country_hits):
        category = "country"
        signals.append("category_rule:macro_outlook_bias_country")
    elif company_hits and not industry_hits:
        category = "company"
        signals.append(f"category_rule:company_keywords:{','.join(company_hits[:5])}")
    elif industry_hits and not country_hits:
        category = "industry"
        signals.append(f"category_rule:industry_keywords:{','.join(industry_hits[:5])}")
    elif industry_hits and country_hits:
        # tie-break: if market sizing/competitive signals exist -> industry; if macro_outlook -> country
        if "macro_outlook" in intents:
            category = "country"
            signals.append("category_rule:mixed_signals_macro_wins")
        else:
            category = "industry"
            signals.append("category_rule:mixed_signals_default_to_industry")
    else:
        category = "generic"
        signals.append("category_rule:no_template_keywords")

    # 5) Expected metric IDs (category + intent)
    expected_metric_ids: List[str] = []
    try:
        expected_metric_ids = get_expected_metric_ids_for_category(category) or []
    except Exception:
        pass
        expected_metric_ids = []

    # Add a few intent-driven metric IDs (only if your registry supports them)
    intent_metric_suggestions = {
        "market_size": ["market_size", "market_size_2024", "market_size_2025"],
        "growth_forecast": ["cagr", "forecast_period", "market_size_2030"],
        "competitive_landscape": ["market_share", "top_players"],
        "pricing": ["avg_price", "asp"],
        "consumer_demand": ["users", "penetration", "arpu"],
        "supply_chain": ["capacity", "shipments"],
        "investment": ["capex", "profit", "ebitda"],
        "macro_outlook": ["gdp", "inflation", "interest_rate", "exchange_rate"],
    }

    for intent in intents:
        for mid in intent_metric_suggestions.get(intent, []):
            if mid not in expected_metric_ids:
                expected_metric_ids.append(mid)

    return {
        "category": category,
        "expected_metric_ids": expected_metric_ids,
        "signals": signals,
        "years": years,
        "regions": regions,
        "intents": intents
    }

    def _contains_any(needle_list: List[str]) -> bool:
        return any(k in q for k in needle_list)

    # Determine intents
    intents: List[str] = []
    for intent, kws in intent_triggers.items():
        if _contains_any(kws):
            intents.append(intent)

    if intents:
        signals.append("intents:" + ",".join(sorted(set(intents))))

    # Determine entity_kind (best-effort heuristic)
    is_marketish = _contains_any(market_entity_kw) or any(i in intents for i in ["size", "growth", "forecast", "share", "segments", "players", "regions"])
    is_companyish = _contains_any(company_entity_kw) and not _contains_any(country_entity_kw)
    is_countryish = _contains_any(country_entity_kw) and not is_companyish
    is_productish = _contains_any(product_entity_kw) and not (is_marketish or is_countryish or is_companyish)

    if is_countryish:
        entity_kind = "country"
        signals.append("entity_kind:country")
    elif is_companyish:
        entity_kind = "company"
        signals.append("entity_kind:company")
    elif is_productish:
        entity_kind = "product"
        signals.append("entity_kind:product")
    elif is_marketish:
        entity_kind = "market"
        signals.append("entity_kind:market")
    else:
        entity_kind = "topic_general"
        signals.append("entity_kind:topic_general")

    # Determine scope
    is_comparative = _contains_any(comparative_kw)
    is_forecasty = _contains_any(forecast_kw) or bool(YEAR_PATTERN.findall(q_raw))

    # Broad overview should win when user explicitly asks for general explainer
    # BUT: if they also mention measurable intents (size/growth/forecast/etc.), treat as metrics_light.
    is_broad_phrase = _contains_any(broad_phrases)

    if is_comparative:
        scope = "comparative"
        signals.append("scope:comparative")
    elif is_forecasty and any(i in intents for i in ["forecast", "growth", "size"]):
        scope = "forecast_specific"
        signals.append("scope:forecast_specific")
    elif is_broad_phrase and not intents:
        scope = "broad_overview"
        signals.append("scope:broad_overview")
    else:
        # metrics light vs heavy
        heavy_asks = ["segments", "share", "volume", "regions", "players"]
        heavy_requested = any(i in intents for i in heavy_asks)
        if heavy_requested:
            scope = "metrics_heavy"
            signals.append("scope:metrics_heavy")
        else:
            scope = "metrics_light"
            signals.append("scope:metrics_light")

    # Map entity_kind -> category (backward compatible)
    if entity_kind == "country":
        category = "country"
    elif entity_kind == "company":
        category = "company"
    elif entity_kind in {"market", "product"}:
        category = "industry"
    else:
        category = "generic"

    # Choose generalized template + tiers
    # Tier meanings:
    #  1 = high extractability (size/growth/forecast)
    #  2 = medium (players/regions/basic segments)
    #  3 = low (granular channels, detailed splits) -> only if explicitly asked
    if category == "country":
        metric_template_id = "ENTITY_OVERVIEW_COUNTRY_LIGHT_V1" if scope != "metrics_heavy" else "ENTITY_OVERVIEW_COUNTRY_LIGHT_V1"
        metric_tiers_enabled = [1]
    elif category == "company":
        metric_template_id = "ENTITY_OVERVIEW_COMPANY_LIGHT_V1"
        metric_tiers_enabled = [1]
    elif category == "industry":
        if scope in {"metrics_heavy", "comparative"}:
            metric_template_id = "ENTITY_OVERVIEW_MARKET_HEAVY_V1"
            metric_tiers_enabled = [1, 2]
        else:
            metric_template_id = "ENTITY_OVERVIEW_MARKET_LIGHT_V1"
            metric_tiers_enabled = [1]
    else:
        metric_template_id = "ENTITY_OVERVIEW_TOPIC_V1"
        metric_tiers_enabled = []

    # Build expected_metric_ids dynamically from intents (domain-agnostic)
    # Slot -> metric id mapping (kept generic; avoids tourism specialization)
    # If you later add more canonical IDs, expand these mappings.
    market_slot_to_id = {
        "size_current": "market_size_current",
        "size_projected": "market_size_projected",
        "growth_cagr": "cagr",
        "growth_yoy": "growth",
        "share_key": "market_share",
        "volume_current": "units_sold",
        "price_avg": "average_price",
        "players_top": "top_players",
        "regions_key": "key_regions",
        "segments_basic": "segments",
        "trends": "key_trends",
        "revenue": "revenue",
    }

    company_slot_to_id = {
        "revenue": "revenue",
        "growth": "growth",
        "gross_margin": "gross_margin",
        "operating_margin": "operating_margin",
        "net_income": "net_income",
        "market_cap": "market_cap",
        "valuation_multiple": "valuation_multiple",
        "trends": "key_trends",
    }

    country_slot_to_id = {
        "population": "population",
        "gdp_nominal": "gdp_nominal",
        "gdp_per_capita": "gdp_per_capita",
        "gdp_growth": "gdp_growth",
        "inflation": "inflation",
        "currency": "currency",
        "unemployment": "unemployment",
        "exports": "exports",
        "imports": "imports",
        "top_industries": "top_industries",
        "trends": "key_trends",
    }

    # Determine slots from intents
    slots: List[str] = []
    if entity_kind == "country":
        # For countries: macro defaults if broad, otherwise macro intents
        if scope == "broad_overview":
            slots = ["population", "gdp_nominal", "gdp_per_capita", "gdp_growth", "inflation", "currency", "top_industries"]
        else:
            # If user asks for macro (or didn’t specify), still give a tight macro set
            slots = ["population", "gdp_nominal", "gdp_growth", "inflation", "currency"]
            if "macro" in intents:
                slots += ["unemployment", "exports", "imports"]

        mapper = country_slot_to_id

    elif entity_kind == "company":
        slots = ["revenue", "growth", "gross_margin", "operating_margin", "net_income", "market_cap", "valuation_multiple"]
        mapper = company_slot_to_id

    elif entity_kind in {"market", "product"}:
        # Tier 1 core (always when metrics_* scope)
        if scope == "broad_overview":
            slots = ["trends", "players_top"]
        else:
            slots = ["size_current", "growth_cagr"]
            if "forecast" in intents:
                slots.append("size_projected")
            if "trends" in intents:
                slots.append("trends")
            # Tier 2 (only when explicitly asked or heavy scope)
            if scope in {"metrics_heavy", "comparative"}:
                if "players" in intents:
                    slots.append("players_top")
                if "regions" in intents:
                    slots.append("regions_key")
                if "segments" in intents:
                    slots.append("segments_basic")
                if "share" in intents:
                    slots.append("share_key")
                if "volume" in intents:
                    slots.append("volume_current")
                if "price" in intents:
                    slots.append("price_avg")
            else:
                # metrics_light: include players/trends only if asked
                if "players" in intents:
                    slots.append("players_top")
                if "regions" in intents:
                    slots.append("regions_key")

        mapper = market_slot_to_id

    else:
        # topic_general
        slots = []
        mapper = {}

    expected_metric_ids = []
    for s in slots:
        mid = mapper.get(s)
        if mid:
            expected_metric_ids.append(mid)

    # If still empty but template provides defaults, use template defaults
    if not expected_metric_ids:
        expected_metric_ids = get_expected_metric_ids_for_category(metric_template_id)

    # De-dup while preserving order
    seen = set()
    expected_metric_ids = [x for x in expected_metric_ids if not (x in seen or seen.add(x))]

    # Preferred source classes (generic)
    if category == "country":
        preferred_source_classes = ["official_stats", "government", "reputable_org", "reference"]
    elif category == "company":
        preferred_source_classes = ["official_filings", "investor_relations", "reputable_org", "news"]
    elif category == "industry":
        preferred_source_classes = ["industry_association", "reputable_org", "official_stats", "news", "research_portal"]
    else:
        preferred_source_classes = ["reference", "official_stats", "reputable_org"]

    # Attach year detection signal
    years = sorted(set(YEAR_PATTERN.findall(q_raw))) if YEAR_PATTERN.findall(q_raw) else []
    if years:
        signals.append("years_detected:" + ",".join(years))

    return {
        "category": category,
        "expected_metric_ids": expected_metric_ids,
        "signals": signals,
        "entity_kind": entity_kind,
        "scope": scope,
        "metric_template_id": metric_template_id,
        "metric_tiers_enabled": metric_tiers_enabled,
        "preferred_source_classes": preferred_source_classes,
        "intents": sorted(set(intents)),
    }

def get_canonical_metric_id(metric_name: str) -> Tuple[str, str]:
    """
    Map a metric name to its canonical ID and display name.

    Returns:
        Tuple of (canonical_id, canonical_display_name)

    Example:
        "2024 Market Size" -> ("market_size_2024", "Market Size (2024)")
        "Global Market Value" -> ("market_size", "Market Size")
        "CAGR 2024-2030" -> ("cagr_2024_2030", "CAGR (2024-2030)")
    """

    if not metric_name:
        return ("unknown", "Unknown Metric")

    name_lower = metric_name.lower().strip()
    name_normalized = re.sub(r"[^\w\s]", " ", name_lower)
    name_normalized = re.sub(r"\s+", " ", name_normalized).strip()

    # Extract years
    years = YEAR_PATTERN.findall(metric_name)
    year_suffix = "_".join(sorted(years)) if years else ""

    name_words = set(name_normalized.split())

    # Explicit money intent (strong)
    money_tokens = {
        "revenue", "turnover", "valuation", "valued", "value", "market", "capex", "opex",
        "profit", "earnings", "ebitda", "income",
        "usd", "sgd", "eur", "gbp", "aud", "cad", "jpy", "cny", "rmb"
    }
    # Currency symbols appear in raw text sometimes
    has_currency_symbol = any(sym in metric_name for sym in ["$", "€", "£", "S$"])

    has_money_intent = bool(name_words & money_tokens) or has_currency_symbol

    # Explicit unit/count intent (strong)
    unit_tokens = {
        "unit", "units", "deliveries", "shipments", "registrations", "vehicles",
        "sold", "salesvolume", "volume", "pcs", "pieces"
    }
    # normalize joined token cases like "sales volume"
    joined = name_normalized.replace(" ", "")
    has_unit_intent = bool(name_words & unit_tokens) or any(t in joined for t in ["salesvolume", "unitsold", "vehiclesold"])

    # Find best matching registry entry
    best_match_id = None
    best_match_score = 0.0

    def _is_revenue_like(metric_id: str, config: dict) -> bool:
        mid = (metric_id or "").lower()
        cname = str((config or {}).get("canonical_name") or "").lower()
        # treat "market value" / "valuation" as currency-like too
        if any(k in cname for k in ["revenue", "market value", "valuation", "market size", "turnover"]):
            return True
        if any(k in mid for k in ["revenue", "market_value", "market_size", "valuation"]):
            return True
        return False

    for metric_id, config in METRIC_REGISTRY.items():
        for alias in config["aliases"]:
            # Remove years from alias for comparison
            alias_no_year = YEAR_PATTERN.sub("", alias).strip().lower()
            alias_no_year = re.sub(r"[^\w\s]", " ", alias_no_year)
            alias_no_year = re.sub(r"\s+", " ", alias_no_year).strip()

            name_no_year = YEAR_PATTERN.sub("", name_normalized).strip()

            score = 0.0

            # Exact match
            if alias_no_year == name_no_year and alias_no_year:
                score = 1.0

            # Containment match
            elif alias_no_year and (alias_no_year in name_no_year or name_no_year in alias_no_year):
                score = len(alias_no_year) / max(len(name_no_year), 1)

            # Word overlap match
            else:
                alias_words = set(alias_no_year.split())
                name_words_local = set(name_no_year.split())
                if alias_words and name_words_local:
                    overlap = len(alias_words & name_words_local) / len(alias_words | name_words_local)
                    score = max(score, overlap)

            # - Block "sales" -> revenue when no money intent
            # - Block unit-intent -> revenue-like
            # - Require explicit money intent for revenue-like (soft guard, not hard stop)
            if score > 0.0:
                revenue_like = _is_revenue_like(metric_id, config)

                # If target is revenue-like but name has strong unit intent, penalize heavily
                if revenue_like and has_unit_intent and not has_money_intent:
                    score *= 0.20  # strong downweight

                # If target is revenue-like but name has NO money intent at all, penalize
                if revenue_like and not has_money_intent:
                    score *= 0.55  # moderate downweight

                # If name includes the word "sales" but no money intent, avoid mapping to revenue-like
                if revenue_like and ("sales" in name_no_year.split()) and not has_money_intent:
                    score *= 0.60

                # Conversely: if target is NOT revenue-like but name has money intent, slight penalty
                if (not revenue_like) and has_money_intent and ("sales" in name_no_year.split()) and not has_unit_intent:
                    score *= 0.85

            if score > best_match_score:
                best_match_id = metric_id
                best_match_score = score

            if best_match_score == 1.0:
                break

        if best_match_score == 1.0:
            break

    # Build canonical ID and display name
    if best_match_id and best_match_score > 0.4:
        config = METRIC_REGISTRY[best_match_id]
        canonical_base = best_match_id
        display_name = config["canonical_name"]

        if year_suffix:
            canonical_id = f"{canonical_base}_{year_suffix}"
            if len(years) == 1:
                display_name = f"{display_name} ({years[0]})"
            else:
                display_name = f"{display_name} ({'-'.join(years)})"
        else:
            canonical_id = canonical_base

        return (canonical_id, display_name)

    # Fallback: create ID from normalized name
    fallback_id = re.sub(r"\s+", "_", name_normalized)
    if year_suffix:
        fallback_id = f"{fallback_id}_{year_suffix}" if year_suffix not in fallback_id else fallback_id

    return (fallback_id, metric_name)

# GEO + PROXY TAGGING (DETERMINISTIC)

REGION_KEYWORDS = {
    "APAC": ["apac", "asia pacific", "asia-pacific"],
    "SOUTHEAST_ASIA": ["southeast asia", "asean", "sea "],  # note space to reduce false matches
    "ASIA": ["asia"],
    "EUROPE": ["europe", "eu", "emea"],
    "NORTH_AMERICA": ["north america"],
    "LATAM": ["latin america", "latam"],
    "MIDDLE_EAST": ["middle east", "mena"],
    "AFRICA": ["africa"],
    "OCEANIA": ["oceania", "australia", "new zealand"],
}

GLOBAL_KEYWORDS = ["global", "worldwide", "world", "international", "across the world"]

# Minimal country map (expand deterministically over time)
COUNTRY_KEYWORDS = {
    "Singapore": ["singapore", "sg"],
    "United States": ["united states", "usa", "u.s.", "us"],
    "United Kingdom": ["united kingdom", "uk", "u.k.", "britain", "england"],
    "China": ["china", "prc"],
    "Japan": ["japan"],
    "India": ["india"],
    "Indonesia": ["indonesia"],
    "Malaysia": ["malaysia"],
    "Thailand": ["thailand"],
    "Vietnam": ["vietnam"],
    "Philippines": ["philippines"],
}

def infer_geo_scope(*texts: str) -> Dict[str, str]:
    """
    Deterministically infer geography from text.
    Returns {"geo_scope": "local|regional|global|unknown", "geo_name": "<name or ''>"}.
    Priority: country > region > global.
    """
    combined = " ".join([t for t in texts if isinstance(t, str) and t.strip()]).lower()
    if not combined:
        return {"geo_scope": "unknown", "geo_name": ""}

    # 1) Country/local (most specific)
    for country, kws in COUNTRY_KEYWORDS.items():
        for kw in kws:
            if kw in combined:
                return {"geo_scope": "local", "geo_name": country}

    # 2) Region
    for region_name, kws in REGION_KEYWORDS.items():
        for kw in kws:
            if kw in combined:
                pretty = region_name.replace("_", " ").title()
                return {"geo_scope": "regional", "geo_name": pretty}

    # 3) Global
    for kw in GLOBAL_KEYWORDS:
        if kw in combined:
            return {"geo_scope": "global", "geo_name": "Global"}

    return {"geo_scope": "unknown", "geo_name": ""}

# "Proxy" = adjacent metric that can help approximate the target but isn't the target definition.
# You can expand these sets deterministically.

PROXY_PATTERNS = [
    # (pattern, proxy_type, reason_template)
    (r"\bapparel\b|\bfashion\b|\bclothing\b", "adjacent_category", "Uses apparel/fashion as an adjacent proxy for streetwear."),
    (r"\bfootwear\b|\bsneaker\b|\bshoes\b", "subsegment", "Uses footwear/sneakers as a subsegment proxy for the broader market."),
    (r"\bresale\b|\bsecondary market\b", "channel_proxy", "Uses resale/secondary-market measures as a channel proxy."),
    (r"\be-?commerce\b|\bonline sales\b|\bsocial commerce\b", "channel_proxy", "Uses e-commerce indicators as a channel proxy."),
    (r"\btourism\b|\bvisitor\b|\btravel retail\b", "demand_driver", "Uses tourism indicators as a demand-driver proxy."),
    (r"\bsearch interest\b|\bgoogle trends\b|\bweb traffic\b", "interest_proxy", "Uses interest/attention measures as a proxy."),
]

# These are words that signal "core market size" style metrics (usually non-proxy if they match the user topic).
CORE_MARKET_PATTERNS = [
    r"\bmarket size\b",
    r"\bmarket value\b",
    r"\brevenue\b",
    r"\bsales\b",
    r"\bcagr\b",
    r"\bgrowth\b",
    r"\bprojected\b|\bforecast\b",
]

def infer_proxy_label(
    metric_name: str,
    question_text: str = "",
    category_hint: str = "",
    *extra_context: str
) -> Dict[str, Any]:
    """
    Deterministically label a metric as proxy/non-proxy.

    Returns fields:
      is_proxy: bool
      proxy_type: str
      proxy_reason: str
      proxy_confidence: float (0-1)
      proxy_target: str (best-guess target topic)
    """
    name = (metric_name or "").lower().strip()
    q = (question_text or "").lower().strip()
    ctx = " ".join([c for c in extra_context if isinstance(c, str)]).lower()

    combined = " ".join([name, q, ctx]).strip()

    # Default: not proxy
    out = {
        "is_proxy": False,
        "proxy_type": "",
        "proxy_reason": "",
        "proxy_confidence": 0.0,
        "proxy_target": ""
    }

    if not combined:
        return out

    # Best-effort target topic extraction (very light heuristic)
    # If you already have question signals elsewhere, you can pass them in category_hint/question_text.
    # Here we just keep a short phrase if present.
    proxy_target = ""
    if "streetwear" in q:
        proxy_target = "streetwear"
    elif "semiconductor" in q:
        proxy_target = "semiconductors"
    elif "battery" in q:
        proxy_target = "batteries"
    out["proxy_target"] = proxy_target

    # If metric name itself looks like core market patterns AND includes the target keyword, treat as non-proxy.
    # (prevents incorrectly labeling "Singapore streetwear market size" as proxy)
    core_like = any(re.search(p, name) for p in CORE_MARKET_PATTERNS)
    if core_like:
        # If it explicitly contains the topic keyword, strongly non-proxy
        if proxy_target and proxy_target in name:
            return out
        # If it says "streetwear market" in name, non-proxy even if target not detected
        if "streetwear" in name:
            return out

    # Detect proxies using patterns.
    for pat, ptype, reason in PROXY_PATTERNS:
        if re.search(pat, combined):
            out["is_proxy"] = True
            out["proxy_type"] = ptype
            out["proxy_reason"] = reason
            # Confidence: stronger if pattern appears in metric name; weaker if only in context.
            if re.search(pat, name):
                out["proxy_confidence"] = 0.9
            elif re.search(pat, ctx):
                out["proxy_confidence"] = 0.7
            else:
                out["proxy_confidence"] = 0.6
            return out

    return out

def merge_group_geo(group: List[Dict[str, Any]]) -> Tuple[str, str]:
    """
    Choose the most frequent geo tag within a merged group deterministically.
    Returns (geo_scope, geo_name).
    """
    items = []
    for g in group:
        s = g.get("geo_scope", "unknown")
        n = g.get("geo_name", "")
        if s and s != "unknown":
            items.append((s, n))

    if not items:
        return "unknown", ""

    counts: Dict[str, int] = {}
    for s, n in items:
        k = f"{s}|{n}"
        counts[k] = counts.get(k, 0) + 1

    best_k = max(counts.items(), key=lambda kv: kv[1])[0]  # deterministic tie via insertion order after stable sort
    s, n = best_k.split("|", 1)
    return s, n

def merge_group_proxy(group: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Merge proxy labels for duplicates deterministically.
    If ANY member is proxy -> merged metric is proxy.
    Choose the highest-confidence proxy candidate.
    """
    best = None
    best_conf = -1.0

    for g in group:
        is_proxy = bool(g.get("is_proxy", False))
        conf = float(g.get("proxy_confidence", 0.0) or 0.0)
        if is_proxy and conf > best_conf:
            best_conf = conf
            best = g

    if best is None:
        return {
            "is_proxy": False,
            "proxy_type": "",
            "proxy_reason": "",
            "proxy_confidence": 0.0,
            "proxy_target": "",
        }

    return {
        "is_proxy": True,
        "proxy_type": best.get("proxy_type", ""),
        "proxy_reason": best.get("proxy_reason", ""),
        "proxy_confidence": float(best.get("proxy_confidence", 0.0) or 0.0),
        "proxy_target": best.get("proxy_target", ""),
    }

# FIX2D59 — Canonical Identity Resolver v1
#
# Exact identity tuple definition (v1):
#   IdentityTupleV1 := {
#       'metric_token':   str,  # schema concept token / canonical_id (concept-level)
#       'time_scope':     str,  # normalized time token (e.g. '2024', 'ytd_2025') if known
#       'geo_scope':      str,  # normalized geo token (e.g. 'global', 'us') if known
#       'dims':           tuple[str,...], # normalized dimension-value tokens (segment/category/channel) if known
#       'dimension':      str,  # 'currency'|'unit_sales'|'percent'|'count'|'index'|'unknown'
#       'unit_family':    str,  # 'currency'|'percent'|'magnitude'|'energy'|'index'|''
#       'unit_tag':       str,  # 'USD'|'%'|'M'|'GWh' etc (normalized)
#       'statistic':      str,  # e.g. 'level'|'yoy_pct'|'cagr'|'share' (optional)
#       'aggregation':    str,  # e.g. 'total'|'avg' (optional)
#   }
#
# Resolver contract:
#   resolve_canonical_identity_v1(identity, metric_schema) -> {
#       'canonical_key': str,          # schema canonical_key if bound, else provisional key
#       'bound': bool,                 # True iff schema-bound
#       'status': str,                 # 'SCHEMA_BOUND' | 'PROVISIONAL'
#       'matched_schema_key': str|''   # the schema key chosen, if any
#   }
#
# Rules:
#   1) Schema-first: if metric_schema contains a canonical_key that matches the identity tuple, return it.
#   2) No silent canonical minting: if identity is under-specified (dimension unknown, or unit_family missing when unit_tag present), return PROVISIONAL.
#   3) Deterministic: matching and tie-breaks must be stable across runs.
#
# Note:
#   This resolver is intended to be used by BOTH Analysis and Evolution finalizers.

def normalize_metric_token_time_scope_v1(metric_token: str, time_scope: str = '') -> tuple:
    """Split embedded time tokens out of metric_token into time_scope (v1).

    Rules (deterministic):
      - Leading year prefix: '2024_global_ev_sales' -> metric_token='global_ev_sales', time_scope='2024'
      - Trailing year suffix: 'global_ev_sales_2024' -> metric_token='global_ev_sales', time_scope='2024'
      - YTD forms: 'global_ev_sales_ytd_2025' -> metric_token='global_ev_sales', time_scope='ytd_2025'
      - Forecast/projected forms: 'forecast_2035_sales' -> metric_token='sales', time_scope='forecast_2035'

    If time_scope is already provided, it is preserved.
    """
    mt = str(metric_token or '').strip().lower()
    ts = str(time_scope or '').strip().lower()
    if not mt:
        return '', ts
    if ts:
        return re.sub(r'_+', '_', mt).strip('_'), ts

    # ytd patterns
    m = re.search(r'(?:^|_)ytd[_-]?(20\d{2})(?:$|_)', mt)
    if m:
        year = m.group(1)
        ts = f'ytd_{year}'
        mt = re.sub(r'(?:^|_)ytd[_-]?%s(?:$|_)' % re.escape(year), '_', mt)

    # forecast/projected patterns (treat as forecast)
    m = re.search(r'(?:^|_)(forecast|projected|projection|estimate|estimated|target)[_-]?(20\d{2})(?:$|_)', mt)
    if m:
        year = m.group(2)
        ts = f'forecast_{year}'
        mt = re.sub(r'(?:^|_)(forecast|projected|projection|estimate|estimated|target)[_-]?%s(?:$|_)' % re.escape(year), '_', mt)

    # leading year
    m = re.match(r'^(20\d{2})_(.+)$', mt)
    if m and not ts:
        ts = m.group(1)
        mt = m.group(2)

    # trailing year
    m = re.match(r'^(.+?)_(20\d{2})$', mt)
    if m and not ts:
        mt = m.group(1)
        ts = m.group(2)

    mt = re.sub(r'_+', '_', mt).strip('_')
    ts = re.sub(r'_+', '_', ts).strip('_')
    return mt, ts

def build_identity_tuple_v1(*, metric_token: str, time_scope: str = '', geo_scope: str = '', dims=None,
                            dimension: str = '', unit_family: str = '', unit_tag: str = '',
                            statistic: str = '', aggregation: str = '') -> dict:
    'Construct a deterministic identity tuple (v1).'
    if dims is None:
        dims = ()
    if not isinstance(dims, (list, tuple)):
        dims = (str(dims),)
    metric_token, time_scope = normalize_metric_token_time_scope_v1(metric_token, time_scope)
    return {
        'metric_token': str(metric_token or '').strip().lower(),
        'time_scope': str(time_scope or '').strip().lower(),
        'geo_scope': str(geo_scope or '').strip().lower(),
        'dims': tuple([str(x or '').strip().lower() for x in list(dims) if str(x or '').strip()]),
        'dimension': str(dimension or '').strip().lower(),
        'unit_family': str(unit_family or '').strip().lower(),
        'unit_tag': str(unit_tag or '').strip(),
        'statistic': str(statistic or '').strip().lower(),
        'aggregation': str(aggregation or '').strip().lower(),
    }

def canonicalize_metrics(
    metrics: Dict,
    metric_schema: Dict = None,
    merge_duplicates_to_range: bool = True,
    question_text: str = "",
    category_hint: str = ""
) -> Dict:
    """
    Convert metrics to canonical IDs, but NEVER merge across incompatible dimensions.

    Key fix:
      - Adds deterministic 'dimension' classification and incorporates it into canonical keys.
      - Prevents revenue vs unit-sales from merging just because the year matches.
      - Keeps your geo + proxy tagging behavior.

    Output:
      canonicalized[canonical_key] -> metric dict with:
        - canonical_id (base id)
        - canonical_key (dimension-safe id you should use everywhere downstream)
        - dimension (currency | unit_sales | percent | count | index | unknown)
        - name (dimension-corrected display name)
    """
    if not isinstance(metrics, dict):
        return {}

    # - Prefer existing normalize_unit_tag/unit_family/canonicalize_numeric_candidate if present.
    # - Never breaks if those helpers are missing.
    def _safe_normalize_unit_tag(u: str) -> str:
        try:
            fn = globals().get("normalize_unit_tag")
            if callable(fn):
                return fn(u or "")
        except Exception:
            pass
        # minimal fallback (kept conservative)
        uu = (u or "").strip()
        ul = uu.lower().replace(" ", "")
        # - Legacy extracted units often arrive as phrases (e.g., 'million units', 'billion USD').
        # - We deterministically map magnitude words even when other tokens are present.
        if 'trillion' in ul or ul.endswith('tn') or ' tn' in (uu.lower()):
            return 'T'
        if 'billion' in ul or ul.endswith('bn') or ' bn' in (uu.lower()):
            return 'B'
        if 'million' in ul or ul.endswith('mn') or ' mn' in (uu.lower()) or 'mio' in ul:
            return 'M'
        if 'thousand' in ul or ul.endswith('k') or ' k' in (uu.lower()):
            return 'K'
        if ul in ("%", "pct", "percent"):
            return "%"
        if ul in ("twh",):
            return "TWh"
        if ul in ("gwh",):
            return "GWh"
        if ul in ("mwh",):
            return "MWh"
        if ul in ("kwh",):
            return "kWh"
        if ul in ("wh",):
            return "Wh"
        if ul in ("t", "trillion", "tn"):
            return "T"
        if ul in ("b", "bn", "billion"):
            return "B"
        if ul in ("m", "mn", "mio", "million"):
            return "M"
        if ul in ("k", "thousand", "000"):
            return "K"
        return uu

    def _safe_unit_family(unit_tag: str) -> str:
        try:
            fn = globals().get("unit_family")
            if callable(fn):
                return fn(unit_tag or "")
        except Exception:
            pass
        ut = (unit_tag or "").strip()
        if ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
            return "energy"
        if ut == "%":
            return "percent"
        if ut in ("T", "B", "M", "K"):
            return "magnitude"
        # currency not reliably derived here (handled elsewhere)
        return ""

    def infer_metric_dimension(metric_name: str, unit_raw: str) -> str:
        n = (metric_name or "").lower()
        u = (unit_raw or "").strip().lower()

        # Percent
        if "%" in u or "percent" in n or "share" in n or "cagr" in n:
            return "percent"

        # Currency signals
        currency_tokens = ["$", "s$", "usd", "sgd", "eur", "€", "gbp", "£", "jpy", "¥", "cny", "rmb", "aud", "cad"]
        if any(t in u for t in currency_tokens) or any(t in n for t in ["revenue", "market value", "valuation", "value (", "usd", "sgd", "eur"]):
            return "currency"

        # Unit sales / shipments
        unit_tokens = ["unit", "units", "sold", "sales", "sales volume", "shipments", "registrations", "deliveries", "vehicles", "pcs", "pieces", "volume"]
        if any(t in n for t in unit_tokens) or any(t in u for t in ["unit", "units", "vehicle", "vehicles", "pcs", "pieces"]):
            return "unit_sales"

        # Handles cases like 'Global EV Sales 2024' with unit 'million units' where name contains 'sales'
        # but unit tokens may not include 'units' in the name itself.
        if ('sales' in n or 'ev sales' in n) and (
            ('million' in u) or ('billion' in u) or ('thousand' in u) or ('mn' in u) or ('bn' in u) or
            ('unit' in u) or ('units' in u) or ('vehicle' in u) or ('vehicles' in u)
        ):
            return 'unit_sales'

        # Pure counts
        if any(t in n for t in ["count", "number of", "install base", "installed base", "users", "subscribers"]) and "revenue" not in n:
            return "count"

        # Index / score
        if any(t in n for t in ["index", "score", "rating"]):
            return "index"

        return "unknown"

    def display_name_for_dimension(original_display: str, dim: str) -> str:
        if not original_display:
            return original_display

        od = original_display.strip()
        od_low = od.lower()

        if dim == "unit_sales":
            if "revenue" in od_low or "market value" in od_low or "valuation" in od_low:
                return re.sub(r"(?i)revenue|market value|valuation", "Unit Sales", od).strip()
            if od_low.startswith("sales"):
                return "Unit Sales" + od[len("Sales"):]
            if "sales" in od_low:
                return re.sub(r"(?i)sales", "Unit Sales", od).strip()
            return od

        if dim == "currency":
            if "unit sales" in od_low:
                return re.sub(r"(?i)unit sales", "Revenue", od).strip()
            return od

        if dim == "percent":
            if "unit sales" in od_low or "revenue" in od_low:
                return od
            return od

        return od

    candidates = []

    for key, metric in metrics.items():
        if not isinstance(metric, dict):
            continue

        original_name = metric.get("name", key)
        canonical_id, canonical_name = get_canonical_metric_id(original_name)

        # - If the canonical base metric is in METRIC_REGISTRY, use its unit_type
        #   as a strong prior for dimension classification.
        # - This reduces mislabel drift like "Revenue" being assigned as unit_sales
        #   (or vice-versa) purely from noisy LLM labels.
        #
        # NOTE (conflict fix, additive):
        # - Your prior code risked UnboundLocalError due to base_id scoping.
        # - We keep your legacy behavior, but guard it and define base_id upfront.

        registry_unit_type = ""

        base_id = ""

        try:
            # - canonical_id may contain underscores inside the base id (e.g., "market_size_2025")
            # - Find the LONGEST registry key that is a prefix of canonical_id.
            try:
                reg = globals().get("METRIC_REGISTRY")
                cid = str(canonical_id or "")
                if isinstance(reg, dict) and cid:
                    # choose the longest matching prefix key
                    for k in reg.keys():
                        ks = str(k)
                        if cid == ks or cid.startswith(ks + "_"):
                            if len(ks) > len(base_id):
                                base_id = ks

                    if base_id and isinstance(reg.get(base_id), dict):
                        registry_unit_type = (reg[base_id].get("unit_type") or "").strip().lower()
            except Exception:
                pass
                # keep safe defaults
                pass

            # - This block is redundant with CM1.B, but we keep it as requested.
            # - Guard prevents:
            #   (1) base_id undefined
            #   (2) overwriting registry_unit_type already computed above
            if not registry_unit_type:
                reg = globals().get("METRIC_REGISTRY")
                if base_id and isinstance(reg, dict) and base_id in reg and isinstance(reg[base_id], dict):
                    registry_unit_type = (reg[base_id].get("unit_type") or "").strip().lower()

        except Exception:
            pass
            registry_unit_type = ""

        # Map registry unit_type -> canonicalize_metrics dimension vocabulary
        # (keep it small + deterministic)
        if registry_unit_type:
            if registry_unit_type in ("currency",):
                registry_dim_hint = "currency"
            elif registry_unit_type in ("percentage", "percent"):
                registry_dim_hint = "percent"
            elif registry_unit_type in ("count",):
                # keep "unit_sales" vs "count" distinction:
                # registry says count; name-based inference decides "unit_sales" if it sees units/shipments/deliveries
                registry_dim_hint = "count"
            else:
                registry_dim_hint = ""
        else:
            registry_dim_hint = ""

        raw_unit = (metric.get("unit") or "").strip()

        # - We keep your existing unit_norm logic for backwards compatibility.
        # - But we ALSO attach unit_tag + unit_family so downstream can gate deterministically.
        unit_tag = metric.get("unit_tag") or _safe_normalize_unit_tag(raw_unit)
        unit_family_tag = metric.get("unit_family") or _safe_unit_family(unit_tag)

        unit_norm = raw_unit.upper()  # keep original behavior (do not change)

        # - Adds an auditable trace showing *how* dimension + canonical_key were minted.
        # - This is intentionally local (no new dependencies) and additive only.
        # - Downstream UI/JSON can surface these fields to diagnose drift (e.g., __unknown).
        dim_inferred = infer_metric_dimension(str(original_name), raw_unit)
        dim = dim_inferred

        # - If registry says currency/percent, force that dimension.
        # - If registry says count, prevent accidental "currency"/"percent".
        _trace_dim_override = ""
        if registry_dim_hint in ("currency", "percent"):
            dim = registry_dim_hint
            _trace_dim_override = "registry_force"
        elif registry_dim_hint == "count":
            # Allow unit_sales if name clearly indicates it; else keep "count"
            if dim in ("currency", "percent"):
                dim = "count"
                _trace_dim_override = "registry_guard"

        _ident = build_identity_tuple_v1(metric_token=canonical_id, dimension=dim, unit_family=unit_family_tag, unit_tag=unit_tag, geo_scope=str(metric.get('geo_scope') or ''), time_scope='')
        _res = resolve_canonical_identity_v1(_ident, metric_schema)
        canonical_key = str(_res.get('canonical_key') or f"{canonical_id}__{dim}")

        # NOTE: metric_enriched is created below; stash trace ingredients now.
        _key_mint_trace = {
            "mint_fn": "canonicalize_metrics",
            "canonical_id": canonical_id,
            "canonical_key": canonical_key,
            "original_name": original_name,
            "canonical_name": canonical_name,
            "raw_unit": raw_unit,
            "unit_norm": unit_norm,
            "unit_tag": unit_tag,
            "unit_family": unit_family_tag,
            "dim_inferred": dim_inferred,
            "dim_final": dim,
            "registry_unit_type": registry_unit_type,
            "registry_dim_hint": registry_dim_hint,
            "dim_override": _trace_dim_override,
            "key_mint_path": ("REGISTRY_OVERRIDE" if _trace_dim_override else "NAME_UNIT_INFER"),
            "identity_tuple_v1": _ident if '_ident' in locals() else {},
            "identity_resolve_v1": _res if '_res' in locals() else {},
        }

        parsed_val = parse_to_float(metric.get("value"))
        value_for_sort = parsed_val if parsed_val is not None else str(metric.get("value", ""))

        stable_sort_key = (
            str(original_name).lower().strip(),
            dim,
            unit_norm,
            str(value_for_sort),
            str(key),
        )

        geo = infer_geo_scope(
            str(original_name),
            str(metric.get("context_snippet", "")),
            str(metric.get("source", "")),
            str(metric.get("source_url", "")),
        )

        proxy = infer_proxy_label(
            str(original_name),
            str(question_text),
            str(category_hint),
            str(metric.get("context_snippet", "")),
            str(metric.get("source", "")),
            str(metric.get("source_url", "")),
        )

        # - If canonicalize_numeric_candidate exists, it will attach:
        #   unit_tag/unit_family/base_unit/multiplier_to_base/value_norm
        # - If not, we attach minimal fields ourselves (still additive).
        metric_enriched = dict(metric)  # never mutate caller's dict
        try:
            fn_can = globals().get("canonicalize_numeric_candidate")
            if callable(fn_can):
                metric_enriched = fn_can(metric_enriched)
        except Exception:
            pass

        # Ensure minimal canonical fields exist (additive)
        metric_enriched.setdefault("unit_tag", unit_tag)
        metric_enriched.setdefault("unit_family", unit_family_tag)

        # Add mint trace (additive). Keep it under debug to avoid polluting top-level.
        try:
            metric_enriched.setdefault("debug", {})
            if isinstance(metric_enriched.get("debug"), dict):
                metric_enriched["debug"]["key_mint_trace"] = _key_mint_trace
        except Exception:
            pass

        candidates.append({
            "canonical_id": canonical_id,
            "canonical_key": canonical_key,
            "canonical_name": display_name_for_dimension(canonical_name, dim),
            "original_name": original_name,

            # NOTE: store enriched metric
            "metric": metric_enriched,

            "unit": unit_norm,
            "parsed_val": parsed_val,
            "dimension": dim,
            "stable_sort_key": stable_sort_key,
            "geo_scope": geo["geo_scope"],
            "geo_name": geo["geo_name"],
            **proxy,
        })

    candidates.sort(key=lambda x: x["stable_sort_key"])

    grouped: Dict[str, List[Dict]] = {}
    for c in candidates:
        grouped.setdefault(c["canonical_key"], []).append(c)

    canonicalized: Dict[str, Dict] = {}

    for ckey, group in grouped.items():
        if len(group) == 1 or not merge_duplicates_to_range:
            g = group[0]
            m = g["metric"]

            # (only adds keys; does not remove/rename existing keys)
            out_row = {
                **m,
                "name": g["canonical_name"],
                "canonical_id": g["canonical_id"],
                "canonical_key": ckey,
                "dimension": g["dimension"],
                "original_name": g["original_name"],
                "geo_scope": g.get("geo_scope", "unknown"),
                "geo_name": g.get("geo_name", ""),
                "is_proxy": bool(g.get("is_proxy", False)),
                "proxy_type": g.get("proxy_type", ""),
                "proxy_reason": g.get("proxy_reason", ""),
                "proxy_confidence": float(g.get("proxy_confidence", 0.0) or 0.0),
                "proxy_target": g.get("proxy_target", ""),
            }
            # Ensure these exist if upstream provided them
            for k in ["anchor_hash", "source_url", "context_snippet", "measure_kind", "measure_assoc",
                      "unit_tag", "unit_family", "base_unit", "multiplier_to_base", "value_norm"]:
                if k in m and k not in out_row:
                    out_row[k] = m.get(k)
            canonicalized[ckey] = out_row
            continue

        # Merge duplicates within SAME dimension-safe canonical_key
        base = group[0]
        base_metric = dict(base["metric"])
        base_metric["name"] = base["canonical_name"]
        base_metric["canonical_id"] = base["canonical_id"]
        base_metric["canonical_key"] = ckey
        base_metric["dimension"] = base["dimension"]

        geo_scope, geo_name = merge_group_geo(group)
        base_metric["geo_scope"] = geo_scope
        base_metric["geo_name"] = geo_name

        merged_proxy = merge_group_proxy(group)
        base_metric.update(merged_proxy)

        vals = [g["parsed_val"] for g in group if g["parsed_val"] is not None]
        raw_vals = [str(g["metric"].get("value", "")) for g in group]
        orig_names = [g["original_name"] for g in group]

        units = [g["unit"] for g in group if g["unit"]]
        unit_base = units[0] if units else (base_metric.get("unit") or "")
        base_metric["unit"] = unit_base

        base_metric["original_names"] = orig_names
        base_metric["raw_values"] = raw_vals

        # - Keeps your existing "range" untouched.
        # - Adds "range_norm" when we can compute it.
        vals_norm = []
        for g in group:
            mm = g.get("metric") if isinstance(g, dict) else {}
            if isinstance(mm, dict) and mm.get("value_norm") is not None:
                try:
                    vals_norm.append(float(mm.get("value_norm")))
                except Exception:
                    pass

        # Why:
        # - Avoid median/aggregate drift between analysis and evolution.
        # - If we already chose a specific evidence candidate (candidate_id/anchor_hash),
        #   that candidate should determine the metric's reported value/value_norm/unit.
        # Determinism:
        # - Select the evidence row with highest confidence if present, else first.
        # - No re-fetching, no new extraction; uses existing evidence payload only.
        _anchored_value_set = False
        try:
            _ev = base_metric.get("evidence")
            if isinstance(_ev, list) and _ev:
                # pick best evidence deterministically
                def _ev_score(e):
                    try:
                        c = e.get("confidence")
                        return float(c) if c is not None else 0.0
                    except Exception:
                        return 0.0
                _ev_sorted = sorted([e for e in _ev if isinstance(e, dict)], key=_ev_score, reverse=True)
                _best = _ev_sorted[0] if _ev_sorted else None

                if isinstance(_best, dict):
                    # Prefer canonical normalized fields if present
                    _bn = _best.get("value_norm")
                    _bu = _best.get("base_unit") or _best.get("unit")
                    _rawv = _best.get("raw") if _best.get("raw") is not None else _best.get("value")

                    if _bn is not None:
                        try:
                            base_metric["value_norm"] = float(_bn)
                        except Exception:
                            pass

                    # Preserve unit/base_unit
                    if _bu:
                        try:
                            base_metric["base_unit"] = str(_bu)
                        except Exception:
                            pass
                    if _best.get("unit"):
                        base_metric["unit"] = _best.get("unit")

                    # Preserve raw/value display from evidence (preferred)
                    if _rawv is not None:
                        base_metric["raw"] = _rawv
                        base_metric["value"] = _rawv

                    # Helpful debug: show that we anchored value from evidence
                    base_metric.setdefault("debug", {})
                    if isinstance(base_metric.get("debug"), dict):
                        base_metric["debug"]["value_origin"] = "evidence_best_candidate"
                        base_metric["debug"]["evidence_candidate_id"] = _best.get("candidate_id") or _best.get("anchor_hash")
                    _anchored_value_set = True
        except Exception:
            pass
        if vals and not _anchored_value_set:

            vals_sorted = sorted(vals)
            vmin, vmax = vals_sorted[0], vals_sorted[-1]
            vmed = vals_sorted[len(vals_sorted) // 2]
            base_metric["value"] = vmed
            base_metric["range"] = {
                "min": vmin,
                "max": vmax,
                "candidates": vals_sorted,
                "n": len(vals_sorted),
            }
        else:
            base_metric["range"] = {"min": None, "max": None, "candidates": [], "n": 0}

        if len(vals_norm) >= 2:
            vn = sorted(vals_norm)
            base_metric["range_norm"] = {
                "min": vn[0],
                "max": vn[-1],
                "candidates": vn,
                "n": len(vn),
                "unit": base_metric.get("base_unit") or base_metric.get("unit") or "",
            }

        canonicalized[ckey] = base_metric

    return canonicalized

# - Do NOT allow dimension=='unknown' or missing unit_family to enter primary_metrics_canonical.
# - Preserve them under primary_metrics_provisional with full debug trace for audit.
def _fix2d58b_split_primary_metrics_canonical(pmc: dict):
    try:
        if not isinstance(pmc, dict):
            return {}, {}
        canonical_ok = {}
        provisional = {}
        for k, v in pmc.items():
            if not isinstance(v, dict):
                provisional[k] = v
                continue
            dim = str(v.get('dimension') or '').strip().lower()
            uf = str(v.get('unit_family') or '').strip().lower()
            ut = str(v.get('unit_tag') or '').strip()
            # If dimension already implies family, fill it deterministically (additive).
            if dim in ('currency', 'percent') and not uf:
                vv = dict(v)
                vv['unit_family'] = dim
                v = vv
                uf = dim
            # Quarantine: unknown dimension OR missing family when we at least have a unit tag.
            if dim == 'unknown' or (not uf and bool(ut)):
                vv = dict(v)
                vv.setdefault('debug', {})
                if isinstance(vv.get('debug'), dict):
                    vv['debug']['quarantined_v1'] = True
                    vv['debug']['quarantine_reason_v1'] = 'unknown_dimension_or_missing_unit_family'
                provisional[k] = vv
            else:
                canonical_ok[k] = v
        return canonical_ok, provisional
    except Exception:
        pass
        # Fail-safe: never drop metrics silently if the splitter errors
        return pmc if isinstance(pmc, dict) else {}, {}

# - After FIX2D59 rekeying, primary_metrics_canonical must contain ONLY schema-bound keys.
# - Any PROVISIONAL/UNSPECIFIED rows are moved into primary_metrics_provisional.

def freeze_metric_schema(canonical_metrics: Dict) -> Dict:
    """
    Lock metric identity + expected schema for future evolution.

    Key fix:
      - Stores canonical_key (dimension-safe)
      - Stores dimension + unit family
      - Keywords include dimension hints to improve later matching
    """
    frozen = {}
    if not isinstance(canonical_metrics, dict):
        return frozen

    # This improves consistency with extractor + attribution gating.
    # Falls back safely to old heuristics.
    def _normalize_unit_safe(u: str) -> str:
        try:
            fn = globals().get("normalize_unit_tag")
            if callable(fn):
                return fn(u or "")
        except Exception:
            return (u or "").strip()

    def _unit_family_safe(unit_raw: str, dim_hint: str = "") -> str:
        # 1) dimension-first (strongest signal)
        d = (dim_hint or "").strip().lower()
        if d in ("percent", "pct"):
            return "percent"
        if d in ("currency",):
            return "currency"
        if d in ("energy",):
            return "energy"
        if d in ("unit_sales", "count"):
            # You’ve been treating M/B/T as "magnitude" for counts; keep aligned.
            return "magnitude"
        if d in ("index", "score"):
            return "index"

        # 2) if you already have a unit-family helper in the codebase, use it
        try:
            fn = globals().get("unit_family")
            if callable(fn):
                uf = fn(_normalize_unit_safe(unit_raw))
                if isinstance(uf, str) and uf.strip():
                    return uf.strip().lower()
        except Exception:
            pass

        # 3) fallback to old heuristic (your original logic)
        u = (unit_raw or "").strip().lower()
        if not u:
            return "unknown"
        if "%" in u:
            return "percent"
        if any(t in u for t in ["$", "s$", "usd", "sgd", "eur", "€", "gbp", "£", "jpy", "¥", "cny", "rmb"]):
            return "currency"
        if any(t in u for t in ["b", "bn", "billion", "m", "mn", "million", "k", "thousand", "t", "trillion"]):
            return "magnitude"
        return "other"

    for ckey, m in canonical_metrics.items():
        if not isinstance(m, dict):
            continue

        dim = (m.get("dimension") or "").strip() or "unknown"
        name = m.get("name")
        unit = (m.get("unit") or "").strip()

        uf = _unit_family_safe(unit, dim_hint=dim)

        # Keywords: name + dimension token to prevent cross-dimension matches later
        kws = extract_context_keywords(name or "") or []
        if dim and dim not in kws:
            kws.append(dim)
        if uf and uf not in kws:
            kws.append(uf)

        # - Keep your existing behavior in 'unit' (backward compatible),
        #   BUT also add 'unit_tag' which is the canonicalized unit used downstream.
        # - This avoids the "SGD -> S" schema corruption that breaks currency gating.
        unit_tag = _normalize_unit_safe(unit)
        # Keep existing 'unit' output to avoid breaking consumers:
        unit_out = unit_clean_first_letter(unit.upper())

        frozen[ckey] = {
            "canonical_key": ckey,
            "canonical_id": m.get("canonical_id") or ckey.split("__", 1)[0],
            "dimension": dim,
            "name": name,

            # Existing field kept exactly (backward compatible)
            "unit": unit_out,

            "unit_tag": unit_tag,          # e.g., "%", "M", "B", "TWh"
            "unit_family": uf,             # e.g., "currency", "percent", "magnitude"

            "keywords": kws[:30],
        }

# Deterministic metric_schema_frozen seeding (schema-frozen keyspace)
# - Replaces legacy FIX2U/FIX2V/FIX2AB schema extension helpers.
# - Purely additive: when applied to an existing schema dict, only missing keys are inserted.
_METRIC_SCHEMA_FROZEN_SEED_V1 = {
    "global_ev_chargers_2040__unit_count": {
        "canonical_key": "global_ev_chargers_2040__unit_count",
        "canonical_id": "global_ev_chargers_2040",
        "dimension": "count",
        "name": "Global EV chargers (2040)",
        "unit": "M",
        "unit_tag": "M",
        "unit_family": "magnitude",
        "keywords": [
            "ev", "electric", "vehicle", "charger", "chargers", "charging",
            "infrastructure", "network", "global", "worldwide", "2040",
            "count", "million", "m"
        ],
    },
    "global_ev_charging_investment_2040__currency": {
        "canonical_key": "global_ev_charging_investment_2040__currency",
        "canonical_id": "global_ev_charging_investment_2040",
        "dimension": "currency",
        "name": "Global EV charging investment (2040)",
        "unit": "U",
        "unit_tag": "USD",
        "unit_family": "currency",
        "keywords": [
            "ev", "electric", "vehicle", "charger", "chargers", "charging",
            "infrastructure", "network", "investment", "spend", "spending",
            "capex", "expenditure", "global", "worldwide", "2040",
            "currency", "usd"
        ],
    },
    "global_ev_chargers_cagr_2026_2040__percent": {
        "canonical_key": "global_ev_chargers_cagr_2026_2040__percent",
        "canonical_id": "global_ev_chargers_cagr_2026_2040",
        "dimension": "percent",
        "name": "Global EV chargers CAGR (2026–2040)",
        "unit": "%",
        "unit_tag": "%",
        "unit_family": "percent",
        "keywords": [
            "ev", "electric", "vehicle", "charger", "chargers", "charging",
            "global", "worldwide", "cagr", "growth", "rate", "percent",
            "2026", "2040"
        ],
    },
    "global_ev_sales_ytd_2025__unit_sales": {
        "canonical_key": "global_ev_sales_ytd_2025__unit_sales",
        "canonical_id": "global_ev_sales_ytd_2025",
        "dimension": "unit_sales",
        "name": "Global EV sales YTD (2025)",
        "unit": "M",
        "unit_tag": "M",
        "unit_family": "magnitude",
        "keywords": [
            "ev", "electric", "vehicle", "sales", "sold", "deliveries",
            "global", "worldwide", "ytd", "year-to-date", "2025",
            "million", "m"
        ],
    },
}

def _seed_metric_schema_frozen_v1(metric_schema_frozen: dict) -> dict:
    """Ensure the canonical schema-frozen keyspace exists (purely additive)."""
    try:
        if not isinstance(metric_schema_frozen, dict):
            metric_schema_frozen = {}
    except Exception:
        metric_schema_frozen = {}
    for _k, _spec in _METRIC_SCHEMA_FROZEN_SEED_V1.items():
        try:
            if _k not in metric_schema_frozen or not isinstance(metric_schema_frozen.get(_k), dict):
                _v = dict(_spec or {})
                if isinstance(_v.get("keywords"), list):
                    _v["keywords"] = list(_v["keywords"])
                metric_schema_frozen[_k] = _v
        except Exception:
            pass
    return metric_schema_frozen

# RANGE + SOURCE ATTRIBUTION (DETERMINISTIC, NO LLM)

def stable_json_hash(obj: Any) -> str:
    try:
        s = json.dumps(obj, sort_keys=True, ensure_ascii=False, separators=(",", ":"))
    except Exception:
        pass
        s = str(obj)
    return hashlib.sha256(s.encode("utf-8", errors="ignore")).hexdigest()

def make_extracted_number_id(source_url: str, fingerprint: str, n: Dict) -> str:
    payload = {
        "url": source_url or "",
        "fp": fingerprint or "",
        "start": n.get("start_idx"),
        "end": n.get("end_idx"),
        "value": n.get("value"),
        "unit": normalize_unit(n.get("unit") or ""),
        "raw": n.get("raw") or "",
        "ctx": " ".join((n.get("context_snippet") or "").split())[:240],
    }
    return stable_json_hash(payload)

def sort_snapshot_numbers(numbers: List[Dict]) -> List[Dict]:
    """
    Deterministic ordering for extracted_numbers in snapshots.

    Backward compatible + robust:
      - Uses start/end idx when present
      - Avoids hard dependency on normalize_unit() (may not exist)
      - Falls back to normalize_unit_tag() if available
    """

    # - Prefer normalize_unit() if it exists
    # - Else fall back to normalize_unit_tag() if present
    # - Else just return stripped unit
    _norm_unit_fn = globals().get("normalize_unit")
    _norm_tag_fn = globals().get("normalize_unit_tag")

    def _safe_norm_unit(u: str) -> str:
        u = (u or "").strip()
        try:
            if callable(_norm_unit_fn):
                return str(_norm_unit_fn(u) or "")
        except Exception:
            pass
        try:
            if callable(_norm_tag_fn):
                # normalize_unit_tag expects tags / unit-ish strings; still better than raw
                return str(_norm_tag_fn(u) or "")
        except Exception:
            return u

    def k(n: Dict[str, Any]):
        n = n or {}
        return (
            n.get("start_idx") if isinstance(n.get("start_idx"), int) else 10**18,
            n.get("end_idx") if isinstance(n.get("end_idx"), int) else 10**18,

            # stable identity ordering
            str(n.get("anchor_hash") or ""),

            # unit + value
            _safe_norm_unit(str(n.get("unit") or "")),
            str(n.get("unit_tag") or ""),
            str(n.get("value_norm") if n.get("value_norm") is not None else n.get("value")),

            # final tie-breakers
            str(n.get("raw") or ""),
            str(n.get("context_snippet") or n.get("context") or "")[:80],
        )

    return sorted((numbers or []), key=k)

def to_billions(value: float, unit_tag: str) -> Optional[float]:
    """Convert T/B/M tagged values into billions. Leaves % unchanged (returns None for % here)."""
    try:
        v = float(value)
    except Exception:
        return None

    if unit_tag == "T":
        return v * 1000.0
    if unit_tag == "B":
        return v
    if unit_tag == "M":
        return v / 1000.0
    return None

def build_metric_keywords(metric_name: str) -> List[str]:
    """Reuse your existing keyword extractor, but ensure we always have something."""
    kws = extract_context_keywords(metric_name) or []
    # Add simple fallback tokens (deterministic)
    for t in re.findall(r"[a-zA-Z]{4,}", str(metric_name).lower()):
        if t not in kws:
            kws.append(t)
    return kws[:25]

def extract_numbers_from_scraped_sources(
    scraped_content: Dict[str, str],
) -> List[Dict[str, Any]]:
    """
    Deterministically extract numeric candidates from all scraped source texts.
    Returns list of {url, value, unit_tag, raw, context}.
    """
    candidates: List[Dict[str, Any]] = []
    if not isinstance(scraped_content, dict):
        return candidates

    for url, content in scraped_content.items():
        if not content or not isinstance(content, str) or len(content) < 200:
            continue

        nums = extract_numbers_with_context(content, source_url=url)

        for n in nums:
            unit_tag = n.get("unit_tag")
            if not unit_tag:
                unit_tag = normalize_unit_tag(n.get("unit", ""))

            row = {
                "url": url,
                "value": n.get("value"),
                "unit_tag": unit_tag,
                "raw": n.get("raw", ""),
                "context": (n.get("context") or ""),
            }

            if "measure_kind" in n:
                row["measure_kind"] = n.get("measure_kind")
            if "measure_assoc" in n:
                row["measure_assoc"] = n.get("measure_assoc")

            # (backwards compatible: we only add keys, never remove)
            for k in [
                "unit", "is_junk", "junk_reason", "anchor_hash",
                "start_idx", "end_idx", "context_snippet",
                "unit_family", "base_unit", "multiplier_to_base", "value_norm"
            ]:
                if k in n:
                    row[k] = n.get(k)

            # Why:
            #   - Some candidates may not carry unit_family/base_unit/value_norm yet
            #   - We want every candidate (analysis + evolution) to have the same
            #     canonical fields so diff + span logic is stable and drift-free.
            #
            # This is additive and safe to call multiple times.
            try:
                fn_can = globals().get("canonicalize_numeric_candidate")
                if callable(fn_can):
                    row = fn_can(row) or row
                else:
                    row = canonicalize_numeric_candidate(row) or row
            except Exception:
                pass

            row.setdefault("unit_family", unit_family(row.get("unit_tag", "") or ""))
            row.setdefault("base_unit", row.get("unit_tag", "") or "")
            row.setdefault("multiplier_to_base", 1.0)
            if row.get("value") is not None and row.get("value_norm") is None:
                try:
                    row["value_norm"] = float(row.get("value"))
                except Exception:
                    pass

            candidates.append(row)

    return candidates

def attribute_span_to_sources(
    metric_name: str,
    metric_unit: str,
    scraped_content: Dict[str, str],
    rel_tol: float = 0.08,
    # - If provided, we enforce schema-first gating for drift stability.
    # - If not provided, we fall back to existing heuristic behavior.
    canonical_key: str = "",
    metric_schema: Dict[str, Any] = None,
) -> Dict[str, Any]:
    """
    Build a deterministic span (min/mid/max) for a metric, and attribute min/max to sources.
    Uses only scraped content + regex extractions (NO LLM).

    Schema-first behavior (when metric_schema/canonical_key provided):
      - Enforces unit_family and currency/count/percent gating from frozen schema
      - Uses measure_kind tags when available to avoid semantic leakage
      - Keeps deterministic tie-breaking
    """
    unit_tag_hint = normalize_unit_tag(metric_unit)
    keywords = build_metric_keywords(metric_name)

    all_candidates = extract_numbers_from_scraped_sources(scraped_content)
    filtered: List[Dict[str, Any]] = []

    metric_l = (metric_name or "").lower()

    schema_entry = None
    if isinstance(metric_schema, dict) and canonical_key and isinstance(metric_schema.get(canonical_key), dict):
        schema_entry = metric_schema.get(canonical_key)

    schema_unit_family = ""
    schema_dimension = ""
    schema_unit = ""
    if isinstance(schema_entry, dict):
        schema_unit_family = (schema_entry.get("unit_family") or "").strip().lower()
        schema_dimension = (schema_entry.get("dimension") or "").strip().lower()
        schema_unit = (schema_entry.get("unit") or "").strip()

    expected_family = ""
    if schema_unit_family in ("percent", "currency", "energy"):
        expected_family = schema_unit_family
    if not expected_family:
        ut = normalize_unit_tag(metric_unit)
        if ut == "%":
            expected_family = "percent"
        elif ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
            expected_family = "energy"
        else:
            expected_family = ""

    currencyish = False
    if schema_unit_family == "currency" or schema_dimension == "currency":
        currencyish = True
    if not currencyish:
        mu = (metric_unit or "").lower()
        if any(x in mu for x in ["usd", "sgd", "eur", "gbp", "$", "s$", "€", "£", "aud", "cad", "jpy", "cny", "rmb"]):
            currencyish = True
    if not currencyish and any(x in metric_l for x in ["revenue", "turnover", "valuation", "market value", "market size",
                                                       "profit", "earnings", "ebitda", "capex", "opex"]):
        currencyish = True

    expected_kind = None

    if expected_family == "percent":
        if any(k in metric_l for k in ["growth", "cagr", "increase", "decrease", "yoy", "qoq", "mom", "rate"]):
            expected_kind = "growth_pct"
        else:
            expected_kind = "share_pct"

    if currencyish:
        expected_kind = "money"

    if expected_kind is None and any(k in metric_l for k in [
        "units", "unit sales", "vehicle sales", "vehicles sold", "sold", "sales volume",
        "deliveries", "shipments", "registrations", "volume"
    ]):
        expected_kind = "count_units"

    metric_is_yearish = any(k in metric_l for k in ["year", "years", "fy", "fiscal", "calendar", "timeline", "target year"])

    def _looks_like_year_value(v) -> bool:
        try:
            iv = int(float(v))
            return 1900 <= iv <= 2099
        except Exception:
            return False

    def _ctx_has_year_range(ctx: str) -> bool:
        return bool(re.search(r"\b(19|20)\d{2}\s*(?:-|–|—|to)\s*(19|20)\d{2}\b", ctx or "", flags=re.I))

    def _has_currency_evidence(raw: str, ctx: str) -> bool:
        r = (raw or "")
        c = (ctx or "").lower()

        if any(s in r for s in ["$", "S$", "€", "£"]):
            return True
        if any(code in c for code in [" usd", "sgd", " eur", " gbp", " aud", " cad", " jpy", " cny", " rmb"]):
            return True

        strong_kw = [
            "revenue", "turnover", "valuation", "valued at", "market value", "market size",
            "sales value", "net profit", "operating profit", "gross profit",
            "ebitda", "earnings", "income", "capex", "opex"
        ]
        if any(k in c for k in strong_kw):
            return True
        return False

    # - Stable across runs, depends only on stable fields
    # - Used ONLY as final tie-breaker (won't change non-tie outcomes)
    def _candidate_id(x: dict) -> str:
        try:
            url = str(x.get("url") or x.get("source_url") or "")
            ah = str(x.get("anchor_hash") or "")
            vn = x.get("value_norm")
            bu = str(x.get("base_unit") or x.get("unit") or x.get("unit_tag") or "")
            mk = str(x.get("measure_kind") or "")
            # normalize numeric string for stability
            vn_s = ""
            if vn is not None:
                try:
                    vn_s = f"{float(vn):.12g}"
                except Exception:
                    pass
                    vn_s = str(vn)
            s = f"{url}|{ah}|{vn_s}|{bu}|{mk}"
            return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()
        except Exception:
            return ""

    for c in all_candidates:
        ctx = c.get("context", "")
        if not ctx:
            continue

        if c.get("is_junk") is True:
            continue

        if not metric_is_yearish:
            if (c.get("unit_tag") in ("", None)) and _looks_like_year_value(c.get("value")):
                continue
            if _looks_like_year_value(c.get("value")) and _ctx_has_year_range(ctx):
                continue

        ctx_score = calculate_context_match(keywords, ctx)
        if ctx_score <= 0.0:
            continue

        cand_ut = c.get("unit_tag") or normalize_unit_tag(c.get("unit") or "")
        cand_fam = (c.get("unit_family") or unit_family(cand_ut) or "").strip().lower()

        if expected_family:
            if expected_family == "percent" and cand_fam != "percent":
                continue
            if expected_family == "currency":
                if cand_fam not in ("currency", "magnitude"):
                    continue
                if not _has_currency_evidence(c.get("raw", ""), ctx):
                    continue
            if expected_family == "energy" and cand_fam != "energy":
                continue

        if expected_kind:
            mk = c.get("measure_kind")
            if mk and mk != expected_kind:
                continue

        val_norm = None
        if expected_family == "percent" or unit_tag_hint == "%":
            if cand_ut != "%":
                continue
            val_norm = c.get("value")

        elif expected_family == "energy":
            val_norm = c.get("value_norm")
            if val_norm is None:
                val_norm = c.get("value")

        elif currencyish or expected_family == "currency":
            if c.get("measure_kind") == "count_units":
                continue
            if cand_ut not in ("T", "B", "M"):
                continue
            val_norm = to_billions(c.get("value"), cand_ut)
            if val_norm is None:
                continue

        else:
            try:
                val_norm = float(c.get("value"))
            except Exception:
                pass
                continue

        row = {
            **c,
            "unit_tag": cand_ut,
            "unit_family": cand_fam,
            "value_norm": val_norm,
            "ctx_score": float(ctx_score),
        }

        row.setdefault("candidate_id", _candidate_id(row))

        filtered.append(row)

    if not filtered:
        return {
            "span": None,
            "source_attribution": None,
            "evidence": []
        }

    # Deterministic selection: value_norm then ctx_score then url then candidate_id
    def min_key(x):
        return (
            float(x["value_norm"]),
            -float(x["ctx_score"]),
            str(x.get("url", "")),
            str(x.get("candidate_id", "")),
        )

    def max_key(x):
        return (
            -float(x["value_norm"]),
            -float(x["ctx_score"]),
            str(x.get("url", "")),
            str(x.get("candidate_id", "")),
        )

    min_item = sorted(filtered, key=min_key)[0]
    max_item = sorted(filtered, key=max_key)[0]

    vmin = float(min_item["value_norm"])
    vmax = float(max_item["value_norm"])
    vmid = (vmin + vmax) / 2.0

    if expected_family == "percent" or unit_tag_hint == "%":
        unit_out = "%"
    elif currencyish or expected_family == "currency":
        unit_out = "billion USD"
    elif expected_family == "energy":
        unit_out = "Wh"
    else:
        unit_out = metric_unit or (schema_unit or "")

    evidence = []
    for it in sorted(filtered, key=lambda x: (-float(x["ctx_score"]), str(x.get("url", "")), str(x.get("candidate_id", ""))))[:12]:
        evidence.append({
            "url": it.get("url"),
            "raw": it.get("raw"),
            "unit_tag": it.get("unit_tag"),
            "unit_family": it.get("unit_family"),
            "measure_kind": it.get("measure_kind"),
            "measure_assoc": it.get("measure_assoc"),
            "value_norm": it.get("value_norm"),
            "candidate_id": it.get("candidate_id"),
            "anchor_hash": it.get("anchor_hash"),
            "start_idx": it.get("start_idx"),
            "end_idx": it.get("end_idx"),
            "value_norm": it.get("value_norm"),
            "base_unit": it.get("base_unit"),
            "multiplier_to_base": it.get("multiplier_to_base"),  # PATCH S11: exposed for transparency
            "context_snippet": (it.get("context") or "")[:220],
            "context_score": round(float(it.get("ctx_score", 0.0)) * 100, 1),
        })

    return {
        "span": {
            "min": round(vmin, 4),
            "mid": round(vmid, 4),
            "max": round(vmax, 4),
            "unit": unit_out
        },
        "source_attribution": {
            "min": {
                "url": min_item.get("url"),
                "raw": min_item.get("raw"),
                "measure_kind": min_item.get("measure_kind"),
                "measure_assoc": min_item.get("measure_assoc"),
                "value_norm": min_item.get("value_norm"),
                "candidate_id": min_item.get("candidate_id"),  # PATCH S11
                "context_snippet": (min_item.get("context") or "")[:220],
                "context_score": round(float(min_item.get("ctx_score", 0.0)) * 100, 1),
            },
            "max": {
                "url": max_item.get("url"),
                "raw": max_item.get("raw"),
                "measure_kind": max_item.get("measure_kind"),
                "measure_assoc": max_item.get("measure_assoc"),
                "value_norm": max_item.get("value_norm"),
                "candidate_id": max_item.get("candidate_id"),  # PATCH S11
                "context_snippet": (max_item.get("context") or "")[:220],
                "context_score": round(float(max_item.get("ctx_score", 0.0)) * 100, 1),
            }
        },
        "evidence": evidence
    }

def add_range_and_source_attribution_to_canonical_metrics(
    canonical_metrics: Dict[str, Any],
    web_context: dict,
    # If provided, attribution uses frozen schema to avoid semantic/unit leakage.
    metric_schema: Dict[str, Any] = None,
) -> Dict[str, Any]:
    """
    Enrich canonical metrics with deterministic range + source attribution.

    IMPORTANT:
    - canonical_metrics is expected to be keyed by canonical_key (dimension-safe),
      i.e. the output of canonicalize_metrics().
    - Schema-first mode (recommended): pass metric_schema=metric_schema_frozen so
      attribute_span_to_sources() can enforce unit_family / measure_kind gates.
    - Backward compatible: if metric_schema not provided, attribution falls back
      to existing heuristic behavior inside attribute_span_to_sources().
    """
    enriched: Dict[str, Any] = {}
    if not isinstance(canonical_metrics, dict):
        return enriched

    scraped = (web_context or {}).get("scraped_content") or {}
    if not isinstance(scraped, dict):
        scraped = {}

    schema = metric_schema if isinstance(metric_schema, dict) else {}

    for ckey, m in canonical_metrics.items():
        if not isinstance(m, dict):
            continue

        metric_name = m.get("name") or m.get("original_name") or str(ckey)
        metric_unit = m.get("unit") or ""

        # - canonical_key is the dict key (ckey)
        # - metric_schema is the frozen schema dict (if provided)
        span_pack = attribute_span_to_sources(
            metric_name=metric_name,
            metric_unit=metric_unit,
            scraped_content=scraped,
            canonical_key=str(ckey),
            metric_schema=schema,
        )

        mm = dict(m)

        # Preserve old behavior: only add keys (don’t remove anything)
        if isinstance(span_pack, dict):
            if span_pack.get("span") is not None:
                mm["source_span"] = span_pack.get("span")
            if span_pack.get("source_attribution") is not None:
                mm["source_attribution"] = span_pack.get("source_attribution")
            if span_pack.get("evidence") is not None:
                mm["evidence"] = span_pack.get("evidence")

        enriched[ckey] = mm

    return enriched

# SEMANTIC FINDING HASH
# Removes wording-based churn from findings comparison

# Semantic components to extract from findings
FINDING_PATTERNS = {
    # Growth/decline patterns
    "growth": [
        r'(?:grow(?:ing|th)?|increas(?:e|ing)|expand(?:ing)?|ris(?:e|ing)|up)\s*(?:by|at|of)?\s*(\d+(?:\.\d+)?)\s*%?',
        r'(\d+(?:\.\d+)?)\s*%?\s*(?:growth|increase|expansion|rise)',
    ],
    "decline": [
        r'(?:declin(?:e|ing)|decreas(?:e|ing)|fall(?:ing)?|drop(?:ping)?|down)\s*(?:by|at|of)?\s*(\d+(?:\.\d+)?)\s*%?',
        r'(\d+(?:\.\d+)?)\s*%?\s*(?:decline|decrease|drop|fall)',
    ],

    # Value patterns
    "value": [
        r'\$\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)?',
        r'(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)',
    ],

    # Ranking patterns
    "rank": [
        r'(?:lead(?:ing|er)?|top|first|largest|biggest|#1|number one)',
        r'(?:second|#2|runner.?up)',
        r'(?:third|#3)',
    ],

    # Trend patterns
    "trend_up": [
        r'(?:bullish|optimistic|positive|strong|robust|accelerat)',
    ],
    "trend_down": [
        r'(?:bearish|pessimistic|negative|weak|slow(?:ing)?|decelerat)',
    ],

    # Entity patterns (will be filled dynamically)
    "entities": []
}

# Common stop words to remove
STOP_WORDS = {
    'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
    'should', 'may', 'might', 'must', 'shall', 'can', 'to', 'of', 'in',
    'for', 'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through',
    'during', 'before', 'after', 'above', 'below', 'between', 'under',
    'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where',
    'why', 'how', 'all', 'each', 'few', 'more', 'most', 'other', 'some',
    'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',
    'too', 'very', 'just', 'also', 'now', 'new'
}

def extract_semantic_components(finding: str) -> Dict[str, Any]:
    """
    Extract semantic components from a finding.

    Example:
        "The market is growing at 15% annually" ->
        {
            "direction": "up",
            "percentage": 15.0,
            "subject": "market",
            "timeframe": "annual",
            "entities": [],
            "keywords": ["market", "growing", "annually"]
        }
    """
    if not finding:
        return {}

    finding_lower = finding.lower()
    components = {
        "direction": None,
        "percentage": None,
        "value": None,
        "value_unit": None,
        "subject": None,
        "timeframe": None,
        "entities": [],
        "keywords": []
    }

    # Extract direction
    for pattern in FINDING_PATTERNS["growth"]:
        match = re.search(pattern, finding_lower)
        if match:
            components["direction"] = "up"
            if match.groups():
                try:
                    components["percentage"] = float(match.group(1))
                except:
                    pass
            break

    if not components["direction"]:
        for pattern in FINDING_PATTERNS["decline"]:
            match = re.search(pattern, finding_lower)
            if match:
                components["direction"] = "down"
                if match.groups():
                    try:
                        components["percentage"] = float(match.group(1))
                    except:
                        pass
                break

    # Extract trend sentiment
    if not components["direction"]:
        for pattern in FINDING_PATTERNS["trend_up"]:
            if re.search(pattern, finding_lower):
                components["direction"] = "up"
                break
        for pattern in FINDING_PATTERNS["trend_down"]:
            if re.search(pattern, finding_lower):
                components["direction"] = "down"
                break

    # Extract value
    for pattern in FINDING_PATTERNS["value"]:
        match = re.search(pattern, finding_lower)
        if match:
            try:
                components["value"] = float(match.group(1))
                if len(match.groups()) > 1 and match.group(2):
                    components["value_unit"] = match.group(2)[0].upper()
            except:
                pass
            break

    # Extract timeframe
    timeframe_patterns = {
        "annual": r'\b(?:annual(?:ly)?|year(?:ly)?|per year|yoy|y-o-y)\b',
        "quarterly": r'\b(?:quarter(?:ly)?|q[1-4])\b',
        "monthly": r'\b(?:month(?:ly)?|per month)\b',
        "2024": r'\b2024\b',
        "2025": r'\b2025\b',
        "2026": r'\b2026\b',
        "2030": r'\b2030\b',
    }
    for tf_name, tf_pattern in timeframe_patterns.items():
        if re.search(tf_pattern, finding_lower):
            components["timeframe"] = tf_name
            break

    # Extract subject keywords
    words = re.findall(r'\b[a-z]{3,}\b', finding_lower)
    keywords = [w for w in words if w not in STOP_WORDS]
    components["keywords"] = keywords[:10]  # Limit to top 10

    # Identify likely subject
    subject_candidates = ["market", "industry", "sector", "segment", "revenue", "sales", "demand", "supply"]
    for word in keywords:
        if word in subject_candidates:
            components["subject"] = word
            break

    return components

def compute_semantic_hash(finding: str) -> str:
    """
    Compute a semantic hash for a finding that's invariant to wording changes.

    Two findings with the same meaning should produce the same hash.

    Example:
        "The market is growing at 15% annually" -> "up_15.0_market_annual"
        "Annual growth rate stands at 15%" -> "up_15.0_market_annual"
    """
    components = extract_semantic_components(finding)

    # Build hash components in consistent order
    hash_parts = []

    # Direction
    if components.get("direction"):
        hash_parts.append(components["direction"])

    # Percentage (rounded to avoid float issues)
    if components.get("percentage") is not None:
        hash_parts.append(f"{components['percentage']:.1f}")

    # Value with unit
    if components.get("value") is not None:
        val_str = f"{components['value']:.1f}"
        if components.get("value_unit"):
            val_str += components["value_unit"]
        hash_parts.append(val_str)

    # Subject
    if components.get("subject"):
        hash_parts.append(components["subject"])

    # Timeframe
    if components.get("timeframe"):
        hash_parts.append(components["timeframe"])

    # If we have enough components, use them for hash
    if len(hash_parts) >= 2:
        return "_".join(hash_parts)

    # Fallback: use sorted keywords
    keywords = sorted(components.get("keywords", []))[:5]
    if keywords:
        return "_".join(keywords)

    # Last resort: normalized text hash
    normalized = re.sub(r'\s+', ' ', finding.lower().strip())
    return hashlib.md5(normalized.encode()).hexdigest()[:12]

# UPDATED METRIC DIFF COMPUTATION
# Using canonical IDs

# NUMERIC PARSING (DETERMINISTIC)

def parse_to_float(value: Any) -> Optional[float]:
    """
    Deterministically parse any value to float.
    Returns None if unparseable.
    """
    if value is None:
        return None
    if isinstance(value, (int, float)):
        return float(value)
    if not isinstance(value, str):
        return None

    # Clean string
    cleaned = value.strip().upper()
    cleaned = re.sub(r'[,$]', '', cleaned)

    # Handle empty/NA
    if cleaned in ['', 'N/A', 'NA', 'NULL', 'NONE', '-', '—']:
        return None

    # Extract multiplier
    multiplier = 1.0
    if 'TRILLION' in cleaned or cleaned.endswith('T'):
        multiplier = 1_000_000
        cleaned = re.sub(r'T(?:RILLION)?', '', cleaned)
    elif 'BILLION' in cleaned or cleaned.endswith('B'):
        multiplier = 1_000
        cleaned = re.sub(r'B(?:ILLION)?', '', cleaned)
    elif 'MILLION' in cleaned or cleaned.endswith('M'):
        multiplier = 1
        cleaned = re.sub(r'M(?:ILLION)?', '', cleaned)
    elif 'THOUSAND' in cleaned or cleaned.endswith('K'):
        multiplier = 0.001
        cleaned = re.sub(r'K(?:THOUSAND)?', '', cleaned)

    # Handle percentages
    if '%' in cleaned:
        cleaned = cleaned.replace('%', '')
        # Don't apply multiplier to percentages
        multiplier = 1.0

    try:
        return float(cleaned.strip()) * multiplier
    except (ValueError, TypeError):
        return None

def _fmt_currency_first(raw: str, unit: str) -> str:
    """
    Display helper: formats value+unit as currency-first when applicable.

    Examples:
      - raw='29.8', unit='S$B'  -> 'S$29.8B'
      - raw='120',  unit='USD M' -> '$120M'
      - raw='29.8', unit='%'     -> '29.8%'
      - raw='90',   unit='M'     -> '90 M'
    """
    raw = (raw or "").strip()
    unit = (unit or "").strip()

    if not raw or raw == "-":
        return "-"

    # If already currency-first, trust it
    if raw.startswith("S$") or raw.startswith("$"):
        return raw

    # Percent case
    if unit == "%":
        return f"{raw}%"

    # Detect currency from unit
    currency = ""
    scale = unit.replace(" ", "")

    if scale.upper().startswith("SGD"):
        currency = "S$"
        scale = scale[3:]
    elif scale.upper().startswith("USD"):
        currency = "$"
        scale = scale[3:]
    elif scale.startswith("S$"):
        currency = "S$"
        scale = scale[2:]
    elif scale.startswith("$"):
        currency = "$"
        scale = scale[1:]

    # Human-readable units
    if unit.lower().endswith("billion"):
        return f"{currency}{raw} billion".strip()
    if unit.lower().endswith("million"):
        return f"{currency}{raw} million".strip()

    # Compact units (B/M/K)
    if scale.upper() in {"B", "M", "K"}:
        return f"{currency}{raw}{scale}".strip()

    # Fallback
    return f"{currency}{raw} {unit}".strip()

# NAME MATCHING (DETERMINISTIC)

# DETERMINISTIC QUERY STRUCTURE EXTRACTION
# - Classify query into a known category (country / industry / etc.)
# - Extract main question + "side questions" deterministically
# - Optional: spaCy dependency parse (if installed)
# - Optional: embedding similarity (if sentence-transformers/sklearn installed)

QUESTION_CATEGORIES = {
    "country": {
        "signals": [
            "gdp", "gdp per capita", "population", "inflation", "interest rate",
            "exports", "imports", "trade balance", "currency", "fx", "central bank",
            "unemployment", "fiscal", "budget", "debt", "sovereign", "country"
        ],
        "template_sections": [
            "GDP & GDP per capita", "Growth rates", "Population & demographics",
            "Key industries", "Exports & imports", "Currency & FX trends",
            "Interest rates & inflation", "Risks & outlook"
        ],
    },
    "industry": {
        "signals": [
            "market size", "tam", "cagr", "industry", "sector", "market",
            "key players", "competitive landscape", "drivers", "challenges",
            "regulation", "technology trends", "forecast"
        ],
        "template_sections": [
            "Total Addressable Market (TAM) / Market size", "Growth rates (CAGR/YoY)",
            "Key players", "Key drivers", "Challenges & risks",
            "Technology trends", "Regulatory / environmental factors", "Outlook"
        ],
    },
    "company": {
        "signals": [
            "revenue", "earnings", "profit", "margins", "guidance",
            "business model", "segments", "customers", "competitors",
            "valuation", "multiple", "pe ratio", "cash flow"
        ],
        "template_sections": [
            "Business overview", "Revenue / profitability", "Segments",
            "Competitive position", "Key risks", "Guidance / outlook"
        ],
    },
    "unknown": {
        "signals": [],
        "template_sections": [],
    }
}

def detect_query_category(query: str) -> Dict[str, Any]:
    """
    Deterministically classify query category using keyword signals.
    Returns: {"category": "...", "confidence": 0-1, "matched_signals": [...]}
    """
    q = (query or "").lower()
    best_cat = "unknown"
    best_hits = 0
    best_matched = []

    for cat, cfg in QUESTION_CATEGORIES.items():
        if cat == "unknown":
            continue
        matched = [s for s in cfg["signals"] if s in q]
        hits = len(matched)
        if hits > best_hits:
            best_hits = hits
            best_cat = cat
            best_matched = matched

    # simple confidence: saturate after ~6 hits
    conf = min(best_hits / 6.0, 1.0) if best_hits > 0 else 0.0
    return {"category": best_cat, "confidence": round(conf, 2), "matched_signals": best_matched[:8]}

# 3A+. LAYERED QUERY STRUCTURE PARSER (Deterministic -> NLP -> Embeddings -> LLM fallback)

_QUERY_SPLIT_PATTERNS = [
    r"\bas well as\b",
    r"\balong with\b",
    r"\bin addition to\b",
    r"\band\b",
    r"\bplus\b",
    r"\bvs\.?\b",
    r"\bversus\b",
    r",",
    r";",
]

_COUNTRY_OVERVIEW_SIGNALS = [
    "in general", "overview", "tell me about", "general", "profile", "facts about",
    "economy", "population", "gdp", "currency", "exports", "imports",
]

def _normalize_q(q: str) -> str:
    q = (q or "").strip()
    q = re.sub(r"\s+", " ", q)
    return q

def _split_clauses_deterministic(q: str) -> List[str]:
    """
    Deterministically split a question into ordered clauses.

    Supports:
    - comma/connector splits (",", "and", "as well as", "in addition to", etc.)
    - multi-side enumerations like:
        "in addition to: 1. X 2. Y"
        "including: (1) X (2) Y"
        "as well as: • X • Y"
    """
    if not isinstance(q, str):
        return []

    s = q.strip()
    if not s:
        return []

    # Normalize whitespace early (keep original casing if present; upstream may lowercase already)
    s = re.sub(r"\s+", " ", s).strip()

    # --- Step A: If there's an enumeration intro, split head vs tail ---
    # Examples: "in addition to:", "including:", "plus:", "as well as:"
    enum_intro = re.search(
        r"\b(in addition to|in addition|including|in addition to the following|as well as|plus)\b\s*:?\s*",
        s,
        flags=re.IGNORECASE,
    )

    head = s
    tail = ""

    if enum_intro:
        # Split at the FIRST occurrence of the enum phrase
        idx = enum_intro.start()
        # head is everything before the phrase if it exists, otherwise keep whole string
        # but we usually want "Tell me about X in general" to remain in head.
        head = s[:idx].strip().rstrip(",")
        tail = s[enum_intro.end():].strip()

        # If head is empty (e.g., query begins with "In addition to:"), treat everything as head
        if not head:
            head = s
            tail = ""

    clauses: List[str] = []

    # --- Step B: Split head using your existing connector patterns ---
    if head:
        parts = [head]
        for pat in _QUERY_SPLIT_PATTERNS:
            next_parts = []
            for p in parts:
                next_parts.extend(re.split(pat, p, flags=re.IGNORECASE))
            parts = next_parts

        for p in parts:
            p = p.strip(" ,;:.").strip()
            if p:
                clauses.append(p)

    # --- Step C: If tail exists, split as enumerated items/bullets ---
    if tail:
        # Split on "1.", "1)", "(1)", "•", "-", "*"
        # Keep it robust: find item starts, then slice.
        item_start = re.compile(r"(?:^|\s)(?:\(?\d+\)?[\.\)]|[•\-\*])\s+", flags=re.IGNORECASE)
        starts = [m.start() for m in item_start.finditer(tail)]

        if starts:
            # Build slices using detected starts
            spans = []
            for i, st0 in enumerate(starts):
                st = st0
                # Move start to the start of token (strip leading whitespace)
                while st < len(tail) and tail[st].isspace():
                    st += 1
                en = starts[i + 1] if i + 1 < len(starts) else len(tail)
                spans.append((st, en))

            for st, en in spans:
                item = tail[st:en].strip(" ,;:.").strip()
                # Remove the leading bullet/number token again (safety)
                item = re.sub(r"^\(?\d+\)?[\.\)]\s+", "", item)
                item = re.sub(r"^[•\-\*]\s+", "", item)
                item = item.strip(" ,;:.").strip()
                if item:
                    clauses.append(item)
        else:
            # If tail doesn't look enumerated, fall back to normal splitter on tail
            parts = [tail]
            for pat in _QUERY_SPLIT_PATTERNS:
                next_parts = []
                for p in parts:
                    next_parts.extend(re.split(pat, p, flags=re.IGNORECASE))
                parts = next_parts

            for p in parts:
                p = p.strip(" ,;:.").strip()
                if p:
                    clauses.append(p)

    # Final cleanup + dedupe while preserving order
    out: List[str] = []
    seen = set()
    for c in clauses:
        c2 = c.strip()
        if not c2:
            continue
        if c2.lower() in seen:
            continue
        seen.add(c2.lower())
        out.append(c2)

    return out

def _dedupe_clauses(clauses: List[str]) -> List[str]:
    seen = set()
    out = []
    for c in clauses:
        c2 = c.strip().lower()
        if not c2 or c2 in seen:
            continue
        seen.add(c2)
        out.append(c.strip())
    return out

def _choose_main_and_side(clauses: List[str]) -> Tuple[str, List[str]]:
    """
    Pick 'main' as the first clause; side = remainder.
    Deterministic, stable across runs.
    """
    clauses = _dedupe_clauses(clauses)
    if not clauses:
        return "", []
    main = clauses[0]
    side = clauses[1:]
    return main, side

def _try_spacy_nlp():
    """
    Optional NLP layer. If spaCy is installed, use it; otherwise return None.
    """
    try:
        import spacy  # type: ignore
        # Avoid heavy model loading; prefer blank model with sentencizer if no model available.
        try:
            nlp = spacy.load("en_core_web_sm")  # common if installed
        except Exception:
            pass
            nlp = spacy.blank("en")
            if "sentencizer" not in nlp.pipe_names:
                nlp.add_pipe("sentencizer")
        return nlp
    except Exception:
        return None

def _nlp_refine_clauses(query: str, clauses: List[str]) -> Dict[str, Any]:
    """
    Use dependency/NER cues to:
      - detect country-overview questions
      - improve main-vs-side decision (coordination / 'as well as' patterns)
    Returns partial overrides: {"main":..., "side":[...], "hints":{...}}
    """
    nlp = _try_spacy_nlp()
    if not nlp:
        return {"hints": {"nlp_used": False}}

    doc = nlp(_normalize_q(query))
    # Named entities that look like places
    gpes = [ent.text for ent in getattr(doc, "ents", []) if ent.label_ in ("GPE", "LOC")]
    gpes_norm = [g.strip() for g in gpes if g and len(g.strip()) > 1]

    # Coordination hint: if query has "as well as" or "and", keep deterministic split,
    # but try to pick the more "general" clause as main when overview signals exist.
    overview_hit = any(sig in (query or "").lower() for sig in _COUNTRY_OVERVIEW_SIGNALS)
    hints = {
        "nlp_used": True,
        "gpe_entities": gpes_norm[:5],
        "overview_signal_hit": bool(overview_hit),
    }

    main, side = _choose_main_and_side(clauses)

    # If overview signals + place entity present, bias main to the overview clause
    if overview_hit and gpes_norm:
        # choose clause with strongest overview signal density
        def score_clause(c: str) -> int:
            c = c.lower()
            return sum(1 for sig in _COUNTRY_OVERVIEW_SIGNALS if sig in c)
        scored = sorted([(score_clause(c), c) for c in clauses], key=lambda x: (-x[0], x[1]))
        if scored and scored[0][0] > 0:
            main = scored[0][1]
            side = [c for c in clauses if c != main]

    return {"main": main, "side": side, "hints": hints}

def _embedding_category_vote(query: str) -> Dict[str, Any]:
    """
    Deterministic 'embedding-like' similarity using TF-IDF (no external model downloads).
    Produces a category suggestion + confidence based on similarity to category descriptors.
    """
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore
        from sklearn.metrics.pairwise import cosine_similarity  # type: ignore
    except Exception:
        return {"category": "unknown", "confidence": 0.0, "method": "tfidf_unavailable"}

    q = _normalize_q(query).lower()
    if not q:
        return {"category": "unknown", "confidence": 0.0, "method": "tfidf_empty"}

    # Build deterministic descriptors from your registry
    cat_texts = []
    cat_names = []
    for cat, cfg in (QUESTION_CATEGORIES or {}).items():
        if not isinstance(cfg, dict) or cat == "unknown":
            continue
        signals = " ".join(cfg.get("signals", [])[:50])
        sections = " ".join((cfg.get("template_sections", []) or [])[:50])
        descriptor = f"{cat} {signals} {sections}".strip()
        if descriptor:
            cat_names.append(cat)
            cat_texts.append(descriptor)

    if not cat_texts:
        return {"category": "unknown", "confidence": 0.0, "method": "tfidf_no_registry"}

    vec = TfidfVectorizer(ngram_range=(1, 2), min_df=1, max_features=8000)
    X = vec.fit_transform(cat_texts + [q])
    sims = cosine_similarity(X[-1], X[:-1]).flatten()

    best_idx = int(sims.argmax()) if sims.size else 0
    best_sim = float(sims[best_idx]) if sims.size else 0.0
    best_cat = cat_names[best_idx] if cat_names else "unknown"

    # Map cosine similarity (~0-1) into a conservative confidence
    conf = max(0.0, min(best_sim / 0.35, 1.0))  # 0.35 sim ~= "high"
    return {"category": best_cat, "confidence": round(conf, 2), "method": "tfidf"}

def _llm_fallback_query_structure(query: str, web_context: Optional[Dict] = None, out_debug: Optional[Dict[str, Any]] = None) -> Optional[Dict[str, Any]]:
    """
    Last resort: ask LLM to output ONLY a small JSON query-structure object.
    Guardrail: do NOT let the LLM invent extra side questions unless the user explicitly enumerated them.
    This path must NOT validate against LLMResponse.
    """
    try:
        q = str(query or "").strip()
        if not q:
            return None

        # --- Detect explicit enumeration / list structure in the USER query ---
        # If the user wrote a list (1., 2), bullets, etc.), it's reasonable to accept multiple sides.
        enum_patterns = [
            r"(^|\n)\s*\d+\s*[\.\)]\s+",     # 1.  / 2)
            r"(^|\n)\s*[-•*]\s+",           # - item / • item
            r"(^|\n)\s*[a-zA-Z]\s*[\.\)]\s+"  # a) / b. etc.
        ]
        has_explicit_enumeration = any(re.search(p, q, flags=re.MULTILINE) for p in enum_patterns)

        # Deterministic baseline (what the system already extracted)
        # We use this to clamp LLM hallucinations.
        det_clauses = _split_clauses_deterministic(_normalize_q(q))
        det_main, det_side = _choose_main_and_side(det_clauses)
        det_side = _dedupe_clauses([s.strip() for s in (det_side or []) if isinstance(s, str) and s.strip()])

        prompt = (
            "Extract a query structure.\n"
            "Return ONLY valid JSON with keys:\n"
            "  category: one of [country, industry, company, finance, market, unknown]\n"
            "  category_confidence: number 0-1\n"
            "  main: string (the main question/topic)\n"
            "  side: array of strings (side questions)\n"
            "No extra keys, no commentary.\n\n"
            f"Query: {q}"
        )

        # LLM22: This fallback is opt-in. Even if the caller forgot to gate it, enforce here too.
        _flag_on, _flag_src = _yureeka_llm_flag_effective_v1("ENABLE_LLM_QUERY_STRUCTURE_FALLBACK")
        if not bool(_flag_on):
            try:
                if isinstance(out_debug, dict):
                    out_debug["llm_fallback_reason"] = "flag_off"
                    out_debug["llm_fallback_flag_source"] = str(_flag_src)[:80]
            except Exception:
                pass
            return None

        # Prepare a strict JSON-only request via the same OpenAI-compatible sidecar path (Perplexity Sonar supported).
        try:
            model = os.environ.get("YUREEKA_LLM_MODEL") or os.environ.get("OPENAI_MODEL") or "gpt-4o-mini"
        except Exception:
            model = "gpt-4o-mini"

        system_prompt = (
            "Extract a query structure. Return ONLY valid JSON with keys:\n"
            "  category: one of [country, industry, company, finance, market, unknown]\n"
            "  category_confidence: number 0-1\n"
            "  main: string (the main question/topic)\n"
            "  side: array of strings (side questions)\n"
            "No extra keys, no commentary."
        )
        input_payload = {"query": q}

        # Deterministic cache key (replayable)
        cache_key = ""
        cache_key_fallback = ""
        try:
            q_hash = _yureeka_llm_text_hash_v1(q)
            in_hash = _yureeka_llm_text_hash_v1(system_prompt + "\n" + q)
            cache_key, cache_key_fallback = get_llm_cache_key_salted_v1(str(model), "llm00_qstruct_v1", "qstruct_v1", in_hash, "", q_hash)
        except Exception:
            cache_key = ""
            cache_key_fallback = ""

        cached = None
        if cache_key:
            try:
                cached = get_cached_llm_response(cache_key)
            except Exception:
                cached = None

        call_diag = None
        if isinstance(cached, dict) and cached:
            parsed = cached
            try:
                if isinstance(out_debug, dict):
                    out_debug["llm_fallback_cache_hit"] = True
                    out_debug["llm_fallback_cache_key"] = str(cache_key)[:80]
            except Exception:
                pass
            try:
                _yureeka_llm_global_agg_update_v1("llm00_query_structure", {"ok": True, "status": None, "reason": "cache_hit", "model": str(model)}, cache_hit=True)
            except Exception:
                pass
        else:
            parsed, call_diag = _yureeka_call_openai_chat_json_v1(
                model=str(model),
                system_prompt=system_prompt,
                user_payload=input_payload,
                timeout_sec=30,
                max_tokens=250,
            )
            try:
                if isinstance(out_debug, dict):
                    out_debug["llm_fallback_cache_hit"] = False
                    out_debug["llm_fallback_cache_key"] = str(cache_key)[:80]
                    if isinstance(call_diag, dict):
                        out_debug["llm_fallback_call_diag"] = {
                            "ok": bool(call_diag.get("ok")),
                            "status": call_diag.get("status"),
                            "reason": str(call_diag.get("reason") or "")[:80],
                            "model": str(call_diag.get("model") or "")[:80],
                        }
            except Exception:
                pass
            try:
                _yureeka_llm_global_agg_update_v1("llm00_query_structure", (call_diag if isinstance(call_diag, dict) else {"ok": False, "status": None, "reason": "no_diag", "model": str(model)}), cache_hit=False)
            except Exception:
                pass
            if isinstance(parsed, dict) and parsed and cache_key:
                try:
                    cache_llm_response(cache_key, parsed)
                except Exception:
                    pass

        # --- Parse / validate ---
        if not isinstance(parsed, dict) or parsed.get("main") is None:
            return None

        # --- Clean/normalize fields ---
        llm_main = str(parsed.get("main") or "").strip()
        llm_side = parsed.get("side") if isinstance(parsed.get("side"), list) else []
        llm_side = [str(s).strip() for s in llm_side if s is not None and str(s).strip()]

        # Reject "invented" side items that look like generic outline bullets
        # (Only apply this rejection when the user did NOT explicitly enumerate a list.)
        def _looks_like_outline_item(s: str) -> bool:
            s2 = s.lower().strip()
            bad_starts = (
                "overview", "key", "key stats", "statistics", "major statistics",
                "policies", "infrastructure", "recent trends", "post-covid", "covid",
                "challenges", "opportunities", "drivers", "headwinds",
                "background", "introduction"
            )
            return any(s2.startswith(b) for b in bad_starts)

        # --- Guardrail policy ---
        # If user didn't enumerate, do NOT accept LLM expansion of side questions.
        if not has_explicit_enumeration:
            # Keep deterministic sides only. (You can allow 1 LLM side if deterministic found none.)
            final_side = det_side
            if not final_side and llm_side:
                # Allow at most one side item as a fallback, but avoid outline-like additions.
                cand = llm_side[0]
                final_side = [] if _looks_like_outline_item(cand) else [cand]
        else:
            # User enumerated: accept multiple sides, but still de-dupe and keep deterministic items first
            merged = []
            for s in (det_side + llm_side):
                s = str(s).strip()
                if not s:
                    continue
                if s not in merged:
                    merged.append(s)
            final_side = merged

        # If LLM main is empty or fragment-y, keep deterministic main
        bad_prefixes = ("as well as", "as well", "and ", "also ", "plus ", "as for ")
        if not llm_main or any(llm_main.lower().startswith(p) for p in bad_prefixes):
            llm_main = (det_main or "").strip()

        # Return only allowed keys
        out = {
            "category": parsed.get("category", "unknown") or "unknown",
            "category_confidence": parsed.get("category_confidence", 0.0),
            "main": llm_main,
            "side": final_side,
        }
        # LLM22: accept only if the model is confident (prevents low-confidence drift).
        try:
            _thr = _yureeka__parse_floatish_v1(os.environ.get("YUREEKA_LLM00_QSTRUCT_CONFIDENCE_THRESHOLD"))
            if _thr is None:
                _thr = float(globals().get("LLM00_QUERY_STRUCTURE_CONFIDENCE_THRESHOLD") or 0.75)
            _conf = 0.0
            try:
                _conf = float(out.get("category_confidence") or 0.0)
            except Exception:
                _conf = 0.0
            if _conf < float(_thr or 0.0):
                try:
                    if isinstance(out_debug, dict):
                        out_debug["llm_fallback_reason"] = "low_confidence"
                        out_debug["llm_fallback_confidence"] = _conf
                        out_debug["llm_fallback_threshold"] = float(_thr)
                except Exception:
                    pass
                return None
            try:
                if isinstance(out_debug, dict):
                    out_debug["llm_fallback_confidence"] = _conf
                    out_debug["llm_fallback_threshold"] = float(_thr)
            except Exception:
                pass
        except Exception:
            pass

        return out

    except Exception:
        return None

def extract_query_structure(query: str) -> Dict[str, Any]:
    """
    Layered query structure extraction:
      1) Deterministic clause split -> main/side
      2) Deterministic category from keyword signals (detect_query_category)
      3) Optional NLP refinement (spaCy if available)
      4) Deterministic similarity vote (TF-IDF)
      5) LLM fallback ONLY if confidence remains low
    """
    q = _normalize_q(query)
    clauses = _split_clauses_deterministic(q)
    main, side = _choose_main_and_side(clauses)

    # --- Layer 1: deterministic keyword category ---
    det_cat = detect_query_category(q)
    category = det_cat.get("category", "unknown")
    cat_conf = float(det_cat.get("confidence", 0.0))

    debug = {
        "deterministic": {
            "clauses": clauses,
            "main": main,
            "side": side,
            "category": category,
            "confidence": cat_conf,
            "matched_signals": det_cat.get("matched_signals", []),
        }
    }

    # --- Layer 2: NLP refinement (optional) ---
    nlp_out = _nlp_refine_clauses(q, clauses)
    if isinstance(nlp_out, dict):
        hints = nlp_out.get("hints", {})
        debug["nlp"] = hints or {"nlp_used": False}

        # Override main/side if NLP produced them (guard against fragment-y mains)
        nlp_main = (nlp_out.get("main") or "").strip()
        if nlp_main:
            bad_prefixes = ("as well as", "as well", "and ", "also ", "plus ", "as for ")
            if not any(nlp_main.lower().startswith(p) for p in bad_prefixes):
                main = nlp_main

        if isinstance(nlp_out.get("side"), list):
            side = nlp_out["side"]

        # If NLP detects a place + overview cue, bias to "country"
        gpes = (hints or {}).get("gpe_entities", []) if isinstance(hints, dict) else []
        overview_hit = (hints or {}).get("overview_signal_hit", False) if isinstance(hints, dict) else False
        if overview_hit and gpes and cat_conf < 0.45:
            category = "country"
            cat_conf = max(cat_conf, 0.55)

    # --- Layer 3: embedding-style category vote ---
    emb_vote = _embedding_category_vote(q)
    debug["similarity_vote"] = emb_vote

    emb_cat = emb_vote.get("category", "unknown")
    emb_conf = float(emb_vote.get("confidence", 0.0))

    if cat_conf < 0.40 and emb_cat != "unknown" and emb_conf >= 0.45:
        category = emb_cat
        cat_conf = max(cat_conf, min(0.75, emb_conf))

    # --- Layer 4: Optional LLM fallback if still ambiguous (default OFF) ---
    llm = None
    if cat_conf < 0.30:
        _f_on, _f_src = _yureeka_llm_flag_effective_v1("ENABLE_LLM_QUERY_STRUCTURE_FALLBACK")
        try:
            debug["llm_fallback_allowed"] = bool(_f_on)
            debug["llm_fallback_flag_source"] = str(_f_src)[:80]
        except Exception:
            pass
        if not bool(_f_on):
            try:
                debug["llm_fallback_used"] = False
                debug["llm_fallback_reason"] = "flag_off"
            except Exception:
                pass
        else:
            llm = _llm_fallback_query_structure(q, out_debug=debug)
            debug["llm_fallback_used"] = bool(llm)

        if isinstance(llm, dict):
            category = llm.get("category", category) or category
            try:
                cat_conf = float(llm.get("category_confidence", cat_conf))
            except Exception:
                pass

            llm_main = (llm.get("main") or "").strip()
            llm_side = llm.get("side") if isinstance(llm.get("side"), list) else []

            det_main = (main or "").strip()
            det_side = side or []

            def _overview_score(s: str) -> int:
                if not s:
                    return 0
                s2 = s.lower()
                signals = [
                    "in general", "overview", "background", "basic facts",
                    "at a glance", "tell me about", "describe", "introduction"
                ]
                return sum(1 for sig in signals if sig in s2)

            def _is_bad_main(s: str) -> bool:
                if not s or len(s) < 8:
                    return True
                return s.lower().startswith(
                    ("as well as", "as well", "and ", "also ", "plus ", "as for ")
                )

            merged_side = []
            for s in det_side + llm_side:
                s = str(s).strip()
                if s and s not in merged_side:
                    merged_side.append(s)

            det_score = _overview_score(det_main)
            llm_score = _overview_score(llm_main)

            if llm_main and not _is_bad_main(llm_main):
                if not det_main or llm_score > det_score:
                    main = llm_main

            side = merged_side

    side = _dedupe_clauses([s.strip() for s in (side or []) if s.strip()])

    return {
        "category": category or "unknown",
        "category_confidence": round(max(0.0, min(cat_conf, 1.0)), 2),
        "main": (main or "").strip(),
        "side": side,
        "debug": debug,
    }

# (Removed in REFACTOR95) legacy compute_metric_diffs() (unused; canonical path remains)

# FINDING DIFF COMPUTATION

# 8C. DETERMINISTIC SOURCE EXTRACTION
# Extract metrics/entities directly from web snippets - NO LLM

# STABILITY SCORE COMPUTATION

# MAIN DIFF COMPUTATION

# LLM EXPLANATION (ONLY INTERPRETS DIFFS)

# 8B. EVOLUTION DASHBOARD RENDERING

# 8D. SOURCE-ANCHORED EVOLUTION
# Re-fetch the SAME sources from previous analysis for true stability

def fetch_url_content_with_status(url: str, timeout: int = 25, force_pdf: bool = False):
    """
    Fetch URL content and return (text, status_detail).

    status_detail:
      - "success"
      - "success_pdf"
      - "http_<code>"
      - "exception:<TypeName>"
      - "empty"
      - "success_scrapingdog"

    Hardened:
      - Uses browser-like headers for direct fetch
      - Falls back to ScrapingDog when blocked/empty and SCRAPINGDOG_KEY is available
      - Avoids returning binary garbage as "text"
    """
    def _normalize_url(s: str) -> str:
        t = (s or "").strip()
        if not t:
            return ""
        if re.match(r"^https?://", t, flags=re.I):
            return t
        if re.match(r"^[a-z0-9.-]+\.[a-z]{2,}(/.*)?$", t, flags=re.I):
            return "https://" + t
        return ""

    url = _normalize_url(url)
    if not url:
        return None, "empty"


    # REFACTOR202: requests is optional; fail gracefully if missing
    if requests is None:
        return None, "skipped:requests_missing_dependency"

    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
    }

    try:
        resp = requests.get(url, timeout=timeout, headers=headers, allow_redirects=True)

        ct = (resp.headers.get("content-type", "") or "").lower()

        if resp.status_code >= 400:
            # If blocked, try ScrapingDog fallback (optional)
            if resp.status_code in (401, 403, 429) and globals().get("SCRAPINGDOG_KEY"):
                txt = _fetch_via_scrapingdog(url, timeout=timeout)
                if txt and txt.strip():
                    return txt, "success_scrapingdog"
            return None, f"http_{resp.status_code}"

        # PDF handling
        if force_pdf or "application/pdf" in ct or url.lower().endswith(".pdf"):
            try:
                try:
                    import pdfplumber  # type: ignore
                except (ModuleNotFoundError, ImportError):
                    return None, "skipped:pdf_unsupported_missing_dependency"

                with pdfplumber.open(io.BytesIO(resp.content)) as pdf:
                    out = []
                    total_pages = int(len(pdf.pages) if getattr(pdf, 'pages', None) is not None else 0)

                    # REFACTOR92: sample across long PDFs and extract tables (not only front matter)
                    # - Extract first N pages + an evenly-spaced spread across the remainder.
                    # - Add bounded table extraction to surface values hidden in table layouts (e.g., 'USD bn').
                    first_n = 10
                    target_total = 50

                    idxs = set(range(min(first_n, total_pages)))
                    remaining_budget = max(0, min(target_total - len(idxs), total_pages - len(idxs)))

                    if remaining_budget > 0 and total_pages > first_n:
                        start_idx = first_n
                        end_idx = max(first_n, total_pages - 1)
                        denom = max(1, remaining_budget - 1)
                        for s in range(remaining_budget):
                            try:
                                idx = int(round(start_idx + (end_idx - start_idx) * (s / denom)))
                            except Exception:
                                idx = start_idx
                            if 0 <= idx < total_pages:
                                idxs.add(idx)

                    nonempty_pages = 0
                    tables_rows_total = 0

                    for pi in sorted(idxs):
                        try:
                            page = pdf.pages[pi]

                            # 1) Plain text extraction
                            t = page.extract_text() or ''
                            if isinstance(t, str) and t.strip():
                                out.append(t)
                                nonempty_pages += 1

                            # 2) Table extraction (bounded)
                            try:
                                tables = page.extract_tables() or []
                            except Exception:
                                tables = []
                            if isinstance(tables, list) and tables:
                                for tbl in tables[:2]:
                                    if not isinstance(tbl, list):
                                        continue
                                    for row in (tbl[:25] if len(tbl) > 25 else tbl):
                                        if not isinstance(row, list):
                                            continue
                                        cells = []
                                        for c in row[:10]:
                                            try:
                                                cs = (str(c) if c is not None else '').strip()
                                            except Exception:
                                                cs = ''
                                            if cs:
                                                cells.append(cs)
                                        if cells:
                                            out.append(' | '.join(cells))
                                            tables_rows_total += 1
                        except Exception:
                            continue

                    text = '\n'.join(out).strip()
                    if not text:
                        return None, 'empty'
                    return text, f"success_pdf_pages={len(idxs)}/{total_pages};nonempty={nonempty_pages};tables={tables_rows_total}"
            except Exception as e:
                return None, f"exception:{type(e).__name__}"

        # Text/HTML
        text = resp.text or ""
        # REFACTOR120: normalize HTML to visible text so numeric extraction can work on HTML-heavy pages
        try:
            _rf120_raw = text or ""
            _rf120_head = (_rf120_raw[:2000] or "").lower()
            _rf120_looks_html = bool(
                ("<html" in _rf120_head) or ("<body" in _rf120_head) or
                re.search(r"(?is)<(div|span|p|article|main|section|header|footer|nav|script|style)\b", _rf120_raw[:2000] or "")
            )
            if _rf120_looks_html:
                _rf120_txt = ""
                try:
                    soup = BeautifulSoup(_rf120_raw, "html.parser")
                    for tag in soup(["script", "style", "nav", "footer", "header", "aside", "noscript", "form"]):
                        try:
                            tag.decompose()
                        except Exception:
                            pass
                    _rf120_txt = soup.get_text(separator="\n")
                except Exception:
                    _rf120_txt = re.sub(r"(?is)<(script|style|noscript).*?>.*?</\1>", " ", _rf120_raw or "")
                    _rf120_txt = re.sub(r"(?is)<[^>]+>", " ", _rf120_txt or "")
                try:
                    _rf120_lines = [ln.strip() for ln in (_rf120_txt or "").splitlines() if ln.strip()]
                    _rf120_clean = "\n".join(_rf120_lines)
                    _rf120_clean = re.sub(r"\n{3,}", "\n\n", _rf120_clean).strip()
                except Exception:
                    _rf120_clean = (_rf120_txt or "").strip()
                if isinstance(_rf120_clean, str) and len(_rf120_clean) >= 200:
                    text = _rf120_clean
        except Exception:
            pass

        # If empty or suspiciously short, attempt ScrapingDog (optional)
        if (not text.strip() or len(text.strip()) < 300) and globals().get("SCRAPINGDOG_KEY"):
            txt = _fetch_via_scrapingdog(url, timeout=timeout)
            if txt and txt.strip():
                return txt, "success_scrapingdog"

        if not text.strip():
            return None, "empty"

        return text, "success"

    except Exception as e:
        # ScrapingDog as last resort for network-y issues
        try:
            if globals().get("SCRAPINGDOG_KEY"):
                txt = _fetch_via_scrapingdog(url, timeout=timeout)
                if txt and txt.strip():
                    return txt, "success_scrapingdog"
        except Exception:
            return None, f"exception:{type(e).__name__}"

def _fetch_via_scrapingdog(url: str, timeout: int = 25) -> str:
    """
    Internal helper used by fetch_url_content_with_status.
    Returns raw HTML text from ScrapingDog (or "" on failure).
    """
    key = globals().get("SCRAPINGDOG_KEY")
    # REFACTOR202: requests is optional; fail gracefully if missing
    if requests is None:
        return ""

    if not key:
        return ""

    params = {"api_key": key, "url": url, "dynamic": "false"}
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        resp = requests.get("https://api.scrapingdog.com/scrape", params=params, headers=headers, timeout=timeout)
        if resp.status_code >= 400:
            return ""
        return resp.text or ""
    except Exception:
        return ""

def get_extractor_fingerprint() -> str:
    """
    Bump this string whenever you change extraction or normalization behavior.
    Used to decide whether cached extracted_numbers are still valid.
    """
    return "extract_v4_pdf_tables_forcepdf_2026-01-29"

def _parse_iso_dt(ts: Optional[str]) -> Optional[datetime]:
    if not ts:
        return None
    try:
        ts2 = ts.replace("Z", "+00:00")
        dt = datetime.fromisoformat(ts2)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception:
        return None

def _yureeka_now_iso_utc() -> str:
    """UTC ISO-8601 timestamp with offset (e.g., 2026-01-23T11:13:15.665069+00:00)."""
    try:
        return datetime.now(timezone.utc).isoformat()
    except Exception:
        try:
            return datetime.now(timezone.utc).isoformat()
        except Exception:
            return ""


# =============================================================================
# [MOD:FRESH01] Source freshness extraction (deterministic, audit-friendly)
#
# Purpose:
# - Best-effort extract a published/updated date from already-scraped text (no web calls).
# - Compute source age (days) relative to fetched_at and bucketize.
# - Attach as additive fields to scraped_meta + baseline_sources_cache for transparency.
# - Does NOT change metric winners/values unless ENABLE_SOURCE_FRESHNESS_TIEBREAK is explicitly enabled.
# =============================================================================

# Default OFF: baseline REFACTOR206/LLM series behavior unchanged (selection remains schema/year-anchor driven).
ENABLE_SOURCE_FRESHNESS_TIEBREAK = False

_FRESH01_MONTHS = {
    "jan": 1, "january": 1,
    "feb": 2, "february": 2,
    "mar": 3, "march": 3,
    "apr": 4, "april": 4,
    "may": 5,
    "jun": 6, "june": 6,
    "jul": 7, "july": 7,
    "aug": 8, "august": 8,
    "sep": 9, "sept": 9, "september": 9,
    "oct": 10, "october": 10,
    "nov": 11, "november": 11,
    "dec": 12, "december": 12,
}

def _fresh01__safe_int_v1(x, default=None):
    try:
        if isinstance(x, bool):
            return default
        if isinstance(x, (int, float)):
            return int(x)
        if isinstance(x, str) and x.strip():
            return int(float(x.strip()))
    except Exception:
        pass
    return default

def _fresh01__make_dt_v1(y: int, m: int, d: int) -> Optional[datetime]:
    try:
        # Use noon UTC to avoid DST edge cases when only a date is known.
        return datetime(int(y), int(m), int(d), 12, 0, 0, tzinfo=timezone.utc)
    except Exception:
        return None

def _fresh01__norm_iso_date_v1(dt: Optional[datetime]) -> str:
    try:
        if not isinstance(dt, datetime):
            return ""
        return dt.astimezone(timezone.utc).date().isoformat()
    except Exception:
        return ""


def _fresh01_extract_date_candidates_v1(text: str, max_scan: int = 5200) -> list:
    """Return candidate dates from text with rough priority hints (deterministic regex-only)."""
    if not isinstance(text, str) or not text.strip():
        return []
    t = text[: int(max_scan or 5200)]
    out = []

    def _ctx(start: int, end: int) -> str:
        try:
            a = max(0, int(start) - 70)
            b = min(len(t), int(end) + 70)
            return t[a:b].lower()
        except Exception:
            return ""

    # ISO-ish: 2025-07-01 or 2025/07/01
    for m in re.finditer(r"\b(20\d{2})[-/](\d{1,2})[-/](\d{1,2})\b", t):
        yy = _fresh01__safe_int_v1(m.group(1))
        mm = _fresh01__safe_int_v1(m.group(2))
        dd = _fresh01__safe_int_v1(m.group(3))
        if not yy or not mm or not dd:
            continue
        if mm < 1 or mm > 12 or dd < 1 or dd > 31:
            continue
        dt = _fresh01__make_dt_v1(yy, mm, dd)
        if not dt:
            continue
        out.append({"dt": dt, "pos": int(m.start()), "raw": m.group(0), "kind": "iso", "ctx": _ctx(m.start(), m.end())})

    # Month day, year: July 1, 2025 / Jul 1 2025
    mon_re = r"Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:t|tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?"
    for m in re.finditer(rf"\b({mon_re})\s+(\d{{1,2}})(?:st|nd|rd|th)?\s*,?\s*(20\d{{2}})\b", t, flags=re.I):
        mon_s = (m.group(1) or "").lower()
        dd = _fresh01__safe_int_v1(m.group(2))
        yy = _fresh01__safe_int_v1(m.group(3))
        mm = _FRESH01_MONTHS.get(mon_s[:3], _FRESH01_MONTHS.get(mon_s, None))
        if not yy or not mm or not dd:
            continue
        if mm < 1 or mm > 12 or dd < 1 or dd > 31:
            continue
        dt = _fresh01__make_dt_v1(yy, mm, dd)
        if not dt:
            continue
        out.append({"dt": dt, "pos": int(m.start()), "raw": m.group(0), "kind": "mdy", "ctx": _ctx(m.start(), m.end())})

    # Day Month year: 1 July 2025
    for m in re.finditer(rf"\b(\d{{1,2}})(?:st|nd|rd|th)?\s+({mon_re})\s+(20\d{{2}})\b", t, flags=re.I):
        dd = _fresh01__safe_int_v1(m.group(1))
        mon_s = (m.group(2) or "").lower()
        yy = _fresh01__safe_int_v1(m.group(3))
        mm = _FRESH01_MONTHS.get(mon_s[:3], _FRESH01_MONTHS.get(mon_s, None))
        if not yy or not mm or not dd:
            continue
        if mm < 1 or mm > 12 or dd < 1 or dd > 31:
            continue
        dt = _fresh01__make_dt_v1(yy, mm, dd)
        if not dt:
            continue
        out.append({"dt": dt, "pos": int(m.start()), "raw": m.group(0), "kind": "dmy", "ctx": _ctx(m.start(), m.end())})

    return out[:24]

def _fresh01_pick_best_candidate_v1(cands: list, fetched_at: str = "") -> Optional[dict]:
    """Pick the best candidate date dict using deterministic heuristics + fetched_at sanity bounds.

    Returns the candidate dict with added key: _heur (int), or None.
    """
    if not isinstance(cands, list) or not cands:
        return None
    fetched_dt = _parse_iso_dt(fetched_at) if fetched_at else None

    best = None
    best_key = None  # (heur_score, dt, -pos)

    for c in cands:
        if not isinstance(c, dict) or not isinstance(c.get("dt"), datetime):
            continue
        dt = c["dt"].astimezone(timezone.utc)
        pos = int(c.get("pos") or 0)
        ctx = str(c.get("ctx") or "").lower()

        # Ignore implausible years
        if dt.year < 1990 or dt.year > 2100:
            continue

        # Bound by fetched_at when available (avoid accidentally treating future dates as publish dates)
        if isinstance(fetched_dt, datetime):
            if dt.date() > (fetched_dt + timedelta(days=7)).date():
                continue
            age = int((fetched_dt.date() - dt.date()).days)
            if age < -3:
                continue

        heur = 0
        if ctx:
            if "last updated" in ctx or "updated" in ctx:
                heur += 6
            if "published" in ctx or "posted" in ctx:
                heur += 4
            if "release date" in ctx or "released" in ctx:
                heur += 2
        # Prefer header-like positions
        if pos <= 600:
            heur += 1

        key = (heur, dt, -pos)
        if (best_key is None) or (key > best_key):
            best_key = key
            best = dict(c)
            try:
                best["_heur"] = int(heur)
            except Exception:
                best["_heur"] = 0

    return best

def _fresh01_pick_best_date_v1(cands: list, fetched_at: str = "") -> Optional[datetime]:
    """Backward-compatible wrapper returning only the datetime."""
    best = _fresh01_pick_best_candidate_v1(cands, fetched_at=fetched_at)
    try:
        if isinstance(best, dict) and isinstance(best.get("dt"), datetime):
            return best["dt"].astimezone(timezone.utc)
    except Exception:
        return None
    return None
def _fresh01_score_from_age_days_v1(age_days: Optional[int]) -> Optional[float]:
    """Deterministic freshness score base (0-100) from age in days.

    LLM37: curve is parameterized via HYPERPARAMS_V1 (defaults preserve LLM36 behavior).
    """
    try:
        if age_days is None:
            return None
        a = int(age_days)
        if a < 0:
            return None

        cut = globals().get("_FRESH01_SCORE_CUTOFFS_DAYS_V1") or [30, 180, 365, 730]
        vals = globals().get("_FRESH01_SCORE_VALUES_V1") or [100.0, 85.0, 70.0, 55.0, 40.0]
        # Validate shape: len(vals) == len(cut) + 1
        try:
            if not (isinstance(cut, list) and isinstance(vals, list) and len(vals) == (len(cut) + 1)):
                raise ValueError("invalid curve")
        except Exception:
            cut = [30, 180, 365, 730]
            vals = [100.0, 85.0, 70.0, 55.0, 40.0]

        for i, c in enumerate(cut):
            try:
                if a <= int(c):
                    return float(vals[i])
            except Exception:
                continue
        return float(vals[-1])
    except Exception:
        return None


def _fresh01_bucket_from_age_days_v1(age_days: Optional[int]) -> str:
    """Deterministic freshness bucket label from age in days (LLM37 param-driven)."""
    try:
        if age_days is None:
            return ""
        a = int(age_days)
        if a < 0:
            return ""
        cut = globals().get("_FRESH01_SCORE_CUTOFFS_DAYS_V1") or [30, 180, 365, 730]
        labels = globals().get("_FRESH01_BUCKET_LABELS_V1") or ["≤30d", "≤6mo", "≤1y", "≤2y", ">2y"]
        try:
            if not (isinstance(cut, list) and isinstance(labels, list) and len(labels) == (len(cut) + 1)):
                raise ValueError("invalid labels")
        except Exception:
            cut = [30, 180, 365, 730]
            labels = ["≤30d", "≤6mo", "≤1y", "≤2y", ">2y"]
        for i, c in enumerate(cut):
            try:
                if a <= int(c):
                    return str(labels[i])
            except Exception:
                continue
        return str(labels[-1])
    except Exception:
        return ""
def _fresh01_confidence_from_heur_v1(heur: int) -> float:
    """Map deterministic heuristic score to a [0.70, 1.00] confidence."""
    try:
        h = int(heur)
    except Exception:
        h = 0
    # heur includes +6 for updated, +4 for published, +2 for released, +1 for header position
    if h >= 7:
        return 1.00   # updated + header
    if h >= 6:
        return 0.95   # updated
    if h >= 5:
        return 0.90   # published + header
    if h >= 4:
        return 0.88   # published
    if h >= 3:
        return 0.82   # released + header
    if h >= 2:
        return 0.80   # released
    if h >= 1:
        return 0.75   # header-only
    return 0.70       # generic date

def _fresh01_compute_source_freshness_v2(text: str, fetched_at: str = "") -> dict:
    """Return additive freshness fields for a source snapshot (includes 0-100 score).

    Determinism rules:
    - Regex-only on already-scraped text (no web calls).
    - Age computed relative to fetched_at (snapshot-local), not wall-clock at render time.
    """
    out = {
        "published_at": "",                 # ISO date (YYYY-MM-DD)
        "freshness_age_days": None,         # int days (>=0) or None
        "freshness_bucket": "",             # categorical bucket
        "freshness_method": "none",         # regex_v1 / none
        "freshness_date_confidence": None,  # float in [0,1] or None
        "freshness_score": None,            # float 0-100 or None
        "freshness_score_method": "none",   # bucket_v1_conf_v1 / none
    }
    try:
        cands = _fresh01_extract_date_candidates_v1(text)
        best = _fresh01_pick_best_candidate_v1(cands, fetched_at=fetched_at)
        if not (isinstance(best, dict) and isinstance(best.get("dt"), datetime)):
            return out
        dt = best["dt"].astimezone(timezone.utc)
        out["published_at"] = _fresh01__norm_iso_date_v1(dt)
        out["freshness_method"] = "regex_v1"

        # confidence from heuristics used in deterministic selection
        heur = int(best.get("_heur") or 0)
        conf = float(_fresh01_confidence_from_heur_v1(heur))
        out["freshness_date_confidence"] = round(conf, 3)

        fdt = _parse_iso_dt(fetched_at) if fetched_at else None
        if isinstance(fdt, datetime):
            age = int((fdt.date() - dt.date()).days)
            if age < 0 and age >= -3:
                age = 0
            if age >= 0:
                out["freshness_age_days"] = int(age)
                out["freshness_bucket"] = _fresh01_bucket_from_age_days_v1(age)

                base = _fresh01_score_from_age_days_v1(age)
                if base is not None:
                    out["freshness_score"] = round(float(base) * float(conf), 1)
                    out["freshness_score_method"] = "bucket_v1_conf_v1"
    except Exception:
        return out
    return out

# Backward-compatible name (LLM31 used v1)
_fresh01_compute_source_freshness_v1 = _fresh01_compute_source_freshness_v2
def _fresh01_aggregate_source_freshness_v1(baseline_sources_cache: Any, stage: str = "") -> dict:
    """Aggregate freshness beacons across sources (diagnostic-only).

    Notes:
    - `sources_with_published_at` counts sources with a non-empty `published_at` OR a valid `freshness_age_days`
      (age implies an extracted date, even if published_at string is missing in older caches).
    - This function is diagnostic-only; it must not affect selection/diff outcomes.
    """
    bsc = baseline_sources_cache if isinstance(baseline_sources_cache, list) else []
    ages: list = []
    bucket_counts: dict = {}
    samples: list = []
    pub_count = 0
    age_count = 0

    for s in bsc:
        if not isinstance(s, dict):
            continue

        pub = str(s.get("published_at") or "").strip()
        age = s.get("freshness_age_days")

        # published_at presence
        if pub:
            pub_count += 1

        # age parsing
        try:
            age_i = int(age) if age is not None else None
        except Exception:
            age_i = None

        if isinstance(age_i, int) and age_i >= 0:
            ages.append(age_i)
            age_count += 1
            # age implies we have (or had) a published_at candidate
            if not pub:
                pub_count += 1

            b = str(s.get("freshness_bucket") or "")
            if b:
                bucket_counts[b] = int(bucket_counts.get(b, 0)) + 1

        samples.append({
            "url": str(s.get("url") or s.get("source_url") or "")[:240],
            "published_at": pub,
            "age_days": age_i,
        })

    # median/min/max over ages when available
    ages_sorted = sorted(ages)
    median_age = None
    try:
        if ages_sorted:
            mid = len(ages_sorted) // 2
            median_age = ages_sorted[mid] if len(ages_sorted) % 2 == 1 else int((ages_sorted[mid-1] + ages_sorted[mid]) / 2)
    except Exception:
        median_age = None

    oldest = [x for x in samples if isinstance(x.get("age_days"), int)]
    oldest = sorted(oldest, key=lambda r: int(r.get("age_days") or 0), reverse=True)[:5]
    newest = [x for x in samples if isinstance(x.get("age_days"), int)]
    newest = sorted(newest, key=lambda r: int(r.get("age_days") or 0))[:5]

    return {
        "stage": str(stage or ""),
        "sources_total": int(len(bsc)),
        "sources_with_published_at": int(pub_count),
        "sources_with_age_days": int(age_count),
        "median_age_days": median_age,
        "min_age_days": (min(ages) if ages else None),
        "max_age_days": (max(ages) if ages else None),
        "bucket_counts": bucket_counts,
        "sample_oldest": oldest,
        "sample_newest": newest,
    }


def _fresh01_backfill_freshness_scores_v1(baseline_sources_cache: Any) -> Any:
    """In-place additive backfill for caches missing derived freshness fields.

    - If `freshness_score` is missing but `freshness_age_days` exists, compute:
        - freshness_bucket (if missing)
        - freshness_score (0-100) using bucket_v1_conf_v1 semantics when confidence exists.
    - Safe/no-op when input is not list-like.
    """
    bsc = baseline_sources_cache if isinstance(baseline_sources_cache, list) else None
    if not isinstance(bsc, list):
        return baseline_sources_cache

    for s in bsc:
        if not isinstance(s, dict):
            continue

        # bucket from age_days
        age = s.get("freshness_age_days")
        try:
            age_i = int(age) if age is not None else None
        except Exception:
            age_i = None

        if isinstance(age_i, int) and age_i >= 0:
            if not str(s.get("freshness_bucket") or "").strip():
                if age_i <= 30:
                    s["freshness_bucket"] = "≤30d"
                elif age_i <= 180:
                    s["freshness_bucket"] = "≤6mo"
                elif age_i <= 365:
                    s["freshness_bucket"] = "≤1y"
                elif age_i <= 730:
                    s["freshness_bucket"] = "≤2y"
                else:
                    s["freshness_bucket"] = ">2y"

        # score from age_days (and optional confidence)
        sc = s.get("freshness_score")
        if sc is None or sc == "":
            base = _fresh01_score_from_age_days_v1(age_i) if isinstance(age_i, int) else None
            if base is not None:
                conf = s.get("freshness_date_confidence")
                try:
                    conf_f = float(conf) if conf is not None else 1.0
                except Exception:
                    conf_f = 1.0
                try:
                    s["freshness_score"] = round(float(base) * float(conf_f), 1)
                    if not str(s.get("freshness_score_method") or "").strip():
                        s["freshness_score_method"] = "bucket_v1_conf_v1"
                except Exception:
                    pass

    return baseline_sources_cache

# REFACTOR26: Centralized source_url attribution helpers (schema-preserving)
# - Goal: ensure row-level injection gating can reliably attribute a current metric
#   to its source URL (production vs injected), even when source_url lives only
#   inside evidence/provenance structures.

def _refactor26_extract_metric_source_url_v1(_m: dict):
    """Best-effort extraction of a metric's source URL without changing schema."""
    if not isinstance(_m, dict):
        return None
    # Direct fields
    for k in ("source_url", "url", "source", "sourceURL", "sourceUrl"):
        try:
            v = _m.get(k)
            if isinstance(v, str) and v.strip():
                return v.strip()
        except Exception:
            pass

    # Evidence list
    try:
        ev = _m.get("evidence")
        if isinstance(ev, list):
            for e in ev:
                if isinstance(e, dict):
                    for k in ("source_url", "url"):
                        v = e.get(k)
                        if isinstance(v, str) and v.strip():
                            return v.strip()
    except Exception:
        pass

    # Winner/debug/provenance structures (common in this codebase)
    try:
        wd = _m.get("winner_candidate_debug")
        if isinstance(wd, dict):
            v = wd.get("source_url") or wd.get("url")
            if isinstance(v, str) and v.strip():
                return v.strip()
    except Exception:
        pass

    try:
        prov = _m.get("provenance") or _m.get("provenance_v1") or _m.get("diag")
        if isinstance(prov, dict):
            # Direct provenance URL
            v = prov.get("source_url") or prov.get("url")
            if isinstance(v, str) and v.strip():
                return v.strip()

            # Common nested winner shape: provenance.best_candidate.source_url
            bc = prov.get("best_candidate") or prov.get("best_candidate_v1") or prov.get("winner") or prov.get("best")
            if isinstance(bc, dict):
                v2 = bc.get("source_url") or bc.get("url")
                if isinstance(v2, str) and v2.strip():
                    return v2.strip()

            # Sometimes stored as list of candidates
            cands = prov.get("candidates") or prov.get("top_candidates") or prov.get("candidates_v1")
            if isinstance(cands, list):
                for c in cands:
                    if isinstance(c, dict):
                        v3 = c.get("source_url") or c.get("url")
                        if isinstance(v3, str) and v3.strip():
                            return v3.strip()
    except Exception:
        pass

    return None

def _refactor26_hydrate_primary_metrics_canonical_source_urls_v1(_pmc: dict) -> dict:
    """In-place: ensure pmc[*].source_url exists when discoverable from evidence."""
    if not isinstance(_pmc, dict):
        return _pmc
    for _ck, _m in list(_pmc.items()):
        if not isinstance(_m, dict):
            continue
        try:
            su = _m.get("source_url")
            if isinstance(su, str) and su.strip():
                continue
            su2 = _refactor26_extract_metric_source_url_v1(_m)
            if isinstance(su2, str) and su2.strip():
                _m["source_url"] = su2.strip()
        except Exception:
            pass
    return _pmc

def _refactor26_extract_row_current_source_url_v1(_row: dict):
    """Prefer row-attributed *current* URL fields before any fallbacks."""
    if not isinstance(_row, dict):
        return None
    for k in ("cur_source_url", "current_source_url", "current_source", "current_source_url_effective", "current_source_effective"):
        try:
            v = _row.get(k)
            if isinstance(v, str) and v.strip():
                return v.strip()
        except Exception:
            pass
    # As a last resort, some rows store the current URL in source_url (ambiguous)
    try:
        v = _row.get("source_url")
        if isinstance(v, str) and v.strip():
            return v.strip()
    except Exception:
        pass
    return None

def _yureeka_humanize_seconds_v1(delta_seconds) -> str:
    """Compact human format for a positive second delta (e.g., '1m 18s')."""
    try:
        if delta_seconds is None:
            return ""
        ds = float(delta_seconds)
        if ds < 0:
            ds = 0.0
        total = int(round(ds))
        mins, secs = divmod(total, 60)
        hrs, mins = divmod(mins, 60)
        days, hrs = divmod(hrs, 24)
        parts = []
        if days:
            parts.append(f"{days}d")
        if hrs:
            parts.append(f"{hrs}h")
        if mins:
            parts.append(f"{mins}m")
        parts.append(f"{secs}s")
        return " ".join(parts)
    except Exception:
        return ""

def _refactor13_get_metric_change_rows_v1(out: dict):
    """
    Return canonical-first diff rows.
    Prefer metric_changes; fallback to metric_changes_v2 for backwards compatibility.
    Returned list is safe (always list).
    """
    try:
        if isinstance(out, dict):
            rows = out.get("metric_changes")
            if isinstance(rows, list):
                return rows
            rows = out.get("metric_changes_v2")
            if isinstance(rows, list) and rows:
                return rows
    except Exception:
        pass
    return []

def _refactor13_recompute_summary_and_stability_v1(out: dict) -> None:
    """
    REFACTOR13: Make results.summary + stability_score reflect canonical-first diff rows.

    - summary.metrics_increased / decreased / unchanged are derived from change_type.
    - stability_score is computed from comparable rows:
        1) discrete score: unchanged + 0.5 * small_change(<10%)/N
        2) fallback when discrete would be 0: max(0, 100 - mean_abs_pct_change)
    """
    if not isinstance(out, dict):
        return

    rows = _refactor13_get_metric_change_rows_v1(out)

    # Count change types
    increased = decreased = unchanged = added = removed = 0
    for r in rows:
        try:
            ct = (r.get("change_type") if isinstance(r, dict) else None) or ""
            if ct == "increased":
                increased += 1
            elif ct == "decreased":
                decreased += 1
            elif ct == "unchanged":
                unchanged += 1
            elif ct in ("added", "missing_baseline", "new_metric"):
                added += 1
            elif ct in ("removed", "missing_current"):
                removed += 1
        except Exception:
            pass

    total = len(rows)

    # Update summary (authoritative for UI)
    try:
        s = out.setdefault("summary", {})
        if isinstance(s, dict):
            s["total_metrics"] = total
            # In our canonical-first world, "found" = row count (includes added/removed)
            s["metrics_found"] = total
            s["metrics_increased"] = increased
            s["metrics_decreased"] = decreased
            s["metrics_unchanged"] = unchanged
            # Preserve backward compatibility: do not remove existing keys
            s.setdefault("metrics_added", added)
            s.setdefault("metrics_removed", removed)
    except Exception:
        pass

    # Compute stability from comparable rows
    # NOTE: We treat "small change" as <10% only for *changed* rows (increased/decreased),
    # so unchanged rows are not double-counted (prevents >100% stability).
    comparable = []
    for r in rows:
        if not isinstance(r, dict):
            continue
        ct = r.get("change_type")
        if ct not in ("increased", "decreased", "unchanged"):
            continue
        # Prefer explicit comparability signal
        if r.get("baseline_is_comparable") is False:
            continue
        if r.get("unit_mismatch") is True:
            continue
        # Require numeric pct (or at least numeric norms)
        cp = r.get("change_pct")
        if isinstance(cp, (int, float)):
            comparable.append((float(cp), ct))
        else:
            # fallback if norms are numeric: compute pct safely
            pv = r.get("prev_value_norm")
            cv = r.get("cur_value_norm")
            try:
                if isinstance(pv, (int, float)) and isinstance(cv, (int, float)) and abs(float(pv)) > 1e-12:
                    comparable.append((((float(cv) - float(pv)) / float(pv)) * 100.0, ct))
            except Exception:
                pass

    stability = 100.0
    method = "no_comparable_rows"
    n = len(comparable)

    # REFACTOR51: track graded stats so stability remains meaningful with extreme deltas
    mean_abs_pct_raw = None
    mean_abs_pct_capped = None
    mean_abs_cap_used = None

    if n > 0:
        # Discrete stability:
        #   stability = (unchanged + 0.5 * small_change_changed_rows) / comparable_n * 100
        stable = 0
        small = 0
        abs_vals = []
        for cp, ct in comparable:
            try:
                a = abs(float(cp))
                abs_vals.append(a)
                if ct == "unchanged":
                    stable += 1
                elif ct in ("increased", "decreased") and a < 10.0:
                    small += 1
            except Exception:
                pass

        discrete = ((stable + (small * 0.5)) / float(max(1, n))) * 100.0
        if discrete > 0.0:
            # Clamp for safety (should already be <=100 with the counting rules above)
            stability = max(0.0, min(100.0, discrete))
            method = "discrete_unchanged_smallchange"
        else:
            # Graded fallback:
            #   Use a per-row cap to avoid a single extreme outlier driving mean_abs>=100 -> 0% stability.
            #   stability = 100 - mean(min(abs_pct, 100))  (clamped [0,100])
            if abs_vals:
                try:
                    mean_abs_pct_raw = sum(abs_vals) / float(len(abs_vals))
                except Exception:
                    mean_abs_pct_raw = None
                try:
                    mean_abs_cap_used = 100.0
                    mean_abs_pct_capped = sum((a if a <= mean_abs_cap_used else mean_abs_cap_used) for a in abs_vals) / float(len(abs_vals))
                except Exception:
                    mean_abs_pct_capped = None
                if isinstance(mean_abs_pct_capped, (int, float)):
                    stability = max(0.0, min(100.0, 100.0 - float(mean_abs_pct_capped)))
                    method = "graded_mean_abs_pct_capped"
                elif isinstance(mean_abs_pct_raw, (int, float)):
                    # last-resort: clamp the mean itself (previous behavior)
                    stability = max(0.0, 100.0 - min(100.0, float(mean_abs_pct_raw)))
                    method = "graded_mean_abs_pct"
                else:
                    stability = 0.0
                    method = "no_pct_values"
            else:
                stability = 0.0
                method = "no_pct_values"

    try:
        out["stability_score"] = round(float(stability), 1)
    except Exception:
        pass

    # Mirror into diff_panel_v2_summary for auditability
    try:
        dbg = out.setdefault("debug", {})
        if isinstance(dbg, dict):
            v2s = dbg.get("diff_panel_v2_summary")
            if isinstance(v2s, dict):
                v2s.setdefault("metrics_increased", increased)
                v2s.setdefault("metrics_decreased", decreased)
                v2s.setdefault("metrics_unchanged", unchanged)
                v2s.setdefault("metrics_added", added)
                v2s.setdefault("metrics_removed", removed)
                v2s.setdefault("stability_score_v1", round(float(stability), 1))
                v2s.setdefault("stability_method_v1", method)
                v2s.setdefault("stability_comparable_n_v1", n)

            dbg["refactor13_summary_stability_v1"] = {
                "rows_total": total,
                "comparable_n": n,
                "metrics_increased": increased,
                "metrics_decreased": decreased,
                "metrics_unchanged": unchanged,
                "metrics_added": added,
                "metrics_removed": removed,
                "stability_score": round(float(stability), 1),
                "stability_method": method,
            }
            _r13 = dbg.get("refactor13_summary_stability_v1")
            if isinstance(_r13, dict) and mean_abs_pct_raw is not None:
                try:
                    _r13["mean_abs_pct_raw"] = round(float(mean_abs_pct_raw), 2)
                except Exception:
                    pass
            if isinstance(_r13, dict) and mean_abs_pct_capped is not None:
                try:
                    _r13["mean_abs_pct_capped"] = round(float(mean_abs_pct_capped), 2)
                    _r13["mean_abs_pct_cap_used"] = mean_abs_cap_used
                except Exception:
                    pass
    except Exception:
        pass

def _summarize_heavy_fields_for_sheets(obj: dict) -> dict:
    """
    Summarize fields that commonly exceed the per-cell limit while keeping debug utility.
    Only used for Sheets serialization; does NOT modify your in-memory analysis dict.
    """
    if not isinstance(obj, dict):
        return {"_type": str(type(obj)), "value": str(obj)[:500]}

    out = dict(obj)

    # Common bloat fields
    if "scraped_meta" in out:
        sm = out.get("scraped_meta")
        if isinstance(sm, dict):
            compact = {}
            for url, meta in list(sm.items())[:12]:
                if isinstance(meta, dict):
                    compact[url] = {
                        "status": meta.get("status"),
                        "status_detail": meta.get("status_detail"),
                        "numbers_found": meta.get("numbers_found"),
                        "fingerprint": meta.get("fingerprint"),
                        "clean_text_len": meta.get("clean_text_len"),
                    }
            out["scraped_meta"] = {"_summary": True, "count": len(sm), "sample": compact}
        else:
            out["scraped_meta"] = {"_summary": True, "type": str(type(sm))}

    for big_key in ("source_results", "baseline_sources_cache", "baseline_sources_cache_compact"):
        if big_key in out:
            sr = out.get(big_key)
            if isinstance(sr, list):
                sample = []
                for item in sr[:2]:
                    if isinstance(item, dict):
                        item2 = dict(item)
                        if isinstance(item2.get("extracted_numbers"), list):
                            item2["extracted_numbers"] = {"_summary": True, "count": len(item2["extracted_numbers"])}
                        sample.append(item2)
                out[big_key] = {"_summary": True, "count": len(sr), "sample": sample}
            else:
                out[big_key] = {"_summary": True, "type": str(type(sr))}

    # If you store full scraped_content anywhere, summarize it too
    if "scraped_content" in out:
        sc = out.get("scraped_content")
        if isinstance(sc, dict):
            out["scraped_content"] = {"_summary": True, "count": len(sc), "keys_sample": list(sc.keys())[:10]}
        else:
            out["scraped_content"] = {"_summary": True, "type": str(type(sc))}

    # Why:
    # - Your biggest payload is typically results.baseline_sources_cache (full snapshots)
    # - The previous summarizer only handled top-level keys, so Sheets payload still exceeded limits
    # - This keeps the saved JSON smaller AND keeps json.loads(get_history) working reliably
    try:
        r = out.get("results")
        if isinstance(r, dict):
            r2 = dict(r)

            for big_key in ("baseline_sources_cache", "source_results"):
                if big_key in r2:
                    sr = r2.get(big_key)
                    if isinstance(sr, list):
                        sample = []
                        for item in sr[:2]:
                            if isinstance(item, dict):
                                item2 = dict(item)
                                if isinstance(item2.get("extracted_numbers"), list):
                                    item2["extracted_numbers"] = {
                                        "_summary": True,
                                        "count": len(item2["extracted_numbers"])
                                    }
                                sample.append(item2)
                        r2[big_key] = {"_summary": True, "count": len(sr), "sample": sample}
                    else:
                        r2[big_key] = {"_summary": True, "type": str(type(sr))}

            out["results"] = r2
    except Exception:
        pass

    return out

def _snapshot_store_dir() -> str:
    d = os.path.join(os.getcwd(), "snapshot_store")
    try:
        os.makedirs(d, exist_ok=True)
    except Exception:
        pass
    return d

def store_full_snapshots_local(baseline_sources_cache: list, source_snapshot_hash: str) -> str:
    """
    Store full snapshots deterministically by hash. Returns a store ref string (path).
    Additive-only helper.
    """
    if not source_snapshot_hash:
        return ""
    if not isinstance(baseline_sources_cache, list) or not baseline_sources_cache:
        return ""

    path = ""
    try:
        _d = _snapshot_store_dir() or os.path.join(os.getcwd(), "snapshot_store")
        path = os.path.join(_d, f"{source_snapshot_hash}.json")
    except Exception:
        return ""
    try:
        # write-once semantics (deterministic)
        if os.path.exists(path) and os.path.getsize(path) > 0:
            return path
    except Exception:
        pass

    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(baseline_sources_cache, f, ensure_ascii=False, default=str)
        return path
    except Exception:
        return ""

def load_full_snapshots_local(snapshot_store_ref: str) -> list:
    """
    Load full snapshots from a store ref string (path). Returns [] if not available.
    """
    try:
        if not snapshot_store_ref or not isinstance(snapshot_store_ref, str):
            return []
        if not os.path.exists(snapshot_store_ref):
            return []
        with open(snapshot_store_ref, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data if isinstance(data, list) else []
    except Exception:
        return []

def compute_source_snapshot_hash(baseline_sources_cache: list) -> str:
    pairs = []
    for sr in (baseline_sources_cache or []):
        if not isinstance(sr, dict):
            continue
        u = (sr.get("source_url") or sr.get("url") or "").strip()
        fp = (sr.get("fingerprint") or sr.get("content_fingerprint") or "").strip()
        if u:
            pairs.append((u, fp))
    pairs.sort()
    sig = "|".join([f"{u}#{fp}" for (u, fp) in pairs])
    return hashlib.sha256(sig.encode("utf-8")).hexdigest() if sig else ""
# Why:
# - Sheets-safe summarization may replace baseline_sources_cache/extracted_numbers
#   with summary dicts. However, evidence_records often remains available and is
#   already deterministic, snapshot-derived data.
# - This helper reconstructs the minimal snapshot shape needed for
#   source-anchored evolution WITHOUT re-fetching or heuristic matching.

# - Keeps v1 compute_source_snapshot_hash() for backward compatibility.
# - v2 includes url + status + fingerprint + (anchor_hash,value_norm,unit_tag) tuples (bounded) for stronger identity.
def compute_source_snapshot_hash_v2(baseline_sources_cache: list, max_items_per_source: int = 120) -> str:
    try:
        sources = baseline_sources_cache if isinstance(baseline_sources_cache, list) else []
        parts = []
        for s in sources:
            if not isinstance(s, dict):
                continue
            url = str(s.get("url") or "")
            status = str(s.get("status") or "")
            status_detail = str(s.get("status_detail") or "")
            fingerprint = str(s.get("fingerprint") or "")

            nums = s.get("extracted_numbers") or s.get("numbers") or []
            # Sometimes stored in summarized form
            if isinstance(nums, dict) and nums.get("_summary") and isinstance(nums.get("count"), int):
                # no details available; just use summary
                num_tuples = [("summary_count", int(nums.get("count")))]
            else:
                num_list = nums if isinstance(nums, list) else []
                num_tuples = []
                for n in num_list[: int(max_items_per_source or 120)]:
                    if not isinstance(n, dict):
                        continue
                    ah = str(n.get("anchor_hash") or "")
                    vn = n.get("value_norm")
                    ut = str(n.get("unit_tag") or n.get("unit") or "")
                    # Use JSON for float stability + None handling
                    num_tuples.append((ah, vn, ut))
                # Deterministic order
                num_tuples = sorted(num_tuples, key=lambda t: (t[0], str(t[1]), t[2]))

            parts.append({
                "url": url,
                "status": status,
                "status_detail": status_detail,
                "fingerprint": fingerprint,
                "nums": num_tuples,
            })

        # Deterministic ordering of sources
        parts = sorted(parts, key=lambda d: (d.get("url",""), d.get("fingerprint",""), d.get("status","")))

        payload = json.dumps(parts, ensure_ascii=False, default=str, separators=(",", ":"))
        return hashlib.sha256(payload.encode("utf-8")).hexdigest()
    except Exception:
        pass
        # Ultra-safe fallback (still deterministic-ish)
        try:
            return hashlib.sha256(str(baseline_sources_cache).encode("utf-8")).hexdigest()
        except Exception:
            return "0"*64

def build_baseline_sources_cache_from_evidence_records(evidence_records):

    """

    Rebuild a minimal baseline_sources_cache from evidence_records deterministically.

    PATCH AI3 (ADDITIVE): anchor integrity

    - Ensures each candidate has anchor_hash + candidate_id (derived if missing)

    - Preserves analysis-aligned numeric normalization fields when present

    - Deterministic ordering by (source_url, fingerprint)

    """
    if not isinstance(evidence_records, list) or not evidence_records:

        return []

    def _ensure_anchor_fields(c: dict, source_url: str = "") -> dict:

        c = c if isinstance(c, dict) else {}

        ctx = c.get("context_snippet") or c.get("context") or ""

        if isinstance(ctx, str):

            ctx = ctx.strip()[:240]

        else:

            ctx = ""

        raw = c.get("raw")

        if raw is None:

            v = c.get("value_norm") if c.get("value_norm") is not None else c.get("value")

            u = c.get("base_unit") or c.get("unit") or ""

            raw = f"{v}{u}"

        raw = str(raw)[:120]

        ah = c.get("anchor_hash") or c.get("anchor") or ""

        if not ah:

            ah = _yureeka_sha1_v1(f"{source_url}|{raw}|{ctx}")

            if ah:

                c["anchor_hash"] = ah

        if not c.get("candidate_id") and ah:

            c["candidate_id"] = str(ah)[:16]

        if source_url and not c.get("source_url"):

            c["source_url"] = source_url

        if ctx and not c.get("context_snippet"):

            c["context_snippet"] = ctx

        return c

    by_url = {}

    for rec in evidence_records:

        if not isinstance(rec, dict):

            continue

        url = rec.get("source_url") or rec.get("url") or ""

        fp = rec.get("fingerprint") or ""

        # candidates may be stored under candidates or extracted_numbers depending on producer

        cand_list = rec.get("candidates")

        if not isinstance(cand_list, list):

            cand_list = rec.get("extracted_numbers")

        if not isinstance(cand_list, list):

            cand_list = []

        out_cands = []

        for c in cand_list:

            if not isinstance(c, dict):

                continue

            cc = _ensure_anchor_fields(dict(c), url)

            out_cands.append(cc)

        if not out_cands:

            continue

        key = (str(url), str(fp))

        by_url.setdefault(key, {"source_url": url, "fingerprint": fp, "extracted_numbers": []})

        by_url[key]["extracted_numbers"].extend(out_cands)

    rebuilt = list(by_url.values())

    # deterministic sort & per-source deterministic candidate order

    try:

        rebuilt.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))

        for s in rebuilt:

            if isinstance(s, dict) and isinstance(s.get("extracted_numbers"), list):

                s["extracted_numbers"] = sorted(

                    s["extracted_numbers"],

                    key=lambda c: (

                        str(c.get("anchor_hash") or ""),

                        str(c.get("candidate_id") or ""),

                        str(c.get("raw") or ""),

                        str(c.get("unit") or ""),

                    )

                )

    except Exception:

        pass

    try:
        if isinstance(rebuilt, dict):
            rebuilt.setdefault("_fix41afc5_debug", {})
            if isinstance(rebuilt.get("_fix41afc5_debug"), dict):
                rebuilt["_fix41afc5_debug"].update(dict(_fix41afc5_dbg2))
    except Exception:
        pass

    return rebuilt
def _sheets_now_ts():
    try:
        return float(time.time())
    except Exception:
        return 0.0

def _sheets_cache_get(key: str):
    try:
        item = _SHEETS_READ_CACHE.get(key)
        if not item:
            return None
        ts, val = item
        if (_sheets_now_ts() - ts) > _SHEETS_READ_CACHE_TTL_SEC:
            return None
        return val
    except Exception:
        return None

def _sheets_cache_set(key: str, val):
    try:
        _SHEETS_READ_CACHE[key] = (_sheets_now_ts(), val)
    except Exception:
        pass

def _is_sheets_rate_limit_error(err: Exception) -> bool:
    s = ""
    try:
        s = str(err) or ""
    except Exception:
        pass
        s = ""
    # Common markers seen via gspread/googleapiclient:
    markers = ["RESOURCE_EXHAUSTED", "Quota exceeded", "RATE_LIMIT_EXCEEDED", "429", "ReadRequestsPerMinutePerUser", "Read requests per minute", "read_requests"]
    return any(m in s for m in markers)

def sheets_get_all_values_cached(ws, cache_key: str):
    """
    Cached wrapper for ws.get_all_values() with rate-limit fallback.
    cache_key should be stable for the worksheet (e.g., 'Snapshots', 'HistoryFull', 'History').
    """
    global _SHEETS_LAST_READ_ERROR
    key = f"get_all_values:{cache_key}"
    cached = _sheets_cache_get(key)
    if cached is not None:
        return cached

    # REFACTOR195: if we recently hit a Sheets 429, avoid hammering the API and prefer cached/stale values.
    if _yureeka_sheets_in_rate_limit_cooldown_v1():
        stale = _SHEETS_READ_CACHE.get(key)
        if stale and isinstance(stale, tuple) and len(stale) == 2:
            return stale[1]
        return []
    try:
        # Previous draft accidentally recursed into itself and referenced an undefined variable.
        # This is a direct execution conflict fix (no behavior change intended beyond correctness).
        values = ws.get_all_values() if ws else []
        _sheets_cache_set(key, values)
        return values
    except Exception as e:
        _SHEETS_LAST_READ_ERROR = str(e)
        # Rate-limit fallback: return last cached value if we have one, else empty list
        if _is_sheets_rate_limit_error(e):
            _yureeka_sheets_mark_rate_limited_v1(e)
            stale = _SHEETS_READ_CACHE.get(key)
            if stale and isinstance(stale, tuple) and len(stale) == 2:
                return stale[1]
            return []
        raise

# Purpose:
#   - Persist full baseline_sources_cache inside the same Spreadsheet
#     but in a dedicated worksheet (tab), chunked across rows.
#   - Enables deterministic rehydration for evolution without refetch.
# Notes:
#   - Write-once semantics by source_snapshot_hash.
#   - Chunking and reassembly are deterministic (part_index ordering).

def get_google_spreadsheet():
    """Connect to Google Spreadsheet (cached connection if available).

    REFACTOR195:
      - Reuse the session-cached gspread client/spreadsheet where possible.
      - Respect the 429 cooldown to avoid repeated read-request quota errors.
    """
    try:
        spreadsheet_name = st.secrets.get("google_sheets", {}).get("spreadsheet_name", "Yureeka_JSON")

        if _yureeka_sheets_in_rate_limit_cooldown_v1():
            ss_key = f"_yureeka_gspread_ss_v1::{spreadsheet_name}"
            cached_ss = st.session_state.get(ss_key)
            if cached_ss is not None:
                return cached_ss
            _yureeka_sheets_rate_limit_notice_once_v1("⚠️ Google Sheets rate limit hit (429). Using local snapshot cache for now.")
            return None

        scopes = _coerce_google_oauth_scopes(SCOPES)
        creds = Credentials.from_service_account_info(dict(st.secrets["gcp_service_account"]), scopes=scopes)

        client_key = "_yureeka_gspread_client_v1"
        ss_key = f"_yureeka_gspread_ss_v1::{spreadsheet_name}"

        client = st.session_state.get(client_key)
        if client is None:
            client = gspread.authorize(creds)
            st.session_state[client_key] = client

        ss = st.session_state.get(ss_key)
        if ss is None:
            ss = client.open(spreadsheet_name)
            st.session_state[ss_key] = ss

        return ss
    except Exception as e:
        try:
            if _is_sheets_rate_limit_error(e):
                _yureeka_sheets_mark_rate_limited_v1(e)
                _yureeka_sheets_rate_limit_notice_once_v1("⚠️ Google Sheets rate limit hit (429). Using local snapshot cache for now.")
                try:
                    spreadsheet_name = st.secrets.get("google_sheets", {}).get("spreadsheet_name", "Yureeka_JSON")
                    ss_key = f"_yureeka_gspread_ss_v1::{spreadsheet_name}"
                    cached_ss = st.session_state.get(ss_key)
                    if cached_ss is not None:
                        return cached_ss
                except Exception:
                    pass
                return None
            st.error(f"❌ Failed to connect to Google Sheets: {e}")
        except Exception:
            pass
        return None

def _ensure_snapshot_worksheet(spreadsheet, title: str = "Snapshots"):
    """Ensure a worksheet tab exists for snapshot storage."""
    try:
        if not spreadsheet:
            return None
        try:
            ws = spreadsheet.worksheet(title)
            return ws
        except Exception:
            pass
            # Create with a reasonable default size; Sheets can expand.
            ws = spreadsheet.add_worksheet(title=title, rows=2000, cols=8)
            try:
                ws.append_row(
                    ["source_snapshot_hash", "part_index", "total_parts", "payload_part", "created_at", "code_version", "fingerprints_sig", "sha256"],
                    value_input_option="RAW",
                )
            except Exception:
                return ws
    except Exception:
        return None

def store_full_snapshots_to_sheet(baseline_sources_cache: list, source_snapshot_hash: str, worksheet_title: str = "Snapshots", chunk_chars: int = 20000) -> str:
    """
    Store full snapshots to a dedicated worksheet tab in chunked rows.
    Returns a ref string like: 'gsheet:Snapshots:<hash>'

    REFACTOR40 (BUGFIX):
    - Previously, the "write-once" gate used ws.findall(hash) and would treat ANY existing rows
      as "already written". If a prior write partially failed (rate-limit / quota / transient),
      we could end up with an incomplete snapshot stored under the hash forever, and subsequent
      runs would never repair it.
    - Now:
      * If rows exist, we first validate that the snapshot is actually loadable.
      * If not loadable, we attempt a repair write (append a fresh batch keyed by created_at).
      * After successful writes, we invalidate the worksheet read cache so recent snapshots
        are immediately retrievable.
    """
    import zlib, base64
    if not source_snapshot_hash:
        return ""
    if not isinstance(baseline_sources_cache, list) or not baseline_sources_cache:
        return ""

    try:
        ss = get_google_spreadsheet()
        ws = _ensure_snapshot_worksheet(ss, worksheet_title) if ss else None
        if not ws:
            return ""

        # If any rows exist for this hash, only short-circuit if it is actually loadable.
        try:
            existing = ws.findall(source_snapshot_hash)
            if existing:
                try:
                    _probe = load_full_snapshots_from_sheet(source_snapshot_hash, worksheet_title=worksheet_title)
                    if isinstance(_probe, list) and _probe:
                        return f"gsheet:{worksheet_title}:{source_snapshot_hash}"
                except Exception:
                    pass
        except Exception:
            pass

        payload = json.dumps(baseline_sources_cache, ensure_ascii=False, default=str)
        # reduce write volume / API calls (helps avoid rate limits).
        # Storage format:
        #   payload_part begins with 'zlib64:' then base64(zlib(json_bytes))
        # Backward compatible: loader detects/decompresses when prefix present.
        try:
            if isinstance(payload, str) and len(payload) > 120000:
                _raw = payload.encode("utf-8", errors="strict")
                _comp = zlib.compress(_raw, level=9)
                _b64 = base64.b64encode(_comp).decode("ascii")
                payload = "zlib64:" + _b64
        except Exception:
            pass

        sha = hashlib.sha256(payload.encode("utf-8")).hexdigest()

        # deterministic chunking
        chunk_size = max(1000, int(chunk_chars or 45000))
        parts = [payload[i:i+chunk_size] for i in range(0, len(payload), chunk_size)]
        total = len(parts)

        # Optional fingerprints signature (stable)
        pairs = []
        for sr in baseline_sources_cache:
            if isinstance(sr, dict):
                u = (sr.get("source_url") or sr.get("url") or "").strip()
                fp = (sr.get("fingerprint") or sr.get("content_fingerprint") or "").strip()
                if u:
                    pairs.append((u, fp))
        pairs.sort()
        fingerprints_sig = "|".join([f"{u}#{fp}" for (u, fp) in pairs]) if pairs else ""

        created_at = _yureeka_now_iso_utc()

        # best-effort: use global if exists
        code_version = ""
        try:
            code_version = globals().get("CODE_VERSION") or ""
        except Exception:
            code_version = ""

        rows = []
        for idx, part in enumerate(parts):
            rows.append([source_snapshot_hash, idx, total, part, created_at, code_version, fingerprints_sig, sha])

        wrote_all = False
        try:
            # Append in small batches to reduce API payload size / rate-limit failures.
            batch_size = 10
            _need_throttle = (len(rows) > 60)
            wrote = 0
            for i in range(0, len(rows), batch_size):
                chunk = rows[i:i+batch_size]
                ws.append_rows(chunk, value_input_option="RAW")
                wrote += len(chunk)
                try:
                    if _need_throttle:
                        time.sleep(0.15)
                except Exception:
                    pass
            wrote_all = (wrote == len(rows))
        except Exception:
            # Fall back to append_row loop; do NOT early-return on the first failure.
            success = 0
            for r in rows:
                try:
                    ws.append_row(r, value_input_option="RAW")
                    success += 1
                except Exception:
                    pass
            wrote_all = (success == len(rows))

        # If we believe we wrote all rows, invalidate snapshot read cache so we can re-load immediately.
        if wrote_all:
            try:
                # This cache key format matches sheets_get_all_values_cached()
                _cache_key = f"get_all_values:{worksheet_title}"
                _cache = globals().get("_SHEETS_READ_CACHE")
                if isinstance(_cache, dict):
                    _cache.pop(_cache_key, None)
            except Exception:
                pass
            return f"gsheet:{worksheet_title}:{source_snapshot_hash}"

        # Partial write: return empty ref to avoid pointing to a broken snapshot.
        return ""
    except Exception:
        return ""

def load_full_snapshots_from_sheet(source_snapshot_hash: str, worksheet_title: str = "Snapshots") -> list:
    """Load and reassemble full snapshots list from a dedicated worksheet.

    REFACTOR40 (BUGFIX):
    - Fix stale cache behavior: if the requested hash is not found in cached values, do a direct read once.
    - Fix partial-write repair behavior: if multiple write batches exist for the same hash, select the
      most recent *complete* batch (grouped by created_at), not a mixed/partial merge.
    """
    if not source_snapshot_hash:
        return []
    try:
        ss = get_google_spreadsheet()
        ws = ss.worksheet(worksheet_title) if ss else None
        if not ws:
            return []

        def _read_cached():
            try:
                return sheets_get_all_values_cached(ws, cache_key=worksheet_title)
            except Exception:
                return []

        def _read_direct():
            try:
                return ws.get_all_values()
            except Exception:
                return []

        values = _read_cached()

        # If empty/too small, do one direct read to bypass stale empty cache.
        if not values or len(values) < 2:
            values = _read_direct()
            if not values or len(values) < 2:
                return []
            # Best-effort cache refresh
            try:
                _sheets_cache_set(f"get_all_values:{worksheet_title}", values)
            except Exception:
                pass

        header = values[0] or []
        # Expect at least: source_snapshot_hash, part_index, total_parts, payload_part
        try:
            col_h = header.index("source_snapshot_hash")
            col_i = header.index("part_index")
            col_t = header.index("total_parts")
            col_p = header.index("payload_part")
            col_ca = header.index("created_at") if "created_at" in header else None
            col_sha = header.index("sha256") if "sha256" in header else None
        except Exception:
            # If headers are missing/misaligned, bail safely
            return []

        def _safe_int(x):
            try:
                return int(x)
            except Exception:
                return 0

        def _parse_iso(s: str):
            try:
                # Lexicographic order works for ISO8601 UTC strings, but parse for safety.
                return datetime.fromisoformat(str(s).replace("Z", "+00:00"))
            except Exception:
                return None

        def _extract_best(values_table):
            rows = []
            for r in values_table[1:]:
                try:
                    if len(r) > col_h and r[col_h] == source_snapshot_hash:
                        rows.append(r)
                except Exception:
                    continue
            if not rows:
                return []

            # Group by created_at (per-write batch). If missing, fall back to a single group.
            groups = {}
            for r in rows:
                k = ""
                try:
                    if col_ca is not None and len(r) > col_ca:
                        k = str(r[col_ca] or "")
                except Exception:
                    k = ""
                if not k:
                    k = "legacy"
                groups.setdefault(k, []).append(r)

            candidates = []
            for created_at, grows in groups.items():
                try:
                    # Determine expected total parts
                    expected_total = 0
                    try:
                        if grows and len(grows[0]) > col_t:
                            expected_total = _safe_int(grows[0][col_t])
                    except Exception:
                        expected_total = 0
                    if expected_total <= 0:
                        continue

                    # Build part map (dedupe by part_index; keep longest payload_part)
                    part_map = {}
                    for rr in grows:
                        try:
                            pi = _safe_int(rr[col_i] if len(rr) > col_i else 0)
                            pp = rr[col_p] if len(rr) > col_p else ""
                            if pi not in part_map or (isinstance(pp, str) and len(pp) > len(part_map.get(pi, ""))):
                                part_map[pi] = pp or ""
                        except Exception:
                            continue

                    # Completeness check: must have all indices 0..expected_total-1
                    if len(part_map) < expected_total:
                        continue
                    missing = False
                    payload_parts = []
                    for i in range(expected_total):
                        if i not in part_map:
                            missing = True
                            break
                        payload_parts.append(part_map[i] or "")
                    if missing:
                        continue

                    payload = "".join(payload_parts)

                    # Optional integrity check
                    try:
                        if col_sha is not None:
                            exp = ""
                            try:
                                exp = (grows[0][col_sha] if len(grows[0]) > col_sha else "") or ""
                            except Exception:
                                exp = ""
                            if exp:
                                actual = hashlib.sha256(payload.encode("utf-8")).hexdigest()
                                if actual != exp:
                                    continue
                    except Exception:
                        pass
                    # JSON decode (supports REFACTOR42 compressed payloads)
                    try:
                        payload_json = payload
                        try:
                            # REFACTOR43 (BUGFIX): transparently decode 'zlib64:' compressed payloads.
                            if isinstance(payload_json, str) and payload_json.startswith("zlib64:"):
                                import base64, zlib
                                b64 = payload_json.split(":", 1)[1] if ":" in payload_json else ""
                                if not b64:
                                    continue
                                comp = base64.b64decode(b64.encode("ascii"), validate=False)
                                raw = zlib.decompress(comp)
                                payload_json = raw.decode("utf-8", errors="strict")
                        except Exception:
                            # If decoding fails, treat as invalid snapshot batch
                            continue

                        data = json.loads(payload_json)
                        if not isinstance(data, list) or not data:
                            continue
                    except Exception:
                        continue
                    # Candidate score: prefer latest created_at when parseable; else fallback to string.
                    dt = _parse_iso(created_at) if created_at and created_at != "legacy" else None
                    candidates.append((dt, created_at, data))
                except Exception:
                    continue

            if not candidates:
                return []
            # Choose best candidate: latest datetime if available else latest created_at string.
            candidates.sort(key=lambda x: (x[0] is not None, x[0] or x[1]), reverse=True)
            return candidates[0][2]

        best = _extract_best(values)

        # If the hash is missing or incomplete in cached values, do ONE direct refresh and retry.
        if not best:
            direct = _read_direct()
            if direct and len(direct) >= 2:
                try:
                    best = _extract_best(direct)
                except Exception:
                    best = []
                # refresh cache best-effort
                try:
                    _sheets_cache_set(f"get_all_values:{worksheet_title}", direct)
                except Exception:
                    pass

        return best if isinstance(best, list) else []
    except Exception:
        return []

# Why:
# - Evolution may receive a sheets-safe wrapper that omits primary_response,
#   metric_schema_frozen, metric_anchors, etc.
# - When wrapper includes full_store_ref ("gsheet:HistoryFull:<analysis_id>"),
#   we can deterministically load the full analysis payload (no re-fetch).
# Notes:
# - Additive only. Safe no-op if sheet/tab not present.

def write_full_history_payload_to_sheet(analysis_id: str, payload: str, worksheet_title: str = "HistoryFull", chunk_size: int = 20000) -> bool:
    """Write a full analysis payload (string) into HistoryFull as chunked rows keyed by analysis_id.

    REFACTOR112:
      - Ensure HistoryFull headers match the 7-column schema:
          analysis_id, part_index, total_parts, payload_part, created_at, code_version, sha256
      - Backward compatible: if sheet is legacy 5-col, we still write 5-col rows.
      - Stores sha256 for integrity (full stitched payload).
    """
    if not analysis_id or not payload:
        return False

    # Derive created_at + code_version from payload when possible (best-effort)
    created_at_iso = ""
    code_version = ""
    try:
        obj = json.loads(payload)
        if isinstance(obj, dict):
            created_at_iso = str(obj.get("timestamp") or "")
            if not created_at_iso:
                r = obj.get("results") if isinstance(obj.get("results"), dict) else {}
                created_at_iso = str(r.get("timestamp") or "")
            code_version = str(obj.get("code_version") or "")
            if not code_version:
                r = obj.get("results") if isinstance(obj.get("results"), dict) else {}
                code_version = str(r.get("code_version") or "")
    except Exception:
        pass

    if not created_at_iso:
        try:
            created_at_iso = datetime.now(timezone.utc).isoformat()
        except Exception:
            created_at_iso = ""

    if not code_version:
        try:
            code_version = str(globals().get("CODE_VERSION") or globals().get("_YUREEKA_CODE_VERSION_LOCK") or "")
        except Exception:
            code_version = ""

    try:
        ss = get_google_spreadsheet()
        if not ss:
            return False
        try:
            ws = ss.worksheet(worksheet_title)
        except Exception:
            # Create sheet if missing (best-effort)
            try:
                ws = ss.add_worksheet(title=worksheet_title, rows=2000, cols=10)
            except Exception:
                ws = ss.worksheet(worksheet_title)

        # Ensure headers exist (prefer 7-col schema)
        headers = []
        try:
            headers = ws.row_values(1) or []
        except Exception:
            headers = []

        want_7 = False
        try:
            if headers and headers[0] == "analysis_id":
                if len(headers) >= 7 and ("created_at" in headers or "code_version" in headers):
                    want_7 = True
                elif len(headers) >= 7:
                    # header length already 7+; assume 7-col layout
                    want_7 = True
        except Exception:
            pass

        # Upgrade headers to 7-col schema if missing/legacy
        try:
            if (not headers) or (len(headers) < 7) or (headers[0] != "analysis_id") or ("created_at" not in headers) or ("code_version" not in headers):
                _ = ws.update('A1:G1', [["analysis_id", "part_index", "total_parts", "payload_part", "created_at", "code_version", "sha256"]])
                want_7 = True
        except Exception:
            # If header upgrade fails, fall back to legacy 5-col schema
            want_7 = False
            try:
                if (not headers) or (len(headers) < 5) or (headers and headers[0] != "analysis_id"):
                    _ = ws.update('A1:E1', [["analysis_id", "part_index", "total_parts", "payload_part", "sha256"]])
            except Exception:
                pass

        sha = hashlib.sha256(payload.encode("utf-8", errors="ignore")).hexdigest()
        parts = [payload[i:i + chunk_size] for i in range(0, len(payload), chunk_size)]
        total = len(parts) if parts else 0
        if total == 0:
            return False

        rows = []
        for i, part in enumerate(parts):
            if want_7:
                rows.append([analysis_id, str(i), str(total), part, created_at_iso, code_version, sha])
            else:
                rows.append([analysis_id, str(i), str(total), part, sha])

        # Append in one batch if possible
        try:
            ws.append_rows(rows, value_input_option="RAW")
        except Exception:
            # Fallback to per-row append
            for r in rows:
                try:
                    ws.append_row(r, value_input_option="RAW")
                except Exception:
                    return False

        # Invalidate HistoryFull cache after successful write so immediate rehydration sees new rows.
        try:
            _cache = globals().get("_SHEETS_READ_CACHE")
            if isinstance(_cache, dict):
                _cache.pop(f"get_all_values:{worksheet_title}", None)
        except Exception:
            pass

        # Mark HistoryFull as dirty so loader bypasses cached reads once (same-session baseline freshness).
        try:
            st.session_state["_historyfull_dirty_v1"] = float(time.time())
            st.session_state["_historyfull_dirty_reason_v1"] = "write_full_history_payload_to_sheet"
        except Exception:
            pass

        return True
    except Exception:
        return False

def load_full_history_payload_from_sheet(analysis_id: str, worksheet_title: str = "HistoryFull") -> dict:
    """
    Load the full analysis JSON payload from the HistoryFull worksheet.

    PATCH HF_LOAD_V2 (ADDITIVE):
    - Supports payloads split across multiple rows (chunked writes).
    - Supports current HistoryFull headers:
        analysis_id, part_index, total_parts, payload_json, created_at, code_version, sha256
    - Backward compatible with older variants:
        id, part_index, total_parts, payload_part / data, sha256
    - Deterministic stitching (sort by part_index) + safe JSON parse.

    PATCH HF_LOAD_V3 (ADDITIVE):
    - Verifies chunk completeness when total_parts is available (0..total_parts-1).
    - Optionally verifies sha256 when present (stitched string).
    - Does not change failure mode: still returns {} on any failure.
    """
    try:
        ss = get_google_spreadsheet()
        if not ss:
            return {}

        try:
            ws = ss.worksheet(worksheet_title)
        except Exception:
            return {}

        # Read all rows (prefer cached getter if present)
        rows = []
        _r110_used_direct = False

        # REFACTOR110: after a HistoryFull write, bypass cached reads once to avoid stale/empty cache.
        try:
            _dirty = st.session_state.get("_historyfull_dirty_v1")
        except Exception:
            _dirty = None
        try:
            if isinstance(_dirty, (int, float)) and (time.time() - float(_dirty) < 120):
                rows = ws.get_all_values() or []
                _r110_used_direct = True
                try:
                    _sheets_cache_set(f"get_all_values:{worksheet_title}", rows)
                except Exception:
                    pass
                try:
                    st.session_state.pop("_historyfull_dirty_v1", None)
                    st.session_state.pop("_historyfull_dirty_reason_v1", None)
                except Exception:
                    pass
        except Exception:
            pass

        if not _r110_used_direct:
            try:
                fn = globals().get("sheets_get_all_values_cached")
                rows = fn(ws, cache_key=worksheet_title) if callable(fn) else (ws.get_all_values() or [])
            except Exception:
                pass
                try:
                    rows = ws.get_all_values() or []
                except Exception:
                    return {}

        # REFACTOR110: if cached read returned empty/short, do ONE direct refresh and retry.
        if (not rows) or (len(rows) < 2):
            try:
                direct = ws.get_all_values() or []
                if direct and len(direct) >= 2:
                    rows = direct
                    try:
                        _sheets_cache_set(f"get_all_values:{worksheet_title}", rows)
                    except Exception:
                        pass
            except Exception:
                return {}

        if not rows or len(rows) < 2:
            return {}

        header = rows[0] or []
        body = rows[1:] or []

        def _col(name: str):
            try:
                return header.index(name)
            except Exception:
                return None

        c_id = _col("analysis_id")
        if c_id is None:
            c_id = _col("id")
        if c_id is None:
            c_id = 0  # last-ditch fallback

        c_part = _col("part_index")
        c_total = _col("total_parts")

        # IMPORTANT: your sheet uses payload_json
        c_payload = _col("payload_json")
        if c_payload is None:
            c_payload = _col("payload_part")
        if c_payload is None:
            c_payload = _col("data")
        if c_payload is None:
            c_payload = len(header) - 1  # last-ditch fallback

        c_sha = _col("sha256")

        target_id = str(analysis_id).strip()
        if not target_id:
            return {}

        # (pidx:int|None, total:int|None, chunk:str, sha:str)
        parts_with_sha = []

        for r in body:
            try:
                if not r:
                    continue

                rid = r[c_id] if c_id < len(r) else ""
                rid = str(rid).strip()
                if rid != target_id:
                    continue

                chunk = r[c_payload] if c_payload < len(r) else ""
                chunk = chunk or ""
                if not isinstance(chunk, str):
                    chunk = str(chunk)

                # part_index (optional)
                pidx = None
                if c_part is not None and c_part < len(r):
                    try:
                        pidx = int(str(r[c_part]).strip())
                    except Exception:
                        pass
                        pidx = None

                # total_parts (optional)
                tparts = None
                if c_total is not None and c_total < len(r):
                    try:
                        tparts = int(str(r[c_total]).strip())
                    except Exception:
                        pass
                        tparts = None

                sha = ""
                if c_sha is not None and c_sha < len(r):
                    sha = str(r[c_sha] or "").strip()

                # keep even tiny chunks; concatenation is deterministic
                if chunk.strip() == "":
                    continue

                parts_with_sha.append((pidx, tparts, chunk, sha))
            except Exception:
                pass
                continue

        if not parts_with_sha:
            return {}

        # Score = (unique part_index count, total payload length)
        parts = []          # list[(pidx, chunk)]
        chosen_sha = ""     # sha bucket selected (if any)
        chosen_total = None # total_parts inferred for chosen bucket (if any)
        try:
            if any(s for _, _, _, s in parts_with_sha):
                buckets = {}
                for pidx, tparts, chunk, sha in parts_with_sha:
                    key = sha or "__no_sha__"
                    buckets.setdefault(key, []).append((pidx, tparts, chunk))

                def _score(items):
                    idxs = [i for i, _, _ in items if i is not None]
                    uniq = len(set(idxs)) if idxs else 0
                    total_len = sum(len(c or "") for _, _, c in items)
                    return (uniq, total_len)

                best_key = sorted(buckets.keys(), key=lambda k: _score(buckets[k]), reverse=True)[0]
                chosen_sha = "" if best_key == "__no_sha__" else best_key
                best_items = buckets[best_key]

                # Infer total_parts for this bucket (mode / max)
                try:
                    totals = [tp for _, tp, _ in best_items if isinstance(tp, int) and tp > 0]
                    chosen_total = max(totals) if totals else None
                except Exception:
                    pass
                    chosen_total = None

                parts = [(pidx, chunk) for (pidx, _tparts, chunk) in best_items]
            else:
                parts = [(pidx, chunk) for pidx, _tparts, chunk, _sha in parts_with_sha]
                # Infer total_parts (mode / max) even without sha
                try:
                    totals = [tp for _, tp, _, _ in parts_with_sha if isinstance(tp, int) and tp > 0]
                    chosen_total = max(totals) if totals else None
                except Exception:
                    pass
                    chosen_total = None
        except Exception:
            pass
            parts = [(pidx, chunk) for pidx, _tparts, chunk, _sha in parts_with_sha]
            try:
                totals = [tp for _, tp, _, _ in parts_with_sha if isinstance(tp, int) and tp > 0]
                chosen_total = max(totals) if totals else None
            except Exception:
                pass
                chosen_total = None

        # Sort parts deterministically by part_index; None last
        def _sort_key(t):
            pidx, _ = t
            return (pidx is None, pidx if pidx is not None else 0)

        parts.sort(key=_sort_key)

        # - If total_parts is known and we have part_index values, require 0..total-1.
        # - If incomplete, return {} (do not attempt parse on partial payload).
        try:
            if isinstance(chosen_total, int) and chosen_total > 0:
                idxs = [p for (p, _c) in parts if isinstance(p, int)]
                if idxs:
                    uniq = sorted(set(idxs))
                    expected = list(range(0, chosen_total))
                    if uniq != expected:
                        return {}
        except Exception:
            pass

        # Stitch chunks
        full_json_str = "".join([chunk for _, chunk in parts]).strip()
        if not full_json_str:
            return {}

        # - If chosen_sha exists, compare against sha256(stitched_bytes).
        # - If mismatch, return {} (treat as corrupted / wrong bucket).
        try:
            if isinstance(chosen_sha, str) and chosen_sha:
                digest = hashlib.sha256(full_json_str.encode("utf-8", errors="ignore")).hexdigest()
                if str(digest).lower() != str(chosen_sha).lower():
                    return {}
        except Exception:
            pass

        try:
            obj = json.loads(full_json_str)
            if isinstance(obj, dict):
                # Only attaches when parse succeeded.
                try:
                    obj["_rehydration_debug"] = {
                        "worksheet": str(worksheet_title or ""),
                        "analysis_id": str(target_id),
                        "parts_used": int(len(parts)),
                        "total_parts_expected": int(chosen_total) if isinstance(chosen_total, int) else None,
                        "sha_verified": bool(chosen_sha),
                    }
                except Exception:
                    pass
                return obj
            return {}
        except Exception:
            pass
            try:
                # Try to isolate first "{" and last "}" if accidental prefix/suffix exists
                a = full_json_str.find("{")
                b = full_json_str.rfind("}")
                if a != -1 and b != -1 and b > a:
                    obj2 = json.loads(full_json_str[a:b+1])
                    if isinstance(obj2, dict):
                        # (keep same meta stamp behavior)
                        try:
                            obj2["_rehydration_debug"] = {
                                "worksheet": str(worksheet_title or ""),
                                "analysis_id": str(target_id),
                                "parts_used": int(len(parts)),
                                "total_parts_expected": int(chosen_total) if isinstance(chosen_total, int) else None,
                                "sha_verified": bool(chosen_sha),
                                "salvaged": True,
                            }
                        except Exception:
                            return obj2
            except Exception:
                return {}

    except Exception:
        return {}

def fingerprint_text(text: str) -> str:
    """Stable short fingerprint for fetched content (deterministic)."""
    if not text:
        return ""
    normalized = re.sub(r"\s+", " ", text.strip().lower())
    return hashlib.md5(normalized.encode("utf-8")).hexdigest()[:12]

# =========================
# REFACTOR111: Prev snapshot picker (deterministic)
# =========================

def _refactor111_norm_question_v1(q: str) -> str:
    try:
        return re.sub(r"\s+", " ", str(q or "").strip().lower())
    except Exception:
        return str(q or "").strip().lower()

def _refactor111_get_question_from_payload_v1(obj: dict) -> str:
    try:
        if not isinstance(obj, dict):
            return ""
        q = obj.get("question") or ""
        if q:
            return str(q)
        r = obj.get("results") if isinstance(obj.get("results"), dict) else {}
        q2 = r.get("question") or ""
        if q2:
            return str(q2)
        pr = obj.get("primary_response") if isinstance(obj.get("primary_response"), dict) else {}
        q3 = pr.get("question") or ""
        return str(q3 or "")
    except Exception:
        return ""

def _refactor111_get_pmc_v1(obj: dict) -> dict:
    try:
        if not isinstance(obj, dict):
            return {}
        pmc = obj.get("primary_metrics_canonical")
        if isinstance(pmc, dict) and pmc:
            return pmc
        r = obj.get("results") if isinstance(obj.get("results"), dict) else {}
        pmc = r.get("primary_metrics_canonical")
        if isinstance(pmc, dict) and pmc:
            return pmc
        pr = obj.get("primary_response") if isinstance(obj.get("primary_response"), dict) else {}
        pmc = pr.get("primary_metrics_canonical")
        if isinstance(pmc, dict) and pmc:
            return pmc
    except Exception:
        pass
    return {}

def _refactor111_extract_bsc_urls_v1(obj: dict) -> list:
    urls = []
    try:
        if not isinstance(obj, dict):
            return []
        # Prefer baseline_sources_cache
        bsc = obj.get("baseline_sources_cache")
        if not isinstance(bsc, list):
            r = obj.get("results") if isinstance(obj.get("results"), dict) else {}
            bsc = r.get("baseline_sources_cache")
        if isinstance(bsc, list):
            for it in bsc:
                if isinstance(it, dict):
                    u = it.get("source_url") or it.get("url")
                    if u:
                        urls.append(str(u))
        # Also allow 'sources' list
        if not urls:
            s = obj.get("sources")
            if isinstance(s, list):
                urls = [str(u) for u in s if isinstance(u, str) and u.strip()]
    except Exception:
        pass
    # stable dedupe
    seen, out = set(), []
    for u in urls:
        uu = (u or "").strip()
        if not uu or uu in seen:
            continue
        seen.add(uu)
        out.append(uu)
    return out

def _refactor111_parse_ts_v1(ts: str):
    try:
        fn = globals().get("_parse_iso_dt")
        if callable(fn):
            dt = fn(ts)
            return dt
    except Exception:
        pass
    return None

def _refactor111_mask_sheet_id_v1(sid: str) -> str:
    try:
        sid = str(sid or "")
        if len(sid) <= 8:
            return sid
        return f"...{sid[-6:]}"
    except Exception:
        return ""
def _refactor112_parse_ts_from_analysis_id_v1(aid: str):
    """REFACTOR112: fallback timestamp inference from analysis_id like YYYYMMDD_HHMMSS_xxxxxx."""
    try:
        s = str(aid or "").strip()
        # Expected: 20260203_003809_7aa711
        if len(s) >= 15 and s[8] == "_" and s[15:16] == "_":
            d = s[0:8]
            t = s[9:15]
        elif len(s) >= 15 and s[8] == "_" and len(s) >= 15:
            d = s[0:8]
            t = s[9:15]
        else:
            return None
        if not (d.isdigit() and t.isdigit()):
            return None
        dt = datetime.strptime(d + t, "%Y%m%d%H%M%S")
        # Treat as UTC (naive)
        return dt
    except Exception:
        return None

def _refactor111_pick_latest_prev_snapshot_v1(previous_data: dict, web_context: dict = None):
    """
    REFACTOR111:
    - Prefer HistoryFull by max parsed timestamp (created_at or payload.timestamp)
    - Filter: exact normalized question match; must have non-empty primary_metrics_canonical
    - Fallback: local history files by mtime; finally snapshot_ref/previous_data
    """
    dbg = {
        "origin_attempted": "",
        "candidates_considered": [],
        "selected_ref": "",
        "selected_timestamp": "",
        "selected_code_version": "",
        "reason": "fallback",
        "selected_baseline_sources_urls": [],
        "sheet_probe": {}
    }

    prev = previous_data if isinstance(previous_data, dict) else {}
    wc = web_context if isinstance(web_context, dict) else {}

    target_q = _refactor111_norm_question_v1(
        wc.get("question") or prev.get("question") or _refactor111_get_question_from_payload_v1(prev)
    )

    # REFACTOR186: preserve in-session previous_data as a safe fallback to avoid baseline downgrade
    prev_ok = False
    prev_ts_val = 0.0
    prev_aid = ""
    try:
        if isinstance(prev, dict) and prev:
            try:
                fr = str(prev.get("full_store_ref") or "")
                if fr.startswith("gsheet:HistoryFull:"):
                    prev_aid = fr.split(":")[-1].strip()
            except Exception:
                prev_aid = ""
            prev_qn = _refactor111_norm_question_v1(_refactor111_get_question_from_payload_v1(prev))
            prev_pmc = _refactor111_get_pmc_v1(prev)
            if (not target_q) or (prev_qn == target_q):
                if isinstance(prev_pmc, dict) and prev_pmc:
                    # require at least one concrete value (avoid empty shells)
                    try:
                        for _k, _v in prev_pmc.items():
                            if isinstance(_v, dict) and _v.get("value") is not None:
                                prev_ok = True
                                break
                    except Exception:
                        prev_ok = False
            if prev_ok:
                _dtp = _refactor111_parse_ts_v1(str(prev.get("timestamp") or ""))
                if _dtp:
                    try:
                        prev_ts_val = _dtp.replace(tzinfo=timezone.utc).timestamp()
                    except Exception:
                        try:
                            prev_ts_val = float(_dtp.timestamp())
                        except Exception:
                            prev_ts_val = 0.0
            try:
                dbg["previous_data_candidate_v1"] = {
                    "ok": bool(prev_ok),
                    "timestamp": str(prev.get("timestamp") or ""),
                    "code_version": str(prev.get("code_version") or ""),
                    "full_store_ref": str(prev.get("full_store_ref") or ""),
                }
            except Exception:
                pass
    except Exception:
        prev_ok = False


    # -------- Attempt A: Sheet HistoryFull --------
    try:
        if FORCE_LATEST_PREV_SNAPSHOT_V1:
            ss = None
            try:
                ss = globals().get("get_google_spreadsheet")() if callable(globals().get("get_google_spreadsheet")) else None
            except Exception:
                ss = None

            if ss:
                sid = ""
                try:
                    sid = getattr(ss, "id", "") or ""
                except Exception:
                    sid = ""

                dbg["origin_attempted"] = "sheet_historyfull"
                try:
                    dbg["sheet_probe"]["sheet_id_masked"] = _refactor111_mask_sheet_id_v1(sid)
                except Exception:
                    pass

                ws = None
                try:
                    ws = ss.worksheet("HistoryFull")
                except Exception:
                    ws = None

                rows = []
                if ws:
                    try:
                        rows = ws.get_all_values() or []
                    except Exception:
                        rows = []
                    dbg["sheet_probe"]["historyfull_list_ok"] = bool(rows)
                    dbg["sheet_probe"]["historyfull_row_count"] = int(len(rows) if isinstance(rows, list) else 0)

                if rows and len(rows) >= 2:
                    header = rows[0]
                    try:
                        dbg["sheet_probe"]["historyfull_header"] = header
                        _lens = [len(_r) for _r in (rows[1:] or []) if isinstance(_r, list)]
                        if _lens:
                            dbg["sheet_probe"]["historyfull_row_len_minmax"] = [int(min(_lens)), int(max(_lens))]
                        dbg["sheet_probe"]["historyfull_expected_cols"] = 7
                    except Exception:
                        pass
                    def _col(name):
                        try:
                            return header.index(name)
                        except Exception:
                            return -1

                    c_aid = _col("analysis_id")
                    if c_aid < 0:
                        c_aid = _col("id")
                    c_created = _col("created_at")
                    c_cv = _col("code_version")

                    # Build analysis_id -> best_ts (REFACTOR112: fallback parse from analysis_id when created_at is not ISO)
                    best = {}
                    for r in rows[1:]:
                        if not isinstance(r, list):
                            continue
                        aid = (r[c_aid] if c_aid >= 0 and c_aid < len(r) else "") if r else ""
                        aid = str(aid or "").strip()
                        if not aid:
                            continue

                        created_raw = (r[c_created] if c_created >= 0 and c_created < len(r) else "") if r else ""
                        created_raw = str(created_raw or "")

                        dt = _refactor111_parse_ts_v1(created_raw)
                        inferred = False
                        display_created = created_raw

                        if not dt:
                            try:
                                _dt2 = _refactor112_parse_ts_from_analysis_id_v1(aid)
                            except Exception:
                                _dt2 = None
                            if _dt2 is not None:
                                dt = _dt2
                                inferred = True
                                try:
                                    display_created = _dt2.replace(tzinfo=timezone.utc).isoformat().replace("+00:00", "Z")
                                except Exception:
                                    pass

                        ts_val = 0.0
                        if dt:
                            try:
                                ts_val = dt.replace(tzinfo=timezone.utc).timestamp()
                            except Exception:
                                try:
                                    ts_val = float(dt.timestamp())
                                except Exception:
                                    ts_val = 0.0

                        if aid not in best or ts_val > float(best[aid].get("ts_val") or 0.0):
                            best[aid] = {
                                "ts_val": ts_val,
                                "created_at": str(display_created or ""),
                                "code_version": (r[c_cv] if c_cv >= 0 and c_cv < len(r) else ""),
                                "inferred_from_analysis_id": bool(inferred),
                            }

                    # Sort candidate aids by ts desc
                    cands = sorted(best.items(), key=lambda kv: float(kv[1]["ts_val"] or 0.0), reverse=True)

                    # record considered (top 50)
                    for aid, meta in cands[:50]:
                        dbg["candidates_considered"].append({
                            "ref": f"gsheet:HistoryFull:{aid}",
                            "timestamp": meta.get("created_at") or "",
                            "code_version": str(meta.get("code_version") or ""),
                            "timestamp_inferred": bool(meta.get("inferred_from_analysis_id")),
                        })

                    # walk candidates in timestamp order until match
                    loader = globals().get("load_full_history_payload_from_sheet")
                    _fallback_missing_q = None

                    def _has_any_value(_pmc: dict) -> bool:
                        try:
                            for _k, _v in (_pmc or {}).items():
                                if isinstance(_v, dict) and _v.get("value") is not None:
                                    return True
                        except Exception:
                            return False
                        return False

                    def _load_full_with_retries(_aid: str, _tries: int = 3):
                        last = {}
                        tries = 0
                        for _i in range(int(_tries) if int(_tries) > 0 else 1):
                            tries = _i + 1
                            try:
                                last = loader(_aid, worksheet_title="HistoryFull") if callable(loader) else {}
                            except Exception:
                                last = {}
                            if isinstance(last, dict) and last:
                                return last, tries
                            try:
                                time.sleep(0.25)
                            except Exception:
                                pass
                        return (last if isinstance(last, dict) else {}), tries

                    for aid, meta in cands[:50]:
                        full, _load_tries = _load_full_with_retries(aid, _tries=3)
                        try:
                            if not isinstance(dbg.get("candidate_load_v1"), list):
                                dbg["candidate_load_v1"] = []
                            if len(dbg["candidate_load_v1"]) < 8:
                                dbg["candidate_load_v1"].append({
                                    "analysis_id": str(aid or ""),
                                    "tries": int(_load_tries),
                                    "ok": bool(isinstance(full, dict) and bool(full)),
                                })
                        except Exception:
                            pass

                        if not isinstance(full, dict) or not full:
                            # If this is the in-session analysis_id and it's not readable yet, fall back to provided previous_data
                            try:
                                if prev_ok and prev_aid and str(aid) == str(prev_aid) and isinstance(prev, dict) and prev:
                                    sel_ts = str(prev.get("timestamp") or "")
                                    dbg["selected_ref"] = str(prev.get("full_store_ref") or "previous_data")
                                    dbg["selected_timestamp"] = sel_ts
                                    dbg["selected_code_version"] = str(prev.get("code_version") or "")
                                    dbg["reason"] = "previous_data_fallback_unreadable_historyfull"
                                    dbg["selected_baseline_sources_urls"] = _refactor111_extract_bsc_urls_v1(prev)
                                    prev["_refactor111_prev_snapshot_is_latest_v1"] = True
                                    prev["_refactor111_prev_snapshot_ref_v1"] = dbg["selected_ref"]
                                    prev["_refactor111_prev_snapshot_pick_v1"] = dbg
                                    return prev, dbg
                            except Exception:
                                pass
                            continue
                        qn = _refactor111_norm_question_v1(_refactor111_get_question_from_payload_v1(full))
                        pmc = _refactor111_get_pmc_v1(full)
                        if not (isinstance(pmc, dict) and pmc):
                            continue
                        if target_q and qn != target_q:
                            # REFACTOR124: allow newest snapshot even if question is missing, as long as PMC has values
                            try:
                                if (not qn) and _has_any_value(pmc) and (_fallback_missing_q is None):
                                    _fallback_missing_q = (full, aid, meta)
                            except Exception:
                                pass
                            continue
                        # REFACTOR186: do not downgrade baseline if provided previous_data is newer + valid
                        try:
                            cand_ts_val = float(meta.get("ts_val") or 0.0)
                            if prev_ok and prev_ts_val and prev_ts_val > cand_ts_val and isinstance(prev, dict) and prev:
                                dbg["selected_ref"] = str(prev.get("full_store_ref") or "previous_data")
                                dbg["selected_timestamp"] = str(prev.get("timestamp") or "")
                                dbg["selected_code_version"] = str(prev.get("code_version") or "")
                                dbg["reason"] = "prefer_previous_data_over_older_sheet_snapshot"
                                dbg["selected_baseline_sources_urls"] = _refactor111_extract_bsc_urls_v1(prev)
                                prev["_refactor111_prev_snapshot_is_latest_v1"] = True
                                prev["_refactor111_prev_snapshot_ref_v1"] = dbg["selected_ref"]
                                prev["_refactor111_prev_snapshot_pick_v1"] = dbg
                                return prev, dbg
                        except Exception:
                            pass


                        # Selected
                        sel_ts = str(full.get("timestamp") or "")
                        dbg["selected_ref"] = f"gsheet:HistoryFull:{aid}"
                        dbg["selected_timestamp"] = sel_ts
                        dbg["selected_code_version"] = str(full.get("code_version") or "")
                        dbg["reason"] = "max_timestamp"
                        try:
                            dbg.setdefault("sheet_probe", {})
                            if isinstance(dbg.get("sheet_probe"), dict):
                                dbg["sheet_probe"]["match_mode"] = "strict_question"
                        except Exception:
                            pass
                        dbg["selected_baseline_sources_urls"] = _refactor111_extract_bsc_urls_v1(full)

                        # Mark payload for downstream fastpath gate
                        full["_refactor111_prev_snapshot_is_latest_v1"] = True
                        full["_refactor111_prev_snapshot_ref_v1"] = dbg["selected_ref"]
                        full["_refactor111_prev_snapshot_pick_v1"] = dbg
                        return full, dbg

                    # REFACTOR124: fallback selection when newest snapshot lacks a question field
                    if target_q and _fallback_missing_q:
                        try:
                            full, aid, meta = _fallback_missing_q
                            sel_ts = str(full.get("timestamp") or meta.get("created_at") or "")
                            dbg["selected_ref"] = f"gsheet:HistoryFull:{aid}"
                            dbg["selected_timestamp"] = sel_ts
                            dbg["selected_code_version"] = str(full.get("code_version") or meta.get("code_version") or "")
                            dbg["reason"] = "max_timestamp_missing_question_with_values"
                            dbg["selected_baseline_sources_urls"] = _refactor111_extract_bsc_urls_v1(full)
                            try:
                                dbg.setdefault("sheet_probe", {})
                                if isinstance(dbg.get("sheet_probe"), dict):
                                    dbg["sheet_probe"]["match_mode"] = "fallback_missing_question"
                            except Exception:
                                pass

                            full["_refactor111_prev_snapshot_is_latest_v1"] = True
                            full["_refactor111_prev_snapshot_ref_v1"] = dbg["selected_ref"]
                            full["_refactor111_prev_snapshot_pick_v1"] = dbg
                            return full, dbg
                        except Exception:
                            pass
                    # REFACTOR186: if sheet selection didn't yield a valid match, keep provided previous_data when it matches the question and contains values
                    try:
                        if prev_ok and isinstance(prev, dict) and prev:
                            dbg["selected_ref"] = str(prev.get("full_store_ref") or "previous_data")
                            dbg["selected_timestamp"] = str(prev.get("timestamp") or "")
                            dbg["selected_code_version"] = str(prev.get("code_version") or "")
                            dbg["reason"] = dbg.get("reason") or "fallback_previous_data_no_sheet_match"
                            dbg["selected_baseline_sources_urls"] = _refactor111_extract_bsc_urls_v1(prev)
                            prev["_refactor111_prev_snapshot_is_latest_v1"] = True
                            prev["_refactor111_prev_snapshot_ref_v1"] = dbg["selected_ref"]
                            prev["_refactor111_prev_snapshot_pick_v1"] = dbg
                            return prev, dbg
                    except Exception:
                        pass


                    # If we saw candidates but did not match question/schema, still emit max timestamp seen
                    try:
                        if cands:
                            top_ts = cands[0][1].get("created_at") or ""
                            dbg["sheet_probe"]["historyfull_max_timestamp_seen"] = str(top_ts)
                    except Exception:
                        pass
    except Exception:
        pass

    # -------- Attempt B: local history by mtime --------
    try:
        dbg["origin_attempted"] = dbg["origin_attempted"] or "local_history"
        search_dirs = []
        try:
            search_dirs.append(os.getcwd())
        except Exception:
            pass
        try:
            search_dirs.append(os.path.dirname(__file__))
        except Exception:
            pass
        for extra in ("history", "runs", "outputs"):
            for base in list(search_dirs):
                try:
                    p = os.path.join(base, extra)
                    if os.path.isdir(p):
                        search_dirs.append(p)
                except Exception:
                    pass

        files = []
        for d in list(dict.fromkeys([x for x in search_dirs if isinstance(x, str) and x])):
            try:
                for fn in os.listdir(d):
                    if not (fn.startswith("yureeka_") and fn.endswith(".json")):
                        continue
                    if "evolution" in fn:
                        continue
                    fp = os.path.join(d, fn)
                    try:
                        mt = os.path.getmtime(fp)
                    except Exception:
                        mt = 0.0
                    files.append((mt, fp))
            except Exception:
                pass

        files.sort(key=lambda t: float(t[0] or 0.0), reverse=True)

        for mt, fp in files[:50]:
            dbg["candidates_considered"].append({"ref": f"local:{fp}", "timestamp": "", "code_version": ""})

            try:
                raw = open(fp, "r", encoding="utf-8").read()
                obj = json.loads(raw)
            except Exception:
                continue
            if not isinstance(obj, dict):
                continue

            qn = _refactor111_norm_question_v1(_refactor111_get_question_from_payload_v1(obj))
            if target_q and qn != target_q:
                continue
            pmc = _refactor111_get_pmc_v1(obj)
            if not (isinstance(pmc, dict) and pmc):
                continue

            dbg["selected_ref"] = f"local:{fp}"
            dbg["selected_timestamp"] = str(obj.get("timestamp") or "")
            dbg["selected_code_version"] = str(obj.get("code_version") or "")
            dbg["reason"] = "fallback"
            dbg["selected_baseline_sources_urls"] = _refactor111_extract_bsc_urls_v1(obj)

            obj["_refactor111_prev_snapshot_is_latest_v1"] = False
            obj["_refactor111_prev_snapshot_ref_v1"] = dbg["selected_ref"]
            obj["_refactor111_prev_snapshot_pick_v1"] = dbg
            return obj, dbg
    except Exception:
        pass

    # -------- Attempt C: snapshot_ref / previous_data --------
    try:
        dbg["origin_attempted"] = dbg["origin_attempted"] or "snapshot_ref"
        dbg["selected_ref"] = str(prev.get("full_store_ref") or prev.get("snapshot_store_ref") or "")
        dbg["selected_timestamp"] = str(prev.get("timestamp") or "")
        dbg["selected_code_version"] = str(prev.get("code_version") or "")
        dbg["selected_baseline_sources_urls"] = _refactor111_extract_bsc_urls_v1(prev)
        prev["_refactor111_prev_snapshot_is_latest_v1"] = False
        prev["_refactor111_prev_snapshot_ref_v1"] = dbg["selected_ref"]
        prev["_refactor111_prev_snapshot_pick_v1"] = dbg
    except Exception:
        pass

    return prev, dbg

def attach_source_snapshots_to_analysis(analysis: dict, web_context: dict) -> dict:
    """
    Attach stable source snapshots (from web_context.scraped_meta) into analysis.

    Enhancements (v7_34 patch):
    - Ensures scraped_meta.extracted_numbers is always list-like
    - Adds RANGE capture per canonical metric using admitted snapshots:
        primary_metrics_canonical[ckey]["value_range"] = {min,max,n,examples}
      This restores earlier "range vs point estimate" behavior in a compatible way.

    NOTE (REFACTOR136): This is a real docstring (first statement) to prevent Streamlit's
    expression "magic" from rendering patch notes into the app UI.
    """
    try:
        _q = str((analysis or {}).get('question') or '')
        _fix2d66_promote_injection_in_web_context(web_context, question=_q)
    except Exception:
        pass
    try:
        _qtxt = str((analysis or {}).get('question') or (analysis or {}).get('query') or '')
        web_context = _fix2d66_promote_injected_urls(web_context or {}, question_text=_qtxt, stage='analysis_attach')
    except Exception:
        pass

    def _fingerprint(text: str) -> str:
        try:
            fn = globals().get("fingerprint_text")
            if callable(fn):
                return fn(text)
        except Exception:
            pass
        try:
            t = re.sub(r"\s+", " ", (text or "").strip().lower())
            return hashlib.md5(t.encode("utf-8", errors="ignore")).hexdigest()[:12]
        except Exception:
            return ""

    # - Does NOT change existing behavior if anchor_hash already present.
    # - Ensures unit_tag/unit_family/base_unit/value_norm are present when possible.
    # - No behavior change if helper missing.
    _canon_fn = globals().get("canonicalize_numeric_candidate")
    def _maybe_canonicalize(n: dict) -> dict:
        try:
            if callable(_canon_fn):
                return _canon_fn(dict(n))
        except Exception:
            return dict(n)

    def _parse_num(value, unit_hint=""):
        try:
            if value is None:
                return None
            s = str(value).strip()
            if not s:
                return None
            s = s.replace('$', '').replace(',', '').strip()
            if s.startswith('(') and s.endswith(')'):
                s = '-' + s[1:-1].strip()
            v = None
            try:
                v = float(s)
            except Exception:
                m = re.findall(r"-?\d+(?:\.\d+)?", s)
                if not m:
                    return None
                v = float(m[0])
            u = normalize_unit(unit_hint)
            # Normalize magnitudes into BILLIONS for currency-like magnitudes.
            if u == 'T':
                return v * 1000.0
            if u == 'B':
                return v
            if u == 'M':
                return v / 1000.0
            if u == 'K':
                return v / 1_000_000.0
            # Percent: keep as percent number.
            if u == '%':
                return v
            return v
        except Exception:
            return None

    def _unit_family_from_metric(mdef: dict) -> str:
        # prefer metric schema
        uf = (mdef or {}).get("unit_family") or ""
        uf = str(uf).lower().strip()
        if uf in ("percent", "pct"):
            return "PCT"
        if uf in ("currency",):
            return "CUR"
        if uf in ("magnitude", "unit_sales", "other"):
            return "MAG"
        return "OTHER"

    def _cand_unit_family(cunit: str, craw: str) -> str:
        u = (cunit or "").strip()
        r = (craw or "")
        uu = u.upper()
        ru = r.upper()

        # Percent
        if uu == "%" or "%" in ru:
            return "PCT"

        # Energy
        if any(x in (u or "").lower() for x in ["twh", "gwh", "mwh", "kwh"]) or any(x in (r or "").lower() for x in ["twh", "gwh", "mwh", "kwh"]):
            return "ENERGY"

        # Currency (symbol/code presence)
        #if any(x in ru for x in ["$", "USD", "SGD", "EUR", "GBP", "S$"]) or uu in ("USD", "SGD", "EUR", "GBP"):
        #    return "CUR"

        if re.search(r"(\$|S\$|€|£)\s*\d", r) or any(x in ru for x in ["USD", "SGD", "EUR", "GBP"]) or uu in ("USD","SGD","EUR","GBP"):
            return "CUR"

        # Magnitude (case-insensitive)
        if uu in ("K", "M", "B", "T") or (u or "").lower() in ("k", "m", "b", "t"):
            return "MAG"

        return "OTHER"

    def _tokenize(s: str):
        return [t for t in re.findall(r"[a-z0-9]+", (s or "").lower()) if len(t) > 2]

    def _safe_norm_unit_tag(x: str) -> str:
        try:
            fn = globals().get("normalize_unit_tag")
            if callable(fn):
                return fn(x or "")
        except Exception:
            return (x or "").strip()

    # Build baseline_sources_cache from scraped_meta (snapshot-friendly)
    baseline_sources_cache = []
    scraped_meta = (web_context or {}).get("scraped_meta") or {}
    if isinstance(scraped_meta, dict):
        for url, meta in scraped_meta.items():
            if not isinstance(meta, dict):
                continue
            nums = meta.get("extracted_numbers") or []
            if nums is None or not isinstance(nums, list):
                nums = []

            content = meta.get("content") or meta.get("clean_text") or (web_context.get("scraped_content", {}) or {}).get(url, "") or ""

            _sd = meta.get("status_detail") or meta.get("status") or ""
            _status = "fetched" if str(_sd).startswith("success") or meta.get("status") == "fetched" else "failed"
            if str(_sd).startswith("skipped:"):
                _status = "skipped"
                try:
                    _sd = str(_sd)[len("skipped:"):] or "pdf_unsupported_missing_dependency"
                except Exception:
                    _sd = "pdf_unsupported_missing_dependency"

            # FRESH01: compute source freshness (published_at + age buckets) from existing content
            try:
                _fa = meta.get("fetched_at") or _yureeka_now_iso_v1()
                meta["fetched_at"] = _fa
                _fresh = _fresh01_compute_source_freshness_v1(content, fetched_at=_fa)
                if isinstance(_fresh, dict):
                    for _k in ("published_at", "freshness_age_days", "freshness_bucket", "freshness_method", "freshness_date_confidence", "freshness_score", "freshness_score_method"):
                        if _k in _fresh:
                            meta[_k] = _fresh.get(_k)
            except Exception:
                pass

            baseline_sources_cache.append({
                "url": url,
                "status": _status,
                "status_detail": _sd,
                "numbers_found": int(meta.get("numbers_found") or (len(nums) if isinstance(nums, list) else 0)),
                "fetched_at": meta.get("fetched_at") or _yureeka_now_iso_v1(),
                "fingerprint": meta.get("fingerprint") or _fingerprint(content),

                # FRESH01: deterministic published/updated date extraction (diagnostic-only)
                "published_at": meta.get("published_at") or "",
                "freshness_age_days": meta.get("freshness_age_days"),
                "freshness_bucket": meta.get("freshness_bucket") or "",
                "freshness_method": meta.get("freshness_method") or "none",
                "freshness_date_confidence": meta.get("freshness_date_confidence"),
                "freshness_score": meta.get("freshness_score"),
                "freshness_score_method": meta.get("freshness_score_method") or "none",

                # REFACTOR121: store a lightweight excerpt for year-anchor backstop (avoid huge payloads)
                "snapshot_text_excerpt": (content[:12000] if isinstance(content, str) else ""),

                # - This is critical for:
                #   * range gating (metric-aware)
                #   * schema-first attribution
                #   * evolution rebuild (anchor_hash + value_norm + unit_family)
                # - Backward compatible: only adds keys; existing keys unchanged.
                "extracted_numbers": [
                    (lambda nn: {
                        "value": nn.get("value"),
                        "unit": nn.get("unit"),
                        "raw": nn.get("raw"),
                        "context_snippet": (nn.get("context_snippet") or nn.get("context") or "")[:240],

                        # keep existing anchor_hash if present; else stable fallback
                        "anchor_hash": (
                            nn.get("anchor_hash")
                            or _yureeka_sha1_v1(
                                f"{url}|{str(nn.get('raw') or '')}|{(nn.get('context_snippet') or nn.get('context') or '')[:240]}"
                            )
                        ),

                        "source_url": nn.get("source_url") or url,

                        "is_junk": nn.get("is_junk"),
                        "junk_reason": nn.get("junk_reason"),
                        "start_idx": nn.get("start_idx"),
                        "end_idx": nn.get("end_idx"),

                        "unit_tag": nn.get("unit_tag"),
                        "unit_family": nn.get("unit_family"),
                        "base_unit": nn.get("base_unit"),
                        "multiplier_to_base": nn.get("multiplier_to_base"),
                        "value_norm": nn.get("value_norm"),

                        "measure_kind": nn.get("measure_kind"),
                        "measure_assoc": nn.get("measure_assoc"),
                    })(_maybe_canonicalize(n))
                    for n in nums
                    if isinstance(n, dict)
                ]
            })

    if baseline_sources_cache:

        for s in (baseline_sources_cache or []):
            if isinstance(s, dict) and isinstance(s.get("extracted_numbers"), list):

                try:
                    if "sort_snapshot_numbers" in globals() and callable(globals()["sort_snapshot_numbers"]):
                        s["extracted_numbers"] = sort_snapshot_numbers(s["extracted_numbers"])
                    else:
                        # safe fallback: anchor_hash then raw
                        s["extracted_numbers"] = sorted(
                            s["extracted_numbers"],
                            key=lambda x: (str((x or {}).get("anchor_hash") or ""), str((x or {}).get("raw") or ""))
                        )
                except Exception:
                    pass

                s["numbers_found"] = len(s["extracted_numbers"])

        baseline_sources_cache = sorted(
            baseline_sources_cache,
            key=lambda x: str((x or {}).get("url") or "")
        )

        # - Adds *synthetic* url-only source records for injected URLs that were
        #   persisted (per diag) but are missing from baseline_sources_cache.
        # - Default OFF; only activates when INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH is enabled.
        # - Does NOT alter fastpath logic or metric selection (synthetic has no numbers).
        _inj_hash_added = []
        _inj_hash_reasons = {}
        try:
            _diag_local = {}
            if isinstance(web_context, dict):
                _diag_local = web_context.get("diag_injected_urls") or web_context.get("extra_urls_debug") or {}
            _persisted_for_hash = []
            if isinstance(_diag_local, dict):
                _persisted_for_hash = _inj_diag_norm_url_list(
                    _diag_local.get("persisted_norm") or _diag_local.get("persisted") or []
                )
            _incl_inj_hash = _inj_hash_policy_should_include(_persisted_for_hash)
            if _incl_inj_hash and _persisted_for_hash:
                _bsc_aug, _inj_hash_added, _inj_hash_reasons = _inj_hash_add_synthetic_sources(
                    baseline_sources_cache,
                    _persisted_for_hash,
                    now_iso=_yureeka_now_iso_v1(),
                )
                baseline_sources_cache = _bsc_aug
        except Exception:
            pass
            _inj_hash_added = []
            _inj_hash_reasons = {}

        # REFACTOR123: Ensure Analysis baseline has the same schema-seeded source pool as Evolution,
        # so previous_value is non-null and diffing can compute deltas.
        try:
            _is_evolution_payload = bool(isinstance(analysis, dict) and analysis.get("analysis_type"))
            if not _is_evolution_payload:
                _rf115_inj_urls = []
                try:
                    _rf115_inj_urls = _refactor115_collect_injection_urls_v1({}, web_context or {})
                except Exception:
                    _rf115_inj_urls = []
                _rf115_injection_present = bool(_rf115_inj_urls)

                _rf115_added = []
                _rf115_extract_diag = []
                _rf115_year_tokens_union = ["2025", "2026", "2040"]

                if isinstance(baseline_sources_cache, list):
                    # normalize existing URLs
                    _existing = set()
                    for _r in (baseline_sources_cache or []):
                        if isinstance(_r, dict):
                            _u = _r.get("url") or _r.get("source_url") or ""
                            _n = ""
                            try:
                                _n = _inj_diag_norm_url_list([_u])[0] if _u else ""
                            except Exception:
                                _n = str(_u or "").strip()
                            if _n:
                                _existing.add(_n)

                    if (not _rf115_injection_present):
                        for _u in (_REFACTOR115_SCHEMA_SEED_URLS_V1 or []):
                            try:
                                _u_norm = _inj_diag_norm_url_list([_u])[0]
                            except Exception:
                                _u_norm = str(_u or "").strip()
                            if not _u_norm or _u_norm in _existing:
                                continue

                            _txt, _detail = None, ""
                            try:
                                _txt, _detail = fetch_url_content_with_status(_u_norm, timeout=25)
                            except Exception as _e:
                                _txt, _detail = None, f"exception:{type(_e).__name__}"

                            # Normalize status for skipped PDF dependency
                            _status = "fetched" if str(_detail).startswith("success") else "failed"
                            _sd = str(_detail or "")
                            if str(_sd).startswith("skipped:"):
                                _status = "skipped"
                                try:
                                    _sd = _sd[len("skipped:"):] or "pdf_unsupported_missing_dependency"
                                except Exception:
                                    _sd = "pdf_unsupported_missing_dependency"

                            if isinstance(_txt, str) and len(_txt.strip()) >= 200:
                                _rf115_extracted_numbers = []
                                _rf115_numbers_found = 0
                                _rf115_fp = ""
                                _rf115_year_hits = {}
                                _rf115_extract_error = ""
                                try:
                                    _rf115_extracted_numbers = extract_numbers_with_context(_txt, source_url=_u_norm, max_results=600) or []
                                except Exception as _ee:
                                    _rf115_extracted_numbers = []
                                    try:
                                        _rf115_extract_error = f"{type(_ee).__name__}:{_ee}"
                                    except Exception:
                                        _rf115_extract_error = "exception"
                                try:
                                    _rf115_numbers_found = sum(
                                        1 for _n in (_rf115_extracted_numbers or [])
                                        if isinstance(_n, dict) and (not bool(_n.get("is_junk")))
                                    )
                                except Exception:
                                    try:
                                        _rf115_numbers_found = int(len(_rf115_extracted_numbers or [])) if isinstance(_rf115_extracted_numbers, list) else 0
                                    except Exception:
                                        _rf115_numbers_found = 0
                                try:
                                    _rf115_fp = fingerprint_text(_txt)
                                except Exception:
                                    _rf115_fp = ""
                                try:
                                    for _yy in (_rf115_year_tokens_union or []):
                                        if isinstance(_yy, str) and _yy:
                                            _rf115_year_hits[_yy] = int(str(_txt).count(_yy))
                                except Exception:
                                    _rf115_year_hits = {}

                                try:
                                    _rf115_extract_diag.append({
                                        "url": _u_norm,
                                        "numbers_found": int(_rf115_numbers_found or 0),
                                        "status": _status,
                                        "status_detail": _sd,
                                        "year_token_hits": _rf115_year_hits,
                                        "extract_error": _rf115_extract_error,
                                    })
                                except Exception:
                                    pass

                                # FRESH01: compute freshness for schema-seeded sources (diagnostic-only)
                                try:
                                    _fa_seed = _yureeka_now_iso_v1()
                                    _fresh_seed = _fresh01_compute_source_freshness_v1(_txt or "", fetched_at=_fa_seed)
                                except Exception:
                                    _fa_seed = _yureeka_now_iso_v1()
                                    _fresh_seed = {}

                                baseline_sources_cache.append({
                                    "source_url": _u_norm,
                                    "url": _u_norm,
                                    "status": _status,
                                    "status_detail": _sd,
                                    "snapshot_text": _txt,
                                    "snapshot_text_excerpt": (_txt[:12000] if isinstance(_txt, str) else ""),
                                    "fingerprint": _rf115_fp,
                                    "extracted_numbers": _rf115_extracted_numbers,
                                    "numbers_found": _rf115_numbers_found,
                                    "seeded": True,
                                    "seeded_reason": "schema_seeds",
                                    "fetched_at": _fa_seed,
                                    "published_at": (_fresh_seed.get("published_at") if isinstance(_fresh_seed, dict) else ""),
                                    "freshness_age_days": (_fresh_seed.get("freshness_age_days") if isinstance(_fresh_seed, dict) else None),
                                    "freshness_bucket": (_fresh_seed.get("freshness_bucket") if isinstance(_fresh_seed, dict) else ""),
                                    "freshness_method": (_fresh_seed.get("freshness_method") if isinstance(_fresh_seed, dict) else "none"),
                                })
                                _existing.add(_u_norm)
                                _rf115_added.append(_u_norm)
                            else:
                                try:
                                    _rf115_extract_diag.append({
                                        "url": _u_norm,
                                        "numbers_found": 0,
                                        "status": ("seeded_pending" if (not _status == "skipped") else "skipped"),
                                        "status_detail": _sd or "seeded_placeholder",
                                        "year_token_hits": {},
                                        "extract_error": "missing_text",
                                    })
                                except Exception:
                                    pass
                                # FRESH01: compute freshness for schema-seeded placeholder sources (diagnostic-only)
                                try:
                                    _fa_seed2 = _yureeka_now_iso_v1()
                                    _fresh_seed2 = _fresh01_compute_source_freshness_v1("", fetched_at=_fa_seed2)
                                except Exception:
                                    _fa_seed2 = _yureeka_now_iso_v1()
                                    _fresh_seed2 = {}

                                baseline_sources_cache.append({
                                    "source_url": _u_norm,
                                    "url": _u_norm,
                                    "status": "seeded_pending" if (not _status == "skipped") else "skipped",
                                    "status_detail": _sd or "seeded_placeholder",
                                    "snapshot_text": "",
                                    "snapshot_text_excerpt": "",
                                    "fingerprint": "",
                                    "extracted_numbers": [],
                                    "numbers_found": 0,
                                    "seeded": True,
                                    "seeded_reason": "schema_seeds_placeholder",
                                    "fetched_at": _fa_seed2,
                                    "published_at": (_fresh_seed2.get("published_at") if isinstance(_fresh_seed2, dict) else ""),
                                    "freshness_age_days": (_fresh_seed2.get("freshness_age_days") if isinstance(_fresh_seed2, dict) else None),
                                    "freshness_bucket": (_fresh_seed2.get("freshness_bucket") if isinstance(_fresh_seed2, dict) else ""),
                                    "freshness_method": (_fresh_seed2.get("freshness_method") if isinstance(_fresh_seed2, dict) else "none"),
                                    "freshness_date_confidence": (_fresh_seed2.get("freshness_date_confidence") if isinstance(_fresh_seed2, dict) else None),
                                    "freshness_score": (_fresh_seed2.get("freshness_score") if isinstance(_fresh_seed2, dict) else None),
                                    "freshness_score_method": (_fresh_seed2.get("freshness_score_method") if isinstance(_fresh_seed2, dict) else "none"),
                                })
                                _existing.add(_u_norm)
                                _rf115_added.append(_u_norm)

                # Emit debug beacons on analysis payload (additive)
                try:
                    analysis.setdefault("debug", {})
                    if isinstance(analysis.get("debug"), dict):
                        analysis["debug"]["schema_seed_sources_v1"] = {
                            "stage": "analysis_attach",
                            "seeds_added": int(len(_rf115_added)),
                            "added_urls": list(_rf115_added)[:10],
                            "injection_present": bool(_rf115_injection_present),
                            "reason": "schema_seeds",
                        }

                        _nonzero = 0
                        _errs = 0
                        _sample = []
                        _year_hits = {}
                        if isinstance(_rf115_extract_diag, list):
                            try:
                                _nonzero = sum(1 for _d in _rf115_extract_diag if isinstance(_d, dict) and int(_d.get("numbers_found") or 0) > 0)
                            except Exception:
                                _nonzero = 0
                            try:
                                _errs = sum(1 for _d in _rf115_extract_diag if isinstance(_d, dict) and str(_d.get("extract_error") or "").strip())
                            except Exception:
                                _errs = 0
                            for _d in (_rf115_extract_diag or [])[:12]:
                                if isinstance(_d, dict):
                                    _sample.append({"url": _d.get("url"), "numbers_found": _d.get("numbers_found"), "status": _d.get("status")})
                                    try:
                                        yh = _d.get("year_token_hits") or {}
                                        if isinstance(yh, dict):
                                            for k, v in yh.items():
                                                _year_hits[k] = int(_year_hits.get(k, 0)) + int(v or 0)
                                    except Exception:
                                        pass

                        analysis["debug"]["schema_seed_extract_v1"] = {
                            "stage": "analysis_attach",
                            "seeds_added": int(len(_rf115_added)),
                            "seeds_extracted_nonzero": int(_nonzero),
                            "extract_errors": int(_errs),
                            "per_seed_numbers_found_sample": _sample,
                            "per_seed_year_token_hits": _year_hits,
                        }
                except Exception:
                    pass

                # If baseline schema values are null, run schema-only rebuild using the now-seeded sources cache.
                try:
                    pmc = (analysis or {}).get("primary_metrics_canonical")
                    if (not pmc) and isinstance((analysis or {}).get("primary_response"), dict):
                        try:
                            pmc = ((analysis or {}).get("primary_response") or {}).get("primary_metrics_canonical")
                        except Exception:
                            pmc = pmc
                    _all_null = True
                    if isinstance(pmc, dict) and pmc:
                        for _k, _mv in pmc.items():
                            if isinstance(_mv, dict) and (_mv.get("value") is not None):
                                _all_null = False
                                break
                    if _all_null and callable(globals().get("rebuild_metrics_from_snapshots_schema_only")):
                        _rebuilt = rebuild_metrics_from_snapshots_schema_only(analysis, baseline_sources_cache, web_context=web_context)
                        if isinstance(_rebuilt, dict):
                            # REFACTOR124: preserve wrapper fields when applying schema-only rebuild
                            try:
                                _orig_wrapper = analysis if isinstance(analysis, dict) else {}
                                _rebuilt_pmc = None
                                _rebuilt_schema = None
                                try:
                                    _rebuilt_pmc = (_rebuilt or {}).get("primary_metrics_canonical") or (((_rebuilt or {}).get("primary_response") or {}).get("primary_metrics_canonical") if isinstance((_rebuilt or {}).get("primary_response"), dict) else None)
                                except Exception:
                                    _rebuilt_pmc = None
                                try:
                                    _rebuilt_schema = (_rebuilt or {}).get("metric_schema_frozen") or (((_rebuilt or {}).get("primary_response") or {}).get("metric_schema_frozen") if isinstance((_rebuilt or {}).get("primary_response"), dict) else None)
                                except Exception:
                                    _rebuilt_schema = None

                                if isinstance(_rebuilt_pmc, dict) and _rebuilt_pmc:
                                    _orig_wrapper["primary_metrics_canonical"] = _rebuilt_pmc
                                    _orig_wrapper.setdefault("primary_response", {})
                                    if isinstance(_orig_wrapper.get("primary_response"), dict):
                                        _orig_wrapper["primary_response"]["primary_metrics_canonical"] = _rebuilt_pmc
                                if isinstance(_rebuilt_schema, dict) and _rebuilt_schema:
                                    _orig_wrapper["metric_schema_frozen"] = _rebuilt_schema
                                    _orig_wrapper.setdefault("primary_response", {})
                                    if isinstance(_orig_wrapper.get("primary_response"), dict):
                                        _orig_wrapper["primary_response"]["metric_schema_frozen"] = _rebuilt_schema

                                analysis = _orig_wrapper
                            except Exception:
                                # worst-case fallback: keep rebuilt object
                                analysis = _rebuilt

                            try:
                                analysis.setdefault("debug", {})
                                if isinstance(analysis.get("debug"), dict):
                                    analysis["debug"]["refactor123_analysis_rebuild_v1"] = {
                                        "applied": True,
                                        "reason": "baseline_schema_null__rebuild_from_seeded_sources_cache",
                                        "sources_n": int(len(baseline_sources_cache)) if isinstance(baseline_sources_cache, list) else 0,
                                    }
                            except Exception:
                                pass
                except Exception:
                    pass
        except Exception:
            pass
        analysis["baseline_sources_cache"] = baseline_sources_cache
        # Also attach to web_context for downstream veracity scoring (additive).
        try:
            if isinstance(web_context, dict):
                web_context["baseline_sources_cache"] = baseline_sources_cache
        except Exception:
            pass
        # FRESH01: aggregate freshness beacon (diagnostic-only; does not affect selection/diff)
        try:
            _fresh_stage = "evolution_attach" if (("metric_changes" in analysis) or ("stability_score" in analysis)) else "analysis_attach"
            analysis.setdefault("debug", {})
            if isinstance(analysis.get("debug"), dict):

                # LLM35: ensure freshness aggregate reflects the same pool used by selection/provenance.
                _pool = baseline_sources_cache
                _pool_key = "baseline_sources_cache"
                try:
                    if str(_fresh_stage).startswith("evolution"):
                        for _k in ["baseline_sources_cache_current", "current_baseline_sources_cache"]:
                            _v = None
                            try:
                                _v = analysis.get(_k) if isinstance(analysis, dict) else None
                            except Exception:
                                _v = None
                            if not (isinstance(_v, list) and _v):
                                try:
                                    _v = web_context.get(_k) if isinstance(web_context, dict) else None
                                except Exception:
                                    _v = None
                            if isinstance(_v, list) and _v:
                                _pool = _v
                                _pool_key = _k
                                break
                except Exception:
                    pass

                try:
                    _fresh01_backfill_freshness_scores_v1(_pool)
                except Exception:
                    pass

                _f01 = _fresh01_aggregate_source_freshness_v1(_pool, stage=_fresh_stage)
                try:
                    if isinstance(_f01, dict):
                        _f01["pool_key"] = str(_pool_key or "")
                except Exception:
                    pass
                analysis["debug"]["fresh01_source_freshness_v1"] = _f01

                # LLM35-A: wire deterministic freshness score into veracity_scores.data_freshness (0-100)
                try:
                    _vs = analysis.get("veracity_scores") if isinstance(analysis, dict) else None
                    if isinstance(_vs, dict) and (_vs.get("data_freshness") in (None, "")):
                        _fs = None
                        try:
                            _fs = source_freshness_score_v1(None, {"baseline_sources_cache": baseline_sources_cache})
                        except Exception:
                            _fs = None
                        if _fs is not None:
                            _vs["data_freshness"] = round(float(_fs), 1)

                    # Best-effort mirror into nested primary_response.veracity_scores if present
                    try:
                        _pr = analysis.get("primary_response") if isinstance(analysis, dict) else None
                        if isinstance(_pr, dict):
                            _pvs = _pr.get("veracity_scores")
                            if isinstance(_pvs, dict) and (_pvs.get("data_freshness") in (None, "")):
                                if isinstance(_vs, dict) and (_vs.get("data_freshness") is not None):
                                    _pvs["data_freshness"] = _vs.get("data_freshness")
                    except Exception:
                        pass
                except Exception:
                    pass
                # FRESH02: attach freshness tie-breaker beacon (if any)
                try:
                    _tb = globals().get("_FRESH02_TIEBREAK_V1")
                    if isinstance(_tb, dict):
                        analysis["debug"]["fresh02_freshness_tiebreak_v1"] = _tb
                except Exception:
                    pass
                # LLM36: derive a run-level freshness tiebreak summary from per-metric beacons (robust; avoids globals()).
                try:
                    _pmc = None
                    try:
                        _pmc = analysis.get("primary_metrics_canonical") if isinstance(analysis, dict) else None
                    except Exception:
                        _pmc = None
                    if not (isinstance(_pmc, dict) and _pmc):
                        try:
                            _pr = analysis.get("primary_response") if isinstance(analysis, dict) else None
                            if isinstance(_pr, dict):
                                _pmc = _pr.get("primary_metrics_canonical")
                        except Exception:
                            _pmc = None
                    if isinstance(_pmc, dict) and _pmc:
                        _used_n = 0
                        _chg_n = 0
                        _chg_keys = []
                        _ex = []
                        for _ck, _m in _pmc.items():
                            if not isinstance(_m, dict):
                                continue
                            _prov = _m.get("provenance")
                            if not isinstance(_prov, dict):
                                continue
                            _ft = _prov.get("fresh_tiebreak_v1")
                            if not isinstance(_ft, dict):
                                continue
                            if bool(_ft.get("used")):
                                _used_n += 1
                            if bool(_ft.get("changed_winner")):
                                _chg_n += 1
                                _chg_keys.append(str(_ck))
                                if len(_ex) < 6:
                                    _ex.append({
                                        "canonical_key": str(_ck),
                                        "winner_url": str(_ft.get("winner_url") or ""),
                                        "base_url": str(_ft.get("base_url") or ""),
                                        "competitor_url": str(_ft.get("competitor_url") or ""),
                                        "a_score": _ft.get("a_score"),
                                        "b_score": _ft.get("b_score"),
                                        "a_freshness": _ft.get("a_freshness"),
                                        "b_freshness": _ft.get("b_freshness"),
                                        "a_published_at": str(_ft.get("a_published_at") or ""),
                                        "b_published_at": str(_ft.get("b_published_at") or ""),
                                        "reason": str(_ft.get("reason") or ""),
                                    })
                        analysis["debug"]["fresh02_freshness_tiebreak_summary_v1"] = {
                            "used_count": int(_used_n),
                            "changed_winner_count": int(_chg_n),
                            "changed_winner_keys": list(_chg_keys[:80]),
                            "examples": list(_ex),
                        }
                except Exception:
                    pass


        except Exception:
            pass
        analysis.setdefault("results", {})
        if isinstance(analysis["results"], dict):

            # - Captures persisted snapshot URLs + exact hash input URL set (A4-A5)
            # - Does NOT alter any gating/selection logic.
            try:
                _diag = {}
                if isinstance(web_context, dict):
                    _diag = web_context.get("diag_injected_urls") or web_context.get("extra_urls_debug") or {}

                _inj_urls = []
                try:
                    _inj_urls = _fix2d66_collect_injected_urls(web_context or {}, question_text=str((analysis or {}).get('question') or ''))
                except Exception:
                    pass
                    _inj_urls = []

                _snap_urls = _inj_diag_hash_inputs_from_bsc(baseline_sources_cache)
                _hash_inputs = _snap_urls

                _h_v1 = ""
                _h_v2 = ""
                try:
                    _h_v1 = compute_source_snapshot_hash(baseline_sources_cache)
                except Exception:
                    pass
                    _h_v1 = ""
                try:
                    _h_v2 = compute_source_snapshot_hash_v2(baseline_sources_cache)
                except Exception:
                    pass
                    _h_v2 = ""

                analysis.setdefault("results", {})
                if isinstance(analysis.get("results"), dict):
                    analysis["results"].setdefault("debug", {})
                    if isinstance(analysis["results"].get("debug"), dict):
                        analysis["results"]["debug"].setdefault("inj_diag", {})
                        analysis["results"]["debug"]["inj_diag"].update({
                            "run_id": str((web_context or {}).get("diag_run_id") or _diag.get("run_id") or ""),
                            "injected_urls": _inj_urls[:50],
                            "snapshot_pool_urls_count": int(len(_snap_urls)),
                            "snapshot_pool_urls_hash": _inj_diag_set_hash(_snap_urls),
                            "hash_input_urls_count": int(len(_hash_inputs)),
                            "hash_input_urls_hash": _inj_diag_set_hash(_hash_inputs),
                            "injected_in_snapshot_pool": sorted(list(set(_inj_urls) & set(_snap_urls)))[:50],
                            "injected_in_hash_inputs": sorted(list(set(_inj_urls) & set(_hash_inputs)))[:50],
                            "computed_hash_v1": _h_v1,
                            "computed_hash_v2": _h_v2,
                        })

                        # Location: analysis.results.debug.inj_trace_v1
                        try:

                            # Ensure inj_trace_v1 shows attempted/persisted evidence even when
                            # upstream diag_injected_urls is partial (e.g., baseline/no-injection).
                            # Diagnostics only.
                            try:
                                if isinstance(_diag, dict):
                                    _diag = _inj_trace_v1_enrich_diag_from_bsc(_diag, baseline_sources_cache)
                            except Exception:
                                pass

                            _trace = _inj_trace_v1_build(
                                diag_injected_urls=_diag if isinstance(_diag, dict) else {},
                                hash_inputs=_hash_inputs,
                                stage="analysis",
                                path="analysis",
                                rebuild_pool=None,
                                rebuild_selected=None,
                                hash_exclusion_reasons=(_inj_hash_reasons if isinstance(locals().get('_inj_hash_reasons'), dict) else {}),
                            )
                            analysis["results"]["debug"].setdefault("inj_trace_v1", {})
                            # Do not overwrite if already present; only fill/merge
                            if isinstance(analysis["results"]["debug"].get("inj_trace_v1"), dict):
                                analysis["results"]["debug"]["inj_trace_v1"].update(_trace)
                        except Exception:
                            pass

            except Exception:
                pass

        analysis["results"]["baseline_sources_cache"] = baseline_sources_cache

    # RANGE capture for canonical metrics
    pmc = analysis.get("primary_response", {}).get("primary_metrics_canonical") if isinstance(analysis.get("primary_response"), dict) else analysis.get("primary_metrics_canonical")
    schema = analysis.get("primary_response", {}).get("metric_schema_frozen") if isinstance(analysis.get("primary_response"), dict) else analysis.get("metric_schema_frozen")

    # Support both placements (your JSON seems to store these at top-level primary_response)
    if pmc is None and isinstance(analysis.get("primary_response"), dict):
        pmc = analysis["primary_response"].get("primary_metrics_canonical")
    if schema is None and isinstance(analysis.get("primary_response"), dict):
        schema = analysis["primary_response"].get("metric_schema_frozen")

    if isinstance(pmc, dict) and isinstance(schema, dict) and baseline_sources_cache:
        # flatten candidates
        all_cands = []
        for sr in baseline_sources_cache:
            for n in (sr.get("extracted_numbers") or []):
                if isinstance(n, dict):
                    all_cands.append(n)

        for ckey, m in pmc.items():
            if not isinstance(m, dict):
                continue
            mdef = schema.get(ckey) or {}
            uf = _unit_family_from_metric(mdef)
            keywords = mdef.get("keywords") or []

            kw_tokens = []
            for k in (keywords or []):
                kw_tokens.extend(_tokenize(str(k)))

            kw_tokens.extend(_tokenize(m.get("name") or m.get("original_name") or ""))
            kw_tokens = list(dict.fromkeys([t for t in kw_tokens if len(t) > 2]))[:40]

            vals = []
            examples = []

            for cand in all_cands:
                craw = str(cand.get("raw") or "")
                cunit = str(cand.get("unit") or "")
                ctx = str(cand.get("context_snippet") or cand.get("context") or "")

                # family gate
                cf = _cand_unit_family(cunit, craw)
                if uf == "PCT" and cf != "PCT":
                    continue
                if uf == "CUR" and cf != "CUR":
                    continue
                # MAG: allow MAG/OTHER but avoid CUR/PCT
                if uf == "MAG" and cf in ("CUR", "PCT"):
                    continue

                # NEW (additive): metric-aware magnitude gate
                if uf == "MAG":

                    cand_tag = _safe_norm_unit_tag(cunit or craw)
                    exp_tag = _safe_norm_unit_tag((mdef.get("unit") or "") or (m.get("unit") or ""))

                    if exp_tag in ("K", "M", "B", "T"):
                        if cand_tag != exp_tag:
                            continue
                    else:
                        if cand_tag not in ("K", "M", "B", "T"):
                            continue

                # token overlap gate
                c_tokens = set(_tokenize(ctx))
                if kw_tokens:
                    overlap = sum(1 for t in kw_tokens if t in c_tokens)
                    if overlap < max(1, min(3, len(kw_tokens) // 8)):
                        continue

                v = _parse_num(cand.get("value"), cunit) or _parse_num(craw, cunit)
                if v is None:
                    continue

                vals.append(float(v))
                if len(examples) < 5:
                    examples.append({
                        "raw": craw[:32],
                        "source_url": cand.get("source_url"),
                        "context_snippet": ctx[:180]
                    })

            if len(vals) >= 2:
                vmin = min(vals)
                vmax = max(vals)
                if abs(vmax - vmin) > max(1e-9, abs(vmin) * 0.02):
                    m["value_range"] = {
                        "min": vmin,
                        "max": vmax,
                        "n": len(vals),
                        "examples": examples,
                        "method": "snapshot_candidates"
                    }
                    try:
                        unit_disp = m.get("unit") or ""
                        m["value_range_display"] = f"{vmin:g}–{vmax:g} {unit_disp}".strip()
                    except Exception:
                        pass
    try:
        _res = analysis.get("results") if isinstance(analysis, dict) else None
        if isinstance(_res, dict):
            for _ck, _m in _res.items():
                if not isinstance(_m, dict):
                    continue
                _ev = _m.get("evidence")
                if not isinstance(_ev, list) or len(_ev) < 2:
                    continue
                _vals = []
                for _e in _ev:
                    if not isinstance(_e, dict):
                        continue
                    _v = _e.get("value_norm")
                    if _v is None:
                        _v = _e.get("value")
                    if isinstance(_v, (int, float)):
                        try:
                            _vals.append(float(_v))
                        except Exception:
                            pass
                if len(_vals) < 2:
                    continue
                _vmin = min(_vals); _vmax = max(_vals)
                if abs(_vmax - _vmin) <= max(1e-9, abs(_vmin) * 0.02):
                    continue
                _m["value_range"] = {"min": _vmin, "max": _vmax, "n": len(_vals), "method": "ph2b_schema_unit_range_v1|fix2b_range2"}
                try:
                    _unit_disp = _m.get("unit") or _m.get("unit_tag") or ""
                    _m["value_range_display"] = f"{_vmin:g}–{_vmax:g} {_unit_disp}".strip()
                except Exception:
                    pass
    except Exception:
        pass

    #
    # Why:
    # - We observed value_range values scaled as if converted to billions (e.g., 17.8M -> 0.0178)
    #   while still displaying "million units", causing downstream eligibility drift.
    # - FIX2B_RANGE2 only patches analysis["results"] (often empty); the dashboard-facing
    #   canonicals live under analysis["primary_response"]["primary_metrics_canonical"].
    #
    # What:
    # - Rebuild value_range from baseline_sources_cache extracted_numbers, treating candidate.value_norm
    #   as schema units (NO double scaling), constrained to the metric's chosen source_url.
    # - Pure post-processing: NO IO, NO refetch, NO hashing changes.
    try:
        _pr = analysis.get("primary_response") if isinstance(analysis, dict) else None
        _pmc = _pr.get("primary_metrics_canonical") if isinstance(_pr, dict) else None
        _schema = (
            (analysis.get("metric_schema_frozen") if isinstance(analysis, dict) else None)
            or (_pr.get("metric_schema_frozen") if isinstance(_pr, dict) else None)
            or {}
        )
        _bsc = analysis.get("baseline_sources_cache") if isinstance(analysis, dict) else None
        if isinstance(_pmc, dict) and isinstance(_schema, dict) and isinstance(_bsc, list) and _bsc:
            # Flatten candidate universe from snapshots
            _flat = []
            for _src in _bsc:
                if not isinstance(_src, dict):
                    continue
                _nums = _src.get("extracted_numbers")
                if isinstance(_nums, list):
                    _flat.extend([n for n in _nums if isinstance(n, dict)])
            # Helper: scale evidence presence for common magnitudes
            def _ph2b_scale_token_ok(_spec_unit_tag: str, _cand: dict) -> bool:
                try:
                    sut = str(_spec_unit_tag or "").lower()
                    if not sut:
                        return True
                    # Only enforce for explicit scaled magnitudes
                    if ("million" not in sut) and ("billion" not in sut) and ("thousand" not in sut) and ("trillion" not in sut):
                        return True
                    raw = str(_cand.get("raw") or "").lower()
                    ut = str(_cand.get("unit_tag") or _cand.get("unit") or "").lower()
                    ctx = str(_cand.get("context_snippet") or "").lower()
                    blob = " ".join([raw, ut, ctx])
                    if "million" in sut:
                        return ("million" in blob) or re.search(r"\bm\b", blob) is not None or " mn" in blob
                    if "billion" in sut:
                        return ("billion" in blob) or re.search(r"\bb\b", blob) is not None or " bn" in blob
                    if "thousand" in sut:
                        return ("thousand" in blob) or re.search(r"\bk\b", blob) is not None
                    if "trillion" in sut:
                        return ("trillion" in blob) or re.search(r"\bt\b", blob) is not None
                except Exception:
                    return True

            for _ck, _m in list(_pmc.items()):
                if not isinstance(_m, dict):
                    continue
                _spec = _schema.get(_ck) if isinstance(_schema, dict) else None
                if not isinstance(_spec, dict):
                    continue
                _src_url = _m.get("source_url") or _spec.get("preferred_url") or ""
                if not _src_url:
                    continue
                _src_url_n = _ph2b_norm_url(_src_url)
                # Collect eligible vals from same source_url
                _vals = []
                _examples = []
                for _c in _flat:
                    try:
                        if _ph2b_norm_url(_c.get("source_url") or "") != _src_url_n:
                            continue
                        if _c.get("is_junk") is True:
                            continue
                        # Reuse FIX16 allowlist if present
                        try:
                            if callable(globals().get("_fix16_candidate_allowed")):
                                if not globals().get("_fix16_candidate_allowed")(_c, _spec, canonical_key=_ck):
                                    continue
                        except Exception:
                            pass
                        # Enforce scale token for scaled magnitudes
                        if not _ph2b_scale_token_ok(_spec.get("unit_tag") or _spec.get("unit") or "", _c):
                            continue
                        _v = _c.get("value_norm")
                        if _v is None:
                            _v = _c.get("value")
                        if isinstance(_v, (int, float)):
                            _vals.append(float(_v))
                            if len(_examples) < 4:
                                _examples.append({
                                    "raw": _c.get("raw"),
                                    "source_url": _c.get("source_url"),
                                    "context_snippet": (str(_c.get("context_snippet") or "")[:180])
                                })
                    except Exception:
                        pass
                        continue
                if len(_vals) < 2:
                    continue
                _vmin = min(_vals); _vmax = max(_vals)
                if abs(_vmax - _vmin) <= max(1e-9, abs(_vmin) * 0.02):
                    continue
                _m["value_range"] = {
                    "min": _vmin,
                    "max": _vmax,
                    "n": len(_vals),
                    "examples": _examples,
                    "method": "ph2b_schema_unit_range_v2|fix2b_range3"
                }
                try:
                    _unit_disp = _m.get("unit") or _m.get("unit_tag") or _spec.get("unit_tag") or ""
                    _m["value_range_display"] = f"{_vmin:g}–{_vmax:g} {_unit_disp}".strip()
                except Exception:
                    pass
    except Exception:
        pass

    #
    # Location:
    # - This runs AFTER the legacy "snapshot_candidates" range builder blocks above.
    #
    # Goal:
    # - Ensure value_range is computed in *schema units* (treat candidate.value_norm as schema units),
    #   avoiding any double-scaling/divide behavior.
    # - Constrain range computation to the metric's chosen current source_url when present,
    #   preventing cross-source range pollution.
    #
    # Notes:
    # - Downstream-only post-processing. Does NOT affect selection, only range/min/max display.
    # - Safe even when evidence is missing: it simply no-ops.
    try:
        _pr = analysis.get("primary_response") if isinstance(analysis, dict) else None
        _pmc = _pr.get("primary_metrics_canonical") if isinstance(_pr, dict) else None
        _schema = (
            (analysis.get("metric_schema_frozen") if isinstance(analysis, dict) else None)
            or (_pr.get("metric_schema_frozen") if isinstance(_pr, dict) else None)
            or {}
        )
        _bsc = analysis.get("baseline_sources_cache") if isinstance(analysis, dict) else None

        if isinstance(_pmc, dict) and isinstance(_schema, dict) and isinstance(_bsc, list) and _bsc:
            # Flatten candidates once
            _flat = []
            for _src in _bsc:
                if not isinstance(_src, dict):
                    continue
                _nums = _src.get("extracted_numbers")
                if isinstance(_nums, list):
                    _flat.extend([n for n in _nums if isinstance(n, dict)])

            def _ph2b_unit_tag_norm(x: str) -> str:
                return _safe_norm_unit_tag(str(x or ""))

            for _ckey, _m in _pmc.items():
                if not isinstance(_m, dict):
                    continue

                _mdef = _schema.get(_ckey) if isinstance(_schema, dict) else None
                _mdef = _mdef if isinstance(_mdef, dict) else {}
                _exp_unit = str(_mdef.get("unit") or _m.get("unit") or _m.get("unit_tag") or "")
                _exp_tag = _ph2b_unit_tag_norm(_exp_unit)

                # If the metric already has a source_url, treat that as the range scope
                _scope_url = _m.get("cur_source_url") or _m.get("source_url") or ""
                _scope_url_n = _norm_url(_scope_url) if _scope_url else ""

                _vals = []
                for _cand in _flat:
                    if not isinstance(_cand, dict):
                        continue
                    # source scope (if known)
                    if _scope_url_n:
                        _c_url = _cand.get("source_url") or ""
                        if _norm_url(_c_url) != _scope_url_n:
                            continue
                    # unit_tag scope (only enforce when schema declares a scaled magnitude tag)
                    if _exp_tag:
                        _cand_tag = _ph2b_unit_tag_norm(_cand.get("unit_tag") or _cand.get("unit") or _cand.get("raw") or "")
                        if _exp_tag in ("K", "M", "B", "T") and _cand_tag != _exp_tag:
                            continue
                    _vn = _cand.get("value_norm")
                    if _vn is None:
                        continue
                    if isinstance(_vn, (int, float)):
                        try:
                            _vals.append(float(_vn))
                        except Exception:
                            pass

                if len(_vals) >= 2:
                    _vmin = min(_vals); _vmax = max(_vals)
                    if abs(_vmax - _vmin) > max(1e-9, abs(_vmin) * 0.02):
                        _m["value_range"] = {
                            "min": _vmin,
                            "max": _vmax,
                            "n": len(_vals),
                            "method": "ph2b_schema_unit_range_v2|fix2b_range4",
                            "scope_url": _scope_url_n or "",
                            "scope_unit_tag": _exp_tag or ""
                        }
                        try:
                            _unit_disp = _m.get("unit") or _m.get("unit_tag") or _exp_unit or ""
                            _m["value_range_display"] = f"{_vmin:g}–{_vmax:g} {_unit_disp}".strip()
                        except Exception:
                            pass
    except Exception:
        pass

    # - attach_source_snapshots_to_analysis receives the *top-level* output dict,
    #   where schema/pmc live under primary_response.
    # - Prior FIX2D31/FIX2D38 baseline_schema_metrics_v1 builder reads analysis.*
    #   and therefore never ran, dropping baseline_schema_metrics_v1 from JSON.
    # - This shim mirrors metric_schema_frozen / primary_metrics_canonical / metric_anchors
    #   from primary_response into top-level analysis so the existing builder executes.
    # - Additive only: does not change selector semantics.
    try:
        _pr = analysis.get('primary_response')
        if isinstance(_pr, dict):
            if not isinstance(analysis.get('metric_schema_frozen'), dict) and isinstance(_pr.get('metric_schema_frozen'), dict):
                analysis['metric_schema_frozen'] = _pr.get('metric_schema_frozen')
            if not isinstance(analysis.get('primary_metrics_canonical'), dict) and isinstance(_pr.get('primary_metrics_canonical'), dict):
                analysis['primary_metrics_canonical'] = _pr.get('primary_metrics_canonical')
            if not isinstance(analysis.get('metric_anchors'), dict) and isinstance(_pr.get('metric_anchors'), dict):
                analysis['metric_anchors'] = _pr.get('metric_anchors')
    except Exception:
        pass

    #
    # Problem observed:
    # - Some Analysis runs (especially narrative questions) emit no metric_schema_frozen at all.
    # - Evolution schema-only rebuild depends on prev_response.metric_schema_frozen keyspace;
    #   when missing, rebuild returns empty and Diff Panel V2 shows an empty metric change table.
    #
    # Policy:
    # - Always ensure metric_schema_frozen exists at top-level analysis output.
    # - Seed deterministically using known schema extension functions when missing/empty.
    # - No fetch, no heuristic matching.
    try:
        _schema0 = analysis.get('metric_schema_frozen')
        if not isinstance(_schema0, dict) or not _schema0:
            _schema_seed = _seed_metric_schema_frozen_v1({})
            analysis['metric_schema_frozen'] = _schema_seed
            try:
                if not isinstance(analysis.get('debug'), dict):
                    analysis['debug'] = {}
                analysis['debug']['fix2d65d_seeded_schema'] = True
                analysis['debug']['fix2d65d_schema_keys'] = int(len(_schema_seed)) if isinstance(_schema_seed, dict) else 0
            except Exception:
                pass
    except Exception:
        pass

    #
    # Problem:
    # - Some narrative / market-size queries return no primary_metrics in primary_response.
    # - When injected URLs are present, we still want schema+baseline canonicals to materialise
    #   so Evolution schema-only rebuild and Diff Panel V2 can activate.
    #
    # Policy:
    # - If injected URLs exist and metric_schema_frozen is empty/missing, seed an empty schema and
    #   apply deterministic schema extension patches. This enables FIX2D31 schema-authority rebuild
    #   to run over the baseline candidate universe without changing UI/diff logic.
    #
    # Safety:
    # - No refetch, no heuristic matching. Additive only.
    try:
        # Collect injected URLs from UI/diagnostic fields only (plus explicit internal marker fallback)
        # to avoid misclassifying production source lists as "injected".
        _inj_urls = _yureeka_extract_injected_urls_v1(web_context)

        _has_inj = bool(_inj_urls)
        if _has_inj:
            try:
                analysis.setdefault("debug", {})
                if isinstance(analysis.get("debug"), dict):
                    analysis["debug"]["fix2d65b_forced_canonical_pipeline"] = True
                    analysis["debug"]["fix2d65b_injected_urls"] = _inj_urls[:10]
                    analysis["debug"]["injected_urls_v1"] = _inj_urls[:10]
            except Exception:
                pass

        # Ensure metric_schema_frozen exists even when LLM emitted no primary_metrics.
        # NOTE: We no longer gate this on injected URLs: Evolution schema-only rebuild
        # requires a baseline keyspace, and the deterministic schema extensions are safe.
        _schema0 = analysis.get("metric_schema_frozen")
        if not isinstance(_schema0, dict) or not _schema0:
            _schema_seed = _seed_metric_schema_frozen_v1({})
            analysis["metric_schema_frozen"] = _schema_seed
    except Exception:
        pass

    # - When metric_schema_frozen exists in Analysis, rebuild primary_metrics_canonical
    #   by running the authoritative Analysis selector (_analysis_canonical_final_selector_v1)
    #   constrained to the frozen schema keys, over the same baseline candidate universe.
    # - This makes Analysis emit schema-aligned baseline metrics so Evolution injection can overlap
    #   and Diff Panel V2 can activate without weakening semantics.
    try:
        _core = analysis
        schema = _core.get("metric_schema_frozen")
        pmc = _core.get("primary_metrics_canonical")
        sel = globals().get("_analysis_canonical_final_selector_v1")
        if isinstance(schema, dict) and schema and callable(sel):
            # Preserve original unconstrained PMC for diagnostics
            if isinstance(pmc, dict) and pmc and "primary_metrics_canonical_unconstrained_v0" not in analysis:
                _core["primary_metrics_canonical_unconstrained_v0"] = dict(pmc)

            # Flatten candidate universe from baseline_sources_cache + web_context.scraped_meta
            flat = []
            bsc = analysis.get("baseline_sources_cache")
            if isinstance(bsc, list):
                for src in bsc:
                    if isinstance(src, dict) and isinstance(src.get("extracted_numbers"), list):
                        flat.extend([n for n in src.get("extracted_numbers") if isinstance(n, dict)])
            if isinstance(web_context, dict):
                sm = web_context.get("scraped_meta")
                if isinstance(sm, dict) and isinstance(sm.get("extracted_numbers"), list):
                    flat.extend([n for n in sm.get("extracted_numbers") if isinstance(n, dict)])

            # Best-effort canonicalize candidates (if helper exists)
            try:
                fn_can = globals().get("canonicalize_numeric_candidate")
                if callable(fn_can):
                    flat = [fn_can(dict(n)) if isinstance(n, dict) else n for n in flat]
            except Exception:
                pass

            anchors = (_core.get("metric_anchors") or analysis.get("metric_anchors"))
            if not isinstance(anchors, dict):
                anchors = {}

            new_pmc = {}
            sel_trace = {}
            for ckey in sorted(schema.keys()):
                spec = schema.get(ckey) or {}
                if not isinstance(spec, dict):
                    continue
                try:
                    out_m, meta = sel(
                        ckey,
                        spec,
                        flat,
                        anchors=anchors,
                        prev_metric=(pmc or {}).get(ckey) if isinstance(pmc, dict) else None,
                        web_context=web_context,
                    )
                    if isinstance(out_m, dict):
                        if out_m.get("value_norm") is not None:
                            # FIX2D39: schema-authoritative dimension/unit_family hard-binding.
                            # - If winner has a concrete (non-unknown) dimension/unit_family that CONFLICTS with schema, reject it.
                            # - Otherwise, set dimension/unit_family to schema spec (authoritative) and stamp audit flags.
                            try:
                                spec_dim = ""
                                spec_uf = ""
                                if isinstance(spec, dict):
                                    spec_dim = str(spec.get("dimension") or spec.get("dim") or "").strip()
                                    spec_uf = str(spec.get("unit_family") or spec.get("unitFamily") or "").strip()
                                prior_dim = out_m.get("dimension")
                                prior_uf = out_m.get("unit_family")
                                prior_dim_s = ("" if prior_dim is None else str(prior_dim).strip())
                                prior_uf_s = ("" if prior_uf is None else str(prior_uf).strip())

                                # Reject conflicts: non-unknown prior value disagrees with schema.
                                if spec_dim and prior_dim_s and prior_dim_s not in ("unknown",) and prior_dim_s != spec_dim:
                                    meta = dict(meta) if isinstance(meta, dict) else {}
                                    meta["fix2d39_schema_dim_conflict_v1"] = True
                                    meta["fix2d39_schema_dim_prior_v1"] = prior_dim_s
                                    meta["fix2d39_schema_dim_spec_v1"] = spec_dim
                                    sel_trace[ckey] = meta
                                    continue
                                if spec_uf and prior_uf_s and prior_uf_s not in ("unknown",) and prior_uf_s != spec_uf:
                                    meta = dict(meta) if isinstance(meta, dict) else {}
                                    meta["fix2d39_schema_uf_conflict_v1"] = True
                                    meta["fix2d39_schema_uf_prior_v1"] = prior_uf_s
                                    meta["fix2d39_schema_uf_spec_v1"] = spec_uf
                                    sel_trace[ckey] = meta
                                    continue

                                # Apply schema authority (overwrite empty/unknown, and also normalize same-values).
                                if spec_dim:
                                    if prior_dim_s != spec_dim:
                                        out_m["dimension_prior_v1"] = prior_dim
                                        out_m["dimension_coerced_from_schema_v1"] = True
                                    out_m["dimension"] = spec_dim
                                if spec_uf:
                                    if prior_uf_s != spec_uf:
                                        out_m["unit_family_prior_v1"] = prior_uf
                                        out_m["unit_family_coerced_from_schema_v1"] = True
                                    out_m["unit_family"] = spec_uf
                            except Exception:
                                pass
                            new_pmc[ckey] = out_m
                    sel_trace[ckey] = meta if isinstance(meta, dict) else {"blocked_reason": "no_meta"}
                except Exception as _e:
                    sel_trace[ckey] = {"blocked_reason": "selector_exception", "error": str(_e)[:200]}

            # FIX2D38: Always build a schema-keyed baseline map for diffing.
            # by filling missing schema keys from flat candidates as proxy baselines. This ensures the prev universe
            # is schema-keyed and numeric where possible, so baseline diffing can activate.
            baseline_schema = {}
            try:
                if isinstance(new_pmc, dict) and new_pmc:
                    baseline_schema.update(new_pmc)
            except Exception:
                pass
            # FIX2D40: Remap baseline metrics from generic canonical keys onto schema canonical keys (one-to-one).
            # Rationale: Analysis may emit strong baseline metrics under generic keys (e.g., "2025_global_ev_sales__unknown")
            # while Evolution/injection emits schema keys (e.g., "global_ev_sales_ytd_2025__unit_sales"). Without remap,
            # prev_value_norm stays null and diff cannot activate.
            try:
                pmc_src = analysis.get('primary_metrics_canonical')
                used_src_keys = set()
                if isinstance(schema, dict) and isinstance(pmc_src, dict):
                    def _fix2d40_score(_ck, _m, _schema_key, _spec):
                        try:
                            txt = " ".join([
                                str(_ck or ""),
                                str(_m.get('name') or ""),
                                str(_m.get('context_snippet') or ""),
                                str(_m.get('source_url') or ""),
                            ]).lower()
                        except Exception:
                            pass
                            txt = str(_ck or "").lower()
                        s = 0
                        # Basic topic cues
                        if 'sales' in _schema_key and 'sales' in txt:
                            s += 3
                        if 'chargers' in _schema_key and ('charger' in txt or 'charging' in txt):
                            s += 3
                        if 'cagr' in _schema_key and ('cagr' in txt or 'growth' in txt or 'rate' in txt):
                            s += 3
                        if 'investment' in _schema_key and ('invest' in txt or 'capex' in txt):
                            s += 3
                        # Year cues
                        if '2025' in _schema_key and '2025' in txt:
                            s += 2
                        if '2040' in _schema_key and '2040' in txt:
                            s += 2
                        if '2026' in _schema_key and '2026' in txt:
                            s += 1
                        if '2030' in _schema_key and '2030' in txt:
                            s += 1
                        # Penalize obvious mismatches
                        if 'china' in txt and 'global' in _schema_key:
                            s -= 5
                        # Unit-family compatibility quick check
                        try:
                            spec_dim = str((_spec or {}).get('dimension') or '').strip().lower()
                            spec_uf = str((_spec or {}).get('unit_family') or (_spec or {}).get('unitFamily') or '').strip().lower()
                            prior_uf = str(_m.get('unit_family') or '').strip().lower()
                            if spec_uf and prior_uf and prior_uf not in ('unknown',) and prior_uf != spec_uf:
                                s -= 4
                            prior_dim = str(_m.get('dimension') or '').strip().lower()
                            if spec_dim and prior_dim and prior_dim not in ('unknown',) and prior_dim != spec_dim:
                                s -= 4
                        except Exception:
                            pass
                        # Prefer numeric
                        if _m.get('value_norm') is not None:
                            s += 2
                        return s

                    # For each schema key missing from baseline_schema, pick best candidate from pmc_src
                    for _skey, _spec in schema.items():
                        if not _skey or _skey in baseline_schema:
                            continue
                        best_k = None
                        best_m = None
                        best_score = None
                        for _ck, _m in pmc_src.items():
                            if _ck in used_src_keys:
                                continue
                            if not isinstance(_m, dict):
                                continue
                            sc = _fix2d40_score(_ck, _m, _skey, _spec)
                            if best_score is None or sc > best_score:
                                best_score = sc
                                best_k = _ck
                                best_m = _m
                        # Require a minimum positive score to avoid accidental remaps
                        if best_m is None or (best_score is None) or (best_score < 4):
                            continue

                        out_m = dict(best_m)
                        # Mark remap audit
                        out_m['schema_remap_v1'] = True
                        out_m['schema_remap_from_ckey_v1'] = best_k
                        out_m['schema_remap_to_ckey_v1'] = _skey
                        out_m['schema_remap_score_v1'] = best_score
                        out_m['is_proxy'] = True
                        out_m['proxy_type'] = 'schema_remap'
                        out_m['proxy_reason'] = 'schema_remap'

                        # Enforce FIX2D39 hard-binding against schema (reject conflicts)
                        try:
                            spec_dim = str((_spec or {}).get('dimension') or (_spec or {}).get('dim') or '').strip()
                            spec_uf = str((_spec or {}).get('unit_family') or (_spec or {}).get('unitFamily') or '').strip()
                            prior_dim = out_m.get('dimension')
                            prior_uf = out_m.get('unit_family')
                            prior_dim_s = ('' if prior_dim is None else str(prior_dim).strip())
                            prior_uf_s = ('' if prior_uf is None else str(prior_uf).strip())
                            if spec_dim and prior_dim_s and prior_dim_s not in ('unknown',) and prior_dim_s != spec_dim:
                                continue
                            if spec_uf and prior_uf_s and prior_uf_s not in ('unknown',) and prior_uf_s != spec_uf:
                                continue
                            if spec_dim:
                                if prior_dim_s != spec_dim:
                                    out_m['dimension_prior_v1'] = prior_dim
                                    out_m['dimension_coerced_from_schema_v1'] = True
                                out_m['dimension'] = spec_dim
                            if spec_uf:
                                if prior_uf_s != spec_uf:
                                    out_m['unit_family_prior_v1'] = prior_uf
                                    out_m['unit_family_coerced_from_schema_v1'] = True
                                out_m['unit_family'] = spec_uf
                        except Exception:
                            pass

                        # Override canonical key fields if present
                        try:
                            out_m['canonical_key'] = _skey
                            out_m['ckey'] = _skey
                        except Exception:
                            pass

                        baseline_schema[_skey] = out_m
                        used_src_keys.add(best_k)
            except Exception:
                pass

            # Fill gaps with schema fallback from flat candidates (proxy baselines)
            try:
                if isinstance(schema, dict) and isinstance(flat, list):
                    for ckey, spec in schema.items():
                        if not ckey or ckey in baseline_schema:
                            continue
                        best = None
                        for cand in flat:
                            try:
                                if not isinstance(cand, dict):
                                    continue
                                ck = cand.get('canonical_key') or cand.get('ckey') or cand.get('canon_key')
                                if ck != ckey:
                                    continue
                                # Prefer numeric candidates
                                if cand.get('value_norm') is not None:
                                    best = cand
                                    break
                                if best is None:
                                    best = cand
                            except Exception:
                                pass
                                continue
                        if best is None:
                            continue
                        out_m = dict(best)
                        # Ensure normalized numeric when possible (reuse FIX2D33 parsing helper if present)
                        try:
                            if out_m.get('value_norm') is None:
                                _vraw = out_m.get('value')
                                if (_vraw is None) and isinstance(out_m.get('evidence'), list) and out_m['evidence']:
                                    _ev0 = out_m['evidence'][0]
                                    if isinstance(_ev0, dict):
                                        _vraw = _ev0.get('raw') or _ev0.get('text')
                                if _vraw is not None:
                                    try:
                                        _parsed = _fix2d33_parse_value_norm_v1(_vraw, out_m) if ' _fix2d33_parse_value_norm_v1'.strip() else None
                                    except Exception:
                                        pass
                                        _parsed = None
                                    if _parsed is not None:
                                        out_m['value_norm'] = _parsed
                            # FIX2D39: schema-authoritative dimension/unit_family hard-binding (fallback path)
                            spec_dim = ''
                            spec_uf = ''
                            if isinstance(spec, dict):
                                spec_dim = str(spec.get('dimension') or spec.get('dim') or '').strip()
                                spec_uf = str(spec.get('unit_family') or spec.get('unitFamily') or '').strip()
                            prior_dim = out_m.get('dimension')
                            prior_uf = out_m.get('unit_family')
                            prior_dim_s = ('' if prior_dim is None else str(prior_dim).strip())
                            prior_uf_s = ('' if prior_uf is None else str(prior_uf).strip())

                            # Reject conflicts: non-unknown prior value disagrees with schema.
                            if spec_dim and prior_dim_s and prior_dim_s not in ('unknown',) and prior_dim_s != spec_dim:
                                continue
                            if spec_uf and prior_uf_s and prior_uf_s not in ('unknown',) and prior_uf_s != spec_uf:
                                continue

                            # Apply schema authority.
                            if spec_dim:
                                if prior_dim_s != spec_dim:
                                    out_m['dimension_coerced_from_schema_v1'] = True
                                    out_m['dimension_prior_v1'] = prior_dim
                                out_m['dimension'] = spec_dim
                            if spec_uf:
                                if prior_uf_s != spec_uf:
                                    out_m['unit_family_coerced_from_schema_v1'] = True
                                    out_m['unit_family_prior_v1'] = prior_uf
                                out_m['unit_family'] = spec_uf
                        except Exception:
                            pass
                        # Mark as proxy baseline fallback
                        out_m['is_proxy'] = True
                        out_m['proxy_type'] = 'schema_fallback'
                        out_m['proxy_reason'] = 'schema_fallback'
                        baseline_schema[ckey] = out_m
            except Exception:
                pass

            # FIX2D71: Always commit a schema-keyed baseline PMC for downstream diffing.
            # - If selector produced schema winners (new_pmc), commit those.
            # - Otherwise, if we have a schema-keyed baseline map (baseline_schema) and PMC is empty,
            #   commit the proxy baseline map so Evolution has prev canonical metrics to diff against.
            # This does NOT reintroduce heuristics: baseline_schema is schema-keyed and fully auditable via
            # is_proxy/proxy_reason flags.
            if new_pmc:
                _core['primary_metrics_canonical'] = new_pmc
                try:
                    if not isinstance(analysis.get('results'), dict):
                        analysis['results'] = {}
                    analysis.setdefault('results', {}).setdefault('debug', {})
                    if isinstance(analysis['results'].get('debug'), dict):
                        analysis['results']['debug']['fix2d71_committed_pmc_mode'] = 'selector_winners'
                except Exception:
                    pass
            else:
                try:
                    _pmc0 = _core.get('primary_metrics_canonical')
                    _pmc0_empty = (not isinstance(_pmc0, dict)) or (not _pmc0)
                    if _pmc0_empty and isinstance(baseline_schema, dict) and baseline_schema:
                        _core['primary_metrics_canonical'] = dict(baseline_schema)
                        # Stamp audit flag at top-level (lightweight)
                        try:
                            if not isinstance(_core.get('debug'), dict):
                                _core['debug'] = {}
                            if isinstance(_core.get('debug'), dict):
                                _core['debug']['fix2d71_committed_proxy_baseline_pmc_v1'] = True
                                _core['debug']['fix2d71_proxy_baseline_pmc_count_v1'] = len(baseline_schema)
                        except Exception:
                            pass
                        try:
                            if not isinstance(analysis.get('results'), dict):
                                analysis['results'] = {}
                            analysis.setdefault('results', {}).setdefault('debug', {})
                            if isinstance(analysis['results'].get('debug'), dict):
                                analysis['results']['debug']['fix2d71_committed_pmc_mode'] = 'proxy_baseline_schema_map'
                                analysis['results']['debug']['fix2d71_proxy_baseline_pmc_count_v1'] = len(baseline_schema)
                        except Exception:
                            pass
                except Exception:
                    pass

            # FIX2D38: emit schema-keyed baseline metrics map for diffing (always, with schema fallback).
            if baseline_schema:
                try:
                    _core['baseline_schema_metrics_v1'] = dict(baseline_schema)
                except Exception:
                    pass
                    _core['baseline_schema_metrics_v1'] = baseline_schema
                try:
                    if not isinstance(analysis.get('results'), dict):
                        analysis['results'] = {}
                    analysis['results']['baseline_schema_metrics_v1'] = _core.get('baseline_schema_metrics_v1')
                    try:
                        if _core is not analysis and isinstance(_core, dict) and 'baseline_schema_metrics_v1' in _core:
                            # keep it visible under primary_response for inspection
                            pass
                    except Exception:
                        pass
                except Exception:
                    pass

            # Debug trace
            try:
                if not isinstance(analysis.get('results'), dict):
                    analysis['results'] = {}
                if not isinstance(analysis['results'].get('debug'), dict):
                    analysis['results']['debug'] = {}
                analysis['results']['debug']['fix2d38_schema_baseline_map'] = {
                    'enabled': True,
                    'schema_key_count': int(len(schema)) if isinstance(schema, dict) else 0,
                    'flat_candidate_count': int(len(flat)) if isinstance(flat, list) else 0,
                    'selected_schema_count': int(len(new_pmc)) if isinstance(new_pmc, dict) else 0,
                    'baseline_schema_count': int(len(baseline_schema)) if isinstance(baseline_schema, dict) else 0,
                }
            except Exception:
                pass
    except Exception:
        pass
    # - Pure metadata, NO logic impact
    # - Allows downstream drift attribution:
    #     * pipeline changes vs source changes
    analysis.setdefault("analysis_pipeline_version", "v7_41_endstate_wip_1")
    analysis.setdefault("metric_identity_version", "canon_v2_dim_safe")
    analysis.setdefault("schema_freeze_version", 1)

    analysis.setdefault("code_version", _yureeka_get_code_version())

    # REFACTOR124 beacon: ensure Analysis wrapper shape is preserved for HistoryFull selection
    try:
        analysis.setdefault("debug", {})
        if isinstance(analysis.get("debug"), dict):
            _pr = (analysis or {}).get("primary_response") if isinstance((analysis or {}).get("primary_response"), dict) else {}
            analysis["debug"]["analysis_wrapper_shape_v1"] = {
                "has_question": bool(str((analysis or {}).get("question") or "").strip()),
                "has_timestamp": bool(str((analysis or {}).get("timestamp") or "").strip()),
                "has_question_profile": bool(isinstance((analysis or {}).get("question_profile"), dict) and bool((analysis or {}).get("question_profile"))),
                "pmc_top_n": int(len((analysis or {}).get("primary_metrics_canonical") or {})) if isinstance((analysis or {}).get("primary_metrics_canonical"), dict) else 0,
                "pmc_primary_response_n": int(len((_pr or {}).get("primary_metrics_canonical") or {})) if isinstance((_pr or {}).get("primary_metrics_canonical"), dict) else 0,
            }
    except Exception:
        pass

    return analysis

def normalize_unit(unit: str) -> str:
    """Normalize unit to one of: T/B/M/%/'' (deterministic)."""
    if not unit:
        return ""
    u = unit.strip().upper().replace("USD", "").replace("$", "").replace(" ", "")
    if u in ["TRILLION", "T"]:
        return "T"
    if u in ["BILLION", "B"]:
        return "B"
    if u in ["MILLION", "M"]:
        return "M"
    if u in ["PERCENT", "%"]:
        return "%"
    if u in ["K", "THOUSAND"]:
        return "K"
    return u

def _refactor61__unwrap_pmc(obj):
    """Best-effort unwrap for primary_metrics_canonical across historical payload shapes."""
    try:
        if isinstance(obj, dict) and isinstance(obj.get("primary_metrics_canonical"), dict):
            return obj.get("primary_metrics_canonical") or {}
        if isinstance(obj, dict) and isinstance(obj.get("primary_response"), dict) and isinstance(obj["primary_response"].get("primary_metrics_canonical"), dict):
            return obj["primary_response"].get("primary_metrics_canonical") or {}
        if isinstance(obj, dict) and isinstance(obj.get("results"), dict) and isinstance(obj["results"].get("primary_metrics_canonical"), dict):
            return obj["results"].get("primary_metrics_canonical") or {}
        if isinstance(obj, dict) and isinstance(obj.get("results"), dict) and isinstance(obj["results"].get("primary_response"), dict) and isinstance(obj["results"]["primary_response"].get("primary_metrics_canonical"), dict):
            return obj["results"]["primary_response"].get("primary_metrics_canonical") or {}
    except Exception:
        pass
    return {}

def _refactor72__unwrap_metric_schema_frozen(obj):
    """Best-effort unwrap for metric_schema_frozen across historical payload shapes."""
    try:
        if isinstance(obj, dict) and isinstance(obj.get("metric_schema_frozen"), dict):
            return obj.get("metric_schema_frozen") or {}
        if isinstance(obj, dict) and isinstance(obj.get("primary_response"), dict) and isinstance(obj["primary_response"].get("metric_schema_frozen"), dict):
            return obj["primary_response"].get("metric_schema_frozen") or {}
        if isinstance(obj, dict) and isinstance(obj.get("results"), dict) and isinstance(obj["results"].get("metric_schema_frozen"), dict):
            return obj["results"].get("metric_schema_frozen") or {}
        if isinstance(obj, dict) and isinstance(obj.get("results"), dict) and isinstance(obj["results"].get("primary_response"), dict) and isinstance(obj["results"]["primary_response"].get("metric_schema_frozen"), dict):
            return obj["results"]["primary_response"].get("metric_schema_frozen") or {}
    except Exception:
        pass
    return {}

def _refactor61__pick_val_unit(m):
    """Extract (value_norm, unit_tag) deterministically from a canonical metric dict."""
    try:
        if not isinstance(m, dict):
            return (None, "")
        vn = m.get("value_norm")
        if vn is None:
            vn = m.get("value")
        unit = (m.get("unit_tag") or m.get("unit") or m.get("unit_cmp") or m.get("base_unit") or "").strip()
        return (vn, unit)
    except Exception:
        return (None, "")

def _refactor61__pick_source_url(m):
    """Best-effort provenance URL extraction (diff-row attribution).

    Order (REFACTOR123):
      1) top-level m["source_url"] / m["url"]
      2) first evidence source_url/url (if any)
      3) provenance.best_candidate.source_url/url
    """
    try:
        if not isinstance(m, dict):
            return None

        # 1) top-level
        try:
            u0 = m.get("source_url") or m.get("url")
            if u0:
                return str(u0)
        except Exception:
            pass

        # 2) evidence list
        try:
            ev = m.get("evidence")
            if isinstance(ev, list) and ev:
                e0 = ev[0] if isinstance(ev[0], dict) else None
                if isinstance(e0, dict):
                    u1 = e0.get("source_url") or e0.get("url")
                    if u1:
                        return str(u1)
        except Exception:
            pass

        # 3) provenance best_candidate
        try:
            prov = m.get("provenance") if isinstance(m.get("provenance"), dict) else None
            bc = prov.get("best_candidate") if isinstance(prov, dict) and isinstance(prov.get("best_candidate"), dict) else None
            if isinstance(bc, dict):
                u2 = bc.get("source_url") or bc.get("url")
                return str(u2) if u2 else None
        except Exception:
            pass
    except Exception:
        pass
    return None

def build_diff_metrics_panel_v2__rows_refactor47(prev_response: dict, cur_response: dict):
    """
    Canonical-first strict join (schema-complete in REFACTOR74):

      - Prefer iterating *frozen schema keys* when metric_schema_frozen is available.
      - For keys present in both baseline and current:
            * join on exact canonical_key only (no heuristics)
            * enforce strict unit comparability; only compute deltas when units match
      - Additionally emit explicit completeness rows for schema keys when either side is missing:
            * missing_baseline: key absent in baseline but present in current
            * missing_current: key present in baseline but absent in current
            * missing_both: key absent in both (still emitted when schema is known)

    Returns: (rows, summary)
    """
    prev_can = _refactor61__unwrap_pmc(prev_response)
    cur_can = _refactor61__unwrap_pmc(cur_response)

    # Prefer frozen schema key order for completeness (stable UI + deterministic diff feed)
    schema = _refactor72__unwrap_metric_schema_frozen(prev_response) or _refactor72__unwrap_metric_schema_frozen(cur_response)
    schema_keys = []
    try:
        if isinstance(schema, dict) and schema:
            schema_keys = sorted([str(k) for k in schema.keys()])
    except Exception:
        schema_keys = []

    rows = []
    try:
        if not isinstance(prev_can, dict):
            prev_can = {}
        if not isinstance(cur_can, dict):
            cur_can = {}

        if schema_keys:
            iter_keys = list(schema_keys)
            mode = "schema_complete"
        else:
            # Fallback: union so we still show anything we can (legacy behavior)
            iter_keys = sorted({str(k) for k in list(prev_can.keys()) + list(cur_can.keys())})
            mode = "union_fallback"

        for ckey in iter_keys:
            pm = prev_can.get(ckey) if isinstance(prev_can.get(ckey), dict) else None
            cm = cur_can.get(ckey) if isinstance(cur_can.get(ckey), dict) else None

            pm_ok = isinstance(pm, dict) and bool(pm)
            cm_ok = isinstance(cm, dict) and bool(cm)

            pv, pu = (None, "")
            if pm_ok:
                pv, pu = _refactor61__pick_val_unit(pm)

            cv, cu = (None, "")
            if cm_ok:
                cv, cu = _refactor61__pick_val_unit(cm)

            # For display clarity, if only one side is present, carry the known unit across.
            try:
                if (not pm_ok) and cm_ok and str(cu).strip():
                    pu = cu
                if pm_ok and (not cm_ok) and str(pu).strip():
                    cu = pu
            except Exception:
                pass
            # If schema is known and both units are empty, fill from schema when safe.
            # (Avoid currency placeholder unit 'U' to prevent misleading display.)
            try:
                if (not str(pu).strip()) and (not str(cu).strip()) and isinstance(schema, dict):
                    sch = schema.get(ckey) if isinstance(schema.get(ckey), dict) else None
                    su = (sch.get("unit") if isinstance(sch, dict) else "") or ""
                    su = str(su).strip()
                    if su and su.upper() != "U":
                        pu = su
                        cu = su
            except Exception:
                pass

            # Best-effort metric name
            name = ""
            try:
                name = (pm.get("name") if pm_ok else "") or (cm.get("name") if cm_ok else "") or str(ckey)
            except Exception:
                name = str(ckey)

            baseline_is_comparable = False
            delta_abs = None
            delta_pct = None
            # Completeness-first change_type defaults (REFACTOR123)
            # Treat null values as missing even if the key exists in schema maps.
            pm_has_value = bool(pm_ok and (pv is not None))
            cm_has_value = bool(cm_ok and (cv is not None))

            if pm_has_value and cm_has_value:
                change_type = "found"
            elif (not pm_has_value) and cm_has_value:
                change_type = "missing_baseline"
            elif pm_has_value and (not cm_has_value):
                change_type = "missing_current"
            else:
                change_type = "missing_both"

            try:
                # Strict comparability: both present + units must match and be non-empty
                if pm_has_value and cm_has_value and str(pu).strip() and str(cu).strip() and str(pu).strip() == str(cu).strip():
                    baseline_is_comparable = True
                    pvf = float(pv) if isinstance(pv, (int, float)) else None
                    cvf = float(cv) if isinstance(cv, (int, float)) else None
                    if pvf is not None and cvf is not None:
                        delta_abs = cvf - pvf
                        if abs(delta_abs) < 1e-12:
                            delta_abs = 0.0
                            change_type = "unchanged"
                        elif delta_abs > 0:
                            change_type = "increased"
                        else:
                            change_type = "decreased"
                        if pvf != 0:
                            delta_pct = (delta_abs / pvf) * 100.0
            except Exception:
                pass

            # If both present but units differ: explicitly mark unit mismatch.
            try:
                if pm_has_value and cm_has_value and str(pu).strip() and str(cu).strip() and str(pu).strip() != str(cu).strip():
                    change_type = "unit_mismatch"
                    baseline_is_comparable = False
            except Exception:
                pass

            row = {
                "canonical_key": str(ckey),
                "name": name,
                "previous_value": pv,
                "previous_unit": pu,
                "prev_value_norm": pv,
                "current_value": cv,
                "current_unit": cu,
                "cur_value_norm": cv,
                "delta_abs": delta_abs,
                "delta_pct": delta_pct,
                "change_type": change_type,
                "baseline_is_comparable": bool(baseline_is_comparable),
                "current_method": "strict_fallback_v2",
                "source_url": _refactor61__pick_source_url(cm) if cm_ok else None,
                "schema_frozen_key": bool(ckey in schema_keys) if schema_keys else False,
            }

            rows.append(row)
    except Exception:
        pass

    summary = {
        "rows_total": int(len(rows)),
        "builder": "REFACTOR74_build_diff_metrics_panel_v2__rows_refactor47",
        "mode": (mode if 'mode' in locals() else ""),
        "schema_keys_total": int(len(schema_keys)) if schema_keys else 0,
    }
    return rows, summary

def _fix2d86_sanitize_pmc_percent_year_tokens_v1(pmc: dict, metric_schema_frozen: dict, label: str):
    """
    FIX2D86 HOTFIX:
    For __percent keys, drop bindings where the chosen evidence is a bare year token (1900-2100),
    e.g. raw="2040" and value_norm=2040.0. This prevents prev=2040 for CAGR percent keys.
    """
    dbg = {
        "label": label,
        "pmc_in_count": len(pmc) if isinstance(pmc, dict) else 0,
        "pmc_out_count": 0,
        "dropped_count": 0,
        "dropped_samples": [],
    }
    if not isinstance(pmc, dict):
        return pmc, dbg

    def _is_percent_key(k: str) -> bool:
        if isinstance(k, str) and k.endswith("__percent"):
            return True
        sch = metric_schema_frozen.get(k) if isinstance(metric_schema_frozen, dict) else None
        if isinstance(sch, dict):
            dim = str(sch.get("dimension") or sch.get("unit_kind") or "").lower()
            return dim == "percent"
        return False

    def _yearlike_num(v) -> bool:
        try:
            x = float(v)
        except Exception:
            return False
        if abs(x - round(x)) > 1e-9:
            return False
        y = int(round(x))
        return 1900 <= y <= 2100

    def _extract_raw_token(vdict: dict) -> str:
        if not isinstance(vdict, dict):
            return ""
        ev = vdict.get("evidence")

        # evidence can be dict or list of dicts in your codebase
        if isinstance(ev, dict):
            return str(ev.get("raw") or ev.get("raw_text") or ev.get("token") or "")
        if isinstance(ev, list) and ev:
            for item in ev:
                if isinstance(item, dict):
                    raw = str(item.get("raw") or item.get("raw_text") or item.get("token") or "")
                    if raw:
                        return raw
        return str(vdict.get("raw") or "")

    def _raw_is_bare_year(raw: str) -> bool:
        if not raw:
            return False
        t = raw.strip().lower()
        # Must be "2040" or "2040.0" style, and MUST NOT contain percent markers
        if ("%" in t) or ("percent" in t) or ("pct" in t):
            return False
        # accept 4-digit integer, or integer with .0
        if t.isdigit() and len(t) == 4:
            y = int(t)
            return 1900 <= y <= 2100
        if t.endswith(".0"):
            base = t[:-2]
            if base.isdigit() and len(base) == 4:
                y = int(base)
                return 1900 <= y <= 2100
        return False

    out = {}
    for k, v in pmc.items():
        if not _is_percent_key(k):
            out[k] = v
            continue

        if isinstance(v, dict):
            val_norm = v.get("value_norm")
            raw = _extract_raw_token(v)
            if _yearlike_num(val_norm) and _raw_is_bare_year(raw):
                dbg["dropped_count"] += 1
                if len(dbg["dropped_samples"]) < 5:
                    dbg["dropped_samples"].append({
                        "canonical_key": k,
                        "value_norm": val_norm,
                        "raw": raw,
                        "source_url": v.get("source_url"),
                        "method": v.get("method"),
                        "anchor_hash": v.get("anchor_hash"),
                    })
                continue

        out[k] = v

    dbg["pmc_out_count"] = len(out)
    return out, dbg

def _fix2d82_extract_raw_token_v2(rec: dict) -> str:
    """Extract the best *token* string for this record (not context)."""
    try:
        if not isinstance(rec, dict):
            return ''
        ev = rec.get('evidence')
        if isinstance(ev, list) and ev:
            e0 = ev[0]
            if isinstance(e0, dict):
                r = e0.get('raw')
                if isinstance(r, str) and r.strip():
                    return r.strip()
        r2 = rec.get('raw')
        if isinstance(r2, str) and r2.strip():
            return r2.strip()
        v = rec.get('value')
        if v is None:
            v = rec.get('value_norm')
        return str(v).strip() if v is not None else ''
    except Exception:
        return ''

def _fix2d82_is_year_token_v2(vnorm, raw_token: str) -> bool:
    """True when vnorm is yearlike (1900-2100 int) AND raw_token is that YYYY."""
    try:
        fv = float(vnorm)
        if not fv.is_integer():
            return False
        iv = int(fv)
        if iv < 1900 or iv > 2100:
            return False
        tok = str(raw_token or '').strip()
        return (len(tok) == 4) and tok.isdigit() and (int(tok) == iv)
    except Exception:
        return False

def _fix2d82_is_percent_key_v2(k: str, rec: dict, schema: dict) -> bool:
    try:
        if isinstance(k, str) and k.endswith('__percent'):
            return True
        if isinstance(schema, dict) and isinstance(k, str):
            spec = schema.get(k)
            if isinstance(spec, dict):
                dim = str(spec.get('dimension') or '').strip().lower()
                uf = str(spec.get('unit_family') or '').strip().lower()
                ut = str(spec.get('unit_tag') or '').strip().lower()
                blob = ' '.join([dim, uf, ut])
                return ('percent' in blob) or ('%' in blob)
        if isinstance(rec, dict):
            dim = str(rec.get('dimension') or '').strip().lower()
            uf = str(rec.get('unit_family') or '').strip().lower()
            ut = str(rec.get('unit_tag') or '').strip().lower()
            blob = ' '.join([dim, uf, ut])
            return ('percent' in blob) or ('%' in blob)
    except Exception:
        return False
    return False

def _fix2d82_get_schema_from_prev_response_v2(prev_response: dict) -> dict:
    try:
        if not isinstance(prev_response, dict):
            return None
        schema = prev_response.get('metric_schema_frozen')
        if isinstance(schema, dict):
            return schema
        pr = prev_response.get('primary_response') if isinstance(prev_response.get('primary_response'), dict) else None
        if isinstance(pr, dict) and isinstance(pr.get('metric_schema_frozen'), dict):
            return pr.get('metric_schema_frozen')
        rr = prev_response.get('results') if isinstance(prev_response.get('results'), dict) else None
        if isinstance(rr, dict) and isinstance(rr.get('metric_schema_frozen'), dict):
            return rr.get('metric_schema_frozen')
    except Exception:
        return None
    return None

def _fix2d82_sanitize_pmc_percent_keys_v2(pmc: dict, metric_schema_frozen: dict = None, label: str = "") -> tuple:
    """Drop poisoned percent bindings: bare-year tokens and missing percent evidence (token/unit)."""
    dbg = {
        "applied": False,
        "label": str(label or ""),
        "input_count": int(len(pmc) if isinstance(pmc, dict) else 0),
        "output_count": 0,
        "checked_percent_keys": 0,
        "dropped_count": 0,
        "dropped_keys_sample": [],
        "reasons_sample": [],
        "fix": "FIX2D82",
    }
    try:
        if not isinstance(pmc, dict) or not pmc:
            dbg["applied"] = True
            dbg["output_count"] = 0
            return (pmc if isinstance(pmc, dict) else {}), dbg

        schema = metric_schema_frozen if isinstance(metric_schema_frozen, dict) else {}
        out = dict(pmc)

        for k in list(out.keys()):
            rec = out.get(k)
            if not _fix2d82_is_percent_key_v2(k, rec if isinstance(rec, dict) else {}, schema):
                continue

            dbg["checked_percent_keys"] += 1
            if not isinstance(rec, dict):
                out.pop(k, None)
                dbg["dropped_count"] += 1
                if len(dbg["dropped_keys_sample"]) < 8:
                    dbg["dropped_keys_sample"].append(str(k))
                    dbg["reasons_sample"].append({"key": str(k), "reason": "non_dict_record"})
                continue

            vnorm = rec.get('value_norm')
            raw_token = _fix2d82_extract_raw_token_v2(rec)

            # Hard drop year-token values for percent keys.
            if _fix2d82_is_year_token_v2(vnorm, raw_token):
                out.pop(k, None)
                dbg["dropped_count"] += 1
                if len(dbg["dropped_keys_sample"]) < 8:
                    dbg["dropped_keys_sample"].append(str(k))
                    dbg["reasons_sample"].append({"key": str(k), "reason": "year_token_for_percent", "raw_token": str(raw_token)[:60], "value_norm": vnorm})
                continue

            # Strong percent evidence must be tied to the token/unit (not context).
            unit = str(rec.get('unit') or rec.get('unit_tag') or '')
            rt = str(raw_token or '').lower()
            token_has_pct = ('%' in rt) or ('percent' in rt) or ('pct' in rt)
            unit_has_pct = ('%' in unit) or ('percent' in unit.lower())

            if (not token_has_pct) and (not unit_has_pct):
                out.pop(k, None)
                dbg["dropped_count"] += 1
                if len(dbg["dropped_keys_sample"]) < 8:
                    dbg["dropped_keys_sample"].append(str(k))
                    dbg["reasons_sample"].append({"key": str(k), "reason": "no_strong_percent_evidence_token", "unit": unit, "raw_token": str(raw_token)[:80], "value_norm": vnorm})
                continue

        dbg["output_count"] = int(len(out))
        dbg["applied"] = True
        return out, dbg
    except Exception as e:
        dbg["applied"] = False
        dbg["error"] = str(type(e).__name__)
        return pmc, dbg

def compute_source_anchored_diff(previous_data: dict, web_context: dict = None) -> dict:
    """
    Tight source-anchored evolution:
      - Prefer snapshots from analysis (baseline_sources_cache)
      - Optionally reconstruct snapshots from web_context.scraped_meta
      - If no valid snapshots: return not_found (no heuristic junk)

    Always returns a dict.
    """

    # REFACTOR139: inline FIX2D82/FIX2D86 percent-key sanitation on previous_data
    # (previously installed via late monkeypatch wrappers).
    try:
        if isinstance(previous_data, dict):
            schema = _fix2d82_get_schema_from_prev_response_v2(previous_data)
            pmc = previous_data.get("primary_metrics_canonical")
            if not isinstance(pmc, dict) or not pmc:
                pr = previous_data.get("primary_response") if isinstance(previous_data.get("primary_response"), dict) else None
                if isinstance(pr, dict) and isinstance(pr.get("primary_metrics_canonical"), dict):
                    pmc = pr.get("primary_metrics_canonical")
            if isinstance(pmc, dict) and pmc:
                # 1) Drop bare-year percent bindings (year-token poisoning guardrail)
                try:
                    fn86 = globals().get("_fix2d86_sanitize_pmc_percent_year_tokens_v1")
                    if callable(fn86):
                        pmc2, dbg86 = fn86(pmc, schema, label="compute_source_anchored_diff_prev")
                        if isinstance(pmc2, dict):
                            pmc = pmc2
                        try:
                            previous_data.setdefault("debug", {})
                            if isinstance(previous_data.get("debug"), dict):
                                previous_data["debug"]["fix2d86_prev_percent_year_token_sanitize"] = dbg86
                        except Exception:
                            pass
                except Exception:
                    pass

                # 2) Enforce strong percent evidence + hard year-token drop for percent keys
                try:
                    fn82 = globals().get("_fix2d82_sanitize_pmc_percent_keys_v2")
                    if callable(fn82):
                        pmc3, dbg82 = fn82(pmc, metric_schema_frozen=schema, label="compute_source_anchored_diff_prev")
                        if isinstance(pmc3, dict):
                            pmc = pmc3
                        try:
                            previous_data.setdefault("debug", {})
                            if isinstance(previous_data.get("debug"), dict):
                                previous_data["debug"]["fix2d82_prev_percent_sanitize"] = dbg82
                        except Exception:
                            pass
                except Exception:
                    pass

                try:
                    previous_data["primary_metrics_canonical"] = pmc
                except Exception:
                    pass
                try:
                    previous_data.setdefault("primary_response", {})
                    if isinstance(previous_data.get("primary_response"), dict):
                        previous_data["primary_response"]["primary_metrics_canonical"] = pmc
                except Exception:
                    pass
    except Exception:
        pass
    try:
        _qtxt = str((analysis or {}).get('question') or (analysis or {}).get('query') or '')
        web_context = _fix2d66_promote_injected_urls(web_context or {}, question_text=_qtxt, stage='analysis_attach')
    except Exception:
        pass

    def _now():
        return datetime.now(timezone.utc).isoformat()

    def _safe_int(x, default=0):
        try:
            return int(x)
        except Exception:
            return default

    def _fingerprint(text: str):
        try:
            fn = globals().get("fingerprint_text")
            if callable(fn):
                return fn(text or "")
        except Exception:
            pass
        try:
            return fingerprint_text(text or "")
        except Exception:
            return None

    # Why:
    # - Some runs store rebuild essentials under primary_response or results.primary_response
    # - Evolution may look only at top-level keys, causing schema=0 / anchors=0
    def _get_nested(d, path, default=None):
        try:
            x = d
            for k in path:
                if not isinstance(x, dict):
                    return default
                x = x.get(k)
            return x if x is not None else default
        except Exception:
            return default

    def _first_present(d, paths, default=None):
        for p in paths:
            v = _get_nested(d, p, None)
            if v is not None:
                return v
        return default

    # Why:
    # - Diff Panel V2 previously called an undefined helper and silently fell back to {}
    #   which then prevented baseline fallback from ever running ({} is a dict).
    # - Provide one deterministic locator for baseline/current primary_metrics_canonical
    #   across the common payload shapes (top-level, primary_response, results, etc.).
    def _refactor89_locate_pmc_dict(obj) -> dict:
        try:
            if not isinstance(obj, dict):
                return {}
            # Most common: top-level
            pmc = obj.get("primary_metrics_canonical")
            if isinstance(pmc, dict) and pmc:
                return pmc
            # Common wrappers
            candidates = [
                ("primary_response", "primary_metrics_canonical"),
                ("results", "primary_metrics_canonical"),
                ("results", "primary_response", "primary_metrics_canonical"),
                ("primary_response", "results", "primary_metrics_canonical"),
                ("primary_response", "results", "primary_response", "primary_metrics_canonical"),
            ]
            for path in candidates:
                x = obj
                ok = True
                for k in path:
                    if not isinstance(x, dict):
                        ok = False
                        break
                    x = x.get(k)
                if ok and isinstance(x, dict) and x:
                    return x
        except Exception:
            pass
        return {}

    # Diff Panel V2 expects this name; define it so it can never NameError.
    def _diffpanel_v2__unwrap_primary_metrics_canonical(obj) -> dict:
        return _refactor89_locate_pmc_dict(obj)
    # Why:
    # - Some UI/Sheets paths provide a summarized wrapper that lacks primary_response,
    #   metric_schema_frozen, metric_anchors, baseline_sources_cache, etc.
    # - If a full_store_ref pointer exists, load the full payload deterministically.
    #
    # NOTE:
    # - Do NOT write to `output` here (output not built yet). We stash flags
    #   and attach them after `output = {...}` is created.
    _prev_rehydrated = False
    _prev_rehydrated_ref = ""

    _refactor89_prev_keys_sample_pre = []
    _refactor89_prev_pmc_count_pre = 0
    _refactor89_prev_pmc_keys_sample_pre = []
    _refactor89_prev_has_full_store_ref = False
    _refactor89_prev_has_snapshot_store_ref = False
    try:
        if isinstance(previous_data, dict):
            _refactor89_prev_keys_sample_pre = list(previous_data.keys())[:25]
            _refactor89_prev_has_full_store_ref = bool(previous_data.get("full_store_ref") or (previous_data.get("results") or {}).get("full_store_ref") or "")
            _refactor89_prev_has_snapshot_store_ref = bool(previous_data.get("snapshot_store_ref") or (previous_data.get("results") or {}).get("snapshot_store_ref") or "")
            _pmc_pre = _refactor89_locate_pmc_dict(previous_data)
            if isinstance(_pmc_pre, dict):
                _refactor89_prev_pmc_count_pre = int(len(_pmc_pre))
                _refactor89_prev_pmc_keys_sample_pre = list(_pmc_pre.keys())[:12]
    except Exception:
        pass

    try:
        if isinstance(previous_data, dict):
            _pr = previous_data.get("primary_response")

            # Determine if we are missing rebuild essentials
            _pmc_present = False
            try:
                _pmc_present = bool(_refactor89_locate_pmc_dict(previous_data) or (_refactor89_locate_pmc_dict(_pr) if isinstance(_pr, dict) else {}))
            except Exception:
                _pmc_present = False

            # Determine if we are missing rebuild essentials (schema OR baseline PMC)
            _need = (
                (not isinstance(_pr, dict))
                or (not _pr)
                or (not isinstance(_pr.get("metric_schema_frozen"), dict))
                or (not _pmc_present)
            )

            if _need:
                # Explicit line (requested): simplest location first
                ref = previous_data.get("full_store_ref", "")  # <-- requested line

                # Then fall back to other known wrapper locations (more robust)
                _ref = (
                    ref
                    or (previous_data.get("results") or {}).get("full_store_ref")
                    or (isinstance(_pr, dict) and _pr.get("full_store_ref"))
                    or ""
                )

                # Last-ditch deterministic fallback: if wrapper carries _sheet_id
                if (not _ref) and isinstance(previous_data.get("_sheet_id"), str) and previous_data.get("_sheet_id"):
                    _ref = f"gsheet:HistoryFull:{previous_data.get('_sheet_id')}"

                if isinstance(_ref, str) and _ref.startswith("gsheet:"):
                    parts = _ref.split(":")
                    _ws_title = parts[1] if len(parts) > 1 and parts[1] else "HistoryFull"
                    _aid = parts[2] if len(parts) > 2 else ""
                    full = load_full_history_payload_from_sheet(_aid, worksheet_title=_ws_title) if _aid else {}
                    if isinstance(full, dict) and full:
                        previous_data = full
                        _prev_rehydrated = True
                        _prev_rehydrated_ref = _ref
    except Exception:
        pass

    try:
        if isinstance(previous_data, dict):
            _pmc_now = _refactor89_locate_pmc_dict(previous_data)
            if not (isinstance(_pmc_now, dict) and _pmc_now):
                try:
                    _pf = _fix24_get_prev_full_payload(previous_data)
                except Exception:
                    _pf = {}
                if isinstance(_pf, dict) and _pf:
                    previous_data = _pf
                    _prev_rehydrated = True
                    if not _prev_rehydrated_ref:
                        try:
                            _prev_rehydrated_ref = str(previous_data.get("full_store_ref") or previous_data.get("snapshot_store_ref") or "fix24_get_prev_full_payload")
                        except Exception:
                            _prev_rehydrated_ref = "fix24_get_prev_full_payload"
    except Exception:
        pass
    _refactor89_prev_payload_probe_v1 = {}
    try:
        _pmc_post = _refactor89_locate_pmc_dict(previous_data) if isinstance(previous_data, dict) else {}
        _refactor89_prev_payload_probe_v1 = {
            "prev_keys_sample": list(_refactor89_prev_keys_sample_pre or []),
            "prev_keys_sample_post": list(previous_data.keys())[:25] if isinstance(previous_data, dict) else [],
            "has_primary_metrics_canonical": bool(_refactor89_prev_pmc_count_pre),
            "has_full_store_ref": bool(_refactor89_prev_has_full_store_ref),
            "has_snapshot_store_ref": bool(_refactor89_prev_has_snapshot_store_ref),
            "rehydrated_prev_ok": bool(_prev_rehydrated),
            "rehydrated_ref": str(_prev_rehydrated_ref or ""),
            "baseline_pmc_count": int(len(_pmc_post)) if isinstance(_pmc_post, dict) else 0,
            "baseline_pmc_keys_sample": list(_pmc_post.keys())[:12] if isinstance(_pmc_post, dict) else [],
            "baseline_pmc_count_pre": int(_refactor89_prev_pmc_count_pre or 0),
            "baseline_pmc_keys_sample_pre": list(_refactor89_prev_pmc_keys_sample_pre or []),
        }
    except Exception:
        _refactor89_prev_payload_probe_v1 = {
            "prev_keys_sample": list(_refactor89_prev_keys_sample_pre or []),
            "rehydrated_prev_ok": bool(_prev_rehydrated),
            "rehydrated_ref": str(_prev_rehydrated_ref or ""),
        }

    snapshot_origin = "none"
    baseline_sources_cache = []

    try:
        if isinstance(previous_data, dict):
            # 1) results.baseline_sources_cache (preferred)
            r = previous_data.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache"), list):
                baseline_sources_cache = r.get("baseline_sources_cache") or []
                if baseline_sources_cache:
                    snapshot_origin = "analysis_results_cache"

            # 2) top-level baseline_sources_cache
            if not baseline_sources_cache and isinstance(previous_data.get("baseline_sources_cache"), list):
                baseline_sources_cache = previous_data.get("baseline_sources_cache") or []
                if baseline_sources_cache:
                    snapshot_origin = "analysis_top_level_cache"
    except Exception:
        pass
        baseline_sources_cache = []

    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            pr = previous_data.get("primary_response") or {}
            if isinstance(pr, dict):
                # A) primary_response.results.baseline_sources_cache
                r2 = pr.get("results")
                if (not baseline_sources_cache) and isinstance(r2, dict) and isinstance(r2.get("baseline_sources_cache"), list):
                    baseline_sources_cache = r2.get("baseline_sources_cache") or []
                    if baseline_sources_cache:
                        snapshot_origin = "primary_response_results_cache"

                # B) primary_response.baseline_sources_cache
                if (not baseline_sources_cache) and isinstance(pr.get("baseline_sources_cache"), list):
                    baseline_sources_cache = pr.get("baseline_sources_cache") or []
                    if baseline_sources_cache:
                        snapshot_origin = "primary_response_top_level_cache"

                # C) primary_response.results.source_results (reconstruct minimal snapshot shape)
                if (not baseline_sources_cache) and isinstance(r2, dict) and isinstance(r2.get("source_results"), list):
                    rebuilt_sr = []
                    for sr in (r2.get("source_results") or []):
                        if not isinstance(sr, dict):
                            continue
                        u = sr.get("source_url") or sr.get("url")
                        ex = sr.get("extracted_numbers")
                        if u and isinstance(ex, list) and ex:
                            rebuilt_sr.append({
                                "source_url": u,
                                "extracted_numbers": ex,
                                "clean_text": sr.get("clean_text") or sr.get("content") or "",
                                "fingerprint": sr.get("fingerprint"),
                                "fetched_at": sr.get("fetched_at"),
                            })
                    rebuilt_sr.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))
                    if rebuilt_sr:
                        baseline_sources_cache = rebuilt_sr
                        snapshot_origin = "primary_response_source_results_rebuild"

        # D) previous_data.results.source_results fallback
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            r3 = previous_data.get("results")
            if isinstance(r3, dict) and isinstance(r3.get("source_results"), list):
                rebuilt_sr2 = []
                for sr in (r3.get("source_results") or []):
                    if not isinstance(sr, dict):
                        continue
                    u = sr.get("source_url") or sr.get("url")
                    ex = sr.get("extracted_numbers")
                    if u and isinstance(ex, list) and ex:
                        rebuilt_sr2.append({
                            "source_url": u,
                            "extracted_numbers": ex,
                            "clean_text": sr.get("clean_text") or sr.get("content") or "",
                            "fingerprint": sr.get("fingerprint"),
                            "fetched_at": sr.get("fetched_at"),
                        })
                rebuilt_sr2.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))
                if rebuilt_sr2:
                    baseline_sources_cache = rebuilt_sr2
                    snapshot_origin = "analysis_source_results_rebuild"
    except Exception:
        pass

    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            _er = None
            if isinstance(previous_data.get("results"), dict):
                _er = previous_data["results"].get("evidence_records")
            if _er is None:
                _er = previous_data.get("evidence_records")
            _rebuilt = build_baseline_sources_cache_from_evidence_records(_er)
            if isinstance(_rebuilt, list) and _rebuilt:
                baseline_sources_cache = _rebuilt
                snapshot_origin = "evidence_records_rebuild"
    except Exception:
        pass

    _snapshot_debug = None
    try:
        _raw_len = int(len(baseline_sources_cache)) if isinstance(baseline_sources_cache, list) else 0
        _kept = []
        if isinstance(baseline_sources_cache, list):
            for s in baseline_sources_cache:
                if not isinstance(s, dict):
                    continue
                u = s.get("source_url") or s.get("url")
                ex = s.get("extracted_numbers")
                if u and isinstance(ex, list):
                    _kept.append(s)
        _kept.sort(key=lambda d: (str(d.get("source_url") or d.get("url") or ""), str(d.get("fingerprint") or "")))
        baseline_sources_cache = _kept
        _snapshot_debug = {
            "origin": snapshot_origin,
            "raw_count": _raw_len,
            "valid_count": int(len(baseline_sources_cache)),
            "example_urls": [x.get("source_url") or x.get("url") for x in (baseline_sources_cache[:3] if isinstance(baseline_sources_cache, list) else [])],
            "prev_keys": sorted(list(previous_data.keys()))[:40] if isinstance(previous_data, dict) else [],
        }
    except Exception:
        pass

    # 3) reconstruct from web_context.scraped_meta (if provided)
    if (not baseline_sources_cache) and isinstance(web_context, dict):
        try:
            scraped_meta = web_context.get("scraped_meta") or {}
            rebuilt = []
            if isinstance(scraped_meta, dict):
                for url, meta in scraped_meta.items():
                    if not isinstance(meta, dict):
                        continue
                    content = meta.get("clean_text") or meta.get("content") or ""
                    fp = meta.get("fingerprint") or _fingerprint(content)
                    if not fp or len(content or "") < 800:
                        continue
                    nums = meta.get("extracted_numbers") or []
                    if not isinstance(nums, list):
                        nums = []
                    rebuilt.append({
                        "url": url,
                        "status": meta.get("status") or "fetched",
                        "status_detail": meta.get("status_detail") or "",
                        "numbers_found": _safe_int(meta.get("numbers_found"), default=len(nums)),
                        "fetched_at": meta.get("fetched_at") or _now(),
                        "fingerprint": fp,
                        "content_type": meta.get("content_type") or "",
                        "extracted_numbers": [
                            {
                                "value": n.get("value"),
                                "unit": n.get("unit"),
                                "raw": n.get("raw"),
                                "context_snippet": (n.get("context_snippet") or n.get("context") or "")[:200],
                                "anchor_hash": n.get("anchor_hash"),
                                "is_junk": n.get("is_junk"),
                                "junk_reason": n.get("junk_reason"),
                                "unit_tag": n.get("unit_tag"),
                                "unit_family": n.get("unit_family"),
                                "base_unit": n.get("base_unit"),
                                "multiplier_to_base": n.get("multiplier_to_base"),
                                "value_norm": n.get("value_norm"),
                                "start_idx": n.get("start_idx"),
                                "end_idx": n.get("end_idx"),
                                "source_url": n.get("source_url") or url,
                            }
                            for n in nums if isinstance(n, dict)
                        ]
                    })
            if rebuilt:
                baseline_sources_cache = rebuilt
                snapshot_origin = "web_context_scraped_meta"
        except Exception:
            pass

    # Also count invalid snapshots for debug (if present)
    invalid_count = 0
    try:
        if isinstance(previous_data, dict):
            r = previous_data.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache_invalid"), list):
                invalid_count = len(r.get("baseline_sources_cache_invalid") or [])
    except Exception:
        pass
        invalid_count = 0

    output = {
        "status": "success",
        "message": "",
        "sources_checked": 0,
        "sources_fetched": 0,
        "numbers_extracted_total": 0,
        "stability_score": 0.0,
        "summary": {
            "total_metrics": 0,
            "metrics_found": 0,
            "metrics_increased": 0,
            "metrics_decreased": 0,
            "metrics_unchanged": 0,
        },
        "metric_changes": [],
        "source_results": [],
        "interpretation": "",
        "snapshot_origin": snapshot_origin,
        "valid_snapshot_count": len(baseline_sources_cache or []),
        "invalid_snapshot_count": int(invalid_count),
        "generated_at": _now(),
    }

    try:
        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict) and isinstance(locals().get("_refactor89_prev_payload_probe_v1"), dict):
            output["debug"]["prev_payload_probe_v1"] = dict(locals().get("_refactor89_prev_payload_probe_v1") or {})
    except Exception:
        pass

    # REFACTOR129: attach precision/overlap beacons (always-on, compact)
    try:
        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            if isinstance(globals().get("_REFACTOR129_DECIMAL_OVERLAP_SUPPRESSION_V1"), dict):
                output["debug"]["decimal_overlap_suppression_v1"] = dict(globals().get("_REFACTOR129_DECIMAL_OVERLAP_SUPPRESSION_V1") or {})
            if isinstance(globals().get("_REFACTOR129_PRECISION_TIEBREAK_V1"), dict):
                output["debug"]["precision_tiebreak_v1"] = dict(globals().get("_REFACTOR129_PRECISION_TIEBREAK_V1") or {})
            if isinstance(globals().get("_REFACTOR131_INJECTED_SEMANTIC_OVERRIDE_V1"), dict):
                output["debug"]["injected_semantic_override_v1"] = dict(globals().get("_REFACTOR131_INJECTED_SEMANTIC_OVERRIDE_V1") or {})
            if isinstance(globals().get("_REFACTOR132_INJECTED_CAGR_RESCUE_V1"), dict):
                output["debug"]["injected_cagr_rescue_v1"] = dict(globals().get("_REFACTOR132_INJECTED_CAGR_RESCUE_V1") or {})
    except Exception:
        pass

    # - Always stamp CODE_VERSION into output
    # - Create output['debug'] container (non-breaking)
    # - Track active Diff Panel V2 entrypoint deterministically
    try:
        output["code_version"] = _yureeka_get_code_version()
    except Exception:
        pass
    try:
        _yureeka_lock_version_globals_v1()
        if not isinstance(output.get("debug"), dict):
            output["debug"] = {}

        # Diff Panel V2 (authoritative for metric_changes_v2)
        _fn_v2 = None
        _bf_v2 = ""
        try:
            _cand = globals().get("build_diff_metrics_panel_v2__rows_refactor47")
        except Exception:
            _cand = None
        if callable(_cand):
            _fn_v2 = _cand
            _bf_v2 = "build_diff_metrics_panel_v2__rows_refactor47"
        try:
            if _bf_v2:
                globals()["_YUREEKA_DIFF_PANEL_V2_ENTRYPOINT_BOUND_FROM"] = _bf_v2
        except Exception:
            pass

        _man = {
            "code_version": _yureeka_get_code_version(),
            "authority_manifest_v1": _yureeka_authority_manifest_v1(),
            "final_bindings_version": _yureeka_get_code_version(),
            "legacy_diff_binding_removed": True,

            "diff_panel_v2_entrypoint_name": str(getattr(_fn_v2, "__name__", "") or ""),
            "diff_panel_v2_entrypoint_qualname": str(getattr(_fn_v2, "__qualname__", "") or ""),
            "diff_panel_v2_entrypoint_module": str(getattr(_fn_v2, "__module__", "") or ""),
            "diff_panel_v2_entrypoint_id": str(id(_fn_v2)) if _fn_v2 is not None else "",
            "diff_panel_v2_entrypoint_bound_from": str(_bf_v2 or ""),
        }

        if not _man.get("diff_panel_v2_entrypoint_name"):
            _man["warning_v2_missing"] = "Diff Panel V2 entrypoint not resolved at manifest time (possible early Streamlit trigger before later defs)."

        output["debug"]["binding_manifest_v1"] = _man
    except Exception:
        pass

    try:
        if not isinstance(output.get("debug"), dict):
            output["debug"] = {}
        output["debug"].setdefault("fix35", {})
        output["debug"]["fix35"]["current_metrics_origin"] = "unknown"
        output["debug"]["fix35"]["fastpath_eligible"] = False
        output["debug"]["fix35"]["fastpath_reason"] = ""
    except Exception:
        pass

    # Attach debug flags (rehydration + snapshot_debug)
    try:
        if _prev_rehydrated:
            output["previous_data_rehydrated"] = True
            output["previous_data_full_store_ref"] = _prev_rehydrated_ref
    except Exception:
        pass
    try:
        if isinstance(_snapshot_debug, dict) and _snapshot_debug:
            output["snapshot_debug"] = _snapshot_debug
    except Exception:
        pass

    try:
        if not baseline_sources_cache and isinstance(previous_data, dict):
            _ref = previous_data.get("snapshot_store_ref") or (previous_data.get("results") or {}).get("snapshot_store_ref")
            _hash = previous_data.get("source_snapshot_hash") or (previous_data.get("results") or {}).get("source_snapshot_hash")

            # Why:
            # - Analysis now emits stable/v2 snapshot hashes (source_snapshot_hash_v2 / _stable) and
            #   snapshot_store_ref_v2 pointing at the same Snapshots row key.
            # - Evolution must prefer these fields to keep fastpath alignment intact.
            try:
                _ref_v2 = previous_data.get("snapshot_store_ref_v2") or previous_data.get("snapshot_store_ref_stable")
                if (not _ref_v2) and isinstance(previous_data.get("results"), dict):
                    _ref_v2 = (previous_data.get("results") or {}).get("snapshot_store_ref_v2") or (previous_data.get("results") or {}).get("snapshot_store_ref_stable")
                if isinstance(_ref_v2, str) and _ref_v2:
                    _ref = _ref_v2  # prefer stable ref
            except Exception:
                pass

            try:
                _hash_stable = previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2")
                if (not _hash_stable) and isinstance(previous_data.get("results"), dict):
                    _hash_stable = (previous_data.get("results") or {}).get("source_snapshot_hash_stable") or (previous_data.get("results") or {}).get("source_snapshot_hash_v2")
                if isinstance(_hash_stable, str) and _hash_stable:
                    _hash = _hash_stable  # prefer stable hash
            except Exception:
                pass

            if isinstance(_ref, str) and _ref.startswith("gsheet:"):
                parts = _ref.split(":")
                _ws_title = parts[1] if len(parts) > 1 and parts[1] else "Snapshots"
                _h = parts[2] if len(parts) > 2 else ""
                baseline_sources_cache = load_full_snapshots_from_sheet(_h, worksheet_title=_ws_title) if _h else []
                if baseline_sources_cache:
                    output["snapshot_origin"] = "sheet_snapshot_store_ref"

            if not baseline_sources_cache and isinstance(_hash, str) and _hash:
                baseline_sources_cache = load_full_snapshots_from_sheet(_hash, worksheet_title="Snapshots")
                if baseline_sources_cache:
                    output["snapshot_origin"] = "sheet_source_snapshot_hash"

            if not baseline_sources_cache and isinstance(_ref, str) and _ref and not _ref.startswith("gsheet:"):
                baseline_sources_cache = load_full_snapshots_local(_ref)
                if baseline_sources_cache:
                    output["snapshot_origin"] = "local_snapshot_store_ref"

            if isinstance(baseline_sources_cache, list):
                output["valid_snapshot_count"] = len(baseline_sources_cache)
    except Exception:
        pass

    # Purpose:
    # - During HistoryFull persistence we may omit baseline_sources_cache to avoid Sheets cell limits,
    #   and instead persist snapshots in the Snapshots worksheet / local snapshot store with a ref/hash.
    # - Source-anchored evolution MUST be able to rehydrate baseline snapshots from:
    #     * snapshot_store_ref / snapshot_store_ref_v2 (gsheet:Snapshots:<hash> OR local path)
    #     * source_snapshot_hash_v2 / source_snapshot_hash
    # Behavior:
    # - If baseline_sources_cache is empty after normal discovery, attempt to load snapshots deterministically.
    # - Still strict: if we cannot load snapshots, we remain snapshot-gated (no fabricated matches).
    _snapshot_store_debug = {}
    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            _res = previous_data.get("results") if isinstance(previous_data.get("results"), dict) else {}
            _pr = previous_data.get("primary_response") if isinstance(previous_data.get("primary_response"), dict) else {}
            _pr_res = _pr.get("results") if isinstance(_pr.get("results"), dict) else {}

            # Prefer explicit refs; fall back to hashes
            _store_ref = (
                previous_data.get("snapshot_store_ref")
                or (_res.get("snapshot_store_ref") if isinstance(_res, dict) else "")
                or (_pr.get("snapshot_store_ref") if isinstance(_pr, dict) else "")
                or (_pr_res.get("snapshot_store_ref") if isinstance(_pr_res, dict) else "")
                or ""
            )

            _store_ref_v2 = (
                previous_data.get("snapshot_store_ref_v2")
                or (_res.get("snapshot_store_ref_v2") if isinstance(_res, dict) else "")
                or (_pr.get("snapshot_store_ref_v2") if isinstance(_pr, dict) else "")
                or (_pr_res.get("snapshot_store_ref_v2") if isinstance(_pr_res, dict) else "")
                or ""
            )

            _ssh_v2 = (
                previous_data.get("source_snapshot_hash_v2")
                or (_res.get("source_snapshot_hash_v2") if isinstance(_res, dict) else "")
                or (_pr.get("source_snapshot_hash_v2") if isinstance(_pr, dict) else "")
                or (_pr_res.get("source_snapshot_hash_v2") if isinstance(_pr_res, dict) else "")
                or ""
            )

            _ssh_v1 = (
                previous_data.get("source_snapshot_hash")
                or (_res.get("source_snapshot_hash") if isinstance(_res, dict) else "")
                or (_pr.get("source_snapshot_hash") if isinstance(_pr, dict) else "")
                or (_pr_res.get("source_snapshot_hash") if isinstance(_pr_res, dict) else "")
                or ""
            )

            def _extract_hash(ref: str) -> str:
                try:
                    ref = str(ref or "")
                    if ref.startswith("gsheet:Snapshots:"):
                        return ref.split(":")[-1]
                except Exception:
                    pass
                return ""

            _hash_to_load = _extract_hash(_store_ref_v2) or _extract_hash(_store_ref) or str(_ssh_v2 or "") or str(_ssh_v1 or "")

            _snapshot_store_debug = {
                "store_ref": str(_store_ref or ""),
                "store_ref_v2": str(_store_ref_v2 or ""),
                "ssh_v2_present": bool(_ssh_v2),
                "ssh_v1_present": bool(_ssh_v1),
                "hash_to_load": str(_hash_to_load or ""),
            }

            _loaded = []
            _loaded_origin = ""

            # 1) Load by explicit gsheet ref (v2 then v1)
            if isinstance(_store_ref_v2, str) and _store_ref_v2.startswith("gsheet:Snapshots:"):
                try:
                    _loaded = load_full_snapshots_from_sheet(_store_ref_v2.split(":")[-1], worksheet_title="Snapshots")
                    _loaded_origin = "gsheet_ref_v2"
                except Exception:
                    pass

            if (not _loaded) and isinstance(_store_ref, str) and _store_ref.startswith("gsheet:Snapshots:"):
                try:
                    _loaded = load_full_snapshots_from_sheet(_store_ref.split(":")[-1], worksheet_title="Snapshots")
                    _loaded_origin = "gsheet_ref_v1"
                except Exception:
                    pass

            # 2) Load by local store ref if it looks like a path
            if (not _loaded) and isinstance(_store_ref, str) and _store_ref and (not _store_ref.startswith("gsheet:")):
                try:
                    _loaded = load_full_snapshots_local(_store_ref)
                    _loaded_origin = "local_ref"
                except Exception:
                    pass

            # 3) Load by hash (sheet first, then deterministic local path)
            if (not _loaded) and _hash_to_load:
                try:
                    _loaded = load_full_snapshots_from_sheet(str(_hash_to_load), worksheet_title="Snapshots")
                    _loaded_origin = "gsheet_hash"
                except Exception:
                    pass

            if (not _loaded) and _hash_to_load:
                try:
                    # Deterministic path used by store_full_snapshots_local
                    _p = os.path.join(_snapshot_store_dir(), f"{str(_hash_to_load)}.json")
                    _loaded = load_full_snapshots_local(_p)
                    if _loaded:
                        _loaded_origin = "local_hash_path"
                except Exception:
                    pass

            if isinstance(_loaded, list) and _loaded:
                baseline_sources_cache = _loaded
                snapshot_origin = f"snapshot_store_fallback:{_loaded_origin}"
                _snapshot_store_debug["loaded_count"] = int(len(_loaded))
                _snapshot_store_debug["loaded_origin"] = str(_loaded_origin)
            else:
                _snapshot_store_debug["loaded_count"] = 0
                _snapshot_store_debug["loaded_origin"] = str(_loaded_origin or "none")
    except Exception:
        pass

    # Re-validate snapshot shape after fallback load (keeps strict invariants)
    try:
        if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
            _kept2 = []
            for s in baseline_sources_cache:
                if not isinstance(s, dict):
                    continue
                u = s.get("source_url") or s.get("url")
                ex = s.get("extracted_numbers")
                # Some legacy stores use "numbers" instead of "extracted_numbers"
                if ex is None and isinstance(s.get("numbers"), list):
                    try:
                        s["extracted_numbers"] = s.get("numbers") or []
                        ex = s.get("extracted_numbers")
                    except Exception:
                        pass
                if u and isinstance(ex, list):
                    _kept2.append(s)
            _kept2.sort(key=lambda d: (str(d.get("source_url") or d.get("url") or ""), str(d.get("fingerprint") or "")))
            baseline_sources_cache = _kept2
            # Update debug if available
            if isinstance(_snapshot_debug, dict):
                _snapshot_debug["origin"] = snapshot_origin
                _snapshot_debug["valid_count"] = int(len(baseline_sources_cache))
    except Exception:
        pass

    # If no valid snapshots, return "not_found"
    if not baseline_sources_cache:
        try:
            output.setdefault("debug", {})
            if isinstance(output.get("debug"), dict):
                if isinstance(_snapshot_debug, dict):
                    output["debug"]["snapshot_debug_v1"] = _snapshot_debug
                if isinstance(_snapshot_store_debug, dict) and _snapshot_store_debug:
                    output["debug"]["snapshot_store_debug_v1"] = _snapshot_store_debug
        except Exception:
            pass
        output["status"] = "failed"
        output["message"] = "No valid snapshots available for source-anchored evolution. (Snapshot store fallback attempted; no re-fetch / no heuristic matching performed.)"
        output["interpretation"] = "Snapshot-gated: evolution refused to fabricate matches without valid cached source text."
        return output    # ---------- Use your existing deterministic metric diff helper ----------
    prev_response = (previous_data or {}).get("primary_response", {}) or {}

    try:
        if (not isinstance(prev_response, dict) or not prev_response) and isinstance(previous_data, dict):
            if isinstance(previous_data.get("primary_metrics_canonical"), dict) or isinstance(previous_data.get("metric_schema_frozen"), dict):
                prev_response = previous_data
    except Exception:
        pass

    prev_metrics = prev_response.get("primary_metrics_canonical") or prev_response.get("primary_metrics") or {}

    # (safe alias for prior `prev_analysis` usage)
    prev_analysis = previous_data  # PATCH CSR_INPUTS1_ALIAS (ADDITIVE)
    try:
        prev_schema = _first_present(prev_analysis, [
            ("metric_schema_frozen",),
            ("primary_response", "metric_schema_frozen"),
            ("results", "metric_schema_frozen"),
            ("results", "primary_response", "metric_schema_frozen"),
        ], default=None)

        prev_canon = _first_present(prev_analysis, [
            ("primary_metrics_canonical",),
            ("primary_response", "primary_metrics_canonical"),
            ("results", "primary_metrics_canonical"),
            ("results", "primary_response", "primary_metrics_canonical"),
        ], default=None)

        prev_anchors = _first_present(prev_analysis, [
            ("metric_anchors",),
            ("primary_response", "metric_anchors"),
            ("results", "metric_anchors"),
            ("results", "primary_response", "metric_anchors"),
        ], default=None)

        if (not isinstance(prev_schema, dict) or not prev_schema) and isinstance(prev_canon, dict) and prev_canon:
            try:
                fn = globals().get("freeze_metric_schema")
                if callable(fn):
                    prev_schema = fn(prev_canon)
            except Exception:
                pass
    except Exception:
        pass

    # Why:
    # - Diff Panel V2 consumes prev_response.primary_metrics_canonical.
    # - HistoryFull rehydrate can place canonical metrics under nested containers
    #   (e.g., previous_data.results.primary_metrics_canonical), leaving prev_response empty.
    # What:
    # - If prev_canon exists, copy into prev_response.primary_metrics_canonical when missing.
    # - Also expose at top-level previous_data.primary_metrics_canonical (debug/compat).
    # - Record debug counts for closure verification.
    try:
        if isinstance(prev_response, dict):
            if (not isinstance(prev_response.get("primary_metrics_canonical"), dict)) or (not prev_response.get("primary_metrics_canonical")):
                if isinstance(prev_canon, dict) and prev_canon:
                    prev_response["primary_metrics_canonical"] = prev_canon
        if isinstance(previous_data, dict):
            if (not isinstance(previous_data.get("primary_metrics_canonical"), dict)) or (not previous_data.get("primary_metrics_canonical")):
                if isinstance(prev_canon, dict) and prev_canon:
                    previous_data["primary_metrics_canonical"] = prev_canon
    except Exception:
        pass

    try:
        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            output["debug"].setdefault("fix2d73", {})
            if isinstance(output["debug"].get("fix2d73"), dict):
                output["debug"]["fix2d73"].update({
                    "prev_canon_count": int(len(prev_canon)) if isinstance(prev_canon, dict) else 0,
                    "prev_response_pmc_count": int(len(prev_response.get("primary_metrics_canonical") or {})) if isinstance(prev_response, dict) and isinstance(prev_response.get("primary_metrics_canonical"), dict) else 0,
                    "previous_data_top_pmc_count": int(len(previous_data.get("primary_metrics_canonical") or {})) if isinstance(previous_data, dict) and isinstance(previous_data.get("primary_metrics_canonical"), dict) else 0,
                })
    except Exception:
        pass
    # Ensure schema/anchors are available inside prev_response (additive copies)
    try:
        if isinstance(prev_response, dict) and not isinstance(prev_response.get("metric_schema_frozen"), dict):
            if isinstance(previous_data.get("metric_schema_frozen"), dict):
                prev_response["metric_schema_frozen"] = previous_data.get("metric_schema_frozen")
    except Exception:
        pass
    try:
        if isinstance(prev_response, dict) and not isinstance(prev_response.get("metric_anchors"), dict):
            if isinstance(previous_data.get("metric_anchors"), dict):
                prev_response["metric_anchors"] = previous_data.get("metric_anchors")
    except Exception:
        pass

    #
    # Principle:
    #   If the source snapshot inputs are proven unchanged, do NOT perform any
    #   anchor-based selection or rebuild "gymnastics". Reuse the already
    #   processed + schema-gated metrics from the previous analysis payload and
    #   publish directly.
    #
    # Implementation notes:
    #   - We compute a stable hash from baseline_sources_cache[*].extracted_numbers
    #     using a reduced, order-independent projection.
    #   - If it matches previous_data/source_snapshot_hash AND a prior processed
    #     canonical metrics dict exists, we set current_metrics to prev_metrics
    #     and force anchors to be ignored by short-circuiting _get_metric_anchors().
    #   - This is purely additive and does not remove legacy paths.
    _fix31_authoritative_reuse = False
    try:
        def _fix31_stable_dumps(obj):
            try:
                return json.dumps(obj, sort_keys=True, ensure_ascii=False, separators=(",", ":"))
            except Exception:
                pass
                # last resort
                return str(obj)

        def _fix31_snapshot_fingerprint(bsc):
            # Reduced projection: stable across benign field additions/ordering
            rows = []
            for sr in (bsc or []):
                if not isinstance(sr, dict):
                    continue
                u = sr.get("source_url") or sr.get("url") or ""
                nums = []
                for n in (sr.get("extracted_numbers") or []):
                    if not isinstance(n, dict):
                        continue
                    nums.append({
                        "anchor_hash": n.get("anchor_hash") or "",
                        "value_norm": n.get("value_norm"),
                        "unit_tag": n.get("unit_tag") or "",
                        "unit": n.get("unit") or n.get("unit_norm") or "",
                        "currency": n.get("currency") or n.get("currency_symbol") or "",
                        "is_percent": bool(n.get("is_percent") or n.get("has_percent")),
                        "is_junk": bool(n.get("is_junk")),
                    })
                # order-independent for candidates
                nums = sorted(nums, key=lambda x: (_fix31_stable_dumps(x)))
                rows.append({"source_url": u, "extracted_numbers": nums})
            rows = sorted(rows, key=lambda r: r.get("source_url") or "")
            payload = _fix31_stable_dumps(rows).encode("utf-8", errors="ignore")
            return hashlib.sha256(payload).hexdigest()

        # - Use the SAME hash function as analysis (compute_source_snapshot_hash_v2) whenever possible.
        # - Falls back to legacy compute_source_snapshot_hash, then to the reduced fingerprint.
        def _fix37_snapshot_hash_stable(bsc):
            try:
                if isinstance(bsc, list) and bsc:
                    try:
                        _h2 = compute_source_snapshot_hash_v2(bsc)
                        if _h2:
                            return str(_h2)
                    except Exception:
                        pass
                    try:
                        _h1 = compute_source_snapshot_hash(bsc)
                        if _h1:
                            return str(_h1)
                    except Exception:
                        pass
            except Exception:
                return _fix31_snapshot_fingerprint(bsc)

        _prev_hash = None
        _prev_hash_stable = None
        if isinstance(previous_data, dict):
            _prev_hash_stable = previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2")
            try:
                if not _prev_hash_stable and isinstance(previous_data.get("results"), dict):
                    _prev_hash_stable = (previous_data.get("results") or {}).get("source_snapshot_hash_stable") or (previous_data.get("results") or {}).get("source_snapshot_hash_v2")
            except Exception:
                pass
            _prev_hash = _prev_hash_stable or previous_data.get("source_snapshot_hash")
            _prev_hash_pref = _prev_hash_stable or previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2") or _prev_hash

            try:
                if not _prev_hash and isinstance(previous_data.get("results"), dict):
                    _prev_hash = (previous_data.get("results") or {}).get("source_snapshot_hash")
            except Exception:
                pass

        # - Record current/previous hashes even on mismatch
        # - Explain which prerequisite failed (no_prev_hash / no_prev_metrics / no_snapshots / hash_mismatch)
        _fix36_cur_hash = None
        _fix36_reason = ""

        #
        # Why:
        # - In your latest evolution JSON, fastpath was correctly bypassed due to injected delta,
        #   but the current snapshot universe (baseline_sources_cache) still did NOT include the
        #   injected URL, so the stable hash still matched and downstream logic treated the run
        #   as "no delta" (no fetch, no rebuild, no injected lifecycle).
        #
        # Goal:
        # - BEFORE computing the current stable hash / fastpath eligibility, deterministically
        #   append placeholder snapshot rows for any injected URLs missing from the current
        #   baseline_sources_cache universe. This makes the hash differ (as it should when the
        #   source universe changes), forcing the normal rebuild/fetch pathways without changing
        #   the hashing algorithm itself.
        #
        # Safety:
        # - Purely additive.
        # - No effect when no injected URLs are present OR all injected URLs already exist in
        #   baseline_sources_cache.
        try:
            _fx15_wc = web_context if isinstance(web_context, dict) else {}
            _fx15_extra = []
            # Prefer already-wired list fields
            if isinstance(_fx15_wc.get("extra_urls"), (list, tuple)):
                _fx15_extra = list(_fx15_wc.get("extra_urls") or [])
            elif isinstance(_fx15_wc.get("diag_extra_urls_ui"), (list, tuple)) and _fx15_wc.get("diag_extra_urls_ui"):
                _fx15_extra = list(_fx15_wc.get("diag_extra_urls_ui") or [])
            elif isinstance(_fx15_wc.get("diag_extra_urls_ui_raw"), str) and (_fx15_wc.get("diag_extra_urls_ui_raw") or "").strip():
                _raw = str(_fx15_wc.get("diag_extra_urls_ui_raw") or "")
                _parts = []
                for _line in _raw.splitlines():
                    _line = (_line or "").strip()
                    if not _line:
                        continue
                    for _p in _line.split(","):
                        _p = (_p or "").strip()
                        if _p:
                            _parts.append(_p)
                _fx15_extra = _parts

            _fx15_inj_norm = _inj_diag_norm_url_list(_fx15_extra) if _fx15_extra else []
            if _fx15_inj_norm and isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                _fx15_base_urls = []
                for _r in (baseline_sources_cache or []):
                    if not isinstance(_r, dict):
                        continue
                    _u = _r.get("source_url") or _r.get("url") or ""
                    if isinstance(_u, str) and _u:
                        _fx15_base_urls.append(_u)
                _fx15_base_set = set(_inj_diag_norm_url_list(_fx15_base_urls)) if _fx15_base_urls else set()
                _fx15_delta = [u for u in _fx15_inj_norm if u and u not in _fx15_base_set]
                if _fx15_delta:
                    # Append stable placeholders so the snapshot hash changes deterministically.
                    for _u in _fx15_delta:
                        baseline_sources_cache.append({
                            "source_url": _u,
                            "url": _u,
                            "status": "injected_pending",
                            "status_detail": "injected_url_placeholder_pre_hash",
                            "snapshot_text": "",
                            "extracted_numbers": [],
                            "numbers_found": 0,
                            "injected": True,
                            "injected_reason": "prehash_placeholder",
                        })
                    # Also ensure downstream sees a consistent universe via web_context["extra_urls"].
                    try:
                        if isinstance(_fx15_wc, dict):
                            _fx15_wc.setdefault("extra_urls", [])
                            if isinstance(_fx15_wc.get("extra_urls"), list):
                                # keep original order; append unique normalized
                                _seen = set(_inj_diag_norm_url_list(_fx15_wc.get("extra_urls") or []))
                                for _u in _fx15_delta:
                                    if _u not in _seen:
                                        _fx15_wc["extra_urls"].append(_u)
                                        _seen.add(_u)
                    except Exception:
                        pass
                    # Debug
                    try:
                        output.setdefault("debug", {})
                        if isinstance(output.get("debug"), dict):
                            output["debug"].setdefault("fix41afc15", {})
                            if isinstance(output["debug"].get("fix41afc15"), dict):
                                output["debug"]["fix41afc15"].update({
                                    "inj_norm_count": int(len(_fx15_inj_norm)),
                                    "inj_norm": list(_fx15_inj_norm),
                                    "inj_delta_count": int(len(_fx15_delta)),
                                    "inj_delta": list(_fx15_delta),
                                    "baseline_sources_cache_count_after_placeholder": int(len(baseline_sources_cache or [])),
                                })
                    except Exception:
                        pass
        except Exception:
            pass

        #
        # Observed gap (from evolution JSON):
        #   - Injected URLs were present in ui/intake/hash_inputs, but remained:
        #       status = "injected_pending" / status_detail = "injected_url_placeholder_pre_hash"
        #   - So they never produced snapshot_text / extracted_numbers, and thus could not
        #     influence metric rebuild beyond a "hash universe" delta.
        #
        # Goal:
        #   - When injection is present, attempt to fetch+extract the injected URLs (delta-only),
        #     and update their baseline_sources_cache rows in-place so downstream rebuild sees them
        #     like normal fetched sources (or explicit failed reasons).
        #
        # Safety:
        #   - Purely additive.
        #   - No effect when no injected URLs are present.
        #   - Only touches rows that are injected placeholders (status == injected_pending) OR
        #     URLs that are injected_delta (not already in baseline).
        try:
            _fx16_wc = web_context if isinstance(web_context, dict) else {}
            _fx16_extra = []
            if isinstance(_fx16_wc.get("extra_urls"), (list, tuple)) and _fx16_wc.get("extra_urls"):
                _fx16_extra = list(_fx16_wc.get("extra_urls") or [])
            elif isinstance(_fx16_wc.get("diag_extra_urls_ui"), (list, tuple)) and _fx16_wc.get("diag_extra_urls_ui"):
                _fx16_extra = list(_fx16_wc.get("diag_extra_urls_ui") or [])
            elif isinstance(_fx16_wc.get("diag_extra_urls_ui_raw"), str) and (_fx16_wc.get("diag_extra_urls_ui_raw") or "").strip():
                _raw = str(_fx16_wc.get("diag_extra_urls_ui_raw") or "")
                _parts = []
                for _line in _raw.splitlines():
                    _line = (_line or "").strip()
                    if not _line:
                        continue
                    for _p in _line.split(","):
                        _p = (_p or "").strip()
                        if _p:
                            _parts.append(_p)
                _fx16_extra = _parts

            _fx16_inj_norm = _inj_diag_norm_url_list(_fx16_extra) if _fx16_extra else []
            _fx16_base_urls = []
            if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                for _r in (baseline_sources_cache or []):
                    if not isinstance(_r, dict):
                        continue
                    _u = _r.get("source_url") or _r.get("url") or ""
                    if isinstance(_u, str) and _u:
                        _fx16_base_urls.append(_u)
            _fx16_base_set = set(_inj_diag_norm_url_list(_fx16_base_urls)) if _fx16_base_urls else set()
            _fx16_delta = [u for u in _fx16_inj_norm if u and u not in _fx16_base_set]

            # Identify placeholder rows that should be fetched
            _fx16_targets = []
            if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                for _r in (baseline_sources_cache or []):
                    if not isinstance(_r, dict):
                        continue
                    _u = _r.get("source_url") or _r.get("url") or ""
                    _u_norm = _inj_diag_norm_url_list([_u])[0] if isinstance(_u, str) and _u else ""
                    if not _u_norm:
                        continue
                    if _r.get("status") == "injected_pending":
                        _fx16_targets.append((_u_norm, _r, "placeholder_row"))
                    elif _u_norm in _fx16_delta:
                        _fx16_targets.append((_u_norm, _r, "delta_row"))

            # Also cover the case where placeholders were not appended (defensive)
            for _u in (_fx16_delta or []):
                if not isinstance(baseline_sources_cache, list):
                    continue
                if any((_inj_diag_norm_url_list([(_r.get("source_url") or _r.get("url") or "")])[0] if isinstance(_r, dict) else "") == _u for _r in (baseline_sources_cache or [])):
                    continue
                baseline_sources_cache.append({
                    "source_url": _u,
                    "url": _u,
                    "status": "injected_pending",
                    "status_detail": "injected_url_placeholder_pre_hash",
                    "snapshot_text": "",
                    "extracted_numbers": [],
                    "numbers_found": 0,
                    "injected": True,
                    "injected_reason": "fx16_defensive_placeholder",
                })
                _fx16_targets.append((_u, baseline_sources_cache[-1], "defensive_placeholder"))

            # Fetch+extract for targets (best-effort)
            _fx16_fetched = []
            _fx16_failed = []
            if _fx16_targets:
                for (_u_norm, _row, _why) in _fx16_targets:
                    # Skip if row already has text/numbers (idempotent)
                    try:
                        if isinstance(_row.get("snapshot_text"), str) and _row.get("snapshot_text").strip():
                            continue
                        if isinstance(_row.get("extracted_numbers"), list) and len(_row.get("extracted_numbers") or []) > 0:
                            continue
                    except Exception:
                        pass

                    _txt = None
                    _detail = ""
                    try:
                        _txt, _detail = fetch_url_content_with_status(_u_norm, timeout=25)
                    except Exception as _e:
                        _txt, _detail = None, f"exception:{type(_e).__name__}"

                    if _txt and isinstance(_txt, str) and len(_txt.strip()) >= 200:
                        # FIX2D69A: ensure numeric extraction runs on snapshot_text, with HTML->text fallback and auditable debug
                        _extract_text = _txt
                        try:
                            _t0 = str(_txt or '')
                            _looks_html = ('<' in _t0 and '>' in _t0 and ('</' in _t0 or '<html' in _t0.lower() or '<body' in _t0.lower()))
                            if _looks_html:
                                try:
                                    _extract_text = BeautifulSoup(_t0, 'html.parser').get_text(' ')
                                except Exception:
                                    _extract_text = re.sub(r'<[^>]+>', ' ', _t0)
                            if not isinstance(_extract_text, str):
                                _extract_text = str(_extract_text or '')
                            _extract_text = re.sub(r'\s+', ' ', _extract_text).strip()
                            if not _extract_text:
                                _extract_text = _t0
                        except Exception:
                            _extract_text = _txt

                        _nums = []
                        _fx69_errors = []
                        _callable = bool(callable(extract_numbers_with_context))
                        try:
                            _row['fix2d68_extract_attempted'] = bool(_callable)
                            _row['fix2d68_extract_input_len'] = int(len(_extract_text) if isinstance(_extract_text, str) else 0)
                            _row['fix2d68_extract_input_head'] = (_extract_text[:200] if isinstance(_extract_text, str) else '')
                        except Exception:
                            pass
                        if _callable:
                            for _mode in ('source_url', 'url', 'plain'):
                                try:
                                    _tmp = None
                                    if _mode == 'source_url':
                                        _tmp = extract_numbers_with_context(_extract_text, source_url=_u_norm)
                                    elif _mode == 'url':
                                        _tmp = extract_numbers_with_context(_extract_text, url=_u_norm)
                                    else:
                                        _tmp = extract_numbers_with_context(_extract_text)
                                    # normalize extractor return
                                    if _tmp is None:
                                        _nums = []
                                    elif isinstance(_tmp, list):
                                        _nums = _tmp
                                    elif isinstance(_tmp, tuple) and len(_tmp) >= 1 and isinstance(_tmp[0], list):
                                        _nums = _tmp[0]
                                    elif isinstance(_tmp, dict) and isinstance(_tmp.get('extracted_numbers'), list):
                                        _nums = _tmp.get('extracted_numbers') or []
                                    else:
                                        _nums = []
                                    _row['fix2d68_extract_call_mode'] = _mode
                                    break
                                except Exception as _e:
                                    _fx69_errors.append({'mode': _mode, 'error': repr(_e)})
                                    _nums = []
                        if _fx69_errors:
                            _row['fix2d68_extract_errors'] = _fx69_errors

                        _row.update({
                            'status': 'fetched',
                            'status_detail': (_detail or 'success'),
                            'snapshot_text': _txt[:7000],
                            'extracted_numbers': _nums,
                            'numbers_found': int(len(_nums or [])),
                            'injected': True,
                            'injected_reason': _row.get('injected_reason') or 'fx16_fetch_and_extract',
                        })
                        _fx16_fetched.append({
                            'url': _u_norm, 'why': _why, 'numbers_found': int(len(_nums or [])), 'status_detail': (_detail or 'success')
                        })
                    else:
                        _row.update({
                            'status': 'failed',
                            'status_detail': (_detail or 'failed:no_text'),
                            'snapshot_text': '',
                            'extracted_numbers': [],
                            'numbers_found': 0,
                            'injected': True,
                            'injected_reason': _row.get('injected_reason') or 'fx16_fetch_failed',
                        })
                        _fx16_failed.append({
                            'url': _u_norm, 'why': _why, 'status_detail': (_detail or 'failed:no_text')
                        })

            # Emit debug
            try:
                output.setdefault("debug", {})
                if isinstance(output.get("debug"), dict):
                    output["debug"].setdefault("fix41afc16", {})
                    if isinstance(output["debug"].get("fix41afc16"), dict):
                        output["debug"]["fix41afc16"].update({
                            "inj_norm_count": int(len(_fx16_inj_norm or [])),
                            "inj_delta_count": int(len(_fx16_delta or [])),
                            "fetch_target_count": int(len(_fx16_targets or [])),
                            "fetched_count": int(len(_fx16_fetched or [])),
                            "failed_count": int(len(_fx16_failed or [])),
                            "fetched": list(_fx16_fetched or []),
                            "failed": list(_fx16_failed or []),
                        })
            except Exception:
                pass
        except Exception:
            pass

        #
        # Observed gap (from evolution JSON after FIX41AFC16):
        #   - Injected URL reaches intake/admitted/attempted/hash_inputs, but snapshot_debug remains empty
        #     (origin none / raw_count 0), and downstream consumers appear to miss the fetched snapshot_text.
        #
        # Goal:
        #   - Ensure the same snapshot-carrier fields used by New Analysis are populated for Evolution,
        #     so that attach_source_snapshots_to_analysis() (and any downstream rebuild plumbing) can
        #     "see" the injected (and other) snapshots deterministically.
        #
        # Safety:
        #   - Purely additive wiring.
        #   - No effect if baseline_sources_cache is missing.
        #   - Does not alter hashing logic; only ensures snapshots are attached consistently.
        try:
            if isinstance(baseline_sources_cache, list) and baseline_sources_cache and isinstance(web_context, dict):
                # Provide canonical aliases for current pool (additive; downstream may read any of these)
                web_context.setdefault("current_baseline_sources_cache", baseline_sources_cache)
                web_context.setdefault("baseline_sources_cache_current", baseline_sources_cache)
                web_context.setdefault("current_source_results", baseline_sources_cache)

                # Mirror into output for downstream consumers/debug (additive)
                try:
                    output.setdefault("baseline_sources_cache_current", baseline_sources_cache)
                except Exception:
                    pass
                try:
                    output.setdefault("baseline_sources_cache", baseline_sources_cache)
                except Exception:
                    pass
                try:
                    output.setdefault("results", {})
                    if isinstance(output.get("results"), dict):
                        output["results"].setdefault("baseline_sources_cache_current", baseline_sources_cache)
                        output["results"].setdefault("baseline_sources_cache", baseline_sources_cache)
                except Exception:
                    pass

                # Call the same snapshot attach helper used by analysis if present (best-effort)
                try:
                    _att_fn = globals().get("attach_source_snapshots_to_analysis")
                    if callable(_att_fn):
                        _att_fn(output, web_context)
                except Exception:
                    pass

                # Small debug breadcrumb
                try:
                    output.setdefault("debug", {})
                    if isinstance(output.get("debug"), dict):
                        output["debug"].setdefault("fix41afc17", {})
                        if isinstance(output["debug"].get("fix41afc17"), dict):
                            output["debug"]["fix41afc17"].update({
                                "attached_pool_count": int(len(baseline_sources_cache)),
                                "attach_called": bool(callable(globals().get("attach_source_snapshots_to_analysis"))),
                            })
                except Exception:
                    pass
        except Exception:
            pass

        try:
            if not (isinstance(baseline_sources_cache, list) and baseline_sources_cache):
                _fix36_reason = "no_snapshots"
            elif not (isinstance(prev_metrics, dict) and prev_metrics):
                _fix36_reason = "no_prev_metrics"
            else:
                _fix36_cur_hash = _fix37_snapshot_hash_stable(baseline_sources_cache)
                if not (isinstance(_prev_hash, str) and _prev_hash):
                    _fix36_reason = "no_prev_hash"
                elif _fix36_cur_hash != _prev_hash:
                    _fix36_reason = "hash_mismatch"
                else:
                    _fix36_reason = "hash_match_and_prev_metrics_present"

            # Intent:
            #   If the Evolution UI supplies injected URLs that are NOT already part of the
            #   baseline source universe, bypass fastpath eligibility even when hashes match.
            #   This does NOT weaken fastpath for the locked/no-injection case; it only prevents
            #   "observed but inert" injections from being ignored when they materially change
            #   the intended source universe.
            #
            #   Key rule:
            #     - If injected_delta (normalized_injected_urls - normalized_baseline_urls) is non-empty
            #       and fastpath would otherwise be eligible, force _fix36_reason to a bypass reason so
            #       fastpath_eligible becomes False and rebuild path can run.
            try:
                _evo_wc = web_context if isinstance(web_context, dict) else {}
                _evo_diag = _evo_wc.get("diag_injected_urls") if isinstance(_evo_wc.get("diag_injected_urls"), dict) else {}
                _evo_extra_urls = []
                # Prefer already-normalized intake/ui lists if present
                for _k in ("intake", "ui_norm", "ui", "extra_urls"):
                    _v = _evo_diag.get(_k)
                    if isinstance(_v, (list, tuple)) and _v:
                        _evo_extra_urls = list(_v)
                        break
                # Fall back to raw web_context extras if diag not populated
                if not _evo_extra_urls:
                    _v2 = _evo_wc.get("extra_urls")
                    if isinstance(_v2, (list, tuple)) and _v2:
                        _evo_extra_urls = list(_v2)

                #   Robustly recover injected/extra URLs for bypass detection even when
                #   diag_injected_urls is not populated yet (common on replay/fastpath).
                #   Sources (in order):
                #     - web_context["extra_urls"] if list
                #     - web_context["diag_extra_urls_ui"] if list
                #     - web_context["diag_extra_urls_ui_raw"] if str (newline/space separated)
                #   This is diagnostic-only: we ONLY use this to decide whether to bypass
                #   fastpath when hashes otherwise match, so injected URLs can be admitted
                #   via the rebuild path and become first-class inputs.
                try:
                    if not _evo_extra_urls:
                        _v3 = _evo_wc.get("diag_extra_urls_ui")
                        if isinstance(_v3, (list, tuple)) and _v3:
                            _evo_extra_urls = list(_v3)
                    if not _evo_extra_urls:
                        _raw = _evo_wc.get("diag_extra_urls_ui_raw")
                        if isinstance(_raw, str) and _raw.strip():
                            # Split on newlines first; also allow commas/spaces as separators
                            _parts = []
                            for _line in _raw.splitlines():
                                _line = (_line or "").strip()
                                if not _line:
                                    continue
                                # allow comma-separated within a line
                                for _p in _line.split(","):
                                    _p = (_p or "").strip()
                                    if _p:
                                        _parts.append(_p)

                            # REFACTOR76: surface schema-coverage / change_type integrity issues (counts only)
                            try:
                                if _schema_keys and isinstance(_inv, dict):
                                    _mrk = _inv.get("metric_changes_schema_missing_keys") or []
                                    _drk = _inv.get("metric_changes_schema_duplicate_keys") or []
                                    _ctm = _inv.get("metric_changes_change_type_mismatches") or []
                                    if isinstance(_mrk, list) and _mrk:
                                        _parts.append(f"row_missing_keys={len(_mrk)}")
                                    if isinstance(_drk, list) and _drk:
                                        _parts.append(f"row_duplicate_keys={len(_drk)}")
                                    if isinstance(_ctm, list) and _ctm:
                                        _parts.append(f"row_change_type_mismatch={len(_ctm)}")
                            except Exception:
                                pass

                            if _parts:
                                _evo_extra_urls = _parts
                except Exception:
                    pass

                #   If we recovered injected URLs from Streamlit diagnostic fields (e.g.,
                #   diag_extra_urls_ui_raw) and web_context["extra_urls"] is empty, wire the
                #   recovered list into web_context["extra_urls"] so downstream admission/
                #   fetch/persist logic can see the same universe deterministically.
                #   No effect on no-injection runs.
                try:
                    if isinstance(_evo_wc, dict):
                        _wc_extra = _evo_wc.get("extra_urls")
                        if (not isinstance(_wc_extra, (list, tuple)) or not _wc_extra) and isinstance(_evo_extra_urls, list) and _evo_extra_urls:
                            _evo_wc["extra_urls"] = list(_evo_extra_urls)
                            try:
                                _evo_wc["__yureeka_extra_urls_are_injection_v1"] = True
                                _evo_wc["__yureeka_injected_urls_v1"] = list(_evo_extra_urls)
                            except Exception:
                                pass

                except Exception:
                    pass

                _evo_inj_set = set(_inj_diag_norm_url_list(_evo_extra_urls)) if _evo_extra_urls else set()

                # Baseline universe = urls present in baseline_sources_cache (the same object used for hashing)
                _evo_base_urls = []
                if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                    for _row in baseline_sources_cache:
                        if isinstance(_row, dict) and isinstance(_row.get("source_url"), str) and _row.get("source_url"):
                            _evo_base_urls.append(_row.get("source_url"))
                _evo_base_set = set(_inj_diag_norm_url_list(_evo_base_urls)) if _evo_base_urls else set()

                _evo_inj_delta = sorted(list(_evo_inj_set - _evo_base_set)) if _evo_inj_set else []
                #   We persist a simple boolean flag in locals so the downstream FIX31
                #   authoritative reuse check can be disabled without refactoring.
                _fix41af_inj_delta_present = bool(_evo_inj_delta)

                # Only bypass when hashes match and we would otherwise take fastpath
                if _evo_inj_delta and _fix36_reason == "hash_match_and_prev_metrics_present":
                    _fix36_reason = "hash_match_but_injected_urls_present_bypass_fastpath"
                    try:
                        if isinstance(output.get("debug"), dict) and isinstance(output.get("debug", {}).get("fix35"), dict):
                            output["debug"]["fix35"]["fastpath_bypass_injected_delta"] = _evo_inj_delta
                            output["debug"]["fix35"]["fastpath_bypass_injected_delta_count"] = len(_evo_inj_delta)
                    except Exception:
                        pass
            except Exception:
                pass
                # Never break evolution on diagnostics / bypass checks
                pass
            if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                # Preserve any earlier reason, but fill if empty
                if not output["debug"]["fix35"].get("fastpath_reason"):
                    output["debug"]["fix35"]["fastpath_reason"] = _fix36_reason
                output["debug"]["fix35"]["fastpath_eligible"] = bool(_fix36_reason == "hash_match_and_prev_metrics_present")
                if _fix36_cur_hash:
                    output["debug"]["fix35"]["source_snapshot_hash_current"] = _fix36_cur_hash
                    output["debug"]["fix35"]["source_snapshot_hash_current_alg"] = "fix37_stable_v2_preferred"
                if isinstance(_prev_hash, str) and _prev_hash:
                    output["debug"]["fix35"]["source_snapshot_hash_previous"] = (_prev_hash_pref if isinstance(locals().get("_prev_hash_pref"), str) and locals().get("_prev_hash_pref") else _prev_hash)
                try:
                    if isinstance(_prev_hash_stable, str) and _prev_hash_stable:
                        output["debug"]["fix35"]["source_snapshot_hash_previous_stable"] = _prev_hash_stable
                except Exception:
                    pass
        except Exception:
            pass

        # REFACTOR115: schema-seeded production sources (minimal, deterministic; skipped when injection active)
        try:
            _rf115_inj_urls = _refactor115_collect_injection_urls_v1({}, web_context or {})
            _rf115_injection_present = bool(_rf115_inj_urls)
            _rf115_added = []
            _rf115_extract_diag = []  # REFACTOR119
            _rf115_year_tokens_union = ["2025", "2026", "2040"]  # REFACTOR119
            if isinstance(baseline_sources_cache, list):
                # normalize existing URLs
                _existing = set()
                for _r in (baseline_sources_cache or []):
                    if isinstance(_r, dict):
                        _u = _r.get("url") or _r.get("source_url") or ""
                        _n = None
                        try:
                            _n = _inj_diag_norm_url_list([_u])[0] if _u else ""
                        except Exception:
                            _n = str(_u or "").strip()
                        if _n:
                            _existing.add(_n)
                if (not _rf115_injection_present):
                    for _u in (_REFACTOR115_SCHEMA_SEED_URLS_V1 or []):
                        try:
                            _u_norm = _inj_diag_norm_url_list([_u])[0]
                        except Exception:
                            _u_norm = str(_u or "").strip()
                        if not _u_norm or _u_norm in _existing:
                            continue
                        _txt, _detail = None, ""
                        try:
                            _txt, _detail = fetch_url_content_with_status(_u_norm, timeout=25)
                        except Exception as _e:
                            _txt, _detail = None, f"exception:{type(_e).__name__}"
                        # Normalize status for skipped PDF dependency
                        _status = "fetched" if str(_detail).startswith("success") else "failed"
                        _sd = str(_detail or "")
                        if str(_sd).startswith("skipped:"):
                            _status = "skipped"
                            try:
                                _sd = _sd[len("skipped:"):] or "pdf_unsupported_missing_dependency"
                            except Exception:
                                _sd = "pdf_unsupported_missing_dependency"
                        if isinstance(_txt, str) and len(_txt.strip()) >= 200:
                            # REFACTOR119: actually extract numeric candidates from schema-seeded snapshot_text
                            _rf115_extracted_numbers = []
                            _rf115_numbers_found = 0
                            _rf115_fp = ""
                            _rf115_year_hits = {}
                            _rf115_extract_error = ""
                            try:
                                _rf115_extracted_numbers = extract_numbers_with_context(_txt, source_url=_u_norm, max_results=600) or []
                            except Exception as _ee:
                                _rf115_extracted_numbers = []
                                try:
                                    _rf115_extract_error = f"{type(_ee).__name__}:{_ee}"
                                except Exception:
                                    _rf115_extract_error = "exception"
                            try:
                                _rf115_numbers_found = sum(1 for _n in (_rf115_extracted_numbers or []) if isinstance(_n, dict) and (not bool(_n.get("is_junk"))))
                            except Exception:
                                try:
                                    _rf115_numbers_found = int(len(_rf115_extracted_numbers or [])) if isinstance(_rf115_extracted_numbers, list) else 0
                                except Exception:
                                    _rf115_numbers_found = 0
                            try:
                                _rf115_fp = fingerprint_text(_txt)
                            except Exception:
                                _rf115_fp = ""
                            try:
                                for _yy in (_rf115_year_tokens_union or []):
                                    if isinstance(_yy, str) and _yy:
                                        _rf115_year_hits[_yy] = int(str(_txt).count(_yy))
                            except Exception:
                                _rf115_year_hits = {}
                            try:
                                _rf115_extract_diag.append({
                                    "url": _u_norm,
                                    "numbers_found": int(_rf115_numbers_found or 0),
                                    "status": _status,
                                    "status_detail": _sd,
                                    "year_token_hits": _rf115_year_hits,
                                    "extract_error": _rf115_extract_error,
                                })
                            except Exception:
                                pass

                            baseline_sources_cache.append({
                                "source_url": _u_norm,
                                "url": _u_norm,
                                "status": _status,
                                "status_detail": _sd,
                                "snapshot_text": _txt,
                                "snapshot_text_excerpt": (_txt[:12000] if isinstance(_txt, str) else ""),
                                "fingerprint": _rf115_fp,
                                "extracted_numbers": _rf115_extracted_numbers,
                                "numbers_found": _rf115_numbers_found,
                                "seeded": True,
                                "seeded_reason": "schema_seeds",
                                "fetched_at": _now(),
                            })
                            _existing.add(_u_norm)
                            _rf115_added.append(_u_norm)
                        else:
                            # Still append a placeholder so hash changes deterministically and future rebuild can fetch.
                            try:
                                _rf115_extract_diag.append({
                                    "url": _u_norm,
                                    "numbers_found": 0,
                                    "status": ("seeded_pending" if (not _status == "skipped") else "skipped"),
                                    "status_detail": _sd or "seeded_placeholder",
                                    "year_token_hits": {},
                                    "extract_error": "missing_text",
                                })
                            except Exception:
                                pass
                            baseline_sources_cache.append({
                                "source_url": _u_norm,
                                "url": _u_norm,
                                "status": "seeded_pending" if (not _status == "skipped") else "skipped",
                                "status_detail": _sd or "seeded_placeholder",
                                "snapshot_text": "",
                                "fingerprint": "",
                                "extracted_numbers": [],
                                "numbers_found": 0,
                                "seeded": True,
                                "seeded_reason": "schema_seeds_placeholder",
                                "fetched_at": _now(),
                            })
                            _existing.add(_u_norm)
                            _rf115_added.append(_u_norm)
            # Emit debug beacon (additive)
            try:
                output.setdefault("debug", {})
                if isinstance(output.get("debug"), dict):
                    output["debug"]["schema_seed_sources_v1"] = {
                        "seeds_added": int(len(_rf115_added)),
                        "added_urls": list(_rf115_added)[:10],
                        "final_sources_urls": [ (r.get("url") or r.get("source_url")) for r in (baseline_sources_cache or []) if isinstance(r, dict) and (r.get("url") or r.get("source_url")) ][:50],
                        "reason": "schema_seeds",
                        "injection_present": bool(_rf115_injection_present),
                    }

                    # REFACTOR119: seed extraction beacon (did seeds produce numeric candidates?)
                    try:
                        _nonzero = 0
                        _errs = 0
                        _sample = []
                        _year_hits = {}
                        if isinstance(_rf115_extract_diag, list):
                            try:
                                _nonzero = sum(1 for _d in _rf115_extract_diag if isinstance(_d, dict) and int(_d.get("numbers_found") or 0) > 0)
                            except Exception:
                                _nonzero = 0
                            try:
                                _errs = sum(1 for _d in _rf115_extract_diag if isinstance(_d, dict) and str(_d.get("extract_error") or "").strip())
                            except Exception:
                                _errs = 0
                            for _d in (_rf115_extract_diag or [])[:12]:
                                if not isinstance(_d, dict):
                                    continue
                                _sample.append({
                                    "url": _d.get("url"),
                                    "numbers_found": _d.get("numbers_found"),
                                    "status": _d.get("status"),
                                })
                                _yh = _d.get("year_token_hits")
                                if isinstance(_yh, dict) and _yh and isinstance(_d.get("url"), str):
                                    _year_hits[str(_d.get("url"))] = dict(_yh)
                        output["debug"]["schema_seed_extract_v1"] = {
                            "seeds_added": int(len(_rf115_added)),
                            "seeds_extracted_nonzero": int(_nonzero or 0),
                            "per_seed_numbers_found_sample": list(_sample or []),
                            "per_seed_year_token_hits": dict(_year_hits or {}),
                            "extract_errors": int(_errs or 0),
                        }
                    except Exception:
                        pass
            except Exception:
                pass
        except Exception:
            pass

        # REFACTOR121: snapshot excerpt coverage beacon (year-anchor backstop)
        try:
            _tot = int(len(baseline_sources_cache) if isinstance(baseline_sources_cache, list) else 0)
            _with = 0
            _seeded_with = 0
            _lens = []
            if isinstance(baseline_sources_cache, list):
                for _s in (baseline_sources_cache or []):
                    if not isinstance(_s, dict):
                        continue
                    _ex = _s.get("snapshot_text_excerpt")
                    if isinstance(_ex, str) and _ex.strip():
                        _with += 1
                        _lens.append(len(_ex))
                        if bool(_s.get("seeded")):
                            _seeded_with += 1
            output.setdefault("debug", {})["snapshot_excerpt_coverage_v1"] = {
                "sources_total": int(_tot),
                "sources_with_excerpt": int(_with),
                "seeded_sources_with_excerpt": int(_seeded_with),
                "excerpt_len_avg": (float(sum(_lens) / max(1, len(_lens))) if _lens else 0.0),
                "excerpt_len_max": (int(max(_lens)) if _lens else 0),
            }
            output["debug"]["year_anchor_page_text_v1"] = {
                "indexed_text_sources": int(_with),
                "sources_total": int(_tot),
                "index_key_preference": "snapshot_text_excerpt>snapshot_text>clean_text>text",
            }
        except Exception:
            pass

        # REFACTOR115: FIX31 fast-path gating to avoid masking progress during validation/injection/null-schema scenarios.
        # Only attempt fast-path if we have snapshots AND prior canonical metrics to reuse
        if isinstance(baseline_sources_cache, list) and baseline_sources_cache and isinstance(prev_metrics, dict) and prev_metrics:
            # - Previously FIX31 compared a v1 fingerprint against prev source_snapshot_hash,
            #   which could mismatch even when data was unchanged.
            # - We now prefer the same stable/v2 hash used by analysis & FIX37 debug.
            _cur_hash_v1 = _fix31_snapshot_fingerprint(baseline_sources_cache)
            try:
                _cur_hash = _fix37_snapshot_hash_stable(baseline_sources_cache)
            except Exception:
                pass
                _cur_hash = _cur_hash_v1

            # Prefer stable/v2 previous hash if present
            _prev_hash_pref = previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2") or _prev_hash

            #
            # Goal:
            #   When hashes are unequal, rebuild should run on the SAME snapshot pool
            #   that "new analysis" just produced, not on the stale baseline cache
            #   embedded in the previous analysis payload.
            #
            # Where it comes from:
            #   - web_context["current_baseline_sources_cache"]  (preferred)
            #   - web_context["baseline_sources_cache_current"]
            #   - web_context["current_source_results"]         (fallback alias)
            #
            # Policy (additive, fastpath-safe):
            #   - If force_rebuild is asserted by UI/web_context, always use current pool.
            #   - Else, only switch to current pool if its stable hash != previous hash.
            #   - If hashes match, we keep existing behavior (but either pool is equivalent).
            _fix42_used_current_pool = False
            _fix42_reason = ""
            _fix42_cur_pool_hash = None
            try:
                if isinstance(web_context, dict):
                    _cur_pool = (
                        web_context.get("current_baseline_sources_cache")
                        or web_context.get("baseline_sources_cache_current")
                        or web_context.get("current_source_results")
                        or web_context.get("current_source_results_cache")
                        or None
                    )
                    if isinstance(_cur_pool, list) and _cur_pool:
                        _force = bool(
                            web_context.get("force_rebuild")
                            or web_context.get("forced_rebuild")
                            or web_context.get("force_full_rebuild")
                            or web_context.get("force_metric_rebuild")
                        )
                        try:
                            _fix42_cur_pool_hash = _fix37_snapshot_hash_stable(_cur_pool)
                        except Exception:
                            pass
                            _fix42_cur_pool_hash = None

                        _prev_hash_for_compare = _prev_hash_pref if (isinstance(_prev_hash_pref, str) and _prev_hash_pref) else _prev_hash
                        _hash_mismatch = bool(
                            (isinstance(_prev_hash_for_compare, str) and _prev_hash_for_compare and isinstance(_fix42_cur_pool_hash, str) and _fix42_cur_pool_hash)
                            and (_fix42_cur_pool_hash != _prev_hash_for_compare)
                        )

                        if _force or _hash_mismatch:
                            baseline_sources_cache = _cur_pool
                            snapshot_origin = (snapshot_origin or "analysis_cache") + "|fix42_current_pool"
                            _fix42_used_current_pool = True
                            _fix42_reason = "forced_rebuild" if _force else "hash_mismatch_use_current_pool"
            except Exception:
                pass

            # Attach FIX42 diagnostics (non-breaking)
            try:
                if _fix42_used_current_pool:
                    output["snapshot_origin"] = snapshot_origin
                if isinstance(output.get("debug"), dict):
                    output["debug"].setdefault("fix42", {})
                    output["debug"]["fix42"]["used_current_pool"] = bool(_fix42_used_current_pool)
                    output["debug"]["fix42"]["reason"] = _fix42_reason
                    if isinstance(_fix42_cur_pool_hash, str) and _fix42_cur_pool_hash:
                        output["debug"]["fix42"]["current_pool_hash_stable"] = _fix42_cur_pool_hash
            except Exception:
                pass

            #   If an injected URL delta exists, we MUST NOT take FIX31 authoritative
            #   reuse (fastpath replay), even if hashes match. We do this additively by
            #   temporarily blanking _prev_hash_pref so the existing hash-match check
            #   remains unchanged for normal runs.
            _fix41af_prev_hash_pref_saved = None
            try:
                if bool(locals().get("_fix41af_inj_delta_present")):
                    _fix41af_prev_hash_pref_saved = _prev_hash_pref
                    _prev_hash_pref = ""
                    try:
                        if isinstance(output.get("debug"), dict) and isinstance(output.get("debug", {}).get("fix35"), dict):
                            output["debug"]["fix35"]["fastpath_reason"] = "hash_match_but_injected_urls_present_bypass_fastpath"
                            output["debug"]["fix35"]["fastpath_eligible"] = False
                    except Exception:
                        pass
            except Exception:
                pass

            # REFACTOR99 safety: do NOT take FIX31 authoritative reuse when previous canonical metrics are too sparse.
            # This prevents being "stuck" with a thin prior run (e.g., 1/4 keys) even though sources are stable.
            try:
                _fix99_prev_pmc = _refactor89_locate_pmc_dict(prev_response) if isinstance(prev_response, dict) else {}
                _fix99_schema = (
                    (prev_response or {}).get("metric_schema_frozen")
                    or ((prev_response or {}).get("primary_response") or {}).get("metric_schema_frozen")
                    or ((prev_response or {}).get("results") or {}).get("metric_schema_frozen")
                    or {}
                )
                _fix99_prev_pmc_count = int(len(_fix99_prev_pmc) if isinstance(_fix99_prev_pmc, dict) else 0)
                _fix99_schema_count = int(len(_fix99_schema) if isinstance(_fix99_schema, dict) else 0)
                _fix99_min_needed = 0
                try:
                    if _fix99_schema_count >= 2:
                        _fix99_min_needed = max(2, int((_fix99_schema_count + 1) // 2))
                except Exception:
                    _fix99_min_needed = 2 if _fix99_schema_count >= 2 else 0
                if _fix99_schema_count and _fix99_min_needed and (_fix99_prev_pmc_count < _fix99_min_needed):
                    _prev_hash_pref = ""  # bypass hash-match fastpath
                    try:
                        if isinstance(output.get("debug"), dict) and isinstance(output.get("debug", {}).get("fix35"), dict):
                            output["debug"]["fix35"]["fastpath_reason"] = f"hash_match_but_prev_pmc_sparse_bypass_fastpath:{_fix99_prev_pmc_count}/{_fix99_schema_count}"
                            output["debug"]["fix35"]["fastpath_eligible"] = False
                            output["debug"]["fix35"]["prev_pmc_count"] = _fix99_prev_pmc_count
                            output["debug"]["fix35"]["schema_key_count"] = _fix99_schema_count
                    except Exception:
                        pass
            except Exception:
                pass

            _fix31_prev_code_version = ""
            try:
                _fix31_prev_code_version = str((previous_data or {}).get("code_version") or "")
            except Exception:
                _fix31_prev_code_version = ""

            # REFACTOR115: compute fast-path eligibility gate
            _rf115_injection_present = False
            _rf115_triad_mode = False
            _rf115_schema_all_null = False
            _rf115_disabled_reason = ""
            try:
                _rf115_injection_present = bool(_refactor115_collect_injection_urls_v1({}, web_context or {}))
            except Exception:
                _rf115_injection_present = False
            try:
                _rf115_triad_mode = bool((web_context or {}).get("triad_validation_mode"))
            except Exception:
                _rf115_triad_mode = False
            try:
                _rf115_schema_all_null = bool(_refactor115_all_schema_values_null_in_payload_v1(previous_data))
            except Exception:
                _rf115_schema_all_null = False
            if _rf115_injection_present:
                _rf115_disabled_reason = "injection_present"
            elif _rf115_triad_mode:
                _rf115_disabled_reason = "triad_validation_mode"
            elif _rf115_schema_all_null:
                _rf115_disabled_reason = "all_schema_values_null"
            # Emit beacon
            # REFACTOR117: honor global DISABLE_FASTPATH_FOR_NOW as a hard gate during refactor triad validation.
            # REFACTOR201: allow explicit opt-in enable_fastpath override for production runs, but always disable fastpath in injection mode.
            _rf201_injection_mode = False
            _rf201_enable_fastpath = False
            try:
                _rf201_injection_mode = bool(_yureeka_is_injection_mode_v1(web_context))
            except Exception:
                _rf201_injection_mode = False
            try:
                _rf201_enable_fastpath = bool((web_context or {}).get("enable_fastpath"))
            except Exception:
                _rf201_enable_fastpath = False
            try:
                if (not _rf115_disabled_reason) and _rf201_injection_mode:
                    _rf115_disabled_reason = "injection_mode_force_rebuild"
                elif (not _rf115_disabled_reason) and bool(globals().get("DISABLE_FASTPATH_FOR_NOW")) and (not _rf201_enable_fastpath):
                    _rf115_disabled_reason = "disable_fastpath_for_now"
            except Exception:
                pass

            try:
                output.setdefault("debug", {})
                if isinstance(output.get("debug"), dict):
                    output["debug"]["fastpath_gate_v2"] = {
                        "eligible": bool(_rf115_disabled_reason == ""),
                        "disabled_reason": _rf115_disabled_reason,
                        "cur_hash": _cur_hash if isinstance(locals().get("_cur_hash"), str) else None,
                        "prev_hash": _prev_hash_pref if isinstance(locals().get("_prev_hash_pref"), str) else None,
                        "enable_fastpath": bool(locals().get("_rf201_enable_fastpath", False)),
                        "injection_mode": bool(locals().get("_rf201_injection_mode", False)),
                    }
            except Exception:
                pass

            if (not _rf115_disabled_reason) and isinstance(_prev_hash_pref, str) and _prev_hash_pref and _cur_hash == _prev_hash_pref and _fix31_prev_code_version == str(globals().get("CODE_VERSION") or ""):
                _fix31_authoritative_reuse = True
                try:
                    if _fix41af_prev_hash_pref_saved is not None:
                        _prev_hash_pref = _fix41af_prev_hash_pref_saved
                except Exception:
                    pass

                try:
                    output["rebuild_skipped"] = True
                    output["rebuild_skipped_reason"] = "fix31_sources_unchanged_reuse_prev_metrics"
                    output["source_snapshot_hash_current"] = _cur_hash
                    output["source_snapshot_hash_previous"] = (_prev_hash_cmp if " _prev_hash_cmp" in locals() else _prev_hash)
                    try:
                        if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                            output["debug"]["fix35"]["fastpath_eligible"] = True
                            output["debug"]["fix35"]["fastpath_reason"] = "hash_match_and_prev_metrics_present"
                            output["debug"]["fix35"]["source_snapshot_hash_current"] = _cur_hash
                            output["debug"]["fix35"]["source_snapshot_hash_previous"] = (_prev_hash_pref if isinstance(locals().get("_prev_hash_pref"), str) and locals().get("_prev_hash_pref") else _prev_hash)
                            output["debug"]["fix35"]["current_metrics_origin"] = "reuse_processed_metrics_fastpath"
                    except Exception:
                        pass
                except Exception:
                    pass
    except Exception:
        pass
        _fix31_authoritative_reuse = False
        try:
            if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                if not output["debug"]["fix35"].get("fastpath_reason"):
                    output["debug"]["fix35"]["fastpath_reason"] = "fastpath_not_taken_or_exception"
        except Exception:
            pass

    # Build a minimal current metrics dict from snapshots:
    current_metrics = {}
    try:

        #   (covers the case where hash-match condition was false and the inline restore
        #   inside the if-body did not execute).
        try:
            if locals().get("_fix41af_prev_hash_pref_saved") is not None and not _prev_hash_pref:
                _prev_hash_pref = locals().get("_fix41af_prev_hash_pref_saved")
        except Exception:
            pass

        if _fix31_authoritative_reuse and isinstance(prev_metrics, dict) and prev_metrics:
            current_metrics = dict(prev_metrics)
            try:
                output["snapshot_origin"] = (output.get("snapshot_origin") or "") + "|fix31_reuse_prev_metrics"
            except Exception:
                pass
    except Exception:
        pass

    except Exception:
        _fix31_authoritative_reuse = False
        try:
            if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                if not output["debug"]["fix35"].get("fastpath_reason"):
                    output["debug"]["fix35"]["fastpath_reason"] = "fastpath_not_taken_or_exception"
        except Exception:
            pass

    # Prefer metric_anchors to rebuild current_metrics (snapshot-gated)
    def _get_metric_anchors(prev: dict) -> dict:
        # so the reused, schema-gated metrics remain untouched.
        try:
            if _fix31_authoritative_reuse:
                return {}
        except Exception:
            pass

        if not isinstance(prev, dict):
            return {}
        a = prev.get("metric_anchors")
        if isinstance(a, dict) and a:
            return a
        pr = prev.get("primary_response")
        if isinstance(pr, dict):
            a2 = pr.get("metric_anchors")
            if isinstance(a2, dict) and a2:
                return a2
        res = prev.get("results")
        if isinstance(res, dict):
            a3 = res.get("metric_anchors")
            if isinstance(a3, dict) and a3:
                return a3
        return {}

    def _canonicalize_candidate(n: dict) -> dict:
        try:
            fn = globals().get("canonicalize_numeric_candidate")
            if callable(fn):
                return fn(dict(n))
        except Exception:
            return dict(n)

    def _build_anchor_to_candidate_map(snapshots: list) -> dict:
        m = {}
        for sr in snapshots or []:
            if not isinstance(sr, dict):
                continue
            for n in (sr.get("extracted_numbers") or []):
                if not isinstance(n, dict):
                    continue
                nn = _canonicalize_candidate(n)
                ah = nn.get("anchor_hash")
                if not ah:
                    continue
                if ah not in m:
                    m[ah] = nn
        return m

    try:
        metric_anchors = _get_metric_anchors(previous_data)
        anchor_to_candidate = _build_anchor_to_candidate_map(baseline_sources_cache)

        if isinstance(metric_anchors, dict) and metric_anchors:
            for ckey, a in metric_anchors.items():
                if not isinstance(a, dict):
                    continue
                ah = a.get("anchor_hash") or a.get("anchor")
                if not ah:
                    continue
                cand = anchor_to_candidate.get(ah)
                if not isinstance(cand, dict):
                    continue

                base = prev_metrics.get(ckey) if isinstance(prev_metrics, dict) else None
                out_row = dict(base) if isinstance(base, dict) else {}
                out_row.update({
                    "canonical_key": ckey,
                    "anchor_hash": ah,
                    "anchor_used": True,
                    "anchor_confidence": a.get("anchor_confidence"),
                    "source_url": cand.get("source_url") or a.get("source_url"),
                    "raw": cand.get("raw"),
                    "value": cand.get("value"),
                    "unit": cand.get("unit"),
                    "value_norm": cand.get("value_norm"),
                    "context_snippet": cand.get("context_snippet") or cand.get("context") or "",
                    "candidate_id": cand.get("candidate_id") or a.get("candidate_id"),
                })
                current_metrics[ckey] = out_row
    except Exception:
        pass

    # Rebuild fallback only if anchors didn't produce metrics
    if not isinstance(current_metrics, dict) or not current_metrics:
        try:
            fn_rebuild = globals().get("rebuild_metrics_from_snapshots_schema_only")
            if callable(fn_rebuild):
                current_metrics = fn_rebuild(prev_response, baseline_sources_cache, web_context=web_context)
        except Exception:
            pass
            current_metrics = {}
    if not isinstance(current_metrics, dict) or not current_metrics:
        # FIX2D65A: do not fail hard here; emit a warning and continue so Evolution can still output JSON.
        try:
            output.setdefault("warnings", [])
            output["warnings"].append({
                "code": "FIX2D65_REBUILD_EMPTY_WITH_SNAPSHOTS",
                "message": "Valid snapshots exist, but metric rebuild returned empty. Continuing (no refetch, no heuristic).",
                "sources_checked": int(len(baseline_sources_cache or [])),
                "sources_fetched": int(len(baseline_sources_cache or [])),
            })
        except Exception:
            pass
        output["status"] = output.get("status") or "ok_with_warnings"
        output["message"] = output.get("message") or "Valid snapshots exist, but metric rebuild returned empty. Continuing (no refetch, no heuristic)."
        # Keep current_metrics as empty dict; downstream code should handle it.
        current_metrics = {}
    #
    # Why:
    # - Latest evo JSON shows current metrics can be selected from non-matching units
    #   (e.g., unit_sales metric receiving a unitless/negative number; percent metric
    #   receiving a magnitude unit like 'B'). This leads the dashboard "Current" column
    #   to display the wrong metric values even though injection plumbing is progressing.
    # - The new analysis pipeline already relies on FIX16-style hard eligibility gates +
    #   anchor_hash deterministic rebuild. Evolution must use the same selection rules
    #   when fastpath is NOT taken (hash mismatch or injection-triggered rebuild).
    #
    # What:
    # - Right before diffing, attempt an anchor-first rebuild using:
    #     rebuild_metrics_from_snapshots_schema_only_fix16(prev_response, pool, web_context)
    #   when available.
    # - If it returns a non-empty dict, it *overrides* the previously computed
    #   current_metrics (additive override only when rebuild succeeded).
    # - Emits explicit debug fields for traceability.
    #
    # Non-negotiables:
    # - Does NOT alter fastpath logic.
    # - Only activates when fastpath is not taken (i.e., not authoritative reuse).
    try:
        _fix41afc19_applied = False
        _fix41afc19_reason = ""
        _fix41afc19_fn_name = ""
        _fix41afc19_rebuilt_count = 0

        # Only consider override when fastpath is not active
        if not bool(locals().get("_fix31_authoritative_reuse")):
            # Attempt to locate the "current" snapshot pool (post-injection merge/attach)
            _fix41afc19_pool = (
                locals().get("baseline_sources_cache_current")
                or (output.get("baseline_sources_cache_current") if isinstance(output, dict) else None)
                or (output.get("results", {}).get("baseline_sources_cache_current") if isinstance(output, dict) else None)
                or locals().get("baseline_sources_cache")
                or locals().get("baseline_sources_cache_prefetched")
                or None
            )

            # - Some pipelines store the post-attach merged pool under different locals()
            #   names (or only inside nested objects). If the pool is missed, FIX41AFC19
            #   appears "not applied" even on rebuild runs.
            # - We search locals() for any list-like baseline_sources_cache* variants and
            #   choose the largest plausible pool as a safe fallback.
            if _fix41afc19_pool is None:
                try:
                    _cand_pools = []
                    for _k, _v in (locals() or {}).items():
                        if not isinstance(_k, str):
                            continue
                        if "baseline_sources_cache" in _k and isinstance(_v, list) and _v:
                            _cand_pools.append((_k, _v))
                    # Choose the largest pool (most likely post-attach merged universe)
                    if _cand_pools:
                        _cand_pools.sort(key=lambda kv: len(kv[1] or []), reverse=True)
                        _fix41afc19_pool = _cand_pools[0][1]
                        _fix41afc19_reason = (_fix41afc19_reason or "") + "|ph2b_s2_pool_fallback:" + str(_cand_pools[0][0])
                except Exception:
                    pass


            # Prefer FIX16 schema-only rebuild for non-injection runs to preserve frozen winner behavior;
            # use analysis-canonical rebuild only when injection URLs are present (injected rows must force-win).
            _fix41afc19_injection_present = False
            try:
                _inj_urls_probe = []
                _inj_collector = globals().get("_refactor115_collect_injection_urls_v1")
                if callable(_inj_collector):
                    _inj_urls_probe = _inj_collector(output, web_context) or []
                else:
                    if isinstance(web_context, dict):
                        _inj_urls_probe = web_context.get("injected_urls") or web_context.get("injected_url") or []
                if isinstance(_inj_urls_probe, str):
                    _inj_urls_probe = [_inj_urls_probe]
                _fix41afc19_injection_present = bool(_inj_urls_probe)
            except Exception:
                _fix41afc19_injection_present = False

            _fix41afc19_fn = None
            _fix41afc19_fn_name = ""
            _fn_schema = globals().get("rebuild_metrics_from_snapshots_schema_only_fix16")
            _fn_analysis = globals().get("rebuild_metrics_from_snapshots_analysis_canonical_v1")

            if _fix41afc19_injection_present:
                # Injection run: allow analysis-canonical rebuild (supports injection force-win behavior)
                if callable(_fn_analysis):
                    _fix41afc19_fn = _fn_analysis
                    _fix41afc19_fn_name = "rebuild_metrics_from_snapshots_analysis_canonical_v1"
                elif callable(_fn_schema):
                    _fix41afc19_fn = _fn_schema
                    _fix41afc19_fn_name = "rebuild_metrics_from_snapshots_schema_only_fix16"
            else:
                # Non-injection run: prefer schema-only rebuild (FIX16) to preserve REFACTOR206 frozen selection
                if callable(_fn_schema):
                    _fix41afc19_fn = _fn_schema
                    _fix41afc19_fn_name = "rebuild_metrics_from_snapshots_schema_only_fix16"
                elif callable(_fn_analysis):
                    _fix41afc19_fn = _fn_analysis
                    _fix41afc19_fn_name = "rebuild_metrics_from_snapshots_analysis_canonical_v1"



            if callable(_fix41afc19_fn) and _fix41afc19_pool is not None:
                try:
                    _fix41afc19_rebuilt = _fix41afc19_fn(prev_response, _fix41afc19_pool, web_context=web_context)
                except TypeError:
                    # Backward-compat: older signature without web_context
                    _fix41afc19_rebuilt = _fix41afc19_fn(prev_response, _fix41afc19_pool)

                if isinstance(_fix41afc19_rebuilt, dict) and _fix41afc19_rebuilt:
                    current_metrics = dict(_fix41afc19_rebuilt)
                    _fix41afc19_applied = True
                    _fix41afc19_rebuilt_count = len(current_metrics)
                    _fix41afc19_reason = "override_current_metrics_with_rebuild:" + str(_fix41afc19_fn_name or "") + "|injection_present=" + str(bool(_fix41afc19_injection_present))
                else:
                    _fix41afc19_reason = (_fix41afc19_reason or "") + "|rebuilt_empty_or_non_dict"
    except Exception:
        pass

    # Emit debug for FIX41AFC19 (non-breaking)
    try:
        if isinstance(output.get("debug"), dict):
            output["debug"].setdefault("fix41afc19", {})
            output["debug"]["fix41afc19"]["applied"] = bool(locals().get("_fix41afc19_applied"))
            output["debug"]["fix41afc19"]["reason"] = locals().get("_fix41afc19_reason") or ""
            output["debug"]["fix41afc19"]["fn"] = locals().get("_fix41afc19_fn_name") or ""
            output["debug"]["fix41afc19"]["rebuilt_count"] = int(locals().get("_fix41afc19_rebuilt_count") or 0)
            output["debug"]["fix41afc19"]["injection_present"] = bool(locals().get("_fix41afc19_injection_present"))
            # REFACTOR126: surface injection-priority beacon from rebuild (if present)
            try:
                if isinstance(web_context, dict):
                    _inj_pri = web_context.pop("__rebuild_injection_priority_v1", None)
                    _inj_sum = web_context.pop("__rebuild_injection_priority_summary_v1", None)
                    if isinstance(_inj_sum, dict):
                        output["debug"]["fix41afc19"]["injection_priority_summary_v1"] = _inj_sum
                    if isinstance(_inj_pri, dict):
                        output["debug"]["fix41afc19"]["injection_priority_v1"] = _inj_pri
            except Exception:
                pass
    except Exception:
        pass

    # Objective:
    # - Ensure Evolution diff "current" side is built from the same canonical semantics as Analysis
    #   by forcing a best-effort canonical rebuild for DISPLAY/DIFF, even when earlier logic
    #   skipped due to authoritative reuse or pool-resolution drift.
    # - Adds two minimal diagnostics:
    #   (2) debug.fix41afc19 truth table (attempted/applied/skip_reason + keys_sample + pool_count)
    #   (3) debug.evo_winner_trace_v1 for key EV metrics (winner provenance + top3 candidate glimpse)
    # Safety:
    # - Additive-only. Does not modify hashing inputs or snapshot attach. Affects only what diff renders.
    _fix41afc19_attempted_v19 = False
    _fix41afc19_skip_reason_v19 = ""
    _fix41afc19_pool_count_v19 = 0
    _fix41afc19_keys_sample_v19 = []
    _fix41afc19_winner_trace_v19 = {}

    try:
        # Start with whatever earlier stage produced
        current_metrics_for_display = locals().get("current_metrics") if isinstance(locals().get("current_metrics"), dict) else {}

        # Force a display rebuild if earlier FIX41AFC19 did not apply or rebuilt_count==0
        _already_applied = bool(locals().get("_fix41afc19_applied"))
        _already_count = int(locals().get("_fix41afc19_rebuilt_count") or 0)

        if (not _already_applied) or (_already_count <= 0):
            _fix41afc19_attempted_v19 = True

            # Resolve the best available snapshot pool (post-attach merged universe)
            # NOTE: Different callers store the "current" snapshot pool under different names.
            # We intentionally scan a wide set of candidate keys and pick the largest non-empty list.
            _pool = None
            _pool_key_used = None
            try:
                _cand = []

                # Common/expected keys (current run)
                _cand.append(("baseline_sources_cache_current", locals().get("baseline_sources_cache_current")))
                _cand.append(("baseline_sources_cache", locals().get("baseline_sources_cache")))
                _cand.append(("baseline_sources_cache_prefetched", locals().get("baseline_sources_cache_prefetched")))

                # Attachment / merge outputs (varies by branch)
                _cand.append(("baseline_sources_cache_attached", locals().get("baseline_sources_cache_attached")))
                _cand.append(("baseline_sources_cache_merged", locals().get("baseline_sources_cache_merged")))
                _cand.append(("attached_pool", locals().get("attached_pool")))
                _cand.append(("current_pool", locals().get("current_pool")))
                _cand.append(("source_snapshots_current", locals().get("source_snapshots_current")))
                _cand.append(("snapshots_current", locals().get("snapshots_current")))

                # Sometimes carried on output dict
                _out = (locals().get("output") if isinstance(locals().get("output"), dict) else None)
                if isinstance(_out, dict):
                    _cand.append(("output.baseline_sources_cache_current", _out.get("baseline_sources_cache_current")))
                    _cand.append(("output.baseline_sources_cache", _out.get("baseline_sources_cache")))
                    _res = _out.get("results") if isinstance(_out.get("results"), dict) else None
                    if isinstance(_res, dict):
                        _cand.append(("output.results.baseline_sources_cache_current", _res.get("baseline_sources_cache_current")))
                        _cand.append(("output.results.baseline_sources_cache", _res.get("baseline_sources_cache")))
                        _cand.append(("output.results.attached_pool", _res.get("attached_pool")))
                        _cand.append(("output.results.current_pool", _res.get("current_pool")))

                # Final sweep: anything in locals containing baseline_sources_cache*
                for _k, _v in (locals() or {}).items():
                    if not isinstance(_k, str):
                        continue
                    if "baseline_sources_cache" in _k and isinstance(_v, list) and _v:
                        _cand.append((_k, _v))

                # Choose best candidate (largest list wins)
                _best = None
                for _k, _v in _cand:
                    if isinstance(_v, list) and _v:
                        if _best is None or len(_v) > len(_best[1] or []):
                            _best = (_k, _v)
                if _best is not None:
                    _pool_key_used, _pool = _best[0], _best[1]
            except Exception:
                pass
                _pool = None
                _pool_key_used = None

            if _pool is None:
                try:
                    _cand_pools = []
                    for _k, _v in (locals() or {}).items():
                        if not isinstance(_k, str):
                            continue
                        if "baseline_sources_cache" in _k and isinstance(_v, list) and _v:
                            _cand_pools.append((_k, _v))
                    if _cand_pools:
                        _cand_pools.sort(key=lambda kv: len(kv[1] or []), reverse=True)
                        _pool = _cand_pools[0][1]
                except Exception:
                    pass
                    _pool = None

            if isinstance(_pool, list):
                _fix41afc19_pool_count_v19 = len(_pool or [])
                _fix41afc19_pool_key_used_v19 = _pool_key_used

            # Pick best available rebuild fn
            _fn = globals().get('rebuild_metrics_from_snapshots_analysis_canonical_v1')
            _fn_name = 'rebuild_metrics_from_snapshots_analysis_canonical_v1'
            if not callable(_fn):
                _fn = globals().get('rebuild_metrics_from_snapshots_schema_only_fix16')
                _fn_name = 'rebuild_metrics_from_snapshots_schema_only_fix16'
            if not callable(_fn):
                _fn = globals().get('rebuild_metrics_from_snapshots_schema_only')
                _fn_name = 'rebuild_metrics_from_snapshots_schema_only'

            if not callable(_fn):
                _fix41afc19_skip_reason_v19 = "fn_missing"
            elif _pool is None:
                _fix41afc19_skip_reason_v19 = "pool_missing"
            elif not isinstance(_pool, list) or not _pool:
                _fix41afc19_skip_reason_v19 = "pool_empty"
            else:
                try:
                    try:
                        _rebuilt = _fn(prev_response, _pool, web_context=web_context)
                    except TypeError:
                        _rebuilt = _fn(prev_response, _pool)
                except Exception as _e:
                    _rebuilt = None
                    _fix41afc19_skip_reason_v19 = "rebuild_exception:" + str(type(_e).__name__)
                    try:
                        _m = str(_e) if _e is not None else ""
                        if _m:
                            _fix41afc19_skip_reason_v19 = _fix41afc19_skip_reason_v19 + ":" + _m[:160]
                    except Exception:
                        pass

                # REFACTOR35: guard against schema-only rebuilds leaking debug keys into PMC
                # Keep only keys that exist in the frozen schema.
                try:
                    _schema_keys = set((analysis.get('metric_schema_frozen') or {}).keys()) if isinstance(analysis.get('metric_schema_frozen'), dict) else set()
                    if isinstance(_rebuilt, dict) and _schema_keys:
                        _rebuilt = {k: v for (k, v) in _rebuilt.items() if isinstance(k, str) and k in _schema_keys and isinstance(v, dict)}
                except Exception:
                    pass

                if isinstance(_rebuilt, dict) and _rebuilt:
                    current_metrics_for_display = dict(_rebuilt)
                    _fix41afc19_skip_reason_v19 = "applied_v19_display_rebuild:" + str(_fn_name)
                    try:
                        _fix41afc19_keys_sample_v19 = list(current_metrics_for_display.keys())[:10]
                    except Exception:
                        pass
                        _fix41afc19_keys_sample_v19 = []
                else:
                    _fix41afc19_skip_reason_v19 = _fix41afc19_skip_reason_v19 or "rebuilt_empty_or_non_dict"

        # Apply display override (used by diff below)
        locals()["current_metrics"] = current_metrics_for_display  # keep variable name used by downstream diff
        # Purpose: Override current_metrics_for_display with schema-anchored rebuild
        try:
            _fix2d9_over, _fix2d9_diag = _fix2d9_schema_anchored_rebuild_current_metrics_v1(
                prev_response=prev_response,
                pool=_pool,
                web_context=web_context,
            )
            try:
                output.setdefault('results', {}).setdefault('debug', {})
                output['results']['debug']['fix2d9_schema_anchored_rebuild_v1'] = _fix2d9_diag
            except Exception:
                pass
            if isinstance(_fix2d9_over, dict) and _fix2d9_over:
                current_metrics_for_display = dict(_fix2d9_over)
                locals()['current_metrics'] = current_metrics_for_display
                try:
                    output.setdefault('results', {})['primary_metrics_canonical'] = current_metrics_for_display
                except Exception:
                    pass
        except Exception:
            pass
        # Purpose:
        #   - Stamp exec code version + join mode into results.debug
        #   - Propagate current_metrics_for_display into results.primary_metrics_canonical
        #     so downstream render/diff can populate the 'Current' column.
        try:
            output.setdefault('results', {}).setdefault('debug', {})
            output['results']['debug']['__exec_code_version'] = globals().get('CODE_VERSION')
            try:
                output['results']['debug']['__exec_join_mode'] = _fix2d6_get_diff_join_mode_v1()
            except Exception:
                pass
        except Exception:
            pass
        try:
            if isinstance(current_metrics_for_display, dict) and current_metrics_for_display:
                output.setdefault('results', {})
                output['results']['primary_metrics_canonical'] = dict(current_metrics_for_display)
                try:
                    output['results'].setdefault('primary_response', {})
                    if isinstance(output['results']['primary_response'], dict):
                        output['results']['primary_response']['primary_metrics_canonical'] = dict(current_metrics_for_display)
                except Exception:
                    pass
                output.setdefault('debug', {})
                if isinstance(output['debug'], dict):
                    output['debug']['fix2d7_propagate_current_canon_v1'] = {
                        'applied': True,
                        'count': int(len(current_metrics_for_display)),
                        'keys_sample': list(current_metrics_for_display.keys())[:10],
                    }
        except Exception:
            pass
        # LLM01: attach evidence snippets + offsets to current metrics (additive only; winners/values unchanged).
        try:
            _pmc_llm01 = None
            try:
                _pmc_llm01 = (output.get("results") or {}).get("primary_metrics_canonical")
            except Exception:
                _pmc_llm01 = None
            _pool_llm01 = None
            try:
                _res = output.get("results") if isinstance(output.get("results"), dict) else {}
                _pool_llm01 = _res.get("baseline_sources_cache") or _res.get("baseline_sources_cache_current") or locals().get("baseline_sources_cache") or locals().get("baseline_sources_cache_current")
            except Exception:
                _pool_llm01 = None
            _schema_llm01 = None
            try:
                _schema_llm01 = (analysis.get("metric_schema_frozen") if isinstance(analysis, dict) else None)
            except Exception:
                _schema_llm01 = None
            _dbg_llm01 = _yureeka_get_debug_bucket_v1(output, default_path="evolution")
            _llm01_attach_evidence_snippets_to_pmc_v1(
                pmc=_pmc_llm01,
                baseline_sources_cache=_pool_llm01,
                metric_schema=_schema_llm01,
                question=str(output.get("question") or (analysis.get("question") if isinstance(analysis, dict) else "") or ""),
                stage="evolution",
                out_debug=_dbg_llm01,
            )
        except Exception:
            pass



        try:
            _fix41afc19_keys_sample_v19 = _fix41afc19_keys_sample_v19 or (list(current_metrics_for_display.keys())[:10] if isinstance(current_metrics_for_display, dict) else [])
        except Exception:
            pass

        # Heuristic: pick the known canonical keys if present, else infer from schema/keys.
        _key_candidates = [
            "units_sold_2024__unit_sales",
            "market_share_2024__percent",
            "projected_market_share_2026__percent",
            "projected_market_share_2030__percent",
            "yoy_growth_rate_2024__percent",
            "cagr_2024__percent",
        ]

        try:
            prev_canon = (prev_response or {}).get("primary_metrics_canonical") if isinstance(prev_response, dict) else {}
            if not isinstance(prev_canon, dict):
                prev_canon = {}

            cur_canon = current_metrics_for_display if isinstance(current_metrics_for_display, dict) else {}


            # Infer projected-share keys if the exact ones are not present
            if isinstance(prev_canon, dict) and isinstance(cur_canon, dict):
                all_keys = set(list(prev_canon.keys()) + list(cur_canon.keys()))
                for k in sorted(all_keys):
                    lk = k.lower()
                    if ("project" in lk or "proj" in lk) and ("share" in lk or "market_share" in lk) and ("2026" in lk or "2030" in lk) and k not in _key_candidates:
                        _key_candidates.append(k)

            # Flatten snapshot candidates once (for top3 glimpse)
            flat = []
            try:
                _pool_for_flat = locals().get("baseline_sources_cache") if isinstance(locals().get("baseline_sources_cache"), list) else []
                for sr in _pool_for_flat or []:
                    if isinstance(sr, dict):
                        for c in (sr.get("extracted_numbers") or []):
                            if isinstance(c, dict):
                                flat.append(c)
            except Exception:
                pass
                flat = []

            def _cand_has_unit_evidence(c: dict) -> bool:
                try:
                    if (c.get("unit_tag") or c.get("unit") or c.get("unit_norm") or c.get("unit_raw") or "").strip():
                        return True
                    if (c.get("currency") or c.get("currency_symbol") or "").strip():
                        return True
                    if c.get("is_percent") or c.get("has_percent"):
                        return True
                    if (c.get("base_unit") or "").strip():
                        return True
                    if (c.get("unit_family") or "").strip():
                        return True
                    if isinstance(c.get("unit_tokens"), list) and c.get("unit_tokens"):
                        return True
                except Exception:
                    return False
                return False

            # Schema lookup (if available)
            schema = {}
            try:
                if isinstance(previous_data, dict):
                    pr = previous_data.get("primary_response") if isinstance(previous_data.get("primary_response"), dict) else previous_data
                    schema = (pr.get("metric_schema_frozen") or {}) if isinstance(pr, dict) else {}
            except Exception:
                pass
                schema = {}

            def _score_candidate_for_key(c: dict, ckey: str) -> int:
                try:
                    md = schema.get(ckey) if isinstance(schema, dict) else {}
                    kws = []
                    if isinstance(md, dict):
                        kws = md.get("keywords") or md.get("keyword_hints") or []
                    kws = [str(k).lower() for k in kws if str(k).strip()]
                    ctx = (c.get("context") or c.get("window") or c.get("context_window") or "").lower()
                    s = 0
                    for k in kws[:25]:
                        if k and k in ctx:
                            s += 1
                    if _cand_has_unit_evidence(c):
                        s += 5
                    # Prefer anchor-matching if both present
                    try:
                        if (c.get("anchor_hash") or "") and isinstance(md, dict) and (md.get("anchor_hash") or ""):
                            if str(c.get("anchor_hash")) == str(md.get("anchor_hash")):
                                s += 10
                    except Exception:
                        return int(s)
                except Exception:
                    return 0

            # Build traces for up to 5 keys that exist in either prev/cur
            _keys_for_trace = []
            for k in _key_candidates:
                if k in (prev_canon or {}) or k in (cur_canon or {}):
                    _keys_for_trace.append(k)
                if len(_keys_for_trace) >= 5:
                    break
            if not _keys_for_trace:
                # fallback: first 5 keys from prev canonical
                _keys_for_trace = list(prev_canon.keys())[:5]

            for k in _keys_for_trace:
                pv = (prev_canon.get(k) or {}) if isinstance(prev_canon, dict) else {}
                cv = (cur_canon.get(k) or {}) if isinstance(cur_canon, dict) else {}

                winner_src = "unknown"
                if _fix41afc19_attempted_v19 and "applied_v19_display_rebuild" in (_fix41afc19_skip_reason_v19 or ""):
                    winner_src = "analysis_canonical_rebuild_v19"
                elif bool(locals().get("_fix41afc19_applied")):
                    winner_src = "analysis_canonical_rebuild"
                else:
                    winner_src = "fallback_snapshot_selector"

                # Top3 glimpse from flat pool
                top3 = []
                try:
                    scored = sorted(flat, key=lambda c: _score_candidate_for_key(c, k), reverse=True)[:3]
                    for t in scored:
                        top3.append({
                            "raw": t.get("raw"),
                            "value_norm": t.get("value_norm"),
                            "unit_tag": t.get("unit_tag") or t.get("unit") or "",
                            "has_unit_evidence": bool(_cand_has_unit_evidence(t)),
                            "anchor_hash": t.get("anchor_hash"),
                        })
                except Exception:
                    pass
                    top3 = []

                _fix41afc19_winner_trace_v19[k] = {
                    "winner_source": winner_src,
                    "prev_value_norm": pv.get("value_norm"),
                    "prev_unit": pv.get("unit") or pv.get("unit_tag") or "",
                    "cur_value_norm": cv.get("value_norm"),
                    "cur_unit": cv.get("unit") or cv.get("unit_tag") or "",
                    "cur_has_unit_evidence": bool((cv.get("unit") or cv.get("unit_tag") or "").strip()),
                    "top3_candidates_glimpse": top3,
                }
        except Exception:
            pass

    except Exception:
        pass

    try:
        if isinstance(output.get("debug"), dict):
            output["debug"].setdefault("fix41afc19", {})
            output["debug"]["fix41afc19"]["attempted"] = bool(_fix41afc19_attempted_v19)
            # Keep the pre-existing 'applied' field as-is, but add applied_v19_display_override
            output["debug"]["fix41afc19"]["applied_v19_display_override"] = bool(_fix41afc19_attempted_v19 and ("applied_v19_display_rebuild" in (_fix41afc19_skip_reason_v19 or "")))
            # Never allow blank reason: prefer v19 skip_reason when earlier reason is empty
            _prev_reason = str(output["debug"]["fix41afc19"].get("reason") or "")
            if (not _prev_reason.strip()) and (_fix41afc19_skip_reason_v19 or "").strip():
                output["debug"]["fix41afc19"]["reason"] = str(_fix41afc19_skip_reason_v19)
            output["debug"]["fix41afc19"]["skip_reason_v19"] = str(_fix41afc19_skip_reason_v19 or "")
            output["debug"]["fix41afc19"]["pool_count_v19"] = int(_fix41afc19_pool_count_v19 or 0)
            output["debug"]["fix41afc19"]["pool_key_used_v19"] = str(locals().get("_fix41afc19_pool_key_used_v19") or locals().get("_pool_key_used") or "")
            output["debug"]["fix41afc19"]["rebuilt_keys_sample_v19"] = list(_fix41afc19_keys_sample_v19 or [])
            if _fix41afc19_winner_trace_v19:
                output["debug"]["evo_winner_trace_v1"] = _fix41afc19_winner_trace_v19
    except Exception:
        pass
    # "Current" from a canonical-for-render payload (analysis-aligned) WITHOUT
    # touching fastpath/hashing/snapshot-attach.
    #
    # Why:
    # - Evolution UI renders from diff rows (metric_changes), not analysis key-metrics.
    # - If diff rows source "Current" from raw extracted pools, unitless survivors
    #   (e.g., 170, 2) can win.
    # - We compute a late, render-only canonical dict from the frozen snapshot pool
    #   using the same rebuild semantics as analysis (best effort), then force the
    #   diff + row hydration to use it.
    #
    # Safety:
    # - Purely post-snapshot, post-hash: affects ONLY dashboard/diff rendering.
    # - Does NOT alter source selection, hashing inputs, injection lifecycle, fastpath.
    #
    # Diagnostics:
    # - output.debug.canonical_for_render_v1
    # - output.debug.canonical_for_render_row_audit_v1
    _canonical_for_render_applied = False
    _canonical_for_render_reason = ""
    _canonical_for_render_fn = ""
    _canonical_for_render_count = 0
    _canonical_for_render_keys_sample = []
    _canonical_for_render_replaced_current_metrics = False

    canonical_for_render = {}
    try:
        # Default: use whatever current_metrics we already have
        canonical_for_render = current_metrics if isinstance(current_metrics, dict) else {}
        # REFACTOR15: if refactors nested the current PMC under output['results'], recover it so diffing can't go empty.
        if (not isinstance(canonical_for_render, dict)) or (isinstance(canonical_for_render, dict) and not canonical_for_render):
            try:
                if isinstance(output.get("primary_metrics_canonical"), dict) and output.get("primary_metrics_canonical"):
                    canonical_for_render = output.get("primary_metrics_canonical")
            except Exception:
                pass
        if (not isinstance(canonical_for_render, dict)) or (isinstance(canonical_for_render, dict) and not canonical_for_render):
            try:
                _pr = output.get("primary_response") if isinstance(output.get("primary_response"), dict) else None
                if isinstance(_pr, dict) and isinstance(_pr.get("primary_metrics_canonical"), dict) and _pr.get("primary_metrics_canonical"):
                    canonical_for_render = _pr.get("primary_metrics_canonical")
            except Exception:
                pass
        if (not isinstance(canonical_for_render, dict)) or (isinstance(canonical_for_render, dict) and not canonical_for_render):
            try:
                _r = output.get("results") if isinstance(output.get("results"), dict) else None
                if isinstance(_r, dict) and isinstance(_r.get("primary_metrics_canonical"), dict) and _r.get("primary_metrics_canonical"):
                    canonical_for_render = _r.get("primary_metrics_canonical")
            except Exception:
                pass
        # Also publish for downstream consumers (UI + v2 diff builder)
        try:
            if isinstance(canonical_for_render, dict):
                output["primary_metrics_canonical"] = canonical_for_render
                if isinstance(output.get("primary_response"), dict):
                    output["primary_response"]["primary_metrics_canonical"] = canonical_for_render
        except Exception:
            pass

        # REFACTOR26: hydrate source_url for canonical metrics (in-place, schema-preserving)
        try:
            _pmc_tmp = output.get("primary_metrics_canonical") if isinstance(output, dict) else None
            if isinstance(_pmc_tmp, dict):
                _refactor26_hydrate_primary_metrics_canonical_source_urls_v1(_pmc_tmp)
        except Exception:
            pass

        # Goal:
        # - Stop seeding canonical_for_render from current_metrics because current_metrics may already
        #   contain year-like / unitless / junk winners (e.g., "2.0 B", "-6441").
        # - Force the downstream rebuild path (which is intended to be analysis-aligned) to run,
        #   while keeping fastpath/hashing/injection/snapshot attach untouched.
        #
        # Mechanism:
        # - If prev_response carries a frozen schema, disable the seed by default.
        # - Allow opt-out via env var EVO_CANONICAL_FOR_RENDER_ALLOW_SEED=1.
        # - Emit a small trace later via _canonical_for_render_reason tag.
        try:
            _allow_seed = str(os.getenv("EVO_CANONICAL_FOR_RENDER_ALLOW_SEED", "") or "").strip() in ("1", "true", "True", "yes", "YES")
            _has_schema = isinstance(prev_response, dict) and isinstance(prev_response.get("metric_schema_frozen") or {}, dict) and bool(prev_response.get("metric_schema_frozen"))
            if _has_schema and not _allow_seed:
                canonical_for_render = {}
                _canonical_for_render_reason = "v30_seed_disabled_force_rebuild"
        except Exception:
            pass

        # Even when current_metrics has "enough" keys, it can still be junk (year-like/unitless winners).
        # Detect suspicious existing canonical dict and force a render-only rebuild in that case.
        def _v21_yearlike(x):
            try:
                if x is None:
                    return False
                fx = float(x)
                if abs(fx - round(fx)) < 1e-9:
                    ix = int(round(fx))
                    return 1900 <= ix <= 2105
                return False
            except Exception:
                return False

        def _v21_metric_suspicious(m):
            try:
                if not isinstance(m, dict):
                    return True
                u = (m.get("unit") or m.get("unit_tag") or "").strip()
                vn = m.get("value_norm")
                if (not u) and (_v21_yearlike(vn) or vn is None):
                    return True
                return False
            except Exception:
                return True

        _suspicious_existing = False
        try:
            if isinstance(canonical_for_render, dict) and canonical_for_render:
                _keys_sample_chk = list(sorted(list(canonical_for_render.keys())))[:25]
                _sus = 0
                _tot = 0
                for _k in _keys_sample_chk:
                    _tot += 1
                    if _v21_metric_suspicious(canonical_for_render.get(_k)):
                        _sus += 1
                if _tot > 0:
                    _suspicious_existing = (_sus / float(_tot)) >= 0.30
        except Exception:
            pass
            _suspicious_existing = False

        # Best-effort: rebuild canonical-for-render from frozen snapshots using analysis-aligned builder.
        # Apply when current canonical is missing/suspiciously small.
        _need_render_rebuild = (not isinstance(canonical_for_render, dict)) or (len(canonical_for_render) < 3) or bool(_suspicious_existing)
        if _need_render_rebuild and isinstance(baseline_sources_cache, list) and baseline_sources_cache:
            # Resolve schema/anchors/canon from prev_response (analysis baseline)
            _schema = {}
            _anchors = {}
            _prev_canon = {}
            try:
                if isinstance(prev_response, dict):
                    _schema = prev_response.get("metric_schema_frozen") or {}
                    _anchors = prev_response.get("metric_anchors") or {}
                    _prev_canon = prev_response.get("primary_metrics_canonical") or {}
            except Exception:
                pass
                _schema, _anchors, _prev_canon = {}, {}, {}

            # Choose the best available analysis-aligned rebuild function
            _fn = globals().get("rebuild_metrics_from_snapshots_analysis_canonical_v1")
            if callable(_fn):
                try:
                    canonical_for_render = _fn(baseline_sources_cache, _schema, _anchors, _prev_canon)
                    _canonical_for_render_fn = "rebuild_metrics_from_snapshots_analysis_canonical_v1"
                except Exception:
                    pass
                    canonical_for_render = {}
            if (not canonical_for_render) and callable(globals().get("rebuild_metrics_from_snapshots_schema_only")):
                try:
                    _fn3 = globals().get("rebuild_metrics_from_snapshots_schema_only")
                    canonical_for_render = _fn3(baseline_sources_cache, _schema)
                    _canonical_for_render_fn = "rebuild_metrics_from_snapshots_schema_only"
                except Exception:
                    pass
                    canonical_for_render = {}

            if isinstance(canonical_for_render, dict) and canonical_for_render:
                _canonical_for_render_applied = True
                _canonical_for_render_reason = "applied_render_only_rebuild" if not bool(_suspicious_existing) else "applied_render_only_rebuild_forced_suspicious"
                _canonical_for_render_count = int(len(canonical_for_render))
                _canonical_for_render_keys_sample = list(sorted(list(canonical_for_render.keys())))[:12]
                _canonical_for_render_replaced_current_metrics = True
                try:
                    if not str(_canonical_for_render_fn or "").strip():
                        _canonical_for_render_fn = "unknown_rebuild_fn"
                except Exception:
                    pass
            else:
                _canonical_for_render_reason = "render_rebuild_failed_or_empty"
        else:
            _canonical_for_render_reason = "used_existing_current_metrics" if not bool(_suspicious_existing) else "forced_render_rebuild_due_to_suspicious_existing_failed"
            _canonical_for_render_count = int(len(canonical_for_render)) if isinstance(canonical_for_render, dict) else 0
            _canonical_for_render_keys_sample = list(sorted(list(canonical_for_render.keys())))[:12] if isinstance(canonical_for_render, dict) else []
    except Exception:
        pass
        canonical_for_render = current_metrics if isinstance(current_metrics, dict) else {}
        _canonical_for_render_reason = "exception_fallback_existing"

    # Problem observed:
    # - canonical_for_render rebuild may select junk numbers from the frozen pool
    #   (e.g., GlobeNewswire footer "2B" or email fragments "-6441") when anchors
    #   are not strictly enforced.
    #
    # Fix:
    # - If prev_response provides metric_anchors for a canonical_key, forcibly
    #   resolve the exact anchored candidate from baseline_sources_cache by matching
    #   anchor_hash and use that to populate canonical_for_render for that key.
    #
    # Scope / Safety:
    # - Render-only: affects ONLY canonical_for_render (dashboard "Current")
    # - Does NOT touch hashing, fastpath, injection lifecycle, or snapshot attach.
    #
    # Diagnostics:
    # - output.debug.canonical_for_render_anchor_enforce_v28 (summary)
    # - per-metric cm["diag"]["v28_anchor_enforced"] (when applied)
    _v28_anchor_enforce = {
        "attempted": False,
        "schema_keys": 0,
        "anchors_keys": 0,
        "hits": 0,
        "misses": 0,
        "hit_keys_sample": [],
        "miss_keys_sample": [],
        "note": "render-only anchor enforcement by anchor_hash against frozen extracted_numbers pool",
    }

    def _v28_iter_numbers_from_sources_cache(_sources_cache):
        try:
            for _src in (_sources_cache or []):
                if not isinstance(_src, dict):
                    continue
                _url = _src.get("url") or _src.get("source_url") or ""
                nums = _src.get("extracted_numbers") or []
                if isinstance(nums, list):
                    for _n in nums:
                        if isinstance(_n, dict):
                            yield _url, _n
        except Exception:
            return

    def _v28_pick_by_anchor_hash(_sources_cache, _anchor_hash: str):
        try:
            ah = str(_anchor_hash or "").strip()
            if not ah or ah == "None":
                return None
            for _url, _n in _v28_iter_numbers_from_sources_cache(_sources_cache):
                try:
                    if str(_n.get("anchor_hash") or "").strip() == ah:
                        out = dict(_n)
                        if _url and (not out.get("source_url")):
                            out["source_url"] = _url
                        return out
                except Exception:
                    pass
                    continue
            return None
        except Exception:
            return None

    def _v28_schema_unit_label(_schema_row: dict) -> str:
        try:
            if not isinstance(_schema_row, dict):
                return ""
            # Prefer schema unit_tag (human-friendly) then unit
            u = (_schema_row.get("unit_tag") or _schema_row.get("unit") or "").strip()
            # Small convenience mapping
            if u == "M":
                return "million units"
            if u == "B":
                return "billion"
            return u
        except Exception:
            return ""

    try:
        if isinstance(canonical_for_render, dict) and isinstance(baseline_sources_cache, list) and baseline_sources_cache:
            _v28_anchor_enforce["attempted"] = True
            _schema = {}
            _anchors = {}
            try:
                if isinstance(prev_response, dict):
                    _schema = prev_response.get("metric_schema_frozen") or {}
                    _anchors = prev_response.get("metric_anchors") or {}
            except Exception:
                pass
                _schema, _anchors = {}, {}
            _v28_anchor_enforce["schema_keys"] = int(len(_schema)) if isinstance(_schema, dict) else 0
            _v28_anchor_enforce["anchors_keys"] = int(len(_anchors)) if isinstance(_anchors, dict) else 0

            if isinstance(_anchors, dict) and _anchors:
                for _ckey, _ainfo in list(_anchors.items()):
                    try:
                        if not _ckey:
                            continue
                        if not isinstance(_ainfo, dict):
                            continue
                        _ah = _ainfo.get("anchor_hash") or _ainfo.get("anchor") or ""
                        if not str(_ah or "").strip() or str(_ah) == "None":
                            continue

                        cand = _v28_pick_by_anchor_hash(baseline_sources_cache, _ah)
                        if not isinstance(cand, dict):
                            _v28_anchor_enforce["misses"] += 1
                            if len(_v28_anchor_enforce["miss_keys_sample"]) < 12:
                                _v28_anchor_enforce["miss_keys_sample"].append(str(_ckey))
                            continue

                        # Build minimal schema-aligned canonical metric
                        srow = _schema.get(_ckey) if isinstance(_schema, dict) else None
                        unit_lbl = _v28_schema_unit_label(srow if isinstance(srow, dict) else {})
                        vnorm = cand.get("value_norm")
                        if vnorm is None:
                            vnorm = cand.get("value")
                        raw = (cand.get("raw") or "").strip()
                        if not raw:
                            try:
                                if vnorm is not None and unit_lbl:
                                    raw = f"{vnorm} {unit_lbl}".strip()
                                elif vnorm is not None:
                                    raw = str(vnorm)
                            except Exception:
                                pass
                                raw = ""

                        cm = canonical_for_render.get(_ckey) if isinstance(canonical_for_render, dict) else None
                        if not isinstance(cm, dict):
                            cm = {}
                        cm["value_norm"] = vnorm
                        cm["unit"] = unit_lbl
                        cm["unit_tag"] = unit_lbl
                        if raw:
                            cm["raw"] = raw
                        # Provide evidence and source hint
                        cm["source_url"] = cand.get("source_url") or cand.get("url") or _ainfo.get("source_url") or ""
                        cm["context_snippet"] = cand.get("context_snippet") or cand.get("context") or _ainfo.get("context_snippet") or ""
                        cm["evidence"] = [cand]
                        cm.setdefault("diag", {})
                        if isinstance(cm.get("diag"), dict):
                            cm["diag"]["v28_anchor_enforced"] = True
                            cm["diag"]["v28_anchor_hash"] = str(_ah)
                            cm["diag"]["v28_anchor_candidate_raw"] = cand.get("raw")
                            cm["diag"]["v28_anchor_candidate_unit"] = cand.get("unit") or cand.get("unit_tag") or ""
                            cm["diag"]["v28_anchor_candidate_value_norm"] = cand.get("value_norm")

                        canonical_for_render[_ckey] = cm
                        _v28_anchor_enforce["hits"] += 1
                        if len(_v28_anchor_enforce["hit_keys_sample"]) < 12:
                            _v28_anchor_enforce["hit_keys_sample"].append(str(_ckey))
                    except Exception:
                        pass
                        continue

    except Exception:
        pass

    # Purpose:
    #   Render-gate fallback (UNION / demo mode only).
    #   If V28 anchor enforcement was attempted but produced zero hits,
    #   populate canonical_for_render from current primary_metrics_canonical,
    #   and label metrics as unanchored-for-render.
    #
    # Safety:
    #   - Render-only (does not affect canonicalisation, hashing, snapshots, fastpath)
    #   - UNION mode only
    #   - No new try/except blocks inside compute_source_anchored_diff

    _fix2d11_join_mode = None
    if "_fix2d6_get_diff_join_mode_v1" in globals():
        _fix2d11_join_mode = _fix2d6_get_diff_join_mode_v1()

    if (
        isinstance(locals().get("_v28_anchor_enforce"), dict)
        and _v28_anchor_enforce.get("attempted")
        and int(_v28_anchor_enforce.get("hits") or 0) == 0
        and str(_fix2d11_join_mode or "").lower() == "union"
    ):
        _pmc = None
        if isinstance(output.get("results"), dict):
            _pmc = output["results"].get("primary_metrics_canonical")

        if isinstance(_pmc, dict) and _pmc:
            canonical_for_render = dict(_pmc)
            for _k, _cm in canonical_for_render.items():
                if isinstance(_cm, dict):
                    _cm.setdefault("diag", {})
                    if isinstance(_cm.get("diag"), dict):
                        _cm["diag"]["render_origin"] = "current_unanchored"

            _v28_anchor_enforce["fix2d11_fallback_applied"] = True
            _v28_anchor_enforce["fix2d11_fallback_count"] = len(canonical_for_render)
        else:
            _v28_anchor_enforce["fix2d11_fallback_applied"] = False
            _v28_anchor_enforce["fix2d11_fallback_reason"] = "no_current_primary_metrics_canonical"

    try:
        if isinstance(output.get("debug"), dict):
            output["debug"]["canonical_for_render_anchor_enforce_v28"] = _v28_anchor_enforce
    except Exception:
        pass

    # Problem:
    # - canonical_for_render can still select "junk" numerics (e.g., footer phone
    #   fragments like -6441 or marketing magnitudes like 2B) as Current, because
    #   the frozen extracted_numbers pool is noisy and some late selection paths
    #   lack strict schema gating.
    #
    # Fix (render-only):
    # - Apply a strict schema-compatibility gate for canonical_for_render values.
    # - Hard-reject phone/contact/email/footer-like contexts.
    # - If the existing canonical_for_render metric is suspicious (unitless for
    #   unit-required dimensions, or unit-incompatible like "B" for percent),
    #   attempt to replace it with the best compatible candidate from the frozen
    #   extracted_numbers pool.
    #
    # Safety:
    # - Render-only: affects ONLY canonical_for_render (dashboard Current).
    # - Does NOT touch fastpath replay, hashing universe, injection lifecycle,
    #   snapshot attach, or extraction.
    #
    # Diagnostics:
    # - output.debug.canonical_for_render_schema_gate_v29 (summary)
    # - per-metric cm["diag"]["v29_schema_gate_*"] flags (when applied)
    _v29_schema_gate = {
        "attempted": False,
        "canonical_keys": 0,
        "suspicious": 0,
        "replaced": 0,
        "kept": 0,
        "candidates_checked": 0,
        "replaced_keys_sample": [],
        "suspicious_keys_sample": [],
        "note": "render-only schema gate + junk reject on canonical_for_render",
    }

    def _v29_s(_x):
        try:
            return str(_x or "")
        except Exception:
            return ""

    def _v29_lower(_x):
        try:
            return _v29_s(_x).lower()
        except Exception:
            return ""

    def _v29_get_text_blob(*parts):
        try:
            out = []
            for p in parts:
                if not p:
                    continue
                if isinstance(p, (list, tuple)):
                    out.extend([_v29_s(z) for z in p if z])
                else:
                    out.append(_v29_s(p))
            return " ".join([z for z in out if z]).strip()
        except Exception:
            return ""

    def _v29_phoneish(text):
        # Catch common phone patterns including "+1-888-600-6441" and fragments.
        try:
            t = _v29_s(text)
            if not t:
                return False
            if re.search(r"\+\d[\d\-\s]{7,}\d", t):
                return True
            if re.search(r"\b\d{3}[-\s]\d{3}[-\s]\d{4}\b", t):
                return True
            if re.search(r"\bext\.?\s*\d+\b", t, re.I):
                return True
            return False
        except Exception:
            return False

    def _v29_junk_context(text):
        try:
            t = _v29_lower(text)
            if not t:
                return False
            junk_terms = [
                "contact", "email", "phone", "tel", "telephone", "fax", "call us",
                "press release", "copyright", "all rights reserved", "subscribe",
                "unsubscribe", "privacy policy", "terms of use", "cookie", "newsletter",
                "about us", "follow us", "for media", "media contact"
            ]
            if any(w in t for w in junk_terms):
                return True
            if _v29_phoneish(text):
                return True
            # Many PR footers include an email address
            if "@" in t and "." in t:
                return True
            return False
        except Exception:
            return False

    def _v29_unit_str(obj):
        try:
            if isinstance(obj, dict):
                return _v29_s(obj.get("unit") or obj.get("unit_tag") or obj.get("unit_cmp") or obj.get("cur_unit_cmp") or "")
            return ""
        except Exception:
            return ""

    def _v29_value_norm(obj):
        try:
            if isinstance(obj, dict):
                v = obj.get("value_norm")
                if v is None:
                    v = obj.get("cur_value_norm")
                if v is None:
                    v = obj.get("value")
                return v
            return None
        except Exception:
            return None

    def _v29_has_unit_evidence(obj):
        try:
            # Conservative: unit evidence if unit string non-empty OR raw contains %/$/€ etc.
            if not isinstance(obj, dict):
                return False
            u = _v29_unit_str(obj).strip()
            if u:
                return True
            raw = _v29_s(obj.get("raw") or "")
            if "%" in raw or "$" in raw or "€" in raw or "£" in raw:
                return True
            return False
        except Exception:
            return False

    def _v29_expected_dimension(schema_row):
        try:
            if isinstance(schema_row, dict):
                # Prefer FIX16 helper if present
                if "_fix16_expected_dimension" in globals():
                    return _fix16_expected_dimension(schema_row)
                return schema_row.get("dimension") or schema_row.get("unit_family") or ""
            return ""
        except Exception:
            return ""

    def _v29_schema_requires_unit(schema_row):
        try:
            dim = _v29_lower(_v29_expected_dimension(schema_row))
            if dim in ("currency", "percent", "rate", "ratio"):
                return True
            uf = _v29_lower(_v29_s(schema_row.get("unit_family") if isinstance(schema_row, dict) else ""))
            if uf in ("currency", "percent", "rate", "ratio"):
                return True
            # unit_sales / unit counts should have some "unit-ness"
            if "unit" in dim or "unit" in uf:
                return True
            return False
        except Exception:
            return False

    def _v29_unit_compatible(schema_row, cand_obj):
        try:
            if isinstance(schema_row, dict) and isinstance(cand_obj, dict):
                if "_fix16_unit_compatible" in globals():
                    return bool(_fix16_unit_compatible(schema_row, cand_obj))
            # fallback heuristic
            dim = _v29_lower(_v29_expected_dimension(schema_row))
            u = _v29_lower(_v29_unit_str(cand_obj))
            raw = _v29_lower(_v29_s(cand_obj.get("raw") or ""))
            if dim == "percent":
                return ("%"
                        in u) or ("percent" in u) or ("%" in raw)
            if dim == "currency":
                return any(sym in raw for sym in ["$", "€", "£"]) or ("usd" in u) or ("eur" in u) or ("sgd" in u) or ("currency" in u)
            if "unit" in dim:
                # must not be percent/currency-like
                if "%" in raw or "%" in u:
                    return False
                if any(sym in raw for sym in ["$", "€", "£"]):
                    return False
                # prefer explicit unit words
                if "unit" in u or "vehicle" in u or "car" in u or "sales" in u:
                    return True
                # allow million/billion with implied units only if raw/context says units/sales
                ctx = _v29_lower(_v29_get_text_blob(cand_obj.get("context_snippet"), cand_obj.get("context")))
                if ("unit" in ctx) or ("sales" in ctx) or ("vehicle" in ctx):
                    return True
                return False
            return True
        except Exception:
            return False

    def _v29_is_suspicious_current(schema_row, cm):
        try:
            if not isinstance(cm, dict):
                return True
            dim = _v29_lower(_v29_expected_dimension(schema_row))
            requires_unit = _v29_schema_requires_unit(schema_row)
            u = _v29_lower(_v29_unit_str(cm))
            v = _v29_value_norm(cm)
            blob = _v29_get_text_blob(cm.get("raw"), cm.get("context_snippet"), cm.get("source_url"))
            if _v29_junk_context(blob):
                return True
            if requires_unit and not _v29_has_unit_evidence(cm):
                return True
            # Percent metrics must not carry magnitude units like B/M
            if dim == "percent" and ("b" in u or "m" in u) and "%" not in u:
                return True
            if dim == "percent" and isinstance(v, (int, float)) and abs(float(v)) > 1000:
                return True
            # Unit sales must not be negative (phone fragments, ids)
            if "unit" in dim and isinstance(v, (int, float)) and float(v) < 0:
                return True
            return not _v29_unit_compatible(schema_row, cm)
        except Exception:
            return True

    def _v29_keywords(schema_row):
        try:
            if not isinstance(schema_row, dict):
                return []
            nm = _v29_lower(schema_row.get("name") or schema_row.get("label") or schema_row.get("display_name") or "")
            toks = [t for t in re.split(r"[^a-z0-9]+", nm) if t and len(t) >= 4]
            # prune common filler
            bad = set(["global", "projected", "market", "share", "sales", "volume", "units", "unit", "year"])
            toks = [t for t in toks if t not in bad]
            return toks[:10]
        except Exception:
            return []

    def _v29_score_candidate(schema_row, cand_obj):
        try:
            score = 0
            u = _v29_lower(_v29_unit_str(cand_obj))
            raw = _v29_lower(_v29_s(cand_obj.get("raw") or ""))
            ctx = _v29_lower(_v29_get_text_blob(cand_obj.get("context_snippet"), cand_obj.get("context")))
            dim = _v29_lower(_v29_expected_dimension(schema_row))
            if dim == "percent":
                if "%" in raw or "%" in u or "percent" in u:
                    score += 5
                if "b" in u or "m" in u:
                    score -= 4
            if "unit" in dim:
                if "unit" in ctx or "sales" in ctx or "vehicle" in ctx:
                    score += 3
                if "%" in raw or "$" in raw:
                    score -= 3
            if not _v29_junk_context(ctx + " " + raw):
                score += 1
            for kw in _v29_keywords(schema_row):
                if kw and kw in ctx:
                    score += 1
            return score
        except Exception:
            return 0

    try:
        if isinstance(canonical_for_render, dict) and isinstance(prev_response, dict) and isinstance(baseline_sources_cache, list) and baseline_sources_cache:
            _v29_schema_gate["attempted"] = True
            _schema = prev_response.get("metric_schema_frozen") or {}
            _v29_schema_gate["canonical_keys"] = int(len(canonical_for_render)) if isinstance(canonical_for_render, dict) else 0

            for _ckey, _cm in list(canonical_for_render.items()):
                try:
                    if not _ckey:
                        continue
                    srow = _schema.get(_ckey) if isinstance(_schema, dict) else None
                    if not isinstance(srow, dict):
                        # without schema, we cannot safely gate; keep
                        _v29_schema_gate["kept"] += 1
                        continue

                    if _v29_is_suspicious_current(srow, _cm):
                        _v29_schema_gate["suspicious"] += 1
                        if len(_v29_schema_gate["suspicious_keys_sample"]) < 12:
                            _v29_schema_gate["suspicious_keys_sample"].append(str(_ckey))

                        best = None
                        best_score = -10**9
                        # search frozen pool for compatible candidates
                        for cand in _v28_iter_numbers_from_sources_cache(baseline_sources_cache):
                            _v29_schema_gate["candidates_checked"] += 1
                            if not isinstance(cand, dict):
                                continue
                            blob = _v29_get_text_blob(cand.get("raw"), cand.get("context_snippet"), cand.get("context"), cand.get("source_url") or cand.get("url"))
                            if _v29_junk_context(blob):
                                continue
                            # normalize candidate
                            cobj = dict(cand)
                            # ensure value_norm present if possible
                            if cobj.get("value_norm") is None and cobj.get("value") is not None:
                                cobj["value_norm"] = cobj.get("value")
                            if not _v29_unit_compatible(srow, cobj):
                                continue
                            if _v29_schema_requires_unit(srow) and not _v29_has_unit_evidence(cobj):
                                continue
                            sc = _v29_score_candidate(srow, cobj)
                            if sc > best_score:
                                best_score = sc
                                best = cobj
                        if isinstance(best, dict):
                            prior_v = _v29_value_norm(_cm if isinstance(_cm, dict) else {})
                            prior_u = _v29_unit_str(_cm if isinstance(_cm, dict) else {})
                            # overwrite with best candidate
                            if not isinstance(_cm, dict):
                                _cm = {}
                            _cm["value_norm"] = best.get("value_norm")
                            _cm["unit"] = _v28_schema_unit_label(srow) or (_v29_unit_str(best) or _v28_schema_unit_label(srow))
                            _cm["unit_tag"] = _cm.get("unit")
                            if best.get("raw"):
                                _cm["raw"] = best.get("raw")
                            _cm["source_url"] = best.get("source_url") or best.get("url") or _cm.get("source_url") or ""
                            _cm["context_snippet"] = best.get("context_snippet") or best.get("context") or _cm.get("context_snippet") or ""
                            _cm["evidence"] = [best]
                            _cm.setdefault("diag", {})
                            if isinstance(_cm.get("diag"), dict):
                                _cm["diag"]["v29_schema_gate_replaced"] = True
                                _cm["diag"]["v29_schema_gate_prior_value_norm"] = prior_v
                                _cm["diag"]["v29_schema_gate_prior_unit"] = prior_u
                                _cm["diag"]["v29_schema_gate_best_score"] = best_score
                                _cm["diag"]["v29_schema_gate_best_raw"] = best.get("raw")
                                _cm["diag"]["v29_schema_gate_best_unit"] = best.get("unit") or best.get("unit_tag") or ""
                            canonical_for_render[_ckey] = _cm
                            _v29_schema_gate["replaced"] += 1
                            if len(_v29_schema_gate["replaced_keys_sample"]) < 12:
                                _v29_schema_gate["replaced_keys_sample"].append(str(_ckey))
                        else:
                            _v29_schema_gate["kept"] += 1
                    else:
                        _v29_schema_gate["kept"] += 1
                except Exception:
                    pass
                    continue
    except Exception:
        pass

    try:
        if isinstance(output.get("debug"), dict):
            output["debug"]["canonical_for_render_schema_gate_v29"] = _v29_schema_gate
    except Exception:
        pass
    # REFACTOR180: legacy diff_metrics_by_name row hydration removed.
    # Diff Metrics Panel V2 is authoritative, so the old 'diff_metrics_by_name' fallback (and its
    # canonical-for-render row rewrites) was dead code and increased maintenance surface.
    # Prepare a minimal wrapper so V2 can still (optionally) inspect a conventional response shape.
    cur_resp_for_diff = {
        "primary_metrics_canonical": (dict(canonical_for_render) if isinstance(canonical_for_render, dict) else dict(current_metrics) if isinstance(current_metrics, dict) else {}),
    }

    # Attach diagnostics for auditability
    try:
        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            output["debug"]["canonical_for_render_v1"] = {
                "applied": bool(_canonical_for_render_applied),
                "reason": str(_canonical_for_render_reason or ""),
                "fn": str(_canonical_for_render_fn or ""),
                "rebuilt_count": int(_canonical_for_render_count or 0),
                "keys_sample": list(_canonical_for_render_keys_sample or []),
                "diag_ext": (lambda: {
                    "current_metrics_count": int(len(current_metrics)) if isinstance(current_metrics, dict) else 0,
                    "baseline_sources_cache_current_rows": int(len(baseline_sources_cache_current)) if isinstance(baseline_sources_cache_current, list) else 0,
                    "baseline_sources_cache_prev_rows": int(len((locals().get("baseline_sources_cache_prev") or baseline_sources_cache or []))) if isinstance((locals().get("baseline_sources_cache_prev") or baseline_sources_cache), list) else 0,
                    "has_web_context": bool(isinstance(web_context, dict)),
                    "web_context_extra_urls_count": int(len(web_context.get("extra_urls") or [])) if isinstance(web_context, dict) and isinstance(web_context.get("extra_urls"), list) else 0,
                    "web_context_fix2v_binding": (web_context.get("fix2v_candidate_binding_v1") or {}) if isinstance(web_context, dict) else {},
                    "reason_is_empty_rebuild": bool(str(_canonical_for_render_reason or "") in ("render_rebuild_failed_or_empty", "forced_render_rebuild_due_to_suspicious_existing_failed")),
                })(),
                "replaced_current_metrics_for_render": bool(_canonical_for_render_replaced_current_metrics),
            }
            # Purpose:
            #   Diff/dashboard diagnostics consume results.debug.*, but canonical_for_render_v1
            #   was attached to output.debug only. Mirror into output.results.debug to prevent
            #   false "missing_output_debug.canonical_for_render_v1" diagnoses.
            #   (Additive; no behavior change)
            try:
                if isinstance(output.get("results"), dict):
                    output["results"].setdefault("debug", {})
                    if isinstance(output["results"].get("debug"), dict):
                        output["results"]["debug"]["canonical_for_render_v1"] = dict(output["debug"].get("canonical_for_render_v1") or {})
            except Exception:
                pass
    except Exception:
        pass

    # REFACTOR56: Diff Panel V2 lastmile (downsizing)
    # Objective:
    # - Keep Diff Panel V2 as the authoritative producer of the metric changes table feed.
    # - Do not emit metric_changes_legacy (removed).
    # Safety:
    # - Render-only. No inference. No changes to hashing/fastpath/snapshots/extraction.
    _diff_v2_rows = []
    _diff_v2_summary = None
    # REFACTOR45: Diff Panel V2 hardening (RecursionError-safe)
    # - Always pass minimal, acyclic wrappers into the V2 builder.
    # - Capture traceback for any V2 builder failure.
    # - If V2 fails, DO NOT overwrite previously computed rows; otherwise
    #   synthesize strict canonical-join rows so the panel never goes empty
    #   when prev/cur canonical maps are present.
    _prev_can_v2 = {}
    _cur_can_v2 = {}

    try:
        _fn_v2 = (globals().get("build_diff_metrics_panel_v2__rows_refactor47"))
        if callable(_fn_v2):
            # Previous (baseline) canonical map
            try:
                _prev_can_v2 = _diffpanel_v2__unwrap_primary_metrics_canonical(prev_response)
            except Exception:
                _prev_can_v2 = {}
            # If the unwrap returned an empty dict (a past silent failure mode), fall back.
            if (not isinstance(_prev_can_v2, dict)) or (not _prev_can_v2):
                try:
                    _prev_can_v2 = (
                        _refactor89_locate_pmc_dict(prev_response)
                        or _refactor89_locate_pmc_dict(previous_data)
                        or (prev_response.get("primary_metrics_canonical") if isinstance(prev_response, dict) else {})
                    )
                except Exception:
                    _prev_can_v2 = {}
            if not isinstance(_prev_can_v2, dict):
                _prev_can_v2 = {}

            # Make baseline failure mode explicit (diagnostic only; do not fabricate deltas)
            try:
                if isinstance(output.get("debug"), dict) and not _prev_can_v2:
                    _reason = "baseline_pmc_not_found"
                    if bool(locals().get("_prev_rehydrated")):
                        _reason += "_after_rehydrate"
                    output["debug"]["baseline_missing_reason_v1"] = str(_reason)
            except Exception:
                pass

            # Current canonical map (prefer already rebuilt current_metrics)
            try:
                if isinstance(current_metrics, dict) and current_metrics:
                    _cur_can_v2 = current_metrics
                else:
                    _cur_can_v2 = _diffpanel_v2__unwrap_primary_metrics_canonical(cur_resp_for_diff) if isinstance(cur_resp_for_diff, dict) else {}
            except Exception:
                _cur_can_v2 = {}
            if not isinstance(_cur_can_v2, dict):
                _cur_can_v2 = {}

            # Minimal wrappers (avoid passing the full evolution output dict)
            _prev_for_v2 = {"primary_metrics_canonical": dict(_prev_can_v2)}
            _cur_for_v2 = {"primary_metrics_canonical": dict(_cur_can_v2)}

            # Carry schema freeze map if present (helps unit-family expectations)
            try:
                _msf = None
                if isinstance(prev_response, dict):
                    _msf = prev_response.get("metric_schema_frozen") or (prev_response.get("results") or {}).get("metric_schema_frozen")
                if _msf is None and isinstance(output, dict):
                    _msf = output.get("metric_schema_frozen") or (output.get("results") or {}).get("metric_schema_frozen")
                if isinstance(_msf, dict) and _msf:
                    _prev_for_v2["metric_schema_frozen"] = _msf
                    _cur_for_v2["metric_schema_frozen"] = _msf
            except Exception:
                pass

            # Attach observed-number pools (source_results) for V2 "observed" promotion
            try:
                _sr = None
                if isinstance(baseline_sources_cache_current, list) and baseline_sources_cache_current:
                    _sr = baseline_sources_cache_current
                elif isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                    _sr = baseline_sources_cache
                elif isinstance(output, dict):
                    _sr = output.get("source_results") or ((output.get("results") or {}).get("source_results"))
                if isinstance(_sr, list) and _sr:
                    _cur_for_v2.setdefault("results", {})
                    if isinstance(_cur_for_v2.get("results"), dict) and (not isinstance(_cur_for_v2["results"].get("source_results"), list) or not _cur_for_v2["results"].get("source_results")):
                        _cur_for_v2["results"]["source_results"] = _sr
            except Exception:
                pass

            _diff_v2_rows, _diff_v2_summary = _fn_v2(_prev_for_v2, _cur_for_v2 or {})
    except Exception as _e:
        try:
            _dbg = output.setdefault("debug", {})
            if isinstance(_dbg, dict):
                # REFACTOR58: suppress diff_panel_v2_error emission; fallback builder handles rows.
                try:
                    if _tb is not None:
                        _dbg["diff_panel_v2_traceback"] = traceback.format_exc()
                except Exception:
                    pass
        except Exception:
            pass

        # If rows were already computed earlier in this function, preserve them.
        try:
            _existing = output.get("metric_changes")
            if isinstance(_existing, list) and _existing:
                _diff_v2_rows = _existing
                _diff_v2_summary = None
            else:
                # Strict schema-complete fallback (no inference, no heuristics)
                _diff_v2_rows = []
                _prev_map = _prev_can_v2 if isinstance(_prev_can_v2, dict) else {}
                _cur_map = _cur_can_v2 if isinstance(_cur_can_v2, dict) else {}

                def _vn(_m):
                    try:
                        if isinstance(_m, dict):
                            v = _m.get("value_norm")
                            if v is None:
                                v = _m.get("value")
                            return float(v) if v is not None else None
                    except Exception:
                        return None
                    return None

                def _unit(_m):
                    try:
                        if isinstance(_m, dict):
                            return (_m.get("unit_tag") or _m.get("unit") or None)
                    except Exception:
                        return None
                    return None

                # Prefer frozen schema keys if available (guarantees schema-complete rows)
                _schema_keys = []
                try:
                    _msf = None
                    if isinstance(prev_response, dict):
                        _msf = prev_response.get("metric_schema_frozen") or (prev_response.get("results") or {}).get("metric_schema_frozen")
                    if _msf is None and isinstance(output, dict):
                        _msf = output.get("metric_schema_frozen") or (output.get("results") or {}).get("metric_schema_frozen")
                    if _msf is None and isinstance(cur_resp_for_diff, dict):
                        _msf = cur_resp_for_diff.get("metric_schema_frozen") or (cur_resp_for_diff.get("results") or {}).get("metric_schema_frozen")
                    if isinstance(_msf, dict) and _msf:
                        _schema_keys = [str(k) for k in _msf.keys()]
                    elif isinstance(_msf, list) and _msf:
                        _schema_keys = [str(x) for x in _msf if x is not None]
                    _schema_keys = sorted([k for k in _schema_keys if isinstance(k, str) and k])
                except Exception:
                    _schema_keys = []

                try:
                    if _schema_keys:
                        _iter_keys = list(_schema_keys)
                        _mode = "schema_complete_fallback"
                    else:
                        _iter_keys = sorted({str(k) for k in list(_prev_map.keys()) + list(_cur_map.keys()) if k})
                        _mode = "union_fallback"
                except Exception:
                    _iter_keys = []
                    _mode = "empty"

                for _ck in _iter_keys:
                    _pm = _prev_map.get(_ck) if isinstance(_prev_map, dict) else None
                    _cm = _cur_map.get(_ck) if isinstance(_cur_map, dict) else None
                    _pm_ok = isinstance(_pm, dict) and bool(_pm)
                    _cm_ok = isinstance(_cm, dict) and bool(_cm)

                    _pv = _vn(_pm) if _pm_ok else None
                    _cv = _vn(_cm) if _cm_ok else None
                    _pu = _unit(_pm) if _pm_ok else None
                    _cu = _unit(_cm) if _cm_ok else None

                    # For display clarity, carry known unit across when only one side is present
                    try:
                        if (not _pm_ok) and _cm_ok and str(_cu or "").strip():
                            _pu = _cu
                        if _pm_ok and (not _cm_ok) and str(_pu or "").strip():
                            _cu = _pu
                    except Exception:
                        pass

                    _d = None
                    _pct = None
                    _is_comp = False

                    if _pm_ok and _cm_ok:
                        if (_pv is not None) and (_cv is not None) and str(_pu or "").strip() and str(_cu or "").strip() and str(_pu).strip() == str(_cu).strip():
                            _is_comp = True
                            _d = float(_cv) - float(_pv)
                            if abs(float(_pv)) > 1e-12:
                                _pct = (_d / float(_pv)) * 100.0
                            if abs(_d) < 1e-9:
                                _ctype = "unchanged"
                                _d = 0.0
                            elif _d > 0:
                                _ctype = "increased"
                            else:
                                _ctype = "decreased"
                        else:
                            # Both present but not comparable (usually unit mismatch)
                            if str(_pu or "").strip() and str(_cu or "").strip() and str(_pu).strip() != str(_cu).strip():
                                _ctype = "unit_mismatch"
                            else:
                                _ctype = "found"
                    elif (not _pm_ok) and _cm_ok:
                        _ctype = "missing_baseline"
                    elif _pm_ok and (not _cm_ok):
                        _ctype = "missing_current"
                    else:
                        _ctype = "missing_both"

                    _name = None
                    try:
                        _name = ((_pm or {}).get("name") if _pm_ok else None) or ((_cm or {}).get("name") if _cm_ok else None) or _ck
                    except Exception:
                        _name = _ck

                    _src = None
                    try:
                        if _cm_ok:
                            _src = _cm.get("source_url") or _cm.get("url")
                    except Exception:
                        _src = None

                    _diff_v2_rows.append({
                        "canonical_key": _ck,
                        "name": _name,
                        "previous_value": _pv,
                        "current_value": (_cv if _cm_ok else "N/A"),
                        "previous_unit": _pu,
                        "current_unit": (_cu if _cm_ok else _cu),
                        "prev_value_norm": _pv,
                        "cur_value_norm": (_cv if _cm_ok else None),
                        "delta_abs": _d,
                        "delta_pct": _pct,
                        "change_type": _ctype,
                        "baseline_is_comparable": bool(_is_comp),
                        "current_method": "strict_schema_fallback_v2",
                        "source_url": _src,
                        "schema_frozen_key": bool(_ck in _schema_keys) if _schema_keys else False,
                    })

                _diff_v2_summary = {
                    "rows_total": int(len(_diff_v2_rows)),
                    "builder_id": "REFACTOR74_STRICT_SCHEMA_FALLBACK",
                    "mode": _mode,
                    "schema_keys_total": int(len(_schema_keys)) if _schema_keys else 0,
                }

        except Exception:
            _diff_v2_rows, _diff_v2_summary = ([], None)
    try:
        if isinstance(_diff_v2_summary, dict):
            _dbg = output.setdefault("debug", {})
            if isinstance(_dbg, dict):
                _dbg["diff_panel_v2_summary"] = _diff_v2_summary
    except Exception:
        pass

    # Option B: override UI feed if V2 emitted any rows
    try:
        if isinstance(_diff_v2_rows, list) and _diff_v2_rows:
            metric_changes = _diff_v2_rows
            try:
                found = int(_diff_v2_summary.get("rows_total", found)) if isinstance(_diff_v2_summary, dict) else found
            except Exception:
                pass
    except Exception:
        pass

    output["metric_changes"] = metric_changes or []
    # REFACTOR68: recompute summary + stability from the final canonical-first diff rows.
    # This prevents stale 0 counters (and stability=0) when we swap the UI feed to canonical-first diff rows.
    try:
        _refactor13_recompute_summary_and_stability_v1(output)
    except Exception:
        pass
    output["source_results"] = baseline_sources_cache[:50]
    output["sources_checked"] = len(baseline_sources_cache)
    output["sources_fetched"] = len(baseline_sources_cache)

    try:
        total_nums = 0
        for sr in baseline_sources_cache or []:
            if isinstance(sr, dict) and isinstance(sr.get("extracted_numbers"), list):
                total_nums += len(sr.get("extracted_numbers") or [])
        output["numbers_extracted_total"] = int(total_nums)
    except Exception:
        pass

    output["message"] = "Source-anchored evolution completed (snapshot-gated, analysis-aligned)."
    output["interpretation"] = "Evolution used cached source snapshots only; no brute-force candidate harvesting."

    # - If a diff row shows a year-like integer as current for a unit-required metric,
    #   emit a compact trace: origin, schema unit_family, current fields, and top candidates.
    try:
        if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
            bad_traces = {}
            # Build a flattened candidate pool once (from snapshots only)
            flat = []
            for sr in baseline_sources_cache or []:
                if isinstance(sr, dict):
                    for c in (sr.get("extracted_numbers") or []):
                        if isinstance(c, dict):
                            flat.append(c)

            def _is_yearlike(v):
                try:
                    iv = int(float(v))
                    return 1900 <= iv <= 2100
                except Exception:
                    return False

            def _schema_unit_required(md: dict, ckey: str = "") -> bool:
                uf = ((md or {}).get("unit_family") or (md or {}).get("unit") or "").strip().lower()
                if uf in {"currency", "percent", "rate", "ratio"}:
                    return True
                ck = (ckey or "").lower().strip()
                return ck.endswith("__currency") or ck.endswith("__percent") or ck.endswith("__rate") or ck.endswith("__ratio")

            def _cand_unit_evidence(c: dict) -> bool:
                if not isinstance(c, dict):
                    return False
                if (c.get("unit_tag") or c.get("unit") or c.get("unit_norm") or c.get("unit_raw") or "").strip():
                    return True
                if (c.get("currency") or c.get("currency_symbol") or "").strip():
                    return True
                if c.get("is_percent") or c.get("has_percent"):
                    return True
                if (c.get("base_unit") or "").strip():
                    return True
                if (c.get("unit_family") or "").strip():
                    return True
                if isinstance(c.get("unit_tokens"), list) and c.get("unit_tokens"):
                    return True
                return False

            schema = {}
            try:
                if isinstance(previous_data, dict):
                    pr = previous_data.get("primary_response") if isinstance(previous_data.get("primary_response"), dict) else previous_data
                    schema = (pr.get("metric_schema_frozen") or {}) if isinstance(pr, dict) else {}
            except Exception:
                pass
                schema = {}

            for row in output.get("metric_changes") or []:
                try:
                    ckey = row.get("canonical_key") or row.get("canonical") or ""
                    md = schema.get(ckey) if isinstance(schema, dict) else None
                    if not _schema_unit_required(md or {}, ckey):
                        continue

                    cur_val = row.get("current_value_norm")
                    cur_unit = (row.get("cur_unit_cmp") or row.get("current_unit") or "").strip()
                    if cur_val is None:
                        continue
                    if not _is_yearlike(cur_val):
                        continue
                    if cur_unit:
                        continue

                    kws = []
                    if isinstance(md, dict):
                        kws = md.get("keywords") or md.get("keyword_hints") or []
                    kws = [str(k).lower() for k in kws if str(k).strip()]

                    def _hit_score(c):
                        ctx = (c.get("context") or c.get("window") or c.get("context_window") or "").lower()
                        score = 0
                        for k in kws[:25]:
                            if k and k in ctx:
                                score += 1
                        if _cand_unit_evidence(c):
                            score += 5
                        if _is_yearlike(c.get("value_norm")) and not _cand_unit_evidence(c):
                            score -= 5
                        return score

                    top = sorted(flat, key=_hit_score, reverse=True)[:10]
                    bad_traces[ckey or row.get("name") or "unknown_metric"] = {
                        "current_value_norm": cur_val,
                        "cur_unit_cmp": cur_unit,
                        "schema_unit_family": (md or {}).get("unit_family") if isinstance(md, dict) else "",
                        "origin": output["debug"]["fix35"].get("current_metrics_origin", "unknown"),
                        "top_candidates": [
                            {
                                "raw": t.get("raw"),
                                "value_norm": t.get("value_norm"),
                                "unit_tag": t.get("unit_tag"),
                                "unit_family": t.get("unit_family"),
                                "base_unit": t.get("base_unit"),
                                "has_unit_evidence": bool(_cand_unit_evidence(t)),
                                "anchor_hash": t.get("anchor_hash"),
                            }
                            for t in top
                        ],
                    }
                except Exception:
                    pass
                    continue

            if bad_traces:
                output["debug"]["fix35"]["bad_current_traces"] = bad_traces
                output["debug"]["fix35"]["bad_current_trace_count"] = len(bad_traces)
    except Exception:
        pass

    # - Mirrors to output.results.debug.inj_trace_v1 for a fixed location across modes
    # - Does NOT affect fastpath decisioning
    try:
        _wc_diag = {}
        if isinstance(web_context, dict):
            _wc_diag = web_context.get("diag_injected_urls") or {}
        _hash_inputs = _inj_diag_hash_inputs_from_bsc(baseline_sources_cache)

        # Determine path from existing fix35 origin stamp
        _path = ""
        try:
            origin = ""
            if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                origin = str(output["debug"]["fix35"].get("current_metrics_origin") or "")
            if "fastpath" in origin:
                _path = "fastpath"
            elif "rebuild" in origin:
                _path = "rebuild"
            else:
                _path = "unknown"
        except Exception:
            pass
            _path = "unknown"

        # Rebuild "selected" URLs: unique source_url from current metrics (if present)
        _selected = []
        try:
            cm = output.get("current_metrics")
            if isinstance(cm, dict):
                for v in cm.values():
                    if isinstance(v, dict):
                        u = (v.get("source_url") or "").strip()
                        if u:
                            _selected.append(u)
            _selected = sorted(set(_selected))
        except Exception:
            pass
            _selected = []

        # For evolution, rebuild_pool is effectively the hash input URL universe available via snapshots
        # - If injected URLs exist but are not in hash_inputs, we record the most likely reason:
        #     * excluded_by_flag_default_off  (when inclusion switch is OFF)
        #     * missing_from_hash_inputs      (when switch ON but still absent)
        _evo_hash_reasons = {}
        try:
            _evo_persisted = []
            if isinstance(_wc_diag, dict):
                _evo_persisted = _inj_diag_norm_url_list(_wc_diag.get("persisted_norm") or _wc_diag.get("persisted") or [])
            _evo_inj = _inj_diag_norm_url_list(
                (_wc_diag.get("intake_norm") if isinstance(_wc_diag, dict) else []) or
                (_wc_diag.get("ui_norm") if isinstance(_wc_diag, dict) else []) or
                []
            )
            _evo_targets = _evo_persisted or _evo_inj
            _hashset = set(_inj_diag_norm_url_list(_hash_inputs or []))
            _incl = _inj_hash_policy_should_include(_evo_targets)
            for u in _evo_targets:
                if u in _hashset:
                    _evo_hash_reasons[u] = "present_in_hash_inputs"
                else:
                    _evo_hash_reasons[u] = ("excluded_by_policy_disable" if _inj_hash_policy_explicit_disable() else ("excluded_by_legacy_switch_default_off" if not _incl else "missing_from_hash_inputs"))
        except Exception:
            pass
            _evo_hash_reasons = {}

        # Goal:
        # - Evolution often bypasses fetch_web_context(), so "admitted" may be unset even
        #   when URLs are actually in the current scrape/hash universe.
        # - This patch makes inj_trace_v1 "admitted_norm" reflect the same practical
        #   universe used for scraping/hashing (without changing any control flow).
        #
        # Policy (diagnostics only):
        # - If diag.admitted is empty but hash_inputs are present, treat hash_inputs as
        #   admitted for trace purposes.
        # - Prefer any explicit FIX24 evo merge set if present (urls_after_merge_norm).
        try:
            if isinstance(_wc_diag, dict):
                _ad = _inj_diag_norm_url_list(_wc_diag.get("admitted") or _wc_diag.get("extra_urls_admitted") or [])
                if not _ad:
                    _pref = []
                    try:
                        _pref = _inj_diag_norm_url_list(_wc_diag.get("urls_after_merge_norm") or [])
                    except Exception:
                        pass
                        _pref = []
                    if not _pref:
                        _pref = _inj_diag_norm_url_list(_hash_inputs or [])
                    if _pref:
                        _wc_diag["admitted"] = list(_pref)
                        _wc_diag.setdefault("admission_reason", "trace_fallback_to_hash_inputs_or_urls_after_merge")
        except Exception:
            pass

        # Goal:
        # - In fastpath/replay or when current_metrics lacks source_url fields,
        #   rebuild_selected_norm can be empty, creating misleading pool_minus_selected.
        #
        # Diagnostics-only fallback:
        # - If rebuild_selected is empty but rebuild_pool/hash_inputs exists, treat
        #   selected as the full pool for trace purposes.
        try:
            if (not _selected) and _hash_inputs:
                _selected = list(_inj_diag_norm_url_list(_hash_inputs))
                if isinstance(_wc_diag, dict):
                    _wc_diag.setdefault("rebuild_selected_reason", "trace_fallback_to_hash_inputs_no_current_metric_sources")
        except Exception:
            pass

        #
        # Why:
        # - inj_trace_v1 shows injected URLs at intake but missing from admitted (unknown_rejected_pre_admission).
        # - We must "pin" injection at the admission boundary for evolution (delta-only), and emit a post-fetch trace
        #   because inj_trace_v1 may be emitted before the fetch/persist stage completes.
        #
        # What:
        # - If injected URLs are present (from web_context.extra_urls OR diag ui fields) we force-add them into
        #   _wc_diag["admitted"] so the trace reflects admission override deterministically (delta-only).
        # - Additionally, emit inj_trace_v2_postfetch using best-effort enrichment from scraped_meta / cur_bsc if available.
        #
        # Safety:
        # - Purely additive; never raises; does not modify fastpath rules or hashing.
        try:
            _fx12_wc = web_context if isinstance(web_context, dict) else {}
            _fx12_diag = _wc_diag if isinstance(locals().get("_wc_diag"), dict) else (_fx12_wc.get("diag_injected_urls") if isinstance(_fx12_wc.get("diag_injected_urls"), dict) else {})
            _fx12_inj_raw = []
            try:
                _fx12_inj_raw = list(_fx12_wc.get("extra_urls") or [])
            except Exception:
                pass
                _fx12_inj_raw = []
            if not _fx12_inj_raw and isinstance(_fx12_diag, dict):
                try:
                    _fx12_inj_raw = list(_fx12_diag.get("ui_norm") or _fx12_diag.get("intake_norm") or [])
                except Exception:
                    pass
                    _fx12_inj_raw = []
            _fx12_inj = _inj_diag_norm_url_list(_fx12_inj_raw or [])
            if _fx12_inj and isinstance(_wc_diag, dict):
                _fx12_prev_ad = _inj_diag_norm_url_list(_wc_diag.get("admitted") or [])
                _fx12_forced = sorted(list(set(_fx12_inj) - set(_fx12_prev_ad)))
                if _fx12_forced:
                    _wc_diag["admitted"] = list(_inj_diag_stable_dedupe_order((_fx12_prev_ad or []) + _fx12_forced))
                    _wc_diag.setdefault("forced_admit_reasons", {})
                    if isinstance(_wc_diag.get("forced_admit_reasons"), dict):
                        for _u in _fx12_forced:
                            _wc_diag["forced_admit_reasons"][_u] = "forced_admit_injected_url_override"
                    try:
                        output.setdefault("debug", {})
                        if isinstance(output.get("debug"), dict):
                            output["debug"].setdefault("fix41afc12", {})
                            if isinstance(output["debug"].get("fix41afc12"), dict):
                                output["debug"]["fix41afc12"].update({
                                    "forced_admit_injected_count": int(len(_fx12_forced)),
                                    "forced_admit_injected_urls": list(_fx12_forced),
                                })
                    except Exception:
                        pass
        except Exception:
            pass
        _trace = _inj_trace_v1_build(
            diag_injected_urls=_wc_diag if isinstance(_wc_diag, dict) else {},
            hash_inputs=_hash_inputs,
            stage="evolution",
            path=_path,
            rebuild_pool=_hash_inputs,
            rebuild_selected=_selected,
            hash_exclusion_reasons=_evo_hash_reasons,
        )

        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            output["debug"]["inj_trace_v1"] = _trace

        # Fixed location mirror: results.debug.inj_trace_v1
        output.setdefault("results", {})
        if isinstance(output.get("results"), dict):
            output["results"].setdefault("debug", {})
            if isinstance(output["results"].get("debug"), dict):
                output["results"]["debug"]["inj_trace_v1"] = _trace

                #
                # Emit a second trace after best-effort enrichment from scraped_meta / baseline cache so that
                # attempted/persisted deltas reflect the true post-fetch state (inj_trace_v1 may be earlier).
                try:
                    _fx12_diag2 = dict(_wc_diag) if isinstance(_wc_diag, dict) else {}
                    try:
                        _sm = locals().get("scraped_meta")
                        if isinstance(_sm, dict):
                            _fx12_diag2 = _inj_trace_v1_enrich_diag_from_scraped_meta(_fx12_diag2, _sm, (_inj_extra_urls or []))
                    except Exception:
                        pass
                    try:
                        _cb = locals().get("cur_bsc") or locals().get("baseline_sources_cache") or locals().get("baseline_sources_cache_current")
                        if isinstance(_cb, list):
                            _fx12_diag2 = _inj_trace_v1_enrich_diag_from_bsc(_fx12_diag2, _cb)
                    except Exception:
                        pass
                    _trace2 = _inj_trace_v1_build(
                        diag_injected_urls=_fx12_diag2 if isinstance(_fx12_diag2, dict) else {},
                        hash_inputs=_hash_inputs,
                        stage="evolution",
                        path=str(_path or "evolution") + "_postfetch",
                        rebuild_pool=_hash_inputs,
                        rebuild_selected=_selected,
                        hash_exclusion_reasons=_evo_hash_reasons,
                    )
                    output["debug"]["inj_trace_v2_postfetch"] = _trace2
                    if isinstance(output.get("results"), dict) and isinstance(output["results"].get("debug"), dict):
                        output["results"]["debug"]["inj_trace_v2_postfetch"] = _trace2
                except Exception:
                    pass

    except Exception:
        pass

    #
    # Why:
    # - Conclusively identify which structure the Evolution dashboard reads for
    #   the "Current" column, and whether upstream patches are modifying that
    #   exact structure.
    #
    # What:
    # - Emit a compact debug payload that:
    #     * states the dashboard read-path ("results.metric_changes[].current_value")
    #     * samples the first N metric_changes rows (canonical_key, current_value, unit hints, diag keys)
    # - Purely additive: no selection, hashing, or fastpath behavior changes.
    try:
        if isinstance(output, dict):
            _dbg = output.get("debug") if isinstance(output.get("debug"), dict) else {}
            if not isinstance(_dbg, dict):
                _dbg = {}
            rows = output.get("metric_changes") or []
            #
            # Why:
            # - Even after schema-only rebuild hardening, dashboard "Current" may still
            #   be hydrated from metric_changes rows that pull values from multiple
            #   paths (canonical, observed, legacy). We must prevent unitless bare
            #   years (e.g., 2024, 2030, including 2030.0) from appearing as metric
            #   values for non-year metrics.
            #
            # What:
            # - Post-process metric_changes in-place before the dashboard consumes it.
            # - If current value is yearlike AND unit is empty AND metric is not a
            #   year-as-value metric, blank it (N/A) and attach diagnostics.
            # - Keep a compact trace in output.debug.fix2d24_yearlike_current_trace_v1.
            def _fix2d24_is_yearlike(v, raw=None):
                try:
                    if v is None:
                        return False
                    fv = float(v)
                    if fv < 1900.0 or fv > 2100.0:
                        return False
                    if abs(fv - round(fv)) > 1e-9:
                        return False
                    rs = str(raw if raw is not None else v).strip()
                    rs = rs.replace(",", "")
                    if re.match(r"^\d{4}(?:\.0+)?$", rs):
                        return True
                    # allow e.g. '2030.0' in string form
                    if re.match(r"^\d{4}\.0+$", rs):
                        return True
                    return True
                except Exception:
                    return False

            def _fix2d24_metric_expects_year(ckey, name=None):
                try:
                    ck = str(ckey or "").lower()
                    nm = str(name or "").lower()
                    if ck.endswith("__year"):
                        return True
                    if "year" in ck and ("__" in ck):
                        # conservative: treat explicit year unit tags as year metrics
                        if "__year" in ck:
                            return True
                    if nm.strip() == "year":
                        return True
                    return False
                except Exception:
                    return False

            _fix2d24_filtered = 0
            _fix2d24_samples = []
            if isinstance(rows, list):
                for _r in rows:
                    if not isinstance(_r, dict):
                        continue
                    _ckey = _r.get("canonical_key")
                    _nm = _r.get("metric") or _r.get("name")
                    _unit = _r.get("cur_unit_cmp")
                    if _unit is None:
                        _unit = _r.get("current_unit")
                    if _unit is None:
                        _unit = _r.get("unit")
                    _unit = str(_unit or "").strip()
                    _cvn = _r.get("current_value_norm")
                    if _cvn is None:
                        _cvn = _r.get("cur_value_norm")
                    _raw = _r.get("current_value")
                    if _raw is None:
                        _raw = _r.get("cur_raw")

                    if _fix2d24_is_yearlike(_cvn, _raw) and (not _unit) and (not _fix2d24_metric_expects_year(_ckey, _nm)):
                        _fix2d24_filtered += 1
                        if len(_fix2d24_samples) < 25:
                            _fix2d24_samples.append({
                                "canonical_key": _ckey,
                                "name": _nm,
                                "blocked_value_norm": _cvn,
                                "blocked_raw": _raw,
                                "source_url": _r.get("source_url") or _r.get("cur_source_url"),
                            })
                        _r.setdefault("diag", {})
                        if isinstance(_r.get("diag"), dict):
                            _r["diag"]["fix2d24_yearlike_current_blocked"] = {
                                "blocked": True,
                                "value_norm": _cvn,
                                "raw": _raw,
                                "unit": _unit,
                            }
                        # blank current for dashboard consumption
                        _r["current_value"] = "N/A"
                        _r["current_value_norm"] = None
                        _r["cur_value_norm"] = None
                        # Why:
                        # - FIX2D24 correctly blocks unitless yearlike current values (e.g. 2024/2030)
                        #   but previously left Current as N/A. This patch immediately falls back to the
                        #   guarded inference pool and commits a binding current value into metric_changes.
                        # What:
                        # - Build a shared extracted-number pool from baseline_sources_cache(_current)
                        # - Score candidates with unit-family + keyword/context hints
                        # - Commit into current_value/current_value_norm/current_source/current_method
                        # - Attach explicit trace yearlike_current_blocked_then_inferred_v1
                        try:
                            _r_diag = _r.get('diag') if isinstance(_r.get('diag'), dict) else {}
                            _blocked_vn = _cvn
                            _blocked_raw = _raw
                            # lazily build pool once
                            if '_fix2d2l_pool_v1' not in locals():
                                _fix2d2l_pool_v1 = []
                                def _fix2d2l_add_from_bsc(_bsc_list):
                                    try:
                                        if not isinstance(_bsc_list, list):
                                            return
                                        for _src in _bsc_list:
                                            if not isinstance(_src, dict):
                                                continue
                                            _surl = _src.get('source_url') or _src.get('url')
                                            _nums = _src.get('extracted_numbers')
                                            if not isinstance(_nums, list):
                                                continue
                                            for _n in _nums:
                                                if not isinstance(_n, dict):
                                                    continue
                                                _vn = _n.get('value_norm')
                                                _rw = _n.get('raw')
                                                _ut = str(_n.get('unit_tag') or '').strip()
                                                _uf = str(_n.get('unit_family') or '').strip()
                                                _ctx = _n.get('context_snippet') or _n.get('context') or ''
                                                # derive minimal unit_family if missing
                                                if not _uf:
                                                    if _ut == '%':
                                                        _uf = 'percent'
                                                    elif _ut in {'USD','US$','$','EUR','€','GBP','£','SGD','S$','JPY','¥','CNY','RMB','CN¥'}:
                                                        _uf = 'currency'
                                                    elif _ut in {'M','B','T','K'}:
                                                        _uf = 'magnitude'
                                                _fix2d2l_pool_v1.append({
                                                    'value_norm': _vn,
                                                    'raw': _rw,
                                                    'unit_tag': _ut,
                                                    'unit_family': _uf,
                                                    'source_url': (_n.get('source_url') or _surl),
                                                    'context_snippet': _ctx,
                                                })
                                    except Exception:
                                        return
                                # collect from common locations
                                _fix2d2l_add_from_bsc(output.get('baseline_sources_cache_current'))
                                _fix2d2l_add_from_bsc(output.get('baseline_sources_cache'))
                                if isinstance(output.get('results'), dict):
                                    _fix2d2l_add_from_bsc(output['results'].get('baseline_sources_cache_current'))
                                    _fix2d2l_add_from_bsc(output['results'].get('baseline_sources_cache'))
                            _pool = locals().get('_fix2d2l_pool_v1') or []
                            # determine desired unit_family from row hints
                            _desired_uf = ''
                            try:
                                _hint_u = str(_r.get('prev_unit_cmp') or _r.get('unit') or _r.get('cur_unit_cmp') or '').strip().lower()
                                _hint_name = str(_nm or '').lower()
                                if '%' in _hint_u or 'percent' in _hint_u or 'share' in _hint_name:
                                    _desired_uf = 'percent'
                                elif any(x in _hint_u for x in ['usd','us$','$','eur','€','gbp','£','sgd','s$','jpy','¥','cny','rmb']):
                                    _desired_uf = 'currency'
                                elif any(x in _hint_name for x in ['sales','units','deliver','ship','vehicle']):
                                    _desired_uf = 'magnitude'
                            except Exception:
                                pass
                                _desired_uf = ''
                            # keyword set for context scoring
                            _kw = set()
                            try:
                                for tok in re.split(r"[^a-z0-9]+", str(_nm or _ckey or '').lower()):
                                    if len(tok) >= 4 and tok not in {'global','total','market','share','sales','units','value','year'}:
                                        _kw.add(tok)
                            except Exception:
                                pass
                                _kw = set()
                            # injected-first preference (FIX2D2M)
                            _inj_urls = set()
                            try:
                                # Prefer explicit injected sources when present in caches
                                def _fix2d2m_scan_injected(bsc):
                                    if not isinstance(bsc, list):
                                        return
                                    for it in bsc:
                                        try:
                                            if not isinstance(it, dict):
                                                continue
                                            if it.get('injected') is True:
                                                su = str(it.get('source_url') or '').strip()
                                                if su:
                                                    _inj_urls.add(su)
                                        except Exception:
                                            pass
                                            continue
                                _fix2d2m_scan_injected(output.get('baseline_sources_cache_current'))
                                _fix2d2m_scan_injected(output.get('baseline_sources_cache'))
                                if isinstance(output.get('results'), dict):
                                    _fix2d2m_scan_injected(output['results'].get('baseline_sources_cache_current'))
                                    _fix2d2m_scan_injected(output['results'].get('baseline_sources_cache'))
                            except Exception:
                                pass
                                _inj_urls = set()

                            def _fix2d2m_score_pool(_pool_in):
                                _sc = []
                                for _cand in _pool_in:
                                    try:
                                        if not isinstance(_cand, dict):
                                            continue
                                        _vn = _cand.get('value_norm')
                                        _rw = _cand.get('raw')
                                        if _fix2d24_is_yearlike(_vn, _rw):
                                            continue
                                        _uf = str(_cand.get('unit_family') or '').strip()
                                        _ut = str(_cand.get('unit_tag') or '').strip()
                                        _ctx = str(_cand.get('context_snippet') or '').lower()
                                        _score = 0.0
                                        if _desired_uf and _uf == _desired_uf:
                                            _score += 10.0
                                        if _desired_uf == 'percent' and ('%' in _ctx or _ut == '%'):
                                            _score += 3.0
                                        if _desired_uf == 'magnitude' and ('million' in _ctx or 'units' in _ctx):
                                            _score += 2.0
                                        if _desired_uf == 'currency' and any(x in _ctx for x in ['$', 'us$', 'usd', 'eur', '€', 'gbp', '£']):
                                            _score += 2.0
                                        _row_surl = str(_r.get('source_url') or _r.get('cur_source_url') or '').strip()
                                        if _row_surl and str(_cand.get('source_url') or '').strip() == _row_surl:
                                            _score += 2.0
                                        if _kw:
                                            _hits = sum(1 for k in _kw if k in _ctx)
                                            _score += min(6.0, 1.5 * _hits)
                                        _sc.append((_score, _cand))
                                    except Exception:
                                        pass
                                        continue
                                _sc.sort(key=lambda t: (t[0] if t and len(t)>0 else 0), reverse=True)
                                return _sc

                            # two-pass selection: injected-only then global
                            _pool_injected = []
                            if _inj_urls:
                                _pool_injected = [c for c in _pool if isinstance(c, dict) and str(c.get('source_url') or '').strip() in _inj_urls]
                            _scored_inj = _fix2d2m_score_pool(_pool_injected) if _pool_injected else []
                            _sel = _scored_inj[0][1] if _scored_inj and _scored_inj[0][0] >= 3.0 else None
                            _used_injected_pass = bool(_sel is not None)
                            _scored = _scored_inj
                            if _sel is None:
                                _scored = _fix2d2m_score_pool(_pool)
                                _sel = _scored[0][1] if _scored and _scored[0][0] >= 3.0 else None
                            _committed = False
                            _reason = 'no_eligible_candidates' if not _scored else ('score_below_threshold' if _sel is None else 'selected')
                            if isinstance(_sel, dict):
                                # commit into metric_changes row (UI-read fields)
                                _r['current_value_norm'] = _sel.get('value_norm')
                                _r['cur_value_norm'] = _sel.get('value_norm')
                                _r['current_value'] = (_sel.get('raw') if _sel.get('raw') not in (None, '') else str(_sel.get('value_norm')))
                                _r['current_source'] = _sel.get('source_url')
                                _r['current_method'] = 'yearlike_blocked_then_inferred'
                                # comparable only if prev exists
                                _pv = _r.get('previous_value_norm')
                                if _pv is None:
                                    _pv = _r.get('prev_value_norm')
                                _r['baseline_is_comparable'] = bool(_pv is not None and _r.get('current_value_norm') is not None)
                                _committed = True
                            # trace
                            _r.setdefault('diag', {})
                            if isinstance(_r.get('diag'), dict):
                                _r['diag']['yearlike_current_blocked_then_inferred_v1'] = {
                                    'blocked_value_norm': _blocked_vn,
                                    'blocked_raw': _blocked_raw,
                                    'desired_unit_family': _desired_uf or None,
                                    'pool_size': int(len(_pool)) if isinstance(_pool, list) else None,
                                    'eligible_scored': int(len(_scored)),
                                    'selected': bool(_sel is not None),
                                    'selected_value_norm': (_sel.get('value_norm') if isinstance(_sel, dict) else None),
                                    'selected_raw': (_sel.get('raw') if isinstance(_sel, dict) else None),
                                    'selected_unit_family': (_sel.get('unit_family') if isinstance(_sel, dict) else None),
                                    'selected_unit_tag': (_sel.get('unit_tag') if isinstance(_sel, dict) else None),
                                    'selected_source_url': (_sel.get('source_url') if isinstance(_sel, dict) else None),
                                    'committed': bool(_committed),
                                    'reason': _reason,
                                    'top3': [
                                        {
                                            'score': float(sc),
                                            'value_norm': c.get('value_norm') if isinstance(c, dict) else None,
                                            'raw': c.get('raw') if isinstance(c, dict) else None,
                                            'unit_family': c.get('unit_family') if isinstance(c, dict) else None,
                                            'unit_tag': c.get('unit_tag') if isinstance(c, dict) else None,
                                            'source_url': c.get('source_url') if isinstance(c, dict) else None,
                                        }
                                        for sc, c in (_scored[:3] if isinstance(_scored, list) else [])
                                    ],
                                }
                            # injected preference trace
                            try:
                                _r['diag'].setdefault('injected_source_preference_trace_v1', {})
                                _r['diag']['injected_source_preference_trace_v1'] = {
                                    'injected_present': bool(_inj_urls),
                                    'injected_urls': sorted(list(_inj_urls))[:3] if _inj_urls else [],
                                    'pass1_injected_pool_size': int(len(_pool_injected)) if isinstance(_pool_injected, list) else 0,
                                    'pass1_selected': bool(_used_injected_pass),
                                    'fallback_used': bool((not _used_injected_pass) and (_sel is not None)),
                                    'selected_source_url': (_sel.get('source_url') if isinstance(_sel, dict) else None),
                                }
                            except Exception:
                                pass

                            # aggregate
                            try:
                                _dbg.setdefault('fix2d2l_yearlike_block_fallback_trace_v1', {})
                                _agg = _dbg.get('fix2d2l_yearlike_block_fallback_trace_v1')
                                if not isinstance(_agg, dict):
                                    _agg = {}
                                _agg['fallback_attempts'] = int(_agg.get('fallback_attempts', 0)) + 1
                                if _committed:
                                    _agg['fallback_commits'] = int(_agg.get('fallback_commits', 0)) + 1
                                _agg.setdefault('samples', [])
                                if isinstance(_agg.get('samples'), list) and len(_agg['samples']) < 10:
                                    _agg['samples'].append({
                                        'canonical_key': _ckey,
                                        'blocked_value_norm': _blocked_vn,
                                        'committed': bool(_committed),
                                        'selected_value_norm': (_sel.get('value_norm') if isinstance(_sel, dict) else None),
                                        'selected_source_url': (_sel.get('source_url') if isinstance(_sel, dict) else None),
                                        'reason': _reason,
                                    })
                                _dbg['fix2d2l_yearlike_block_fallback_trace_v1'] = _agg
                                output['debug'] = _dbg
                            except Exception:
                                pass
                        except Exception:
                            pass

            # attach trace
            try:
                _dbg = output.get("debug") if isinstance(output.get("debug"), dict) else {}
                if not isinstance(_dbg, dict):
                    _dbg = {}
                _dbg.setdefault("fix2d24_yearlike_current_trace_v1", {})
                _dbg["fix2d24_yearlike_current_trace_v1"] = {
                    "filtered_count": int(_fix2d24_filtered),
                    "samples": _fix2d24_samples,
                    "note": "Blocks unitless yearlike current values at metric_changes hydration",
                }
                output["debug"] = _dbg
            except Exception:
                pass

            _sample = []
            if isinstance(rows, list):
                for _r in rows[:25]:
                    if not isinstance(_r, dict):
                        continue
                    _diag = _r.get("diag") if isinstance(_r.get("diag"), dict) else {}
                    _sample.append({
                        "canonical_key": _r.get("canonical_key"),
                        "name": _r.get("metric") or _r.get("name"),
                        "current_value": _r.get("current_value"),
                        "current_value_norm": (_r.get("current_value_norm") if _r.get("current_value_norm") is not None else _r.get("cur_value_norm")),
                        "cur_unit_cmp": (_r.get("cur_unit_cmp") if _r.get("cur_unit_cmp") is not None else _r.get("current_unit")),
                        "anchor_used": _r.get("anchor_used"),
                        "unit_mismatch": _r.get("unit_mismatch"),
                        "diag_keys": (list(_diag.keys()) if isinstance(_diag, dict) else []),
                    })
            _dbg["dashboard_current_source_v25"] = {
                "dashboard_reads": "results.metric_changes[].current_value",
                "rows_sample_n": len(_sample),
                "rows_sample": _sample,
            }
            # REFACTOR12: canonical_for_render debug block must be present (avoid stale "missing" diagnostics)
            try:
                _cfr = _dbg.get("canonical_for_render_v1")
                if (not isinstance(_cfr, dict)) or (not _cfr):
                    _dbg["canonical_for_render_v1"] = {
                        "present": True,
                        "code_version": _yureeka_get_code_version(),
            "authority_manifest_v1": _yureeka_authority_manifest_v1(),
                        "note": "REFACTOR12 standardized debug block (was intermittently missing)",
                    }
            except Exception:
                pass
            _dbg["canonical_for_render_present_v25"] = bool(_dbg.get("canonical_for_render_v1"))

            # Why:
            # - We sometimes observe canonical_for_render_present_v25 == False and/or
            #   missing bound canonical entities in the dashboard, even when injection
            #   extracted numbers exist.
            #
            # What:
            # - Emit a single compact debug object that explains, deterministically,
            #   why canonical_for_render did not run / did not apply / did not produce
            #   candidates / did not change any rows.
            # - Purely additive; does not change selection, hashing, or rendering.
            try:
                _cfr_dbg = _dbg.get("canonical_for_render_v1") if isinstance(_dbg.get("canonical_for_render_v1"), dict) else None

                # Row-level diag aggregation (if present)
                _rows = None
                try:
                    _rows = output.get("metric_changes") if isinstance(output, dict) else None
                except Exception:
                    pass
                    _rows = None

                _row_diag_present = 0
                _row_diag_applied_true = 0
                _row_diag_applied_false = 0
                _row_diag_reasons = {}
                _row_diag_reason_samples = []

                if isinstance(_rows, list):
                    for _r in _rows[:250]:
                        if not isinstance(_r, dict):
                            continue
                        _d = _r.get("diag") if isinstance(_r.get("diag"), dict) else {}
                        _c = _d.get("canonical_for_render_v1") if isinstance(_d.get("canonical_for_render_v1"), dict) else None
                        if not isinstance(_c, dict):
                            continue
                        _row_diag_present += 1
                        _ap = _c.get("applied")
                        if _ap is True:
                            _row_diag_applied_true += 1
                        elif _ap is False:
                            _row_diag_applied_false += 1
                        _rsn = str(_c.get("reason") or "")
                        if _rsn:
                            _row_diag_reasons[_rsn] = int(_row_diag_reasons.get(_rsn, 0)) + 1
                            if len(_row_diag_reason_samples) < 12:
                                _row_diag_reason_samples.append({
                                    "canonical_key": _r.get("canonical_key"),
                                    "reason": _rsn,
                                    "fn": str(_c.get("fn") or ""),
                                })

                # Determine the most precise reason we can provide
                _reason = ""
                _reason_detail = {}

                if not isinstance(_cfr_dbg, dict):
                    _reason = "missing_output_debug.canonical_for_render_v1"
                    _reason_detail = {
                        "has_output_debug": bool(isinstance(_dbg, dict)),
                        "row_diag_present": int(_row_diag_present),
                        "row_diag_applied_true": int(_row_diag_applied_true),
                        "row_diag_applied_false": int(_row_diag_applied_false),
                    }
                else:
                    _applied = bool(_cfr_dbg.get("applied"))
                    _rb = int(_cfr_dbg.get("rebuilt_count") or 0)
                    _rsn = str(_cfr_dbg.get("reason") or "")
                    if not _applied:
                        _reason = "canonical_for_render_not_applied"
                        _reason_detail = {
                            "canonical_for_render_reason": _rsn,
                            "rebuilt_count": _rb,
                        }
                    elif _applied and _rb <= 0:
                        _reason = "canonical_for_render_applied_but_empty"
                        _reason_detail = {
                            "canonical_for_render_reason": _rsn,
                            "rebuilt_count": _rb,
                        }
                    elif _applied and _rb > 0 and _row_diag_applied_true == 0:
                        _reason = "canonical_for_render_rebuilt_but_no_rows_hydrated"
                        _reason_detail = {
                            "canonical_for_render_reason": _rsn,
                            "rebuilt_count": _rb,
                        }
                    else:
                        _reason = "canonical_for_render_present"
                        _reason_detail = {
                            "canonical_for_render_reason": _rsn,
                            "rebuilt_count": _rb,
                        }

                _dbg["canonical_for_render_diagnosis_fix2ac_v1"] = {
                    "reason": _reason,
                    "reason_detail": _reason_detail,
                    "canonical_for_render_present_v25": bool(_dbg.get("canonical_for_render_v25") or _dbg.get("canonical_for_render_present_v25")),
                    "row_diag_present": int(_row_diag_present),
                    "row_diag_applied_true": int(_row_diag_applied_true),
                    "row_diag_applied_false": int(_row_diag_applied_false),
                    "row_diag_reason_counts": _row_diag_reasons,
                    "row_diag_reason_samples": _row_diag_reason_samples,
                    "has_canonical_for_render_v1": bool(isinstance(_cfr_dbg, dict)),
                    "canonical_for_render_v1_summary": (
                        {
                            "applied": bool(_cfr_dbg.get("applied")),
                            "reason": str(_cfr_dbg.get("reason") or ""),
                            "fn": str(_cfr_dbg.get("fn") or ""),
                            "rebuilt_count": int(_cfr_dbg.get("rebuilt_count") or 0),
                            "keys_sample": list(_cfr_dbg.get("keys_sample") or [])[:10],
                        }
                        if isinstance(_cfr_dbg, dict) else {}
                    ),
                }
            except Exception:
                pass

            # Surface which response-shape path diff used to hydrate "current".
            try:
                _paths = None
                try:
                    _paths = cur_response.get("_diff_panel_canonical_paths_v33") if isinstance(cur_response, dict) else None
                except Exception:
                    pass
                    _paths = None
                if isinstance(_paths, dict) and _paths:
                    _dbg["diff_panel_canonical_paths_v33"] = _paths
            except Exception:
                pass

            output["debug"] = _dbg
    except Exception:
        pass

    # If any later code overwrote current_* fields, restore the locked canonical-for-render
    # fields right before returning the evolution output.
    try:
        _lock_dbg = {"rows_total": 0, "rows_locked": 0, "rows_restored": 0, "rows_missing_lock": 0, "restored_keys_sample": []}
        for _r in (output.get("metric_changes") or []):
            if not isinstance(_r, dict):
                continue
            _lock_dbg["rows_total"] += 1
            if not _r.get("_lock_current_v26"):
                continue
            _lock_dbg["rows_locked"] += 1
            _d = _r.get("diag") if isinstance(_r.get("diag"), dict) else {}
            _lc = None
            try:
                if isinstance(_d, dict):
                    _cfr = _d.get("canonical_for_render_v1") if isinstance(_d.get("canonical_for_render_v1"), dict) else {}
                    _lc = _cfr.get("locked_current_v26") if isinstance(_cfr, dict) else None
            except Exception:
                pass
                _lc = None
            if not isinstance(_lc, dict):
                _lock_dbg["rows_missing_lock"] += 1
                continue
            # If current fields differ from locked, restore
            _changed = False
            for _k in ("current_value", "current_value_norm", "cur_value_norm", "cur_unit_cmp", "current_unit",
                       "current_value_range", "current_value_range_display"):
                if _k in _lc:
                    if _r.get(_k) != _lc.get(_k):
                        _r[_k] = _lc.get(_k)
                        _changed = True
            if _changed:
                _lock_dbg["rows_restored"] += 1
                ck = _r.get("canonical_key") or _r.get("canonical") or ""
                if ck and len(_lock_dbg["restored_keys_sample"]) < 20:
                    _lock_dbg["restored_keys_sample"].append(str(ck))
        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            output["debug"]["lock_current_v26"] = _lock_dbg
    except Exception:
        pass

    # REFACTOR56: enforce removal of legacy metric_changes output (safety rail)
    try:
        output.pop("metric_changes_legacy", None)
    except Exception:
        pass

    # REFACTOR66: De-duplicate nested output['results'] mirror (footprint control)
    #
    # Problem:
    # - Some legacy flows attach a nested 'results' dict that re-includes heavy fields
    #   such as baseline_sources_cache / baseline_sources_cache_current / primary_response.
    # - This duplicates large payload segments in Evolution JSON and can aggravate
    #   Sheets cell limits / local snapshot store bloat.
    #
    # Behavior:
    # - Preserve backward compatibility by keeping output['results'] as a lightweight
    #   stub containing only small, commonly expected fields.
    # - Never remove top-level authoritative fields.
    #
    # Safety:
    # - Purely a payload-shape cleanup; does not change diffing, schema, or key grammar.
    try:
        _nested = output.get("results")
        if isinstance(_nested, dict) and _nested:
            # Promote a few lightweight fields if they exist only under nested results
            for _k in ("run_delta_seconds", "run_delta_human", "code_version"):
                try:
                    if _k in _nested and _k not in output:
                        output[_k] = _nested.get(_k)
                except Exception:
                    pass

            # Build a lightweight compatibility mirror (avoid duplicating heavy caches)
            _light = {}
            try:
                _light["code_version"] = str(output.get("code_version") or globals().get("_YUREEKA_CODE_VERSION_LOCK") or "")
            except Exception:
                pass
            for _k in ("run_delta_seconds", "run_delta_human"):
                try:
                    if _k in output:
                        _light[_k] = output.get(_k)
                except Exception:
                    pass

            # Optionally keep a small primary_metrics_canonical mirror (usually tiny: 4 keys)
            try:
                _pmc = output.get("primary_metrics_canonical")
                if isinstance(_pmc, dict) and _pmc:
                    _light["primary_metrics_canonical"] = _pmc
            except Exception:
                pass

            # Keep a minimal debug marker for older consumers
            try:
                _d = output.get("debug") if isinstance(output.get("debug"), dict) else {}
                if isinstance(_d, dict) and _d:
                    _light["debug"] = {"__exec_code_version": _d.get("__exec_code_version")}
            except Exception:
                pass

            output["results"] = _light
    except Exception:
        pass
    # END REFACTOR66

    # REFACTOR70 (trimmed REFACTOR164): Metric-changes + stability output bridge (safety rail)
    #
    # Why:
    # - During controlled downsizing, minor nesting changes can cause the UI/export
    #   path to miss the authoritative metric_changes list.
    #
    # What (REFACTOR164):
    # - Enforce a single authoritative metric_changes list at output["metric_changes"].
    # - Do NOT export metric_changes_v2 (it was redundant and doubled payload size).
    # - Mirror stability_score to top-level when present.
    # - Never emit metric_changes_legacy.
    try:
        if isinstance(output, dict):
            _mc = None
            if isinstance(output.get("metric_changes"), list):
                _mc = output.get("metric_changes")
            else:
                _r = output.get("results")
                if isinstance(_r, dict) and isinstance(_r.get("metric_changes"), list):
                    _mc = _r.get("metric_changes")
            if _mc is None:
                _mc = []
            if not isinstance(_mc, list):
                try:
                    _mc = list(_mc)
                except Exception:
                    _mc = []
            output["metric_changes"] = _mc

            # Remove redundant/legacy feeds if present
            output.pop("metric_changes_v2", None)
            output.pop("metric_changes_legacy", None)
            try:
                _r = output.get("results")
                if isinstance(_r, dict):
                    _r.pop("metric_changes_v2", None)
                    _r.pop("metric_changes_legacy", None)
            except Exception:
                pass

            # Stability score mirror (clamped to [0, 100])
            _ss = None
            if isinstance(output.get("stability_score"), (int, float)):
                _ss = float(output.get("stability_score"))
            else:
                _r = output.get("results")
                if isinstance(_r, dict) and isinstance(_r.get("stability_score"), (int, float)):
                    _ss = float(_r.get("stability_score"))
            if _ss is not None:
                if _ss < 0:
                    _ss = 0.0
                if _ss > 100:
                    _ss = 100.0
                output["stability_score"] = round(_ss, 1)
    except Exception:
        pass

    # Why:
    # - External source flakiness (e.g., failed:no_text) can silently reduce
    #   primary_metrics_canonical_count below the frozen schema size.
    # - This patch stamps a deterministic debug warning with missing keys
    #   and source failure summaries so the harness can't "silently degrade".
    try:
        _schema_frozen = None
        try:
            _schema_frozen = _first_present(previous_data or {}, [
                ["metric_schema_frozen"],
                ["results", "metric_schema_frozen"],
                ["primary_response", "metric_schema_frozen"],
                ["results", "primary_response", "metric_schema_frozen"],
            ], default=None)
        except Exception:
            _schema_frozen = None
        if _schema_frozen is None:
            _schema_frozen = output.get("metric_schema_frozen")

        _schema_keys = []
        if isinstance(_schema_frozen, dict):
            _schema_keys = list(_schema_frozen.keys())
        elif isinstance(_schema_frozen, list):
            _schema_keys = [str(x) for x in _schema_frozen if x is not None]

        # baseline (prev) canonical metrics keys
        _prev_pmc = _first_present(previous_data or {}, [
            ["primary_metrics_canonical"],
            ["results", "primary_metrics_canonical"],
            ["primary_response", "primary_metrics_canonical"],
            ["results", "primary_response", "primary_metrics_canonical"],
        ], default={}) or {}
        _prev_keys = set(_prev_pmc.keys()) if isinstance(_prev_pmc, dict) else set()

        # current (cur) canonical metrics keys
        _cur_pmc = output.get("primary_metrics_canonical") or _get_nested(output, ["results", "primary_metrics_canonical"], default={}) or {}
        _cur_keys = set(_cur_pmc.keys()) if isinstance(_cur_pmc, dict) else set()

        _missing_prev = []
        _missing_cur = []
        if _schema_keys:
            _missing_prev = [k for k in _schema_keys if k not in _prev_keys]
            _missing_cur = [k for k in _schema_keys if k not in _cur_keys]

        def _summ_failures(_lst):
            _out = []
            if not isinstance(_lst, list):
                return _out
            for _e in _lst:
                if not isinstance(_e, dict):
                    continue
                _url = _e.get("url") or _e.get("source_url") or _e.get("source") or ""
                _st = str(_e.get("status") or "")
                _sd = str(_e.get("status_detail") or "")
                # mark anything explicitly failed, or any status_detail that starts with "failed"
                _is_fail = False
                if _st and _st.lower() in ("failed", "error", "timeout", "blocked"):
                    _is_fail = True
                if _sd and _sd.lower().startswith("failed"):
                    _is_fail = True
                if _is_fail:
                    _out.append({"url": _url, "status": _st, "status_detail": _sd, "numbers_found": _e.get("numbers_found")})
            return _out

        def _summ_fallbacks(_lst):
            _out = []
            if not isinstance(_lst, list):
                return _out
            for _e in _lst:
                if not isinstance(_e, dict):
                    continue
                _url = _e.get("url") or _e.get("source_url") or _e.get("source") or ""
                _st = str(_e.get("status") or "")
                _sd = str(_e.get("status_detail") or "")
                _fb = bool(_e.get("fallback_used") or _e.get("reused_snapshot"))
                if (_sd and _sd.lower().startswith("fallback")) or _fb:
                    _out.append({"url": _url, "status": _st, "status_detail": _sd, "numbers_found": _e.get("numbers_found")})
            return _out

        _prev_cache = _first_present(previous_data or {}, [
            ["baseline_sources_cache"],
            ["results", "baseline_sources_cache"],
            ["primary_response", "baseline_sources_cache"],
            ["results", "primary_response", "baseline_sources_cache"],
        ], default=None)
        _cur_sources = output.get("source_results") or output.get("baseline_sources_cache_current") or []

        _prev_failures = _summ_failures(_prev_cache)
        _cur_failures = _summ_failures(_cur_sources)
        _prev_fallbacks = _summ_fallbacks(_prev_cache)
        _cur_fallbacks = _summ_fallbacks(_cur_sources)

        _inv = {
            "schema_frozen_key_count": len(_schema_keys),
            "baseline_key_count": len(_prev_keys),
            "current_key_count": len(_cur_keys),
            "missing_baseline_keys": _missing_prev,
            "missing_current_keys": _missing_cur,
            "baseline_source_failures": _prev_failures,
            "current_source_failures": _cur_failures,
            "baseline_source_fallbacks": _prev_fallbacks,
            "current_source_fallbacks": _cur_fallbacks,
        }
        # REFACTOR74: Diff row count vs schema size (completeness-first invariant)
        _rows_for_cnt = output.get("metric_changes")
        if not isinstance(_rows_for_cnt, list):
            _rows_for_cnt = []
        _row_count = int(len(_rows_for_cnt)) if isinstance(_rows_for_cnt, list) else 0
        _expected_rows = int(len(_schema_keys)) if _schema_keys else 0
        try:
            _inv["metric_changes_row_count"] = _row_count
            _inv["metric_changes_expected_row_count"] = _expected_rows
            _inv["metric_changes_row_count_matches_schema"] = (bool(_expected_rows and (_row_count == _expected_rows)) if _expected_rows else None)
        except Exception:
            pass

        # REFACTOR76: Schema-key coverage + change_type integrity (warning-only invariant)
        try:
            _row_keys = []
            if isinstance(_rows_for_cnt, list):
                for _r in _rows_for_cnt:
                    if not isinstance(_r, dict):
                        continue
                    _ck = _r.get("canonical_key") or _r.get("canonical_metric_key") or _r.get("key")
                    if isinstance(_ck, str) and _ck:
                        _row_keys.append(_ck)
            _row_key_set = set(_row_keys)
            _dups = []
            try:
                _c = Counter(_row_keys)
                _dups = [k for k, v in _c.items() if int(v) > 1]
            except Exception:
                _dups = []
            _schema_key_set = set(_schema_keys or [])
            _missing_rows = [k for k in (_schema_keys or []) if k not in _row_key_set] if _schema_keys else []
            _extra_rows = [k for k in _row_key_set if (_schema_keys and (k not in _schema_key_set))] if _schema_keys else []
            _inv["metric_changes_schema_missing_keys"] = _missing_rows
            _inv["metric_changes_schema_extra_keys"] = sorted(list(_extra_rows))[:100] if isinstance(_extra_rows, list) else []
            _inv["metric_changes_schema_duplicate_keys"] = _dups
            _inv["metric_changes_schema_coverage_ok"] = (bool(_schema_keys) and (not _missing_rows) and (not _extra_rows) and (not _dups)) if _schema_keys else None

            _row_by_key = {}
            if isinstance(_rows_for_cnt, list):
                for _r in _rows_for_cnt:
                    if not isinstance(_r, dict):
                        continue
                    _ck = _r.get("canonical_key") or _r.get("canonical_metric_key") or _r.get("key")
                    if isinstance(_ck, str) and _ck and (_ck not in _row_by_key):
                        _row_by_key[_ck] = _r

            _ct_mismatches = []
            if _schema_keys:
                for _k in (_missing_prev or []):
                    if _k in _cur_keys:
                        _row = _row_by_key.get(_k) or {}
                        _ct = str(_row.get("change_type") or "")
                        if _ct not in ("missing_baseline", "new_metric"):
                            _ct_mismatches.append({"key": _k, "expected": "missing_baseline", "got": _ct})
                for _k in (_missing_cur or []):
                    if _k in _prev_keys:
                        _row = _row_by_key.get(_k) or {}
                        _ct = str(_row.get("change_type") or "")
                        if _ct not in ("missing_current",):
                            _ct_mismatches.append({"key": _k, "expected": "missing_current", "got": _ct})

            _inv["metric_changes_change_type_mismatches"] = _ct_mismatches[:50]
            _inv["metric_changes_change_type_ok"] = (len(_ct_mismatches) == 0) if _schema_keys else None
        except Exception:
            pass

        # REFACTOR78: Version-stamp self-check vs patch tracker (warning-only)
        try:
            _cv = str(output.get("code_version") or _yureeka_get_code_version() or "")
            _latest = None
            _maxn = None
            _pt = globals().get("PATCH_TRACKER_V1")

            _head = None
            try:
                if isinstance(_pt, list) and _pt and isinstance(_pt[0], dict):
                    _head = str(_pt[0].get("patch_id") or "")
            except Exception:
                _head = None
            if isinstance(_pt, list):
                for _e in _pt:
                    if not isinstance(_e, dict):
                        continue
                    _pid = str(_e.get("patch_id") or "")
                    _m = re.match(r"REFACTOR(\d+)$", _pid)
                    if _m:
                        try:
                            _n = int(_m.group(1))
                        except Exception:
                            continue
                        if (_maxn is None) or (_n > _maxn):
                            _maxn = _n
                            _latest = _pid
            _inv["code_version"] = _cv
            _inv["patch_tracker_latest_refactor"] = _latest
            _inv["patch_tracker_head_patch_id"] = _head
            _inv["code_version_matches_patch_tracker_head"] = ((_cv == _head) if (_cv and _head) else None)
            _inv["code_version_matches_patch_tracker_latest_refactor"] = ((_cv == _latest) if (_cv and _latest) else None)
        except Exception:
            pass

        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            output["debug"]["harness_invariants_v1"] = _inv

        # Add a short banner string for UI visibility (non-breaking additive field)
        _parts = []
        if (_missing_prev or _missing_cur or _prev_failures or _cur_failures or _prev_fallbacks or _cur_fallbacks) and _schema_keys:
            if _missing_prev:
                _parts.append(f"baseline_missing={len(_missing_prev)}/{len(_schema_keys)}")
            if _missing_cur:
                _parts.append(f"current_missing={len(_missing_cur)}/{len(_schema_keys)}")
            if _prev_failures:
                _parts.append(f"baseline_failures={len(_prev_failures)}")
            if _cur_failures:
                _parts.append(f"current_failures={len(_cur_failures)}")
            if _prev_fallbacks:
                _parts.append(f"baseline_fallbacks={len(_prev_fallbacks)}")
            if _cur_fallbacks:
                _parts.append(f"current_fallbacks={len(_cur_fallbacks)}")

        # REFACTOR78: Surface version mismatch banner (if any)
        try:
            _cv = str(_inv.get("code_version") or "")
            _head = str(_inv.get("patch_tracker_head_patch_id") or "")
            if _cv and _head and (_cv != _head):
                _parts.append(f"version_mismatch={_cv}!={_head}")
        except Exception:
            pass

        # Always surface a diff-row count mismatch against frozen schema keys (if any).
        try:
            _rc = int(_inv.get("metric_changes_row_count") or 0) if isinstance(_inv, dict) else 0
            _er = int(_inv.get("metric_changes_expected_row_count") or 0) if isinstance(_inv, dict) else 0
            if _schema_keys and _er and (_rc != _er):
                _parts.append(f"row_count_mismatch={_rc}/{_er}")
        except Exception:
            pass

        if _parts:
            output["harness_warning_v1"] = " | ".join(_parts)
    except Exception:
        pass

    return output

def extract_context_keywords(metric_name: str) -> List[str]:
    """
    General-purpose keyword extraction for matching metric names to page contexts.

    Goals:
    - Work for ANY topic (not tourism-specific)
    - Keep deterministic behavior
    - Extract years/quarters, key financial/stat terms, and meaningful tokens
    """
    if not metric_name:
        return []

    name = str(metric_name)
    n = name.lower()

    keywords: List[str] = []

    # Years (e.g., 2019, 2024)
    years = re.findall(r"(19\d{2}|20\d{2})", name)
    keywords.extend(years)

    # Quarters / time buckets
    q = re.findall(r"\bq[1-4]\b", n)
    keywords.extend([x.upper() for x in q])

    # Common metric concepts (broad, cross-industry)
    concept_phrases = [
        "market size", "revenue", "sales", "turnover", "profit", "operating profit",
        "ebit", "ebitda", "net income", "gross margin", "margin",
        "growth", "yoy", "cagr", "share", "penetration",
        "forecast", "projected", "projection", "estimate", "expected",
        "actual", "baseline", "target",
        "volume", "units", "shipments", "users", "subscribers", "visitors",
        "price", "asp", "arpu", "aov",
        "inflation", "gdp", "unemployment", "interest rate"
    ]
    for p in concept_phrases:
        if p in n:
            keywords.append(p)

    # Units / scales that help matching
    unit_hints = ["trillion", "billion", "million", "thousand", "%", "percent"]
    for u in unit_hints:
        if u in n:
            keywords.append(u)

    # Tokenize remaining meaningful words
    tokens = re.findall(r"[a-z0-9]+", n)
    stop = {
        "the","and","or","of","in","to","for","by","from","with","on","at","as",
        "total","overall","average","avg","number","rate","value","amount",
        "annual","year","years","monthly","month","daily","day","quarter","quarters"
    }
    for t in tokens:
        if t in stop:
            continue
        if len(t) <= 2:
            continue
        keywords.append(t)

    # De-dup, keep stable ordering
    seen = set()
    out = []
    for k in keywords:
        if k and k not in seen:
            seen.add(k)
            out.append(k)

    return out[:30]

def extract_numbers_with_context(text, source_url: str = "", max_results: int = 350):
    """
    Extract numeric candidates with context windows (analysis-aligned, hardened).

    Fixes / tightening:
    - ALWAYS returns a list (never None)  ✅ critical for snapshots & evolution
    - Strips HTML tags/scripts/styles if HTML-like
    - Nav/chrome/junk rejection (analytics, cookie banners, menus, footers, etc.)
    - Suppress year-only candidates (e.g., "2024") unless clearly a metric
    - Suppress ID-like long integers, phone-like patterns, DOI/ISBN-like contexts
    - Captures currency + scale + percent + common magnitude suffixes
    - Adds anchor_hash for stable matching
    """
    if not text or not str(text).strip():
        return []

    raw = str(text)

    def _normalize_unit(u: str) -> str:
        u = (u or "").strip()
        if not u:
            return ""
        ul = u.lower().replace(" ", "")

        # Energy units (must come before magnitude)
        if "twh" in ul:
            return "TWh"
        if "gwh" in ul:
            return "GWh"
        if "mwh" in ul:
            return "MWh"
        if "kwh" in ul:
            return "kWh"
        if ul == "wh":
            return "Wh"

        # Magnitudes (case-insensitive; fix: accept single-letter suffixes)
        if ul in ("bn", "billion", "b"):
            return "B"
        if ul in ("mn", "mio", "million", "m"):
            return "M"
        if ul in ("k", "thousand", "000"):
            return "K"
        if ul in ("trillion", "tn", "t"):
            return "T"

        if ul in ("pct", "percent", "%"):
            return "%"

        return u

    def _looks_html(s: str) -> bool:
        sl = s.lower()
        return ("<html" in sl) or ("<div" in sl) or ("<p" in sl) or ("<script" in sl) or ("</" in sl)

    def _html_to_text(s: str) -> str:
        # Prefer BeautifulSoup if available
        try:
            soup = BeautifulSoup(s, "html.parser")
            for tag in soup(["script", "style", "noscript", "svg", "canvas", "iframe", "header", "footer", "nav", "form"]):
                try:
                    tag.decompose()
                except Exception:
                    pass
            txt = soup.get_text(separator=" ", strip=True)
            txt = re.sub(r"\s+", " ", txt).strip()
            return txt
        except Exception:
            pass
            # fallback: cheap strip
            s2 = re.sub(r"(?is)<(script|style|noscript).*?>.*?</\1>", " ", s)
            s2 = re.sub(r"(?is)<[^>]+>", " ", s2)
            s2 = re.sub(r"\s+", " ", s2).strip()
            return s2

    def _is_phone_like(ctx: str, rawnum: str) -> bool:
        # strict phone pattern or phone keywords nearby
        if re.search(r"\b\d{3}-\d{3}-\d{4}\b", rawnum):
            return True
        c = (ctx or "").lower()
        if any(k in c for k in ["call", "phone", "tel:", "telephone", "contact us", "whatsapp"]):
            if re.search(r"\b\d{7,}\b", rawnum):
                return True
        return False

    def _is_id_like(val_str: str, ctx: str) -> bool:
        # very long digit strings typically IDs, unless explicitly monetary with symbols
        digits = re.sub(r"\D", "", val_str or "")
        if len(digits) >= 13:
            c = (ctx or "").lower()
            if any(k in c for k in ["isbn", "doi", "issn", "arxiv", "repec", "id:", "order", "invoice", "reference"]):
                return True
            # generic ID-like (too many digits)
            return True
        return False

    def _chrome_junk(ctx: str) -> bool:
        c = (ctx or "").lower()
        # common site chrome / analytics / cookie / nav junk
        bad = [
            "googleanalyticsobject", "gtag(", "googletagmanager", "analytics", "doubleclick",
            "cookie", "consent", "privacy", "terms", "copyright", "all rights reserved",
            "subscribe", "newsletter", "sign in", "login", "menu", "search", "breadcrumb",
            "share this", "follow us", "social media", "footer", "header", "nav", "sitemap"
        ]
        if any(b in c for b in bad):
            return True
        # css/js-like
        if any(b in c for b in ["function(", "var ", "const ", "let ", "webpack", "sourcemappingurl", ".css", "{", "};"]):
            return True
        # low alpha ratio
        if len(c) > 80:
            letters = sum(ch.isalpha() for ch in c)
            if letters / max(1, len(c)) < 0.18:
                return True
        return False

    def _year_only_suppression(num: float, unit: str, rawnum: str, ctx: str) -> bool:
        # suppress standalone 4-digit years like 2024 with no unit/currency
        if unit:
            return False
        s = (rawnum or "").strip()
        if re.fullmatch(r"\d{4}", s):
            year = int(s)
            if 1900 <= year <= 2099:
                c = (ctx or "").lower()
                allow_kw = ["cagr", "growth", "inflation", "gdp", "revenue", "market", "sales", "shipments", "capacity"]
                if not any(k in c for k in allow_kw):
                    return True
        return False

    # Do this AFTER HTML->text and BEFORE regex extraction.

    if _looks_html(raw):
        raw = _html_to_text(raw)

    # cap huge pages
    raw = raw[:250_000]

    raw = re.sub(r"\b((?:19|20)\d)\s+(\d)\b", r"\1\2", raw)

    def _is_year_range_context(ctx: str) -> bool:
        return bool(re.search(r"\b(19|20)\d{2}\s*(?:-|–|—|to)\s*(19|20)\d{2}\b", ctx or "", flags=re.I))

    # - We DO NOT filter here; we tag and downstream excludes by default.
    def _junk_tag(value: float, unit: str, raw_disp: str, ctx: str):
        """
        Non-destructive junk classifier.
        Returns (is_junk: bool, reason: str).
        """
        c = (ctx or "").lower()
        u = (unit or "").strip()

        try:
            iv = int(float(value))
            if u == "" and 1900 <= iv <= 2099 and _is_year_range_context(ctx):
                return True, "year_range"
        except Exception:
            pass

        nav_hits = [
            "skip to content", "menu", "search", "login", "sign in", "sign up",
            "subscribe", "newsletter", "cookie", "privacy", "terms", "copyright",
            "all rights reserved", "back to top", "next", "previous", "page ",
            "home", "about", "contact", "sitemap", "breadcrumb"
        ]
        if any(h in c for h in nav_hits):
            try:
                if u == "" and abs(float(value)) <= 20:
                    return True, "nav_small_int"
            except Exception:
                pass

        if u == "":
            try:
                if abs(float(value)) <= 12:
                    if any(h in c for h in ["•", "–", "step", "chapter", "section", "item", "no."]):
                        return True, "enumeration_small_int"
            except Exception:
                pass

        if u == "":
            try:
                iv = int(abs(float(value)))
                if 190 <= iv <= 209:
                    if any(x in (raw_disp or "") for x in ["202", "203", "204", "205", "206", "207", "208", "209"]):
                        return True, "year_fragment_3digit"
            except Exception:
                return False, ""

    # NOTE: moved OUTSIDE the loop for determinism + speed (no behavioral change).
    # Also emits a "measure_assoc" label that downstream can display easily.
    def _classify_measure(unit_tag: str, ctx: str):
        """
        Returns (measure_kind, measure_assoc):
          - measure_kind: stable internal tag (share_pct / growth_pct / count_units / money / etc.)
          - measure_assoc: human-meaning label ("share", "growth", "units", "money", "energy", etc.)
        """
        c = (ctx or "").lower()
        ut = (unit_tag or "").strip()

        if ut == "%":
            if any(k in c for k in ["market share", "share of", "share", "penetration", "portion", "contribution"]):
                return "share_pct", "share"
            if any(k in c for k in ["growth", "cagr", "increase", "decrease", "yoy", "mom", "qoq", "rate"]):
                return "growth_pct", "growth"
            return "percent_other", "percent"

        if ut in ("K", "M", "B", "T", ""):
            if any(k in c for k in ["units", "unit", "vehicles", "cars", "sold", "sales volume", "shipments", "deliveries", "registrations"]):
                return "count_units", "units"
            if any(k in c for k in ["revenue", "sales ($", "usd", "$", "market size", "valuation", "turnover"]):
                return "money", "money"
            return "magnitude_other", "magnitude"

        if ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
            return "energy", "energy"

        return "other", "other"

    # - Fix US$ being parsed as S$ by matching US\$ first.
    # - Also accept "US$" as a single token (case-insensitive).

    pat_dec = re.compile(
        r"(US\$|US\$(?!\w)|S\$|\$|USD|SGD|EUR|€|GBP|£)?\s*"
        # REFACTOR129: decimals first
        r"(-?\d{1,3}(?:[,\s]\d{3})*\.\d+|-?\d+\.\d+)(?!\d)\s*"
        r"(TWh|GWh|MWh|kWh|Wh|tn|(?:T|B|M|K)(?![A-Za-z])|trillion|billion|million|bn|mn|%|percent)?",
        flags=re.I
    )

    pat_int = re.compile(
        r"(US\$|US\$(?!\w)|S\$|\$|USD|SGD|EUR|€|GBP|£)?\s*"
        # REFACTOR129: integers (can overlap decimals, will be suppressed)
        r"(-?\d{1,3}(?:[,\s]\d{3})*|-?\d+)(?!\d)\s*"
        r"(TWh|GWh|MWh|kWh|Wh|tn|(?:T|B|M|K)(?![A-Za-z])|trillion|billion|million|bn|mn|%|percent)?",
        flags=re.I
    )

    _rf129_dec = list(pat_dec.finditer(raw))
    _rf129_int = list(pat_int.finditer(raw))

    _rf129_dec_spans = []
    try:
        _rf129_dec_spans = [(int(d.start()), int(d.end())) for d in _rf129_dec]
    except Exception:
        _rf129_dec_spans = []

    _rf129_drop = []
    _rf129_keep_int = []
    for im in _rf129_int:
        _over = False
        try:
            is0, ie0 = int(im.start()), int(im.end())
            for ds, de in _rf129_dec_spans:
                if is0 < de and ie0 > ds:
                    _over = True
                    break
        except Exception:
            _over = False
        if _over:
            _rf129_drop.append(im)
        else:
            _rf129_keep_int.append(im)

    # REFACTOR129 beacon: decimal overlap suppression (aggregate, compact)
    try:
        _b = globals().get("_REFACTOR129_DECIMAL_OVERLAP_SUPPRESSION_V1")
        if not isinstance(_b, dict):
            _b = {"total_numbers": 0, "overlaps_detected": 0, "overlaps_dropped": 0, "sample_drops": []}
            globals()["_REFACTOR129_DECIMAL_OVERLAP_SUPPRESSION_V1"] = _b
        _b["total_numbers"] = int(_b.get("total_numbers", 0) or 0) + int(len(_rf129_dec) + len(_rf129_int))
        _b["overlaps_detected"] = int(_b.get("overlaps_detected", 0) or 0) + int(len(_rf129_drop))
        _b["overlaps_dropped"] = int(_b.get("overlaps_dropped", 0) or 0) + int(len(_rf129_drop))
        _sd = _b.get("sample_drops")
        if not isinstance(_sd, list):
            _sd = []
        for _m in list(_rf129_drop)[:8]:
            if len(_sd) >= 6:
                break
            try:
                _txt = raw[_m.start():_m.end()]
                _txt = str(_txt or "").strip()
                if not _txt:
                    continue
                _sd.append({"source_url": str(source_url or ""), "dropped_text": _txt})
            except Exception:
                pass
        _b["sample_drops"] = _sd[:6]
    except Exception:
        pass

    _rf129_matches = list(_rf129_dec) + list(_rf129_keep_int)
    try:
        _rf129_matches.sort(key=lambda mm: (int(mm.start()), int(mm.end())))
    except Exception:
        pass

    out = []
    for m in _rf129_matches:
        cur = (m.group(1) or "").strip()
        num_s = (m.group(2) or "").strip()
        unit_s = (m.group(3) or "").strip()

        if not num_s:
            continue

        start = max(0, m.start() - 160)
        end = min(len(raw), m.end() + 160)
        ctx = raw[start:end].replace("\n", " ")
        ctx = re.sub(r"\s+", " ", ctx).strip()
        ctx_store = ctx[:240]

        # numeric parse
        try:
            val = float(num_s.replace(",", "").replace(" ", ""))
        except Exception:
            pass
            continue

        # normalize unit
        unit = _normalize_unit(unit_s)

        raw_disp = f"{cur} {num_s}{unit_s}".strip()
        raw_num_only = (cur + num_s).strip()

        if _chrome_junk(ctx_store):
            continue
        if _is_phone_like(ctx_store, raw_disp):
            continue
        if _is_id_like(raw_disp, ctx_store):
            continue
        if _year_only_suppression(val, unit, num_s, ctx_store):
            continue

        # Example: "CAGR 2025-2030" producing "-2030"
        # - Do NOT drop here (keep non-destructive policy); just tag.
        neg_from_hyphen_range = False
        neg_year_from_range = False
        try:
            if num_s.startswith("-") and m.start() > 0 and raw[m.start() - 1].isdigit():
                neg_from_hyphen_range = True
                iv = int(abs(float(val)))
                if 1900 <= iv <= 2099:
                    neg_year_from_range = True
        except Exception:
            pass
            neg_from_hyphen_range = False
            neg_year_from_range = False

        anchor_hash = _yureeka_sha1_v1(f"{source_url}|{raw_disp}|{ctx_store}")
        # FIX2D69B: defensive tuple normalization (prevent unpack None)
        try:
            _jt = _junk_tag(val, unit, raw_disp, ctx_store)
            if isinstance(_jt, tuple) and len(_jt) == 2:
                is_junk, junk_reason = _jt
            else:
                is_junk, junk_reason = (False, "")
        except Exception:
            is_junk, junk_reason = (False, "")

        if neg_year_from_range:
            is_junk = True
            junk_reason = "year_range_negative_endpoint"
        elif neg_from_hyphen_range:
            is_junk = True
            junk_reason = "hyphen_range_negative_endpoint"

            # Why:
            # - Years (e.g., 2025) frequently appear in headings/ranges and should not
            #   compete with real metric values (currency, %, volumes) in evolution.
            # - We keep years only if there is strong metric context nearby.
            # Rules:
            # - If value is an integer-like 4-digit year in [1900..2100],
            #   unit is empty, and context lacks currency/%/magnitude cues => mark junk.
            try:
                if (not is_junk) and (not str(unit or "").strip()):
                    _v_int = None
                    try:
                        _v_int = int(float(val)) if val is not None else None
                    except Exception:
                        pass
                        _v_int = None

                    if _v_int is not None and 1900 <= _v_int <= 2100:
                        # Treat 4-digit years as non-metric tokens; keep but mark as junk.
                        is_junk = True
                        if not junk_reason:
                            junk_reason = "year_token"
            except Exception:
                pass
# semantic association tags
        # FIX2D69B: defensive tuple normalization (prevent unpack None)
        try:
            _cm = _classify_measure(unit, ctx_store)
            if isinstance(_cm, tuple) and len(_cm) == 2:
                measure_kind, measure_assoc = _cm
            else:
                measure_kind, measure_assoc = ("other", "other")
        except Exception:
            measure_kind, measure_assoc = ("other", "other")

        out.append({
            "value": val,
            "unit": unit,
            "raw": raw_disp,
            "source_url": source_url,
            "context": ctx_store,
            "context_snippet": ctx_store,
            "anchor_hash": anchor_hash,

            "is_junk": bool(is_junk),
            "junk_reason": junk_reason,
            "start_idx": int(m.start()),
            "end_idx": int(m.end()),

            "measure_kind": measure_kind,
            "measure_assoc": measure_assoc,
        })

        if len(out) >= int(max_results or 350):
            break

    # - Ensures out entries include unit_tag, unit_family, and corrected currency measure_kind/assoc
    # - Adds compact unit_measure_classifier_trace_v1 for audit
    try:
        fn_can = globals().get("canonicalize_numeric_candidate")
        if not callable(fn_can):
            fn_can = canonicalize_numeric_candidate
        out2 = []
        for _c in out:
            if isinstance(_c, dict):
                try:
                    _c2 = fn_can(dict(_c)) or dict(_c)
                except Exception:
                    pass
                    _c2 = dict(_c)
                out2.append(_c2)
            else:
                out2.append(_c)
        out = out2
    except Exception:
        pass

    return out

def calculate_context_match(keywords: List[str], context: str) -> float:
    """Calculate how well keywords match the context (deterministic)."""
    if not context:
        return 0.0

    context_lower = context.lower()

    # If no keywords, give a small baseline (we'll rely more on value_score)
    if not keywords:
        return 0.25

    # Year keywords MUST match if present
    year_keywords = [kw for kw in keywords if re.fullmatch(r"20\d{2}", kw)]
    if year_keywords:
        if not any(y in context_lower for y in year_keywords):
            return 0.0

    matches = sum(1 for kw in keywords if kw.lower() in context_lower)

    # Instead of hard "matches < 2 = reject", scale smoothly:
    match_ratio = matches / max(len(keywords), 1)

    # If nothing matches, reject
    if matches == 0:
        return 0.0

    # Score between 0.35 and 1.0 depending on ratio
    return 0.35 + (match_ratio * 0.65)

def render_source_anchored_results(results, query: str):
    """Render source-anchored evolution results (guarded + backward compatible + tuned debug UI)."""
    import math
    st.header("📈 Source-Anchored Evolution Analysis")
    st.markdown(f"**Query:** {query}")

    if not isinstance(results, dict):
        st.error("❌ Evolution returned an invalid result payload (not a dict).")
        st.write(results)
        return

    status = (results.get("status") or "").strip().lower()
    message = results.get("message") or ""

    def _safe_int(x, default=0) -> int:
        try:
            if x is None:
                return default
            return int(x)
        except Exception:
            return default

    def _safe_float(x, default=0.0) -> float:
        try:
            if x is None:
                return default
            return float(x)
        except Exception:
            return default

    def _fmt_pct(x, default="—") -> str:
        try:
            if x is None:
                return default
            v = float(x)
            if math.isnan(v):
                return default
            return f"{v:.0f}%"
        except Exception:
            return default

    def _fmt_change_pct(x) -> str:
        try:
            if x is None:
                return "-"
            v = float(x)
            if math.isnan(v):
                return "-"
            return f"{v:+.1f}%"
        except Exception:
            return "-"

    def _short(u: str, n: int = 95) -> str:
        if not u:
            return ""
        return (u[:n] + "…") if len(u) > n else u

    if status != "success":
        st.error(f"❌ {message or 'Evolution failed'}")
        sr = results.get("source_results") or []
        if isinstance(sr, list) and sr:
            st.subheader("🔗 Source Verification")
            for src in sr:
                if not isinstance(src, dict):
                    continue
                u = _short((src.get("url") or ""), 90)
                st.error(f"❌ {u} - {src.get('status_detail', 'Unknown error')}")
        return

    sources_checked = _safe_int(results.get("sources_checked"), 0)
    sources_fetched = _safe_int(results.get("sources_fetched"), 0)
    # NLP05: Prefer injection-aware (production-only) stability/summary when running against injected URLs.
    dbg = results.get("debug") or {}
    inj_gate = dbg.get("row_delta_gating_v4") if isinstance(dbg, dict) else None
    is_injection_run = bool(isinstance(inj_gate, dict) and inj_gate.get("injection_run"))

    summary = results.get("summary") or {}
    if is_injection_run and isinstance(results.get("summary_effective"), dict):
        summary = results.get("summary_effective") or summary
    if not isinstance(summary, dict):
        summary = {}

    stability = None
    if is_injection_run:
        stability = _safe_float(results.get("stability_score_effective"), None)
    if stability is None:
        stability = _safe_float(results.get("stability_score"), 0.0)

    metrics_inc = _safe_int(summary.get("metrics_increased"), 0)
    metrics_dec = _safe_int(summary.get("metrics_decreased"), 0)
    metrics_unch = _safe_int(summary.get("metrics_unchanged"), 0)

    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Sources Checked", sources_checked)
    col2.metric("Sources Fetched", sources_fetched)
    col3.metric("Stability", _fmt_pct(stability))
    metrics_inj = _safe_int(summary.get("metrics_injected"), 0)
    if is_injection_run:
        col4.info(f"🧪 Injection Mode ({metrics_inj} injected)")
    elif metrics_inc > metrics_dec:
        col4.success("📈 Trending Up")
    elif metrics_dec > metrics_inc:
        col4.error("📉 Trending Down")
    else:
        col4.info("➡️ Stable")

    if message:
        st.caption(message)

    st.markdown("---")

    # Source status
    st.subheader("🔗 Source Verification")
    src_results = results.get("source_results") or []
    if not isinstance(src_results, list):
        src_results = []

    # If everything failed, show breakdown
    if sources_checked > 0 and sources_fetched == 0 and src_results:
        reasons = []
        for s in src_results:
            if isinstance(s, dict):
                reasons.append((s.get("status_detail") or "unknown").split(":")[0])
        top = Counter(reasons).most_common(6)
        if top:
            st.warning("No sources were fetched successfully. Top failure types:")
            st.write({k: v for k, v in top})

    for src in src_results:
        if not isinstance(src, dict):
            continue
        url = src.get("url") or ""
        sstatus = src.get("status") or ""
        detail = src.get("status_detail") or ""
        ctype = src.get("content_type") or ""
        nfound = _safe_int(src.get("numbers_found"), 0)

        short = _short(url, 95)

        # show extra debug flags if present
        flags = []
        if src.get("snapshot_origin"):
            flags.append(f"origin={src.get('snapshot_origin')}")
        if src.get("is_homepage"):
            flags.append("homepage")
        if src.get("skip_reason"):
            flags.append(f"skip={src.get('skip_reason')}")
        if src.get("quality_score") is not None:
            try:
                flags.append(f"q={float(src.get('quality_score')):.2f}")
            except Exception:
                pass
                flags.append(f"q={src.get('quality_score')}")

        flag_txt = f" • {' • '.join(flags)}" if flags else ""

        if str(sstatus).startswith("fetched"):
            extra = f" ({nfound} nums)"
            if ctype:
                extra += f" • {ctype}"
            st.success(f"✅ {short}{extra}{flag_txt}")
        else:
            extra = f" - {detail}" if detail else ""
            if ctype:
                extra += f" • {ctype}"
            st.error(f"❌ {short}{extra}{flag_txt}")

    st.markdown("---")

    # Metric changes table
    st.subheader("💰 Metric Changes")

    # Prefer the V2 schema if present; fall back to legacy key for older snapshots.
    rows = results.get("metric_changes") or results.get("metric_changes_v2") or []
    if not isinstance(rows, list) or not rows:
        st.info("No metric changes to display.")
        return

    # REFACTOR91: hide "missing_both" rows by default (coverage gaps), with an opt-in toggle.
    def _refactor91__row_change_type(_r: dict) -> str:
        try:
            return str(_r.get("change_type") or _r.get("status") or "").strip().lower()
        except Exception:
            return ""

    _refactor91_missing_both_count = 0
    try:
        _refactor91_missing_both_count = sum(
            1 for _r in rows
            if isinstance(_r, dict) and _refactor91__row_change_type(_r) == "missing_both"
        )
    except Exception:
        _refactor91_missing_both_count = 0

    if _refactor91_missing_both_count:
        st.caption(f"Coverage gaps (missing_both): {_refactor91_missing_both_count}")
        _refactor91_show_missing_both = st.checkbox(
            "Show missing-both rows (coverage gaps)",
            value=False,
            key="yureeka_show_missing_both_rows_v1",
        )
        if not _refactor91_show_missing_both:
            _refactor91_filtered = [
                _r for _r in rows
                if not (isinstance(_r, dict) and _refactor91__row_change_type(_r) == "missing_both")
            ]
            if _refactor91_filtered:
                rows = _refactor91_filtered
            else:
                st.info("All rows are coverage gaps (missing_both). Enable the toggle to view them.")
                return

    def _is_v2_row(_r: dict) -> bool:
        try:
            return isinstance(_r, dict) and (
                ("delta_pct" in _r) or ("prev_value_norm" in _r) or ("cur_value_norm" in _r)
            )
        except Exception:
            return False

    is_v2 = any(_is_v2_row(r) for r in rows)

    # Only show Δt column if at least one row has a non-empty value.
    show_delta = any(
        isinstance(r, dict) and str(r.get("analysis_evolution_delta_human") or "").strip()
        for r in rows
    )

    def _fmt_vu(v, u: str) -> str:
        """Format value + unit compactly for tables."""
        u = (u or "").strip()
        if v is None or v == "":
            return ""
        try:
            vv = float(v)
            # Use general format; keep it compact.
            s = f"{vv:g}"
        except Exception:
            s = str(v)
        return f"{s} {u}".strip()

    def _fmt_delta_pct_v2(v) -> str:
        if v is None or v == "":
            return ""
        try:
            return f"{_safe_float(v, 0.0):.2f}%"
        except Exception:
            return str(v)

    table_rows = []
    for r in rows:
        if not isinstance(r, dict):
            continue

        if is_v2:
            prev_u = r.get("previous_unit") or ""
            cur_u = r.get("current_unit") or prev_u or ""

            out_row = {
                "Metric": (r.get("name") or r.get("metric") or ""),
                "Canonical Key": (r.get("canonical_key") or ""),
                "Previous": _fmt_vu(r.get("previous_value"), prev_u),
                "Current": _fmt_vu(r.get("current_value"), cur_u),
                "Δ": ("" if r.get("delta_abs") is None else f"{_safe_float(r.get('delta_abs'), 0.0):g}"),
                "Δ%": _fmt_delta_pct_v2(r.get("delta_pct")),
                "Status": (r.get("change_type") or r.get("status") or ""),
                "Comparable": ("✅" if r.get("baseline_is_comparable") else "⚠"),
                "Method": (r.get("current_method") or ""),
            }
            if show_delta:
                out_row["Δt (A→E)"] = r.get("analysis_evolution_delta_human") or ""
            table_rows.append(out_row)
        else:
            metric_label = r.get("metric") or r.get("name") or ""
            status_label = r.get("status") or r.get("change_type") or ""

            out_row = {
                "Metric": metric_label,
                "Canonical Key": r.get("canonical_key", "") or "",
                "Match Stage": r.get("match_stage", "") or "",
                "Previous": r.get("previous_value", "") or "",
                "Current": r.get("current_value", "") or "",
                "Δ%": _fmt_change_pct(r.get("change_pct") if r.get("change_pct") is not None else r.get("delta_pct")),
                "Status": status_label,
                "Match": _fmt_pct(r.get("match_confidence")),
                "Score": ("" if r.get("match_score") is None else f"{_safe_float(r.get('match_score'), 0.0):.2f}"),
                "Anchor": "✅" if r.get("anchor_used") else "",
            }
            if show_delta:
                out_row["Δt (A→E)"] = r.get("analysis_evolution_delta_human") or ""
            table_rows.append(out_row)

    st.dataframe(table_rows, use_container_width=True)
    # REFACTOR137: Restored UI panels from FIX2D88 (render-only; best-effort for evolution payload).
    try:
        _wc = results.get("web_context") if isinstance(results.get("web_context"), dict) else {}
        _vs = results.get("veracity_scores") if isinstance(results.get("veracity_scores"), dict) else None
        _yureeka_render_sources_reliability_panel_v1(results, web_context=_wc, source_reliability=None)
        _yureeka_render_evidence_quality_scores_panel_v1(results, veracity_scores=_vs)
        st.markdown("---")
        _yureeka_render_data_visualization_panel_v1(results)
    except Exception:
        pass

    # REFACTOR138: Optional diagnostics beacons (collapsed by default)
    try:
        _yureeka_render_diagnostics_expander_v1(results, web_context=_wc, label='(Evolution)')
    except Exception:
        pass

    st.markdown("---")

# 9. DASHBOARD RENDERING

def detect_x_label_dynamic(labels: list) -> str:
    """Enhanced X-axis detection with better region matching"""
    if not labels:
        return "Category"

    # Convert to lowercase for comparison
    label_texts = [str(l).lower().strip() for l in labels]
    all_text = ' '.join(label_texts)

    # 1. GEOGRAPHIC REGIONS (PRIORITY 1)
    region_keywords = [
        'north america', 'asia pacific', 'asia-pacific', 'apac', 'europe', 'emea',
        'latin america', 'latam', 'middle east', 'africa', 'oceania',
        'rest of world', 'row', 'china', 'usa', 'india', 'japan', 'germany'
    ]

    # Count how many labels contain region keywords
    region_matches = sum(
        1 for label in label_texts
        if any(keyword in label for keyword in region_keywords)
    )

    # If 40%+ of labels are regions → "Regions"
    if region_matches / len(labels) >= 0.4:
        return "Regions"

    # 2. YEARS (e.g., 2023, 2024, 2025)
    year_pattern = r'\b(19|20)\d{2}\b'
    year_count = sum(1 for label in label_texts if re.search(year_pattern, label))
    if year_count / len(labels) > 0.5:
        return "Years"

    # 3. QUARTERS (Q1, Q2, Q3, Q4)
    quarter_pattern = r'\bq[1-4]\b'
    quarter_count = sum(1 for label in label_texts if re.search(quarter_pattern, label, re.IGNORECASE))
    if quarter_count >= 2:
        return "Quarters"

    # 4. MONTHS
    months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']
    month_count = sum(1 for label in label_texts if any(month in label for month in months))
    if month_count >= 3:
        return "Months"

    # 5. COMPANIES (common suffixes)
    company_keywords = ['inc', 'corp', 'ltd', 'llc', 'gmbh', 'ag', 'sa', 'plc']
    company_count = sum(1 for label in label_texts if any(kw in label for kw in company_keywords))
    if company_count >= 2:
        return "Companies"

    # 6. PRODUCTS/SEGMENTS (if contains "segment", "product", "category")
    if any(word in all_text for word in ['segment', 'product line', 'category', 'type']):
        return "Segments"

    # Default
    return "Categories"

def detect_y_label_dynamic(values: list) -> str:
    """Fully dynamic Y-axis label based on magnitude + context"""
    if not values:
        return "Value"

    numeric_values = []
    for v in values:
        try:
            numeric_values.append(abs(float(v)))
        except (ValueError, TypeError):
            continue

    if not numeric_values:
        return "Value"

    avg_mag = (sum(numeric_values) / float(len(numeric_values))) if numeric_values else 0.0
    max_mag = max(numeric_values)

    # Non-overlapping ranges with clear boundaries
    # 1. BILLIONS (large market sizes)
    if max_mag > 100 or avg_mag > 50:
        return "USD B"

    # 2. MILLIONS (medium values)
    elif max_mag > 10 or avg_mag > 5:
        return "USD M"

    # 3. PERCENTAGES (typical 0-100 range, but also small decimals)
    elif max_mag <= 100 and avg_mag <= 50:
        # Check if values look like percentages (mostly 0-100)
        if all(0 <= v <= 100 for v in numeric_values):
            return "Percent %"
        else:
            return "USD K"

    # 4. Default
    else:
        return "Units"

# 3A. QUESTION CATEGORIZATION + SIGNALS (DETERMINISTIC)

def categorize_question_signals(query: str, qs: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Build a question_profile used for structured reporting.

    IMPORTANT:
      - category must follow query_structure if provided (single source of truth).
      - signals can be rich, but must not contradict the chosen category.
    """
    qs = qs or {}
    q = (query or "").strip()

    # Prefer category/main/side from query_structure when available
    category = (qs.get("category") or "").strip() or "unknown"
    main_q = (qs.get("main") or "").strip() or q
    side_qs = qs.get("side") if isinstance(qs.get("side"), list) else []

    # Deterministic signals (richer classifier)
    base = classify_question_signals(q) or {}

    # NLP01: compute expected_metric_ids early so deterministic query-frame enrichment can use it (proposal-only)
    expected_metric_ids: List[str] = []
    try:
        expected_metric_ids = get_expected_metric_ids_for_category(category) or []
    except Exception:
        expected_metric_ids = []

    # Optional: enrich with intent-based suggestions (won't remove anything)
    intent_metric_suggestions = {
        "market_size": ["market_size", "market_size_2024", "market_size_2025"],
        "growth_forecast": ["cagr", "market_size_2030"],
        "competitive_landscape": ["market_share", "top_players"],
        "pricing": ["avg_price", "asp"],
        "consumer_demand": ["users", "penetration", "arpu"],
        "supply_chain": ["capacity", "shipments"],
        "investment": ["capex", "profit", "ebitda"],
        "macro_outlook": ["gdp", "inflation", "interest_rate", "exchange_rate"],
    }

    try:
        _base_intents = base.get("intents") or []
        if not isinstance(_base_intents, list):
            _base_intents = []
        for intent in _base_intents:
            for mid in intent_metric_suggestions.get(str(intent or "").strip(), []):
                if mid and mid not in expected_metric_ids:
                    expected_metric_ids.append(mid)
    except Exception:
        pass

    # Inject expected_metric_ids into base_signals so query-frame preview can enrich metric_families deterministically.
    try:
        if isinstance(base, dict):
            base = dict(base)
            base["expected_metric_ids"] = list(expected_metric_ids)
    except Exception:
        pass

    # LLM03: query framing / intent decomposition (proposal-only; rules decide)
    _llm03_frame_eff = None
    _llm03_frame_dbg = None
    try:
        _llm03_frame_eff, _llm03_frame_dbg = _llm03_get_query_frame_v1(q, base_signals=base)
    except Exception:
        _llm03_frame_eff, _llm03_frame_dbg = None, None

    # Force category + expected_metric_ids to match query_structure category
    # (but preserve other extracted info like years/regions/intents)
    signals: Dict[str, Any] = {}
    signals["category"] = category

    # Attach query frame for downstream (search boosting / scoring hints). Safe: proposals only.
    try:
        if isinstance(_llm03_frame_eff, dict):
            signals["query_frame_v1"] = _llm03_frame_eff
            signals["query_frame_used_llm"] = bool((_llm03_frame_dbg or {}).get("llm_used"))
            signals["query_frame_llm_ok"] = bool((_llm03_frame_dbg or {}).get("llm_ok"))
    except Exception:
        pass

    # Carry over extracted fields
    signals["years"] = base.get("years", []) or []
    signals["regions"] = base.get("regions", []) or []
    signals["intents"] = base.get("intents", []) or []

    # Keep raw signals for debugging
    raw_hits = list(base.get("signals") or [])
    signals["raw_signals"] = raw_hits

    def _signal_consistent_with_category(sig: str, cat: str) -> bool:
        s = (sig or "").lower()
        c = (cat or "").lower()
        if not s:
            return False

        # If final category is country, drop industry/company category-rule strings
        if c == "country":
            if "industry_keywords" in s or "mixed_signals_default_to_industry" in s or "company_keywords" in s:
                return False

        # If final category is industry, drop explicit country-rule strings
        if c == "industry":
            if "macro_outlook_bias_country" in s or "country_keywords" in s:
                return False

        return True

    signals["signals"] = [s for s in raw_hits if _signal_consistent_with_category(s, category)]

        # Expected metric IDs (category-derived; precomputed for NLP01 query-frame enrichment)
    signals["expected_metric_ids"] = expected_metric_ids
    profile: Dict[str, Any] = {
        "category": category,
        "signals": signals,
        "main_question": main_q,
        "side_questions": side_qs,
    }

    # Keep debug for traceability
    if qs.get("debug") is not None:
        profile["debug_query_structure"] = qs.get("debug")

    # LLM03: query frame debug (no prompts; proposal-only)
    try:
        if isinstance(_llm03_frame_dbg, dict):
            profile["debug_query_frame_v1"] = _llm03_frame_dbg
    except Exception:
        pass

    return profile

# REFACTOR137: UI panels restored from FIX2D88 (render-only; no pipeline logic changes).
def _yureeka_payload_views_v1(data: Any) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    """Return (wrapper, primary) views for nested payload compatibility."""
    wrapper: Dict[str, Any] = data if isinstance(data, dict) else {}
    primary: Dict[str, Any] = {}
    try:
        pr = wrapper.get("primary_response")
        if isinstance(pr, dict) and pr:
            primary = pr
    except Exception:
        primary = {}
    if not primary:
        primary = wrapper
    return wrapper, primary

def _yureeka_as_dict_v1(obj: Any) -> Optional[Dict[str, Any]]:
    """Coerce pydantic models / mappings into a dict (best-effort)."""
    if obj is None:
        return None
    if isinstance(obj, dict):
        return obj
    try:
        if hasattr(obj, "model_dump"):
            return obj.model_dump()
    except Exception:
        pass
    try:
        return dict(obj)
    except Exception:
        return None

def _yureeka_render_sources_reliability_panel_v1(
    data: Any,
    web_context: Optional[Dict[str, Any]] = None,
    source_reliability: Optional[List[str]] = None,
) -> None:
    """Render Sources & Reliability panel (optional, safe)."""
    st.subheader("🔗 Sources & Reliability")

    web_context = web_context if isinstance(web_context, dict) else {}
    wrapper, primary = _yureeka_payload_views_v1(data)

    # Collect sources from both payload + web_context (preserve order, de-dup)
    _raw_sources: List[str] = []
    try:
        _candidates = []
        for _d in (wrapper, primary):
            _candidates.append(_d.get("sources"))
            _candidates.append(_d.get("web_sources"))
            _candidates.append(_d.get("web_context_sources"))

        # Optional: evolution result shapes
        _candidates.append(wrapper.get("baseline_sources_cache"))
        _candidates.append(wrapper.get("baseline_sources_cache_current"))
        _candidates.append(wrapper.get("source_results"))

        _candidates.append(web_context.get("sources"))
        _candidates.append(web_context.get("urls"))

        def _push_url(u: Any):
            try:
                if isinstance(u, str) and u.strip():
                    _raw_sources.append(u.strip())
            except Exception:
                pass

        for _cand in _candidates:
            if isinstance(_cand, (list, tuple)):
                for _s in _cand:
                    if isinstance(_s, str):
                        _push_url(_s)
                    elif isinstance(_s, dict):
                        for _k in ("url", "source_url", "href"):
                            _push_url(_s.get(_k))
            elif isinstance(_cand, dict):
                # some pipelines store sources as dict{name:url}
                for _k, _v in list(_cand.items())[:50]:
                    _push_url(_v)
    except Exception:
        pass

    _seen = set()
    all_sources: List[str] = []
    for _s in _raw_sources:
        if _s not in _seen:
            _seen.add(_s)
            all_sources.append(_s)

    if not all_sources:
        st.info("No sources found.")
        return

    st.success(f"📊 Found {len(all_sources)} sources")

    # Reliability breakdown
    try:
        _rels = [classify_source_reliability(s) for s in all_sources]
        _cnt = Counter(_rels)
        c1, c2, c3 = st.columns(3)
        c1.metric("High", str(_cnt.get("✅ High", 0)))
        c2.metric("Medium", str(_cnt.get("⚠️ Medium", 0)))
        c3.metric("Low", str(_cnt.get("❌ Low", 0)))
    except Exception:
        pass

    # Compact display (2 columns, short URLs)
    cols = st.columns(2)
    for i, src in enumerate(all_sources[:10], 1):
        col = cols[(i - 1) % 2]
        short_url = src[:60] + "..." if len(src) > 60 else src
        reliability = classify_source_reliability(str(src))
        try:
            col.markdown(
                f"**{i}.** [{short_url}]({src})<br><small>{reliability}</small>",
                unsafe_allow_html=True,
            )
        except Exception:
            col.write(f"{i}. {short_url} ({reliability})")

    # Metadata (freshness + computed source quality)
    col_fresh, col_score = st.columns(2)
    with col_fresh:
        freshness = None
        try:
            freshness = wrapper.get("freshness") or primary.get("freshness")
        except Exception:
            freshness = None
        if not freshness:
            freshness = web_context.get("freshness")
        st.metric("Data Freshness", str(freshness or "Current"))

    with col_score:
        try:
            st.metric("Source Quality", f"{source_quality_score(all_sources):.0f}%")
        except Exception:
            pass

    # Optional: pipeline-provided reliability lines (if present)
    if isinstance(source_reliability, list) and source_reliability:
        with st.expander("Reliability details"):
            for line in source_reliability[:120]:
                if line:
                    st.write(line)

    with st.expander("All sources"):
        for s in all_sources[:200]:
            if s:
                st.write(s)
        if len(all_sources) > 200:
            st.write(f"... (+{len(all_sources)-200} more)")

    st.markdown("---")

def _yureeka_render_evidence_quality_scores_panel_v1(
    data: Any,
    veracity_scores: Optional[Dict[str, Any]] = None,
) -> None:
    """Render Evidence Quality Scores panel (optional, safe)."""
    st.subheader("✅ Evidence Quality Scores")
    wrapper, primary = _yureeka_payload_views_v1(data)

    _vs = veracity_scores
    if not (isinstance(_vs, dict) and _vs):
        _vs = wrapper.get("veracity_scores") if isinstance(wrapper, dict) else None
    if not (isinstance(_vs, dict) and _vs):
        _vs = primary.get("veracity_scores") if isinstance(primary, dict) else None

    if not (isinstance(_vs, dict) and _vs):
        st.info("No evidence quality scores available.")
        return

    cols = st.columns(6)
    metrics_display = [
        ("Sources", "source_quality"),
        ("Numbers", "numeric_consistency"),
        ("Citations", "citation_density"),
        ("Consensus", "source_consensus"),
        ("Freshness", "data_freshness"),
        ("Overall", "overall"),
    ]
    for i, (label, key) in enumerate(metrics_display):
        val = _vs.get(key, None)
        if val is None or val == "":
            cols[i].metric(label, "—")
            continue
        try:
            val_f = float(val)
        except Exception:
            cols[i].metric(label, "—")
            continue

        cols[i].metric(label, f"{val_f:.0f}%")

    # LLM35: optional freshness context (median age + bucket counts)
    try:
        if isinstance(_vs, dict) and (_vs.get("data_freshness") not in (None, "", "—")):
            dbg = wrapper.get("debug") if isinstance(wrapper, dict) else None
            f01 = dbg.get("fresh01_source_freshness_v1") if isinstance(dbg, dict) else None
            if isinstance(f01, dict):
                med = f01.get("median_age_days")
                bcnt = f01.get("bucket_counts") if isinstance(f01.get("bucket_counts"), dict) else {}
                pool_key = str(f01.get("pool_key") or "")
                parts = []
                try:
                    if med is not None:
                        parts.append(f"median age {int(med)}d")
                except Exception:
                    pass
                try:
                    if bcnt:
                        items = sorted(((str(k), int(v)) for k, v in bcnt.items()), key=lambda kv: kv[0])
                        parts.append("buckets " + ", ".join([f"{k}:{v}" for k, v in items[:6]]))
                except Exception:
                    pass
                if pool_key:
                    parts.append(f"pool {pool_key}")
                if parts:
                    st.caption("Freshness context: " + " · ".join(parts))
    except Exception:
        pass
def _yureeka_render_data_visualization_panel_v1(data: Any) -> None:
    """Render Data Visualization panel (optional dependency on Plotly)."""
    st.subheader("📊 Data Visualization")
    wrapper, primary = _yureeka_payload_views_v1(data)

    # Optional Plotly import
    px = None
    try:
        import plotly.express as px  # type: ignore
    except Exception:
        px = None

    def _get_pd():
        try:
            return pd
        except Exception:
            try:
                import pandas as _pd  # type: ignore
                return _pd
            except Exception:
                return None

    # Primary visualization
    viz = _yureeka_as_dict_v1(wrapper.get("visualization_data")) or _yureeka_as_dict_v1(primary.get("visualization_data"))

    if viz and isinstance(viz, dict):
        labels = viz.get("chart_labels") or viz.get("labels") or []
        values = viz.get("chart_values") or viz.get("values") or []
        title = viz.get("chart_title") or viz.get("title") or "Trend Analysis"
        chart_type = str(viz.get("chart_type") or "line").strip().lower()

        if not isinstance(labels, list):
            labels = [labels]
        if not isinstance(values, list):
            values = [values]

        pairs: List[Tuple[str, float]] = []
        try:
            for l, v in zip(labels, values):
                fv = parse_to_float(v)
                if fv is None:
                    continue
                pairs.append((str(l), float(fv)))
        except Exception:
            pairs = []

        if pairs:
            pairs = pairs[:20]
            x_vals = [p[0] for p in pairs]
            y_vals = [p[1] for p in pairs]

            # axis labels (best effort)
            try:
                x_label = (viz.get("x_axis_label") or "").strip() or detect_x_label_dynamic(x_vals)
            except Exception:
                x_label = "Category"
            try:
                y_label = (viz.get("y_axis_label") or "").strip() or detect_y_label_dynamic(y_vals)
            except Exception:
                y_label = "Value"

            _pd = _get_pd()
            if _pd is None:
                st.info("📊 Visualization data present, but pandas is unavailable.")
            else:
                df_viz = _pd.DataFrame({"x": x_vals, "y": y_vals})

                if px is None:
                    # Safe fallback: show the data table + Streamlit chart
                    st.dataframe(df_viz, use_container_width=True, hide_index=True)
                    try:
                        st.line_chart(df_viz.set_index("x")["y"])
                    except Exception:
                        pass
                else:
                    try:
                        if chart_type == "bar":
                            try:
                                fig = px.bar(df_viz, x="x", y="y", title=title, text_auto=True)
                            except Exception:
                                fig = px.bar(df_viz, x="x", y="y", title=title)
                        else:
                            fig = px.line(df_viz, x="x", y="y", title=title, markers=True)

                        fig.update_layout(
                            xaxis_title=x_label,
                            yaxis_title=y_label,
                            title_font_size=16,
                            font=dict(size=12),
                        )
                        if len(x_vals) > 5:
                            fig.update_layout(xaxis=dict(tickangle=-45))

                        st.plotly_chart(fig, use_container_width=True)
                    except Exception as e:
                        st.info(f"⚠️ Chart rendering failed: {e}")
        else:
            st.info("📊 No usable numeric visualization data available.")
    else:
        st.info("📊 No visualization data available.")

    # Comparison bars (optional secondary chart)
    comp = _yureeka_as_dict_v1(wrapper.get("comparison_bars")) or _yureeka_as_dict_v1(primary.get("comparison_bars"))
    if comp and isinstance(comp, dict):
        cats = comp.get("categories") or []
        vals = comp.get("values") or []
        ctitle = comp.get("title") or "Comparison"

        if not isinstance(cats, list):
            cats = [cats]
        if not isinstance(vals, list):
            vals = [vals]

        comp_pairs: List[Tuple[str, float]] = []
        try:
            for c, v in zip(cats, vals):
                fv = parse_to_float(v)
                if fv is None:
                    continue
                comp_pairs.append((str(c), float(fv)))
        except Exception:
            comp_pairs = []

        if comp_pairs:
            comp_pairs = comp_pairs[:20]
            _pd = _get_pd()
            if _pd is not None:
                df_comp = _pd.DataFrame({"Category": [p[0] for p in comp_pairs], "Value": [p[1] for p in comp_pairs]})
                if px is None:
                    st.dataframe(df_comp, use_container_width=True, hide_index=True)
                else:
                    try:
                        try:
                            fig2 = px.bar(df_comp, x="Category", y="Value", title=ctitle, text_auto=True)
                        except Exception:
                            fig2 = px.bar(df_comp, x="Category", y="Value", title=ctitle)
                        st.plotly_chart(fig2, use_container_width=True)
                    except Exception:
                        pass

    # Benchmark table (optional)
    bt = wrapper.get("benchmark_table")
    if bt is None:
        bt = primary.get("benchmark_table")
    if isinstance(bt, list) and bt:
        _pd = _get_pd()
        if _pd is not None:
            try:
                st.markdown("**Benchmark Table**")
                st.dataframe(_pd.DataFrame(bt), use_container_width=True, hide_index=True)
            except Exception:
                pass

    st.markdown("---")

def _yureeka_render_diagnostics_expander_v1(payload: Dict, web_context: Optional[Dict] = None, label: str = ""):
    """Optional (collapsed) UI diagnostics beacons. Presentation-only."""
    try:
        title = "🧪 Diagnostics"
        if label:
            title = f"{title} {label}".strip()
        with st.expander(title, expanded=False):
            if not isinstance(payload, dict):
                st.info("No diagnostics payload available.")
                return

            # --- Schema key count / coverage ---
            schema = payload.get("metric_schema_frozen") or payload.get("metric_schema") or {}
            schema_key_count = len(schema) if isinstance(schema, dict) else None

            pr = payload.get("primary_response") if isinstance(payload.get("primary_response"), dict) else {}
            pmc = pr.get("primary_metrics_canonical") if isinstance(pr.get("primary_metrics_canonical"), dict) else None
            if pmc is None and isinstance(payload.get("primary_metrics_canonical"), dict):
                pmc = payload.get("primary_metrics_canonical")

            inv = None
            dbg = payload.get("debug") if isinstance(payload.get("debug"), dict) else {}
            if isinstance(dbg.get("harness_invariants_v1"), dict):
                inv = dbg.get("harness_invariants_v1")

            schema_summary = {
                "schema_frozen_key_count": (inv.get("schema_frozen_key_count") if isinstance(inv, dict) else schema_key_count),
                "baseline_key_count": (inv.get("baseline_key_count") if isinstance(inv, dict) else None),
                "current_key_count": (inv.get("current_key_count") if isinstance(inv, dict) else (len(pmc) if isinstance(pmc, dict) else None)),
                "coverage_ok": (inv.get("metric_changes_schema_coverage_ok") if isinstance(inv, dict) else None),
            }
            # drop Nones for cleanliness
            schema_summary = {k: v for k, v in schema_summary.items() if v is not None}
            if schema_summary:
                st.markdown("**Schema / Coverage**")
                st.json(schema_summary)

            # --- Fastpath / injection ---
            fastpath = dbg.get("fastpath_gate_v2") if isinstance(dbg.get("fastpath_gate_v2"), dict) else (dbg.get("fastpath_gate_v1") if isinstance(dbg.get("fastpath_gate_v1"), dict) else None)
            inj_present = None
            inj_seed = dbg.get("schema_seed_sources_v1") if isinstance(dbg.get("schema_seed_sources_v1"), dict) else None
            if isinstance(inj_seed, dict) and isinstance(inj_seed.get("injection_present"), bool):
                inj_present = inj_seed.get("injection_present")

            if isinstance(fastpath, dict) or (inj_present is not None):
                st.markdown("**Fast-path / Injection**")
                out = {}
                if inj_present is not None:
                    out["injection_present"] = inj_present
                if isinstance(fastpath, dict):
                    for k in ("eligible", "disabled_reason", "prev_hash", "cur_hash"):
                        if k in fastpath:
                            out[k] = fastpath.get(k)
                if out:
                    st.json(out)

            # --- Snapshot pick summary (Evolution) ---
            sp = dbg.get("prev_snapshot_pick_v1") if isinstance(dbg.get("prev_snapshot_pick_v1"), dict) else None
            if isinstance(sp, dict):
                st.markdown("**Snapshot pick**")
                sp_small = {k: sp.get(k) for k in ("origin_attempted", "selected_ref", "selected_timestamp", "selected_code_version", "reason") if sp.get(k) is not None}
                if sp_small:
                    st.json(sp_small)

            # --- Δt gating / rows ---
            mc_rows = payload.get("metric_changes") if isinstance(payload.get("metric_changes"), list) else None
            if mc_rows:
                delta_vals = [r.get("analysis_evolution_delta_seconds") for r in mc_rows if isinstance(r, dict)]
                st.markdown("**Δt gating**")
                st.json({
                    "rows_total": len(mc_rows),
                    "rows_with_delta_seconds": sum(isinstance(d, (int, float)) for d in delta_vals),
                    "rows_without_delta_seconds": sum(d is None for d in delta_vals),
                })

            # --- Web context collector counters (Analysis) ---
            if isinstance(web_context, dict):
                dbg_counts = web_context.get("debug_counts") if isinstance(web_context.get("debug_counts"), dict) else None
                if isinstance(dbg_counts, dict) and dbg_counts:
                    st.markdown("**Collector debug counts**")
                    st.json(dbg_counts)

                try:
                    exdbg = web_context.get("extra_urls_debug") if isinstance(web_context.get("extra_urls_debug"), dict) else None
                    if (not exdbg) and isinstance(web_context.get("debug_counts"), dict):
                        exdbg = (web_context.get("debug_counts") or {}).get("extra_urls_debug") if isinstance((web_context.get("debug_counts") or {}).get("extra_urls_debug"), dict) else None
                    if isinstance(exdbg, dict) and exdbg:
                        st.markdown("**Extra URLs trace (injected sources)**")
                        st.json(exdbg)
                except Exception:
                    pass
    except Exception:
        pass

def render_dashboard(
    primary_json: str,
    final_conf: float,
    web_context: Dict,
    base_conf: float,
    user_question: str,
    veracity_scores: Optional[Dict] = None,
    source_reliability: Optional[List[str]] = None,
    wrapper_output: Optional[Dict] = None,
):
    """Render the analysis dashboard"""

    # Parse primary response

    # - Prevents slice errors when primary_json is dict/list/etc.
    # - Keeps original behavior for strings
    def _preview(x, limit: int = 1000) -> str:
        try:
            if isinstance(x, (dict, list)):
                s = json.dumps(x, ensure_ascii=False, indent=2, default=str)
            else:
                s = str(x)
        except Exception:
            pass
            s = repr(x)
        return s[:limit]

    try:
        # - If caller passes dict (primary_data), just use it
        # - If caller passes list, wrap it (keeps downstream dict access safe)
        # - Else try json.loads on string
        if isinstance(primary_json, dict):
            data = primary_json
        elif isinstance(primary_json, list):
            data = {"_list": primary_json}
        else:
            data = json.loads(primary_json)

    except Exception as e:
        st.error(f"❌ Cannot render dashboard: {e}")
        st.code(_preview(primary_json))
        return

    # Helper: metric value formatting (currency + compact units) + RANGE SUPPORT
    def _format_metric_value(m: Any) -> str:
        """
        Format metric values cleanly, with RANGE SUPPORT:
        - If value_range exists (min/max), show min–max using the same currency/unit rules
        - Otherwise show the point value as before
        """
        if not isinstance(m, dict):
            if m is None:
                return "N/A"
            return str(m)

        # Helper: format a single numeric endpoint (val+unit)
        def _format_point(val: Any, unit: str) -> str:
            if val is None or val == "":
                return "N/A"

            unit = (unit or "").strip()
            raw_val = str(val).strip()

            # Try parse numeric
            try:
                num = float(raw_val.replace(",", ""))
            except Exception:
                pass
                # If we can't parse as float, just glue value+unit neatly
                return f"{raw_val}{unit}".strip() if unit else raw_val

            # Normalize unit spacing
            unit = unit.replace(" ", "")
            currency_prefix = ""
            u_upper = unit.upper()

            # Common patterns: "S$B", "SGDB", "USD B", "$B"
            if u_upper.startswith("S$"):
                currency_prefix = "S$"
                unit = unit[2:]
            elif u_upper.startswith("SGD"):
                currency_prefix = "S$"
                unit = unit[3:]
            elif u_upper.startswith("USD"):
                currency_prefix = "$"
                unit = unit[3:]
            elif u_upper.startswith("$"):
                currency_prefix = "$"
                unit = unit[1:]

            unit = unit.strip()

            # Percent
            if unit == "%":
                return f"{num:.1f}%"

            # Compact units
            unit_upper = unit.upper()
            if unit_upper in ("B", "BILLION"):
                formatted = f"{num:.2f}".rstrip("0").rstrip(".") + "B"
                return f"{currency_prefix}{formatted}".strip()
            if unit_upper in ("M", "MILLION"):
                formatted = f"{num:.2f}".rstrip("0").rstrip(".") + "M"
                return f"{currency_prefix}{formatted}".strip()
            if unit_upper in ("K", "THOUSAND"):
                formatted = f"{num:.2f}".rstrip("0").rstrip(".") + "K"
                return f"{currency_prefix}{formatted}".strip()

            # Plain number formatting
            if abs(num) >= 1000:
                if float(num).is_integer():
                    formatted = f"{int(num):,}"
                else:
                    formatted = f"{num:,.2f}".rstrip("0").rstrip(".")
            else:
                formatted = f"{num:g}"

            # Unit glue
            if unit:
                formatted = f"{formatted} {unit}".strip()

            return f"{currency_prefix}{formatted}".strip()

        # RANGE: prefer value_range if present and meaningful
        unit = (m.get("unit") or "").strip()
        vr = m.get("value_range")

        if isinstance(vr, dict):
            vmin = vr.get("min")
            vmax = vr.get("max")
            if vmin is not None and vmax is not None:
                left = _format_point(vmin, unit)
                right = _format_point(vmax, unit)
                if left != "N/A" and right != "N/A" and left != right:
                    return f"{left}–{right}"

        # Precomputed range display (optional)
        vr_disp = m.get("value_range_display")
        if isinstance(vr_disp, str) and vr_disp.strip():
            return vr_disp.strip()

        # POINT VALUE fallback
        val = m.get("value")
        if val is None or val == "":
            return "N/A"

        return _format_point(val, unit)

    # Header + confidence row
    st.header("📊 Yureeka Market Report")
    st.markdown(f"**Question:** {user_question}")

    col1, col2, col3 = st.columns(3)
    col1.metric("Final Confidence", f"{float(final_conf):.1f}%")
    col2.metric("Base Model", f"{float(base_conf):.1f}%")
    if isinstance(veracity_scores, dict):
        col3.metric("Evidence", f"{float(veracity_scores.get('overall', 0) or 0):.1f}%")
    else:
        col3.metric("Evidence", "N/A")

    st.markdown("---")

    # Executive Summary
    st.subheader("📋 Executive Summary")
    st.markdown(f"**{data.get('executive_summary', 'No summary available')}**")

    # Optional: expand summary if side-questions exist
    side_questions = data.get("side_questions") or (data.get("question_profile", {}) or {}).get("side_questions", [])
    if side_questions:
        st.markdown("")
        st.markdown("**Also addressed:**")
        for sq in side_questions[:6]:
            if sq:
                st.markdown(f"- {sq}")

    st.markdown("---")

    # Key Metrics
    st.subheader("💰 Key Metrics")
    metrics = data.get("primary_metrics", {}) or {}

    question_category = data.get("question_category") or (data.get("question_profile", {}) or {}).get("category")
    question_signals = data.get("question_signals") or (data.get("question_profile", {}) or {}).get("signals", {})
    expected_ids = data.get("expected_metric_ids") or ((data.get("question_signals") or {}).get("expected_metric_ids") or [])

    metric_rows: List[Dict[str, str]] = []

    if question_category:
        metric_rows.append({"Metric": "Question Category", "Value": str(question_category)})
    if isinstance(question_signals, dict) and question_signals:
        metric_rows.append({"Metric": "Signals", "Value": ", ".join([str(x) for x in (question_signals.get("signals") or [])][:10])})
    if expected_ids:
        metric_rows.append({"Metric": "Expected Metrics", "Value": ", ".join([str(x) for x in expected_ids][:10])})

    # Render primary metrics
    if isinstance(metrics, dict) and metrics:
        for _, m in metrics.items():
            if isinstance(m, dict):
                name = m.get("name") or "Metric"
                metric_rows.append({"Metric": str(name), "Value": _format_metric_value(m)})

    # Display metrics table
    if metric_rows:
        try:
            import pandas as pd  # optional dependency in your environment
            df_metrics = pd.DataFrame(metric_rows)
            st.dataframe(df_metrics, use_container_width=True, hide_index=True)
        except Exception:
            pass
            for r in metric_rows:
                st.write(f"**{r.get('Metric','')}**: {r.get('Value','')}")

    st.markdown("---")

    # Key Findings
    st.subheader("🧠 Key Findings")
    kf = data.get("key_findings") or []
    if isinstance(kf, list) and kf:
        for item in kf[:12]:
            if item:
                st.markdown(f"- {item}")
    else:
        st.info("No key findings available.")

    st.markdown("---")

    # Trends / Forecast
    st.subheader("📈 Trends & Forecast")
    tf = data.get("trends_forecast") or []
    if isinstance(tf, list) and tf:
        for t in tf[:12]:
            if isinstance(t, dict):
                trend = t.get("trend") or ""
                direction = t.get("direction") or ""
                timeline = t.get("timeline") or ""
                st.markdown(f"- **{trend}** {direction} ({timeline})")
            elif t:
                st.markdown(f"- {t}")
    else:
        st.info("No trends forecast available.")

    st.markdown("---")

    # REFACTOR138: De-duplicate legacy Sources & Evidence block.
    # Sources, reliability, and evidence traces are now displayed through the canonical Sources & Reliability panel below.

    # REFACTOR137: Restored UI panels from FIX2D88 (render-only).
    try:
        _yureeka_render_sources_reliability_panel_v1(data, web_context=web_context, source_reliability=source_reliability)
        _yureeka_render_evidence_quality_scores_panel_v1(data, veracity_scores=veracity_scores)
        st.markdown("---")
        _yureeka_render_data_visualization_panel_v1(data)
    except Exception:
        pass

    # REFACTOR138: Optional diagnostics beacons (collapsed by default)
    try:
        _payload = wrapper_output if isinstance(wrapper_output, dict) else data
        _yureeka_render_diagnostics_expander_v1(_payload, web_context=web_context, label='(Analysis)')
    except Exception:
        pass

def render_native_comparison(baseline: Dict, compare: Dict):
    """Render a clean comparison between two analyses"""

    st.header("📊 Analysis Comparison")

    # Time info
    baseline_time = baseline.get('timestamp', '')
    compare_time = compare.get('timestamp', '')

    try:
        baseline_dt = datetime.fromisoformat(baselinetime.replace('Z', '+00:00'))
        compare_dt = datetime.fromisoformat(comparetime.replace('Z', '+00:00'))
        delta = compare_dt - baseline_dt
        if delta.days > 0:
            delta_str = f"{delta.days}d {delta.seconds // 3600}h"
        else:
            delta_str = f"{delta.seconds // 3600}h {(delta.seconds % 3600) // 60}m"
    except:
        delta_str = "Unknown"

    # Overview row
    col1, col2, col3 = st.columns(3)
    col1.metric("Baseline", baseline_time[:16] if baseline_time else "N/A")
    col2.metric("Current", compare_time[:16] if compare_time else "N/A")
    col3.metric("Time Delta", delta_str)

    st.markdown("---")

    # Extract metrics
    baseline_metrics = baseline.get('primary_response', {}).get('primary_metrics', {})
    compare_metrics = compare.get('primary_response', {}).get('primary_metrics', {})

    # Build metric diff table
    st.subheader("💰 Metric Changes")

    diff_rows = []
    stability_count = 0
    total_count = 0

    # Canonicalize metrics for stable matching
    baseline_canonical = canonicalize_metrics(baseline_metrics)
    compare_canonical = canonicalize_metrics(compare_metrics)

    # Build lookup by canonical ID
    baseline_by_id = {}
    compare_by_id = {}

    for cid, m in baseline_canonical.items():
        baseline_by_id[cid] = m

    for cid, m in compare_canonical.items():
        compare_by_id[cid] = m

    all_ids = set(baseline_by_id.keys()).intersection(compare_by_id.keys())

    for cid in sorted(all_ids):
        baseline_m = baseline_by_id.get(cid)
        compare_m = compare_by_id.get(cid)

        # Use canonical name for display, fallback to original
        display_name = cid
        if baseline_m and baseline_m.get('name'):
            display_name = baseline_m['name']

        if baseline_m and compare_m:
            old_val = baseline_m.get('value', 'N/A')
            new_val = compare_m.get('value', 'N/A')
            unit = compare_m.get('unit', baseline_m.get('unit', ''))

            old_num = parse_to_float(old_val)
            new_num = parse_to_float(new_val)

            if old_num is not None and new_num is not None and old_num != 0:
                change_pct = ((new_num - old_num) / abs(old_num)) * 100

                if abs(change_pct) < 1:
                    icon, reason = "➡️", "No change"
                    stability_count += 1
                elif abs(change_pct) < 5:
                    icon, reason = "➡️", "Minor change"
                    stability_count += 1
                elif change_pct > 0:
                    icon, reason = "📈", "Increased"
                else:
                    icon, reason = "📉", "Decreased"

                delta_str = f"{change_pct:+.1f}%"
            else:
                icon, delta_str, reason = "➡️", "-", "Non-numeric"
                stability_count += 1

            diff_rows.append({
                '': icon,
                'Metric': display_name,
                'Old': _fmt_currency_first(str(old_val), str(unit)),
                'New': _fmt_currency_first(str(new_val), str(unit)),
                'Δ': delta_str,
                'Reason': reason
            })
            total_count += 1

        elif baseline_m:
            old_val = baseline_m.get('value', 'N/A')
            unit = baseline_m.get('unit', '')
            diff_rows.append({
                '': '❌',
                'Metric': display_name,
                'Old': f"{old_val} {unit}".strip(),
                'New': '-',
                'Δ': '-',
                'Reason': 'Removed'
            })
            total_count += 1
        else:
            new_val = compare_m.get('value', 'N/A')
            unit = compare_m.get('unit', '')
            diff_rows.append({
                '': '🆕',
                'Metric': display_name,
                'Old': '-',
                'New': f"{new_val} {unit}".strip(),
                'Δ': '-',
                'Reason': 'New'
            })
            total_count += 1

    if diff_rows:
        try:
            import pandas as _pd  # optional dependency
            st.dataframe(_pd.DataFrame(diff_rows), hide_index=True, use_container_width=True)
        except Exception:
            st.dataframe(diff_rows, use_container_width=True)

        # Show canonical ID mapping for debugging
    else:
        st.info("No metrics to compare")

    # Stability score
    stability_pct = (stability_count / total_count * 100) if total_count > 0 else 100

    st.markdown("---")
    st.subheader("📊 Stability Score")

    col1, col2, col3 = st.columns(3)
    col1.metric("Stable Metrics", f"{stability_count}/{total_count}")
    col2.metric("Stability", f"{stability_pct:.0f}%")

    if stability_pct >= 80:
        col3.success("🟢 Highly Stable")
    elif stability_pct >= 60:
        col3.warning("🟡 Moderate Changes")
    else:
        col3.error("🔴 Significant Drift")

    # Confidence comparison
    st.markdown("---")
    st.subheader("🎯 Confidence Change")

    col1, col2, col3 = st.columns(3)
    baseline_conf = baseline.get('final_confidence', 0)
    compare_conf = compare.get('final_confidence', 0)
    conf_change = compare_conf - baseline_conf if isinstance(baseline_conf, (int, float)) and isinstance(compare_conf, (int, float)) else 0

    col1.metric("Baseline", f"{baseline_conf:.1f}%" if isinstance(baseline_conf, (int, float)) else "N/A")
    col2.metric("Current", f"{compare_conf:.1f}%" if isinstance(compare_conf, (int, float)) else "N/A")
    col3.metric("Change", f"{conf_change:+.1f}%")

    # Download comparison
    st.markdown("---")
    comparison_output = {
        "comparison_timestamp": _yureeka_now_iso_utc(),
        "baseline": baseline,
        "current": compare,
        "stability_score": stability_pct,
        "metrics_compared": total_count,
        "metrics_stable": stability_count
    }

    st.download_button(
        label="💾 Download Comparison Report",
        data=json.dumps(comparison_output, indent=2, ensure_ascii=False).encode('utf-8'),
        file_name=f"yureeka_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        mime="application/json"
    )

# 10. MAIN APPLICATION

#
# Why:
# - Even if upstream selection is tightened, some paths (UI render, sheet publish,
#   legacy mappings) can still surface unit-less year-like integers (e.g., 2024/2025)
#   in the "Current" column for unit-required metrics (currency/percent/rate/ratio).
# - FIX39 enforces the invariant at the last mile: right before rendering/publishing.
#
# Behavior:
# - For each metric change row (dict-form) and each EvolutionDiff metric entry (object-form),
#   if schema indicates unit required (via unit_family or canonical_key suffix) AND
#   current value lacks token-level unit evidence, then:
#     * blank out Current/new_raw
#     * set unit_mismatch flag / change_type to "unit_mismatch" where possible
# - Purely additive; does not refactor upstream pipelines.

# [MOD:UI_APP]
def main():
    st.set_page_config(
        page_title="Yureeka Market Report",
        page_icon="💹",
        layout="wide"
    )

    st.title("💹 Yureeka Market Intelligence")
    # Info section
    col_info, col_status = st.columns([3, 1])
    with col_info:
        st.markdown("""
        **Yureeka** provides AI-powered market research and analysis for finance,
        economics, and business questions.
        Powered by evidence-based verification and real-time web search.

        *Currently in prototype stage.*
        """)

    # Create tabs
    tab1, tab2 = st.tabs(["🔍 New Analysis", "📈 Evolution Analysis"])

    with tab1:
        query = st.text_input(
            "Enter your question about markets, industries, finance, or economics:",
            placeholder="e.g., What is the size of the global EV battery market?"
        )

        col_opt1, col_opt2 = st.columns(2)
        with col_opt1:
            use_web = st.checkbox(
                "Enable web search (recommended)",
                value=bool(SERPAPI_KEY),
                disabled=not SERPAPI_KEY
            )

            # - Add extra URL injection UI directly to TAB 1 (New Analysis)

            # - Does NOT alter behavior unless user supplies URLs

            extra_sources_text_tab1 = st.text_area(

                "Extra source URLs (optional, one per line)",

                placeholder="https://example.com/report\nhttps://another-source.com/page",

                help="Add these URLs to the admitted source list for this analysis run (useful for hash-mismatch tests).",

                height=90,

                key="ui_extra_sources_tab1",

            )

        if st.button("🔍 Analyze", type="primary") and query:
            if len(query.strip()) < 5:
                st.error("❌ Please enter a question with at least 5 characters")
                return

            query = query.strip()[:500]

            # LLM05: reset per-run LLM diagnostics
            try:
                _yureeka_llm_reset_run_state_v1(stage="analysis")
            except Exception:
                pass

            query_structure = extract_query_structure(query) or {}
            question_profile = categorize_question_signals(query, qs=query_structure)
            question_signals = question_profile.get("signals", {}) or {}

            web_context = {}
            if use_web:
                with st.spinner("🌐 Searching the web..."):

                    existing_snapshots = None

                    # If you have an analysis dict already in scope, reuse its cache
                    try:
                        if isinstance(locals().get("analysis"), dict):
                            existing_snapshots = (
                                analysis.get("baseline_sources_cache")
                                or (analysis.get("results", {}) or {}).get("baseline_sources_cache")
                                or (analysis.get("results", {}) or {}).get("source_results")
                            )
                    except Exception:
                        pass
                        existing_snapshots = None

                    # Optional: if you keep a prior analysis in session_state, reuse it
                    try:
                        prev = st.session_state.get("last_analysis")
                        if existing_snapshots is None and isinstance(prev, dict):
                            existing_snapshots = (
                                prev.get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("source_results")
                            )
                    except Exception:
                        pass

                    extra_urls = []
                    try:
                        for _l in str(extra_sources_text_tab1 or "").splitlines():
                            _u = _l.strip()
                            if not _u:
                                continue
                            if _u.startswith("http://") or _u.startswith("https://"):
                                extra_urls.append(_u)
                    except Exception:
                        pass
                        extra_urls = []

                    _analysis_run_id = _inj_diag_make_run_id("analysis")

                    web_context = fetch_web_context(
                        _llm03_boost_web_query_v1(query, (question_signals or {}).get("query_frame_v1")),
                        num_sources=3,
                        existing_snapshots=existing_snapshots,
                        extra_urls=extra_urls,
                        diag_run_id=_analysis_run_id,
                        diag_extra_urls_ui_raw=(extra_sources_text_tab1 or ""),
                    )

            if not web_context or not web_context.get("search_results"):
                st.info("💡 Using AI knowledge without web search")
                web_context = {
                    "search_results": [],
                    "scraped_content": {},
                    "summary": "",
                    "sources": [],
                    "source_reliability": []
                }

            with st.spinner("🤖 Analyzing query..."):
                primary_response = query_perplexity(query, web_context, query_structure=query_structure)

            if not primary_response:
                st.error("❌ Primary model failed to respond")
                return

            try:
                primary_data = json.loads(primary_response)
            except Exception as e:
                st.error(f"❌ Failed to parse primary response: {e}")
                st.code(primary_response[:1000])
                return

            with st.spinner("✅ Verifying evidence quality..."):
                veracity_scores = evidence_based_veracity(primary_data, web_context)

            base_conf = float(primary_data.get("confidence", 75))
            final_conf = calculate_final_confidence(base_conf, veracity_scores.get("overall", 0))

            # Optional: canonicalize + attribution + schema freeze (only if your codebase defines these)
            try:
                # 1) canonicalize (unchanged)
                if primary_data.get("primary_metrics"):
                    _pmc_raw = canonicalize_metrics(
                        primary_data.get("primary_metrics", {}),
                        merge_duplicates_to_range=True,
                        question_text=query,
                        category_hint=str(primary_data.get("question_category", ""))
                    )
                    _pmc_ok, _pmc_prov = _fix2d58b_split_primary_metrics_canonical(_pmc_raw)
                    primary_data["primary_metrics_canonical"] = _pmc_ok
                    if _pmc_prov:
                        primary_data["primary_metrics_provisional"] = _pmc_prov

                # 2) freeze schema FIRST ✅ (so attribution can be schema-first)
                if primary_data.get("primary_metrics_canonical"):
                    primary_data["metric_schema_frozen"] = freeze_metric_schema(
                        primary_data["primary_metrics_canonical"]
                    )

                try:
                    primary_data["metric_schema_frozen"] = _seed_metric_schema_frozen_v1(primary_data.get("metric_schema_frozen") or {})
                except Exception:
                    pass

                # - Build schema proposals from primary_metrics_provisional using freeze_metric_schema.
                # - Auto-promote into metric_schema_frozen (governance can later restrict via allowlist).
                # - Record proposals/promotions for audit.
                try:
                    _prov = primary_data.get("primary_metrics_provisional")
                    _schema = primary_data.get("metric_schema_frozen")
                    if isinstance(_prov, dict) and _prov and isinstance(_schema, dict):
                        _prov_schema = freeze_metric_schema(_prov)
                        if isinstance(_prov_schema, dict) and _prov_schema:
                            primary_data["schema_promotion_proposals_v1"] = sorted([str(k) for k in _prov_schema.keys()])
                            # Auto-promote: merge proposals into frozen schema
                            for _k, _spec in _prov_schema.items():
                                if _k not in _schema:
                                    _schema[_k] = _spec
                            primary_data["metric_schema_frozen"] = _schema
                            primary_data["schema_promoted_v1"] = sorted([str(k) for k in _prov_schema.keys() if str(k) in _schema])
                except Exception:
                    pass

                # 2.B) FIX2D59: schema-first canonical identity rekey (Analysis)
                # - Routes existing pmc keys through the resolver using the frozen schema (after schema extension patches).
                try:
                    if isinstance(primary_data.get('primary_metrics_canonical'), dict) and isinstance(primary_data.get('metric_schema_frozen'), dict):
                        primary_data['primary_metrics_canonical'] = rekey_metrics_via_identity_resolver_v1(
                            primary_data.get('primary_metrics_canonical') or {},
                            primary_data.get('metric_schema_frozen') or {},
                        )
                except Exception:
                    pass
                try:
                    if isinstance(primary_data.get('primary_metrics_provisional'), dict) and isinstance(primary_data.get('metric_schema_frozen'), dict):
                        primary_data['primary_metrics_provisional'] = rekey_metrics_via_identity_resolver_v1(
                            primary_data.get('primary_metrics_provisional') or {},
                            primary_data.get('metric_schema_frozen') or {},
                        )
                except Exception:
                    pass

                # - After schema promotion + rekey, merge provisional into canonical so bound rows can be retained.
                try:
                    _prov = primary_data.get("primary_metrics_provisional")
                    if isinstance(_prov, dict) and _prov:
                        _can = primary_data.get("primary_metrics_canonical")
                        if not isinstance(_can, dict):
                            _can = {}
                        for _k, _v in _prov.items():
                            _can[_k] = _v
                        primary_data["primary_metrics_canonical"] = _can
                        primary_data["primary_metrics_provisional"] = {}
                except Exception:
                    pass

                # - After rekeying, keep ONLY schema-bound keys in primary_metrics_canonical.
                # - Move everything else into primary_metrics_provisional (quarantined for audit).
                try:
                    _pmc_bound, _pmc_not_bound = _fix2d60_split_schema_bound_only(primary_data.get('primary_metrics_canonical') or {})
                    if isinstance(_pmc_bound, dict):
                        primary_data['primary_metrics_canonical'] = _pmc_bound
                    if isinstance(_pmc_not_bound, dict) and _pmc_not_bound:
                        _prov = primary_data.get('primary_metrics_provisional')
                        if not isinstance(_prov, dict):
                            _prov = {}
                        # merge (schema-bound rule is stronger than any earlier provisional split)
                        for _k, _v in _pmc_not_bound.items():
                            _prov[_k] = _v
                        primary_data['primary_metrics_provisional'] = _prov
                except Exception:
                    pass

                # 3) attribution using frozen schema ✅
                if primary_data.get("primary_metrics_canonical"):
                    primary_data["primary_metrics_canonical"] = add_range_and_source_attribution_to_canonical_metrics(
                        primary_data.get("primary_metrics_canonical", {}),
                        web_context,
                        metric_schema=(primary_data.get("metric_schema_frozen") or {}),
                    )

                try:
                    primary_data = apply_schema_validation_and_evidence_gating(primary_data)
                except Exception:
                    pass

            except Exception:
                pass

            # Hash key findings (optional)
            try:
                if primary_data.get("key_findings"):
                    findings_with_hash = []
                    for finding in primary_data.get("key_findings", []):
                        if finding:
                            findings_with_hash.append({
                                "text": finding,
                                "semantic_hash": compute_semantic_hash(finding)
                            })
                    primary_data["key_findings_hashed"] = findings_with_hash
            except Exception:
                pass

            # Save baseline numeric cache if available (existing behavior)

            # Build output
            output = {
                "question": query,
                "question_profile": question_profile,
                "question_category": question_profile.get("category"),
                "question_signals": question_signals,
                "side_questions": question_profile.get("side_questions", []),
                "timestamp": _yureeka_now_iso_utc(),
                "primary_response": primary_data,
                "final_confidence": final_conf,
                "veracity_scores": veracity_scores,
                "web_sources": web_context.get("sources", []),
                "code_version": _yureeka_get_code_version(),
            "authority_manifest_v1": _yureeka_authority_manifest_v1(),
                }

            try:
                output.setdefault("debug", {})
                if isinstance(output.get("debug"), dict):
                    output["debug"].setdefault("runtime_identity_v1", _yureeka_runtime_identity_v1())
            except Exception:
                pass

            try:
                if isinstance(output.get("primary_response"), dict):
                    output["primary_response"]["code_version"] = _yureeka_get_code_version()
            except Exception:
                pass

            # ✅ NEW: attach analysis-aligned snapshots (from scraped_meta)
            # This is the stable cache evolution should reuse.
            try:
                output = attach_source_snapshots_to_analysis(output, web_context)
            except Exception:
                pass

            # Why:
            # - Evolution diffing requires previous_data.primary_metrics_canonical to exist and be schema-keyed.
            # - Some analysis paths seed metric_schema_frozen but do not emit primary_metrics_canonical into the
            #   persisted payload. This causes Evolution to report no_prev_metrics even when keys are stable.
            # What:
            # - If primary_metrics_canonical is missing/empty, rebuild it deterministically from the frozen schema
            #   using the same authoritative rebuild helper used by Evolution.
            # - Write into BOTH top-level and results for maximum persistence compatibility.
            try:
                _pmc0 = output.get('primary_metrics_canonical')
                _pmc0_empty = (not isinstance(_pmc0, dict)) or (not _pmc0)
                if _pmc0_empty:
                    fn_rebuild = globals().get('rebuild_metrics_from_snapshots_analysis_canonical_v1')
                    if callable(fn_rebuild):
                        _rebuilt = fn_rebuild(output.get('primary_response') or {}, output.get('baseline_sources_cache') or [], web_context=web_context)
                        if isinstance(_rebuilt, dict) and _rebuilt:
                            output['primary_metrics_canonical'] = _rebuilt
                            output.setdefault('results', {})
                            if isinstance(output.get('results'), dict):
                                output['results']['primary_metrics_canonical'] = _rebuilt
                            # lightweight debug stamp
                            try:
                                output.setdefault('debug', {})
                                if isinstance(output.get('debug'), dict):
                                    output['debug']['fix2d72_analysis_pmc_written'] = True
                                    output['debug']['fix2d72_pmc_key_count'] = len(_rebuilt)
                            except Exception:
                                pass
            except Exception:
                pass



            # LLM01: attach evidence snippets + offsets (additive only; winners/values unchanged).
            try:
                _pmc_llm01 = None
                _schema_llm01 = None
                if isinstance(output.get("primary_response"), dict):
                    _pmc_llm01 = output["primary_response"].get("primary_metrics_canonical")
                    _schema_llm01 = output["primary_response"].get("metric_schema_frozen")
                _bsc_llm01 = output.get("baseline_sources_cache") or (output.get("results") or {}).get("baseline_sources_cache")
                _dbg_llm01 = _yureeka_get_debug_bucket_v1(output, default_path="analysis")
                _llm01_attach_evidence_snippets_to_pmc_v1(
                    pmc=_pmc_llm01,
                    baseline_sources_cache=_bsc_llm01,
                    metric_schema=_schema_llm01,
                    question=str(output.get("question") or ""),
                    stage="analysis",
                    out_debug=_dbg_llm01,
                )
                # Mirror onto top-level PMC if present (for compatibility)
                try:
                    if isinstance(output.get("primary_metrics_canonical"), dict) and isinstance(_pmc_llm01, dict) and _pmc_llm01:
                        output["primary_metrics_canonical"] = _pmc_llm01
                except Exception:
                    pass
            except Exception:
                pass

            with st.spinner("💾 Saving to history..."):
                if add_to_history(output):
                    st.success("✅ Analysis saved to Google Sheets")
                else:
                    st.warning("⚠️ Saved to session only (Google Sheets unavailable)")


            try:
                inj_url = ""
                try:
                    inj_url = str(INJECTED_URL or "")
                except Exception:
                    inj_url = ""
                _yureeka_attach_build_meta_v1(output, stage="analysis", injected_url=inj_url)
                _yureeka_attach_endstate_check_v1(output, stage="analysis", analysis_wrapper=output, injected_url=inj_url)
            except Exception:
                pass

            # LLM01H hotfix: ensure evidence snippet fields land in the final exported wrapper.
            try:
                _llm01_hotfix_apply_evidence_snippets_final_v1(output, stage="analysis_final", question=str(output.get("question") or query or ""))
            except Exception:
                pass

            # LLM05: attach compact LLM sidecar health snapshot (non-sensitive)
            try:
                output.setdefault("debug", {})
                if isinstance(output.get("debug"), dict):
                    output["debug"]["llm_sidecar_health_v1"] = _yureeka_llm_health_snapshot_v1(stage="analysis")
                    try:
                        if _yureeka_llm_flag_bool_v1("ENABLE_LLM_SMOKE_TEST"):
                            output["debug"]["llm_smoke_test_v1"] = _yureeka_llm_smoke_test_v1(stage="analysis")
                    except Exception:
                        pass
            except Exception:
                pass

            json_bytes = json.dumps(output, indent=2, ensure_ascii=False).encode("utf-8")
            filename = f"yureeka_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

            st.download_button(
                label="💾 Download Analysis JSON",
                data=json_bytes,
                file_name=filename,
                mime="application/json"
            )

            render_dashboard(
            primary_data,
            final_conf,
            web_context,
            base_conf,
            query,
            veracity_scores,
            web_context.get("source_reliability", []),
            wrapper_output=output,
            )

    with tab2:
        st.markdown("""
        ### 📈 Track the evolution of key metrics over time using **deterministic source-anchored analysis**.

        **How it works:**
        - Select a baseline from your history (stored in Google Sheets)
        - Re-fetches the **exact same sources** from that analysis
        - Extracts current numbers using regex (no LLM variance)
        - Computes deterministic diffs with context-aware matching
        """)

        with st.sidebar:
            st.subheader("📚 History")

            if st.button("🔄 Refresh"):
                st.cache_resource.clear()
                st.rerun()

            sheet = get_google_sheet()
            if sheet:
                st.success("✅ Google Sheets connected")
            else:
                st.warning("⚠️ Using session storage")

            # - Streamlit Cloud UI has no free-text question editing (dropdown-only).
            # - This toggle lets you intentionally bypass the unchanged fastpath so you
            #   can validate the rebuild path + FIX39 publish invariants.
            # - Pure UI flag; no logic changes unless explicitly enabled.
            force_rebuild = st.checkbox(
                "🧪 Force rebuild (ignore snapshot fastpath)",
                value=False,
                key="fix41_force_rebuild_toggle",
                help="Debug only: forces evolution to rebuild even if sources+data are unchanged."
            )

            enable_fastpath = st.checkbox(
                "⚡ Enable fastpath reuse (prod only)",
                value=False,
                key="ui_enable_fastpath_toggle",
                help="Advanced: allows reuse of unchanged evolution results to reduce fetch/load. Automatically bypassed in injection mode."
            )

            st.markdown("---")
            st.subheader("🧠 LLM Sidecar (experimental)")
            st.caption("LLM assists; deterministic rules decide. Defaults OFF to preserve REFACTOR206 behavior.")

            try:
                _ss = st.session_state
                if not isinstance(_ss.get("YUREEKA_LLM_FLAGS"), dict):
                    _ss["YUREEKA_LLM_FLAGS"] = {}
                _llm_ui_flags = _ss.get("YUREEKA_LLM_FLAGS") or {}
            except Exception:
                _llm_ui_flags = {}

            def _llm_ui_checkbox(flag: str, label: str, default: bool = False, help: str = "") -> None:
                try:
                    cur = bool(_llm_ui_flags.get(flag, default))
                    val = st.checkbox(label, value=cur, key=f"ui_llmflag_{flag}", help=help)
                    _llm_ui_flags[flag] = bool(val)
                except Exception:
                    pass

            _llm_ui_checkbox(
                "ENABLE_LLM_EVIDENCE_SNIPPETS",
                "Enable LLM evidence snippet ranking (assist only)",
                default=False,
                help="When ON, the sidecar may rank candidate evidence snippets. Metric winners/values remain deterministic."
            )



            # LLM01 evidence-snippet assist acceptance policy (UI controls; defaults preserve baseline)
            try:
                _ss = st.session_state
                if not isinstance(_ss.get("YUREEKA_LLM_POLICY"), dict):
                    _ss["YUREEKA_LLM_POLICY"] = {}
                _llm_ui_policy = _ss.get("YUREEKA_LLM_POLICY") or {}
            except Exception:
                _llm_ui_policy = {}

            def _llm_ui_slider_float(key: str, label: str, min_v: float, max_v: float, step_v: float, default_v: float, help: str = "") -> None:
                try:
                    cur = _llm_ui_policy.get(key, default_v)
                    try:
                        cur_f = float(cur)
                    except Exception:
                        cur_f = float(default_v)
                    val = st.slider(label, min_value=float(min_v), max_value=float(max_v), value=float(cur_f), step=float(step_v), key=f"ui_llmpol_{key}", help=help)
                    _llm_ui_policy[key] = float(val)
                except Exception:
                    pass

            with st.expander("Advanced LLM flags", expanded=False):


                st.caption("LLM01 evidence assist policy (controls acceptance, not metric winners).")
                _llm_ui_checkbox(
                    "LLM01_EVIDENCE_FORCE_CALL",
                    "Force LLM rank call (debug only)",
                    default=False,
                    help="When ON, the LLM rank call runs even without a deterministic tie-set. Proposal is still accepted only on ties unless confidence-gated agreement is detected."
                )
                _llm_ui_slider_float(
                    "LLM01_EVIDENCE_CONFIDENCE_THRESHOLD",
                    "LLM01 confidence threshold (accept ≥)",
                    min_v=0.0,
                    max_v=1.0,
                    step_v=0.05,
                    default_v=float(globals().get("LLM01_EVIDENCE_CONFIDENCE_THRESHOLD") or 0.75),
                    help="Higher = more conservative acceptance of the LLM proposal."
                )
                _llm_ui_slider_float(
                    "LLM01_EVIDENCE_SCORE_TIE_DELTA",
                    "LLM01 tie delta (defines ambiguity set)",
                    min_v=0.05,
                    max_v=12.0,
                    step_v=0.25,
                    default_v=float(globals().get("LLM01_EVIDENCE_SCORE_TIE_DELTA") or 2.5),
                    help="Larger = more windows treated as a tie-set; makes it easier to see LLM assist in action without changing metric values."
                )

                _llm_ui_checkbox("ENABLE_LLM_SOURCE_CLUSTERING", "Enable LLM source clustering", default=False, help="Experimental.")
                _llm_ui_checkbox("ENABLE_NLP_QUERY_FRAME", "Enable LLM query framing", default=False, help="Experimental; may change search terms.")
                _llm_ui_checkbox("ENABLE_NLP_QUERY_BOOST", "Enable deterministic NLP query boosting", default=False, help="Deterministic: appends stable keywords from query_frame (no LLM call). May change retrieval terms.")
                _llm_ui_checkbox("ENABLE_LLM_QUERY_STRUCTURE_FALLBACK", "Enable LLM query-structure fallback (legacy)", default=False, help="OFF by default. When ON, low-confidence query structure may be refined by the sidecar (confidence-gated); may change retrieval terms.")
                _llm_ui_checkbox("ENABLE_LLM_ANOMALY_FLAGS", "Enable LLM anomaly flags", default=False, help="Experimental.")
                _llm_ui_checkbox("ENABLE_LLM_DATASET_LOGGING", "Enable LLM dataset logging", default=False, help="Experimental; may write small local logs.")

            st.markdown("**Diagnostics**")
            _llm_ui_checkbox("ENABLE_LLM_SMOKE_TEST", "Run LLM smoke test (connectivity)", default=False, help="Records non-sensitive status in the JSON debug.")
            _llm_ui_checkbox("LLM_BYPASS_CACHE", "Bypass LLM cache (debug)", default=False, help="Forces live calls; turn OFF for cached/replayable runs.")

        # ✅ REFACTOR99: Evolution baseline MUST be an Analysis payload (exclude evolution reports) and default to latest.
        history_all = get_history() or []

        def _r99_is_analysis_payload(h: dict) -> bool:
            try:
                if not isinstance(h, dict):
                    return False
                # Evolution reports use analysis_type='source_anchored' at top-level
                if str(h.get("analysis_type") or "").strip().lower() in ("source_anchored", "evolution"):
                    return False
                # If it looks like an evolution report shape (minimal wrapper with results+interpretation), exclude
                if ("interpretation" in h) and ("primary_response" not in h) and (h.get("analysis_type") == "source_anchored"):
                    return False

                ms = (
                    h.get("metric_schema_frozen")
                    or (h.get("primary_response") or {}).get("metric_schema_frozen")
                    or (h.get("results") or {}).get("metric_schema_frozen")
                )
                if isinstance(ms, dict) and ms:
                    return True

                # REFACTOR103:
                # The latest Analysis row in Sheet1 may be a sheets-safe / truncated wrapper (no schema),
                # but it is still a valid baseline if it can be deterministically rehydrated from HistoryFull
                # via full_store_ref and/or _sheet_id.
                ref = (
                    h.get("full_store_ref")
                    or (h.get("results") or {}).get("full_store_ref")
                    or (h.get("primary_response") or {}).get("full_store_ref")
                )
                if isinstance(ref, str) and ref:
                    return True
                if h.get("_sheet_id"):
                    return True

                # Snapshot pointers also allow deterministic rehydration paths.
                if (
                    h.get("snapshot_store_ref")
                    or (h.get("results") or {}).get("snapshot_store_ref")
                    or h.get("source_snapshot_hash")
                    or (h.get("results") or {}).get("source_snapshot_hash")
                ):
                    return True

                return False
            except Exception:
                return False

        history = [h for h in history_all if _r99_is_analysis_payload(h)]
        # Sort newest-first so the default selection is the most recent baseline.
        def _r99_ts_key(h: dict):
            try:
                # Prefer top-level timestamp, but fall back to nested results/primary_response timestamps.
                _t = (
                    (h.get("timestamp") if isinstance(h, dict) else "")
                    or ((h.get("results") or {}).get("timestamp") if isinstance(h.get("results"), dict) else "")
                    or ((h.get("primary_response") or {}).get("timestamp") if isinstance(h.get("primary_response"), dict) else "")
                    or ""
                )
                _dt = _parse_iso_dt(_t) if _t else None
                return _dt.timestamp() if _dt else 0.0
            except Exception:
                return 0.0

        history.sort(key=_r99_ts_key, reverse=True)

        # REFACTOR104: Prefer in-session last_analysis as newest baseline when Sheets History is stale/cached.
        _r104_session_last = None
        try:
            _r104_session_last = st.session_state.get("last_analysis")
        except Exception:
            _r104_session_last = None

        def _r104_parse_ts(_obj: dict):
            try:
                if isinstance(_obj, dict):
                    _t = _obj.get("timestamp") or (_obj.get("results") or {}).get("timestamp") or ""
                    _dt = _parse_iso_dt(_t) if _t else None
                    return (_t, _dt.timestamp() if _dt else 0.0)
            except Exception:
                pass
            return ("", 0.0)

        try:
            if isinstance(_r104_session_last, dict):
                _sess_t, _sess_ts = _r104_parse_ts(_r104_session_last)
                _hist_t, _hist_ts = _r104_parse_ts(history[0]) if history else ("", 0.0)
                _sess_is_newer = bool(_sess_ts) and (_sess_ts > (_hist_ts or 0.0) + 0.5)
                if (not history) or _sess_is_newer:
                    # De-dup by timestamp to avoid doubles when Sheets has already refreshed.
                    _dedup = []
                    for _h in (history or []):
                        try:
                            _t, _ = _r104_parse_ts(_h)
                            if _t and _sess_t and _t == _sess_t:
                                continue
                        except Exception:
                            pass
                        _dedup.append(_h)
                    history = [_r104_session_last] + _dedup
                    try:
                        history.sort(key=_r99_ts_key, reverse=True)
                    except Exception:
                        pass
        except Exception:
            pass

        if not history:
            st.info("📭 No previous analyses found. Run an analysis in the 'New Analysis' tab first.")
            return

        baseline_options = [
            f"{i+1}. {h.get('question', 'N/A')}  ({h.get('timestamp', '')})"
            for i, h in enumerate(history)
        ]
        _r104_baseline_select_key = "baseline_select"
        try:
            _r104_baseline_select_key = f"baseline_select_{(history[0].get('timestamp') or '')}"
        except Exception:
            pass
        # Default to the most recent baseline (index=0 because list is newest-first).
        baseline_choice = st.selectbox("Select baseline analysis:", baseline_options, index=0, key=_r104_baseline_select_key)
        baseline_idx = int(baseline_choice.split(".")[0]) - 1
        baseline_data = history[baseline_idx]
        # REFACTOR106: Final guard against stale baseline dropdown state.
        # If user leaves the selector on default (index 0) but we have a newer Analysis payload
        # for the SAME question (in-session last_analysis or freshly-read Sheet1), use it.
        _r106_autobump_v1 = {"attempted": False, "did_autobump": False}
        try:
            _r106_autobump_v1["attempted"] = True
            def _r106_norm_q(_q):
                try:
                    return re.sub(r"\s+", " ", str(_q or "").strip().lower())
                except Exception:
                    return str(_q or "").strip().lower()
            def _r106_is_analysis_like(_obj):
                """Loose-but-safe predicate for 'can be used as an Analysis baseline'.

                Why:
                - Some Analysis payloads store primary_metrics_canonical under primary_response.
                - Some rows are sheets-safe/truncated wrappers (no schema/pmc) but are still deterministically rehydratable
                  via full_store_ref/_sheet_id/snapshot refs (HistoryFull).
                - Evolution baseline selection/autobump must treat these as valid so we don't fall back to older snapshots.
                """
                try:
                    if not isinstance(_obj, dict):
                        return False

                    # Exclude Evolution-shaped payloads.
                    if str(_obj.get("analysis_type") or "").strip().lower() in ("source_anchored", "evolution"):
                        return False

                    _q = _obj.get("question") or ((_obj.get("results") or {}).get("question") if isinstance(_obj.get("results"), dict) else "") or ""
                    _t = _obj.get("timestamp") or ((_obj.get("results") or {}).get("timestamp") if isinstance(_obj.get("results"), dict) else "") or ""
                    if not (str(_q).strip() and str(_t).strip()):
                        return False

                    _schema = (
                        _obj.get("metric_schema_frozen")
                        or ((_obj.get("primary_response") or {}).get("metric_schema_frozen") if isinstance(_obj.get("primary_response"), dict) else None)
                        or ((_obj.get("results") or {}).get("metric_schema_frozen") if isinstance(_obj.get("results"), dict) else None)
                    )
                    if isinstance(_schema, dict) and _schema:
                        return True

                    _pmc = (
                        _obj.get("primary_metrics_canonical")
                        or ((_obj.get("primary_response") or {}).get("primary_metrics_canonical") if isinstance(_obj.get("primary_response"), dict) else None)
                        or ((_obj.get("results") or {}).get("primary_metrics_canonical") if isinstance(_obj.get("results"), dict) else None)
                    )
                    if isinstance(_pmc, dict) and _pmc:
                        return True

                    # Rehydratable wrappers: treat as valid baselines (HistoryFull).
                    _ref = (
                        _obj.get("full_store_ref")
                        or ((_obj.get("results") or {}).get("full_store_ref") if isinstance(_obj.get("results"), dict) else "")
                        or ((_obj.get("primary_response") or {}).get("full_store_ref") if isinstance(_obj.get("primary_response"), dict) else "")
                        or ""
                    )
                    if isinstance(_ref, str) and _ref.strip():
                        return True
                    if _obj.get("_sheet_id"):
                        return True
                    if (
                        _obj.get("snapshot_store_ref")
                        or ((_obj.get("results") or {}).get("snapshot_store_ref") if isinstance(_obj.get("results"), dict) else "")
                        or _obj.get("source_snapshot_hash")
                        or ((_obj.get("results") or {}).get("source_snapshot_hash") if isinstance(_obj.get("results"), dict) else "")
                    ):
                        return True

                    return False
                except Exception:
                    return False
            if int(baseline_idx or 0) == 0:
                _sel_q = _r106_norm_q((baseline_data or {}).get("question") or (((baseline_data or {}).get("results") or {}).get("question")))
                _sel_ts = _parse_iso_dt((baseline_data or {}).get("timestamp") or (((baseline_data or {}).get("results") or {}).get("timestamp")))
                # 1) Prefer in-session last_analysis if it's analysis-like and newer for same question
                _sess_last = None
                try:
                    _sess_last = st.session_state.get("last_analysis")
                except Exception:
                    _sess_last = None
                if _r106_is_analysis_like(_sess_last):
                    _sess_q = _r106_norm_q(_sess_last.get("question") or ((_sess_last.get("results") or {}).get("question")))
                    _sess_ts = _parse_iso_dt(_sess_last.get("timestamp") or ((_sess_last.get("results") or {}).get("timestamp")))
                    _r106_autobump_v1.update({"selected_timestamp": str((baseline_data or {}).get("timestamp") or ""), "session_last_timestamp": str(_sess_last.get("timestamp") or "")})
                    if _sel_q and _sess_q and _sel_q == _sess_q and _sess_ts and (not _sel_ts or _sess_ts > _sel_ts):
                        baseline_data = _sess_last
                        _r106_autobump_v1["did_autobump"] = True
                        _r106_autobump_v1["reason"] = "session_last_analysis_newer_same_question_default_selection"
                # 2) If still stale, do a direct (non-cached) Sheet1 scan for the newest matching Analysis
                if not _r106_autobump_v1.get("did_autobump") and _sel_q:
                    try:
                        _sheet_direct = get_google_sheet()
                    except Exception:
                        _sheet_direct = None
                    if _sheet_direct:
                        try:
                            _rows = _sheet_direct.get_all_values() or []
                        except Exception:
                            _rows = []
                        _best = None
                        _best_ts = None
                        try:
                            for _r in (_rows[1:] if len(_rows) > 1 else []):
                                if not isinstance(_r, list) or len(_r) < 5:
                                    continue
                                _q = _r106_norm_q(_r[2] if len(_r) > 2 else "")
                                if not _q or _q != _sel_q:
                                    continue
                                _payload_raw = _r[4]
                                if not isinstance(_payload_raw, str) or not _payload_raw.strip():
                                    continue
                                try:
                                    _obj = json.loads(_payload_raw)
                                except Exception:
                                    continue
                                if not _r106_is_analysis_like(_obj):
                                    continue
                                _ts = _parse_iso_dt(_obj.get("timestamp") or ((_obj.get("results") or {}).get("timestamp")))
                                if not _ts:
                                    continue
                                if _best_ts is None or _ts > _best_ts:
                                    _best_ts = _ts
                                    _best = _obj
                            if _best and _best_ts and (not _sel_ts or _best_ts > _sel_ts):
                                baseline_data = _best
                                _r106_autobump_v1["did_autobump"] = True
                                _r106_autobump_v1["reason"] = "direct_sheet_scan_newer_same_question_default_selection"
                                _r106_autobump_v1["sheet_scan_newest_timestamp"] = str(_best.get("timestamp") or "")
                        except Exception:
                            pass
        except Exception as _e:
            try:
                _r106_autobump_v1["error"] = str(_e)
            except Exception:
                pass

        compare_method = st.selectbox(
            "Comparison method:",
            [
                "source-anchored evolution (re-fetch same sources)",
                "another saved analysis (deterministic)",
                "fresh analysis (volatile)"
            ]
        )

        extra_sources_text = st.text_area(
            "Extra source URLs (optional, one per line)",
            placeholder="https://example.com/report\nhttps://another-source.com/page",
            help="Adds these URLs to the admitted source list for this run. Useful to test hash-mismatch rebuilds.",
            height=110,
        )

        compare_data = None
        if "another saved analysis" in compare_method:
            compare_options = [
                f"{i+1}. {h.get('question', 'N/A')}  ({h.get('timestamp', '')})"
                for i, h in enumerate(history) if i != baseline_idx
            ]
            if compare_options:
                compare_choice = st.selectbox("Select comparison analysis:", compare_options)
                compare_idx = int(compare_choice.split(".")[0]) - 1
                compare_data = history[compare_idx]
            else:
                st.warning("No other saved analyses to compare with.")

        st.markdown("---")

        if st.button("🧬 Run Evolution Analysis", type="primary"):

            if "source-anchored evolution" in compare_method:
                evolution_query = baseline_data.get("question", "")
                if not evolution_query:
                    st.error("❌ No question found in baseline.")
                    return

                # LLM05: reset per-run LLM diagnostics
                try:
                    _yureeka_llm_reset_run_state_v1(stage="evolution")
                except Exception:
                    pass

                with st.spinner("🧬 Running source-anchored evolution..."):

                    try:

                        _evo_run_id = _inj_diag_make_run_id("evo")

                        _extra_urls_evo = []

                        try:

                            for _l in str(extra_sources_text or "").splitlines():

                                _u = _l.strip()

                                if not _u:

                                    continue

                                if _u.startswith("http://") or _u.startswith("https://"):

                                    _extra_urls_evo.append(_u)

                        except Exception:
                            pass

                            _extra_urls_evo = []

                        _diag_baseline_freshness_v1 = {}
                        try:
                            _b_ts = (baseline_data or {}).get("timestamp") or ((baseline_data or {}).get("results") or {}).get("timestamp") or ""
                            _s_last = None
                            try:
                                _s_last = st.session_state.get("last_analysis")
                            except Exception:
                                _s_last = None
                            _s_ts = ""
                            if isinstance(_s_last, dict):
                                _s_ts = _s_last.get("timestamp") or (_s_last.get("results") or {}).get("timestamp") or ""
                            _diag_baseline_freshness_v1 = {
                                "baseline_selected_timestamp": str(_b_ts or ""),
                                "session_last_analysis_timestamp": str(_s_ts or ""),
                                "baseline_is_session_last": bool(_b_ts and _s_ts and str(_b_ts) == str(_s_ts)),
                                "history_newest_timestamp": str((history[0].get("timestamp") if isinstance(history, list) and history and isinstance(history[0], dict) else "") or ""),
                                "baseline_autobump_v1": (_r106_autobump_v1 if isinstance(_r106_autobump_v1, dict) else {}),
                            }
                        except Exception:
                            _diag_baseline_freshness_v1 = {}

                        results = run_source_anchored_evolution(

                            baseline_data,

                            web_context={

                                "force_rebuild": bool(force_rebuild),

                                "enable_fastpath": bool(locals().get("enable_fastpath", False)),

                                "extra_urls": _extra_urls_evo,

                                "diag_run_id": _evo_run_id,

                                "diag_extra_urls_ui_raw": (extra_sources_text or ""),

                                "diag_baseline_freshness_v1": _diag_baseline_freshness_v1,

                            },

                        )

                    except Exception as e:

                        st.error(f"❌ Evolution failed: {e}")

                        return

                interpretation = ""
                try:
                    if results and isinstance(results, dict):
                        interpretation = results.get("interpretation", "") or ""
                except Exception:
                    pass
                    interpretation = ""

                # REFACTOR25: Analysis→Evolution timing delta (production only)
                # - Standardize timestamps to UTC with offset (+00:00)
                # - Compute/stamp run_timing_v1 in Evolution results
                # REFACTOR99: record which baseline payload was selected (helps detect accidental evolution-as-baseline)
                try:
                    if isinstance(results, dict):
                        _dbg = results.get("debug") if isinstance(results.get("debug"), dict) else None
                        if _dbg is None:
                            results["debug"] = {}
                            _dbg = results["debug"]
                        _pmc = _refactor89_locate_pmc_dict(baseline_data) if isinstance(baseline_data, dict) else {}
                        _ms = (baseline_data or {}).get("metric_schema_frozen") or ((baseline_data or {}).get("primary_response") or {}).get("metric_schema_frozen") or ((baseline_data or {}).get("results") or {}).get("metric_schema_frozen") or {}
                        _dbg["baseline_selector_v1"] = {
                            "selected_timestamp": (baseline_data or {}).get("timestamp"),
                            "selected_code_version": (baseline_data or {}).get("code_version"),
                            "selected_analysis_type": (baseline_data or {}).get("analysis_type"),
                            "baseline_pmc_count": int(len(_pmc) if isinstance(_pmc, dict) else 0),
                            "schema_key_count": int(len(_ms) if isinstance(_ms, dict) else 0),
                            "autobump_v1": (_r106_autobump_v1 if isinstance(_r106_autobump_v1, dict) else {}),
                        }
                except Exception:
                    pass

                # - Attach per-row delta fields; blank when current metric is injected-sourced
                _analysis_ts_raw = None
                _analysis_ts_norm = None
                _evo_ts = _yureeka_now_iso_utc()
                try:
                    # REFACTOR115: Use the effective baseline timestamp from the snapshot actually selected.
                    # Prefer Evolution output/selector fields over stale embedded baseline_data.timestamp.
                    _prev_ts_field = None
                    try:
                        _prev_ts_field = (results or {}).get("previous_timestamp")
                    except Exception:
                        _prev_ts_field = None
                    if not _prev_ts_field:
                        try:
                            _prev_ts_field = _first_present(results or {}, [
                                ["debug", "prev_snapshot_pick_v1", "selected_timestamp"],
                                ["results", "debug", "prev_snapshot_pick_v1", "selected_timestamp"],
                                ["debug", "baseline_selector_v1", "selected_timestamp"],
                                ["results", "debug", "baseline_selector_v1", "selected_timestamp"],
                            ], default=None)
                        except Exception:
                            _prev_ts_field = None

                    _raw_previous_data_timestamp = None
                    try:
                        _raw_previous_data_timestamp = (previous_data or {}).get("timestamp")
                    except Exception:
                        _raw_previous_data_timestamp = None

                    _analysis_ts_raw = _prev_ts_field or _raw_previous_data_timestamp or (baseline_data or {}).get("timestamp")
                    _dt_a = _parse_iso_dt(_analysis_ts_raw) if _analysis_ts_raw else None
                    _analysis_ts_norm = _dt_a.isoformat() if _dt_a else (_analysis_ts_raw or None)
                except Exception:
                    _analysis_ts_norm = _analysis_ts_raw or None

                _delta_seconds = None
                _delta_human = ""
                _delta_warnings = []
                try:
                    _dt_a2 = _parse_iso_dt(_analysis_ts_norm) if _analysis_ts_norm else None
                    _dt_e2 = _parse_iso_dt(_evo_ts) if _evo_ts else None
                    if _dt_a2 and _dt_e2:
                        _ds = (_dt_e2 - _dt_a2).total_seconds()
                        if _ds < 0:
                            _delta_warnings.append("delta_negative_clamped_to_zero")
                            _ds = 0.0
                        _delta_seconds = float(_ds)
                        _delta_human = _yureeka_humanize_seconds_v1(_delta_seconds)
                    else:
                        _delta_warnings.append("delta_uncomputed_missing_timestamp")
                except Exception:
                    _delta_warnings.append("delta_uncomputed_exception")
                # REFACTOR115: Do not suppress run-level Δt in injection runs.
                # Row-level suppression is handled during metric_changes hydration when a row is sourced from injected URLs.
                try:
                    _inj_probe = _refactor115_collect_injection_urls_v1(results or {}, web_context or {})
                    if isinstance(_inj_probe, list) and _inj_probe:
                        _delta_warnings.append("injection_present_row_delta_suppression_only")
                except Exception:
                    pass
# Attach run timing to Evolution results (debug + non-debug copy)
                try:
                    if isinstance(results, dict):
                        _dbg = results.get("debug")
                        if not isinstance(_dbg, dict):
                            _dbg = {}
                            results["debug"] = _dbg
                        _dbg["run_timing_v1"] = {
                            "analysis_timestamp": _analysis_ts_norm,
                            "evolution_timestamp": _evo_ts,
                            "delta_seconds": _delta_seconds,
                            "delta_human": _delta_human,
                            "warnings": list(_delta_warnings),
                        }
                        results["run_delta_seconds"] = _delta_seconds
                        results["run_delta_human"] = _delta_human
                        if isinstance(results.get("results"), dict):
                            results["results"]["run_delta_seconds"] = _delta_seconds
                            results["results"]["run_delta_human"] = _delta_human
                            _dbg2 = results["results"].get("debug")
                            if not isinstance(_dbg2, dict):
                                _dbg2 = {}
                                results["results"]["debug"] = _dbg2
                            _dbg2["run_timing_v1"] = dict(_dbg["run_timing_v1"])
                except Exception:
                    pass

                # Always record the flag truthfully (prevents false-positive harness banners)
                try:
                    _dbg_rt = results.get("debug") if isinstance(results.get("debug"), dict) else {}
                    _rt = _dbg_rt.get("run_timing_v1") if isinstance(_dbg_rt.get("run_timing_v1"), dict) else None
                    if isinstance(_rt, dict):
                        _rt["suppressed_by_injection"] = False
                        _rt["injection_present_v2"] = bool(_refactor115_collect_injection_urls_v1(results or {}, web_context or {}))
                except Exception:
                    pass

# Add per-row delta fields (production only; blank if injected)
                try:
                    # REFACTOR115: robust injection URL set (supports legacy debug fields)
                    _inj_urls = _refactor115_collect_injection_urls_v1(results or {}, web_context or {})
                    _inj_norm = None
                    try:
                        _inj_norm = _inj_diag_norm_url_list(_inj_urls)
                    except Exception:
                        _inj_norm = None
                    _inj_set = set([str(u).strip() for u in (_inj_norm or _inj_urls) if isinstance(u, str) and u.strip()])

                    def _refactor25_extract_metric_source_url(_m: dict):
                        try:
                            return _refactor26_extract_metric_source_url_v1(_m)
                        except Exception:
                            return None

                    _pmc = {}
                    if isinstance(results, dict):
                        _pmc = results.get("primary_metrics_canonical") or {}
                        if (not isinstance(_pmc, dict)) and isinstance(results.get("results"), dict):
                            _pmc = results["results"].get("primary_metrics_canonical") or {}
                    if not isinstance(_pmc, dict):
                        _pmc = {}

                                        # REFACTOR29: collect per-row gating stats for injection/production delta display
                    _row_delta_gating = {
                        "inj_set_size": len(_inj_set),
                        "inj_urls_sample": list(list(_inj_set)[:3]),
                        "rows_total": 0,
                        "injected_rows_total": 0,
                        "injected_rows_blank_delta": 0,
                        "production_rows_total": 0,
                        "production_rows_with_delta": 0,
                        "rows_with_source_url": 0,
                        "rows_missing_source_url": 0,
                        "rows_suppressed_by_injection": 0,
                        "unattributed_rows": 0,
                        "duplicate_rows_skipped": 0,
                    }

                    _seen_ck = set()

                    def _apply_delta_to_rows(_rows: list):
                        if not isinstance(_rows, list):
                            return
                        for _r in _rows:
                            if not isinstance(_r, dict):
                                continue

                            _ckey_for_count = None
                            try:
                                _ckey_for_count = _r.get("canonical_key")
                            except Exception:
                                _ckey_for_count = None

                            _uniq_key = _ckey_for_count if (isinstance(_ckey_for_count, str) and _ckey_for_count) else ("__row_%s" % (id(_r),))
                            _count_row = True
                            try:
                                if _uniq_key in _seen_ck:
                                    _count_row = False
                                    try:
                                        _row_delta_gating["duplicate_rows_skipped"] += 1
                                    except Exception:
                                        pass
                                else:
                                    _seen_ck.add(_uniq_key)
                            except Exception:
                                pass

                            if _count_row:
                                try:
                                    _row_delta_gating["rows_total"] += 1
                                except Exception:
                                    pass

                            is_injected = False
                            _ckey = _r.get("canonical_key")

                            # REFACTOR80: always count source-attribution (even when injection set is empty)
                            _su = None
                            try:
                                _su = _refactor26_extract_row_current_source_url_v1(_r)
                            except Exception:
                                _su = None
                            if _su is None:
                                _cm = _pmc.get(_ckey) if isinstance(_ckey, str) else None
                                _su = _refactor25_extract_metric_source_url(_cm) if isinstance(_cm, dict) else None

                            if _su is None:
                                # Cannot attribute -> treat as production (do not suppress)
                                if _count_row:
                                    try:
                                        _row_delta_gating["unattributed_rows"] += 1
                                        _row_delta_gating["rows_missing_source_url"] += 1
                                    except Exception:
                                        pass
                                is_injected = False
                            else:
                                if _count_row:
                                    try:
                                        _row_delta_gating["rows_with_source_url"] += 1
                                    except Exception:
                                        pass
                                if _inj_set:
                                    _su_norm = _su
                                    try:
                                        _tmp = _inj_diag_norm_url_list([_su])
                                        if isinstance(_tmp, list) and _tmp:
                                            _su_norm = str(_tmp[0] or _su).strip()
                                    except Exception:
                                        _su_norm = _su
                                    is_injected = (_su_norm in _inj_set)
                                    # REFACTOR131: injection runs should blank delta for missing_current rows even when source_url is absent.
                                    if (not is_injected) and _inj_set and (not _su_norm):
                                        try:
                                            if str((_r or {}).get("status") or "") == "missing_current":
                                                is_injected = True
                                        except Exception:
                                            pass

                            if _count_row:
                                try:
                                    if is_injected:
                                        _row_delta_gating["injected_rows_total"] += 1
                                        _row_delta_gating["rows_suppressed_by_injection"] += 1
                                    else:
                                        _row_delta_gating["production_rows_total"] += 1
                                except Exception:
                                    pass

                            if (not is_injected) and (_delta_human or _delta_seconds is not None):
                                _r["analysis_evolution_delta_human"] = _delta_human
                                _r["analysis_evolution_delta_seconds"] = _delta_seconds
                                try:
                                    _row_delta_gating["production_rows_with_delta"] += 1
                                except Exception:
                                    pass
                            else:
                                _r["analysis_evolution_delta_human"] = ""
                                _r["analysis_evolution_delta_seconds"] = None
                                if is_injected and _count_row:
                                    try:
                                        _row_delta_gating["injected_rows_blank_delta"] += 1
                                    except Exception:
                                        pass

                    # REFACTOR164: metric_changes is the single authoritative diff-row feed.
                    _primary_rows = results.get("metric_changes")
                    if isinstance(_primary_rows, list):
                        _apply_delta_to_rows(_primary_rows)

                    # Harness / invariants (soft assertions + diagnostics)
                    try:
                        if isinstance(results, dict) and isinstance(results.get("debug"), dict):
                            rt = results["debug"].get("run_timing_v1")
                            if isinstance(rt, dict):
                                rt.setdefault("assertions", {})
                                if isinstance(rt.get("assertions"), dict):
                                    if (not _inj_set) and _analysis_ts_norm:
                                        rt["assertions"]["delta_computed_non_negative"] = bool((_delta_seconds is not None) and (float(_delta_seconds) >= 0))
                                    if _inj_set:
                                        rt["assertions"]["injected_rows_have_blank_delta"] = bool(
                                            _row_delta_gating.get("injected_rows_blank_delta", 0) == _row_delta_gating.get("injected_rows_total", 0)
                                        )
                                    if (_delta_seconds is not None) and (_row_delta_gating.get("production_rows_total", 0) > 0):
                                        rt["assertions"]["production_rows_have_delta"] = bool(
                                            _row_delta_gating.get("production_rows_with_delta", 0) == _row_delta_gating.get("production_rows_total", 0)
                                        )

                        # Keep nested results copy aligned (best-effort)
                        if isinstance(results, dict) and isinstance(results.get("results"), dict):
                            _dbg_nested = results["results"].get("debug")
                            if not isinstance(_dbg_nested, dict):
                                _dbg_nested = {}
                                results["results"]["debug"] = _dbg_nested
                            if isinstance(results.get("debug"), dict) and isinstance(results["debug"].get("run_timing_v1"), dict):
                                _dbg_nested["run_timing_v1"] = dict(results["debug"]["run_timing_v1"])
                    except Exception:
                        pass
                except Exception:
                    pass

                # REFACTOR116: Post-hoc Δt + per-row Δt stamping using the *selected* baseline snapshot timestamp.
                # This corrects stale previous_data.timestamp usage when evolution rehydrates a newer baseline snapshot.
                try:
                    _rf117_wrapper = {
                        "timestamp": _evo_ts,
                        "previous_timestamp": (
                            ((results or {}).get("debug") or {}).get("prev_snapshot_pick_v1", {}).get("selected_timestamp")
                            or _analysis_ts_norm
                        ),
                        "results": results,
                    }
                    _refactor116_apply_effective_timing_and_row_deltas_v1(
                        _rf117_wrapper,
                        (previous_data if "previous_data" in locals() else baseline_data),
                        web_context if "web_context" in locals() else {},
                    )
                except Exception:
                    pass

                evolution_output = {
                    "question": evolution_query,
                    "timestamp": _evo_ts,
                    "code_version": _yureeka_get_code_version(),
                    "analysis_type": "source_anchored",
                    "previous_timestamp": (
                        ((results or {}).get("debug") or {}).get("prev_snapshot_pick_v1", {}).get("selected_timestamp")
                        or _analysis_ts_norm
                    ),
                    "results": results,
                    "interpretation": {
                        "text": interpretation,
                        "authoritative": False,
                        "source": "llm_optional"
                    }

                }

                # REFACTOR205: attach endstate_check_v1 on FINAL evolution output wrapper (post Δt stamping + timestamps)
                try:
                    inj_url = ""
                    try:
                        if isinstance(web_context, dict):
                            inj_url = str(
                                web_context.get("injected_url")
                                or web_context.get("injected_url_effective")
                                or web_context.get("injected_url_input")
                                or ""
                            )
                    except Exception:
                        inj_url = ""
                    if not str(inj_url or "").strip():
                        try:
                            inj_url = str(INJECTED_URL or "")
                        except Exception:
                            pass
                    # Surface fastpath gate into results.debug for audit (best-effort)
                    try:
                        fp_gate = None
                        try:
                            fp_gate = ((web_context or {}).get("debug") or {}).get("fastpath_gate_v2")
                        except Exception:
                            fp_gate = None
                        if isinstance(fp_gate, dict) and isinstance(results, dict):
                            results.setdefault("debug", {})
                            if isinstance(results.get("debug"), dict):
                                results["debug"]["fastpath_gate_v2"] = dict(fp_gate)
                    except Exception:
                        pass

                    _yureeka_attach_build_meta_v1(evolution_output, stage="evolution", injected_url=inj_url)
                    _yureeka_attach_endstate_check_v1(evolution_output, stage="evolution", analysis_wrapper=baseline_data, injected_url=inj_url)
                except Exception:
                    pass

                # LLM01H hotfix: ensure evidence snippet fields land in the final exported wrapper.
                try:
                    _llm01_hotfix_apply_evidence_snippets_final_v1(evolution_output, stage="evolution_final", question=str(evolution_output.get("question") or ""))
                except Exception:
                    pass

                # LLM05: attach compact LLM sidecar health snapshot (non-sensitive)
                try:
                    evolution_output.setdefault("results", {})
                    _dbg = (evolution_output.get("results", {}) or {}).get("debug")
                    if not isinstance(_dbg, dict):
                        try:
                            evolution_output["results"]["debug"] = {}
                            _dbg = evolution_output["results"]["debug"]
                        except Exception:
                            _dbg = None
                    if isinstance(_dbg, dict):
                        _dbg["llm_sidecar_health_v1"] = _yureeka_llm_health_snapshot_v1(stage="evolution")
                        try:
                            if _yureeka_llm_flag_bool_v1("ENABLE_LLM_SMOKE_TEST"):
                                _dbg["llm_smoke_test_v1"] = _yureeka_llm_smoke_test_v1(stage="evolution")
                        except Exception:
                            pass
                except Exception:
                    pass

                st.download_button(
                    label="💾 Download Evolution Report",
                    data=json.dumps(evolution_output, indent=2, ensure_ascii=False).encode("utf-8"),
                    file_name=f"yureeka_evolution_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json"
                )

                # ✅ FIX: guarded renderer to avoid stability_score=None formatting crashes
                render_source_anchored_results(results, evolution_query)

            elif "another saved analysis" in compare_method:
                if compare_data:
                    st.success("✅ Comparing two saved analyses (deterministic)")
                    render_native_comparison(baseline_data, compare_data)
                else:
                    st.error("❌ Please select a comparison analysis")

            else:
                st.warning("⚠️ Running fresh analysis - results may vary")

                query = baseline_data.get("question", "")
                if not query:
                    st.error("❌ No query found")
                    return

                with st.spinner("🌐 Fetching current data..."):
                    existing_snapshots = None

                    try:
                        prev = st.session_state.get("last_analysis")
                        if isinstance(prev, dict):
                            existing_snapshots = (
                                prev.get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("source_results")
                            )
                    except Exception:
                        pass
                        existing_snapshots = None

                    web_context = fetch_web_context(
                        _llm03_boost_web_query_v1(query, (question_signals or {}).get("query_frame_v1")),
                        num_sources=3,
                        existing_snapshots=existing_snapshots,
                    )

                if not web_context:
                    web_context = {
                        "search_results": [],
                        "scraped_content": {},
                        "summary": "",
                        "sources": [],
                        "source_reliability": []
                    }

                with st.spinner("🤖 Running analysis..."):
                    new_response = query_perplexity(query, web_context)

                if new_response:
                    try:
                        new_parsed = json.loads(new_response)
                        veracity = evidence_based_veracity(new_parsed, web_context)
                        base_conf = float(new_parsed.get("confidence", 75))
                        final_conf = calculate_final_confidence(base_conf, veracity.get("overall", 0))

                        compare_data = {
                            "question": query,
                            "timestamp": _yureeka_now_iso_utc(),
                            "primary_response": new_parsed,
                            "final_confidence": final_conf,
                            "veracity_scores": veracity,
                            "web_sources": web_context.get("sources", [])
                        }


                        add_to_history(compare_data)

                        try:
                            inj_url = ""
                            try:
                                inj_url = str(INJECTED_URL or "")
                            except Exception:
                                inj_url = ""
                            _yureeka_attach_build_meta_v1(compare_data, stage="analysis_compare", injected_url=inj_url)
                            _yureeka_attach_endstate_check_v1(compare_data, stage="analysis_compare", analysis_wrapper=compare_data, injected_url=inj_url)
                        except Exception:
                            pass
                        st.success("✅ Saved to history")

                        render_native_comparison(baseline_data, compare_data)
                    except Exception as e:
                        st.error(f"❌ Failed: {e}")
                else:
                    st.error("❌ Analysis failed")

# - Additive only: does not remove or refactor existing code.
# - Only applied in TAB 1 (New Analysis) via a small post-pass hook.
# - Does NOT alter evolution behavior (no changes to evolution functions).

def validate_metric_schema_frozen(metric_schema_frozen: dict) -> dict:
    """
    Validate frozen metric schema for internal consistency.
    Returns: {"ok": bool, "errors": [...], "warnings": [...], "by_key": {...}}
    """
    issues = {"ok": True, "errors": [], "warnings": [], "by_key": {}}

    def _add(kind: str, canonical_key: str, msg: str):
        issues["ok"] = issues["ok"] and (kind != "errors")
        issues[kind].append({"canonical_key": canonical_key, "message": msg})
        issues["by_key"].setdefault(canonical_key, {"errors": [], "warnings": []})
        issues["by_key"][canonical_key][kind].append(msg)

    if not isinstance(metric_schema_frozen, dict):
        _add("errors", "__schema__", "metric_schema_frozen missing or not a dict")
        return issues

    for canonical_key, spec in metric_schema_frozen.items():
        if not isinstance(spec, dict):
            _add("errors", canonical_key, "schema entry not a dict")
            continue

        dim = (spec.get("dimension") or spec.get("measure_kind") or "").lower().strip()
        unit = (spec.get("unit") or spec.get("unit_tag") or "").strip()
        unit_family = (spec.get("unit_family") or spec.get("unit_family_tag") or "").lower().strip()
        name = (spec.get("name") or canonical_key or "").lower()

        # Hard conflict: currency + percent
        if dim in ("currency", "revenue", "market_value", "value") and unit in ("%", "percent", "percentage"):
            _add("errors", canonical_key, "dimension=currency but unit is percent (%)")

        # Soft checks for percent metrics without percent unit
        if ("cagr" in name or dim in ("percent", "percentage", "growth_rate")) and unit and unit not in ("%", "percent", "percentage"):
            _add("warnings", canonical_key, f"percent-like metric but unit='{unit}' (expected '%')")

        # Common drift hazard: CAGR schema includes 'share'
        kw = " ".join([str(x) for x in (spec.get("keywords") or [])]).lower()
        if "cagr" in name and "share" in kw:
            _add("warnings", canonical_key, "CAGR schema keywords include 'share' (risk of mapping share% to CAGR)")

        # Unit family conflicts
        if dim == "currency" and unit_family and unit_family not in ("currency", "money"):
            _add("warnings", canonical_key, f"dimension=currency but unit_family='{unit_family}'")

    return issues

def _metric_evidence_list(metric: dict):
    ev = metric.get("evidence")
    if isinstance(ev, list):
        return ev
    return []

def _synthesize_evidence_from_examples(metric: dict, max_items: int = 5) -> list:
    """
    If metric has value_range.examples (from attribution pass), synthesize evidence records.
    This keeps JSON stable and makes evolution rebuild auditing possible.
    """
    examples = None
    vr = metric.get("value_range")
    if isinstance(vr, dict):
        examples = vr.get("examples")
    if not isinstance(examples, list) or not examples:
        return []

    out = []
    for ex in examples[:max_items]:
        if not isinstance(ex, dict):
            continue
        url = ex.get("source_url") or ex.get("url") or ""
        raw = ex.get("raw") or ""
        ctx = ex.get("context") or ex.get("context_window") or ex.get("snippet") or ""
        ah = ex.get("anchor_hash") or ""
        out.append({
            "source_url": url,
            "raw": raw,
            "context_snippet": ctx[:500] if isinstance(ctx, str) else "",
            "anchor_hash": ah,
            "method": "value_range_examples",
        })
    return out

def ensure_metric_has_evidence(metric: dict) -> dict:
    """
    Evidence gating for a single metric:
    - If evidence already exists -> no change
    - Else synthesize from value_range.examples if available
    - Else mark as proxy (do not delete or zero the metric)
    """
    if not isinstance(metric, dict):
        return metric

    ev = _metric_evidence_list(metric)
    if ev:
        return metric

    synth = _synthesize_evidence_from_examples(metric)
    if synth:
        metric["evidence"] = synth
        return metric

    # No evidence at all: mark proxy (do not alter numeric payload)
    metric.setdefault("evidence", [])
    metric["is_proxy"] = True
    metric["proxy_type"] = "evidence_missing"
    metric["proxy_reason"] = "no_evidence_anchors_available"
    metric["proxy_confidence"] = float(metric.get("proxy_confidence") or 0.2)
    return metric

def enforce_evidence_gating(primary_metrics_canonical: dict) -> dict:
    """
    Apply evidence gating across canonical metrics.
    Returns the (mutated) dict for compatibility.
    """
    if not isinstance(primary_metrics_canonical, dict):
        return primary_metrics_canonical

    for k, m in list(primary_metrics_canonical.items()):
        if isinstance(m, dict):
            primary_metrics_canonical[k] = ensure_metric_has_evidence(m)

    return primary_metrics_canonical

def apply_schema_validation_and_evidence_gating(primary_data: dict) -> dict:
    """
    New Analysis post-pass hook:
    - validates metric_schema_frozen
    - evidence-gates primary_metrics_canonical
    - marks schema-conflict metrics as proxy (does not remove anything)
    """
    if not isinstance(primary_data, dict):
        return primary_data

    # Where schema is stored
    schema = (
        primary_data.get("metric_schema_frozen")
        or (primary_data.get("primary_response") or {}).get("metric_schema_frozen")
        or (primary_data.get("results") or {}).get("metric_schema_frozen")
        or {}
    )

    validation = validate_metric_schema_frozen(schema)
    primary_response = primary_data.setdefault("primary_response", {})
    primary_response["schema_validation"] = validation

    # Mark schema-conflict metrics as proxy (additive)
    pmc = primary_data.get("primary_metrics_canonical")
    if isinstance(pmc, dict) and validation.get("by_key"):
        for ck, iss in validation["by_key"].items():
            if ck in pmc and isinstance(pmc[ck], dict) and iss.get("errors"):
                pmc[ck]["is_proxy"] = True
                pmc[ck]["proxy_type"] = "schema_conflict"
                pmc[ck]["proxy_reason"] = "schema_validation_error"
                pmc[ck]["proxy_confidence"] = float(pmc[ck].get("proxy_confidence") or 0.15)
                pmc[ck]["schema_issues"] = {"errors": iss.get("errors", []), "warnings": iss.get("warnings", [])}

    # Evidence gating
    pmc2 = primary_data.get("primary_metrics_canonical")
    if isinstance(pmc2, dict):
        before = sum(1 for v in pmc2.values() if isinstance(v, dict) and _metric_evidence_list(v))
        enforce_evidence_gating(pmc2)
        after = sum(1 for v in pmc2.values() if isinstance(v, dict) and _metric_evidence_list(v))
        prox = sum(1 for v in pmc2.values() if isinstance(v, dict) and v.get("is_proxy"))
        primary_response["evidence_gating_summary"] = {
            "total_metrics": len(pmc2),
            "metrics_with_evidence_before": before,
            "metrics_with_evidence_after": after,
            "metrics_marked_proxy": prox,
        }

    return primary_data

def _fix16_is_year_token(s: str) -> bool:
    try:
        s2 = str(s or "").strip()
        return bool(re.fullmatch(r"(19\d{2}|20\d{2})", s2))
    except Exception:
        return False

def _fix16_metric_is_year_like(metric_spec: dict, canonical_key: str = "") -> bool:
    """Deterministic allow-list for metrics whose value is genuinely a year."""
    try:
        spec = metric_spec or {}
        blob = " ".join([
            str(canonical_key or ""),
            str(spec.get("name") or ""),
            str(spec.get("canonical_key") or spec.get("canonical_id") or ""),
            " ".join([str(x) for x in (spec.get("keywords") or spec.get("keyword_hints") or []) if x]),
            str(spec.get("dimension") or ""),
        ]).lower()
        # year-ish intents
        return any(k in blob for k in (" year", "year_", "founded", "since", "established", "launch year", "model year"))
    except Exception:
        return False

def _fix16_prune_year_keywords(keywords: list, metric_is_year_like: bool) -> list:
    """Remove YYYY tokens from keyword scoring unless the metric is year-like."""
    try:
        if metric_is_year_like:
            return list(keywords or [])
        out = []
        for k in (keywords or []):
            if _fix16_is_year_token(k):
                continue
            out.append(k)
        return out
    except Exception:
        return list(keywords or [])

def _fix16_expected_dimension(metric_spec: dict) -> str:
    try:
        spec = metric_spec or {}
        dim = (spec.get("dimension") or spec.get("unit_family") or spec.get("expected_unit_family") or "").strip().lower()
        return dim
    except Exception:
        return ""

def _fix16_infer_dimension_from_canonical_key(canonical_key: str) -> str:
    """Infer an expected dimension when schema row is missing dimension/unit_family.
    Keeps REFACTOR02 behavior deterministic and blocks cross-dimension leakage.
    """
    try:
        ck = str(canonical_key or "").strip().lower()
        if not ck:
            return ""
        if "__percent" in ck or ck.endswith("_percent"):
            return "percent"
        if "__currency" in ck or ck.endswith("_currency"):
            return "currency"
        if "__unit_" in ck:
            return "magnitude"
        return ""
    except Exception:
        return ""

def _fix16_candidate_has_any_unit(c: dict) -> bool:
    try:
        if not isinstance(c, dict):
            return False
        for k in ("base_unit", "unit", "unit_tag", "unit_family"):
            if str(c.get(k) or "").strip():
                return True
        # raw sometimes carries $ or % even if unit field blank
        raw = str(c.get("raw") or "")
        if "$" in raw or "%" in raw:
            return True
        return False
    except Exception:
        return False

def _fix16_unit_compatible(c: dict, expected_dim: str) -> bool:
    """Hard gate: if schema expects a unit family/dimension, candidate must be compatible.

    Backward-compatible:
      - Some legacy call sites pass (metric_spec_dict, candidate_dict). In that case we swap.
    """
    try:
        if isinstance(expected_dim, dict) and isinstance(c, dict):
            spec_like = any(k in c for k in ("dimension", "unit_family", "expected_unit_family", "canonical_key", "name"))
            cand_like = any(k in expected_dim for k in ("raw", "value", "value_norm", "unit", "unit_tag", "unit_family", "base_unit"))
            if spec_like and cand_like:
                _candidate = expected_dim
                expected_dim = _fix16_expected_dimension(c)
                c = _candidate

        if not expected_dim:
            return True
        if not isinstance(c, dict):
            return False

        dim = str(expected_dim).strip().lower()

        # Normalize common synonyms
        if dim in ("magnitude", "count", "quantity", "units", "unit_count", "unit_sales", "number", "numbers", "volume"):
            dim = "magnitude"
        if dim in ("pct", "percentage"):
            dim = "percent"
        if dim in ("money",):
            dim = "currency"

        raw = str(c.get("raw") or "").lower()
        u = (c.get("base_unit") or c.get("unit") or c.get("unit_tag") or "").strip().lower()
        cand_fam = (c.get("unit_family") or "").strip().lower()

        def _has_percent_marker() -> bool:
            try:
                return ("%" in raw) or ("percent" in raw) or ("%" in u) or ("percent" in u)
            except Exception:
                return False

        def _has_currency_marker() -> bool:
            try:
                return (
                    ("$" in raw) or ("us$" in raw) or ("usd" in raw) or ("sgd" in raw) or ("eur" in raw) or ("gbp" in raw)
                    or ("aud" in raw) or ("cny" in raw) or ("jpy" in raw) or ("€" in raw) or ("£" in raw) or ("¥" in raw)
                    or ("$" in u) or ("usd" in u) or ("sgd" in u) or ("eur" in u) or ("gbp" in u) or ("€" in u) or ("£" in u) or ("¥" in u)
                )
            except Exception:
                return False

        if dim == "percent":
            if not _has_percent_marker():
                return False
            try:
                v = c.get("value") if c.get("value") is not None else c.get("value_norm")
                if isinstance(v, (int, float)):
                    iv = int(v)
                    if 1900 <= iv <= 2100 and abs(float(v) - float(iv)) < 1e-9:
                        # if it really is a percent, raw should contain an explicit '%'
                        if "%" not in raw:
                            return False
            except Exception:
                pass
            if cand_fam:
                return cand_fam == "percent"
            return True

        if dim == "currency":
            if not _has_currency_marker():
                return False
            if cand_fam:
                return cand_fam == "currency"
            return True

        if dim == "magnitude":
            if cand_fam in ("currency", "percent", "rate", "ratio"):
                return False
            if _has_currency_marker() or _has_percent_marker():
                return False
            return True

        requires_unit = dim in ("rate", "ratio")
        if requires_unit and not _fix16_candidate_has_any_unit(c):
            return False
        if cand_fam and dim in ("rate", "ratio"):
            return cand_fam == dim

        return True
    except Exception:
        return True

def _fix16_candidate_allowed(c: dict, metric_spec: dict, canonical_key: str = "") -> bool:
    """Compose fix15 exclusion + fix16 hard unit gate + year-token guard."""
    try:
        if not isinstance(c, dict):
            return False

        # Respect fix15 junk/year-only exclusion if present
        fn = globals().get("_candidate_disallowed_for_metric")
        if callable(fn):
            if fn(c, dict(metric_spec or {}, canonical_key=canonical_key)):
                return False
        expected_dim = _fix16_expected_dimension(metric_spec)
        if not expected_dim:
            expected_dim = _fix16_infer_dimension_from_canonical_key(canonical_key)
        if not _fix16_unit_compatible(c, expected_dim):
            return False

        # Extra deterministic guard: unitless year-like numbers should never compete
        # for non-year metrics even if upstream tagging missed them.
        if not _fix16_metric_is_year_like(metric_spec, canonical_key=canonical_key):
            v = c.get("value") if c.get("value") is not None else c.get("value_norm")
            u = (c.get("base_unit") or c.get("unit") or "").strip()
            if u == "" and isinstance(v, (int, float)):
                iv = int(v)
                if 1900 <= iv <= 2100:
                    return False

        return True
    except Exception:
        return True

def _fix2s_extract_year_from_candidate(c: dict) -> int:
    """Deterministically extract a 4-digit year from candidate fields or text."""
    try:
        if not isinstance(c, dict):
            return 0
        # Preferred explicit fields
        for k in ("year", "year_int", "target_year"):
            v = c.get(k)
            try:
                iv = int(v)
                if 1900 <= iv <= 2100:
                    return iv
            except Exception:
                pass
        # Fallback: strict regex scan of available text fields
        blob = " ".join([
            str(c.get("context_snippet") or ""),
            str(c.get("label") or ""),
            str(c.get("raw") or ""),
            str(c.get("text") or ""),
        ])
        m = re.search(r"(19\d{2}|20\d{2})", blob)
        if m:
            iv = int(m.group(1))
            if 1900 <= iv <= 2100:
                return iv
    except Exception:
        return 0

def _fix2s_apply_observed_to_canonical_rules_v1(candidates: list, metric_schema: dict, web_context=None) -> dict:
    """
    Apply deterministic mapping rules to candidate dicts.

    Returns a diagnostics dict:
      {
        "mapping_rules_version": str,
        "mapping_hits": [ {anchor_hash, target_key, year, source_url} ... ],
        "observed_rows_canonicalized_by_mapping": int,
        "mapping_misses": [ {anchor_hash, reason, source_url} ... ]   # optional
      }
    """
    diag = {
        "mapping_rules_version": "fix2s_rules_v1",
        "mapping_hits": [],
        "observed_rows_canonicalized_by_mapping": 0,
        "mapping_misses": [],
    }

    if not isinstance(candidates, list) or not candidates:
        return diag
    if not isinstance(metric_schema, dict) or not metric_schema:
        return diag

    # NOTE: target_key must already exist in Analysis' canonical namespace (schema).
    # We only promote if target_key exists in metric_schema.
    rule_table = [
        # Market share (%)
        {
            "anchor_hash": "f6cb33abc9aff96c8280223ee5f62cfe7be064f5",
            "target_key": "global_2025_market_share",
            "year": 2025,
            "kind": "percent",
            "domain_substr": "ev-volumes.com",
            "context_substr_any": ["market share", "ev share", "%", "share of"],
        },
        {
            "anchor_hash": "eedc7b779f3d87c41943da9d82d04b3f2b9a02e5",
            "target_key": "global_2026_market_share",
            "year": 2026,
            "kind": "percent",
            "domain_substr": "ev-volumes.com",
            "context_substr_any": ["market share", "ev share", "%", "share of"],
        },
        {
            "anchor_hash": "2fa90308784a5827b2e8c63b0a3992c5846e8e22",
            "target_key": "global_2030_market_share",
            "year": 2030,
            "kind": "percent",
            "domain_substr": "ev-volumes.com",
            "context_substr_any": ["market share", "ev share", "%", "share of"],
        },
        # Units sold (unit_sales)
        {
            "anchor_hash": "47da6bcf38afcc47c8b36819b69d88dffc772618",
            "target_key": "global_2024_units_sold",
            "year": 2024,
            "kind": "unit_sales",
            "domain_substr": "ev-volumes.com",
            "context_substr_any": ["ev sales", "sales", "sold", "registrations", "million"],
        },
        {
            "anchor_hash": "5679e35e4abefef7f6a3842414eceed7c3020508",
            "target_key": "global_2025_units_sold",
            "year": 2025,
            "kind": "unit_sales",
            "domain_substr": "ev-volumes.com",
            "context_substr_any": ["ev sales", "sales", "sold", "registrations", "million"],
        },
        {
            "anchor_hash": "37d6d391458630e5bbd4d34564c7490792dfbaf0",
            "target_key": "global_2040_units_sold",
            "year": 2040,
            "kind": "unit_sales",
            "domain_substr": "ev-volumes.com",
            "context_substr_any": ["ev sales", "sales", "sold", "registrations", "million"],
        },
    ]

    rules_by_anchor = {r.get("anchor_hash"): r for r in rule_table if isinstance(r, dict) and r.get("anchor_hash")}

    def _norm(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    for c in candidates:
        if not isinstance(c, dict):
            continue
        ah = str(c.get("anchor_hash") or "")
        if not ah or ah not in rules_by_anchor:
            continue

        r = rules_by_anchor.get(ah) or {}
        tgt = r.get("target_key") or ""
        if not tgt:
            continue

        # Guard: only map into existing schema keys
        if tgt not in metric_schema:
            diag["mapping_misses"].append({"anchor_hash": ah, "reason": "target_not_in_schema", "source_url": c.get("source_url")})
            continue

        url = str(c.get("source_url") or "")
        if r.get("domain_substr") and r["domain_substr"] not in url:
            diag["mapping_misses"].append({"anchor_hash": ah, "reason": "domain_mismatch", "source_url": url})
            continue

        yr = _fix2s_extract_year_from_candidate(c)
        if r.get("year") and yr != int(r["year"]):
            diag["mapping_misses"].append({"anchor_hash": ah, "reason": "year_mismatch", "source_url": url, "year": yr})
            continue

        # Exact substring allowlist (deterministic, no fuzzy matching)
        blob = " ".join([
            str(c.get("context_snippet") or ""),
            str(c.get("label") or ""),
            str(c.get("raw") or ""),
            str(c.get("text") or ""),
        ])
        blob_n = _norm(blob)
        allow = r.get("context_substr_any") or []
        if allow:
            ok = False
            for lit in allow:
                if _norm(str(lit)) and _norm(str(lit)) in blob_n:
                    ok = True
                    break
            if not ok:
                diag["mapping_misses"].append({"anchor_hash": ah, "reason": "context_mismatch", "source_url": url})
                continue

        # Deterministic unit tagging to satisfy schema/unit gates
        kind = r.get("kind")
        if kind == "percent":
            c.setdefault("unit_tag", "percent")
            if not c.get("unit"):
                c["unit"] = "%"
            c.setdefault("measure_kind", "percent")
        elif kind == "unit_sales":
            c.setdefault("unit_tag", "unit_sales")
            if not c.get("unit"):
                c["unit"] = "unit_sales"
            c.setdefault("measure_kind", "unit_sales")

        # Force competition only for this target_key (prevents cross-metric leakage)
        c["fix2s_force_canonical_key"] = tgt

        # Provide canonical key hint (selector remains authority)
        c["canonical_key"] = tgt

        diag["mapping_hits"].append({"anchor_hash": ah, "target_key": tgt, "year": yr, "source_url": url})
        diag["observed_rows_canonicalized_by_mapping"] += 1

    # Attach to web_context for downstream summary merge (additive)
    try:
        if isinstance(web_context, dict):
            web_context["_fix2s_mapping_diag"] = diag
    except Exception:
        return diag

def _ph2b_norm_url(url: str) -> str:
    return str((url or "").strip())


# - Uses LOCAL context_snippet (tight window) instead of page-wide context_window.
# - Prevents cross-metric pollution in schema-only rebuild paths.
# - Applied in BOTH Analysis selector and Evolution schema-only rebuild(s).

def _fix2d2u_norm_text(s: str) -> str:
    try:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()
    except Exception:
        return ""

def _fix2d63_is_yearlike_value(cand: dict) -> bool:
    try:
        v = cand.get('value_norm')
        if v is None:
            try:
                v = float(cand.get('value') or 0.0)
            except Exception:
                pass
                v = None
        if v is None:
            return False
        iv = int(float(v))
        # Conservative year window
        return (1900 <= iv <= 2100) and abs(float(v) - float(iv)) < 1e-9
    except Exception:
        return False

def _fix2d63_has_unit_evidence(cand: dict) -> bool:
    try:
        ut = str(cand.get('unit_tag') or cand.get('unit') or '').strip()
        uf = str(cand.get('unit_family') or '').strip().lower()
        mk = str(cand.get('measure_kind') or '').strip().lower()
        ma = str(cand.get('measure_assoc') or '').strip().lower()
        if ut:
            return True
        if uf in ('magnitude', 'percent', 'currency', 'energy', 'index'):
            return True
        if mk in ('count_units', 'count', 'quantity'):
            return True
        if ma in ('units', 'unit_sales', 'sales'):
            return True
        ctx = (str(cand.get('context_snippet') or cand.get('context') or cand.get('context_window') or '') + ' ' + str(cand.get('raw') or '')).lower()
        # Contextual unit hints
        if any(w in ctx for w in ['million', 'billion', 'thousand', 'trillion', 'units', 'unit', 'vehicles', '%', 'percent', 'usd', 'sgd', 'eur', '$', '€', '£', '¥']):
            return True
        return False
    except Exception:
        return False

def _fix2d63_schema_expects_unit_or_count(canonical_key: str, spec: dict) -> bool:
    try:
        ck = str(canonical_key or '')
        dim = str((spec or {}).get('dimension') or '').strip().lower()
        uf = str((spec or {}).get('unit_family') or '').strip().lower()
        # Suffix-based (most reliable given existing schema patterns)
        if ck.endswith('__unit_sales') or ck.endswith('__units') or ck.endswith('__unit'):
            return True
        # Schema hints
        if dim in ('unit_sales', 'count', 'quantity'):
            return True
        if uf == 'magnitude' and dim:
            # magnitude metrics are generally non-year values unless explicitly year metrics
            return True
        return False
    except Exception:
        return False

def _analysis_canonical_final_selector_v1(
    canonical_key: str,
    schema_frozen: dict,
    candidates: list,
    anchors: dict = None,
    prev_metric: dict = None,
    web_context: dict = None,
) -> tuple:
    """Pure selector: returns (best_metric_or_None, meta_dict)."""

    meta = {
        "selector_used": "analysis_canonical_v1",
        "canonical_key": canonical_key or "",
        "anchor_used": False,
        "blocked_reason": "",
        "preferred_url": "",
        "chosen_source_url": "",
        "tie_break": "",
        "eligible_count": 0,
        "range_method": "",
    }

    spec = schema_frozen or {}
    if not isinstance(spec, dict) or not spec:
        meta["blocked_reason"] = "missing_schema"
        return None, meta

    # Determine preferred URL from anchors (strongest) or schema if present
    preferred_url = ""
    anchor = None
    if isinstance(anchors, dict) and canonical_key in anchors and isinstance(anchors.get(canonical_key), dict):
        anchor = anchors.get(canonical_key) or {}
        preferred_url = anchor.get("source_url") or ""
    preferred_url = preferred_url or (spec.get("preferred_url") or spec.get("source_url") or "")
    if preferred_url:
        meta["preferred_url"] = _ph2b_norm_url(preferred_url)

    # Prepare FIX16 keyword scoring (same as rebuild_metrics_from_snapshots_schema_only_fix16)
    def _norm(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def _cand_sort_key(c: dict):
        try:
            return (
                str(c.get("anchor_hash") or ""),
                str(c.get("source_url") or ""),
                int(c.get("start_idx") or 0),
                str(c.get("raw") or ""),
                str(c.get("unit") or ""),
                float(c.get("value_norm") or 0.0),
            )
        except Exception:
            return ("", "", 0, "", "", 0.0)

    # FIX16 keyword pruning helper if present; otherwise keep keywords as-is
    metric_is_year_like = False
    try:
        fn_year = globals().get("_fix16_metric_is_year_like")
        if callable(fn_year):
            metric_is_year_like = bool(fn_year(spec, canonical_key=canonical_key))
    except Exception:
        pass
        metric_is_year_like = False

    keywords = spec.get("keywords") or spec.get("keyword_hints") or []
    if isinstance(keywords, str):
        keywords = [keywords]
    try:
        fn_prune = globals().get("_fix16_prune_year_keywords")
        if callable(fn_prune):
            keywords = fn_prune(list(keywords), metric_is_year_like)
    except Exception:
        pass
        keywords = list(keywords) if isinstance(keywords, list) else []


    # REFACTOR117: For charging investment-by-2040 schema key, accept common synonyms used in sources
    # (e.g., 'spend', 'spending', 'market', 'total investments') without changing the frozen schema.
    try:
        if str(canonical_key or "") == "global_ev_charging_investment_2040__currency":
            _extra_kw = [
                "investment", "investments", "total investments",
                "spend", "spending", "annual spend", "annual spending",
                "infrastructure spend", "infrastructure spending",
                "market", "market size", "capex",
            ]
            if not isinstance(keywords, list):
                keywords = list(keywords) if keywords is not None else []
            for _k in _extra_kw:
                if _k and _k not in keywords:
                    keywords.append(_k)
    except Exception:
        pass

    kw_norm = [_norm(k) for k in (keywords or []) if k]

    # Candidate filtering
    cands = [c for c in (candidates or []) if isinstance(c, dict)]
    try:
        meta["candidate_count_in"] = int(len(cands))
    except Exception:
        pass
    # Enforce preferred source lock when available (prevents cross-source hijack)
    if meta["preferred_url"]:
        pref = meta["preferred_url"]
        cands_pref = []
        for c in cands:
            cu = _ph2b_norm_url(c.get("source_url") or "")
            if cu and cu == pref:
                cands_pref.append(c)
        # If preferred exists but yields zero candidates, we keep empty (hard lock).
        cands = cands_pref
        try:
            meta["candidate_count_pref"] = int(len(cands))
        except Exception:
            pass

    eligible = []
    for c in cands:
        try:
            # Many snapshot candidates omit unit_family even when unit_tag/raw clearly indicates
            # magnitude/percent/currency. The analysis selector treats unit_family as authoritative
            # for schema gating; leaving it blank causes false ineligibility (empty Current).
            try:
                if isinstance(c, dict) and not str(c.get("unit_family") or "").strip():
                    _raw = str(c.get("raw") or "")
                    _ut = str(c.get("unit_tag") or c.get("unit") or "")
                    _ctx = str(c.get("context_snippet") or "")
                    _blob = (" ".join([_raw, _ut, _ctx])).lower()
                    uf = ""
                    if "%" in _blob or "percent" in _blob or "percentage" in _blob:
                        uf = "percent"
                    elif any(tok in _blob for tok in ["usd", "sgd", "eur", "gbp", "$", "€", "£", "¥", "aud", "cad", "inr", "cny", "rmb"]):
                        uf = "currency"
                    else:
                        # Magnitude / counts (incl. unit sales)
                        if any(w in _blob for w in ["million", "billion", "thousand", "trillion"]) or re.search(r"[mbkt]", _blob):
                            uf = "magnitude"
                        elif str(c.get("measure_kind") or "").lower() in ("count_units", "count", "quantity"):
                            uf = "magnitude"
                        elif str(c.get("measure_assoc") or "").lower() in ("units", "unit_sales", "sales"):
                            uf = "magnitude"
                    if uf:
                        c["unit_family"] = uf
                        # unit_cmp hint (best-effort; used only for display/debug)
                        if not str(c.get("unit_cmp") or "").strip():
                            if uf == "percent":
                                c["unit_cmp"] = "%"
                            elif uf == "currency":
                                c["unit_cmp"] = "currency"
                            else:
                                c["unit_cmp"] = (_ut or "").strip()
            except Exception:
                pass

            if not _fix16_candidate_allowed(c, spec, canonical_key=canonical_key):
                continue

            # FIX2D2R: forbid bare-year tokens when a better sibling exists nearby (parity guard)
            try:
                _ek = 'other'
                _ed = (expected_dim or '').lower().strip()
                if canonical_key.endswith('__percent') or _ed == 'percent':
                    _ek = 'percent'
                elif canonical_key.endswith('__currency') or _ed == 'currency':
                    _ek = 'currency'
                elif canonical_key.endswith('__unit_sales') or 'unit' in canonical_key.lower() or 'sales' in canonical_key.lower():
                    _ek = 'unit'
                elif canonical_key.endswith('__year') or 'year' in _ed:
                    _ek = 'year'

                if _ek != 'year' and _fix2d2r_is_bare_year_cand(c) and not str(c.get('unit') or c.get('unit_tag') or '').strip():
                    if _fix2d2r_has_better_sibling(c, candidates, _ek):
                        continue
            except Exception:
                pass

            # - Rejects percent/currency evidence when schema expects magnitude (prevents % hijacks).
            # - Suppresses unitless bare years (e.g., 2030) as candidates.
            # - Enforces unit evidence for scaled magnitude schemas (million/billion/etc.).
            try:
                _raw0 = str(c.get("raw") or "").strip()
                _raw0_l = _raw0.lower()
                _v0 = c.get("value_norm", None)
                if _v0 is None:
                    _v0 = c.get("value", None)

                _cand_family = str(c.get("unit_family") or "").strip().lower()
                _cand_ucmp = str(c.get("unit_cmp") or c.get("unit_tag") or "").strip().lower()
                _spec_family = str(spec.get("unit_family") or "").strip().lower()
                # _spec_family is schema-declared; legacy FIX17 inference removed (dead).
                _spec_ut = str(spec.get("unit_tag") or spec.get("unit") or "").strip().lower()

                # evidence flags
                _has_pct = ("%" in _raw0) or ("percent" in _raw0_l) or ("%" in _cand_ucmp) or (_cand_family == "percent")
                _has_ccy = any(sym in _raw0 for sym in ("$", "€", "£", "¥")) or any(tok in _raw0_l for tok in ("usd", "eur", "sgd", "gbp", "jpy")) or (_cand_family == "currency")

                _has_unit_ev0 = False
                try:
                    for _k in ("base_unit", "unit", "unit_tag", "unit_family"):
                        if str(c.get(_k) or "").strip():
                            _has_unit_ev0 = True
                            break
                    if not _has_unit_ev0:
                        if any(tok in _raw0_l for tok in ("million", "billion", "trillion", "mn", "bn", "thousand")):
                            _has_unit_ev0 = True
                        if _has_pct or _has_ccy:
                            _has_unit_ev0 = True
                except Exception:
                    pass

                # unit-family hard gating (schema-driven)
                try:
                    if _spec_family == "percent":
                        if not _has_pct:
                            meta["blocked_reason"] = "percent_evidence_missing_hard_block"
                            continue
                    elif _spec_family == "currency":
                        if not _has_ccy:
                            meta["blocked_reason"] = "currency_evidence_missing_hard_block"
                            continue
                    elif _spec_family == "magnitude":
                        # reject percent/currency candidates outright
                        if _has_pct or _has_ccy:
                            meta["blocked_reason"] = "unit_mismatch_hard_block"
                            continue
                except Exception:
                    pass

                # year-only candidate suppression (strict): only when unit evidence is missing
                try:
                    if (not _has_unit_ev0) and isinstance(_v0, (int, float)):
                        _iv0 = int(float(_v0))
                        if 1900 <= _iv0 <= 2100:
                            if re.fullmatch(r"(19\d{2}|20\d{2})", _raw0 or str(_iv0)):
                                continue
                except Exception:
                    pass

                # scaled magnitude requires unit evidence (schema implies million/billion/etc.)
                try:
                    _spec_nm = str(spec.get("name") or "").lower()
                    _scaled = any(t in _spec_ut for t in ("million", "billion", "trillion", "thousand")) or (_spec_ut in ("m", "b", "t", "k"))
                    _countish = any(t in _spec_nm for t in ("unit", "units", "sales", "deliveries", "shipments", "registrations", "volume"))
                    if _spec_family == "magnitude" and _scaled and _countish and (not _has_unit_ev0):
                        meta["blocked_reason"] = "unit_evidence_missing_hard_block"
                        continue
                except Exception:
                    pass

                # Purpose: Fix broken scale/countish gating where earlier patch strings were truncated
                #          (e.g., "milli..." / "uni...") and therefore never matched.
                #          Enforce: if schema implies scaled magnitude (million/billion/etc.), unit evidence
                #          must exist in the candidate (unit_tag/unit_cmp/raw/context), else hard-block.
                # Safety: additive-only; does not change fastpath/hashing/injection/snapshot attach.
                try:
                    _spec_nm2 = str(spec.get("name") or spec.get("label") or "").lower()
                    _spec_ut2 = str(_spec_ut or "").lower()

                    def _ph2b_has_scale_token(_s: str) -> bool:
                        try:
                            _s = str(_s or "").lower()
                        except Exception:
                            pass
                            _s = ""
                        if not _s:
                            return False
                        toks = [
                            "million", "mn", "m", "millions",
                            "billion", "bn", "b", "billions",
                            "trillion", "tn", "t", "trillions",
                            "thousand", "k", "000",
                        ]
                        # require word-boundary-ish for single-letter tokens
                        if "million" in _s or "millions" in _s or "billion" in _s or "billions" in _s or "trillion" in _s or "trillions" in _s or "thousand" in _s:
                            return True
                        if re.search(r"\b(mn|bn|tn|k)\b", _s):
                            return True
                        # "m" / "b" / "t" are too ambiguous; only accept when adjacent to "usd/units/sales" etc.
                        if re.search(r"\b(m|b|t)\b", _s) and re.search(r"(units?|sales|deliveries|shipments|vehicles|usd|eur|sgd|gbp|jpy|cny|aud|cad)", _s):
                            return True
                        return False

                    def _ph2b_has_unit_evidence_candidate(_c: dict) -> bool:
                        if not isinstance(_c, dict):
                            return False
                        if str(_c.get("unit_tag") or "").strip():
                            return True
                        if str(_c.get("unit_cmp") or "").strip():
                            return True
                        if str(_c.get("unit_family") or "").strip():
                            return True
                        # raw/context tokens
                        if _ph2b_has_scale_token(_c.get("raw")):
                            return True
                        if _ph2b_has_scale_token(_c.get("context_snippet") or _c.get("context")):
                            return True
                        return False

                    _schema_scaled = _ph2b_has_scale_token(_spec_ut2)
                    # If schema is scaled, we hard-require candidate unit evidence regardless of name.
                    if _spec_family == "magnitude" and _schema_scaled and (not _ph2b_has_unit_evidence_candidate(c0)):
                        meta["blocked_reason"] = "unit_evidence_missing_hard_block"
                        continue

                    # Extra guard: suppress obvious document-structure numbers (pages/figures) when schema is scaled.
                    try:
                        if _spec_family == "magnitude" and _schema_scaled:
                            _ctx = str((c0 or {}).get("context_snippet") or (c0 or {}).get("context") or "").lower()
                            if ("pages" in _ctx) or ("figures" in _ctx) or ("page " in _ctx):
                                # treat as ineligible rather than junking globally
                                continue
                    except Exception:
                        pass
                except Exception:
                    pass

                # Why:
                # - Earlier gating treated inferred unit_family='magnitude' as "unit evidence", allowing unitless
                #   integers (e.g., 170) to pass for schemas like "million units".
                # - For scaled schemas, we require explicit scale evidence in raw/unit_tag/unit_cmp (million/billion/etc.).
                # Safety:
                # - Only applies when schema implies a scale (million/billion/thousand/trillion or M/B/K/T).
                # - Does NOT change fastpath/hashing/injection/snapshot attach.
                try:
                    _scaled2 = False
                    try:
                        _scaled2 = any(t in _spec_ut for t in ("million", "billion", "trillion", "thousand")) or (_spec_ut in ("m", "b", "t", "k"))
                    except Exception:
                        pass
                        _scaled2 = False
                    if _spec_family == "magnitude" and _scaled2:
                        _blob = (" ".join([
                            str(c.get("raw") or ""),
                            str(c.get("unit_cmp") or ""),
                            str(c.get("unit_tag") or c.get("unit") or ""),
                            str(c.get("context_snippet") or c.get("context") or ""),
                        ])).lower()
                        _has_scale_ev = any(tok in _blob for tok in ("million", "billion", "trillion", "thousand", "mn", "bn")) or bool(re.search(r"\b[mbkt]\b", _blob))
                        # For scaled schemas, require scale to *match* the schema (e.g., million vs billion).
                        # Prevents wrong-scale candidates from surviving for 'million units' schemas.
                        try:
                            _schema_scale = ""
                            if "million" in _spec_ut or _spec_ut == "m":
                                _schema_scale = "m"
                            elif "billion" in _spec_ut or _spec_ut == "b":
                                _schema_scale = "b"
                            elif "thousand" in _spec_ut or _spec_ut == "k":
                                _schema_scale = "k"
                            elif "trillion" in _spec_ut or _spec_ut == "t":
                                _schema_scale = "t"

                            _cand_scale = ""
                            if any(t in _blob for t in ("million", "mn")) or bool(re.search(r"m", _blob)):
                                _cand_scale = "m"
                            elif any(t in _blob for t in ("billion", "bn")) or bool(re.search(r"b", _blob)):
                                _cand_scale = "b"
                            elif "thousand" in _blob or bool(re.search(r"k", _blob)):
                                _cand_scale = "k"
                            elif "trillion" in _blob or bool(re.search(r"t", _blob)):
                                _cand_scale = "t"

                            if _schema_scale and _cand_scale and _schema_scale != _cand_scale:
                                meta["blocked_reason"] = "scale_mismatch_hard_block"
                                continue
                        except Exception:
                            pass
                        if not _has_scale_ev:
                            meta["blocked_reason"] = "scale_evidence_missing_hard_block"
                            continue
                except Exception:
                    pass
            except Exception:
                pass
        except Exception:
            pass
            continue
        # Prevents year tokens like 2030 from being committed as metric values for non-year metrics,
        # and prevents unrelated snippets (e.g., "By 2030 ... sales ...") from satisfying chargers/investment metrics.
        try:
            _fix2d17_spec = spec if isinstance(spec, dict) else {}
            _fix2d17_ckey = str(canonical_key or "")
            _fix2d17_dim = str(_fix2d17_spec.get("dimension") or "")
            _fix2d17_unitfam = str(_fix2d17_spec.get("unit_family") or "")
            _fix2d17_ut = str(_fix2d17_spec.get("unit_tag") or _fix2d17_spec.get("unit") or "")
            _fix2d17_year_metric = (
                ("year" in _fix2d17_dim.lower())
                or ("year" in _fix2d17_unitfam.lower())
                or (_fix2d17_ut.lower() in ["year", "yr", "years"])
                or (re.search(r"(?:^|_)(?:19|20)\d{2}(?:_|$)", _fix2d17_ckey) is not None)
            )

            _raw = str(c.get("raw") or "")
            try:
                _vf = float(c.get("value_norm") or 0.0)
            except Exception:
                pass
                _vf = 0.0
            _raw_digits = re.sub(r"[^0-9]", "", _raw or "")
            _looks_year = (1900.0 <= _vf <= 2100.0) and (len(_raw_digits) == 4 and _raw_digits == str(int(_vf)))

            _cand_unit = str(c.get("unit") or c.get("unit_tag") or "").strip()
            _cand_uf = str(c.get("unit_family") or "").strip().lower()

            if _looks_year and not _fix2d17_year_metric:
                # Allow only if this "year" token is clearly attached to a real unit (rare); otherwise reject.
                if (not _cand_unit) and (_cand_uf in ["", "unknown", "none"]):
                    meta["fix2d17_reject_bare_year"] = int(meta.get("fix2d17_reject_bare_year") or 0) + 1
                    continue

            # Domain keyword overlap enforcement for certain metrics to prevent cross-metric pollution.
            ctx_blob = (" " + str(c.get("context_snippet") or c.get("context") or "") + " " + _raw + " ").lower()
            # Normalize to token-ish spaces
            ctx_tok = " " + re.sub(r"[^a-z0-9]+", " ", ctx_blob) + " "

            domain_tokens = {
                "charger", "chargers", "charging", "station", "stations", "infrastructure",
                "investment", "invest", "capex", "spend", "spending",
                "sales", "sold", "market", "share", "revenue", "turnover",
                "units", "deliveries", "registrations",
            }

            kw = _fix2d17_spec.get("keywords") or []
            if isinstance(kw, (list, tuple)):
                kw_l = [str(x).lower().strip() for x in kw if str(x).strip()]
            else:
                kw_l = []
            required = [k for k in kw_l if k in domain_tokens]

            # If schema explicitly includes domain tokens, require at least one to appear in candidate context/raw.
            if required:
                if not any((" " + t + " ") in ctx_tok for t in required):
                    meta["fix2d17_reject_domain_mismatch"] = int(meta.get("fix2d17_reject_domain_mismatch") or 0) + 1
                    continue
        except Exception:
            pass

        eligible.append(c)

    meta["eligible_count"] = int(len(eligible) or 0)
    try:
        meta["candidate_count_eligible"] = int(len(eligible))
    except Exception:
        pass

    if not eligible:
        meta["blocked_reason"] = "no_eligible_candidates_in_preferred_source" if meta["preferred_url"] else "no_eligible_candidates"
        return None, meta

    # Deterministic scoring: keyword hits + stable tie-break
    best = None
    best_tie = None
    for c in sorted(eligible, key=_cand_sort_key):
        ctx = _norm(c.get("context_snippet") or c.get("context") or c.get("context_window") or "")
        raw = _norm(c.get("raw") or "")
        hits = 0
        for k in kw_norm:
            if not k:
                continue
            if k in ctx:
                hits += 2
            if k in raw:
                hits += 1

        # Prefer anchored candidate_id/anchor_hash when anchors exist
        anchor_bonus = 0
        if isinstance(anchor, dict) and anchor:
            ah = str(anchor.get("anchor_hash") or "")
            cid = str(anchor.get("candidate_id") or "")
            if ah and str(c.get("anchor_hash") or "") == ah:
                anchor_bonus += 10
            if cid and str(c.get("candidate_id") or "") == cid:
                anchor_bonus += 10

        score = hits + anchor_bonus

        # Tie-break: higher score, then earlier occurrence, then stable sort key
        tie = (int(score), -int(c.get("start_idx") or 0), _cand_sort_key(c))
        if best is None or tie > best_tie:
            best = c
            best_tie = tie

    if best is None:
        meta["blocked_reason"] = "no_winner_after_scoring"
        return None, meta

    try:
        _raw = str(best.get("raw") or "")
        try:
            _vf = float(best.get("value_norm") or 0.0)
        except Exception:
            pass
            _vf = 0.0
        _raw_digits = re.sub(r"[^0-9]", "", _raw or "")
        _looks_year = (1900.0 <= _vf <= 2100.0) and (len(_raw_digits) == 4 and _raw_digits == str(int(_vf)))
        _dim = str(spec.get("dimension") or "")
        _uf = str(spec.get("unit_family") or "")
        _ut = str(spec.get("unit_tag") or spec.get("unit") or "")
        _year_metric = (
            ("year" in _dim.lower()) or ("year" in _uf.lower()) or (_ut.lower() in ["year", "yr", "years"])
            or (re.search(r"(?:^|_)(?:19|20)\\d{2}(?:_|$)", str(canonical_key or "")) is not None)
        )
        _cand_unit = str(best.get("unit") or best.get("unit_tag") or "").strip()
        _cand_uf = str(best.get("unit_family") or "").strip().lower()
        if _looks_year and (not _year_metric) and (not _cand_unit) and (_cand_uf in ["", "unknown", "none"]):
            meta["blocked_reason"] = "fix2d17_reject_bare_year_best"
            meta["fix2d17_bare_year_best"] = True
            return None, meta
    except Exception:
        pass

    # Build value_range in schema units (NO double divide)
    try:
        vals = []
        for c in eligible:
            v = c.get("value_norm")
            if v is None:
                continue
            try:
                vals.append(float(v))
            except Exception:
                pass
        if len(vals) >= 2:
            vmin = min(vals); vmax = max(vals)
            meta["value_range"] = {"min": vmin, "max": vmax, "n": len(vals), "method": "ph2b_schema_unit_range_v2|fix2b_range4"}
            meta["range_method"] = "ph2b_schema_unit_range_v2|fix2b_range4"
    except Exception:
        pass

    out = {
        "name": spec.get("name") or spec.get("label") or canonical_key,
        "canonical_key": canonical_key,
        "value": best.get("value"),
        "unit": best.get("unit") or spec.get("unit_tag") or "",
        "value_norm": best.get("value_norm"),
        "source_url": best.get("source_url") or "",
        "anchor_hash": best.get("anchor_hash") or "",
        "candidate_id": best.get("candidate_id") or "",
        "context_snippet": best.get("context_snippet") or best.get("context") or "",
        "anchor_used": bool(isinstance(anchor, dict) and anchor and (
            (anchor.get("anchor_hash") and str(best.get("anchor_hash") or "") == str(anchor.get("anchor_hash")))
            or (anchor.get("candidate_id") and str(best.get("candidate_id") or "") == str(anchor.get("candidate_id")))
        )),
        "evidence": [{
            "source_url": best.get("source_url") or "",
            "raw": best.get("raw") or "",
            "context_snippet": (best.get("context_snippet") or best.get("context") or best.get("context_window") or "")[:400],
            "anchor_hash": best.get("anchor_hash") or "",
            "method": "analysis_canonical_selector_v1",
        }],
    }

    meta["anchor_used"] = bool(out.get("anchor_used"))
    meta["chosen_source_url"] = _ph2b_norm_url(out.get("source_url") or "")

    # - Adds winner_candidate_debug + would_block_reason for scaled schemas.
    try:
        _winner = out if isinstance(out, dict) else {}
        meta["winner_candidate_debug"] = {
        "canonical_key": str(canonical_key),
        "source_url": str(_winner.get("source_url") or ""),
        "source_url_norm": _ph2b_norm_url(_winner.get("source_url") or ""),
        "candidate_id": str(_winner.get("candidate_id") or _winner.get("id") or _winner.get("anchor_hash") or ""),
        "value_norm": _winner.get("value_norm"),
        "raw": str(_winner.get("raw") or _winner.get("value") or ""),
        "unit_cmp": str(_winner.get("unit_cmp") or ""),
        "unit_family": str(_winner.get("unit_family") or ""),
        "unit_tag": str(_winner.get("unit_tag") or ""),
        "context_snippet": str(_winner.get("context_snippet") or _winner.get("context") or ""),
        }

    # "Would block" diagnostic: scaled magnitude schema but chosen candidate lacks unit evidence.
        _spec_unit = str((schema_frozen or {}).get("unit_tag") or (schema_frozen or {}).get("unit") or "")
        _spec_unit_l = _spec_unit.lower()
        _is_scaled = any(tok in _spec_unit_l for tok in ["million", "billion", "trillion", "thousand", "mn", "bn", "m ", "b ", "k "]) or (_spec_unit.strip() in ["M", "B", "K", "T"])
        _winner_unit_cmp = str(_winner.get("unit_cmp") or "")
        _winner_unit_tag = str(_winner.get("unit_tag") or "")
        _winner_raw = str(_winner.get("raw") or _winner.get("value") or "")
        _winner_has_scale = any(tok in (_winner_raw.lower()) for tok in ["million", "billion", "trillion", "thousand"]) or (_winner_unit_tag.strip().upper() in ["M", "B", "K", "T"]) or (_winner_unit_cmp.strip().upper() in ["M", "B", "K", "T", "%"])
        if _is_scaled and not _winner_has_scale and not _winner_unit_cmp:
            meta["would_block_reason"] = "unit_evidence_missing_for_scaled_schema"
        else:
            meta["would_block_reason"] = ""
    except Exception:
        pass

    # - Does NOT change selection; purely diagnostic.
    try:
        meta["analysis_selector_trace_v1"] = {
            "selector_used": meta.get("selector_used"),
            "preferred_url": meta.get("preferred_url"),
            "chosen_source_url": meta.get("chosen_source_url"),
            "n_candidates_in": int(meta.get("candidate_count_in") or 0),
            "n_candidates_pref": int(meta.get("candidate_count_pref") or 0),
            "n_candidates_eligible": int(meta.get("candidate_count_eligible") or 0),
            "blocked_reason": meta.get("blocked_reason") or "",
            "anchor_used": bool(meta.get("anchor_used")),
            "would_block_reason": meta.get("would_block_reason") or "",
            "winner_candidate_debug": dict(meta.get("winner_candidate_debug") or {}) if isinstance(meta.get("winner_candidate_debug"), dict) else {},
        }
    except Exception:
        return out, meta


# =============================================================================
# LLM38: FRESH02 tie-key helper (module-level fallback)
#
# Why: rebuild_metrics_from_snapshots_analysis_canonical_v1 uses _fresh02_candidate_tie_key_v1
# for deterministic freshness tie-breaking when ENABLE_SOURCE_FRESHNESS_TIEBREAK is enabled.
# In LLM37, the only implementation lived inside the schema-only rebuild, causing a NameError
# when the analysis-canonical rebuild path executed (e.g., FIX41AFC19 in injection evolution).
#
# Design: cache-free, deterministic, best-effort. Uses candidate-attached freshness fields
# (freshness_score / freshness_age_days / freshness_date_confidence) and falls back to
# age->score curve when score is missing. Returns an empty tuple when tie-break is disabled.
# =============================================================================
def _fresh02_candidate_tie_key_v1(c: dict) -> tuple:
    """Return a tuple suitable for lexicographic ascending sort (smaller is better)."""
    try:
        # Only active when the explicit flag is enabled.
        enabled = False
        try:
            fn = globals().get("_yureeka_llm_flag_effective_v1")
            if callable(fn):
                enabled = bool(fn("ENABLE_SOURCE_FRESHNESS_TIEBREAK")[0])
            else:
                enabled = bool(globals().get("ENABLE_SOURCE_FRESHNESS_TIEBREAK"))
        except Exception:
            enabled = bool(globals().get("ENABLE_SOURCE_FRESHNESS_TIEBREAK"))
        if not enabled or not isinstance(c, dict):
            return tuple()

        sc = c.get("freshness_score")
        if sc is None:
            sc = c.get("source_freshness_score")
        age = c.get("freshness_age_days")
        if age is None:
            age = c.get("source_freshness_age_days")
        conf = c.get("freshness_date_confidence")
        if conf is None:
            conf = c.get("source_freshness_date_confidence")

        sc_f = None
        try:
            sc_f = float(sc)
        except Exception:
            sc_f = None

        age_i = None
        try:
            age_i = int(float(age)) if age is not None else None
        except Exception:
            age_i = None

        # Compute score from age+confidence when needed.
        if sc_f is None and age_i is not None:
            base = None
            try:
                fn_base = globals().get("_fresh01_score_from_age_days_v1")
                if callable(fn_base):
                    base = fn_base(age_i)
            except Exception:
                base = None
            if base is not None:
                cconf = 1.0
                try:
                    cconf = float(conf) if conf is not None else 1.0
                except Exception:
                    cconf = 1.0
                cconf = max(0.0, min(1.0, cconf))
                try:
                    sc_f = float(base) * cconf
                except Exception:
                    sc_f = None

        # Optional quantization: treat near-equal freshness scores as equal.
        sc_eff = sc_f
        try:
            md = float(globals().get("_yureeka_hp_get_v1", lambda *_a, **_k: 0.0)("freshness.tiebreak.min_score_delta", 0.0) or 0.0)
        except Exception:
            md = 0.0
        if (sc_eff is not None) and (md is not None) and (md > 0.0):
            try:
                sc_eff = float(int(float(sc_eff) // float(md)) * float(md))
            except Exception:
                sc_eff = sc_f

        missing = 0 if sc_eff is not None else 1
        score_key = (-sc_eff) if sc_eff is not None else 9999
        age_key = int(age_i) if age_i is not None else 999999

        # Stable fallback ordering by (normalized) source_url.
        su = str(c.get("source_url") or "").strip()
        if not su:
            stable = ""
        else:
            try:
                nf = globals().get("_fix2af_norm_url")
                stable = nf(su) if callable(nf) else su.strip().lower()
            except Exception:
                stable = su.strip().lower()

        return (missing, score_key, age_key, stable)
    except Exception:
        return tuple()



def rebuild_metrics_from_snapshots_analysis_canonical_v1(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """Batch rebuild using the extracted canonical selector (pure, deterministic)."""
    if not isinstance(prev_response, dict):
        return {}
    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )
    if not isinstance(metric_schema, dict) or not metric_schema:
        return {}

    metric_anchors = (
        prev_response.get("metric_anchors")
        or (prev_response.get("results") or {}).get("metric_anchors")
        or {}
    )
    if not isinstance(metric_anchors, dict):
        metric_anchors = {}

    # Flatten candidates from snapshots
    if isinstance(baseline_sources_cache, dict) and isinstance(baseline_sources_cache.get("snapshots"), list):
        sources = baseline_sources_cache.get("snapshots", [])
    elif isinstance(baseline_sources_cache, list):
        sources = baseline_sources_cache
    else:
        sources = []

    # Detect injected URL set (true UI injections only) so injection runs can force
    # injected candidates to win deterministically (restores injected diffing contract).
    _inj_urls = []
    _inj_norm_set = set()
    try:
        fn_inj = globals().get("_yureeka_extract_injected_urls_v1")
        if callable(fn_inj):
            _inj_urls = fn_inj(web_context)
    except Exception:
        _inj_urls = []
    try:
        for _u in (_inj_urls or []):
            try:
                _u2 = _fix2af_norm_url(_u)
            except Exception:
                _u2 = str(_u or "").strip()
            if _u2:
                _inj_norm_set.add(_u2)
    except Exception:
        pass

    def _is_injected_candidate_v1(_c: dict) -> bool:
        try:
            if not _inj_norm_set:
                return False
            if bool((_c or {}).get("injected_source_v1")) or bool((_c or {}).get("injected")):
                return True
            _su = str((_c or {}).get("source_url") or "").strip()
            try:
                _su2 = _fix2af_norm_url(_su)
            except Exception:
                _su2 = _su
            return bool(_su2 and (_su2 in _inj_norm_set))
        except Exception:
            return False

    _inj_priority_dbg = {}

    def _refactor132_try_injected_cagr_rescue_v1(canonical_key: str, spec: dict, candidates_all: list) -> Any:
        """Deterministic fallback to bind injected CAGR percent when selection would drop it (injection runs)."""
        try:
            if not _inj_norm_set:
                return None
            if str(canonical_key or "") != "global_ev_chargers_cagr_2026_2040__percent":
                return None
        except Exception:
            return None

        best = None
        best_tie = None
        for cc in (candidates_all or []):
            if not isinstance(cc, dict):
                continue
            try:
                if not _is_injected_candidate_v1(cc):
                    continue
            except Exception:
                continue
            raw = str(cc.get("raw") or cc.get("value") or "")
            ctx = str(cc.get("context_snippet") or cc.get("context") or cc.get("context_window") or "")
            blob = (raw + " " + ctx).lower()
            if ("cagr" not in blob) or ("%" not in blob) or ("2026" not in blob) or ("2040" not in blob):
                continue

            vn = cc.get("value_norm")
            try:
                vn_f = float(vn)
            except Exception:
                continue
            if not (0.0 < vn_f < 100.0):
                continue

            try:
                ok2, _r2 = _fix17_candidate_allowed_with_reason(cc, spec, canonical_key=canonical_key)
                if not ok2:
                    continue
            except Exception:
                pass

            dp = 0
            try:
                s = str(vn)
                if "." in s:
                    dp = len(s.split(".", 1)[1].rstrip("0"))
            except Exception:
                dp = 0
            has_dec = 1 if dp > 0 else 0

            tie = (0, -has_dec, -dp, str(cc.get("source_url") or ""), raw[:80])
            if best is None or tie < best_tie:
                best = cc
                best_tie = tie

        if isinstance(best, dict):
            try:
                g = globals().get("_REFACTOR132_INJECTED_CAGR_RESCUE_V1")
                if isinstance(g, dict):
                    g["applied"] = int(g.get("applied") or 0) + 1
                    g.setdefault("samples", [])
                    if len(g.get("samples") or []) < 10:
                        g["samples"].append({
                            "canonical_key": "global_ev_chargers_cagr_2026_2040__percent",
                            "value_norm": best.get("value_norm"),
                            "raw": str(best.get("raw") or "")[:80],
                            "source_url": str(best.get("source_url") or "")[:220],
                        })
            except Exception:
                pass
        return best

    candidates = []
    for s in sources:
        if not isinstance(s, dict):
            continue
        url = s.get("source_url") or s.get("url") or ""
        try:
            url_norm = _fix2af_norm_url(url) if url else ""
        except Exception:
            url_norm = str(url or "").strip()
        src_injected = False
        try:
            src_injected = bool(s.get("injected") or s.get("injected_reason"))
        except Exception:
            src_injected = False
        if (not src_injected) and url_norm and _inj_norm_set and (url_norm in _inj_norm_set):
            src_injected = True

        xs = s.get("extracted_numbers")
        if isinstance(xs, list) and xs:
            for c in xs:
                if not isinstance(c, dict):
                    continue
                c2 = dict(c)
                if url and not c2.get("source_url"):
                    c2["source_url"] = url
                if src_injected:
                    c2["injected_source_v1"] = True
                candidates.append(c2)

    try:
        _fix2s_apply_observed_to_canonical_rules_v1(candidates, metric_schema, web_context=web_context)
    except Exception:
        pass
    def _norm(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def _cand_sort_key(c: dict):
        try:
            return (
                str(c.get("anchor_hash") or ""),
                str(c.get("source_url") or ""),
                int(c.get("start_idx") or 0),
                str(c.get("raw") or ""),
                str(c.get("unit") or ""),
                float(c.get("value_norm") or 0.0),
            )
        except Exception:
            return ("", "", 0, "", "", 0.0)

    _fix2d16_disable_fix2d15 = False
    _fix2d17_disable_fix2d16 = True

    # (Legacy block retained for context but disabled)
    # Goals:
    #   1) Prevent bare-year tokens (e.g., 2030) from being selected as metric values
    #      when the metric does NOT explicitly expect a year-as-value.
    #   2) Prevent cross-metric pollution where a sales/market snippet satisfies
    #      unrelated schema metrics (e.g., chargers, charging investment).
    #   3) Enforce light unit-family expectations (percent / currency).
    # Notes:
    #   - This patch intentionally supersedes (and replaces) FIX2D15's earlier guard.
    #   - Keep changes local to schema_only_rebuild_fix17 selection.

    def _fix2d15_expects_year_value(spec: dict, canonical_key: str) -> bool:
        try:
            unit_hint = str(spec.get("unit_tag") or spec.get("unit") or "").lower()
            dim_hint = str(spec.get("dimension") or spec.get("value_type") or "").lower()
            if dim_hint == "year":
                return True
            if "year" in unit_hint:
                return True
            ck = str(canonical_key or "")
            if ck.endswith("__year") or ("_year_" in ck):
                return True
            return False
        except Exception:
            return False

    def _fix2d15_is_bare_year_token(cand: dict) -> bool:
        try:
            raw = str(cand.get("raw") or cand.get("value") or "").strip()
            v = cand.get("value_norm")
            try:
                vf = float(v)
            except Exception:
                pass
                try:
                    vf = float(re.sub(r"[^0-9\.\-]+", "", raw or ""))
                except Exception:
                    return False

            if vf < 1900 or vf > 2100:
                return False

            # Accept raw like "2030" and "2030.0" (and "2030.00") as year tokens
            looks_year = False
            try:
                if re.fullmatch(r"\d{4}", raw or ""):
                    looks_year = True
                elif re.fullmatch(r"\d{4}\.0+", raw or ""):
                    looks_year = True
            except Exception:
                pass
                looks_year = False

            if not looks_year:
                try:
                    iv = int(vf)
                    looks_year = (abs(vf - iv) < 1e-6) and (1900 <= iv <= 2100)
                except Exception:
                    pass
                    looks_year = False

            if not looks_year:
                return False

            unit = str(cand.get("unit") or cand.get("unit_tag") or "").strip()
            if unit:
                return False
            if "%" in raw or "$" in raw or "€" in raw or "£" in raw:
                return False

            return True
        except Exception:
            return False

    def _fix2d15_metric_domain_tokens(canonical_key: str, spec: dict) -> list:
        """Derive a small set of domain tokens from canonical_key/name."""
        try:
            ck = _norm(str(canonical_key or ""))
            nm = _norm(str(spec.get("name") or spec.get("metric_name") or ""))
            base = " ".join([ck, nm]).strip()
            toks = [w for w in base.split() if w and not re.fullmatch(r"\d{4}", w)]
            stop = set(["global","world","worldwide","total","overall","ev","electric","vehicle","vehicles","metric","market"])
            out = []
            for w in toks:
                if w in stop:
                    continue
                if len(w) < 4:
                    continue
                if w not in out:
                    out.append(w)
            return out[:6]
        except Exception:
            return []

    # - Prevent generic keyword hits (e.g., 'global', 'market') from allowing
    #   unrelated candidates (e.g., year tokens) to satisfy domain-specific
    #   metrics like chargers/investment.
    # - For certain metrics, require at least one strong domain token to
    #   appear in candidate context/raw.
    def _fix2d19_required_domain_tokens(canonical_key: str, spec: dict) -> list:
        try:
            if bool(globals().get("_FIX2D20_DISABLE_FIX2D19", False)):
                return []
            ck = str(canonical_key or '').lower()
            dim = str(spec.get('dimension') or spec.get('value_type') or '').lower()
            uf = str(spec.get('unit_family') or '').lower()
            req = []
            if 'charger' in ck or 'charging' in ck:
                req += ['charger', 'charging', 'station', 'infrastructure']
            if 'investment' in ck or 'capex' in ck or dim == 'currency' or uf == 'currency':
                # still allow currency cues to satisfy, but require at least one semantic token
                req += ['invest', 'investment', 'capex', 'spend', 'spending', 'cost']
            if 'share' in ck or dim == 'percent' or uf == 'percent':
                req += ['share', 'market share', 'ev share']
            if 'sale' in ck or 'sales' in ck or dim in ('unit_sales','sales','units') or uf in ('unit_sales','sales','units'):
                req += ['sales', 'sold', 'deliveries', 'deliver']
            # de-dupe while keeping order
            out=[]
            for r in req:
                r=str(r).strip().lower()
                if r and r not in out:
                    out.append(r)
            return out[:8]
        except Exception:
            return []

    # - This is the decisive fix: reject invalid candidates *before* ranking.
    # - Prevents bare year tokens (e.g., 2024/2030/2030.0) from ever being
    #   eligible evidence for non-year metrics.
    # - Enforces unit-family requirements and required domain-token binding.
    def _fix2d22_candidate_eligible(cand: dict, spec: dict, canonical_key: str, kw_norm: list) -> (bool, str):
        try:
            # 1) Hard bare-year rejection unless metric explicitly expects year-as-value
            if _fix2d15_is_bare_year_token(cand) and not _fix2d15_expects_year_value(spec, canonical_key):
                return False, 'bare_year_token'

            # 2) Unit-family enforcement (percent/currency/unit_sales)
            if not _fix2d15_unit_family_ok(cand, spec):
                return False, 'unit_family_mismatch'

            # 3) Required domain token binding (strong)
            ctx = _norm(cand.get('context_snippet') or cand.get('context') or cand.get('context_window') or '')
            rawn = _norm(cand.get('raw') or '')

            try:
                _yrs = re.findall(r"(19\d{2}|20\d{2})", str(canonical_key or ""))
                if _yrs:
                    _hit_all = True
                    for _y in _yrs:
                        if _y and (_y not in ctx) and (_y not in rawn):
                            _hit_all = False
                            break
                    if not _hit_all:
                        return False, 'missing_required_year_token'
            except Exception:
                pass

            req_dom = _fix2d19_required_domain_tokens(canonical_key, spec)
            if req_dom:
                hit = 0
                for r in req_dom:
                    rr = _norm(r)
                    if rr and (rr in ctx or rr in rawn):
                        hit += 1
                        break
                if hit <= 0:
                    return False, 'missing_required_domain_token'

            # 4) If schema provides keywords/domain hints, require at least one hit
            dom = _fix2d15_metric_domain_tokens(canonical_key, spec)
            if (kw_norm or dom):
                hit_kw = 0
                for k in (kw_norm or []):
                    if k and (k in ctx or k in rawn):
                        hit_kw += 1
                        break
                hit_dom = 0
                for d in (dom or []):
                    if d and (d in ctx or d in rawn):
                        hit_dom += 1
                        break
                if hit_kw <= 0 and hit_dom <= 0:
                    return False, 'no_keyword_or_domain_hits'

            return True, ''
        except Exception:
            return True, ''

    def _fix2d15_unit_family_ok(cand: dict, spec: dict) -> bool:
        try:
            dim = str(spec.get("dimension") or "").lower()
            uf = str(spec.get("unit_family") or "").lower()
            raw = str(cand.get("raw") or "")
            unit = str(cand.get("unit") or cand.get("unit_tag") or "").lower()

            s = (raw + " " + unit).lower()
            has_currency = ("$" in raw) or ("us$" in s) or ("usd" in s) or ("eur" in s) or ("gbp" in s) or ("€" in raw) or ("£" in raw)
            has_percent = ("%" in raw) or ("percent" in s) or ("pct" in s)

            if dim == "percent" or uf == "percent":
                return has_percent
            if dim == "currency" or uf == "currency":
                return has_currency


            # REFACTOR125: metrics that declare unit 'M' must carry an explicit million cue.
            # This prevents ratio values (e.g., '7.5 vehicles per charger') from populating
            # million-scaled schema keys like chargers_2040 / sales_ytd.
            try:
                spec_unit = str(spec.get('unit') or spec.get('unit_tag') or '').strip()
                if spec_unit and spec_unit.upper() == 'M':
                    ctx0 = str(cand.get('context_snippet') or cand.get('context') or cand.get('context_window') or '')
                    blobm = (raw + ' ' + unit + ' ' + ctx0).lower()
                    # strong cues
                    has_million = ('million' in blobm) or bool(re.search(r"\bmn\b", blobm))
                    # compact forms: 206.6m, 18.5m
                    if not has_million:
                        has_million = bool(re.search(r"\b\d+(?:\.\d+)?\s*m\b", (raw or '').lower()))
                    if not has_million:
                        # allow explicit unit tag
                        has_million = (str(cand.get('unit') or cand.get('unit_tag') or '').strip().upper() == 'M')
                    if not has_million:
                        return False
            except Exception:
                pass
# FIX2D18: unit-sales expectations (prevents bare years like 2030 from winning)
            if not bool(globals().get("_FIX2D20_DISABLE_FIX2D18", False)):
                if dim in ("unit_sales","units","sales") or uf in ("unit_sales","units","sales"):
                    # require some unit cue
                    has_units = ("unit" in s) or ("units" in s) or ("million" in s) or ("mn" in s) or ("m " in s) or (" m" in s)
                    if not has_units:
                        return False

            return True
        except Exception:
            return True

    candidates.sort(key=_cand_sort_key)

    # Debug sink
    dbg = prev_response.setdefault("_evolution_rebuild_debug", {})
    # REFACTOR130: reset injected semantic override beacon (per run)
    try:
        globals()["_REFACTOR131_INJECTED_SEMANTIC_OVERRIDE_V1"] = {"overrides": 0, "samples": []}
    except Exception:
        pass
    dbg.setdefault("schema_only_zero_hit_metrics_fix17", [])
    dbg.setdefault("fix2d15_reject_reasons", {})
    dbg.setdefault("fix2d22_reject_reasons", {})
    dbg.setdefault("fix2d22_year_reject_samples", [])
    dbg.setdefault("fix2d15_year_reject_samples", [])

    rebuilt = {}

    for canonical_key, sch in metric_schema.items():
        if not isinstance(sch, dict):
            continue

        spec = dict(sch)
        spec.setdefault("canonical_key", canonical_key)
        spec.setdefault("name", sch.get("name") or canonical_key)

        # Year-like pruning is handled elsewhere; legacy FIX17 hook removed (dead).
        metric_is_year_like = False

        keywords = sch.get("keywords") or sch.get("keyword_hints") or []
        if isinstance(keywords, str):
            keywords = [keywords]

        fn_prune = globals().get("_fix16_prune_year_keywords")
        if callable(fn_prune):
            keywords2 = fn_prune(list(keywords), metric_is_year_like)
        else:
            keywords2 = list(keywords)

        kw_norm = [_norm(k) for k in (keywords2 or []) if k]

        best = None
        best_tie = None
        best_hits = 0

        # REFACTOR135: enable precision trace (always) for the chargers_2040 key
        _rf135_trace_enabled = (str(canonical_key or '') == 'global_ev_chargers_2040__unit_count')
        _rf135_trace_top = []
        _rf135_trace_seen = 0

        # Motivation:
        #   schema_only_rebuild can still end up selecting a bare year token (e.g. 2024/2026)
        #   when that token appears in the same snippet as the real metric value.
        # Policy:
        #   If the schema does NOT expect a year-as-value and there exists at least one eligible
        #   non-year candidate in the pool for this canonical_key, then exclude bare-year tokens
        #   from competition for this canonical_key.
        # Notes:
        #   - This is a local, deterministic filter applied BEFORE keyword scoring.
        #   - It does not weaken year-blocking; it strengthens selection parity.
        _fix2d2s_expect_year = False
        try:
            _fix2d2s_expect_year = bool(_fix2d15_expects_year_value(spec, str(canonical_key)))
        except Exception:
            pass
            _fix2d2s_expect_year = False

        _fix2d2s_eligible = []
        _fix2d2s_has_non_year = False
        for _c in (candidates or []):
            ok, _reason = _fix17_candidate_allowed_with_reason(_c, spec, canonical_key=canonical_key)
            if not ok:
                continue
            try:
                # FIX2D63: bugfix - use the correct candidate variable (_c), not the outer loop's c.
                _ok_u, _why_u = _fix2d2u_semantic_eligible(_c, spec, canonical_key=str(canonical_key))
                if not _ok_u:
                    _allow_injected = False
                    try:
                        # REFACTOR130: If injection is active, allow injected candidates through
                        # when semantic token gates are too strict, as long as minimal schema-safe
                        # criteria are satisfied (unit-family + required years + keyword/domain hit).
                        if _inj_norm_set and _is_injected_candidate_v1(_c):
                            # Don't allow bare years into non-year metrics
                            if _fix2d15_is_bare_year_token(_c) and (not _fix2d15_expects_year_value(spec, str(canonical_key))):
                                _allow_injected = False
                            # Unit-family must still match schema expectations
                            elif not _fix2d15_unit_family_ok(_c, spec):
                                _allow_injected = False
                            else:
                                _ctx2 = _norm(_c.get("context_snippet") or _c.get("context") or _c.get("context_window") or "")
                                _raw2 = _norm(_c.get("raw") or "")

                                # Required years from canonical key (e.g., 2026, 2040)
                                _req_years = []
                                try:
                                    _req_years = re.findall(r"(19\d{2}|20\d{2})", str(canonical_key or ""))
                                except Exception:
                                    _req_years = []

                                _hit_years = True
                                for _y in (_req_years or []):
                                    if _y and (_y not in _ctx2) and (_y not in _raw2):
                                        _hit_years = False
                                        break

                                # Require at least one keyword OR domain-token hit
                                _dom2 = _fix2d15_metric_domain_tokens(canonical_key, spec)
                                _hit2 = False
                                for _k in (kw_norm or []):
                                    if _k and (_k in _ctx2 or _k in _raw2):
                                        _hit2 = True
                                        break
                                if not _hit2:
                                    for _d in (_dom2 or []):
                                        if _d and (_d in _ctx2 or _d in _raw2):
                                            _hit2 = True
                                            break

                                if _hit_years and _hit2:
                                    _allow_injected = True
                                    # beacon (global + local dbg)
                                    try:
                                        g = globals().get("_REFACTOR131_INJECTED_SEMANTIC_OVERRIDE_V1")
                                        if isinstance(g, dict):
                                            g["overrides"] = int(g.get("overrides") or 0) + 1
                                            g.setdefault("samples", [])
                                            if len(g.get("samples") or []) < 10:
                                                g["samples"].append({
                                                    "canonical_key": str(canonical_key),
                                                    "raw": str(_c.get("raw") or "")[:60],
                                                    "value_norm": _c.get("value_norm"),
                                                    "unit": str(_c.get("unit") or _c.get("unit_tag") or "")[:12],
                                                    "source_url": str(_c.get("source_url") or "")[:200],
                                                    "semantic_reject_reason": str(_why_u or "")[:120],
                                                })
                                    except Exception:
                                        pass
                                    try:
                                        dbg.setdefault("injected_semantic_override_v1", {"overrides": 0, "samples": []})
                                        dbg["injected_semantic_override_v1"]["overrides"] = int(dbg["injected_semantic_override_v1"].get("overrides") or 0) + 1
                                        if len(dbg["injected_semantic_override_v1"].get("samples") or []) < 10:
                                            dbg["injected_semantic_override_v1"]["samples"].append({
                                                "canonical_key": str(canonical_key),
                                                "raw": str(_c.get("raw") or "")[:60],
                                                "value_norm": _c.get("value_norm"),
                                                "unit": str(_c.get("unit") or _c.get("unit_tag") or "")[:12],
                                                "source_url": str(_c.get("source_url") or "")[:200],
                                                "semantic_reject_reason": str(_why_u or "")[:120],
                                            })
                                    except Exception:
                                        pass
                    except Exception:
                        _allow_injected = False

                    if not _allow_injected:
                        continue
            except Exception:
                pass

            # FIX2D63: upstream reject yearlike numeric tokens for unit/count metrics
            # unless the candidate carries unit evidence (prevents YTD 2025 headings from hijacking values).
            try:
                if _fix2d63_schema_expects_unit_or_count(str(canonical_key), spec):
                    if _fix2d63_is_yearlike_value(_c) and (not _fix2d63_has_unit_evidence(_c)):
                        dbg.setdefault('fix2d63_reject_yearlike_no_unit_evidence', 0)
                        dbg['fix2d63_reject_yearlike_no_unit_evidence'] = int(dbg.get('fix2d63_reject_yearlike_no_unit_evidence') or 0) + 1
                        continue
            except Exception:
                pass
            try:
                _ok2, _why2 = _fix2d22_candidate_eligible(_c, spec, canonical_key=str(canonical_key), kw_norm=kw_norm)
                if not _ok2:
                    dbg.setdefault('fix2d22_reject_reasons', {})
                    dbg['fix2d22_reject_reasons'][_why2] = int(dbg['fix2d22_reject_reasons'].get(_why2) or 0) + 1
                    if _why2 == 'bare_year_token':
                        dbg.setdefault('fix2d22_year_reject_samples', [])
                        if len(dbg.get('fix2d22_year_reject_samples') or []) < 20:
                            dbg['fix2d22_year_reject_samples'].append({
                                'canonical_key': str(canonical_key),
                                'raw': str(_c.get('raw') or _c.get('value') or '')[:80],
                                'value_norm': _c.get('value_norm'),
                                'unit': str(_c.get('unit') or ''),
                                'source_url': str(_c.get('source_url') or '')[:120],
                            })
                    continue
            except Exception:
                pass

            _fix2d2s_eligible.append(_c)
            try:
                if (not _fix2d2s_expect_year) and (not _fix2d15_is_bare_year_token(_c)):
                    _fix2d2s_has_non_year = True
            except Exception:
                pass

        if _fix2d2s_eligible and _fix2d2s_has_non_year and (not _fix2d2s_expect_year):
            try:
                dbg.setdefault('fix2d2s_filtered_bare_year_tokens', 0)
                _before = len(_fix2d2s_eligible)
                _fix2d2s_eligible = [x for x in _fix2d2s_eligible if not _fix2d15_is_bare_year_token(x)]
                dbg['fix2d2s_filtered_bare_year_tokens'] = int(dbg.get('fix2d2s_filtered_bare_year_tokens') or 0) + int(max(0, _before - len(_fix2d2s_eligible)))
            except Exception:
                pass

        # If this is an injection run and injected candidates exist for this key,
        # force selection to occur within the injected pool (two-pass deterministic).
        _fix2d2s_pool = _fix2d2s_eligible or []
        _fix2d2s_used_inj_pool = False
        _fix2d2s_inj_pool = []
        _fix2d2s_inj_pool_relaxed = []
        _fix2d2s_inj_pool_mode = ""
        try:
            if _inj_norm_set:
                _fix2d2s_inj_pool = [x for x in (_fix2d2s_pool or []) if _is_injected_candidate_v1(x)]
                # REFACTOR131: prevent strict injected pool from being satisfied by bare year tokens.
                try:
                    if _fix2d2s_inj_pool and (not _fix2d15_expects_year_value(spec, str(canonical_key))):
                        _before = len(_fix2d2s_inj_pool)
                        _fix2d2s_inj_pool = [x for x in _fix2d2s_inj_pool if not _fix2d15_is_bare_year_token(x)]
                        _after = len(_fix2d2s_inj_pool)
                        if _before != _after:
                            try:
                                g = globals().get("_REFACTOR131_INJECTED_SEMANTIC_OVERRIDE_V1")
                                if isinstance(g, dict):
                                    g["strict_injected_year_tokens_filtered"] = int(g.get("strict_injected_year_tokens_filtered") or 0) + int(_before - _after)
                            except Exception:
                                pass
                except Exception:
                    pass
                if _fix2d2s_inj_pool:
                    _fix2d2s_pool = _fix2d2s_inj_pool
                    _fix2d2s_used_inj_pool = True
                    _fix2d2s_inj_pool_mode = "strict_eligible_injected_pool"
                else:
                    # REFACTOR127: relaxed injected pool fallback.
                    # Some injected candidates can be filtered out by strict eligibility rules
                    # (e.g., percent/growth assoc + domain-token gates). When that happens, we
                    # still want injection to override *if* a candidate satisfies minimal
                    # schema-safe gates (unit family + required year tokens + at least one
                    # keyword/domain hit). Deterministic; no refetch.
                    _req_years = []
                    try:
                        _req_years = re.findall(r"(19\d{2}|20\d{2})", str(canonical_key or ''))
                    except Exception:
                        _req_years = []
                    for _cc in (candidates or []):
                        try:
                            if not _is_injected_candidate_v1(_cc):
                                continue
                            # Don't allow bare year tokens to populate non-year-valued metrics
                            if _fix2d15_is_bare_year_token(_cc) and (not _fix2d15_expects_year_value(spec, canonical_key)):
                                continue
                            # Unit-family gate (percent/currency cues)
                            if not _fix2d15_unit_family_ok(_cc, spec):
                                continue

                            _ctx2 = _norm(_cc.get('context_snippet') or _cc.get('context') or _cc.get('context_window') or '')
                            _raw2 = _norm(_cc.get('raw') or '')

                            # Required year tokens (if any in canonical key)
                            _hit_years = True
                            for _y in (_req_years or []):
                                if _y and (_y not in _ctx2) and (_y not in _raw2):
                                    _hit_years = False
                                    break
                            if not _hit_years:
                                continue

                            # Require at least one keyword OR domain-token hit to avoid wrong-percent bleed
                            _dom2 = _fix2d15_metric_domain_tokens(canonical_key, spec)
                            _hit2 = False
                            for _k in (kw_norm or []):
                                if _k and (_k in _ctx2 or _k in _raw2):
                                    _hit2 = True
                                    break
                            if not _hit2:
                                for _d in (_dom2 or []):
                                    if _d and (_d in _ctx2 or _d in _raw2):
                                        _hit2 = True
                                        break
                            if not _hit2:
                                continue

                            _fix2d2s_inj_pool_relaxed.append(_cc)
                        except Exception:
                            continue

                    if _fix2d2s_inj_pool_relaxed:
                        _fix2d2s_pool = _fix2d2s_inj_pool_relaxed
                        _fix2d2s_used_inj_pool = True
                        _fix2d2s_inj_pool_mode = "relaxed_injected_pool"
        except Exception:
            pass

        # record pool sizes for beacon/debug (per canonical_key)
        try:
            _inj_priority_dbg[str(canonical_key)] = {
                "inj_set_size": int(len(_inj_norm_set or [])),
                "eligible_total": int(len(_fix2d2s_eligible or [])),
                "eligible_injected_strict": int(len(_fix2d2s_inj_pool or [])),
                "eligible_injected_relaxed": int(len(_fix2d2s_inj_pool_relaxed or [])),
                "used_injected_pool": bool(_fix2d2s_used_inj_pool),
                "injected_pool_mode": str(_fix2d2s_inj_pool_mode or ""),
            }
        except Exception:
            pass

        # LLM04: anomaly context (per-canonical_key)
        _llm04_question_text = ""
        _llm04_pool_stats = {}
        if _yureeka_llm_flag_bool_v1("ENABLE_LLM_ANOMALY_FLAGS"):
            try:
                _llm04_question_text = str(prev_response.get("question") or "")
            except Exception:
                _llm04_question_text = ""
            try:
                _vals = []
                for _pc in (_fix2d2s_pool or []):
                    try:
                        _vn = _pc.get("value_norm")
                        if isinstance(_vn, (int, float)) and not isinstance(_vn, bool):
                            if _fix2d24_is_yearlike(_vn, _pc.get("raw")):
                                continue
                            _vals.append(float(_vn))
                    except Exception:
                        continue
                _vals.sort()
                if _vals:
                    mid = len(_vals) // 2
                    med = _vals[mid] if (len(_vals) % 2 == 1) else 0.5 * (_vals[mid - 1] + _vals[mid])
                    _llm04_pool_stats = {"n": int(len(_vals)), "median": float(med), "min": float(_vals[0]), "max": float(_vals[-1])}
            except Exception:
                _llm04_pool_stats = {}
            try:
                dbg.setdefault("llm04_anomaly_summary_v1", {"enabled": True, "keys": 0, "llm_attempts": 0, "llm_ok": 0, "llm_fail": 0, "flag_source": str(_yureeka_llm_flag_effective_v1("ENABLE_LLM_ANOMALY_FLAGS")[1])[:80]})
            except Exception:
                pass

        for c in (_fix2d2s_pool or []):
            ok, _reason = _fix17_candidate_allowed_with_reason(c, spec, canonical_key=canonical_key)
            if not ok:
                continue

            # REFACTOR190: removed dead injected-candidate hook (_is_injected_candidate_v1).
            try:
                _ok_u, _why_u = _fix2d2u_semantic_eligible(c, spec, str(canonical_key))
                if not _ok_u:
                    dbg.setdefault('fix2d2u_reject_reasons_fix17', {})
                    dbg['fix2d2u_reject_reasons_fix17'][_why_u] = int(dbg['fix2d2u_reject_reasons_fix17'].get(_why_u) or 0) + 1
                    continue
            except Exception:
                pass

# PATCH FIX2D22 (ADD): eligibility-before-scoring gate
            try:
                _ok2, _why2 = _fix2d22_candidate_eligible(c, spec, canonical_key=str(canonical_key), kw_norm=kw_norm)
                if not _ok2:
                    dbg.setdefault("fix2d22_reject_reasons", {})
                    dbg["fix2d22_reject_reasons"][_why2] = int(dbg["fix2d22_reject_reasons"].get(_why2) or 0) + 1
                    if _why2 == 'bare_year_token':
                        dbg.setdefault("fix2d22_year_reject_samples", [])
                        if len(dbg.get("fix2d22_year_reject_samples") or []) < 20:
                            dbg["fix2d22_year_reject_samples"].append({
                                'canonical_key': str(canonical_key),
                                'raw': str(c.get('raw') or c.get('value') or '')[:80],
                                'value_norm': c.get('value_norm'),
                                'unit': str(c.get('unit') or ''),
                                'source_url': str(c.get('source_url') or '')[:120],
                            })
                    continue
            except Exception:
                pass

            ctx = _norm(c.get("context_snippet") or c.get("context") or c.get("context_window") or "")
            raw = _norm(c.get("raw") or "")

            hits = 0
            for k in kw_norm:
                if k and (k in ctx or k in raw):
                    hits += 1

            # REFACTOR135: million-scale precision preference (million cue + decimals) as a deterministic tie-break
            _rf135_is_m_metric = False
            try:
                _u_rf135 = str(spec.get('unit_tag') or spec.get('unit') or '').strip().upper()
                _rf135_is_m_metric = (_u_rf135 == 'M')
            except Exception:
                _rf135_is_m_metric = False

            _rf135_has_million_cue = 0
            _rf135_has_decimal = 0
            _rf135_decimal_places = 0
            if _rf135_is_m_metric:
                try:
                    _raw0 = str(c.get('raw') or '')
                    _ctx0 = str(c.get('context_snippet') or c.get('context') or c.get('context_window') or '')
                    _blob0 = (_raw0 + ' ' + _ctx0).lower()
                    _ut0 = str(c.get('unit_tag') or c.get('unit') or '').strip().lower()
                    if _ut0 == 'm' or ('million' in _blob0) or re.search(r"\bmn\b", _blob0) or _blob0.strip().endswith('m'):
                        _rf135_has_million_cue = 1

                    _m = re.search(r"(\d+)\.(\d+)", _raw0)
                    if _m:
                        _rf135_decimal_places = len((_m.group(2) or '').rstrip('0'))
                    else:
                        try:
                            _vn = c.get('value_norm')
                            if isinstance(_vn, (int, float)):
                                _frac = abs(float(_vn) - int(float(_vn)))
                                if _frac > 1e-9:
                                    _rf135_decimal_places = 1
                        except Exception:
                            pass
                    if _rf135_decimal_places > 0:
                        _rf135_has_decimal = 1
                except Exception:
                    pass

            # LLM04: anomaly penalty (rule-based) — only affects tie-break when ENABLE_LLM_ANOMALY_FLAGS is ON
            _llm04_penalty_points = 0
            _llm04_anom_obj = None
            try:
                if _yureeka_llm_flag_bool_v1("ENABLE_LLM_ANOMALY_FLAGS"):
                    _llm04_anom_obj = _llm04_rule_anomaly_flags_v1(
                        candidate=c,
                        spec=spec,
                        canonical_key=str(canonical_key),
                        pool_stats=_llm04_pool_stats,
                        question_text=_llm04_question_text,
                    )
                    if isinstance(_llm04_anom_obj, dict):
                        _llm04_penalty_points = int(_llm04_anom_obj.get("penalty_points") or 0)
                        try:
                            c["_llm04_anomaly_v1"] = _llm04_anom_obj
                        except Exception:
                            pass
            except Exception:
                pass

            if _rf135_is_m_metric:
                if _yureeka_llm_flag_bool_v1("ENABLE_LLM_ANOMALY_FLAGS"):
                    tie = (-hits, int(_llm04_penalty_points), -int(bool(_rf135_has_million_cue)), -int(bool(_rf135_has_decimal)), -int(_rf135_decimal_places)) + _cand_sort_key(c)
                else:
                    tie = (-hits, -int(bool(_rf135_has_million_cue)), -int(bool(_rf135_has_decimal)), -int(_rf135_decimal_places)) + _cand_sort_key(c)
            else:
                if _yureeka_llm_flag_bool_v1("ENABLE_LLM_ANOMALY_FLAGS"):
                    tie = (-hits, int(_llm04_penalty_points)) + _cand_sort_key(c)
                else:
                    tie = (-hits,) + _fresh02_candidate_tie_key_v1(c) + _cand_sort_key(c)

            # FRESH02: base tie (no freshness) for diagnostics
            tie_base = (-hits,) + _cand_sort_key(c)


            # REFACTOR135 trace: keep a compact top-N for chargers_2040
            try:
                if _rf135_trace_enabled:
                    _rf135_trace_seen += 1
                    _rec = {
                        'source_url': str(c.get('source_url') or '')[:220],
                        'raw': str(c.get('raw') or '')[:80],
                        'value_norm': c.get('value_norm'),
                        'unit': str(c.get('unit') or c.get('unit_tag') or '')[:12],
                        'hit_score': int(hits),
                        'has_decimal': int(bool(_rf135_has_decimal)),
                        'decimal_places': int(_rf135_decimal_places or 0),
                        'has_million_cue': int(bool(_rf135_has_million_cue)),
                    }
                    _rf135_trace_top.append((tie, _rec))
                    _rf135_trace_top.sort(key=lambda x: x[0])
                    _rf135_trace_top = _rf135_trace_top[:8]
            except Exception:
                pass
            if best is None or tie < best_tie:
                best = c
                best_tie = tie
                best_hits = hits

        if not isinstance(best, dict):
            _resc = _refactor132_try_injected_cagr_rescue_v1(str(canonical_key), spec, candidates)
            if isinstance(_resc, dict):
                best = _resc
                best_hits = 1
            else:
                continue

        # If schema has keywords, require at least one hit.
        if kw_norm and best_hits <= 0:
            _resc = _refactor132_try_injected_cagr_rescue_v1(str(canonical_key), spec, candidates)
            if isinstance(_resc, dict):
                best = _resc
                best_hits = 1
            else:
                dbg["schema_only_zero_hit_metrics_fix17"].append({"canonical_key": canonical_key, "reason": "no_keyword_hits"})
                continue

        # Even if earlier eligibility gates miss it, prevent committing a pure year
        # (e.g., 2030) as the metric value for non-year metrics.
        # REFACTOR128: record chosen URL/is_injected for injection priority beacon
        try:
            if _inj_norm_set and isinstance(_inj_priority_dbg.get(str(canonical_key)), dict):
                _inj_priority_dbg[str(canonical_key)]["chosen_url"] = str(best.get("source_url") or "")[:220]
                _inj_priority_dbg[str(canonical_key)]["chosen_is_injected"] = False
        except Exception:
            pass

        try:
            # REFACTOR132: if injection-run CAGR binding fell into a year-token trap, try deterministic injected rescue before rejecting commit.
            try:
                if _inj_norm_set and str(canonical_key or '') == 'global_ev_chargers_cagr_2026_2040__percent' and _fix2d15_is_bare_year_token(best):
                    _resc = _refactor132_try_injected_cagr_rescue_v1(str(canonical_key), spec, candidates)
                    if isinstance(_resc, dict):
                        best = _resc
            except Exception:
                pass

            if _fix2d15_is_bare_year_token(best) and not _fix2d15_expects_year_value(spec, str(canonical_key)):
                dbg.setdefault("fix2d16_rejected_bare_year_commit", 0)
                dbg["fix2d16_rejected_bare_year_commit"] = int(dbg.get("fix2d16_rejected_bare_year_commit") or 0) + 1
                dbg.setdefault("fix2d16_rejected_bare_year_samples", [])
                if len(dbg.get("fix2d16_rejected_bare_year_samples") or []) < 20:
                    dbg["fix2d16_rejected_bare_year_samples"].append({
                        "canonical_key": str(canonical_key),
                        "raw": str(best.get("raw") or best.get("value") or "")[:80],
                        "value_norm": best.get("value_norm"),
                        "source_url": str(best.get("source_url") or "")[:120],
                    })
                continue
        except Exception:
            pass

        # REFACTOR135: persist precision-tiebreak trace for chargers_2040 (even when no tie)
        try:
            if _rf135_trace_enabled:
                _top = [r for _, r in (_rf135_trace_top or [])]
                _chosen = None
                if _top:
                    _chosen = dict(_top[0])
                elif isinstance(best, dict):
                    _chosen = {
                        'source_url': str(best.get('source_url') or '')[:220],
                        'raw': str(best.get('raw') or '')[:80],
                        'value_norm': best.get('value_norm'),
                        'unit': str(best.get('unit') or best.get('unit_tag') or '')[:12],
                        'hit_score': int(best_hits or 0),
                    }

                _reason = ''
                if len(_top) >= 2:
                    a, b = _top[0], _top[1]
                    try:
                        if a.get('hit_score') != b.get('hit_score'):
                            _reason = 'max_keyword_hits'
                        elif a.get('has_million_cue') != b.get('has_million_cue'):
                            _reason = 'million_cue_preference'
                        elif a.get('has_decimal') != b.get('has_decimal'):
                            _reason = 'decimal_precision_preference'
                        elif a.get('decimal_places') != b.get('decimal_places'):
                            _reason = 'more_decimal_places'
                        else:
                            _reason = 'stable_sort_fallback'
                    except Exception:
                        _reason = ''

                globals()['_REFACTOR129_PRECISION_TIEBREAK_V1'] = {
                    'canonical_key': str(canonical_key),
                    'spec_unit_tag': str(spec.get('unit_tag') or spec.get('unit') or ''),
                    'candidates_considered': int(_rf135_trace_seen or 0),
                    'top_candidates': _top,
                    'chosen_candidate': _chosen,
                    'winner_reason': str(_reason),
                }
        except Exception:
            pass
        _entry = {
            "canonical_key": canonical_key,
            "name": spec.get("name") or canonical_key,
            "value": best.get("value"),
            "unit": best.get("unit") or "",
            "value_norm": best.get("value_norm"),
            "source_url": best.get("source_url") or "",
            "anchor_hash": best.get("anchor_hash") or "",
            "evidence": [{
                "source_url": best.get("source_url") or "",
                "raw": best.get("raw") or "",
                "context_snippet": (best.get("context_snippet") or best.get("context") or best.get("context_window") or "")[:400],
                "anchor_hash": best.get("anchor_hash") or "",
                "method": "schema_only_rebuild_fix17",
            }],
            "anchor_used": False,
        }

        # LLM04: attach anomaly flags for the chosen winner + optional relevance probe (no prompt leakage)
        if _yureeka_llm_flag_bool_v1("ENABLE_LLM_ANOMALY_FLAGS"):
            try:
                _anom_best = None
                try:
                    _anom_best = best.get("_llm04_anomaly_v1")
                except Exception:
                    _anom_best = None
                if not isinstance(_anom_best, dict):
                    _anom_best = _llm04_rule_anomaly_flags_v1(
                        candidate=best,
                        spec=spec,
                        canonical_key=str(canonical_key),
                        pool_stats=_llm04_pool_stats,
                        question_text=_llm04_question_text,
                    )
                if isinstance(_anom_best, dict) and (int(_anom_best.get("penalty_points") or 0) > 0 or (_anom_best.get("flags") or [])):
                    _entry["anomaly_flags_v1"] = list(_anom_best.get("flags") or [])
                    _entry["anomaly_penalty_points"] = int(_anom_best.get("penalty_points") or 0)

                    # Optional LLM relevance check for high-risk anomalies only
                    _llm_obj = None
                    _llm_diag = None
                    try:
                        if int(_entry.get("anomaly_penalty_points") or 0) >= 2:
                            _llm_obj, _llm_diag = _llm04_llm_relevance_check_v1(
                                question_text=_llm04_question_text,
                                canonical_key=str(canonical_key),
                                spec=spec,
                                candidate=best,
                            )
                    except Exception:
                        _llm_obj = None
                        _llm_diag = None

                    if isinstance(_llm_obj, dict):
                        _entry["anomaly_llm_relevance_v1"] = {
                            "relevant": bool(_llm_obj.get("relevant")),
                            "confidence": float(_llm_obj.get("confidence") or 0.0),
                            "rationale": str(_llm_obj.get("rationale") or "")[:120],
                        }
                        # Only penalize (never force acceptance)
                        if (not bool(_llm_obj.get("relevant"))) and float(_llm_obj.get("confidence") or 0.0) >= 0.65:
                            try:
                                _entry["anomaly_flags_v1"].append("llm_irrelevant")
                            except Exception:
                                pass
                            try:
                                _entry["anomaly_penalty_points"] = int(_entry.get("anomaly_penalty_points") or 0) + 2
                            except Exception:
                                pass

                    # Debug: store non-sensitive call diagnostics
                    try:
                        if isinstance(_llm_diag, dict):
                            dbg.setdefault("llm04_anomaly_by_key_v1", {})[str(canonical_key)] = {
                                "penalty_points": int(_entry.get("anomaly_penalty_points") or 0),
                                "flags": list(_entry.get("anomaly_flags_v1") or []),
                                "llm_diag": _llm_diag,
                                "llm_response": _entry.get("anomaly_llm_relevance_v1"),
                            }
                            _s = dbg.get("llm04_anomaly_summary_v1") or {}
                            _s["keys"] = int(_s.get("keys") or 0) + 1
                            if bool(_llm_diag.get("llm_used")):
                                _s["llm_attempts"] = int(_s.get("llm_attempts") or 0) + 1
                                if bool(_llm_diag.get("ok")):
                                    _s["llm_ok"] = int(_s.get("llm_ok") or 0) + 1
                                else:
                                    _s["llm_fail"] = int(_s.get("llm_fail") or 0) + 1
                            dbg["llm04_anomaly_summary_v1"] = _s
                    except Exception:
                        pass
            except Exception:
                pass

        rebuilt[canonical_key] = _entry

    return rebuilt

# - Uses LOCAL context_snippet (not page-wide context_window)
# - Prevents cross-metric pollution in schema-only rebuild and other projections
# - Called by BOTH Analysis selector and Evolution rebuild paths
def _fix2d2u_norm(s: str) -> str:
    # REFACTOR166: keep name for backward-compat; single source of truth.
    return _fix2d2u_norm_text(s)

def _fix2d2u_local_text(cand: dict) -> str:
    try:
        if not isinstance(cand, dict):
            return ""
        for k in ("context_snippet", "context", "context_window", "context_window_raw", "context_window_text"):
            v=cand.get(k)
            if isinstance(v, str) and v.strip():
                return v
        return str(cand.get("raw") or cand.get("value") or "")
    except Exception:
        return ""

def _fix2d2u_required_token_groups(canonical_key: str, spec: dict) -> list:
    """Return a list of OR-groups; each group must have at least one hit in local text."""
    try:
        ck = str(canonical_key or "").lower()
        nm = str((spec or {}).get("name") or (spec or {}).get("metric_name") or "").lower()
        uf = str((spec or {}).get("unit_family") or "").lower()
        dim = str((spec or {}).get("dimension") or (spec or {}).get("value_type") or "").lower()
        base = ck + " " + nm

        groups = []

        # Geo cues
        if "china" in base or re.search(r"\bcn\b", base):
            groups.append(["china", "chinese"])

        # Time cues: if canonical key/name includes a year, require it locally
        years = re.findall(r"(19\d{2}|20\d{2})", base)
        for y in years[:2]:
            groups.append([y])

        # Metric family cues
        if ("charger" in base) or ("charging" in base):
            groups.append(["charger", "chargers", "charging", "station", "stations", "infrastructure"])
        if ("investment" in base) or ("capex" in base) or ("spend" in base) or (uf == "currency") or (dim == "currency"):
            groups.append(["investment", "invest", "capex", "spend", "spending", "cost", "expenditure"])
        if ("share" in base) or (uf == "percent") or (dim == "percent"):
            groups.append(["market share", "share", "%", "percent"])
        if ("sale" in base) or ("sales" in base) or (uf in ("unit_sales", "sales", "units")) or (dim in ("unit_sales", "sales", "units")):
            groups.append(["sales", "sold", "deliveries", "registrations", "units"])
        if "ytd" in base:
            groups.append(["ytd", "year to date"])

        # De-dupe groups
        out=[]
        for g in groups:
            gg=[]
            for t in g:
                tt=str(t or "").strip().lower()
                if tt and tt not in gg:
                    gg.append(tt)
            if gg:
                out.append(gg)
        return out
    except Exception:
        return []

def _fix2d2u_semantic_eligible(cand: dict, spec: dict, canonical_key: str) -> tuple:
    try:
        if not bool(globals().get("_FIX2D2U_ENABLE", True)):
            return True, ""
        groups = _fix2d2u_required_token_groups(canonical_key, spec)
        if not groups:
            return True, ""
        txt = _fix2d2u_local_text(cand)
        blob = _fix2d2u_norm(txt + " " + str(cand.get('raw') or ''))
        for g in groups:
            hit=False
            for tok in g:
                tn=_fix2d2u_norm(tok)
                if tn and tn in blob:
                    hit=True
                    break
            if not hit:
                return False, "missing_required_tokens"
        return True, ""
    except Exception:
        return True, ""

# - Keep names identical so evolution uses these as the LAST definitions

# Schema-only rebuild entrypoint (kept as the last definition for evolution dispatch)
def rebuild_metrics_from_snapshots_schema_only(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:  # noqa: F811
    """Guarded dispatch wrapper.

    FIX18 wrapper has been pruned (it depended on missing FIX17 functions).
    Keep a single, stable dispatch to FIX16 to avoid silent empty outputs.
    """
    try:
        fn = globals().get('rebuild_metrics_from_snapshots_schema_only_fix16')
        if callable(fn):
            return fn(prev_response, baseline_sources_cache, web_context=web_context)
        return {}
    except Exception:
        return {}

def _fix24_get_prev_full_payload(previous_data: dict) -> dict:
    """
    Load the FULL prior analysis payload from Google Sheets if possible.
    Falls back to previous_data if already full.
    """
    try:
        if not isinstance(previous_data, dict):
            return {}
        # If it already looks like a full payload (contains canonical metrics), return as-is
        if isinstance(previous_data.get("primary_metrics_canonical"), dict) and previous_data["primary_metrics_canonical"]:
            try:
                previous_data = _fix2d73_promote_rehydrated_prevdata_v1(previous_data)
            except Exception:
                pass
            return previous_data

        # Preferred: explicit snapshot_store_ref / full_store_ref
        ref = previous_data.get("full_store_ref") or previous_data.get("snapshot_store_ref") or ""
        # Fallback: sheet id
        if (not ref) and isinstance(previous_data.get("_sheet_id"), str) and previous_data.get("_sheet_id"):
            # Assume HistoryFull
            ref = f"gsheet:HistoryFull:{previous_data.get('_sheet_id')}"

        if isinstance(ref, str) and ref.startswith("gsheet:"):
            parts = ref.split(":")
            ws_title = parts[1] if len(parts) > 1 and parts[1] else "HistoryFull"
            aid = parts[2] if len(parts) > 2 else ""
            if aid:
                fn = globals().get("load_full_history_payload_from_sheet")
                if callable(fn):
                    full = fn(aid, worksheet_title=ws_title)
                    if isinstance(full, dict) and full:
                        try:
                            full = _fix2d73_promote_rehydrated_prevdata_v1(full)
                        except Exception:
                            pass
                        return full
    except Exception:
        return previous_data if isinstance(previous_data, dict) else {}

    return previous_data if isinstance(previous_data, dict) else {}

def _fix24_extract_source_urls(prev_full: dict) -> list:
    """
    Determine the URL list to fetch for current-hash computation.
    Uses analysis 'sources' if available, else URLs from baseline_sources_cache.
    """
    urls = []
    try:
        if isinstance(prev_full, dict):
            s = prev_full.get("sources")
            if isinstance(s, list) and s:
                urls = [str(u) for u in s if isinstance(u, str) and u.strip()]
            if not urls:
                # Try results.source_results urls
                r = prev_full.get("results") if isinstance(prev_full.get("results"), dict) else {}
                sr = r.get("source_results") if isinstance(r, dict) else None
                if isinstance(sr, list):
                    for item in sr:
                        if isinstance(item, dict):
                            u = item.get("url") or item.get("source_url")
                            if u:
                                urls.append(str(u))
            if not urls:
                # Try baseline_sources_cache urls
                r = prev_full.get("results") if isinstance(prev_full.get("results"), dict) else {}
                bsc = None
                if isinstance(r, dict):
                    bsc = r.get("baseline_sources_cache")
                if not isinstance(bsc, list):
                    bsc = prev_full.get("baseline_sources_cache")
                if isinstance(bsc, list):
                    for item in bsc:
                        if isinstance(item, dict):
                            u = item.get("source_url") or item.get("url")
                            if u:
                                urls.append(str(u))
    except Exception:
        pass

    # Stable de-dupe order
    seen = set()
    out = []
    for u in urls:
        uu = (u or "").strip()
        if not uu or uu in seen:
            continue
        seen.add(uu)
        out.append(uu)
    return out[:25]

def _fix24_build_scraped_meta(urls: list, max_chars_per_source: int = 180000) -> dict:
    """
    Fetch each URL (deterministically) and return scraped_meta in the same shape
    attach_source_snapshots_to_analysis expects: {url: {"status":..., "text":..., "extracted_numbers":[...]}}
    """
    _fix2af_ledger = globals().get("_fix2af_last_scrape_ledger")
    try:
        _fix2af_norm_urls, _fix2af_norm_diag = _fix2af_normalize_url_items(urls)
        urls = _fix2af_norm_urls
        if isinstance(_fix2af_ledger, dict):
            _fix2af_ledger["__fix2af_url_normalize_diag__"] = _fix2af_norm_diag
    except Exception:
        pass

    scraped_meta = {}
    fetch_fn = globals().get("fetch_url_content_with_status")
    extract_fn = globals().get("extract_numbers_with_context")

    for u in urls or []:
        url = str(u or "").strip()
        try:
            _fix2af_ledger_put(_fix2af_ledger, url, stage="attempted", reason="entered_loop")
        except Exception:
            pass
        if not url:
            continue
        try:
            if callable(fetch_fn):
                try:
                    if str(url).lower().endswith(".pdf"):
                        text, status = fetch_fn(url, force_pdf=True)
                    else:
                        text, status = fetch_fn(url)
                except TypeError:
                    text, status = fetch_fn(url)
            else:
                text, status = (None, "no_fetch_fn")

            txt = _fix2af_scraped_text_accessor(text)
            if max_chars_per_source and len(txt) > int(max_chars_per_source):
                txt = txt[: int(max_chars_per_source)]

            try:
                _fix2af_fail_class = _fix2af_classify_fetch_failure(status, txt)
                _fix2af_ledger_put(_fix2af_ledger, url, stage="fetched", reason=_fix2af_fail_class, extra={"status": status, "text_len": len(txt or "")})
            except Exception:
                pass

            nums = []
            if callable(extract_fn) and txt.strip():
                try:
                    nums = extract_fn(txt, source_url=url)
                    if nums is None:
                        nums = []
                except Exception:
                    pass
                    nums = []

            try:
                _fix2af_ledger_put(_fix2af_ledger, url, stage="extracted", reason="ok" if (isinstance(nums, list) and len(nums)>0) else "no_numbers", extra={"numbers_count": (len(nums) if isinstance(nums, list) else -1)})
            except Exception:
                pass

            scraped_meta[url] = {
                "status": status,
                "text": txt,
                "extracted_numbers": nums if isinstance(nums, list) else [],
                "fix2af_fetch_diag": {
                    "url_norm": _fix2af_norm_url(url),
                    "status": status,
                    "text_len": len(txt or ""),
                    "failure_class": _fix2af_classify_fetch_failure(status, txt),
                    "numbers_count": (len(nums) if isinstance(nums, list) else -1),
                },
            }
        except Exception as e:
            try:
                _fix2af_ledger_put(_fix2af_ledger, url, stage="exception", reason=type(e).__name__, extra={"msg": str(e)[:300]})
            except Exception:
                pass
            scraped_meta[url] = {"status": f"exception:{type(e).__name__}", "text": "", "extracted_numbers": [], "fix2af_fetch_diag": {"url_norm": _fix2af_norm_url(url), "status": f"exception:{type(e).__name__}", "text_len": 0, "failure_class": type(e).__name__, "numbers_count": 0}}

    return scraped_meta

def _fix24_baseline_sources_cache_from_scraped_meta(scraped_meta: dict) -> list:
    """
    Use attach_source_snapshots_to_analysis (existing deterministic normalizer) to produce
    baseline_sources_cache from scraped_meta, ensuring value_norm/unit_tag fields are present.
    """
    try:
        fn = globals().get("attach_source_snapshots_to_analysis")
        if not callable(fn):
            return []
        dummy = {"results": {}}
        web_context = {"scraped_meta": scraped_meta or {}}
        fn(dummy, web_context)
        r = dummy.get("results") if isinstance(dummy.get("results"), dict) else {}
        bsc = r.get("baseline_sources_cache") if isinstance(r, dict) else None
        return bsc if isinstance(bsc, list) else []
    except Exception:
        return []

def _fix24_get_prev_hashes(prev_full: dict) -> dict:
    """
    Extract prior snapshot hashes (v2 preferred) from a full analysis payload.
    """
    out = {"v2": "", "v1": ""}
    try:
        if not isinstance(prev_full, dict):
            return out
        out["v2"] = str(prev_full.get("source_snapshot_hash_v2") or "")
        out["v1"] = str(prev_full.get("source_snapshot_hash") or "")
        r = prev_full.get("results") if isinstance(prev_full.get("results"), dict) else {}
        if isinstance(r, dict):
            out["v2"] = out["v2"] or str(r.get("source_snapshot_hash_v2") or "")
            out["v1"] = out["v1"] or str(r.get("source_snapshot_hash") or "")
    except Exception:
        return out

    return out

def _fix24_compute_current_hashes(baseline_sources_cache: list) -> dict:
    """
    Compute current snapshot hashes (v2 preferred).
    """
    out = {"v2": "", "v1": ""}
    try:
        fn1 = globals().get("compute_source_snapshot_hash")
        fn2 = globals().get("compute_source_snapshot_hash_v2")
        if callable(fn2):
            out["v2"] = str(fn2(baseline_sources_cache) or "")
        if callable(fn1):
            out["v1"] = str(fn1(baseline_sources_cache) or "")
    except Exception:
        return out

    return out

def _fix24_make_replay_output(prev_full: dict, hashes: dict) -> dict:
    """
    Build a minimal evolution payload for the dashboard that reflects the prior analysis
    payload verbatim (no rebuild). This avoids the diff panel showing years by ensuring
    the "evolution column" is sourced from stored canonical metrics.
    """
    pmc = prev_full.get("primary_metrics_canonical") if isinstance(prev_full, dict) else {}
    pmc = pmc if isinstance(pmc, dict) else {}

    # Build a deterministic "no-change" metric_changes list WITHOUT re-selecting metrics.
    metric_changes = []
    try:
        for ckey in sorted(pmc.keys()):
            m = pmc.get(ckey) if isinstance(pmc.get(ckey), dict) else {}
            name = str(m.get("name") or m.get("metric_name") or ckey)
            v = m.get("value_norm", m.get("value"))
            unit = m.get("base_unit") or m.get("unit_tag") or m.get("unit") or ""
            metric_changes.append({
                "canonical_key": ckey,
                "name": name,
                "previous_value": v,
                "current_value": v,
                "previous_unit": unit,
                "current_unit": unit,
                "change_type": "unchanged",
                "confidence": 1.0,
            })
    except Exception:
        pass
        metric_changes = []

    return {
        "status": "ok",
        "mode": "replay_unchanged_fix24",
        "message": "Sources + data unchanged (hash match). Replaying prior analysis snapshot from Sheets.",
        "sources_checked": int(len(prev_full.get("sources") or [])) if isinstance(prev_full, dict) else 0,
        "sources_fetched": 0,
        "sources_failed": 0,
        "sources_skipped": 0,
        "source_results": [],
        "metric_changes": metric_changes,
        "change_stats": {
            "metrics_increased": 0,
            "metrics_decreased": 0,
            "metrics_unchanged": len(metric_changes),
            "metrics_total": len(metric_changes),
        },
        "debug": {
            "fix24": True,
            "prev_source_snapshot_hash_v2": hashes.get("prev_v2",""),
            "cur_source_snapshot_hash_v2": hashes.get("cur_v2",""),
            "prev_source_snapshot_hash": hashes.get("prev_v1",""),
            "cur_source_snapshot_hash": hashes.get("cur_v1",""),
            "hash_equal_v2": bool(hashes.get("prev_v2") and hashes.get("cur_v2") and hashes.get("prev_v2")==hashes.get("cur_v2")),
            "hash_equal_v1": bool(hashes.get("prev_v1") and hashes.get("cur_v1") and hashes.get("prev_v1")==hashes.get("cur_v1")),
        },
        # Provide the replay payload so the dashboard can render canonical metrics directly if desired
        "replay_analysis_payload": prev_full,
    }

# REFACTOR88 (HOTFIX): Restore/guard FIX2D55 prev-lift helper used by FIX24 recompute
# Why:
# - REFACTOR87 pruned a legacy ladder block which previously defined _fix2d55_apply_prev_lift.
# - The FIX24 changed-case evolution recompute path calls this helper before compute_source_anchored_diff.
# - Without it, Evolution shows: "FIX24: Evolution recompute failed (compute_source_anchored_diff path)."
# What:
# - Provide a small, deterministic "lift" that copies canonical maps/schema/anchors from any nested
# - Idempotent and safe: if nothing is found, it becomes a no-op.

def _fix2d55_apply_prev_lift(prev_full: dict, web_context: dict = None) -> None:
    try:
        if not isinstance(prev_full, dict):
            return

        # Find a primary_response container if present
        pr = None
        try:
            pr = prev_full.get("primary_response") if isinstance(prev_full.get("primary_response"), dict) else None
            if pr is None and isinstance(prev_full.get("results"), dict):
                pr = prev_full["results"].get("primary_response") if isinstance(prev_full["results"].get("primary_response"), dict) else None
        except Exception:
            pr = None

        # Locate canonical metrics map from common storage shapes
        pmc = None
        try:
            for cand in [
                prev_full.get("primary_metrics_canonical"),
                (prev_full.get("results") or {}).get("primary_metrics_canonical") if isinstance(prev_full.get("results"), dict) else None,
                (pr or {}).get("primary_metrics_canonical") if isinstance(pr, dict) else None,
                ((pr or {}).get("results") or {}).get("primary_metrics_canonical") if isinstance(pr, dict) and isinstance(pr.get("results"), dict) else None,
            ]:
                if isinstance(cand, dict) and cand:
                    pmc = cand
                    break
        except Exception:
            pmc = None

        # Locate schema + anchors similarly
        schema = None
        anchors = None
        try:
            for cand in [
                prev_full.get("metric_schema_frozen"),
                (prev_full.get("results") or {}).get("metric_schema_frozen") if isinstance(prev_full.get("results"), dict) else None,
                (pr or {}).get("metric_schema_frozen") if isinstance(pr, dict) else None,
                ((pr or {}).get("results") or {}).get("metric_schema_frozen") if isinstance(pr, dict) and isinstance(pr.get("results"), dict) else None,
            ]:
                if isinstance(cand, dict) and cand:
                    schema = cand
                    break
        except Exception:
            schema = None

        try:
            for cand in [
                prev_full.get("metric_anchors"),
                (prev_full.get("results") or {}).get("metric_anchors") if isinstance(prev_full.get("results"), dict) else None,
                (pr or {}).get("metric_anchors") if isinstance(pr, dict) else None,
                ((pr or {}).get("results") or {}).get("metric_anchors") if isinstance(pr, dict) and isinstance(pr.get("results"), dict) else None,
            ]:
                if isinstance(cand, dict) and cand:
                    anchors = cand
                    break
        except Exception:
            anchors = None

        # Apply lifts (additive only; do not overwrite non-empty)
        try:
            if isinstance(pmc, dict) and pmc:
                if not isinstance(prev_full.get("primary_metrics_canonical"), dict) or not prev_full.get("primary_metrics_canonical"):
                    prev_full["primary_metrics_canonical"] = pmc
                if isinstance(pr, dict):
                    if not isinstance(pr.get("primary_metrics_canonical"), dict) or not pr.get("primary_metrics_canonical"):
                        pr["primary_metrics_canonical"] = pmc
        except Exception:
            pass

        try:
            if isinstance(schema, dict) and schema:
                if not isinstance(prev_full.get("metric_schema_frozen"), dict) or not prev_full.get("metric_schema_frozen"):
                    prev_full["metric_schema_frozen"] = schema
                if isinstance(pr, dict):
                    if not isinstance(pr.get("metric_schema_frozen"), dict) or not pr.get("metric_schema_frozen"):
                        pr["metric_schema_frozen"] = schema
        except Exception:
            pass

        try:
            if isinstance(anchors, dict) and anchors:
                if not isinstance(prev_full.get("metric_anchors"), dict) or not prev_full.get("metric_anchors"):
                    prev_full["metric_anchors"] = anchors
                if isinstance(pr, dict):
                    if not isinstance(pr.get("metric_anchors"), dict) or not pr.get("metric_anchors"):
                        pr["metric_anchors"] = anchors
        except Exception:
            pass

        # Ensure primary_response container is present if we found one
        try:
            if isinstance(pr, dict):
                prev_full.setdefault("primary_response", pr)
        except Exception:
            pass

        # Debug stamp (safe)
        try:
            prev_full.setdefault("debug", {})
            if isinstance(prev_full.get("debug"), dict):
                prev_full["debug"].setdefault("fix2d55_prev_lift_v1", {})
                if isinstance(prev_full["debug"].get("fix2d55_prev_lift_v1"), dict):
                    prev_full["debug"]["fix2d55_prev_lift_v1"].update({
                        "lifted_pmc": bool(isinstance(pmc, dict) and pmc),
                        "lifted_schema": bool(isinstance(schema, dict) and schema),
                        "lifted_anchors": bool(isinstance(anchors, dict) and anchors),
                    })
        except Exception:
            pass
    except Exception:
        # Never block evolution recompute due to lift helper
        return

# Goal:
#   - Provide a stable gate for refactor/consolidation work.
#   - Executes: Analysis (headless) -> Evolution (source-anchored) and asserts invariants.
#
# Invocation:
#   python REFACTOR04_full_codebase_streamlit_safe.py --run_refactor_harness
#   or RUN_REFACTOR_HARNESS=1
#
# Optional env overrides:
#   REFACTOR_HARNESS_QUERY                      - analysis question text
#   REFACTOR_HARNESS_NUM_SOURCES               - int (default: 3)
#   REFACTOR_HARNESS_EXTRA_URLS                - newline-separated URLs (applied to both analysis+evolution)
#   REFACTOR_HARNESS_EXTRA_URLS_ANALYSIS       - newline-separated URLs (analysis only)
#   REFACTOR_HARNESS_EXTRA_URLS_EVOLUTION      - newline-separated URLs (evolution only)
#   REFACTOR_HARNESS_FORCE_REBUILD             - 1/0 (default: 1)
#   REFACTOR_HARNESS_REPORT_PATH               - directory path for JSON report (default: cwd)

#

# REFACTOR09: DIFF ENGINE CONSOLIDATION (WRAPPER)
#
# Purpose:
#   - Establish one stable, explicit implementation target for future extraction.
#   - Preserve current working behavior by delegating to the legacy implementation
#     captured at end-of-file time.
#   - Final bindings will point to this wrapper (single edit point).

def _refactor28_schema_only_rebuild_authoritative_v1(
    prev_response: dict,
    baseline_sources_cache: list,
    web_context: dict = None
) -> dict:
    """Schema-driven deterministic rebuild from cached snapshots only.

    This is intentionally minimal:
      - It does NOT attempt free-form metric discovery.
      - It ONLY populates metrics declared in the frozen schema.
      - Candidate selection is driven by schema fields (keywords + unit family/tag).
      - Deterministic sorting ensures stable output ordering.

    Returns:
      Dict[str, Dict] shaped like primary_metrics_canonical.
    """

    # Why:
    #   - When anchors are not used (anchor_used:false), schema-only rebuild can still
    #     select unit-less year tokens (e.g., 2024/2025) for currency/percent metrics.
    #   - This patch hard-rejects candidates with no token-level unit evidence when
    #     the schema (or canonical_key suffix) implies a unit is required.
    #   - Also optionally emits compact debug metadata for top candidates/rejections.
    # Determinism:
    #   - Pure filtering + stable ordering; no refetch; no randomness.

    def _fix33_schema_unit_required(spec_unit_family: str, spec_unit_tag: str, canonical_key: str) -> bool:
        uf = str(spec_unit_family or "").strip().lower()
        ut = str(spec_unit_tag or "").strip().lower()
        ck = str(canonical_key or "").strip().lower()
        # Explicit unit families
        if uf in {"currency", "percent", "rate", "ratio"}:
            return True
        if ut in {"%", "percent"}:
            return True
        # Unit-sales metrics require a unit (prevents bare-year selection)
        if ck.endswith("__unit_sales") or ck.endswith("__units") or ck.endswith("__unit"):
            return True
        # Canonical-key suffix conventions (backstop)
        if ck.endswith("__currency") or ck.endswith("__percent") or ck.endswith("__rate") or ck.endswith("__ratio"):
            return True
        return False

    def _fix33_candidate_has_unit_evidence(c: dict) -> bool:
        if not isinstance(c, dict):
            return False
        # Any explicit unit/currency/% evidence is enough to qualify as "has unit".
        if str(c.get("unit_tag") or "").strip():
            return True
        if str(c.get("unit_family") or "").strip():
            return True
        if str(c.get("base_unit") or "").strip():
            return True
        if str(c.get("unit") or "").strip():
            return True
        if str(c.get("currency_symbol") or c.get("currency") or "").strip():
            return True
        if bool(c.get("is_percent") or c.get("has_percent")):
            return True
        mk = str(c.get("measure_kind") or "").strip().lower()
        if mk in {"money", "percent", "percentage", "rate", "ratio"}:
            return True
        toks = c.get("unit_tokens") or c.get("unit_evidence_tokens") or []
        if isinstance(toks, (list, tuple)) and len(toks) > 0:
            return True
        return False

    _fix33_dbg = False
    try:
        _fix33_dbg = bool((web_context or {}).get("debug_evolution") or ((prev_response or {}).get("debug") or {}).get("debug_evolution"))
    except Exception:
        pass
        _fix33_dbg = False

    # Resolve frozen schema (supports multiple storage locations)
    schema = None
    try:
        if isinstance(prev_response, dict):
            if isinstance(prev_response.get("metric_schema_frozen"), dict):
                schema = prev_response.get("metric_schema_frozen")
            elif isinstance(prev_response.get("primary_response"), dict) and isinstance(prev_response["primary_response"].get("metric_schema_frozen"), dict):
                schema = prev_response["primary_response"].get("metric_schema_frozen")
            elif isinstance(prev_response.get("results"), dict) and isinstance(prev_response["results"].get("metric_schema_frozen"), dict):
                schema = prev_response["results"].get("metric_schema_frozen")
    except Exception:
        pass
        schema = None

    if not isinstance(schema, dict) or not schema:
        return {}

    # Collect candidates from snapshots (no re-fetch)
    candidates = []
    if isinstance(baseline_sources_cache, list):
        for src in baseline_sources_cache:
            if not isinstance(src, dict):
                continue
            nums = src.get("extracted_numbers")
            if not isinstance(nums, list) or not nums:
                continue
            for n in nums:
                if not isinstance(n, dict):
                    continue
                # Filter junk deterministically (strict rebuild exclusion)
                if _candidate_disallowed_for_metric(n, None):
                    continue
                # Normalize a few fields to ensure stable downstream access
                c = dict(n)
                if not c.get("source_url"):
                    c["source_url"] = src.get("url", "") or src.get("source_url", "") or ""
                candidates.append(c)

    # Deterministic candidate ordering (no set/dict iteration surprises)
    def _cand_sort_key(c: dict):
        return (
            str(c.get("source_url") or ""),
            str(c.get("anchor_hash") or ""),
            int(c.get("start_idx") or 0),
            str(c.get("raw") or ""),
            str(c.get("unit_tag") or ""),
            str(c.get("unit_family") or ""),
            float(c.get("value_norm") or c.get("value") or 0.0),
        )

    candidates.sort(key=_cand_sort_key)
    # FRESH02: optional deterministic freshness tie-break (default OFF).
    # When enabled (ENABLE_SOURCE_FRESHNESS_TIEBREAK), ties on keyword-hit score will prefer fresher sources.
    _fresh02_enabled = False
    _fresh02_src = "code:ENABLE_SOURCE_FRESHNESS_TIEBREAK"
    try:
        _fresh02_enabled, _fresh02_src = _yureeka_llm_flag_effective_v1("ENABLE_SOURCE_FRESHNESS_TIEBREAK")
    except Exception:
        try:
            _fresh02_enabled = bool(globals().get("ENABLE_SOURCE_FRESHNESS_TIEBREAK"))
        except Exception:
            _fresh02_enabled = False

    _fresh02_url2meta = {}
    _fresh02_sources_indexed = 0
    try:
        for _s in (baseline_sources_cache or []):
            if not isinstance(_s, dict):
                continue
            _u = _s.get("url") or _s.get("source_url") or ""
            if not _u:
                continue
            try:
                _un = _normalize_url(_u)
            except Exception:
                _un = str(_u).strip().lower()
            age = _s.get("freshness_age_days")
            conf = _s.get("freshness_date_confidence")
            sc = _s.get("freshness_score")
            # Compute score fallback from age+confidence if needed.
            sc_f = None
            try:
                sc_f = float(sc)
            except Exception:
                sc_f = None
            if sc_f is None:
                try:
                    age_i = int(float(age)) if age is not None else None
                except Exception:
                    age_i = None
                if age_i is not None:
                    try:
                        base = _fresh01_score_from_age_days_v1(age_i)
                    except Exception:
                        base = None
                    try:
                        cconf = float(conf) if conf is not None else 1.0
                    except Exception:
                        cconf = 1.0
                    cconf = max(0.0, min(1.0, cconf))
                    if base is not None:
                        sc_f = float(base) * cconf
            try:
                age_i2 = int(float(age)) if age is not None else None
            except Exception:
                age_i2 = None
            _fresh02_url2meta[_un] = {
                "freshness_score": sc_f,
                "freshness_age_days": age_i2,
                "published_at": _s.get("published_at") or _s.get("source_published_at") or "",
                "freshness_bucket": _s.get("freshness_bucket") or "",
                "freshness_method": _s.get("freshness_method") or "",
            }
            _fresh02_sources_indexed += 1
    except Exception:
        pass

    # FRESH02: initialize a run-level tie-breaker beacon (filled per metric when it changes the winner).
    # Note: This is deterministic and purely diagnostic unless ENABLE_SOURCE_FRESHNESS_TIEBREAK is ON.
    global _FRESH02_TIEBREAK_V1
    try:
        _FRESH02_TIEBREAK_V1 = {
            "enabled": bool(_fresh02_enabled),
            "flag_source": str(_fresh02_src or ""),
            "sources_indexed": int(_fresh02_sources_indexed or 0),
            "applied_count": 0,
            "applied_keys": [],
            "examples": [],
        }
    except Exception:
        pass

    # LLM37: tie-break quantization knob (only effective when freshness tie-break is enabled).
    _fresh02_min_score_delta = 0.0
    try:
        _fresh02_min_score_delta = float(_yureeka_hp_get_v1("freshness.tiebreak.min_score_delta", 0.0) or 0.0)
        if _fresh02_min_score_delta < 0.0:
            _fresh02_min_score_delta = 0.0
    except Exception:
        _fresh02_min_score_delta = 0.0

    def _fresh02_candidate_tie_key_v1(c: dict) -> tuple:
        """Return a tuple suitable for lexicographic ascending sort (smaller is better)."""
        if not _fresh02_enabled or not isinstance(c, dict):
            return tuple()
        u = str(c.get("source_url") or "").strip()
        if not u:
            return (9999, 999999)
        try:
            un = _normalize_url(u)
        except Exception:
            un = u.lower()
        meta = _fresh02_url2meta.get(un) or {}
        sc = meta.get("freshness_score")
        age = meta.get("freshness_age_days")
        try:
            sc_f = float(sc)
        except Exception:
            sc_f = None
        # Optional quantization: treat near-equal freshness scores as equal (min_score_delta).
        sc_eff = sc_f
        try:
            md = float(_fresh02_min_score_delta or 0.0)
        except Exception:
            md = 0.0
        if (sc_eff is not None) and (md is not None) and (md > 0.0):
            try:
                sc_eff = float(int(float(sc_eff) // float(md)) * float(md))
            except Exception:
                sc_eff = sc_f
        score_key = (-sc_eff) if sc_eff is not None else 9999
        try:
            age_i = int(age) if age is not None else 999999
        except Exception:
            age_i = 999999
        return (score_key, age_i)

    # REFACTOR118: define local normalizer BEFORE using it for year-anchor indexing
    def _ref100_norm_text_v1(s: str) -> str:
        try:
            return re.sub(r"\s+", " ", (s or "").lower()).strip()
        except Exception:
            return (s or "").lower().strip()
    # REFACTOR100: Pre-index source snapshot text by URL for year-anchor scanning.
    _ref100_src_text_by_url = {}
    try:
        if isinstance(baseline_sources_cache, list):
            for _src in baseline_sources_cache:
                if not isinstance(_src, dict):
                    continue
                _u = str(_src.get("url") or _src.get("source_url") or "").strip()
                if not _u:
                    continue
                _t = _src.get("snapshot_text_excerpt") or _src.get("snapshot_text") or _src.get("clean_text") or _src.get("text") or ""
                if not isinstance(_t, str) or not _t.strip():
                    continue
                _tn = _ref100_norm_text_v1(_t)
                if len(_tn) > len(_ref100_src_text_by_url.get(_u, "")):
                    _ref100_src_text_by_url[_u] = _tn
    except Exception:
        _ref100_src_text_by_url = {}

    if not candidates:
        return {}

    # Deterministic schema-driven selection
    def _norm_text(s: str) -> str:
        return re.sub(r"\s+", " ", (s or "").lower()).strip()

    # REFACTOR100: Year-anchor gating for year-explicit canonical keys.
    #   - If canonical_key contains one or more year tokens (e.g., 2026, 2040, 2025),
    #     prefer candidates whose source text contains ALL required years.
    #   - If no strong (year-complete) candidate exists, fall back to the best weak
    #     candidate and mark used_fallback_weak=True for traceability.
    def _refactor100_required_year_tokens_from_key(canonical_key: str) -> list:
        try:
            ck = str(canonical_key or "")
        except Exception:
            ck = ""
        years = []
        try:
            for y in re.findall(r"(?<!\d)(19\d{2}|20\d{2}|21\d{2})(?!\d)", ck):
                if y and (y not in years):
                    years.append(y)
        except Exception:
            years = []
        out = []
        for y in years:
            try:
                iv = int(y)
                if 1900 <= iv <= 2100:
                    out.append(str(iv))
            except Exception:
                continue
        return out

    def _refactor100_year_anchor_scan(candidate: dict, required_year_tokens: list, ctx_norm: str, src_text_by_url: dict) -> tuple:
        """Return (found_years_list, year_ok_bool) for candidate against required years."""
        if not required_year_tokens:
            return ([], True)
        try:
            c = candidate if isinstance(candidate, dict) else {}
            surl = str(c.get("source_url") or c.get("url") or "").strip()
        except Exception:
            c = {}
            surl = ""
        try:
            raw_norm = _norm_text(str(c.get("raw") or c.get("display_value") or ""))
        except Exception:
            raw_norm = ""
        try:
            page_norm = src_text_by_url.get(surl, "") if isinstance(src_text_by_url, dict) else ""
        except Exception:
            page_norm = ""
        try:
            ctx2 = _norm_text(str(c.get("context") or ""))
        except Exception:
            ctx2 = ""
        hay = " ".join([str(ctx_norm or ""), str(ctx2 or ""), str(raw_norm or ""), str(page_norm or "")]).lower()
        found = []
        for y in required_year_tokens:
            try:
                if re.search(r"(?<!\d)" + re.escape(str(y)) + r"(?!\d)", hay):
                    if str(y) not in found:
                        found.append(str(y))
            except Exception:
                try:
                    if str(y) in hay and str(y) not in found:
                        found.append(str(y))
                except Exception:
                    pass
        year_ok = len(found) == len(required_year_tokens)
        return (found, bool(year_ok))

    out = {}

    for canonical_key in sorted(schema.keys()):
        spec = schema.get(canonical_key) or {}
        if not isinstance(spec, dict):
            continue

        spec_keywords = spec.get("keywords") or []
        if not isinstance(spec_keywords, list):
            spec_keywords = []
        spec_keywords_norm = [str(k).lower().strip() for k in spec_keywords if str(k).strip()]
        # REFACTOR118: investment-by-2040 sources often phrase this as spend/spending/capex; bridge keywords ONLY for this schema key
        if canonical_key == "global_ev_charging_investment_2040__currency":
            _rf118_extra = ["investment", "investments", "spend", "spending", "expenditure", "capex", "total spend", "annual spend", "market size"]
            for _k in _rf118_extra:
                _kn = str(_k).lower().strip()
                if _kn and _kn not in spec_keywords_norm:
                    spec_keywords_norm.append(_kn)

        spec_unit_tag = str(spec.get("unit_tag") or spec.get("unit") or "").strip()
        spec_unit_family = str(spec.get("unit_family") or "").strip()
        _rf129_prec_pool_enabled = bool(str(canonical_key or '').strip() == 'global_ev_chargers_2040__unit_count')
        _rf129_prec_pool = [] if _rf129_prec_pool_enabled else None

        # Score candidates by schema keyword hits, then filter by unit constraints if present.

        # REFACTOR100: apply year-anchor gating for year-explicit canonical keys.

        best = None

        best_key = None

        best_strong = None

        best_strong_key = None

        best_strong_years = []

        best_weak = None

        best_weak_key = None

        best_weak_years = []

        _ref100_required_years = _refactor100_required_year_tokens_from_key(canonical_key)

        _ref100_top3_pairs = []  # (tie, summary_dict)

        # FRESH02: track the base (non-freshness) winner so we can detect when freshness changes the winner.
        _fresh02_base_best = None
        _fresh02_base_best_key = None
        _fresh02_base_best_strong = None
        _fresh02_base_best_strong_key = None
        _fresh02_base_best_strong_years = []
        _fresh02_base_best_weak = None
        _fresh02_base_best_weak_key = None
        _fresh02_base_best_weak_years = []


        _fix33_top = []
        _fix33_rej = {}

        for c in candidates:
            if _candidate_disallowed_for_metric(c, spec):
                continue
            if _refactor03_candidate_rejected_by_unit_family_v1(c, spec):
                continue
            try:
                if _refactor27_candidate_rejected_currency_date_fragment_v1(c, spec):
                    continue
            except Exception:
                pass

            # Why:
            # - Some sources contain many years (e.g., 2023, 2024) that can outscore true values.
            # - For currency-ish metrics, suppress candidates that look like bare years unless context clearly indicates money.
            # Determinism:
            # - Pure filter; does not invent candidates or refetch content.
            try:
                def _ai2_is_year_only(c: dict):
                    """Return True if candidate is a likely standalone year (1900-2100) with no unit."""
                    try:
                        c = c if isinstance(c, dict) else {}
                        # Prefer canonical numeric
                        v = c.get("value_norm")
                        if v is None:
                            v = c.get("value")
                        try:
                            iv = int(float(v))
                        except Exception:
                            return False
                        if iv < 1900 or iv > 2100:
                            return False
                        # Must be truly 4-digit (avoid 2023.5 etc)
                        try:
                            if abs(float(v) - float(iv)) > 1e-9:
                                return False
                        except Exception:
                            pass

                        # If the candidate itself signals time/year, do not treat as "junk year".
                        u = str(c.get("base_unit") or c.get("unit") or "").strip().lower()
                        ut = str(c.get("unit_tag") or "").strip().lower()
                        uf = str(c.get("unit_family") or "").strip().lower()
                        if "year" in u or "year" in ut or "year" in uf or "time" in uf:
                            return False

                        raw = str(c.get("raw") or "").strip()
                        sval = str(iv)

                        # - Some extractors store a wider raw window (e.g. includes '$721m ... in 2023')
                        # - Currency symbols elsewhere in raw should NOT make a year candidate non-year.
                        # - Only treat as non-year if the currency symbol is directly attached to the year.
                        try:
                            if re.search(r"(\$|usd|eur|gbp|aud|cad|sgd)\s*"+re.escape(sval)+r"\b", raw.lower()):
                                return False
                        except Exception:
                            pass

                        # If raw is basically just the year token (allow brackets/punctuation), it's year-only.
                        try:
                            raw2 = re.sub(r"[\s,.;:()\[\]{}<>]", "", raw)
                            if raw2 == sval:
                                return True
                        except Exception:
                            pass

                        # If raw contains multiple numbers, it's likely context; still treat as year-only
                        # when this candidate has no unit.
                        try:
                            nums = re.findall(r"\d{2,}", raw)
                            if len(nums) >= 2:
                                return True
                        except Exception:
                            pass

                        # If raw contains month names, likely a date; treat as year-only (we suppress dates too).
                        try:
                            if re.search(r"\b(jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)\b", raw.lower()):
                                return True
                        except Exception:
                            return True
                    except Exception:
                        return False
                def _ai2_schema_currencyish(sd: dict) -> bool:
                    try:
                        if not isinstance(sd, dict):
                            return False
                        u = str(sd.get('unit') or sd.get('base_unit') or '').lower()
                        if any(x in u for x in ('usd','sgd','eur','gbp','jpy','$','€','£')):
                            return True
                        # heuristic keywords on definition (safe, schema-driven-ish)
                        nm = str(sd.get('name') or '').lower()
                        if any(x in nm for x in ('revenue','sales','cost','price','capex','opex','investment','spend','spending','expenditure','value')):
                            return True
                        return False
                    except Exception:
                        return False

                _sd = locals().get('schema_def')
                if _ai2_schema_currencyish(_sd) and _ai2_is_year_only(c):
                    continue

                # Why: year tokens (e.g., 2025) can outrank true percent values when unit evidence is weak.
                # Safe: only suppress when candidate has no explicit unit and looks like a bare year.
                try:
                    if _ai2_is_year_only(c):
                        _sd_name = str((_sd or {}).get('name') or '').lower()
                        _sd_ckey = str((_sd or {}).get('canonical_key') or ckey or '').lower()
                        _sd_unit_tag = str((_sd or {}).get('unit_tag') or '').lower()
                        _sd_unit_family = str((_sd or {}).get('unit_family') or '').lower()
                        if ('cagr' in _sd_name) or ('cagr' in _sd_ckey) or (_sd_unit_tag in ('percent','pct')) or (_sd_unit_family in ('percent','ratio','rate')):
                            continue
                except Exception:
                    pass
            except Exception:
                pass

            ctx = _norm_text(c.get("context_snippet") or c.get("context") or "")
            if not ctx:
                continue

            # Keyword hits: schema-driven (no external heuristics)
            hits = 0
            if spec_keywords_norm:
                for kw in spec_keywords_norm:
                    if kw and kw in ctx:
                        hits += 1

            if spec_keywords_norm and hits == 0:
                continue

            # Unit constraints (only if schema declares them)
            if spec_unit_family:
                if str(c.get("unit_family") or "").strip() != spec_unit_family:
                    # allow a unit_tag-only match when family is missing in candidate
                    if not (spec_unit_tag and str(c.get("unit_tag") or "").strip() == spec_unit_tag):
                        continue

            if spec_unit_tag:
                # if a tag is specified, prefer exact tag matches
                if str(c.get("unit_tag") or "").strip() != spec_unit_tag:
                    # allow family match when tag differs
                    if not (spec_unit_family and str(c.get("unit_family") or "").strip() == spec_unit_family):
                        continue

            try:
                _vnorm = c.get("value_norm", None)
                if _vnorm is None:
                    _vnorm = c.get("value", None)
                _is_year = _is_yearish_value(_vnorm)
                _mk0 = str(c.get("measure_kind") or "").strip().lower()
                _cand_ut0 = str(c.get("unit_tag") or "").strip()
                _cand_fam0 = str(c.get("unit_family") or "").strip().lower()
                _is_pct0 = bool(c.get("is_percent") or c.get("has_percent") or (_cand_ut0 == "%") or (_cand_fam0 == "percent"))
                _has_curr0 = bool(str(c.get("currency_symbol") or c.get("currency") or "").strip())
                _has_unit_ev0 = bool(_cand_ut0 or _cand_fam0 or _is_pct0 or _has_curr0 or str(c.get("base_unit") or c.get("unit") or "").strip())
                if _is_year and (not _has_unit_ev0) and (not _is_pct0) and (not _has_curr0) and (_mk0 in ("magnitude_other", "count_units", "count", "number", "")):
                    try:
                        _fix41afc5_dbg2["rejected_year_only"] = int(_fix41afc5_dbg2.get("rejected_year_only", 0) or 0) + 1
                    except Exception:
                        pass
                    continue
            except Exception:
                pass

            try:
                _req = _fix33_schema_unit_required(spec_unit_family, spec_unit_tag, canonical_key)
                _has_unit_ev = _fix33_candidate_has_unit_evidence(c)
                # unit_sales keys represent quantities; they must never take a bare year token as the value.
                try:
                    if str(canonical_key or '').strip().lower().endswith('__unit_sales'):
                        _v = c.get('value_norm', None)
                        if _v is None:
                            _v = c.get('value', None)
                        if _is_yearish_value(_v):
                            if _fix33_dbg:
                                try:
                                    _fix33_rej['rejected_year_for_unit_sales'] = int(_fix33_rej.get('rejected_year_for_unit_sales', 0) or 0) + 1
                                except Exception:
                                    pass
                            continue
                except Exception:
                    pass

                if _req and not _has_unit_ev:
                    # Track rejection (debug)
                    if _fix33_dbg:
                        try:
                            _fix33_rej["missing_unit_required"] = int(_fix33_rej.get("missing_unit_required", 0) or 0) + 1
                        except Exception:
                            pass
                    continue

                # Track top candidates (debug)
                if _fix33_dbg:
                    try:
                        _fix33_top.append({
                            "raw": c.get("raw"),
                            "value_norm": c.get("value_norm"),
                            "unit_tag": c.get("unit_tag"),
                            "unit_family": c.get("unit_family"),
                            "base_unit": c.get("base_unit") or c.get("unit"),
                            "measure_kind": c.get("measure_kind"),
                            "hits": hits,
                            "has_unit_ev": bool(_has_unit_ev),
                            "source_url": c.get("source_url"),
                            "anchor_hash": c.get("anchor_hash"),
                        })
                    except Exception:
                        pass
            except Exception:
                pass

            # Deterministic tie-break:

            #   (-hits, then stable candidate identity tuple)

            # REFACTOR129: when schema expects unit=M, prefer explicit million cues and decimal precision.
            _rf129_spec_ut = str(spec_unit_tag or "").strip()
            _rf129_has_decimal = False
            _rf129_decimal_places = 0
            _rf129_has_million_cue = False
            try:
                _rraw = str(c.get("raw") or "")
                if re.search(r"\.\d", _rraw):
                    _rf129_has_decimal = True
                    try:
                        _mnum = re.search(r"(-?\d[\d,\s]*\.(\d+))", _rraw)
                        if _mnum and _mnum.group(2):
                            _rf129_decimal_places = int(len(_mnum.group(2)))
                    except Exception:
                        pass
                _ut = str(c.get("unit_tag") or c.get("unit") or "").strip().lower()
                if _ut in ("m", "mn", "million"):
                    _rf129_has_million_cue = True
                if (not _rf129_has_million_cue) and re.search(r"(million|mn)", _rraw, flags=re.I):
                    _rf129_has_million_cue = True
                if (not _rf129_has_million_cue) and re.search(r"(?<![A-Za-z])m(?![A-Za-z])", _rraw, flags=re.I):
                    _rf129_has_million_cue = True
            except Exception:
                pass

            if _rf129_spec_ut.upper() == "M":
                tie = (-hits, -int(bool(_rf129_has_million_cue)), -int(bool(_rf129_has_decimal)), -int(_rf129_decimal_places)) + _fresh02_candidate_tie_key_v1(c) + _cand_sort_key(c)
                # FRESH02: base tie (no freshness) for diagnostics
                tie_base = (-hits, -int(bool(_rf129_has_million_cue)), -int(bool(_rf129_has_decimal)), -int(_rf129_decimal_places)) + _cand_sort_key(c)

            else:
                # FRESH02: base tie (no freshness) for diagnostics (avoid stale tie_base leakage across schema keys)
                tie_base = (-hits,) + _cand_sort_key(c)
                tie = (-hits,) + _fresh02_candidate_tie_key_v1(c) + _cand_sort_key(c)


            # REFACTOR100: compute year_ok/year_found before tracker updates (fix ordering for freshness beacons)
            year_found, year_ok = ([], True)
            try:
                if _ref100_required_years:
                    year_found, year_ok = _refactor100_year_anchor_scan(c, _ref100_required_years, ctx, _ref100_src_text_by_url)
            except Exception:
                year_found, year_ok = ([], True)

# FRESH02: update base-winner trackers (no freshness key)
            try:
                if _fresh02_enabled:
                    if _ref100_required_years:
                        if year_ok:
                            if (_fresh02_base_best_strong is None) or (tie_base < _fresh02_base_best_strong_key):
                                _fresh02_base_best_strong = c
                                _fresh02_base_best_strong_key = tie_base
                                _fresh02_base_best_strong_years = list(year_found or [])
                        else:
                            if (_fresh02_base_best_weak is None) or (tie_base < _fresh02_base_best_weak_key):
                                _fresh02_base_best_weak = c
                                _fresh02_base_best_weak_key = tie_base
                                _fresh02_base_best_weak_years = list(year_found or [])
                    else:
                        if (_fresh02_base_best is None) or (tie_base < _fresh02_base_best_key):
                            _fresh02_base_best = c
                            _fresh02_base_best_key = tie_base
            except Exception:
                pass

            # REFACTOR129 beacon: collect precision candidates for chargers_2040 (compact)
            try:
                if _rf129_prec_pool_enabled and isinstance(_rf129_prec_pool, list) and len(_rf129_prec_pool) < 120:
                    _rf129_prec_pool.append({
                        "url": str(c.get("source_url") or ""),
                        "raw_value": str(c.get("raw") or ""),
                        "value_norm": c.get("value_norm"),
                        "hit_score": int(hits),
                        "has_decimal": bool(_rf129_has_decimal),
                        "decimal_places": int(_rf129_decimal_places),
                        "has_million_cue": bool(_rf129_has_million_cue),
                        "candidate_id": str(c.get("candidate_id") or ""),
                    })
            except Exception:
                pass

                        # Track top-3 candidates summary for year-anchored keys only (compact, deterministic)

            if _ref100_required_years:

                try:

                    _sum = {

                        "url": c.get("source_url"),

                        "score": int(hits),

                        "found_years": list(year_found),

                        "year_ok": bool(year_ok),
                        "freshness_score": None,
                        "freshness_age_days": None,
                        "published_at": "",
                        "freshness_bucket": "",

                    }

                    _u = str(_sum.get("url") or "").strip()
                    try:
                        if _u:
                            _un = _normalize_url(_u)
                            _fm = _fresh02_url2meta.get(_un) or {}
                            _sum["freshness_score"] = _fm.get("freshness_score")
                            _sum["freshness_age_days"] = _fm.get("freshness_age_days")
                            _sum["published_at"] = _fm.get("published_at") or ""
                            _sum["freshness_bucket"] = _fm.get("freshness_bucket") or ""
                    except Exception:
                        pass
                    _replaced = False
                    if _u:
                        for _i, (_t, _s) in enumerate(list(_ref100_top3_pairs)):
                            try:
                                if str((_s or {}).get("url") or "").strip() == _u:
                                    # Keep the better (lower) tie for the same URL
                                    if tie < _t:
                                        _ref100_top3_pairs[_i] = (tie, _sum)
                                    _replaced = True
                                    break
                            except Exception:
                                pass
                    if not _replaced:
                        _ref100_top3_pairs.append((tie, _sum))

                    _ref100_top3_pairs.sort(key=lambda p: p[0])

                    if len(_ref100_top3_pairs) > 3:

                        _ref100_top3_pairs = _ref100_top3_pairs[:3]

                except Exception:

                    pass

            if _ref100_required_years:

                if year_ok:

                    if best_strong is None or tie < best_strong_key:

                        best_strong = c

                        best_strong_key = tie

                        best_strong_years = list(year_found)

                else:

                    if best_weak is None or tie < best_weak_key:

                        best_weak = c

                        best_weak_key = tie

                        best_weak_years = list(year_found)

            else:

                if best is None or tie < best_key:

                    best = c

                    best_key = tie

        # REFACTOR113: hard year-anchor enforcement for year-stamped schema keys (no weak fallback).

        _ref100_used_fallback_weak = False

        _ref100_winner_years = []

        _ref100_winner_has_all_years = None

        _ref113_missing_reason = ""

        if _ref100_required_years:

            if isinstance(best_strong, dict):

                best = best_strong

                _ref100_winner_years = list(best_strong_years or [])

                _ref100_winner_has_all_years = True

            elif isinstance(best_weak, dict):

                # Weak winner exists but lacks required year tokens -> treat as missing.
                best = None

                _ref100_winner_years = list(best_weak_years or [])

                _ref100_winner_has_all_years = False

                _ref113_missing_reason = "year_anchor_missing_required_year_tokens"

            else:

                best = None

                _ref100_winner_has_all_years = False

                _ref113_missing_reason = "year_anchor_no_candidate"

        # REFACTOR113: For year-anchored schema keys, emit a placeholder row (value=None)
        # instead of incorrectly binding a weak candidate or dropping the row.
        if _ref100_required_years and not isinstance(best, dict):

            best = {}

        # REFACTOR129 beacon: precision tiebreak trace for chargers_2040
        try:
            if _rf129_prec_pool_enabled:
                _rf129_top = []
                if isinstance(_rf129_prec_pool, list):
                    _rf129_top = sorted(
                        _rf129_prec_pool,
                        key=lambda e: (
                            -int(e.get("hit_score", 0) or 0),
                            -int(bool(e.get("has_million_cue"))),
                            -int(bool(e.get("has_decimal"))),
                            -int(e.get("decimal_places", 0) or 0),
                            str(e.get("url") or ""),
                            str(e.get("raw_value") or ""),
                        )
                    )[:10]

                _rf129_chosen = {}
                try:
                    _rf129_chosen = {
                        "url": str(best.get("source_url") or ""),
                        "raw_value": str(best.get("raw") or ""),
                        "value_norm": best.get("value_norm"),
                    }
                    _rr = str(best.get("raw") or "")
                    _rf129_chosen["has_decimal"] = bool(re.search(r"\.\d", _rr))
                    _rf129_chosen["has_million_cue"] = bool(
                        re.search(r"(million|mn)", _rr, flags=re.I) or re.search(r"(?<![A-Za-z])m(?![A-Za-z])", _rr)
                    )
                    _dp = 0
                    _mnum = re.search(r"(-?\d[\d,\s]*\.(\d+))", _rr)
                    if _mnum and _mnum.group(2):
                        _dp = len(_mnum.group(2))
                    _rf129_chosen["decimal_places"] = int(_dp)
                except Exception:
                    pass

                globals()["_REFACTOR129_PRECISION_TIEBREAK_V1"] = {
                    "canonical_key": str(canonical_key or ""),
                    "spec_unit_tag": str(spec_unit_tag or ""),
                    "top_candidates": list(_rf129_top),
                    "chosen": dict(_rf129_chosen or {}),
                    "tie_rule": "unit=M => (-hits, -million_cue, -has_decimal, -decimal_places) + stable_key",
                }
        except Exception:
            pass

        if not isinstance(best, dict):

            continue

        # FRESH02: record when freshness tie-break changes the winner (diagnostic; only when enabled).
        try:
            if _fresh02_enabled and isinstance(best, dict) and isinstance(globals().get("_FRESH02_TIEBREAK_V1"), dict):
                _base_winner = None
                _base_years = []
                if _ref100_required_years:
                    if isinstance(_fresh02_base_best_strong, dict):
                        _base_winner = _fresh02_base_best_strong
                        _base_years = list(_fresh02_base_best_strong_years or [])
                    else:
                        _base_winner = None
                else:
                    if isinstance(_fresh02_base_best, dict):
                        _base_winner = _fresh02_base_best
                if isinstance(_base_winner, dict):
                    # If the selected candidate differs from the base winner, freshness must have changed the ordering.
                    if str(best.get("source_url") or "") != str(_base_winner.get("source_url") or "") or str(best.get("raw") or "") != str(_base_winner.get("raw") or ""):
                        _tb = globals().get("_FRESH02_TIEBREAK_V1") or {}
                        try:
                            _tb["applied_count"] = int(_tb.get("applied_count") or 0) + 1
                        except Exception:
                            _tb["applied_count"] = 1
                        try:
                            _k = str(canonical_key or "")
                            if _k and _k not in (_tb.get("applied_keys") or []):
                                _tb.setdefault("applied_keys", [])
                                _tb["applied_keys"].append(_k)
                        except Exception:
                            pass
                        try:
                            # Build a compact example for JSON audit
                            _burl = str(_base_winner.get("source_url") or "")
                            _curl = str(best.get("source_url") or "")
                            try:
                                _bmeta = _fresh02_url2meta.get(_normalize_url(_burl)) or _fresh02_url2meta.get(_burl.strip().lower()) or {}
                            except Exception:
                                _bmeta = _fresh02_url2meta.get(_burl.strip().lower()) or {}
                            try:
                                _cmeta = _fresh02_url2meta.get(_normalize_url(_curl)) or _fresh02_url2meta.get(_curl.strip().lower()) or {}
                            except Exception:
                                _cmeta = _fresh02_url2meta.get(_curl.strip().lower()) or {}
                            ex = {
                                "canonical_key": str(canonical_key or ""),
                                "base_url": _burl,
                                "chosen_url": _curl,
                                "base_freshness_score": _bmeta.get("freshness_score"),
                                "chosen_freshness_score": _cmeta.get("freshness_score"),
                                "base_age_days": _bmeta.get("freshness_age_days"),
                                "chosen_age_days": _cmeta.get("freshness_age_days"),
                                "base_published_at": _bmeta.get("published_at") or "",
                                "chosen_published_at": _cmeta.get("published_at") or "",
                                "required_year_tokens": list(_ref100_required_years or []),
                                "base_years": list(_base_years or []),
                            }
                            _tb.setdefault("examples", [])
                            if len(_tb["examples"]) < 5:
                                _tb["examples"].append(ex)
                        except Exception:
                            pass
                        globals()["_FRESH02_TIEBREAK_V1"] = _tb
        except Exception:
            pass

        # Emit a minimal canonical metric row (schema-driven, deterministic)
        metric = {
            "name": spec.get("name") or spec.get("canonical_id") or canonical_key,
            "value": best.get("value"),
            "unit": best.get("unit") or spec.get("unit") or "",
            "unit_tag": best.get("unit_tag") or spec.get("unit_tag") or "",
            "unit_family": best.get("unit_family") or spec.get("unit_family") or "",
            "currency_code": best.get("currency_code") or "",
            "base_unit": best.get("base_unit") or best.get("unit_tag") or spec.get("unit_tag") or "",
            "multiplier_to_base": best.get("multiplier_to_base") if best.get("multiplier_to_base") is not None else 1.0,
            "value_norm": best.get("value_norm") if best.get("value_norm") is not None else best.get("value"),
            "canonical_id": spec.get("canonical_id") or spec.get("canonical_key") or canonical_key,
            "canonical_key": canonical_key,
            "dimension": spec.get("dimension") or "",
            "original_name": spec.get("name") or "",
            "geo_scope": "unknown",
            "geo_name": "",
            "is_proxy": False,
            "proxy_type": "",
            "provenance": {
                "method": "schema_keyword_match",
                "best_candidate": {
                    "raw": best.get("raw"),
                    "source_url": best.get("source_url"),
                    "context_snippet": best.get("context_snippet"),
                    "anchor_hash": best.get("anchor_hash"),
                    "start_idx": best.get("start_idx"),
                    "end_idx": best.get("end_idx"),
                },
            },
        }

        # REFACTOR100: attach year-anchor selection provenance for traceability (only when applicable)
        try:
            if _ref100_required_years and isinstance(metric, dict):
                metric.setdefault("provenance", {})
                try:
                    top3 = [p[1] for p in (_ref100_top3_pairs or [])]
                except Exception:
                    top3 = []
                metric["provenance"]["selection_year_anchor_v1"] = {
                    "canonical_key": canonical_key,
                    "required_year_tokens": list(_ref100_required_years or []),
                    "winner_has_all_years": bool(_ref100_winner_has_all_years),
                    "winner_found_years": list(_ref100_winner_years or []),
                    "used_fallback_weak": bool(_ref100_used_fallback_weak),
                    "top3": list(top3 or []),
                }
                # LLM36: explicit per-metric freshness tie-break beacon (deterministic; flag-gated)
                try:
                    _ftb = {
                        "used": False,
                        "reason": "",
                        "flag_enabled": bool(_fresh02_enabled),
                        "changed_winner": False,
                        "a_score": None,
                        "b_score": None,
                        "a_freshness": None,
                        "b_freshness": None,
                        "a_age_days": None,
                        "b_age_days": None,
                        "a_published_at": "",
                        "b_published_at": "",
                        "a_bucket": "",
                        "b_bucket": "",
                        "winner_url": "",
                        "base_url": "",
                        "competitor_url": "",
                        "stable_fallback_key": "",
                    }

                    _top = list(top3 or [])
                    _tie = False
                    if len(_top) >= 2:
                        a0 = _top[0] if isinstance(_top[0], dict) else {}
                        b0 = _top[1] if isinstance(_top[1], dict) else {}
                        a_score = a0.get("score")
                        b_score = b0.get("score")
                        if (a0.get("year_ok") is True) and (b0.get("year_ok") is True) and (a_score not in (None, "")) and (b_score not in (None, "")):
                            try:
                                _tie = float(a_score) == float(b_score)
                            except Exception:
                                _tie = (a_score == b_score)

                    if not _tie:
                        _ftb["used"] = False
                        _ftb["reason"] = "no_score_tie"
                    elif not bool(_fresh02_enabled):
                        _ftb["used"] = False
                        _ftb["reason"] = "flag_disabled"
                    else:
                        _ftb["used"] = True

                        # Winner URL from the selected candidate (NOT from the metric shell)
                        win_url = ""
                        try:
                            win_url = str((best or {}).get("source_url") or (best or {}).get("url") or "")
                        except Exception:
                            win_url = ""
                        if not win_url:
                            try:
                                win_url = str((((metric.get("provenance") or {}).get("best_candidate") or {}).get("source_url")) or "")
                            except Exception:
                                win_url = ""

                        # Base (non-freshness) winner URL captured via tie_base tracking
                        base_url = ""
                        try:
                            base_best = None
                            if bool(_ref100_required_years):
                                if isinstance(_fresh02_base_best_strong, dict):
                                    base_best = _fresh02_base_best_strong
                                elif isinstance(_fresh02_base_best_weak, dict):
                                    base_best = _fresh02_base_best_weak
                            else:
                                if isinstance(_fresh02_base_best, dict):
                                    base_best = _fresh02_base_best
                            if isinstance(base_best, dict):
                                base_url = str(base_best.get("source_url") or base_best.get("url") or "")
                        except Exception:
                            base_url = ""

                        def _ftb_norm_url(u: str) -> str:
                            try:
                                return _normalize_url(u) if u else ""
                            except Exception:
                                return (u or "").strip().lower()

                        win_n = _ftb_norm_url(win_url)
                        base_n = _ftb_norm_url(base_url)

                        _ftb["winner_url"] = win_url
                        _ftb["base_url"] = base_url
                        _ftb["stable_fallback_key"] = base_n or win_n
                        _ftb["changed_winner"] = bool(base_n and win_n and (base_n != win_n))

                        # competitor URL: runner-up in top3 (useful even when winner changed)
                        try:
                            _ftb["competitor_url"] = str((_top[1] if (isinstance(_top, list) and len(_top) > 1 and isinstance(_top[1], dict)) else {}).get("url") or "")
                        except Exception:
                            _ftb["competitor_url"] = ""

                        if _ftb["changed_winner"]:
                            _ftb["reason"] = "score_tie_resolved_by_freshness"
                        else:
                            # More specific reason when freshness is missing/equal
                            try:
                                a0 = _top[0] if isinstance(_top[0], dict) else {}
                                b0 = _top[1] if isinstance(_top[1], dict) else {}
                                a_fs = a0.get("freshness_score")
                                b_fs = b0.get("freshness_score")
                                if (a_fs in (None, "")) or (b_fs in (None, "")):
                                    _ftb["reason"] = "score_tie_missing_freshness_stable_fallback"
                                else:
                                    try:
                                        _ftb["reason"] = "score_tie_freshness_equal_stable_fallback" if float(a_fs) == float(b_fs) else "score_tie_winner_unchanged"
                                    except Exception:
                                        _ftb["reason"] = "score_tie_winner_unchanged"
                            except Exception:
                                _ftb["reason"] = "score_tie_winner_unchanged"

                        # Attach top-2 score/freshness context (as seen in selection_year_anchor_v1.top3)
                        try:
                            a0 = _top[0] if isinstance(_top[0], dict) else {}
                            b0 = _top[1] if isinstance(_top[1], dict) else {}
                            _ftb["a_score"] = a0.get("score")
                            _ftb["b_score"] = b0.get("score")
                            _ftb["a_freshness"] = a0.get("freshness_score")
                            _ftb["b_freshness"] = b0.get("freshness_score")
                            _ftb["a_age_days"] = a0.get("freshness_age_days")
                            _ftb["b_age_days"] = b0.get("freshness_age_days")

                            # NLP07: alias age-days fields for grep/compat (keep existing a_age_days/b_age_days)
                            try:
                                if _ftb.get("a_freshness_age_days") is None:
                                    _ftb["a_freshness_age_days"] = _ftb.get("a_age_days")
                                if _ftb.get("b_freshness_age_days") is None:
                                    _ftb["b_freshness_age_days"] = _ftb.get("b_age_days")
                            except Exception:
                                pass
                            _ftb["a_published_at"] = a0.get("published_at") or ""
                            _ftb["b_published_at"] = b0.get("published_at") or ""
                            _ftb["a_bucket"] = a0.get("freshness_bucket") or ""
                            _ftb["b_bucket"] = b0.get("freshness_bucket") or ""
                        except Exception:
                            pass
                except Exception:
                    _ftb = {"used": False, "reason": "error", "flag_enabled": bool(_fresh02_enabled)}
                try:
                    metric.setdefault("provenance", {})
                    if isinstance(metric.get("provenance"), dict):
                        metric["provenance"]["fresh_tiebreak_v1"] = _ftb
                except Exception:
                    pass


        except Exception:
            pass
        # REFACTOR113: explicit missing reason when year-anchor gating blocks binding
        try:
            if _ref100_required_years and isinstance(metric, dict) and _ref113_missing_reason:
                metric.setdefault("provenance", {})
                metric["provenance"]["missing_reason_v1"] = str(_ref113_missing_reason)
        except Exception:
            pass

        try:
            if _fix33_dbg and isinstance(metric, dict):
                try:
                    _fix33_top_sorted = sorted(
                        _fix33_top,
                        key=lambda d: (-(int(d.get("hits") or 0)), str(d.get("value_norm") or ""), str(d.get("raw") or "")),
                    )
                except Exception:
                    pass
                    _fix33_top_sorted = _fix33_top
                metric.setdefault("provenance", {})
                metric["provenance"]["fix33_top_candidates"] = list(_fix33_top_sorted[:10])
                metric["provenance"]["fix33_rejected_reason_counts"] = dict(_fix33_rej or {})
        except Exception:
            pass
        out[canonical_key] = metric

    return out

# REFACTOR28: authoritative FIX16 schema-only wrapper (single definition; guard/duplicates removed in REFACTOR158).
def rebuild_metrics_from_snapshots_schema_only_fix16(prev_response, snapshot_pool, web_context=None):
    """Authoritative schema-only rebuild wrapper (REFACTOR28).

    Contract:
      - Deterministic selection from baseline snapshots (no re-fetch).
      - Preserves percent-year poisoning sanitization for percent keys.
      - Ensures currency date-fragment candidates (e.g., 'July 01, 2025') are not eligible.
    """
    rebuilt = {}
    try:
        rebuilt = _refactor28_schema_only_rebuild_authoritative_v1(prev_response, snapshot_pool, web_context=web_context)
    except Exception:
        rebuilt = {}

    # Preserve FIX2D86 sanitization: percent keys must not bind to bare year tokens.
    try:
        schema = {}
        if isinstance(prev_response, dict):
            schema = prev_response.get("metric_schema_frozen") or (prev_response.get("results") or {}).get("metric_schema_frozen") or {}
        if isinstance(rebuilt, dict) and rebuilt:
            rebuilt2, _sdbg = _fix2d86_sanitize_pmc_percent_year_tokens_v1(
                pmc=rebuilt,
                metric_schema_frozen=schema if isinstance(schema, dict) else {},
                label="schema_only_rebuild_refactor28_final",
            )
            rebuilt = rebuilt2

            # Preserve FIX2D82: percent keys must also have strong percent evidence (token/unit).
            try:
                fn82 = globals().get("_fix2d82_sanitize_pmc_percent_keys_v2")
                if callable(fn82) and isinstance(rebuilt, dict) and rebuilt:
                    rebuilt3, sdbg2 = fn82(rebuilt, metric_schema_frozen=schema if isinstance(schema, dict) else {}, label="schema_only_rebuild_fix16")
                    if isinstance(rebuilt3, dict):
                        rebuilt = rebuilt3
                    try:
                        if isinstance(prev_response, dict):
                            prev_response.setdefault("debug", {})
                            if isinstance(prev_response.get("debug"), dict):
                                prev_response["debug"]["fix2d82_percent_sanitize_schema_only"] = sdbg2
                    except Exception:
                        pass
            except Exception:
                pass
    except Exception:
        pass

    # FRESH02: publish tie-breaker beacon for audit (best-effort, additive).
    try:
        globals()["_FRESH02_TIEBREAK_V1"] = {
            "enabled": bool(_fresh02_enabled),
            "source": str(_fresh02_src or ""),
            "sources_indexed": int(_fresh02_sources_indexed or 0),
        }
    except Exception:
        pass

    return rebuilt

try:
    rebuild_metrics_from_snapshots_schema_only_fix16._fix2d86_wrapped = True  # type: ignore[attr-defined]
    rebuild_metrics_from_snapshots_schema_only_fix16._refactor28_authoritative = True  # type: ignore[attr-defined]
except Exception:
    pass

# Preserve legacy dispatch: schema-only entrypoint resolves to FIX16 wrapper.
try:
    globals()["rebuild_metrics_from_snapshots_schema_only_fix16"] = rebuild_metrics_from_snapshots_schema_only_fix16
except Exception:
    pass
try:
    globals()["rebuild_metrics_from_snapshots_schema_only"] = rebuild_metrics_from_snapshots_schema_only_fix16
except Exception:
    pass

# REFACTOR37 patch tracker
# REFACTOR38 patch tracker
# - We intentionally call main() only after ALL patch blocks and helper defs have executed,
#   so late overrides (diff engine, schema-only rebuild, etc.) are active during runs.
def _refactor134_run_source_anchored_evolution_core_v1(previous_data: dict, web_context: dict = None) -> dict:
    """
    PATCH FIX24 (ADDITIVE): Evolution flow is:
      1) Rehydrate prior full analysis payload from Sheets (HistoryFull)
      2) Scrape/fetch current sources to build scraped_meta + baseline_sources_cache_current
      3) Compute current snapshot hash (v2 preferred)
      4) If hash matches prior analysis: STOP and replay from Sheets (no rebuild/selection)
      5) If changed: proceed with the existing deterministic evolution path, but ensure
         it routes through the same snapshot/anchor deterministic plumbing used elsewhere.

    Note: This does NOT refactor existing evolution code; it wraps it.
    """
    # Step 1: Rehydrate prior payload
    prev_full = _fix24_get_prev_full_payload(previous_data or {})
    prev_hashes = _fix24_get_prev_hashes(prev_full)

    # Step 2: Build current scraped_meta by fetching the same URLs used previously
    urls = _fix24_extract_source_urls(prev_full)
    #
    # Purpose:
    # - Streamlit may provide injected URLs only via diagnostic fields
    #   (diag_extra_urls_ui / diag_extra_urls_ui_raw).
    # - Downstream evolution admission & fetch logic keys off web_context['extra_urls'].
    #
    # Behavior:
    # - If web_context['extra_urls'] is empty/missing, recover from (in order):
    #     1) web_context['diag_extra_urls_ui']     (list)
    #     2) web_context['diag_extra_urls_ui_raw'] (str; newline/comma separated)
    # - Normalize/canonicalize via _inj_diag_norm_url_list (tracking params stripped).
    #
    # Safety:
    # - Purely additive wiring; no effect when no injection is present.
    # - Never raises; falls back silently.
    try:
        if isinstance(web_context, dict):
            _wc_extra0 = web_context.get('extra_urls')
            _needs = (not isinstance(_wc_extra0, (list, tuple)) or not _wc_extra0)
            if _needs:
                _recovered = []
                # FIX2D66: also recover from extra_urls_ui_raw and question text
                try:
                    _qtxt = str((prev_full or {}).get('question') or (previous_data or {}).get('question') or '')
                    _more = _fix2d66_collect_injected_urls(web_context or {}, question_text=_qtxt)
                    if _more:
                        _recovered.extend(_more)
                except Exception:
                    pass
                _v_list = web_context.get('diag_extra_urls_ui')
                if isinstance(_v_list, (list, tuple)) and _v_list:
                    _recovered = list(_v_list)
                if not _recovered:
                    _raw = web_context.get('diag_extra_urls_ui_raw') or web_context.get('extra_urls_ui_raw')
                    if isinstance(_raw, str) and _raw.strip():
                        _parts = []
                        for _line in _raw.splitlines():
                            _line = (_line or '').strip()
                            if not _line:
                                continue
                            for _p in _line.split(','):
                                _p = (_p or '').strip()
                                if _p:
                                    _parts.append(_p)
                        if _parts:
                            _recovered = _parts
                if _recovered:
                    _recovered_norm = _inj_diag_norm_url_list(_recovered)
                    if _recovered_norm:
                        web_context['extra_urls'] = list(_recovered_norm)
                        # Also consider URLs embedded in the question text (last resort)

                        web_context.setdefault('debug', {})
                        if isinstance(web_context.get('debug'), dict):
                            web_context['debug'].setdefault('fix41afc3', {})
                            if isinstance(web_context['debug'].get('fix41afc3'), dict):
                                web_context['debug']['fix41afc3'].update({
                                    'extra_urls_recovered': True,
                                    'extra_urls_recovered_count': int(len(_recovered_norm)),
                                })
    except Exception:
        pass

    #
    # Goal:
    # - When Evolution UI provides injected URLs, route them through the SAME
    #   admission/normalization/dedupe logic used by analysis (fetch_web_context),
    #   but in IDENTITY-ONLY mode (no scraping).
    #
    # Why:
    # - Previously, injected URLs could appear in ui_norm/intake_norm but never
    #   reach the admission gate, yielding empty admission_decisions.
    #
    # Behavior:
    # - Only active when web_context['extra_urls'] is non-empty.
    # - Does NOT change fastpath logic directly; it only defines the "current URL
    #   universe" inputs (urls) used for hashing/scrape_meta building.
    #
    # Safety:
    # - identity_only=True prevents any network scrape inside fetch_web_context.
    # - Purely additive; if anything fails, it falls back to existing urls list.
    try:
        _evo_extra_urls_raw = (web_context or {}).get("extra_urls") or []
        _evo_extra_urls_norm = _inj_diag_norm_url_list(_evo_extra_urls_raw)
        if _evo_extra_urls_norm:
            _baseline_urls_for_fwc = _fix24_extract_source_urls(prev_full) or []
            _baseline_urls_for_fwc_norm = _inj_diag_norm_url_list(_baseline_urls_for_fwc)
            _q_for_fwc = str((prev_full or {}).get("question") or (previous_data or {}).get("question") or "").strip()
            _fwc = fetch_web_context(
                _q_for_fwc or "evolution_identity_only",
                num_sources=int(min(12, max(1, len(_baseline_urls_for_fwc_norm) + len(_evo_extra_urls_norm)))),
                fallback_mode=True,
                fallback_urls=_baseline_urls_for_fwc_norm,
                existing_snapshots=(prev_full or {}).get("baseline_sources_cache") or (prev_full or {}).get("baseline_sources_cache_v2") or None,
                extra_urls=_evo_extra_urls_norm,
                diag_run_id=str((web_context or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                diag_extra_urls_ui_raw=(web_context or {}).get("diag_extra_urls_ui_raw"),
                identity_only=True,
            ) or {}
            _fwc_admitted = _fwc.get("web_sources") or _fwc.get("sources") or []
            if isinstance(_fwc_admitted, list) and _fwc_admitted:
                urls = list(_fwc_admitted)
            # Attach the admission decisions to web_context for unified inj_trace reporting
            if isinstance(web_context, dict):
                if isinstance(_fwc.get("diag_injected_urls"), dict):
                    web_context.setdefault("diag_injected_urls", {})
                    if isinstance(web_context.get("diag_injected_urls"), dict):
                        # do not clobber if already present
                        for _k, _v in _fwc.get("diag_injected_urls").items():
                            web_context["diag_injected_urls"].setdefault(_k, _v)
                web_context.setdefault("debug", {})
                if isinstance(web_context.get("debug"), dict):
                    web_context["debug"].setdefault("evo_fwc_identity_only", {})
                    if isinstance(web_context["debug"].get("evo_fwc_identity_only"), dict):
                        web_context["debug"]["evo_fwc_identity_only"].update({
                            "called": True,
                            "baseline_urls_count": int(len(_baseline_urls_for_fwc_norm)),
                            "extra_urls_count": int(len(_evo_extra_urls_norm)),
                            "admitted_count": int(len(urls or [])),
                            "admitted_set_hash": _inj_diag_set_hash(_inj_diag_norm_url_list(urls or [])),
                        })
    except Exception:
        pass

    #
    # Problem observed (inj_trace_v1):
    # - Injected URL shows up in ui_norm/intake_norm, but can still vanish from admitted_norm/hash_inputs_norm.
    # - Root cause: the identity-only fetch_web_context() step may replace `urls` with an admitted list
    #   that excludes injected URLs, and later injected logic may read from a different variable/path.
    #
    # Goal:
    # - If injected URLs are present in web_context['extra_urls'], ensure they are ALWAYS merged into the
    #   local `urls` list used by _fix24_build_scraped_meta(), so the injected URLs are at least
    #   attempted (scraped_meta populated) and can become part of current hash identity when successful.
    #
    # Safety:
    # - Only active when injection is present.
    # - Purely additive: does not change hashing algorithm or fastpath rules; it only ensures the URL
    #   universe includes the injected URLs when the user provided them.
    # - Never raises.
    try:
        _fx9_wc = web_context if isinstance(web_context, dict) else {}
        _fx9_inj = _inj_diag_norm_url_list((_fx9_wc or {}).get("extra_urls") or [])
        if _fx9_inj and isinstance(urls, list):
            _fx9_seen = set(_inj_diag_norm_url_list([(_d.get("url") if isinstance(_d, dict) else _d) for _d in urls]))
            _fx9_added = []
            for _u in _fx9_inj:
                if _u in _fx9_seen:
                    continue
                _fx9_seen.add(_u)
                urls.append(_u)
                _fx9_added.append(_u)

            if isinstance(_fx9_wc, dict):
                _fx9_wc.setdefault("debug", {})
                if isinstance(_fx9_wc.get("debug"), dict):
                    _fx9_wc["debug"].setdefault("fix41afc9", {})
                    if isinstance(_fx9_wc["debug"].get("fix41afc9"), dict):
                        _fx9_wc["debug"]["fix41afc9"].update({
                            "merged_into_urls_universe": True,
                            "injected_urls_count": int(len(_fx9_inj)),
                            "added_to_urls_count": int(len(_fx9_added)),
                            "added_to_urls": list(_fx9_added),
                            "urls_count_after_merge": int(len(urls)),
                        })
    except Exception:
        pass

    #
    # Observation (from inj_trace_v1 in evolution JSON):
    # - Injected URL appears in ui_norm/intake_norm, but attempted/persisted remain empty,
    #   and the injected URL never reaches hash_inputs because evolution never performs a
    #   fetch cycle for the injected delta (it only replays cached snapshots).
    #
    # Goal:
    # - ONLY when injected URLs introduce a true delta vs the baseline source universe,
    #   run fetch_web_context() in normal mode (identity_only=False) so the injected URL
    #   is actually attempted/persisted and can become a first-class current source
    #   (and thus can affect downstream identity/hash inputs).
    #
    # Safety:
    # - No effect on no-injection runs.
    # - No effect when injection is empty or introduces no delta.
    # - Uses existing_snapshots to avoid re-fetching baseline sources.
    # - Never raises; falls back to existing behavior.
    try:
        _fix41afc6_wc = web_context if isinstance(web_context, dict) else {}
        _fix41afc6_inj = _inj_diag_norm_url_list((_fix41afc6_wc or {}).get("extra_urls") or [])
        _fix41afc6_base = _inj_diag_norm_url_list(_fix24_extract_source_urls(prev_full) or [])
        _fix41afc6_delta = sorted(list(set(_fix41afc6_inj) - set(_fix41afc6_base))) if _fix41afc6_inj else []
        if _fix41afc6_inj:
            _fix41afc6_q = str((prev_full or {}).get("question") or (previous_data or {}).get("question") or "").strip()
            _fix41afc6_prev_snap = (prev_full or {}).get("baseline_sources_cache") or (prev_full or {}).get("baseline_sources_cache_v2") or None

            _fix41afc6_fwc = fetch_web_context(
                _fix41afc6_q or "evolution_injection_fetch",
                num_sources=int(min(12, max(1, len(_fix41afc6_base) + len(_fix41afc6_inj)))),
                fallback_mode=True,
                fallback_urls=_fix41afc6_base,
                existing_snapshots=_fix41afc6_prev_snap,
                extra_urls=_fix41afc6_inj,
                diag_run_id=str((_fix41afc6_wc or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                diag_extra_urls_ui_raw=(_fix41afc6_wc or {}).get("diag_extra_urls_ui_raw"),
                force_scrape_extra_urls=True,
                force_admit_extra_urls=True,
                identity_only=False,
            ) or {}

            # Prefer the admitted list from fetch_web_context (it includes injected URLs that pass admission)
            _fix41afc6_admitted = _fix41afc6_fwc.get("web_sources") or _fix41afc6_fwc.get("sources") or []
            if isinstance(_fix41afc6_admitted, list) and _fix41afc6_admitted:
                urls = list(_fix41afc6_admitted)

            # Bubble up a small marker so inj_trace_v1 can report whether evolution actually called FWC
            if isinstance(web_context, dict):
                web_context["evolution_calls_fetch_web_context"] = True
                try:
                    # FIX2D56: record that injection fetch is allowed/enabled
                    web_context.setdefault("debug", {})
                    if isinstance(web_context.get("debug"), dict):
                        web_context["debug"].setdefault("fix2d56", {})
                        if isinstance(web_context["debug"].get("fix2d56"), dict):
                            web_context["debug"]["fix2d56"].update({
                                "force_fetch_injection": True,
                                "injected_urls_count": int(len(_fix41afc6_inj or [])),
                            })
                except Exception:
                    pass
                web_context.setdefault("debug", {})
                if isinstance(web_context.get("debug"), dict):
                    web_context["debug"].setdefault("fix41afc6", {})
                    if isinstance(web_context["debug"].get("fix41afc6"), dict):
                        web_context["debug"]["fix41afc6"].update({
                            "called_fetch_web_context": True,
                            "injected_delta_count": int(len(_fix41afc6_delta)),
                            "injected_delta": list(_fix41afc6_delta),
                            "admitted_count": int(len(_fix41afc6_admitted or [])) if isinstance(_fix41afc6_admitted, list) else 0,
                        })

                    try:
                        web_context.setdefault("debug", {})
                        if isinstance(web_context.get("debug"), dict):
                            web_context["debug"].setdefault("fix41afc8", {})
                            if isinstance(web_context["debug"].get("fix41afc8"), dict):
                                # delta URLs are what we intend to force-attempt
                                _fx8_delta_urls = list(_fix41afc6_delta or [])
                                # attempted/persist outcomes can be inferred from fetch_web_context scraped_meta
                                _fx8_results = {}
                                try:
                                    _fx8_sm = _fix41afc6_fwc.get("scraped_meta") or {}
                                    if isinstance(_fx8_sm, dict):
                                        for _u in _fx8_delta_urls:
                                            meta = _fx8_sm.get(_u) or {}
                                            if isinstance(meta, dict) and meta:
                                                _fx8_results[_u] = meta.get("status") or meta.get("fetch_status") or meta.get("reason") or "attempted"
                                            else:
                                                _fx8_results[_u] = "not_in_scraped_meta"
                                except Exception:
                                    pass
                                web_context["debug"]["fix41afc8"].update({
                                    "forced_fetch_reason": "injected_delta_present_force_fetch_even_if_not_admitted",
                                    "forced_fetch_urls": _fx8_delta_urls,
                                    "forced_fetch_count": int(len(_fx8_delta_urls)),
                                    "forced_fetch_results": _fx8_results,
                                })
                    except Exception:
                        pass

    except Exception:
        pass

    #
    # Problem observed in evolution JSON:
    # - ui_norm/intake_norm contains the injected URL (from Streamlit textarea),
    #   but web_context["extra_urls"] can still be empty at evolution core, which
    #   causes downstream "fetch injected delta" and "forced admit" patches to see
    #   an empty injected set and skip.
    #
    # Goal:
    # - If web_context["extra_urls"] is empty, recover injected URLs from the same
    #   Streamlit diagnostic fields used at intake:
    #     1) web_context["diag_extra_urls_ui"] (list)
    #     2) web_context["diag_extra_urls_ui_raw"] (string, newline/comma separated)
    # - Normalize/canonicalize deterministically via _inj_diag_norm_url_list().
    # - Latch the recovered list back into web_context["extra_urls"] so ALL later
    #   injected URL logic (fetch + forced admit + hash identity) sees the same set.
    #
    # Safety:
    # - Purely additive. No effect when extra_urls already present.
    # - Never raises; falls back silently.
    try:
        if isinstance(web_context, dict):
            _fix41afc7_norm = []
            _existing = web_context.get("extra_urls")
            _need = (not isinstance(_existing, (list, tuple)) or not list(_existing))
            if _need:
                _raw = []
                _v_list = web_context.get("diag_extra_urls_ui")
                if isinstance(_v_list, (list, tuple)) and _v_list:
                    _raw = list(_v_list)
                if not _raw:
                    _v_raw = web_context.get("diag_extra_urls_ui_raw")
                    if isinstance(_v_raw, str) and _v_raw.strip():
                        _parts = []
                        for _line in _v_raw.splitlines():
                            _line = (_line or "").strip()
                            if not _line:
                                continue
                            for _p in _line.split(","):
                                _p = (_p or "").strip()
                                if _p:
                                    _parts.append(_p)
                        if _parts:
                            _raw = _parts

                _fix41afc7_norm = _inj_diag_norm_url_list(_raw)
                if _fix41afc7_norm:
                    web_context["extra_urls"] = list(_fix41afc7_norm)

            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].setdefault("fix41afc7", {})
                if isinstance(web_context["debug"].get("fix41afc7"), dict):
                    web_context["debug"]["fix41afc7"].update({
                        "recovery_needed": bool(_need),
                        "recovered_extra_urls_count": int(len(_fix41afc7_norm or [])),
                        "recovered_extra_urls": list(_fix41afc7_norm or [])[:20],
                        "extra_urls_present_after_recovery": bool(isinstance(web_context.get("extra_urls"), (list, tuple)) and list(web_context.get("extra_urls") or [])),
                    })
    except Exception:
        pass

    # - Only active if caller provides web_context['extra_urls']
    # - Does NOT affect default fastpath behavior.
    _inj_diag_run_id = ""
    _inj_extra_urls = []
    try:
        _inj_diag_run_id = str((web_context or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo")
        _inj_extra_urls = _inj_diag_norm_url_list((web_context or {}).get("extra_urls") or [])
    except Exception:
        pass
        _inj_diag_run_id = _inj_diag_make_run_id("evo")
        _inj_extra_urls = []

    try:
        if _inj_extra_urls:
            _u_seen = set([str(u or "").strip() for u in (urls or []) if str(u or "").strip()])
            for _u in _inj_extra_urls:
                if _u not in _u_seen:
                    _u_seen.add(_u)
                    urls.append(_u)
    except Exception:
        pass

    #
    # Problem (observed in evolution JSON):
    # - Injected URL appears in ui_norm/intake_norm but is missing from admitted_norm,
    #   so it never reaches attempted/persisted/hash_inputs.
    # - This typically happens when admission/allowlist logic rejects injected URLs
    #   before the fetch loop, leaving attempted empty.
    #
    # Goal:
    # - ONLY when injection is present AND it introduces a true delta vs the baseline
    #   source universe, ensure the injected URLs are included in `urls` (the universe
    #   FIX24 uses for scrape_meta building).
    #
    # Safety:
    # - Purely additive; no effect when no injection or no delta.
    # - Does not modify fastpath logic/hashing; it only ensures injected URLs are
    #   present in the post-intake universe when delta exists.
    # - Never raises; falls back silently.
    try:
        _fix41afc4_inj_norm = _inj_diag_norm_url_list(_inj_extra_urls or [])
        _fix41afc4_base_norm = _inj_diag_norm_url_list(_fix24_extract_source_urls(prev_full) or [])
        _fix41afc4_delta = sorted(list(set(_fix41afc4_inj_norm) - set(_fix41afc4_base_norm))) if _fix41afc4_inj_norm else []
        _fix41afc4_applied = False

        if _fix41afc4_delta:
            # Determine expected URL container shape (strings vs dicts)
            _urls_list = urls if isinstance(urls, list) else []
            _urls_are_dicts = bool(_urls_list) and isinstance(_urls_list[0], dict)

            # Build a normalized "seen" set from existing urls
            if _urls_are_dicts:
                _seen_norm = set(_inj_diag_norm_url_list([(_d.get("url") if isinstance(_d, dict) else "") for _d in _urls_list]))
            else:
                _seen_norm = set(_inj_diag_norm_url_list(_urls_list))

            for _u in _fix41afc4_delta:
                if _u in _seen_norm:
                    continue
                _seen_norm.add(_u)
                if _urls_are_dicts:
                    _urls_list.append({"url": _u})
                else:
                    _urls_list.append(_u)
                _fix41afc4_applied = True

            urls = _urls_list  # rebind defensively

        if isinstance(web_context, dict):
            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].setdefault("fix41afc4", {})
                if isinstance(web_context["debug"].get("fix41afc4"), dict):
                    web_context["debug"]["fix41afc4"].update({
                        "forced_admit_applied": bool(_fix41afc4_applied),
                        "forced_admit_injected_urls_count": int(len(_fix41afc4_delta)),
                        "forced_admit_injected_urls": list(_fix41afc4_delta),
                        "baseline_urls_count": int(len(_fix41afc4_base_norm)),
                        "urls_after_forced_admit_count": int(len(_inj_diag_norm_url_list([(_d.get("url") if isinstance(_d, dict) else _d) for _d in (urls or [])] if isinstance(urls, list) else []))),
                    })
    except Exception:
        pass

    #
    # Why:
    # - When a URL appears in ui_norm/intake_norm but not in admitted_norm, we need
    #   an explicit reason before we change any behavior.
    #
    # What this records (debug only):
    # - Whether evolution is using fetch_web_context (it is not in FIX24 path)
    # - The pre- and post-injection URL universe
    # - Per-injected-URL admission decision + reason codes
    #
    # Safety:
    # - Does NOT alter control flow, fastpath eligibility, scraping, hashing, or selection.
    try:
        _urls_prev_full = _fix24_extract_source_urls(prev_full)
        _urls_prev_full_norm = _inj_diag_norm_url_list(_urls_prev_full or [])
        _urls_after_merge_norm = _inj_diag_norm_url_list(urls or [])

        _admission_decisions = {}
        for _u in (_inj_extra_urls or []):
            if _u in set(_urls_after_merge_norm):
                _admission_decisions[_u] = {
                    "decision": "admitted",
                    "reason_code": "merged_into_urls_for_scrape",
                }
            else:
                _admission_decisions[_u] = {
                    "decision": "rejected",
                    "reason_code": "not_present_in_urls_after_merge",
                }

        if isinstance(web_context, dict):
            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].setdefault("evo_injection_trace", {})
                if isinstance(web_context["debug"].get("evo_injection_trace"), dict):
                    web_context["debug"]["evo_injection_trace"].update({
                        "uses_fetch_web_context": False,
                        "urls_prev_full_count": int(len(_urls_prev_full_norm)),
                        "urls_prev_full_set_hash": _inj_diag_set_hash(_urls_prev_full_norm),
                        "urls_after_merge_count": int(len(_urls_after_merge_norm)),
                        "urls_after_merge_set_hash": _inj_diag_set_hash(_urls_after_merge_norm),
                        "inj_extra_urls_norm": list(_inj_extra_urls or []),
                        "inj_merge_applied": bool(_inj_extra_urls),
                        "inj_admission_decisions": _admission_decisions,
                    })

            # Also attach to diag_injected_urls for unified downstream reporting
            web_context.setdefault("diag_injected_urls", {})
            if isinstance(web_context.get("diag_injected_urls"), dict):
                web_context["diag_injected_urls"].setdefault("admission_decisions", {})
                if isinstance(web_context["diag_injected_urls"].get("admission_decisions"), dict):
                    web_context["diag_injected_urls"]["admission_decisions"].update(_admission_decisions)
                web_context["diag_injected_urls"].setdefault("urls_prev_full_norm", _urls_prev_full_norm)
                web_context["diag_injected_urls"].setdefault("urls_after_merge_norm", _urls_after_merge_norm)
    except Exception:
        pass

    #
    # Problem:
    # - Injected URLs can appear in UI intake but get dropped pre-admission, resulting in:
    #     attempted=0, persisted_norm=0, hash_inputs_norm unchanged.
    #
    # Goal:
    # - When injected URL DELTA exists (vs current urls baseline), deterministically:
    #     1) Force-admit injected delta into local `urls` universe (so downstream meta sees it)
    #     2) Force-fetch injected delta via fetch_web_context(force_scrape_extra_urls=True),
    #        so we get attempted/persisted entries or an explicit failure reason.
    #
    # Safety:
    # - No effect when no injection / no delta.
    # - Does not weaken normal fastpath logic (already bypassed upstream when delta exists).
    try:
        _fix41afc11_wc = web_context if isinstance(web_context, dict) else {}
        # Robust recovery (order required)
        _fix41afc11_extra = []
        if isinstance(_fix41afc11_wc.get("extra_urls"), (list, tuple)) and _fix41afc11_wc.get("extra_urls"):
            _fix41afc11_extra = list(_fix41afc11_wc.get("extra_urls") or [])
        elif isinstance(_fix41afc11_wc.get("diag_extra_urls_ui"), (list, tuple)) and _fix41afc11_wc.get("diag_extra_urls_ui"):
            _fix41afc11_extra = list(_fix41afc11_wc.get("diag_extra_urls_ui") or [])
        elif isinstance(_fix41afc11_wc.get("diag_extra_urls_ui_raw"), str) and (_fix41afc11_wc.get("diag_extra_urls_ui_raw") or "").strip():
            _raw = str(_fix41afc11_wc.get("diag_extra_urls_ui_raw") or "")
            _parts = []
            for _line in _raw.splitlines():
                _line = (_line or "").strip()
                if not _line:
                    continue
                for _p in _line.split(","):
                    _p = (_p or "").strip()
                    if _p:
                        _parts.append(_p)
            _fix41afc11_extra = _parts

        _fix41afc11_inj_norm = _inj_diag_norm_url_list(_fix41afc11_extra) if _fix41afc11_extra else []
        _fix41afc11_urls_norm = _inj_diag_norm_url_list(urls) if urls else []
        _fix41afc11_inj_set = set(_fix41afc11_inj_norm or [])
        _fix41afc11_base_set = set(_fix41afc11_urls_norm or [])
        _fix41afc11_delta = sorted(list(_fix41afc11_inj_set - _fix41afc11_base_set)) if _fix41afc11_inj_set else []

        if _fix41afc11_delta:
            # (1) Force-admit delta into local urls universe deterministically
            _added = []
            if urls and isinstance(urls[0], dict):
                _seen = set(_fix41afc11_urls_norm or [])
                for _u in _fix41afc11_delta:
                    if _u in _seen:
                        continue
                    urls.append({"url": _u, "source": "injected_force_admit", "is_injected": True})
                    _seen.add(_u)
                    _added.append(_u)
            else:
                _seen = set(_fix41afc11_urls_norm or [])
                for _u in _fix41afc11_delta:
                    if _u in _seen:
                        continue
                    urls.append(_u)
                    _seen.add(_u)
                    _added.append(_u)

            # (2) Must-fetch lane: force scrape extra urls even if not admitted by normal filter
            _q = str((prev_full or {}).get("question") or (previous_data or {}).get("question") or "evolution_injection_force_fetch").strip()
            _prev_snap = (prev_full or {}).get("baseline_sources_cache") or (prev_full or {}).get("baseline_sources_cache_v2") or None
            try:
                _fwc = fetch_web_context(
                    _q or "evolution_injection_force_fetch",
                    num_sources=int(min(12, max(1, len(_fix41afc11_urls_norm or []) + len(_fix41afc11_inj_norm or [])))),
                    fallback_mode=True,
                    fallback_urls=list(_fix41afc11_urls_norm or []),
                    existing_snapshots=_prev_snap,
                    extra_urls=list(_fix41afc11_inj_norm or []),
                    diag_run_id=str((_fix41afc11_wc or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                    diag_extra_urls_ui_raw=(_fix41afc11_wc or {}).get("diag_extra_urls_ui_raw"),
                    identity_only=False,
                    force_scrape_extra_urls=True,
                force_admit_extra_urls=True,
                ) or {}
            except TypeError:
                # Backward-compat: older fetch_web_context without force_scrape_extra_urls
                _fwc = fetch_web_context(
                    _q or "evolution_injection_force_fetch",
                    num_sources=int(min(12, max(1, len(_fix41afc11_urls_norm or []) + len(_fix41afc11_inj_norm or [])))),
                    fallback_mode=True,
                    fallback_urls=list(_fix41afc11_urls_norm or []),
                    existing_snapshots=_prev_snap,
                    extra_urls=list(_fix41afc11_inj_norm or []),
                    diag_run_id=str((_fix41afc11_wc or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                    diag_extra_urls_ui_raw=(_fix41afc11_wc or {}).get("diag_extra_urls_ui_raw"),
                    identity_only=False,
                ) or {}

            # If fetch_web_context returns a concrete web_sources list, prefer it for downstream scraped_meta
            _fwc_sources = _fwc.get("web_sources") or _fwc.get("sources") or None
            if isinstance(_fwc_sources, list) and _fwc_sources:
                urls = list(_fwc_sources)

            # Emit explicit debug fields
            if isinstance(web_context, dict):
                web_context.setdefault("debug", {})
                if isinstance(web_context.get("debug"), dict):
                    web_context["debug"].setdefault("fix41afc11", {})
                    if isinstance(web_context["debug"].get("fix41afc11"), dict):
                        web_context["debug"]["fix41afc11"].update({
                            "inj_force_admit_applied": True,
                            "inj_force_admit_count": int(len(_added)),
                            "inj_force_admit_urls": list(_added),
                            "inj_delta_count": int(len(_fix41afc11_delta)),
                            "inj_delta": list(_fix41afc11_delta),
                            "inj_must_fetch_called": True,
                            "inj_must_fetch_sources_count": int(len(_fwc_sources or [])) if isinstance(_fwc_sources, list) else 0,
                        })
    except Exception:
        pass
    # Goal:
    #   Ensure injected URLs are merged into the scrape/fetch URL universe in a shape-aware way.
    #   If urls is a list of dicts, append {"url": u}; otherwise append the string u.
    # Diagnostics:
    #   web_context.debug.fix2ae reports counts and added URLs.
    # Safety:
    #   Additive only. Never removes or reorders existing entries.
    try:
        _fx2ae_inj_raw = list(_inj_extra_urls or [])
        _fx2ae_inj_norm = _inj_diag_norm_url_list(_fx2ae_inj_raw) if _fx2ae_inj_raw else []
        _fx2ae_added = []
        _fx2ae_shape = "unknown"
        _fx2ae_urls_before = []
        _fx2ae_urls_after = []
        if isinstance(urls, list):
            _fx2ae_urls_before = [(_d.get("url") if isinstance(_d, dict) else _d) for _d in (urls or [])]
            _urls_are_dicts = bool(urls) and isinstance(urls[0], dict)
            _fx2ae_shape = "dicts" if _urls_are_dicts else "strings"
            _seen_norm = set(_inj_diag_norm_url_list(_fx2ae_urls_before))
            for _u in (_fx2ae_inj_norm or []):
                if not _u:
                    continue
                if _u in _seen_norm:
                    continue
                _seen_norm.add(_u)
                if _urls_are_dicts:
                    urls.append({"url": _u})
                else:
                    urls.append(_u)
                _fx2ae_added.append(_u)
            _fx2ae_urls_after = [(_d.get("url") if isinstance(_d, dict) else _d) for _d in (urls or [])]
        try:
            if isinstance(web_context, dict):
                web_context.setdefault("debug", {})
                if isinstance(web_context.get("debug"), dict):
                    web_context["debug"].setdefault("fix2ae", {})
                    if isinstance(web_context["debug"].get("fix2ae"), dict):
                        web_context["debug"]["fix2ae"].update({
                            "inj_count_raw": int(len(_fx2ae_inj_raw or [])),
                            "inj_count_norm": int(len(_fx2ae_inj_norm or [])),
                            "urls_shape": _fx2ae_shape,
                            "urls_before_count": int(len(_inj_diag_norm_url_list(_fx2ae_urls_before) or [])),
                            "urls_after_count": int(len(_inj_diag_norm_url_list(_fx2ae_urls_after) or [])),
                            "added_count": int(len(_fx2ae_added or [])),
                            "added_urls": list(_fx2ae_added or [])[:50],
                        })
        except Exception:
            pass
    except Exception:
        pass

    scraped_meta = _fix24_build_scraped_meta(urls)

    try:
        _fix2af_led = globals().get("_fix2af_last_scrape_ledger")
        if isinstance(web_context, dict) and isinstance(_fix2af_led, dict):
            web_context["fix2af_scrape_ledger_v1"] = _fix2af_led
    except Exception:
        pass

    # Step 3: Normalize into baseline_sources_cache and hash
    cur_bsc = _fix24_baseline_sources_cache_from_scraped_meta(scraped_meta)

    # Policy + wiring alignment for Evolution UI injected URLs
    #
    # Goal:
    # - Treat Evolution-tab injected URLs as part of the *current source universe*
    #   for hash identity *when they fetch successfully*, consistent with baseline
    #   sources (successful snapshots contribute to identity).
    #
    # Behavior (safe):
    # - If an injected URL was provided (web_context['extra_urls']) and its scrape
    #   status is success, it must appear in cur_bsc so that hash inputs can include it.
    # - If success-but-missing occurs (unexpected), we add a synthetic url-only entry
    #   tagged for hash identity (debug only; no numbers).
    # - If fetch failed, we do NOT force hash mismatch (consistent with policy),
    #   but we record explicit attempted status + reason into diagnostics.
    #
    # Safety:
    # - Does NOT alter fastpath eligibility logic directly; it only ensures that
    #   the identity inputs reflect the actual successfully fetched current sources.
    # - Purely additive; never removes or refactors existing logic.
    try:
        _inj_sm = scraped_meta if isinstance(scraped_meta, dict) else {}
        _inj_attempted_rows = []
        _inj_success_urls = set()
        for _u in (_inj_extra_urls or []):
            _m = _inj_sm.get(_u) if isinstance(_inj_sm.get(_u), dict) else {}
            _st = str(_m.get('status') or _m.get('fetch_status') or '')
            _reason = str(_m.get('status_detail') or _m.get('fail_reason') or '')
            _clen = _m.get('clean_text_len') or _m.get('content_len') or 0
            _inj_attempted_rows.append({
                'url': _u,
                'status': _st or 'unknown',
                'reason': _reason,
                'content_len': int(_clen) if str(_clen).isdigit() else 0,
            })
            if (_st or '').lower() in ('success','ok','fetched'):
                _inj_success_urls.add(_u)

        # Ensure success injected urls are represented in cur_bsc (hash identity)
        if _inj_success_urls and isinstance(cur_bsc, list):
            _bsc_urls = set()
            for _row in cur_bsc:
                if isinstance(_row, dict):
                    _bu = str(_row.get('url') or _row.get('source_url') or '').strip()
                    if _bu:
                        _bsc_urls.add(_bu)
            _missing_success = sorted(list(_inj_success_urls - _bsc_urls))
            for _u in _missing_success:
                cur_bsc.append({
                    'url': _u,
                    'status': 'success',
                    'status_detail': 'synthetic_success_missing_in_bsc',
                    'clean_text': '',
                    'clean_text_len': 0,
                    'extracted_numbers': [],
                    'numbers_found': 0,
                    'fingerprint': 'synthetic_url_only_for_hash',
                    'is_synthetic_for_hash': True,
                })

        # Write attempted status into web_context diagnostics for transparency
        if isinstance(web_context, dict):
            web_context.setdefault('diag_injected_urls', {})
            if isinstance(web_context.get('diag_injected_urls'), dict):
                web_context['diag_injected_urls'].setdefault('attempted', [])
                # Only overwrite if empty to avoid clobbering richer traces
                if not web_context['diag_injected_urls'].get('attempted'):
                    web_context['diag_injected_urls']['attempted'] = _inj_attempted_rows
                web_context['diag_injected_urls']['success_urls'] = sorted(list(_inj_success_urls))
    except Exception:
        pass

    cur_hashes = _fix24_compute_current_hashes(cur_bsc)

    try:
        _hash_inputs = _inj_diag_hash_inputs_from_bsc(cur_bsc)
        if isinstance(web_context, dict):
            web_context.setdefault("diag_injected_urls", {})
            if isinstance(web_context.get("diag_injected_urls"), dict):
                web_context["diag_injected_urls"].update({
                    "run_id": _inj_diag_run_id,
                    "ui_raw": (web_context or {}).get("diag_extra_urls_ui_raw") or "",
                    "ui_norm": _inj_extra_urls,
                    "intake_norm": _inj_extra_urls,
                    "admitted": list(urls or []),
                    "hash_inputs": _hash_inputs,
                    "injected_in_hash_inputs": sorted(list(set(_inj_extra_urls) & set(_hash_inputs))),
                    "set_hashes": {
                        "hash_inputs": _inj_diag_set_hash(_hash_inputs),
                        "admitted": _inj_diag_set_hash(list(urls or [])),
                    }
                })
    except Exception:
        pass

    # Step 4: Compare (v2 preferred)
    equal_v2 = bool(prev_hashes.get("v2") and cur_hashes.get("v2") and prev_hashes["v2"] == cur_hashes["v2"])
    equal_v1 = bool(prev_hashes.get("v1") and cur_hashes.get("v1") and prev_hashes["v1"] == cur_hashes["v1"])
    unchanged = equal_v2 or (not prev_hashes.get("v2") and equal_v1)

    # If the UI (or caller) requests force_rebuild, we intentionally bypass
    # the unchanged fastpath even if hashes match, to exercise rebuild logic.
    _force_rebuild = False
    _fix41_injection_force_rebuild = False
    try:
        _force_rebuild = bool((web_context or {}).get("force_rebuild"))
    except Exception:
        pass
        _force_rebuild = False

    # REFACTOR201: injection mode always forces a rebuild (bypass unchanged fastpath reuse).
    try:
        if bool(_yureeka_is_injection_mode_v1(web_context)):
            _force_rebuild = True
            _fix41_injection_force_rebuild = True
    except Exception:
        pass
    if _force_rebuild:
        unchanged = False
        _fix41_force_rebuild_honored = True
    else:
        _fix41_force_rebuild_honored = False

    if unchanged:
        hashes = {
            "prev_v2": prev_hashes.get("v2",""),
            "cur_v2": cur_hashes.get("v2",""),
            "prev_v1": prev_hashes.get("v1",""),
            "cur_v1": cur_hashes.get("v1",""),
        }
        out_replay = _fix24_make_replay_output(prev_full, hashes)
        try:
            if isinstance(out_replay, dict):
                out_replay.setdefault("code_version", _yureeka_get_code_version())
                out_replay.setdefault("debug", {}).setdefault("fix41", {})
                out_replay["debug"]["fix41"].update({
                    "force_rebuild_seen": bool(_fix41_force_rebuild_seen),
                    "force_rebuild_honored": bool(locals().get("_fix41_force_rebuild_honored", False)),
                    "injection_force_rebuild": bool(locals().get("_fix41_injection_force_rebuild", False)),
                    "path": "replay_unchanged",
                })
        except Exception:
            pass

        # Why:
        # - The FIX24 replay path returns early (skipping compute_source_anchored_diff),
        #   which previously meant results.debug.inj_trace_v1 might be missing.
        # - We need injected-URL lifecycle visibility even when hashes match (fastpath/replay)
        #   to validate UI wiring and to explain why a mismatch did/did not occur.
        # Safety:
        # - Pure debug emission only; does NOT affect hash logic, scraping, or fastpath decisions.
        try:
            _wc = web_context if isinstance(web_context, dict) else {}
            _diag = _wc.get("diag_injected_urls") if isinstance(_wc.get("diag_injected_urls"), dict) else {}

            # Populate attempted/persisted for injected URLs from scraped_meta/cur_bsc
            # even when replay fastpath returns early.
            # Also attach an explicit reason when injected URLs are present but not
            # admitted/hashed due to replay semantics.
            try:
                if isinstance(_diag, dict):
                    # Enrich from scraped_meta (injected only) and from BSC (all)
                    _diag = _inj_trace_v1_enrich_diag_from_scraped_meta(_diag, scraped_meta, (_inj_extra_urls or []))
                    _diag = _inj_trace_v1_enrich_diag_from_bsc(_diag, cur_bsc if isinstance(cur_bsc, list) else [])
                    # Explain replay semantics when UI extras exist
                    _ui_norm = _inj_diag_norm_url_list(_diag.get("ui_norm") or [])
                    if _ui_norm:
                        _diag.setdefault("admission_reason", "fastpath_replay_no_rebuild_no_admission")
                        _diag.setdefault("injection_effective", False)
            except Exception:
                pass

            _hash_inputs_replay = _inj_diag_hash_inputs_from_bsc(cur_bsc)
            _trace_replay = _inj_trace_v1_build(
                diag_injected_urls=_diag,
                hash_inputs=_hash_inputs_replay,
                stage="evolution",
                path="fastpath_replay",
                rebuild_pool=None,
                rebuild_selected=None,
            )
            out_replay.setdefault("results", {})
            if isinstance(out_replay.get("results"), dict):
                out_replay["results"].setdefault("debug", {})
                if isinstance(out_replay["results"].get("debug"), dict):
                    out_replay["results"]["debug"]["inj_trace_v1"] = _trace_replay
        except Exception:
            pass

        return out_replay
    # Step 5: Changed -> run deterministic evolution diff using existing machinery.
    # Provide web_context with scraped_meta so compute_source_anchored_diff can reconstruct snapshots deterministically.
    wc = {"scraped_meta": scraped_meta}
    # so downstream diff/rebuild logic can record provenance if needed.
    try:
        if isinstance(web_context, dict):
            wc.update({k: v for k, v in web_context.items() if k != "scraped_meta"})
    except Exception:
        pass

    # REFACTOR109: Ensure compute_source_anchored_diff can access the rebuilt *current* pool
    # (Otherwise FIX42 can silently fall back to baseline_sources_cache as 'current'.)
    try:
        wc["current_baseline_sources_cache"] = cur_bsc
        wc["current_sources_cache"] = cur_bsc
        wc["diag_current_sources_cache"] = cur_bsc
        wc.setdefault("debug", {})
        if isinstance(wc.get("debug"), dict):
            wc["debug"]["refactor109_current_pool_wired"] = True
            wc["debug"]["refactor109_current_pool_size"] = len(cur_bsc) if isinstance(cur_bsc, dict) else None
    except Exception:
        pass

    # REFACTOR67: changed-case recompute routes directly through compute_source_anchored_diff

    fn = globals().get("compute_source_anchored_diff")
    if callable(fn):
        try:
            # FIX2D55: lift prev_full onto schema keys BEFORE diff computation

            try:
                _fix2d55_apply_prev_lift(prev_full, wc)
            except Exception:
                pass

            out_changed = fn(prev_full, web_context=wc)
            try:
                if isinstance(out_changed, dict):
                    out_changed.setdefault("code_version", _yureeka_get_code_version())
                    out_changed.setdefault("debug", {}).setdefault("fix41", {})
                    out_changed["debug"]["fix41"].update({
                        "force_rebuild_seen": bool(_fix41_force_rebuild_seen),
                        "force_rebuild_honored": bool(locals().get("_fix41_force_rebuild_honored", False)) or bool(_fix41_force_rebuild_seen),
                        "injection_force_rebuild": bool(locals().get("_fix41_injection_force_rebuild", False)),
                        "path": "changed_compute_source_anchored_diff",
                    })
            except Exception:
                pass

            # REFACTOR67: always mark changed recompute path with FIX24 debug (no base runner branch)
            try:
                if isinstance(out_changed, dict):
                    out_changed.setdefault("debug", {})
                    if isinstance(out_changed.get("debug"), dict):
                        out_changed["debug"]["fix24"] = True
                        out_changed["debug"]["fix24_mode"] = "recompute_changed"
                        out_changed["debug"]["prev_source_snapshot_hash_v2"] = prev_hashes.get("v2", "")
                        out_changed["debug"]["cur_source_snapshot_hash_v2"] = cur_hashes.get("v2", "")
                        out_changed["debug"]["prev_source_snapshot_hash"] = prev_hashes.get("v1", "")
                        out_changed["debug"]["cur_source_snapshot_hash"] = cur_hashes.get("v1", "")
            except Exception:
                pass

            return out_changed
        except Exception:
            return {
        "status": "failed",
        "message": "FIX24: Evolution recompute failed (compute_source_anchored_diff path).",
        "sources_checked": len(urls),
        "sources_fetched": len(urls),
        "metric_changes": [],
        "debug": {"fix24": True, "fix24_mode": "recompute_failed"},
    }

try:
    _REFACTOR37_RUN_SOURCE_ANCHORED_EVOLUTION_IMPL = _refactor134_run_source_anchored_evolution_core_v1
except Exception:
    _REFACTOR37_RUN_SOURCE_ANCHORED_EVOLUTION_IMPL = None

#
# Motivation:
# - Some late patch blocks may overwrite results['baseline_sources_cache'] with an injected-only
#   placeholder row (while results['baseline_sources_cache_current'] still contains the full
#   current pool). This can confuse UI diagnostics and harness messaging.
#
# Behavior:
# - If baseline_sources_cache_current (or source_results) is a list with more rows than
#   baseline_sources_cache, widen baseline_sources_cache to match the full current pool.
# - Debug-only stamp under results.debug.refactor83_source_cache_normalize_v1 when applied.
#
# Safety:
# - Does NOT affect metric selection/diffing/stability. Payload-only normalization.
# - Never raises.
def _refactor83_normalize_evolution_source_caches_v1(payload: dict) -> dict:
    if not isinstance(payload, dict):
        return payload

    # REFACTOR85: Some evolution payloads have a nested "results" mirror dict
    # (e.g., res["results"]={"code_version", ...}) while the real results envelope
    # lives at the top-level of this dict. Prefer the dict that actually contains
    # evolution-run fields like status/metric_changes/source caches.
    _inner = payload.get("results") if isinstance(payload.get("results"), dict) else None
    _payload_looks_like_results = bool(
        isinstance(payload.get("status"), str)
        or isinstance(payload.get("metric_changes"), list)
        or isinstance(payload.get("baseline_sources_cache_current"), list)
        or isinstance(payload.get("source_results"), list)
    )
    if _payload_looks_like_results:
        res = payload
    elif isinstance(_inner, dict):
        res = _inner
    else:
        res = payload
    if not isinstance(res, dict):
        return payload

    cur = res.get("baseline_sources_cache_current")
    if not isinstance(cur, list) or not cur:
        cur = res.get("source_results")
    base = res.get("baseline_sources_cache")

    if isinstance(cur, list):
        base_list = base if isinstance(base, list) else []
        if len(cur) > len(base_list):
            try:
                res["baseline_sources_cache"] = cur
            except Exception:
                pass
            try:
                dbg = res.setdefault("debug", {})
                if isinstance(dbg, dict):
                    d = dbg.setdefault("refactor83_source_cache_normalize_v1", {})
                    if isinstance(d, dict):
                        d.update({
                            "applied": True,
                            "before_baseline_sources_cache_rows": int(len(base_list)),
                            "after_baseline_sources_cache_rows": int(len(cur)),
                            "reason": "widen_baseline_sources_cache_to_match_current_pool",
                        })
            except Exception:
                pass

    # REFACTOR113: normalize per-source status when status_detail indicates success
    try:
        _keys = ["baseline_sources_cache_current", "source_results", "baseline_sources_cache"]
        _touched = 0
        _changed = 0
        for _k in _keys:
            rows = res.get(_k)
            if not isinstance(rows, list):
                continue
            for r in rows:
                if not isinstance(r, dict):
                    continue
                _touched += 1
                sd = str(r.get("status_detail") or r.get("detail") or "").strip().lower()
                st = str(r.get("status") or "").strip().lower()
                if sd == "success" and st in ("failed", "error", ""):
                    try:
                        r["status"] = "success"
                        _changed += 1
                    except Exception:
                        pass
        if _changed:
            try:
                res.setdefault("debug", {})
                if isinstance(res.get("debug"), dict):
                    res["debug"].setdefault("refactor83_status_normalize_v1", {})
                    if isinstance(res["debug"].get("refactor83_status_normalize_v1"), dict):
                        res["debug"]["refactor83_status_normalize_v1"].update({
                            "applied": True,
                            "touched_rows": int(_touched),
                            "changed_rows": int(_changed),
                        })
            except Exception:
                pass
    except Exception:
        pass

    return payload

def run_source_anchored_evolution(previous_data: dict, web_context: dict = None) -> dict:
    # Input coercion (avoid None.get)
    if not isinstance(previous_data, dict):
        previous_data = {}
    if web_context is None or not isinstance(web_context, dict):
        web_context = {}

    # =========================
    # REFACTOR111: deterministically override stale baseline_data with latest Analysis snapshot
    # =========================
    _ref111_prev_pick_dbg = {}
    try:
        _picker = globals().get("_refactor111_pick_latest_prev_snapshot_v1")
        if callable(_picker):
            _sel, _ref111_prev_pick_dbg = _picker(previous_data or {}, web_context=web_context)
            if isinstance(_sel, dict) and _sel:
                previous_data = _sel
    except Exception:
        pass

    def _fail(msg: str, tb: str = "") -> dict:
        out = {
            "status": "failed",
            "message": msg,
            "sources_checked": 0,
            "sources_fetched": 0,
            "numbers_extracted_total": 0,
            "stability_score": 0.0,
            "summary": {
                "total_metrics": 0,
                "metrics_found": 0,
                "metrics_increased": 0,
                "metrics_decreased": 0,
                "metrics_unchanged": 0,
            },
            "metric_changes": [],
            "source_results": [],
            "interpretation": "Evolution failed.",
            "code_version": str(globals().get("_YUREEKA_CODE_VERSION_LOCK") or ""),
            "debug": {
                "refactor37": {
                    "error": msg,
                }
            },
        }

        # REFACTOR111: always attach prev snapshot pick debug (if available)
        try:
            if isinstance(out.get("debug"), dict):
                out["debug"]["prev_snapshot_pick_v1"] = _ref111_prev_pick_dbg
        except Exception:
            pass

        if tb:
            try:
                out["debug"]["refactor37"]["traceback"] = tb
            except Exception:
                pass
            try:
                _cs = ""
                for _ln in str(tb).splitlines():
                    _lns = _ln.strip()
                    if _lns.startswith("File "):
                        _cs = _lns
                if _cs:
                    try:
                        out["debug"]["refactor37"]["callsite"] = _cs
                    except Exception:
                        pass
                    try:
                        out["message"] = f"{msg} | {_cs}"
                    except Exception:
                        pass
            except Exception:
                pass
        return out

    impl = _REFACTOR37_RUN_SOURCE_ANCHORED_EVOLUTION_IMPL
    if not callable(impl):
        return _fail("run_source_anchored_evolution implementation is not callable.")

    try:
        _res = impl(previous_data, web_context=web_context)
        if not isinstance(_res, dict):
            return _fail("run_source_anchored_evolution returned non-dict result (impl)", tb=traceback.format_stack())
        try:
            _res = _refactor83_normalize_evolution_source_caches_v1(_res)
        except Exception:
            pass

        # REFACTOR111: attach prev snapshot pick debug to successful output
        try:
            _res.setdefault("debug", {})
            if isinstance(_res.get("debug"), dict):
                _res["debug"]["prev_snapshot_pick_v1"] = _ref111_prev_pick_dbg
        except Exception:
            pass


        # NLP07: attach run-scope FRESH02 tiebreak summary to evolution debug bucket
        try:
            _fresh02_attach_run_tiebreak_summary_v1(_res, default_path="evolution")
        except Exception:
            pass

        return _res
    except TypeError:
        # Backward-compat: some historical defs accept only previous_data
        try:
            _res = impl(previous_data)
            if not isinstance(_res, dict):
                return _fail("run_source_anchored_evolution returned non-dict result (impl fallback)", tb=traceback.format_stack())
            try:
                _res = _refactor83_normalize_evolution_source_caches_v1(_res)
            except Exception:
                pass

            # REFACTOR111: attach prev snapshot pick debug to successful output (fallback path)
            try:
                _res.setdefault("debug", {})
                if isinstance(_res.get("debug"), dict):
                    _res["debug"]["prev_snapshot_pick_v1"] = _ref111_prev_pick_dbg
            except Exception:
                pass

            # NLP07: attach run-scope FRESH02 tiebreak summary to evolution debug bucket
            try:
                _fresh02_attach_run_tiebreak_summary_v1(_res, default_path="evolution")
            except Exception:
                pass

            return _res
        except Exception as e:
            return _fail(f"run_source_anchored_evolution crashed: {e}", tb=traceback.format_exc())
    except Exception as e:
        return _fail(f"run_source_anchored_evolution crashed: {e}", tb=traceback.format_exc())

# LLM05: patch tracker overlay (LLM sidecar health beacons + circuit breaker).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM05" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM05", "scope": "llm-sidecar", "summary": "Add run-scope LLM health beacons + circuit breaker/call-budget guards and unify non-sensitive diagnostics across LLM03 query framing and LLM04 anomaly relevance probes. Attach llm_sidecar_health_v1 to exported JSON (analysis + evolution) with status/last error hints; no winner/value changes.", "risk": "low"})
except Exception:
    pass


# LLM09: patch tracker overlay (LLM01 evolution evidence snippet attachment).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM09" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM09", "scope": "llm-sidecar", "summary": "LLM01 evidence snippets: make evolution-friendly by sourcing snippet text from baseline_sources_cache (results first, then analysis fallback) and deriving the best candidate from metric.evidence[0] (or shallow fields) when provenance/best_candidate is absent. Adds evidence_best_snippet + evidence_offsets to evolution output for auditability only; winners/values unchanged.", "risk": "low"})
except Exception:
    pass

# LLM08: patch tracker overlay (Perplexity structured outputs compatibility).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM08" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM08", "scope": "llm-sidecar", "summary": "Perplexity compatibility hotfix: when OPENAI_BASE_URL points at api.perplexity.ai and LLM_STRICT_MODE is on, use response_format.type='json_schema' with a minimal object schema (json_object unsupported). Add response_format_type to call diagnostics and provide a targeted 400 hint. No behavior changes unless strict_mode+Perplexity are used.", "risk": "low"})
except Exception:
    pass

# LLM07: patch tracker overlay (Perplexity auth bridging).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM07" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM07", "scope": "llm-sidecar", "summary": "Perplexity provider bridging for sidecar OpenAI-compatible calls: when OPENAI_BASE_URL points at api.perplexity.ai, accept PERPLEXITY_API_KEY/YUREEKA_PERPLEXITY_API_KEY (and PERPLEXITY_KEY) as the auth token; propagate non-sensitive api_key_source into diagnostics and improve hints. No winner/value changes.", "risk": "low"})
except Exception:
    pass

# LLM06: patch tracker overlay (Perplexity determinism patch).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM06" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM06", "scope": "llm-sidecar", "summary": "If OPENAI_BASE_URL points at Perplexity (api.perplexity.ai), automatically add disable_search=true to OpenAI-compatible chat/completions payload for deterministic sidecar behavior; add non-sensitive diag flag. No winner/value changes.", "risk": "low"})
except Exception:
    pass


# LLM12: patch tracker overlay (cache bypass + smoke-test diagnostics).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM12" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM12", "scope": "llm-sidecar", "summary": "Add optional LLM_BYPASS_CACHE flag (env overridable) to ignore disk/mem cache for live-call verification, and ENABLE_LLM_SMOKE_TEST to run a one-shot provider connectivity check (non-sensitive diag only). Enhance llm_sidecar_health_v1 with live_calls and cache_bypass state. Defaults OFF; no winner/value changes unless smoke/bypass flags are enabled.", "risk": "low"})
except Exception:
    pass

# LLM11: patch tracker overlay (LLM01 evidence-snippet validator fix + correct post-validation aggregation).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM11" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM11", "scope": "llm-sidecar", "summary": "Fix LLM01 evidence-snippet ranking validator bug (previously returned fallback unconditionally). Aggregate LLM attempt diagnostics once per attempt (post-validation) and emit bounded attempt records for grep debugging, including cache_hit and status. No metric winner/value changes; evidence snippets may improve when ENABLE_LLM_EVIDENCE_SNIPPETS is ON.", "risk": "low"})
except Exception:
    pass




# LLM18: patch tracker overlay (LLM01 acceptance policy: confidence threshold + deterministic tie-break).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM18" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM18", "scope": "llm-sidecar", "summary": "LLM01 evidence snippet assist: add confidence threshold gate and explicit deterministic tie-break set. LLM proposals are accepted only when confidence >= threshold and the proposed window falls inside the deterministic tie-set (within score_tie_delta of best). Adds non-leaky proposed/accepted markers + policy snapshot in debug/provenance. No winner/value changes.", "risk": "low"})
except Exception:
    pass

# LLM16: patch tracker overlay (LLM-series patch hygiene + diagnostics polish).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM16" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM16", "scope": "llm-sidecar", "summary": "Patch hygiene: bump CODE_VERSION to LLM16 and add missing LLM13–LLM15 tracker entries. Diagnostics: enrich llm01_evidence_snippets_summary with metric-level counts (while preserving existing fields) and clarify cache-vs-live call counts. No winner/value changes; assist flags remain OFF by default.", "risk": "low"})
except Exception:
    pass

# LLM15: patch tracker overlay (LLM01 evidence snippets controlled test run).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM15" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM15", "scope": "llm-sidecar", "summary": "Controlled validation: ENABLE_LLM_EVIDENCE_SNIPPETS can be enabled to LLM-rank snippet windows; winners/values remain deterministic. Adds additional grep-friendly beacons for llm_used/cache_hit and preserves strict_mode behavior with Perplexity.", "risk": "low"})
except Exception:
    pass

# LLM14: patch tracker overlay (patch hygiene restore + safer defaults).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM14" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM14", "scope": "llm-sidecar", "summary": "Restore patch hygiene invariants for LLM-series branch: align CODE_VERSION with patch tracker head, keep assist flags OFF by default, and default LLM_BYPASS_CACHE to OFF (debug override only).", "risk": "low"})
except Exception:
    pass

# LLM13: patch tracker overlay (Perplexity wiring smoke-test validated).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM13" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM13", "scope": "llm-sidecar", "summary": "Perplexity provider wiring validated (sonar-pro, strict_mode). Smoke test confirms response_format json_schema and 200 status. Assist features remain OFF by default; only diagnostics active.", "risk": "low"})
except Exception:
    pass



# LLM19: patch tracker overlay (LLM01 summary return fix + pre-gate skip / force-call toggle).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM19" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM19", "scope": "llm-sidecar", "summary": "LLM01 evidence snippets: fix stats return so llm01_evidence_snippets_summary accurately reflects applied/llm counts; add optional pre-gate to skip sidecar ranking when there is no deterministic tie-set, with an override toggle LLM01_EVIDENCE_FORCE_CALL for controlled call-path tests. Policy snapshot now surfaces force_call source. No metric winner/value changes; assist flags remain OFF by default.", "risk": "low"})
except Exception:
    pass

# LLM20: patch tracker overlay (LLM01 evidence-snippet calls populate run-scope health agg).
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM20" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM20", "scope": "llm-sidecar", "summary": "Sync successful LLM01 evidence-snippet rank outcomes into the global per-run LLM health aggregation, so llm_sidecar_health_v1.agg reflects real feature calls (not only smoke tests). No winner/value changes; assist behavior unchanged unless ENABLE_LLM_EVIDENCE_SNIPPETS is enabled.", "risk": "low"})
except Exception:
    pass


# LLM21: patch tracker overlay (avoid touching the compressed canonical blob)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM21" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM21", "scope": "llm-sidecar", "summary": "LLM21: Fix FIX41AFC19 current_metrics override drift by preferring FIX16 schema-only rebuild on non-injection evolution runs, while retaining analysis-canonical rebuild only when injection URLs are present (injection force-win contract). Restores prod stability; no LLM winner/value changes.", "risk": "low"})
except Exception:
    pass





# LLM22: patch tracker overlay (gate hidden query-structure fallback behind explicit flag)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM22" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM22", "scope": "llm-sidecar", "summary": "LLM22: Gate the legacy query-structure LLM fallback behind ENABLE_LLM_QUERY_STRUCTURE_FALLBACK (default OFF) so no hidden LLM calls occur when assist flags are disabled. Route the fallback through the same OpenAI-compatible strict-JSON sidecar call path (Perplexity supported), add replayable caching + run-scope health aggregation, and enforce a confidence threshold before accepting the proposal. Prevents unexpected winner/value drift caused by untracked query-structure LLM usage.", "risk": "low"})
except Exception:
    pass


# LLM23: patch tracker overlay (module index header + modularization seams)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM23" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM23", "scope": "llm-sidecar", "summary": "Add a Module Index header and [MOD:*] anchors to enforce clean modular seams (single-file today, easy split later). Bump version stamp + release tag to LLM23; patch tracker updated accordingly. No pipeline behavior changes.", "risk": "low"})
except Exception:
    pass


# LLM24: patch tracker overlay (fix [MOD:*] anchors + dedupe evidence-snippet attach across wrapper PMCs)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM24" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM24", "scope": "llm-sidecar", "summary": "Fix [MOD:*] anchors to be valid Python comments (prevents SyntaxError) while keeping the Module Index + search anchors for future modularization. Also dedupe LLM01 evidence-snippet attach on final wrappers: attach once to a master PMC dict and sync audit-only fields to other PMC dict copies to avoid duplicate work and duplicate LLM calls. No winner/value changes; assist flags remain OFF by default.", "risk": "low"})
except Exception:
    pass

# LLM25: patch tracker overlay (version stamp + patch-tracker head alignment)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM25" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM25", "scope": "llm-sidecar", "summary": "Patch hygiene: bump version stamp/release tag to LLM25 and advance patch-tracker head accordingly (restores CODE_VERSION ↔ patch_tracker_head invariant). No pipeline behavior changes; all LLM assist flags remain OFF by default and cache bypass stays an explicit debug-only override.", "risk": "low"})
except Exception:
    pass


# LLM26: patch tracker overlay (flag-off LLM noise fix + head normalization)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM26" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM26", "scope": "llm-sidecar", "summary": "LLM01 evidence snippets: when ENABLE_LLM_EVIDENCE_SNIPPETS is OFF, hard-gate rank calls so no flag_off attempt noise is recorded (llm01_llm_diag_agg_v1 attempts stays 0). Patch-tracker head reorder now always includes the current CODE_VERSION automatically. No winner/value changes; assist flags remain OFF by default.", "risk": "low"})
except Exception:
    pass



# LLM27: patch tracker overlay (fix Streamlit session_state override for LLM flags + add UI policy knobs)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM27" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM27", "scope": "llm-sidecar", "summary": "Fix LLM flag effective resolution to work with Streamlit SessionStateProxy (ui overrides now apply). Add sidebar controls for LLM01 evidence assist policy (confidence threshold, tie delta, force-call). Defaults remain OFF / conservative; no winner/value changes unless user explicitly enables.", "risk": "low"})
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM28" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM28", "scope": "llm-sidecar", "summary": "Fix Streamlit secrets flag/policy resolution to accept mapping-like sections (e.g., st.secrets[general]) and not require plain dicts; ENABLE_LLM_EVIDENCE_SNIPPETS in secrets now takes effect. No behavior changes unless flags enabled.", "risk": "low"})
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM29" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM29", "scope": "llm-sidecar", "summary": "Add LLM_FORCE_REFRESH_ONCE (one-run-only cache bypass for first eligible cached key) + live_calls beacons to prove a true live HTTP call without breaking cache-first determinism.", "risk": "low"})
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM30" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM30", "scope": "llm-triad-safety", "summary": "Triad safety hardening: do not overwrite existing disk cache on forced refresh; add llm01_evidence_snippets_safety_v1 signature beacon to assert evidence-snippet assist makes no numeric/winner changes.", "risk": "low"})

    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM31" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM31", "scope": "freshness", "summary": "Add deterministic source freshness extraction beacons (published_at + age buckets) into scraped_meta + baseline_sources_cache and emit debug.fresh01_source_freshness_v1 for audit. Metric selection/diff unchanged unless an explicit tiebreak flag is enabled (default OFF).", "risk": "low"})
except Exception:
    pass

# LLM32: patch tracker overlay (freshness score in Evidence Quality panel)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM32" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM32", "scope": "freshness", "summary": "Compute deterministic 0-100 data freshness score from source published_at/fetched_at and surface it in veracity_scores + Evidence Quality Scores panel (additive; no metric selection changes).", "risk": "low"})
except Exception:
    pass


# LLM33: patch tracker overlay (freshness tie-breaker wiring)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM33" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM33", "scope": "freshness", "summary": "Wire ENABLE_SOURCE_FRESHNESS_TIEBREAK into schema-only rebuild tie-breaking and make data_freshness compute from freshness_age_days/score (robust even when baseline_sources_cache is missing freshness_score).", "risk": "medium"})
except Exception:
    pass


# LLM34: patch tracker overlay (freshness score end-to-end + tiebreak audit beacons)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM34" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM34", "scope": "freshness", "summary": "Add global URL normalizer; make data_freshness score compute deterministically from scraped_meta/scraped_content even before snapshots are attached; backfill per-source freshness fields; and emit fresh02_freshness_tiebreak_v1 audit beacons when freshness changes the schema winner (flag-gated).", "risk": "medium"})
except Exception:
    pass
# LLM10_PATCH_TRACKER_REORDER_V1: move known patch ids to the front in descending order (best-effort)
try:
    _order = ["LLM00", "LLM01", "LLM01H", "LLM01D", "LLM01E", "LLM01F", "LLM02", "LLM03", "LLM04H", "LLM05", "LLM06", "LLM07", "LLM08", "LLM09", "LLM10", "LLM11", "LLM12", "LLM13", "LLM14", "LLM15", "LLM16", "LLM17", "LLM18", "LLM19", "LLM20", "LLM21", "LLM22", "LLM23", "LLM24", "LLM25", "LLM26", "LLM27", "LLM28"]
    # Always ensure the current stamped version ends up as the tracker head (append last).
    try:
        _cv = str(globals().get("_YUREEKA_CODE_VERSION_LOCK") or globals().get("CODE_VERSION") or "").strip()
        if _cv and _cv not in _order:
            _order.append(_cv)
        elif _cv and _order and _order[-1] != _cv:
            # Move to end so the final ensure_head sets head==_cv
            _order = [p for p in _order if p != _cv] + [_cv]
    except Exception:
        pass
    for _pid in _order:
        _yureeka_patch_tracker_ensure_head_v1(_pid, {})
except Exception:
    pass

# LLM35: patch tracker overlay (freshness score wiring + per-metric tiebreak beacons + beacon pool fix)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM35" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM35", "scope": "freshness", "summary": "Wire deterministic freshness score into veracity_scores.data_freshness and surface it in Evidence Quality Scores; add per-metric fresh_tiebreak_v1 beacon (used/reason/stable fallback); and fix fresh01_source_freshness_v1 to aggregate from the same (current) source pool used by selection in evolution.", "risk": "low"})
except Exception:
    pass

# LLM36: patch tracker overlay (freshness tie-break beacon fixes + run-level summary)
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM36" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {"patch_id": "LLM36", "scope": "freshness", "summary": "Fix per-metric fresh_tiebreak_v1 fields (winner/base URLs + changed_winner detection) so freshness-driven tie-breaking is provable; add debug.fresh02_freshness_tiebreak_summary_v1 derived from per-metric beacons (analysis + evolution). No winner/value changes; cache-first determinism preserved.", "risk": "low"})
except Exception:
    pass

# [MOD:PATCH_TRACKER]
# Patch tracker tail entries (LLM/NLP roadmap) + governance hardening.

# LLM37: patch tracker entry
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM37" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {
            "patch_id": "LLM37",
            "date": "2026-02-17",
            "title": "Hyperparameter block v1",
            "summary": [
                "Add [MOD:HYPERPARAMS_V1] at top-of-file to centralize policy knobs for deterministic + LLM/NLP layers.",
                "Parameterize key thresholds (LLM confidence gates, freshness curve, cache salting, ops TTLs) while preserving LLM36 defaults.",
                "Attach additive hyperparams snapshot + fingerprints into wrapper.debug for auditability (no behavior change).",
            ],
            "acceptance": [
                "Defaults preserve REFACTOR206/LLM36 deterministic outcomes (no metric winner/value changes).",
                "Hyperparameters are declared in one top section; optional overrides via YUREEKA_HYPERPARAMS_JSON.",
                "LLM cache key salting is backward-compatible via fallback to legacy unsalted key.",
            ],
            "notes": [
                "Override JSON is deep-merged and sanitized; invalid freshness curve overrides fall back to defaults.",
                "Freshness tie-break quantization only takes effect when ENABLE_SOURCE_FRESHNESS_TIEBREAK is enabled.",
            ],
            "risk": "low",
        })
except Exception:
    pass

# LLM38: patch tracker entry
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "LLM38" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {
            "patch_id": "LLM38",
            "date": "2026-02-17",
            "title": "Freshness scope + injected-row Δt gating",
            "summary": [
                "Fix NameError in rebuild_metrics_from_snapshots_analysis_canonical_v1 by adding a module-level _fresh02_candidate_tie_key_v1 fallback (uses candidate-attached freshness fields; no cache; deterministic).",
                "Refine evolution row delta stamping: blank Δt only for rows whose source_url is an injected URL (instead of blanking all rows whenever injection is present).",
                "Patch hygiene: bump _YUREEKA_CODE_VERSION_LOCK; keep cache-first determinism; no winner/value changes unless freshness tie-break flag is explicitly enabled.",
            ],
            "risk": "low",
        })
except Exception:
    pass

# NLP01: patch tracker entry
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "NLP01" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {
            "patch_id": "NLP01",
            "date": "2026-02-17",
            "title": "NLP01 query-frame enrichment + freshness beacon fix",
            "summary": [
                "Deterministically enrich query_frame_v1.metric_families using expected_metric_ids + lightweight keyword rules (proposal-only; no winner/value changes).",
                "Expose nlp01_enrichment_v1 debug beacon for auditability.",
                "Fix freshness tiebreak beacon accounting bug in REFACTOR100 year-anchor scan so FRESH02 base-winner trackers use the current candidate's year_ok/year_found (fresh_tiebreak_v1 beacons match top3 reality).",
                "Make fresh_tiebreak_v1.competitor_url reflect the runner-up candidate (avoids base_url duplication in changed-winner cases).",
            ],
            "risk": "low",
        })
except Exception:
    pass

# NLP02: patch tracker entry
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "NLP02" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {
            "patch_id": "NLP02",
            "date": "2026-02-17",
            "title": "NLP02 hyperparams reference fix + always-on freshness attachment",
            "summary": [
                "Fix NLP01 NameError: _nlp01_derive_metric_families_v1 now reads hyperparams from HYPERPARAMS_V1 (so nlp01_enrichment_v1 and metric_families enrichment populate as intended).",
                "Always attach per-source freshness fields (published_at / age_days / freshness_score / bucket) into selection_year_anchor_v1.top3 summaries, even when ENABLE_SOURCE_FRESHNESS_TIEBREAK is OFF (visibility without behavior change).",
                "Preserve determinism: freshness tie-breaking still only affects winner selection when ENABLE_SOURCE_FRESHNESS_TIEBREAK is explicitly enabled; otherwise it remains diagnostic-only.",
            ],
            "risk": "low",
        })
except Exception:
    pass

# NLP03: patch tracker entry
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "NLP03" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {
            "patch_id": "NLP03",
            "date": "2026-02-17",
            "title": "NLP03 deterministic query boost flag + audit beacons",
            "summary": [
                "Add ENABLE_NLP_QUERY_BOOST (default OFF) to apply deterministic query boosting using query_frame_v1 boost terms without invoking the LLM.",
                "Extend query-frame debug beacons to report whether boosting is effectively enabled via LLM query framing vs NLP query boost, with explicit flag sources.",
                "Parameterise query-boost limits via hyperparams (nlp.query_boost.max_terms / max_chars) and keep stable ordering/truncation.",
            ],
            "risk": "low",
        })
except Exception:
    pass

# NLP04: patch tracker entry
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "NLP04" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {
            "patch_id": "NLP04",
            "date": "2026-02-18",
            "title": "NLP04 freshness tiebreak base-tracker fix",
            "summary": [
                "Fix FRESH02 base-winner tracking for non-'M' schema keys by defining tie_base deterministically on every candidate (prevents stale tie_base leakage across canonical keys).",
                "As a result, fresh_tiebreak_v1.base_url / changed_winner / reason now reflect the true pre-freshness winner for percent/currency keys (audit beacons match selection_year_anchor_v1.top3 reality).",
                "Purely diagnostic + auditability fix: winner/value selection logic is unchanged; cache-first determinism preserved; ENABLE_SOURCE_FRESHNESS_TIEBREAK remains OFF by default.",
            ],
            "risk": "low",
        })
except Exception:
    pass

# NLP05: patch tracker entry
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "NLP05" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {
            "patch_id": "NLP05",
            "date": "2026-02-18",
            "title": "NLP05 freshness scoring wired + tie-break proof beacons + query boost preview",
            "summary": [
                "Wire data_freshness into veracity_scores (panel-ready; populated alongside other evidence scores).",
                "Add fresh_tiebreak_v1 audit beacon capturing when a score-tie is resolved by freshness, including per-candidate age_days and changed_winner flag.",
                "Make NLP query boost visibly auditable via boosted_query_preview + boost_preview_terms when ENABLE_NLP_QUERY_BOOST is enabled (deterministic; no LLM call).",
            ],
            "risk": "low",
        })
except Exception:
    pass

# NLP06: patch tracker entry
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "NLP06" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {
            "patch_id": "NLP06",
            "date": "2026-02-18",
            "title": "NLP06 patch tracker + governance hardening",
            "summary": [
                "Fix patch-tracker head drift in evolution beacons by enforcing head==code_version at import-time (post-overlays).",
                "Add/confirm patch tracker entries for NLP03–NLP06 (including missing NLP05) with guarded inserts to avoid duplicate IDs.",
                "No pipeline behavior changes: deterministic winners/values remain identical when NLP/LLM assist flags are OFF.",
            ],
            "acceptance": [
                "debug.harness_invariants_v1.patch_tracker_head_patch_id == code_version (NLP06).",
                "debug.harness_invariants_v1.code_version_matches_patch_tracker_head == true in prod evolution.",
            ],
            "risk": "low",
        })
except Exception:
    pass


# NLP07: patch tracker entry
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "NLP07" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {
            "patch_id": "NLP07",
            "date": "2026-02-18",
            "title": "NLP07 freshness tie-break run-summary durability + alias fields",
            "summary": [
                "Harden FRESH02 auditability by deriving a run-scope tie-break summary directly from primary_metrics_canonical (per-metric fresh_tiebreak_v1 beacons), and attach it to the durable debug bucket for both analysis and evolution wrappers (prevents wrapper-depth/persistence drops).",
                "Add grep-friendly alias fields a_freshness_age_days / b_freshness_age_days into per-metric fresh_tiebreak_v1 (keeps existing a_age_days/b_age_days).",
                "Purely diagnostic + provenance hardening: no changes to deterministic winner/value selection; REFACTOR206 behavior preserved when assist flags are OFF.",
            ],
            "acceptance": [
                "analysis.debug.fresh02_freshness_tiebreak_summary_v1 is present and well-formed (derived_from=primary_metrics_canonical...).",
                "evolution.results.debug.fresh02_freshness_tiebreak_summary_v1 is present and well-formed.",
                "Per-metric provenance.fresh_tiebreak_v1 contains both a_age_days/b_age_days and a_freshness_age_days/b_freshness_age_days when available.",
            ],
            "risk": "low",
        })
except Exception:
    pass


# NLP08: patch tracker entry
try:
    if isinstance(PATCH_TRACKER_V1, list) and not any(isinstance(e, dict) and str(e.get("patch_id") or "") == "NLP08" for e in PATCH_TRACKER_V1):
        PATCH_TRACKER_V1.insert(0, {
            "patch_id": "NLP08",
            "date": "2026-02-18",
            "title": "NLP08 injection run-delta parity + row-delta gating cleanup",
            "summary": [
                "Populate analysis→evolution run-delta fields (analysis_evolution_delta_seconds / analysis_evolution_delta_human) for injected rows too, so prod and injection evolutions show consistent timing context.",
                "Preserve injection identification in debug row_delta_gating_v4 while removing the forced-blank behavior (injection_run_force_blank_rows now remains 0).",
                "No changes to deterministic metric winner/value selection; REFACTOR206 behavior remains identical when assist flags are OFF."
            ],
            "risk": "low"
        })
except Exception:
    pass

# Governance hardening: enforce patch tracker head == stamped code version (after all overlays)
try:
    _cv = str(_yureeka_get_code_version() or "").strip()
    if _cv:
        _yureeka_patch_tracker_ensure_head_v1(_cv, {"patch_id": _cv})
except Exception:
    pass



# Why:
# - Streamlit can execute main() before later end-of-file patch-tracker "ADD" blocks run.
# - This block registers the current patch *before* main() executes, so harness/version checks see an up-to-date tracker.
try:
    if __name__ == "__main__":
        main()
except Exception:
    try:
        st.exception(Exception(f"Yureeka app crashed during main() execution ({_yureeka_get_code_version()})."))
    except Exception:
        pass

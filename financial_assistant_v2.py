# ===============================================================================
# YUREEKA AI RESEARCH ASSISTANT v7.41
# With Web Search, Evidence-Based Verification, Confidence Scoring
# SerpAPI Output with Evolution Layer Version
# Updated SerpAPI parameters for stable output
# Deterministic Output From LLM
# Deterministic Evolution Core Using Python Diff Engine
# Anchored Evolution Analysis Using JSON As Input Into Model
# Implementation of Source-Based Evolution
# Saving of JSON output Files into Google Sheets
# Canonical Metric Registry + Semantic Hashing of Findings
# Removal of Evolution Decisions from LLM
# Further Enhancements to Minimize Evolution Drift (Metric)
# Saving of Extraction Cache in JSON
# Prioritize High Quality Sources With Source Freshness Tracking
# Timestamps = Timezone Naive
# Improved Stability of Handling of Duplicate Canonicalized IDs
# Deterministic Main and Side Topic Extractor
# Range Aware Canonical Metrics
# Range + Source Attribution
# Proxy Labeler + Geo Tagging
# Improved Main Topic + Side Topic Extractor Using Deterministic-->NLP-->LLM layer
# Guardrails For Main + Side Topic Handling
# Numeric Consistency Scores
# Multi-Side Enumerations
# Dashboard Unit Presentation Fixes (Main + Evolution)
# Domain-Agnostic Question Profiling
# Baseline Caching Contains HTTP Validators + Numeric Data
# URL canonicalization
# Evolution Layer Leverage On New Analysis Pipeline to Minimise Volatility
# Canonicalization of Evolution Layer Metrics To Match Analysis Layer
# Fix URL/path Collapese Issue Causing + Tighten Evolution Extraction (Topic Gating)
# canonical-key-first matching
# Evolution Pipeline to Consume analysis upstream artifacts
# safety-net hard gates (minimal) before matching
# Tighten canonical identity + unit-family constraints
# Fingerprint freshness gating to evolution
# Fix SerpAPI access and fetching
# Keeps your snapshot-friendly scraped_meta (with extracted numbers + fingerprint fields)
# Safe fallback scraper when ScrapingDog is unavailable
# Prevent caching “empty results” from SerpAPI (no poisoned cache)
# Restoration of Range Estimates For Metrics
# Improved Junk Tagging and Rejection
# One Canononical Operator for Analysis + Evolution Layers
# Metric Aware Range Construction Everywhere
# Anchor Matching Correctness
# Unit Measure + Attribute Association e.g. M + units (sold)
# Enriched metric_schema_frozen (analysis side)
# ================================================================================

import io
import os
import re
import json
import requests
import pandas as pd
import plotly.express as px
import streamlit as st
import base64
import hashlib
import numpy as np
import difflib
import gspread
import google.generativeai as genai
from pypdf import PdfReader
from pathlib import Path
from google.oauth2.service_account import Credentials
from dataclasses import dataclass
from typing import Dict, List, Optional, Any, Union, Tuple
from sentence_transformers import SentenceTransformer, util
from transformers import pipeline
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup
from collections import Counter
from pydantic import BaseModel, Field, ValidationError, ConfigDict

# =========================
# VERSION STAMP (ADDITIVE)
# =========================
CODE_VERSION = "fix41afc69_unit_evidence_gate_no_synth_magnitude_v1"  # PATCH FIX41AFC69
# =====================================================================
# PATCH FIX41AFC58 START
# Canonical downstream selector scaffold (single-source-of-truth target)
#
# Purpose:
#   Introduce a shared, pipeline-agnostic selector function that can be
#   called by BOTH Analysis and Evolution near the dashboard boundary.
#
# Scope:
#   - Additive only; NO wiring changes in this patch.
#   - Does not modify fastpath, hashing, snapshot plumbing, or existing
#     selection behavior.
#
# Contract:
#   _select_current_metric_canonical_v1(...) returns:
#     (best_candidate_or_none, meta_dict)
#
# Notes:
#   - The implementation is conservative and delegates to existing helpers
#     (anchor resolution, eligibility checks) when present.
#   - Call-sites will be wired in a subsequent patch (FIX41AFC59/60).
# =====================================================================

ENABLE_CANONICAL_SELECTOR_FIX41AFC58 = True  # scaffold only; not wired yet

# =====================================================================
# PATCH FIX41AFC59 START (ADDITIVE)
# Canonical selector: strict schema unit_tag gating + preferred-source lock
#
# Goals:
#   1) Prevent unitless / wrong-scale magnitude values (e.g., "170") from
#      satisfying a schema that expects "million units".
#   2) Enforce preferred-source lock deterministically when a preferred URL
#      is available (anchors or prev canonical metric).
#   3) Keep behavior additive: only engaged when canonical selector wiring
#      calls it (Evolution/Analysis downstream).
# =====================================================================

def _fix41afc59_schema_unit_tag_requires_token(unit_tag: str) -> str:
    """Return a normalized token hint (e.g., 'million', 'billion') from a schema unit_tag."""
    import re
    ut = (unit_tag or "").lower().strip()
    if not ut:
        return ""
    # common scale hints
    if re.search(r"\b(million|mn|m)\b", ut):
        return "million"
    if re.search(r"\b(billion|bn|b)\b", ut):
        return "billion"
    if re.search(r"\b(thousand|k)\b", ut):
        return "thousand"
    if re.search(r"\b(trillion|tn|t)\b", ut):
        return "trillion"
    return ""

def _fix41afc59_candidate_matches_schema_unit_tag(metric_schema: dict, cand: dict) -> bool:
    """Hard gate candidates based on schema.unit_tag when it implies a specific scale."""
    import re
    if not isinstance(metric_schema, dict) or not isinstance(cand, dict):
        return True
    ut = (metric_schema.get("unit_tag") or metric_schema.get("unit") or "").strip()
    fam = (metric_schema.get("unit_family") or "").strip().lower()
    if not ut or fam not in ("magnitude", "currency", "energy", "count", "unit_sales", "units", "unit"):
        return True

    hint = _fix41afc59_schema_unit_tag_requires_token(ut)
    if not hint:
        return True

    raw = (cand.get("raw") or "")
    unit = (cand.get("unit") or "")
    unit_tag = (cand.get("unit_tag") or "")
    ctx = (cand.get("context_snippet") or cand.get("context") or "")
    hay = f"{raw} {unit} {unit_tag} {ctx}".lower()
    # =====================================================================
    # PATCH FIX41AFC61 START (ADDITIVE): unit_tag/base_unit synonyms for schema scale gate
    # Why:
    # - Some extractors emit scale in unit_tag (e.g., 'M') while leaving unit blank.
    # - The prior gate only inspected 'unit' (and text), causing legitimate
    #   magnitude candidates (e.g., 17.8 'M') to be rejected and allowing fallback
    #   selection to roam into unrelated sources (e.g., GlobeNewswire '1.3B').
    # - We keep the hard scale gate (still blocks unitless '170') but accept
    #   common scale tokens in unit_tag/base_unit for parity with analysis.
    # =====================================================================
    base_unit = (cand.get("base_unit") or "")
    unit_tag_l = (unit_tag or "").strip().lower()
    base_unit_l = (base_unit or "").strip().lower()
    unit_l = (unit or "").strip().lower()
    # PATCH FIX41AFC61 END


    # If schema expects a scale (e.g., million), require an explicit mention.
    # This prevents unitless '170' from being treated as 'million units'.
    
    # =====================================================================
    # PATCH FIX41AFC61B START (ADDITIVE): accept million tokens in unit_tag/base_unit
    # =====================================================================
    if hint == "million":
        # accept explicit scale tokens even when unit is blank
        if unit_tag_l in ("m", "mn", "mio", "mm") or base_unit_l in ("m", "mn", "mio", "mm") or unit_l in ("m", "mn", "mio", "mm"):
            return True
        if (unit_tag or "").strip() == "M" or (base_unit or "").strip() == "M" or (unit or "").strip() == "M":
            return True
        # fall through to original textual/unit heuristic
    # =====================================================================
    # PATCH FIX41AFC61B END
    if hint == "million":
        return bool(re.search(r"\b(million|mn)\b", hay)) or ("m" in unit.lower() and len(unit.strip()) <= 2)
    
    # =====================================================================
    # PATCH FIX41AFC61C START (ADDITIVE): accept billion tokens in unit_tag/base_unit
    # =====================================================================
    if hint == "billion":
        if unit_tag_l in ("b", "bn", "bb") or base_unit_l in ("b", "bn", "bb") or unit_l in ("b", "bn", "bb"):
            return True
        if (unit_tag or "").strip() == "B" or (base_unit or "").strip() == "B" or (unit or "").strip() == "B":
            return True
        # fall through to original textual/unit heuristic
    # =====================================================================
    # PATCH FIX41AFC61C END
    if hint == "billion":
        return bool(re.search(r"\b(billion|bn)\b", hay)) or ("b" in unit.lower() and len(unit.strip()) <= 2)
    if hint == "thousand":
        return bool(re.search(r"\b(thousand)\b", hay)) or ("k" == unit.strip().lower())
    if hint == "trillion":
        return bool(re.search(r"\b(trillion|tn)\b", hay)) or ("t" == unit.strip().lower())
    return True

# =====================================================================
# PATCH FIX41AFC59 END
# =====================================================================

# =====================================================================
# PATCH FIX41AFC69 START
# Unit-evidence gate to prevent false-positive magnitude matches when unit is blank.
#
# Problem observed:
# - Some candidates (e.g. raw "170" with unit="") get their cur_unit_cmp synthesized from schema
#   (see FIX41AFC40B), which can cause unit_mismatch to incorrectly appear False and leak to dashboard.
#
# Rule (deterministic, additive):
# - For magnitude / unit_sales style metrics: if the chosen candidate has no explicit unit evidence
#   (unit/unit_tag/base_unit AND no scale-token in raw/context), we DO NOT allow schema-unit synthesis
#   to make it look compatible. We hard-block at diff-boundary with reason "unit_missing_evidence_hard_block".
# =====================================================================

def _fix41afc69_candidate_has_unit_evidence(cand: dict, expected_unit_tag: str = "", expected_unit_family: str = "") -> bool:
    try:
        if not isinstance(cand, dict):
            return False
        u = str(cand.get("unit") or "").strip()
        ut = str(cand.get("unit_tag") or "").strip()
        bu = str(cand.get("base_unit") or "").strip()
        if u or ut or bu:
            return True

        raw = str(cand.get("raw") or cand.get("raw_disp") or "").strip()
        ctx = str(cand.get("context_snippet") or cand.get("context") or "").strip()
        hay = f"{raw} {ctx}".lower()

        # Quick allow for obvious unit words
        if re.search(r"\b(%|percent|percentage|usd|sgd|eur|gbp|jpy|cny|rmb|\$|€|£)\b", hay):
            return True

        exp_tag = (expected_unit_tag or "").lower().strip()

        # Scale-token evidence for magnitude style tags
        if any(k in exp_tag for k in ["million", "mn", "m ", "m$", "m\$", "mio", "m vehicles", "m units"]):
            return _fix41afc59_scale_hint_in_text("million", hay, unit=u or "")
        if any(k in exp_tag for k in ["billion", "bn", "b ", "b$", "b\$"]):
            return _fix41afc59_scale_hint_in_text("billion", hay, unit=u or "")
        if any(k in exp_tag for k in ["thousand", "k "]):
            return _fix41afc59_scale_hint_in_text("thousand", hay, unit=u or "")
        if any(k in exp_tag for k in ["trillion", "tn", "t "]):
            return _fix41afc59_scale_hint_in_text("trillion", hay, unit=u or "")

        # If schema expects a specific family, and candidate provided none, treat as no evidence.
        return False
    except Exception:
        return False

# =====================================================================
# PATCH FIX41AFC69 END
# =====================================================================
def _select_current_metric_canonical_v1(
    canonical_key: str,
    metric_schema: dict,
    candidates: list,
    metric_anchors: dict = None,
    prev_metric: dict = None,
    web_context: dict = None,
    cand_index: dict = None,
    prev_response: dict = None,
) -> tuple:
    """
    Canonical selector (scaffold):
      - Anchor-first (hash/id) when anchors exist
      - Preferred source lock when available
      - Eligibility parity via _metric_candidate_is_eligible_v2 when present
      - Deterministic fallback ordering (stable tie-breaks)

    Returns:
      (best_candidate_or_none, meta)
        meta includes:
          - canonical_selector_used: bool
          - anchor_used: bool
          - anchor_resolve_method: str
          - preferred_url: str
          - blocked_reason: str
          - debug: dict (optional)
    """
    meta = {
        "canonical_selector_used": True,
        "anchor_used": False,
        "anchor_resolve_method": "",
        "preferred_url": "",
        "blocked_reason": "",
        "debug": {},
    }

    metric_anchors = metric_anchors or {}
    prev_metric = prev_metric or {}
    web_context = web_context or {}
    candidates = candidates or []

    # Preferred URL derivation (anchors first, then prev_metric, then prev_response canonical)
    preferred_url = ""
    try:
        if isinstance(metric_anchors, dict) and canonical_key in metric_anchors:
            a = metric_anchors.get(canonical_key) or {}
            if isinstance(a, dict):
                preferred_url = (a.get("source_url") or a.get("url") or "").strip()
        if not preferred_url and isinstance(prev_metric, dict):
            preferred_url = (prev_metric.get("source_url") or prev_metric.get("url") or "").strip()
        if not preferred_url and isinstance(prev_response, dict):
            # best-effort
            pmc = prev_response.get("primary_metrics_canonical")
            if isinstance(pmc, dict):
                pm = pmc.get(canonical_key) or {}
                if isinstance(pm, dict):
                    preferred_url = (pm.get("source_url") or pm.get("url") or "").strip()
        # =====================================================================
        # PATCH FIX41AFC67 START (ADDITIVE): preferred_url fallback from prev_response.results.metric_changes
        #
        # Why:
        # - Some stored prev_response payloads (especially older evolution baselines) may not
        #   carry primary_metrics_canonical or anchors for every canonical_key.
        # - However, the evolution diff layer often persists source_url per metric in
        #   results.metric_changes. When present, this is still a deterministic indicator of
        #   the preferred source and should be used to prevent cross-source roaming.
        #
        # What:
        # - If preferred_url is still empty, attempt to locate a matching entry in
        #   prev_response['results']['metric_changes'] and use its source_url.
        # - Add debug marker to meta to aid troubleshooting.
        # =====================================================================
        if not preferred_url and isinstance(prev_response, dict):
            try:
                _res = prev_response.get("results") or {}
                _mc = _res.get("metric_changes") or []
                if isinstance(_mc, list):
                    for _row in _mc:
                        if not isinstance(_row, dict):
                            continue
                        if str(_row.get("canonical_key") or "") == str(canonical_key):
                            preferred_url = (_row.get("source_url") or _row.get("url") or "").strip()
                            if preferred_url:
                                meta["debug"]["fix41afc67_preferred_from"] = "prev_response.results.metric_changes"
                            break
            except Exception:
                pass
        # PATCH FIX41AFC67 END
    except Exception:
        preferred_url = preferred_url or ""

    meta["preferred_url"] = preferred_url

    # Build candidate index (anchor_hash -> candidate) if not provided
    if cand_index is None:
        try:
            cand_index = {}
            for c in candidates:
                if not isinstance(c, dict):
                    continue
                ah = (c.get("anchor_hash") or "").strip()
                if ah and ah not in cand_index:
                    cand_index[ah] = c
        except Exception:
            cand_index = {}

    def _eligible(c: dict) -> bool:
        try:
            # FIX41AFC59: schema unit_tag hard gate (prevents unitless scale drift)
            if not _fix41afc59_candidate_matches_schema_unit_tag(metric_schema, c):
                return False
            fn = globals().get("_metric_candidate_is_eligible_v2")
            if callable(fn):
                # ==============================================================
                # PATCH FIX41AFC66 START
                # Eligibility signature compatibility shim.
                # Some code paths define _metric_candidate_is_eligible_v2 as:
                #   fn(metric_schema, cand)  (or with web_context),
                # while older paths used:
                #   fn(cand, metric_schema, web_context)
                # We try common signatures deterministically, then fall back.
                # ==============================================================
                try:
                    return bool(fn(metric_schema, c, web_context))
                except TypeError:
                    pass
                try:
                    return bool(fn(metric_schema, c))
                except TypeError:
                    pass
                try:
                    return bool(fn(c, metric_schema, web_context))
                except TypeError:
                    pass
                try:
                    return bool(fn(c, metric_schema))
                except TypeError:
                    pass
                # PATCH FIX41AFC66 END
                return bool(fn(c, metric_schema, web_context))
        except Exception:
            return False
        # If helper absent, be conservative: require dict + value_norm numeric
        try:
            return isinstance(c, dict) and isinstance(c.get("value_norm"), (int, float))
        except Exception:
            return False

    def _url_of(c: dict) -> str:
        try:
            return (c.get("source_url") or c.get("url") or "").strip()
        except Exception:
            return ""

    # =====================================================================
    # PATCH FIX41AFC60 START
    # Canonical selector: normalize URLs for preferred-source matching
    # Why:
    # - upstream sources sometimes vary by trailing slash / www / scheme
    # - strict string equality causes preferred-source filtering to miss
    #   and allows "roaming" to injected/unrelated sources (e.g., GlobeNewswire).
    # =====================================================================
    def _fix41afc60_norm_url(u: str) -> str:
        try:
            import re
            s = (u or "").strip().lower()
            if not s:
                return ""
            # strip scheme
            s = re.sub(r"^https?://", "", s)
            # strip www
            s = re.sub(r"^www\.", "", s)
            # strip trailing slash
            s = s.rstrip("/")
            return s
        except Exception:
            return (u or "").strip().lower().rstrip("/")

    _preferred_norm = _fix41afc60_norm_url(preferred_url)
    # PATCH FIX41AFC60 END

    # -------------------------
    # 1) Anchor-first resolution
    # -------------------------
    anchor_obj = {}
    try:
        if isinstance(metric_anchors, dict):
            anchor_obj = metric_anchors.get(canonical_key) or {}
            if not isinstance(anchor_obj, dict):
                anchor_obj = {}
    except Exception:
        anchor_obj = {}

    # Try: anchor_hash hit
    try:
        ah = (anchor_obj.get("anchor_hash") or anchor_obj.get("anchor_hash_v1") or "").strip()
        if ah and isinstance(cand_index, dict) and ah in cand_index:
            c = cand_index.get(ah)
            if isinstance(c, dict) and _eligible(c):
                if (not preferred_url) or (_fix41afc60_norm_url(_url_of(c)) == _preferred_norm):
                    meta["anchor_used"] = True
                    meta["anchor_resolve_method"] = "anchor_hash"
                    return (c, meta)
    except Exception:
        pass

    # Try: candidate_id prefix compatibility (when present)
    try:
        cid = (anchor_obj.get("candidate_id") or "").strip()
        if cid:
            # scan preferred-url pool first if preferred_url known
            pool = candidates
            if preferred_url:
                pool = [c for c in candidates if isinstance(c, dict) and _fix41afc60_norm_url(_url_of(c)) == _preferred_norm]
            for c in pool:
                if not isinstance(c, dict):
                    continue
                ccid = (c.get("candidate_id") or "").strip()
                if ccid and (ccid == cid or ccid.startswith(cid) or cid.startswith(ccid)):
                    if _eligible(c):
                        meta["anchor_used"] = True
                        meta["anchor_resolve_method"] = "candidate_id"
                        return (c, meta)
    except Exception:
        pass

    # Try: same-source context rescue (delegate to FIX41AFC45 fallback if present)
    try:
        fn = globals().get("_fix41afc45_try_source_context_fallback")
        if callable(fn) and preferred_url:
            # build a minimal anchor_obj with source_url/context_snippet
            _a = dict(anchor_obj or {})
            if preferred_url and not (_a.get("source_url") or _a.get("url")):
                _a["source_url"] = preferred_url
            c2, r2 = fn(canonical_key, _a, candidates, metric_schema, web_context)
            if isinstance(c2, dict) and _eligible(c2):
                if (not preferred_url) or (_fix41afc60_norm_url(_url_of(c2)) == _preferred_norm):
                    meta["anchor_used"] = True
                    meta["anchor_resolve_method"] = "same_source_context"
                    meta["debug"]["same_source_reason"] = r2
                    return (c2, meta)
    except Exception:
        pass

    # -------------------------
    # 2) Preferred-source filter
    # -------------------------
    filtered = candidates
    if preferred_url:
        try:
            filtered = [c for c in candidates if isinstance(c, dict) and _fix41afc60_norm_url(_url_of(c)) == _preferred_norm]
        except Exception:
            filtered = candidates

    # -------------------------
    # 3) Deterministic fallback
    # -------------------------
    eligible_cands = []
    for c in filtered:
        if not isinstance(c, dict):
            continue
        if _eligible(c):
            eligible_cands.append(c)

    if not eligible_cands:
        meta["blocked_reason"] = "no_eligible_candidate"
        return (None, meta)

    # deterministic tie-break:
    # - presence of unit_cmp, then presence of anchor_hash, then abs(value_norm) desc, then anchor_hash asc
    def _tie(c: dict):
        try:
            unit_cmp = (c.get("unit_cmp") or c.get("unit_norm") or "").strip()
            has_unit = 1 if unit_cmp else 0
            ah = (c.get("anchor_hash") or "").strip()
            has_ah = 1 if ah else 0
            vn = c.get("value_norm")
            vn = float(vn) if isinstance(vn, (int, float)) else 0.0
            return (-has_unit, -has_ah, -abs(vn), ah)
        except Exception:
            return (0, 0, 0, "")

    eligible_cands.sort(key=_tie)
    return (eligible_cands[0], meta)

# =====================================================================
# PATCH FIX41AFC58 END
# =====================================================================
# PATCH FIX41AFC24_VERSION START
# Additive override: later assignment ensures runtime CODE_VERSION matches filename for auditability.
# PATCH FIX41AFC49C START — bump CODE_VERSION
#CODE_VERSION = "fix41afc51_evo_anchor_payload_enrich_v1"
# PATCH FIX41AFC49C END — bump CODE_VERSION
# PATCH FIX41AFC24_VERSION END

# =====================================================================
# PATCH FIX41AFC23V (ADDITIVE): bump CODE_VERSION marker for this patched build
# =====================================================================
#CODE_VERSION = "fix41afc51_evo_anchor_payload_enrich_v1"
# =====================================================================
# END PATCH FIX41AFC23V
# =====================================================================

# PATCH FIX41AFC6 (ADD): bump CODE_VERSION to new patch filename
#CODE_VERSION = "fix41afc6_evo_fetch_injected_urls_when_delta_v1"

# =====================================================================
# PATCH FIX41T (ADDITIVE): bump CODE_VERSION marker for this patched build
# - Purely a version label for debugging/traceability.
# - Does NOT alter runtime logic.
# =====================================================================
#CODE_VERSION = "fix41t_evo_extra_url_injection_trace_replay"
# =====================================================================
# PATCH FIX41U (ADDITIVE): bump CODE_VERSION marker for this patched build
# =====================================================================
#CODE_VERSION = "fix41u_evo_diag_prewire_replay_visibility"
# =====================================================================
# PATCH FIX41J (ADD): bump CODE_VERSION to this file version (additive override)
# PATCH FIX40 (ADD): prior CODE_VERSION preserved above
# PATCH FIX33E (ADD): previous CODE_VERSION was: CODE_VERSION = "fix33_fixed_indent.py"  # PATCH FIX33D (ADD): set CODE_VERSION to filename
# PATCH FIX33D (ADD): previous CODE_VERSION was: CODE_VERSION = "v7_41_endstate_fix24_sheets_replay_scrape_unified_engine_fix27_strict_schema_gate_v2"
# =====================================================================
# PATCH FINAL (ADDITIVE): end-state single bump label (non-breaking)
# NOTE: We do not overwrite CODE_VERSION to avoid any legacy coupling.
# =====================================================================
# PATCH FIX41AFC18 (ADDITIVE): bump CODE_VERSION to this file version
# =====================================================================
#CODE_VERSION = "fix41afc51_evo_anchor_payload_enrich_v1"
# =====================================================================
# PATCH FIX41AFC20 (ADDITIVE): bump CODE_VERSION to this file version
# - Purely a version label for debugging/traceability.
# - Does NOT alter runtime logic.
# =====================================================================
#CODE_VERSION = "fix41afc51_evo_anchor_payload_enrich_v1"
# =====================================================================

# =====================================================================
# Consumers can prefer ENDSTATE_FINAL_VERSION when present.
# =====================================================================
ENDSTATE_FINAL_VERSION = "v7_41_endstate_final_1"
INJ_TRACE_PATCH_VERSION = "fix41q_inj_trace_v1_always_emit"
# =====================================================================

# =====================================================================
# PATCH ES2/ES8/ES9 (ADDITIVE): shared determinism helpers for drift=0
# - Deterministic sorting / tie-breaking helpers
# - Deterministic candidate index builder (anchor_hash -> best candidate)
# - Lightweight schema + universe hashing for convergence checks
# - One-button end-state validation harness (callable)
# NOTE: Additive only; existing logic remains intact.
# =====================================================================
import hashlib as _es_hashlib

def _es_hash_text(s: str) -> str:
    try:
        return _es_hashlib.sha256((s or "").encode("utf-8")).hexdigest()
    except Exception:
        return ""

def _es_stable_sort_key(v):
    """
    Deterministic sort key that never relies on Python's randomized hash().
    Keeps ordering stable across runs for mixed types.
    """
    try:
        if v is None:
            return (0, "")
        if isinstance(v, (int, float)):
            return (1, f"{v:.17g}")
        if isinstance(v, str):
            return (2, v)
        if isinstance(v, bytes):
            return (3, v.decode("utf-8", "ignore"))
        if isinstance(v, dict):
            items = sorted(((str(k), _es_stable_sort_key(vv)) for k, vv in v.items()), key=lambda x: x[0])
            return (4, str(items))
        if isinstance(v, (list, tuple, set)):
            lst = list(v)
            try:
                lst.sort(key=_es_stable_sort_key)
            except Exception:
                lst = sorted(lst, key=lambda x: str(x))
            return (5, str([_es_stable_sort_key(x) for x in lst]))
        return (9, str(v))
    except Exception:
        return (9, str(v))

def _es_sorted_pairs_from_sources_cache(baseline_sources_cache):
    pairs = []
    for sr in (baseline_sources_cache or []):
        if not isinstance(sr, dict):
            continue
        u = (sr.get("source_url") or sr.get("url") or "").strip()
        fp = (sr.get("source_fingerprint") or sr.get("fingerprint") or sr.get("content_fingerprint") or "").strip()
        if u and fp:
            pairs.append((u, fp))
    pairs.sort(key=lambda t: (t[0], t[1]))
    return pairs

def _es_compute_canonical_universe_hash(primary_metrics_canonical: dict, metric_schema_frozen: dict) -> str:
    try:
        keys = set()
        if isinstance(primary_metrics_canonical, dict):
            keys.update([str(k) for k in primary_metrics_canonical.keys()])
        if isinstance(metric_schema_frozen, dict):
            keys.update([str(k) for k in metric_schema_frozen.keys()])
        return _es_hash_text("|".join(sorted(keys)))
    except Exception:
        return ""

def _es_compute_schema_hash(metric_schema_frozen: dict) -> str:
    """
    Deterministic hash of schema fields that affect numeric comparisons.
    Keeps it lightweight: tolerances + units + scale hints only.
    """
    try:
        if not isinstance(metric_schema_frozen, dict):
            return ""
        rows = []
        for k in sorted(metric_schema_frozen.keys()):
            s = metric_schema_frozen.get(k) or {}
            if not isinstance(s, dict):
                continue
            abs_eps = s.get("abs_eps", s.get("ABS_EPS"))
            rel_eps = s.get("rel_eps", s.get("REL_EPS"))
            unit = s.get("unit") or s.get("units") or ""
            scale = s.get("scale") or s.get("magnitude") or ""
            rows.append(f"{k}::abs={abs_eps}::rel={rel_eps}::unit={unit}::scale={scale}")
        return _es_hash_text("|".join(rows))
    except Exception:
        return ""

def _es_build_candidate_index_deterministic(baseline_sources_cache):
    """
    Deterministically build anchor_hash -> candidate map.
    If multiple candidates share the same anchor_hash, choose the best by a stable
    tie-breaker that prefers:
      - higher anchor_confidence
      - longer context_snippet (more evidence)
      - stable context_hash / numeric value / unit
      - stable source_url
    """
    try:
        buckets = {}
        for sr in (baseline_sources_cache or []):
            if not isinstance(sr, dict):
                continue
            su = sr.get("source_url") or sr.get("url") or ""
            for cand in (sr.get("extracted_numbers") or []):
                if not isinstance(cand, dict):
                    continue
                ah = cand.get("anchor_hash") or cand.get("anchor") or ""
                if not ah:
                    continue
                c2 = dict(cand)
                if "source_url" not in c2:
                    c2["source_url"] = su
                buckets.setdefault(ah, []).append(c2)

        out = {}
        for ah in sorted(buckets.keys()):
            cands = buckets.get(ah) or []
            def _cand_key(c):
                try:
                    conf = c.get("anchor_confidence")
                    conf_key = -(float(conf) if conf is not None else 0.0)
                except Exception:
                    conf_key = 0.0
                ctx = (c.get("context_snippet") or c.get("context") or "")
                ctx_len = -len(str(ctx))
                ctx_hash = c.get("context_hash") or ""
                val = c.get("value")
                unit = c.get("unit") or ""
            # PATCH FIX27 (ADDITIVE): Eligibility gate BEFORE scoring.
            # Reject bare-year tokens for non-year metrics when there is no token unit evidence.
            if expected_kind != "year":
                raw_token = (c.get("raw") or "").strip()
                if _fix27_is_bare_year_token(raw_token, c.get("value_norm")) and not _fix27_has_any_unit_evidence(c):
                    continue
            # Typed metrics require explicit token-level unit evidence
            if expected_kind == "currency" and not _fix27_has_currency_evidence(c):
                continue
            if expected_kind == "percent" and not _fix27_has_percent_evidence(c):
                continue
            if expected_kind == "unit" and not _fix27_has_any_unit_evidence(c):
                continue
                su = c.get("source_url") or ""
                return (conf_key, ctx_len, str(ctx_hash), _es_stable_sort_key(val), str(unit), str(su))
            cands_sorted = sorted(cands, key=_cand_key)
            out[ah] = cands_sorted[0] if cands_sorted else None
        return out
    except Exception:
        return {}

def end_state_validation_harness(baseline_analysis: dict, evolution_output: dict, min_stability: float = 99.9) -> dict:
    """
    PATCH ES9 (ADDITIVE): one-button end-state validation (warn-only helper)
    Use this to assert drift=0 on identical inputs.

    Returns a dict with pass/fail booleans and diagnostic fields.
    This does NOT mutate inputs.
    """
    report = {
        "passed": False,
        "checks": {},
        "notes": [],
    }
    try:
        base_prev = baseline_analysis or {}
        evo = evolution_output or {}

        # Snapshot hash
        base_snap = base_prev.get("source_snapshot_hash") or base_prev.get("results", {}).get("source_snapshot_hash")
        evo_snap = evo.get("source_snapshot_hash")

        # Universe + schema hashes
        base_uni = base_prev.get("canonical_universe_hash") or base_prev.get("results", {}).get("canonical_universe_hash")
        base_sch = base_prev.get("schema_hash") or base_prev.get("results", {}).get("schema_hash")
        evo_uni = evo.get("canonical_universe_hash")
        evo_sch = evo.get("schema_hash")

        report["checks"]["snapshot_hash_match"] = bool(base_snap and evo_snap and base_snap == evo_snap)
        report["checks"]["canonical_universe_hash_match"] = bool(base_uni and evo_uni and base_uni == evo_uni)
        report["checks"]["schema_hash_match"] = bool(base_sch and evo_sch and base_sch == evo_sch)

        # Stability threshold (warn-only semantics: "passed" includes match + stability)
        try:
            st = float(evo.get("stability_score") or 0.0)
        except Exception:
            st = 0.0
        report["checks"]["stability_meets_threshold"] = bool(st + 1e-9 >= float(min_stability))

        # Drift suspicion flag (if your pipeline sets it)
        report["checks"]["drift_suspected_flag_false"] = (evo.get("drift_suspected") is False)

        # Final pass condition
        report["passed"] = (
            report["checks"]["snapshot_hash_match"]
            and report["checks"]["canonical_universe_hash_match"]
            and report["checks"]["schema_hash_match"]
            and report["checks"]["stability_meets_threshold"]
        )

        if not report["passed"]:
            report["notes"].append("If hashes match but stability is low, inspect candidate tie-breaks and ordering.")
    except Exception:
        report["notes"].append("Validation harness encountered an exception (non-fatal).")
    return report
# =====================================================================

            # =========================


# =========================================================
# GOOGLE SHEETS HISTORY STORAGE
# =========================================================

SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive"
]
MAX_HISTORY_ITEMS = 50

@st.cache_resource
def get_google_sheet():
    """Connect to Google Sheet (cached connection)"""
    try:
        creds = Credentials.from_service_account_info(
            dict(st.secrets["gcp_service_account"]),
            scopes=SCOPES
        )
        client = gspread.authorize(creds)

        # ===================== PATCH GS1 (ADDITIVE): prefer explicit History worksheet =====================
        # Why:
        # - Your spreadsheet contains multiple tabs (e.g., "New Analysis", "History", "HistoryFull", "Snapshots")
        # - sheet1 is often NOT "History", so get_history() reads the wrong tab and sees "no analyses"
        # Behavior:
        # - Default worksheet_title = "History" (override via secrets: google_sheets.history_worksheet)
        # - Fallback to sheet1 only if the worksheet doesn't exist
        spreadsheet_name = (
            st.secrets.get("google_sheets", {}).get("spreadsheet_name", "Yureeka_JSON")
        )
        ss = client.open(spreadsheet_name)

        worksheet_title = st.secrets.get("google_sheets", {}).get("history_worksheet", "History")
        try:
            sheet = ss.worksheet(worksheet_title)
        except Exception:
            sheet = ss.sheet1
        # =================== END PATCH GS1 (ADDITIVE) ===================

        # Ensure headers exist - handle response object
        try:
            headers = sheet.row_values(1)
            if not headers or len(headers) == 0 or headers[0] != "id":
                # update() returns a response object in newer gspread - ignore it
                _ = sheet.update('A1:E1', [["id", "timestamp", "question", "confidence", "data"]])
        except gspread.exceptions.APIError:
            _ = sheet.update('A1:E1', [["id", "timestamp", "question", "confidence", "data"]])
        except Exception:
            pass  # Headers probably already exist

        return sheet

    except gspread.exceptions.SpreadsheetNotFound:
        st.error("❌ Spreadsheet not found. Create 'Yureeka_JSON' (or your configured name) and share with service account.")
        return None
    except Exception as e:
        error_str = str(e)
        # Ignore Response [200] - it's actually success
        if "Response [200]" in error_str:
            # This means the connection worked, try to return the sheet anyway
            try:
                creds = Credentials.from_service_account_info(
                    dict(st.secrets["gcp_service_account"]),
                    scopes=SCOPES
                )
                client = gspread.authorize(creds)

                # ===================== PATCH GS1b (ADDITIVE): same worksheet selection in fallback =====================
                spreadsheet_name = st.secrets.get("google_sheets", {}).get("spreadsheet_name", "Yureeka_JSON")
                ss = client.open(spreadsheet_name)
                worksheet_title = st.secrets.get("google_sheets", {}).get("history_worksheet", "History")
                try:
                    return ss.worksheet(worksheet_title)
                except Exception:
                    return ss.sheet1
                # =================== END PATCH GS1b (ADDITIVE) ===================
            except:
                pass
        st.error(f"❌ Failed to connect to Google Sheets: {e}")
        return None

def generate_analysis_id() -> str:
    """Generate unique ID for analysis"""
    return f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hashlib.md5(str(datetime.now().timestamp()).encode()).hexdigest()[:6]}"


# =====================================================================
# PATCH AI_A (ADDITIVE): emit metric_anchors in analysis payload (analysis-time)
# Why:
# - Evolution/diff are now anchor-driven; analysis must persist a deterministic
#   canonical_key -> anchor_hash mapping for drift=0 convergence.
# - Some UI/Sheets wrappers omit anchors unless explicitly emitted.
# Determinism:
# - Only uses existing evidence/candidates already present in the analysis payload.
# - No re-fetching; no heuristic matching.
# =====================================================================
def _emit_metric_anchors_in_analysis_payload(analysis_obj: dict) -> dict:
    try:
        if not isinstance(analysis_obj, dict):
            return analysis_obj

        # If already present and non-empty, keep as-is
        existing = analysis_obj.get("metric_anchors")
        if isinstance(existing, dict) and existing:
            return analysis_obj

        # Identify the primary response container (some payloads store it nested)
        pr = analysis_obj.get("primary_response") if isinstance(analysis_obj.get("primary_response"), dict) else None
        pr = pr or analysis_obj

        # Canonical metrics (preferred)
        pmc = pr.get("primary_metrics_canonical") if isinstance(pr, dict) else None
        if not isinstance(pmc, dict) or not pmc:
            pmc = analysis_obj.get("primary_metrics_canonical") if isinstance(analysis_obj.get("primary_metrics_canonical"), dict) else {}

        # Candidate lookup table from baseline snapshots (if present)
        bsc = None
        try:
            r = analysis_obj.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache"), list):
                bsc = r.get("baseline_sources_cache")
        except Exception:
            bsc = None
        if bsc is None and isinstance(analysis_obj.get("baseline_sources_cache"), list):
            bsc = analysis_obj.get("baseline_sources_cache")

        def _safe_str(x):
            try:
                return str(x).strip()
            except Exception:
                return ""

        # Build (anchor_hash -> best candidate) index deterministically
        anchor_to_candidate = {}
        cand_to_candidate = {}
        try:
            if isinstance(bsc, list):
                for sr in bsc:
                    if not isinstance(sr, dict):
                        continue
                    surl = sr.get("source_url") or sr.get("url")
                    for n in (sr.get("extracted_numbers") or []):
                        if not isinstance(n, dict):
                            continue
                        ah = _safe_str(n.get("anchor_hash"))
                        cid = _safe_str(n.get("candidate_id"))
                        if ah and ah not in anchor_to_candidate:
                            anchor_to_candidate[ah] = dict(n, source_url=n.get("source_url") or surl)
                        if cid and cid not in cand_to_candidate:
                            cand_to_candidate[cid] = dict(n, source_url=n.get("source_url") or surl)
        except Exception:
            pass

        metric_anchors = {}

        # Deterministic iteration for stable JSON output
        for ckey in sorted([str(k) for k in (pmc or {}).keys()]):
            m = pmc.get(ckey)
            if not isinstance(m, dict):
                continue

            # Pick anchor identifiers from evidence first (most authoritative)
            ev = m.get("evidence") or []
            if not isinstance(ev, list):
                ev = []

            best = None
            for e in ev:
                if not isinstance(e, dict):
                    continue
                ah = _safe_str(e.get("anchor_hash") or e.get("anchor"))
                cid = _safe_str(e.get("candidate_id"))
                if ah or cid:
                    best = e
                    break

            # Fallback: sometimes metric row carries anchor_hash directly
            if best is None:
                best = {
                    "anchor_hash": m.get("anchor_hash") or m.get("anchor"),
                    "candidate_id": m.get("candidate_id"),
                    "source_url": m.get("source_url") or m.get("url"),
                    "context_snippet": m.get("context_snippet") or m.get("context"),
                    "anchor_confidence": m.get("anchor_confidence"),
                }

            ah = _safe_str(best.get("anchor_hash") or best.get("anchor"))
            cid = _safe_str(best.get("candidate_id"))
            surl = best.get("source_url") or best.get("url")
            ctx = best.get("context_snippet") or best.get("context")
            aconf = best.get("anchor_confidence")

            # Enrich from candidate index if needed
            if (not surl) or (not ctx):
                cand = None
                if ah and ah in anchor_to_candidate:
                    cand = anchor_to_candidate.get(ah)
                elif cid and cid in cand_to_candidate:
                    cand = cand_to_candidate.get(cid)
                if isinstance(cand, dict):
                    surl = surl or (cand.get("source_url") or cand.get("url"))
                    ctx = ctx or (cand.get("context_snippet") or cand.get("context"))

            # Only emit if we actually have an anchor id
            if not (ah or cid):
                continue

            try:
                if isinstance(ctx, str):
                    ctx = ctx.strip()[:220]
                else:
                    ctx = None
            except Exception:
                ctx = None

            try:
                aconf = float(aconf) if aconf is not None else None
            except Exception:
                aconf = None

            metric_anchors[ckey] = {
                "canonical_key": ckey,
                "anchor_hash": ah or None,
                "candidate_id": cid or None,
                "source_url": surl or None,
                "context_snippet": ctx,
                "anchor_confidence": aconf,
            }

            # -----------------------------------------------------------------
            # PATCH AI_B (ADDITIVE): also backfill anchor fields onto the metric row
            # -----------------------------------------------------------------
            try:
                if ah and not _safe_str(m.get("anchor_hash")):
                    m["anchor_hash"] = ah
                if cid and not _safe_str(m.get("candidate_id")):
                    m["candidate_id"] = cid
                if surl and not (m.get("source_url") or m.get("url")):
                    m["source_url"] = surl
                if ctx and not (m.get("context_snippet") or m.get("context")):
                    m["context_snippet"] = ctx
                if aconf is not None and m.get("anchor_confidence") is None:
                    m["anchor_confidence"] = aconf
            except Exception:
                pass
            # -----------------------------------------------------------------

        if metric_anchors:
            # -----------------------------------------------------------------
            # PATCH AI_C (ADDITIVE): persist in all common locations
            # -----------------------------------------------------------------
            try:
                analysis_obj["metric_anchors"] = metric_anchors
            except Exception:
                pass
            try:
                if isinstance(pr, dict):
                    pr.setdefault("metric_anchors", metric_anchors)
            except Exception:
                pass
            try:
                analysis_obj.setdefault("results", {})
                if isinstance(analysis_obj["results"], dict):
                    analysis_obj["results"].setdefault("metric_anchors", metric_anchors)
            except Exception:
                pass
            # -----------------------------------------------------------------

        return analysis_obj
    except Exception:
        return analysis_obj
# =====================================================================

def add_to_history(analysis: dict) -> bool:
    """
    Save analysis to Google Sheet (or session fallback).

    ADDITIVE end-state wiring:
      - If a baseline source cache exists, build & store:
          * evidence_records (structured, cached)
          * metric_anchors (baseline metrics anchored to evidence)
      - Prevent Google Sheets 50,000-char single-cell limit errors by shrinking only
        the JSON payload written into the single "analysis json" cell when necessary.

    Backward compatible:
      - Only adds keys; does not remove existing fields.
      - Never blocks saving if enrichment fails.
      - If Sheets unavailable, falls back to session_state.
    """

    # =====================================================================
    # PATCH AI_A_CALL (ADDITIVE): ensure metric_anchors emitted before persistence
    # =====================================================================
    try:
        analysis = _emit_metric_anchors_in_analysis_payload(analysis)
    except Exception:
        pass
    # =====================================================================

    import json
    import re
    import streamlit as st
    from datetime import datetime

    SHEETS_CELL_LIMIT = 50000

    # -----------------------
    # PATCH A1 (ADDITIVE): robustly locate baseline_sources_cache
    # - Added primary_response.baseline_sources_cache as extra fallback
    # -----------------------
    baseline_cache = (
        analysis.get("baseline_sources_cache")
        or (analysis.get("primary_response", {}) or {}).get("baseline_sources_cache")
        or (analysis.get("results", {}) or {}).get("baseline_sources_cache")
        or (analysis.get("results", {}) or {}).get("source_results")
    )

    # -----------------------
    # PATCH A2 (ADDITIVE): build evidence_records deterministically
    # -----------------------
    def _build_evidence_records_from_baseline_cache(baseline_cache_obj):
        records = []
        if not isinstance(baseline_cache_obj, list):
            return records

        # helper: safe sha1 fallback if needed
        def _sha1(s: str) -> str:
            try:
                import hashlib
                return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()
            except Exception:
                return ""

        for sr in baseline_cache_obj:
            if not isinstance(sr, dict):
                continue
            url = sr.get("url") or ""
            fp = sr.get("fingerprint")
            fetched_at = sr.get("fetched_at")

            nums = sr.get("extracted_numbers") or []
            clean_nums = []

            if isinstance(nums, list):
                for n in nums:
                    if not isinstance(n, dict):
                        continue

                    # optional canonicalization hook
                    try:
                        fn = globals().get("canonicalize_numeric_candidate")
                        if callable(fn):
                            n = fn(dict(n))
                    except Exception:
                        n = dict(n)

                    raw = (n.get("raw") or "").strip()
                    ctx = (n.get("context_snippet") or n.get("context") or "").strip()
                    anchor_hash = n.get("anchor_hash") or _sha1(f"{url}|{raw}|{ctx[:240]}")

                    clean_nums.append({
                        "value": n.get("value"),
                        "unit": n.get("unit"),
                        "unit_tag": n.get("unit_tag"),
                        "unit_family": n.get("unit_family"),
                        "base_unit": n.get("base_unit"),
                        "multiplier_to_base": n.get("multiplier_to_base"),
                        "value_norm": n.get("value_norm"),

                        "raw": raw,
                        "context_snippet": ctx[:240],
                        "anchor_hash": anchor_hash,
            # PATCH FIX41AFC50 START — propagate candidate_id for anchor parity
            "candidate_id": (anchor_hash[:16] if isinstance(anchor_hash, str) else ""),
            # PATCH FIX41AFC50 END

                "candidate_id": hashlib.sha1(str(anchor_hash or "").encode("utf-8")).hexdigest()[:16] if anchor_hash else None,

            # =====================================================================
            # PATCH AI2 (ADDITIVE): anchor integrity fields
            # - candidate_id is a stable short id derived from anchor_hash
            # - anchor_basis documents what the anchor_hash was built from
            # =====================================================================
            "candidate_id": (str(anchor_hash)[:16] if anchor_hash else None),
            "anchor_basis": "url|raw|context",
            # =====================================================================
                        "source_url": n.get("source_url") or url,

                        "start_idx": n.get("start_idx"),
                        "end_idx": n.get("end_idx"),

                        "is_junk": bool(n.get("is_junk")) if isinstance(n.get("is_junk"), bool) else False,
                        "junk_reason": n.get("junk_reason") or "",

                        "measure_kind": n.get("measure_kind"),
                        "measure_assoc": n.get("measure_assoc"),
                    })

            # stable ordering (prefer your helper if present)
            try:
                if "sort_snapshot_numbers" in globals() and callable(globals()["sort_snapshot_numbers"]):
                    clean_nums = sort_snapshot_numbers(clean_nums)
                else:
                    clean_nums = sorted(
                        clean_nums,
                        key=lambda x: (str(x.get("anchor_hash") or ""), str(x.get("raw") or ""))
                    )
            except Exception:
                pass

            records.append({
                "url": url,
                "fetched_at": fetched_at,
                "fingerprint": fp,
                "numbers": clean_nums,
            })

        # stable ordering (prefer helper if present)
        try:
            if "sort_evidence_records" in globals() and callable(globals()["sort_evidence_records"]):
                records = sort_evidence_records(records)
            else:
                records = sorted(records, key=lambda r: str(r.get("url") or ""))
        except Exception:
            pass

        return records

    # -----------------------
    # PATCH A3 (ADDITIVE): build metric_anchors deterministically (schema-first if present)
    # -----------------------
    def _build_metric_anchors(primary_metrics_canonical, evidence_records):
        """
        Build a deterministic metric_anchors mapping for drift=0.

        PATCH AI1 (ADDITIVE): Anchor integrity
        - Prefer the anchor_hash/candidate_id already chosen during analysis (metric["evidence"]).
        - Fall back to scanning evidence_records for a candidate with the same anchor_hash/candidate_id.
        - As a last resort, pick a best candidate deterministically by (abs(value_norm-target), context length).
        - NEVER invent anchors; if no usable candidate, omit the anchor for that metric.
        """
        anchors = {}
        if not isinstance(primary_metrics_canonical, dict) or not primary_metrics_canonical:
            return anchors
        if not isinstance(evidence_records, list):
            evidence_records = []

        import hashlib

        def _sha1(s: str) -> str:
            try:
                return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()
            except Exception:
                return ""

        def _ensure_anchor_fields(c: dict, source_url: str = "") -> dict:
            c = c if isinstance(c, dict) else {}
            # context snippet normalization
            ctx = c.get("context_snippet") or c.get("context") or ""
            if isinstance(ctx, str):
                ctx = ctx.strip()[:240]
            else:
                ctx = ""
            raw = c.get("raw")
            if raw is None:
                # stable raw representation
                v = c.get("value_norm") if c.get("value_norm") is not None else c.get("value")
                u = c.get("base_unit") or c.get("unit") or ""
                raw = f"{v}{u}"
            raw = str(raw)[:120]

            ah = c.get("anchor_hash") or c.get("anchor") or ""
            if not ah:
                ah = _sha1(f"{source_url}|{raw}|{ctx}")
                if ah:
                    c["anchor_hash"] = ah

            if not c.get("candidate_id") and ah:
                c["candidate_id"] = str(ah)[:16]

            # keep normalized ctx/source_url for downstream
            if source_url and not c.get("source_url"):
                c["source_url"] = source_url
            if ctx and not c.get("context_snippet"):
                c["context_snippet"] = ctx

            return c

        # Pre-index evidence_records by (anchor_hash, candidate_id)
        anchor_index = {}
        candidate_index = {}
        value_index = {}  # ckey -> list of candidates (for fallback)

        for rec in evidence_records:
            if not isinstance(rec, dict):
                continue
            url = rec.get("source_url") or rec.get("url") or ""
            for c in (rec.get("candidates") or rec.get("extracted_numbers") or []):
                if not isinstance(c, dict):
                    continue
                c = _ensure_anchor_fields(c, url)
                ah = c.get("anchor_hash")
                cid = c.get("candidate_id")
                if ah and ah not in anchor_index:
                    anchor_index[ah] = c
                if cid and cid not in candidate_index:
                    candidate_index[cid] = c

        # Determine anchors per metric
        for ckey, m in primary_metrics_canonical.items():
            if not isinstance(m, dict):
                continue

            # --- Preferred: use the analysis-chosen evidence (integrity) ---
            chosen = None
            ev = m.get("evidence") or []
            if isinstance(ev, list) and ev:
                # pick first usable evidence deterministically
                for e in ev:
                    if not isinstance(e, dict):
                        continue
                    url = e.get("source_url") or e.get("url") or ""
                    e2 = _ensure_anchor_fields(dict(e), url)
                    ah = e2.get("anchor_hash")
                    cid = e2.get("candidate_id")
                    if ah or cid:
                        chosen = e2
                        break

            # --- Fallback 1: resolve by anchor_hash/candidate_id in evidence_records ---
            if isinstance(chosen, dict):
                ah = chosen.get("anchor_hash")
                cid = chosen.get("candidate_id")
                if ah and ah in anchor_index:
                    chosen = dict(anchor_index[ah])
                elif cid and cid in candidate_index:
                    chosen = dict(candidate_index[cid])

            # --- Fallback 2: deterministic best-by-value in the same source_url (if any) ---
            if not isinstance(chosen, dict) or not (chosen.get("anchor_hash") or chosen.get("candidate_id")):
                # gather candidates from evidence_records that match the metric's preferred source_url (if known)
                preferred_url = ""
                try:
                    if isinstance(ev, list) and ev:
                        preferred_url = str((ev[0] or {}).get("source_url") or (ev[0] or {}).get("url") or "")
                except Exception:
                    preferred_url = ""

                target = m.get("value_norm")
                try:
                    target = float(target) if target is not None else None
                except Exception:
                    target = None

                pool = []
                for rec in evidence_records:
                    if not isinstance(rec, dict):
                        continue
                    url = str(rec.get("source_url") or rec.get("url") or "")
                    if preferred_url and url != preferred_url:
                        continue
                    for c in (rec.get("candidates") or rec.get("extracted_numbers") or []):
                        if not isinstance(c, dict):
                            continue
                        cc = _ensure_anchor_fields(dict(c), url)
                        pool.append(cc)

                if pool:
                    def _score(cc):
                        ctx = cc.get("context_snippet") or ""
                        try:
                            v = cc.get("value_norm")
                            v = float(v) if v is not None else None
                        except Exception:
                            v = None
                        dv = abs(v - target) if (v is not None and target is not None) else 1e30
                        return (dv, -len(str(ctx)), str(cc.get("anchor_hash") or ""), str(url))
                    pool.sort(key=_score)
                    chosen = pool[0]

            if not isinstance(chosen, dict):
                continue

            # emit anchor record (stable shape)
            anchors[ckey] = {
                "canonical_key": ckey,
                "anchor_hash": chosen.get("anchor_hash"),
# =====================================================================
# PATCH AI3 (ADDITIVE): anchor integrity fingerprint (analysis-time)
# Why:
# - Provides a stable, inspectable signature tying the anchor to a specific
#   candidate (url + anchor_hash + value_norm + base_unit).
# - Helps detect silent anchor drift across analysis/evolution.
# =====================================================================
"anchor_integrity": {
    "candidate_id": chosen.get("candidate_id"),
    "value_norm": chosen.get("value_norm"),
    "base_unit": chosen.get("base_unit") or chosen.get("unit"),
    "fingerprint": chosen.get("fingerprint"),
    "integrity_hash": _es_hash_text(
        f"{ckey}|{chosen.get('anchor_hash')}|{chosen.get('source_url') or chosen.get('url') or ''}|{chosen.get('value_norm')}|{chosen.get('base_unit') or chosen.get('unit') or ''}"
    ) if callable(globals().get("_es_hash_text")) else None,
},
# =====================================================================
                "candidate_id": chosen.get("candidate_id"),
                "source_url": chosen.get("source_url") or chosen.get("url"),
                "context_snippet": chosen.get("context_snippet") or chosen.get("context"),
                "anchor_confidence": chosen.get("anchor_confidence") or chosen.get("confidence"),
            }

        # deterministic ordering (stable JSON)
        try:
            anchors = dict(sorted(anchors.items(), key=lambda kv: str(kv[0])))
        except Exception:
            pass

        return anchors
        def _tokenize(s: str):
            return [t for t in re.findall(r"[a-z0-9]+", (s or "").lower()) if len(t) > 2]

        # PATCH A3.1 (ADDITIVE): tiny float helper for deterministic closeness scoring
        def _to_float(x):
            try:
                return float(x)
            except Exception:
                return None

        # PATCH A3.9 (ADDITIVE): currency evidence helper
        # - Needed because many currency metrics appear as magnitude-tagged numbers (e.g., "40.7M")
        #   with currency implied in nearby context ("USD", "revenue", "$", etc.)
        def _has_currency_evidence(raw: str, ctx: str) -> bool:
            r = (raw or "")
            c = (ctx or "").lower()
            if any(s in r for s in ["$", "S$", "€", "£"]):
                return True
            if any(code in c for code in [" usd", "sgd", " eur", " gbp", " aud", " cad", " jpy", " cny", " rmb"]):
                return True
            strong_kw = [
                "revenue", "turnover", "valuation", "valued at", "market value", "market size",
                "sales value", "net profit", "operating profit", "gross profit",
                "ebitda", "earnings", "income", "capex", "opex"
            ]
            if any(k in c for k in strong_kw):
                return True
            return False
        # =========================

        # flatten candidates
        all_nums = []
        for rec in evidence_records:
            if not isinstance(rec, dict):
                continue
            for n in (rec.get("numbers") or []):
                if isinstance(n, dict):
                    all_nums.append(n)

        # PATCH A3.2 (ADDITIVE): normalize_unit_tag + unit_family hooks (if present)
        _norm_tag_fn = globals().get("normalize_unit_tag")
        _unit_family_fn = globals().get("unit_family")

        for ckey, m in primary_metrics_canonical.items():
            if not isinstance(m, dict):
                continue

            schema = (metric_schema_frozen or {}).get(ckey) if isinstance(metric_schema_frozen, dict) else None
            expected_family = (schema.get("unit_family") or "").lower().strip() if isinstance(schema, dict) else ""
            expected_unit = (schema.get("unit") or "").strip() if isinstance(schema, dict) else ""
            expected_dim = (schema.get("dimension") or "").lower().strip() if isinstance(schema, dict) else ""

            # tokens: schema keywords + metric name tokens
            toks = []
            if isinstance(schema, dict):
                for k in (schema.get("keywords") or []):
                    toks.extend(_tokenize(str(k)))
            toks.extend(_tokenize(m.get("name") or m.get("original_name") or ""))
            toks = list(dict.fromkeys(toks))[:40]

            best = None
            best_key = None

            # PATCH A3.3 (ADDITIVE): metric value reference for closeness bonus
            m_val = _to_float(m.get("value_norm") if m.get("value_norm") is not None else m.get("value"))

            # PATCH A3.4 (ADDITIVE): normalized expected tag (schema unit may be "M", "%", etc.)
            exp_tag = expected_unit
            try:
                if callable(_norm_tag_fn):
                    exp_tag = _norm_tag_fn(expected_unit)
            except Exception:
                pass

            # PATCH A3.10 (ADDITIVE): metric unit_tag (if available) to gate closeness bonus
            m_tag = (m.get("unit_tag") or "").strip()

            for cand in all_nums:
                if cand.get("is_junk") is True:
                    continue

                ctx = cand.get("context_snippet") or ""
                c_ut = (cand.get("unit_tag") or "").strip()
                c_fam = (cand.get("unit_family") or "").lower().strip()

                # =========================
                # PATCH A3.5 (ADDITIVE): derive candidate family if missing
                # - prevents leakage when unit_family wasn't populated upstream
                # =========================
                if not c_fam:
                    try:
                        if callable(_unit_family_fn):
                            c_fam = str(_unit_family_fn(c_ut or "") or "").lower().strip()
                    except Exception:
                        pass
                # =========================

                # =========================
                # PATCH A3.7 (ADDITIVE): prefer unit_tag matching (normalized) over raw unit matching
                # PATCH A3.11 (ADDITIVE): extend normalization fallback to raw/context
                # - helps older snapshots where unit_tag/unit may be empty but raw/context carries scale ("million", "%")
                # =========================
                cand_tag = c_ut
                try:
                    if callable(_norm_tag_fn):
                        cand_tag = _norm_tag_fn(c_ut or cand.get("unit") or cand.get("raw") or ctx)
                except Exception:
                    pass
                # =========================

                # =========================
                # PATCH A3.6 (FIX): schema-first family gate with currency exception
                # - Currency metrics often appear as magnitude candidates ("40.7M") + currency evidence in context.
                # - We allow cand_fam == "magnitude" for expected_family == "currency" ONLY when currency evidence exists.
                # =========================
                if expected_family in ("percent", "currency", "magnitude", "energy"):
                    if expected_family == "currency":
                        if c_fam not in ("currency", "magnitude"):
                            continue
                        if c_fam == "magnitude" and not _has_currency_evidence(cand.get("raw", ""), ctx):
                            continue
                    else:
                        if (c_fam or "") != expected_family:
                            continue
                # =========================

                # dimension/meaning gate using measure_kind when present (soft but helpful)
                mk = cand.get("measure_kind")
                if expected_dim == "percent" and mk and mk not in ("share_pct", "growth_pct", "percent_other"):
                    continue
                if expected_dim == "currency" and mk and mk == "count_units":
                    continue

                c_tokens = set(_tokenize(ctx))
                overlap = sum(1 for t in toks if t in c_tokens) if toks else 0
                score = overlap / max(1, len(toks))

                bonus = 0.0

                # =========================
                # PATCH A3.7 (ADDITIVE): tag-based unit bonus (stronger)
                # =========================
                if exp_tag and cand_tag and cand_tag == exp_tag:
                    bonus += 0.07
                # keep a small legacy bonus if exact unit string matches too
                if expected_unit and (str(cand.get("unit") or "").strip() == expected_unit):
                    bonus += 0.03
                # =========================

                # =========================
                # PATCH A3.8 (ADDITIVE): deterministic value closeness bonus (guarded)
                # - Only apply when units are comparable (tag match or both use value_norm).
                # - Prevents misleading closeness when one side is normalized and the other isn't.
                # =========================
                c_val = _to_float(cand.get("value_norm") if cand.get("value_norm") is not None else cand.get("value"))
                comparable = False
                if m_tag and cand_tag and m_tag == cand_tag:
                    comparable = True
                elif (m.get("value_norm") is not None) and (cand.get("value_norm") is not None):
                    comparable = True

                if comparable and m_val is not None and c_val is not None:
                    denom = max(1e-9, abs(m_val))
                    rel_err = abs(c_val - m_val) / denom
                    if rel_err <= 0.02:
                        bonus += 0.06
                    elif rel_err <= 0.10:
                        bonus += 0.03
                # =========================

                score = float(score + bonus)

                # stable tie-breaker
                key = (
                    score,
                    str(cand.get("source_url") or ""),
                    str(cand.get("anchor_hash") or ""),
                    str(cand.get("raw") or ""),
                )

                if best_key is None or key > best_key:
                    best_key = key
                    best = cand

            if best and best_key and best_key[0] >= 0.10:
                anchors[ckey] = {
                    # =========================
                    # PATCH MA1 (ADDITIVE): legacy compat fields
                    # =========================
                    "metric_id": ckey,
                    "metric_name": (m.get("name") or m.get("original_name") or ckey),
                    # =========================

                    "canonical_key": ckey,
                    "anchor_hash": best.get("anchor_hash"),
                    "source_url": best.get("source_url"),
                    "raw": best.get("raw"),
                    "unit": best.get("unit"),
                    "unit_tag": best.get("unit_tag"),
                    "unit_family": best.get("unit_family"),
                    "base_unit": best.get("base_unit"),
                    "value": best.get("value"),
                    "value_norm": best.get("value_norm"),
                    "measure_kind": best.get("measure_kind"),
                    "measure_assoc": best.get("measure_assoc"),
                    "context_snippet": (best.get("context_snippet") or "")[:220],
                    "anchor_confidence": float(min(100.0, best_key[0] * 100.0)),

                    # =========================
                    # PATCH A3.12 (ADDITIVE): optional fingerprint passthrough (if present)
                    # - Useful later for evolution/debugging; harmless if missing.
                    # =========================
                    "fingerprint": best.get("fingerprint"),
                    # =========================
                }
            else:
                anchors[ckey] = {
                    # =========================
                    # PATCH MA1 (ADDITIVE): legacy compat fields
                    # =========================
                    "metric_id": ckey,
                    "metric_name": (m.get("name") or m.get("original_name") or ckey),
                    # =========================

                    "canonical_key": ckey,
                    "anchor_hash": None,
                    "source_url": None,
                    "raw": None,
                    "anchor_confidence": 0.0,

                    # PATCH A3.12 (ADDITIVE): keep key present for stable shape
                    "fingerprint": None,
                }

        # stable ordering (prefer helper if present)
        try:
            if "sort_metric_anchors" in globals() and callable(globals()["sort_metric_anchors"]):
                ordered = sort_metric_anchors(list(anchors.values()))
                anchors = {
                    a.get("canonical_key"): a
                    for a in ordered
                    if isinstance(a, dict) and a.get("canonical_key")
                }
        except Exception:
            pass

        # =====================================================================
        # PATCH AI_ANCHHASH1 (ADDITIVE): propagate anchor_hash into metric rows
        # Why:
        # - Drift=0 requires prev metrics to carry anchor_hash so diff can compare
        #   prev_anchor_hash vs cur_anchor_hash deterministically.
        # - We ONLY copy existing anchor_hash from anchors map; no fabrication.
        # =====================================================================
        try:
            if isinstance(primary_metrics_canonical, dict) and isinstance(anchors, dict):
                for _ck, _a in anchors.items():
                    if not isinstance(_a, dict):
                        continue
                    _ah = _a.get("anchor_hash") or _a.get("anchor")
                    if not _ah:
                        continue
                    _mrow = primary_metrics_canonical.get(_ck)
                    if isinstance(_mrow, dict) and not _mrow.get("anchor_hash"):
                        _mrow["anchor_hash"] = _ah
        except Exception:
            pass
        # =====================================================================
        return anchors

    # -----------------------
    # PATCH A4 (ADDITIVE): enrich analysis (never block saving)
    # -----------------------
    try:
        if isinstance(baseline_cache, list) and baseline_cache:
            evidence_records = _build_evidence_records_from_baseline_cache(baseline_cache)

            # =========================
            # PATCH A4.1 (ADDITIVE): evidence layer versioning (pipeline attribution)
            # - Use CODE_VERSION if available; else keep numeric fallback
            # =========================
            try:
                cv = globals().get("CODE_VERSION")
                analysis.setdefault("evidence_layer_version", cv or 1)
            except Exception:
                analysis.setdefault("evidence_layer_version", 1)
            analysis.setdefault("evidence_layer_schema_version", 1)
            # =========================

            # stash on analysis (additive)
            analysis["evidence_records"] = evidence_records

            # build anchors using canonical metrics + frozen schema if present
            primary_resp = analysis.get("primary_response") or {}
            if isinstance(primary_resp, dict):
                pmc = primary_resp.get("primary_metrics_canonical") or analysis.get("primary_metrics_canonical") or {}
                schema = primary_resp.get("metric_schema_frozen") or analysis.get("metric_schema_frozen") or {}
            else:
                pmc = analysis.get("primary_metrics_canonical") or {}
                schema = analysis.get("metric_schema_frozen") or {}

            metric_anchors = _build_metric_anchors(pmc, schema, evidence_records)
            # =====================================================================
            # PATCH ANCH_EMIT1 (ADDITIVE): emit metric_anchors into analysis payload
            # Why:
            # - Evolution (and diff) expects anchors to be discoverable without guessing.
            # - Some storage paths wrap/summarize analysis objects; we persist anchors
            #   at multiple stable locations to survive those wrappers.
            # Determinism:
            # - Anchors are derived only from existing evidence_records / schema / pmc.
            # - No re-fetching; no heuristic matching.
            # =====================================================================
            try:
                if isinstance(metric_anchors, dict) and metric_anchors:
                    # Top-level (preferred)
                    if not isinstance(analysis.get("metric_anchors"), dict):
                        analysis["metric_anchors"] = metric_anchors

                    # Under primary_response (common for older shapes)
                    pr = analysis.get("primary_response")
                    if isinstance(pr, dict) and not isinstance(pr.get("metric_anchors"), dict):
                        pr["metric_anchors"] = metric_anchors

                    # Under results (some evolution lookups)
                    res = analysis.get("results")
                    if isinstance(res, dict) and not isinstance(res.get("metric_anchors"), dict):
                        res["metric_anchors"] = metric_anchors

                    # Lightweight debug hint for wrappers
                    try:
                        dbg = analysis.get("debug")
                        if not isinstance(dbg, dict):
                            dbg = {}
                            analysis["debug"] = dbg
                        dbg.setdefault("metric_anchor_count", int(len(metric_anchors)))
                    except Exception:
                        pass
            except Exception:
                pass
            # =====================================================================

            analysis["metric_anchors"] = metric_anchors
# =====================================================================
            # =====================================================================
            # PATCH AI4 (ADDITIVE): anchor integrity audit (analysis-time)
            # Why:
            # - Detect duplicate anchor_hash values across canonical keys.
            # - Detect missing anchor_hash on anchors and on baseline canonical metrics.
            # - Provide non-breaking debug/audit fields for drift investigations.
            # Notes:
            # - Purely additive; does not mutate anchors beyond attaching audit metadata.
            # =====================================================================
            try:
                if isinstance(analysis, dict):
                    _ma = analysis.get('metric_anchors')
                    _pmc = analysis.get('primary_metrics_canonical')
                    _dup = {}  # anchor_hash -> [canonical_key,...]
                    _missing_anchor = []
                    _missing_metric_anchor = []
                    if isinstance(_ma, dict):
                        for _ck, _a in _ma.items():
                            if not isinstance(_a, dict):
                                _missing_anchor.append(str(_ck))
                                continue
                            _ah = _a.get('anchor_hash') or _a.get('anchor')
                            if not _ah:
                                _missing_anchor.append(str(_ck))
                                continue
                            _ah = str(_ah)
                            _dup.setdefault(_ah, []).append(str(_ck))
                    # anchors missing on baseline metrics (best-effort diagnostic)
                    if isinstance(_pmc, dict):
                        for _ck, _m in _pmc.items():
                            if not isinstance(_m, dict):
                                continue
                            if not (_m.get('anchor_hash') or _m.get('anchor') or _m.get('candidate_id')):
                                _missing_metric_anchor.append(str(_ck))
                    _dup_only = {k: v for k, v in _dup.items() if isinstance(v, list) and len(v) > 1}
                    analysis['anchor_integrity_audit'] = {
                        'anchor_count': int(len(_ma)) if isinstance(_ma, dict) else 0,
                        'duplicate_anchor_hash_count': int(len(_dup_only)),
                        'duplicate_anchor_hash_examples': dict(list(_dup_only.items())[:10]) if _dup_only else {},
                        'missing_anchor_hash_count': int(len(_missing_anchor)),
                        'missing_anchor_hash_examples': _missing_anchor[:20],
                        'metrics_missing_any_anchor_id_count': int(len(_missing_metric_anchor)),
                        'metrics_missing_any_anchor_id_examples': _missing_metric_anchor[:20],
                    }
            except Exception:
                pass
            # =====================================================================

    # -----------------------
    # Existing Google Sheet save behavior (guarded)
    # -----------------------
    except Exception:
        pass

    # =====================================================================
    # PATCH D (ADDITIVE): propagate metric_anchors onto metric rows + evidence
    # Why:
    # - Drift=0 depends on analysis and evolution sharing the SAME anchor IDs.
    # - Some downstream code paths expect anchor_hash on the metric row itself
    #   and/or inside evidence entries (not only in analysis["metric_anchors"]).
    # - This patch copies existing anchor metadata only (no fabrication, no refetch).
    # =====================================================================
    try:
        import re
        import hashlib

        def _norm_ctx(s: str) -> str:
            try:
                return re.sub(r"\s+", " ", (s or "").strip())
            except Exception:
                return (s or "").strip()

        def _compute_anchor_hash_fallback(url: str, ctx: str) -> str:
            try:
                u = (url or "").strip()
                c = _norm_ctx(ctx or "")
                if not u or not c:
                    return ""
                return hashlib.sha1((u + "||" + c).encode("utf-8")).hexdigest()[:16]
            except Exception:
                return ""

        def _compute_anchor_hash(url: str, ctx: str) -> str:
            try:
                fn = globals().get("compute_anchor_hash")
                if callable(fn):
                    return str(fn(url, ctx) or "")
            except Exception:
                pass
            return _compute_anchor_hash_fallback(url, ctx)

        # Locate canonical metrics dict (prefer primary_response)
        _pmc = None
        _pr0 = analysis.get("primary_response") if isinstance(analysis, dict) else None
        if isinstance(_pr0, dict) and isinstance(_pr0.get("primary_metrics_canonical"), dict):
            _pmc = _pr0.get("primary_metrics_canonical")
        if _pmc is None and isinstance(analysis, dict) and isinstance(analysis.get("primary_metrics_canonical"), dict):
            _pmc = analysis.get("primary_metrics_canonical")

        if isinstance(metric_anchors, dict) and isinstance(_pmc, dict):
            for _ckey, _a in metric_anchors.items():
                if not isinstance(_ckey, str) or not _ckey:
                    continue
                if not isinstance(_a, dict) or not _a:
                    continue

                _m = _pmc.get(_ckey)
                if not isinstance(_m, dict):
                    continue

                _ah = str(_a.get("anchor_hash") or _a.get("anchor") or "").strip()
                _src = str(_a.get("source_url") or _a.get("url") or "").strip()
                _ctx = _a.get("context_snippet") or _a.get("context") or ""
                _ctx = _ctx.strip() if isinstance(_ctx, str) else ""

                # Copy onto metric row (only if missing)
                if _ah and not _m.get("anchor_hash"):
                    _m["anchor_hash"] = _ah
                if _src and not (_m.get("source_url") or _m.get("url")):
                    _m["source_url"] = _src
                if _ctx and not (_m.get("context_snippet") or _m.get("context")):
                    _m["context_snippet"] = _ctx[:220]

                # Pass through extra metadata if present (additive)
                if _a.get("anchor_confidence") is not None and _m.get("anchor_confidence") is None:
                    _m["anchor_confidence"] = _a.get("anchor_confidence")
                if _a.get("candidate_id") and not _m.get("candidate_id"):
                    _m["candidate_id"] = _a.get("candidate_id")
                if _a.get("fingerprint") and not _m.get("fingerprint"):
                    _m["fingerprint"] = _a.get("fingerprint")

                # Ensure evidence entries carry anchor_hash (deterministic; no new evidence)
                _ev = _m.get("evidence")
                if isinstance(_ev, list) and _ev:
                    for _e in _ev:
                        if not isinstance(_e, dict):
                            continue
                        if _e.get("anchor_hash"):
                            continue
                        _e_url = str(_e.get("url") or _e.get("source_url") or _src or "").strip()
                        _e_ctx = _e.get("context_snippet") or _e.get("context") or _ctx or ""
                        _e_ctx = _e_ctx.strip() if isinstance(_e_ctx, str) else ""
                        _eh = _compute_anchor_hash(_e_url, _e_ctx)
                        if _eh:
                            _e["anchor_hash"] = _eh
    except Exception:
        pass
    # =====================================================================


    def _try_make_sheet_json(obj: dict) -> str:
        try:
            fn = globals().get("make_sheet_safe_json")
            if callable(fn):
                return fn(obj)
        except Exception:
            pass
        return json.dumps(obj, ensure_ascii=False, default=str)

    def _shrink_for_sheets(original: dict) -> dict:
        base_copy = dict(original)
        s = _try_make_sheet_json(base_copy)
        if isinstance(s, str) and len(s) <= SHEETS_CELL_LIMIT:
            return base_copy

        reduced = dict(base_copy)
        removed = []

        for k in [
            "evidence_records",
            "baseline_sources_cache",
            "metric_anchors",
            "source_results",
            "web_context",
            "scraped_meta",
            "raw_sources",
            "raw_text",
            "debug",
        ]:
            if k in reduced:
                reduced.pop(k, None)
                removed.append(k)

        reduced.setdefault("_sheet_write", {})
        if isinstance(reduced["_sheet_write"], dict):
            reduced["_sheet_write"]["truncated"] = True
            reduced["_sheet_write"]["removed_keys"] = removed[:50]

        s2 = _try_make_sheet_json(reduced)
        if isinstance(s2, str) and len(s2) <= SHEETS_CELL_LIMIT:
            return reduced

        return {
            "question": original.get("question"),
            "timestamp": original.get("timestamp"),
            "final_confidence": original.get("final_confidence"),
            "question_profile": original.get("question_profile"),
            "primary_response": original.get("primary_response") or {},
            "_sheet_write": {
                "truncated": True,
                "mode": "minimal_fallback",
                "note": "Full analysis too large for single Google Sheets cell (50k limit).",
            },
        }

    # Try Sheets
    try:
        sheet = get_google_sheet()
    except Exception:
        sheet = None

    if not sheet:
        if "analysis_history" not in st.session_state:
            st.session_state.analysis_history = []
        st.session_state.analysis_history.append(analysis)
        try:
            st.session_state["last_analysis"] = analysis
        except Exception:
            pass
        return False

    try:
        analysis_id = generate_analysis_id()


        # =====================================================================
        # PATCH ES1F (ADDITIVE): persist full snapshots + pointer for Sheets rows
        # - If full baseline_sources_cache exists (list-shaped), store it outside
        #   Sheets keyed by source_snapshot_hash, and attach pointer fields into
        #   analysis/results for deterministic evolution rehydration.
        # - Pure enrichment only (no refetch, no heuristics).
        # =====================================================================
        try:
            _bsc = None
            if isinstance(analysis, dict):
                _bsc = analysis.get("results", {}).get("baseline_sources_cache") or analysis.get("baseline_sources_cache")


            # =================================================================
            # PATCH SS6B (ADDITIVE): if snapshots were already summarized away,
            # rebuild minimal snapshot shape from evidence_records (deterministic).
            # This enables snapshot persistence even when baseline_sources_cache
            # is a summary dict in the main analysis object.
            # =================================================================
            try:
                if (not isinstance(_bsc, list)) and isinstance(analysis, dict):
                    _er = None
                    # prefer nested results evidence_records first
                    if isinstance(analysis.get("results"), dict):
                        _er = analysis["results"].get("evidence_records")
                    if _er is None:
                        _er = analysis.get("evidence_records")
                    _rebuilt = build_baseline_sources_cache_from_evidence_records(_er)
                    if isinstance(_rebuilt, list) and _rebuilt:
                        _bsc = _rebuilt
            except Exception:
                pass
            # =================================================================

            if isinstance(_bsc, list) and _bsc:
                _ssh = compute_source_snapshot_hash(_bsc)

                # =========================
                # PATCH A2 (ADD): also compute snapshot hash v2 for stronger identity
                # =========================
                _ssh_v2 = None
                try:
                    _ssh_v2 = compute_source_snapshot_hash_v2(_bsc)
                except Exception:
                    _ssh_v2 = None
                if _ssh:
                    # =============================================================
                    # PATCH SS4 (ADDITIVE): store snapshots to Snapshots worksheet when possible
                    # - Persists full baseline_sources_cache in a dedicated worksheet tab.
                    # - Falls back to local snapshot_store file if Sheets snapshot store unavailable.
                    # - Pointer ref stored as 'gsheet:Snapshots:<hash>' when successful.
                    # =============================================================
                    _gs_ref = ""
                    try:
                        _gs_ref = store_full_snapshots_to_sheet(_bsc, _ssh, worksheet_title="Snapshots")
                        # =========================
                        # PATCH A3 (ADD): mirror-write snapshots under v2 hash as well
                        # =========================
                        if _ssh_v2 and isinstance(_ssh_v2, str) and _ssh_v2 != _ssh:
                            try:
                                store_full_snapshots_to_sheet(_bsc, _ssh_v2, worksheet_title="Snapshots")
                            except Exception:
                                pass
                    except Exception:
                        _gs_ref = ""

                    _ref = store_full_snapshots_local(_bsc, _ssh)

                    analysis["source_snapshot_hash"] = analysis.get("source_snapshot_hash") or _ssh
                    analysis.setdefault("results", {})
                    if isinstance(analysis["results"], dict):
                        analysis["results"]["source_snapshot_hash"] = analysis["results"].get("source_snapshot_hash") or _ssh
                        # PATCH A4 (ADD): store v2 hash in results for downstream consumers
                        try:
                            if _ssh_v2:
                                analysis["results"]["source_snapshot_hash_v2"] = analysis["results"].get("source_snapshot_hash_v2") or _ssh_v2
                                # =========================
                                # PATCH FIX37 (ADD): stable snapshot hash alias for fastpath alignment
                                # - Prefer v2 (stable) when present; fall back to legacy v1.
                                # =========================
                                try:
                                    _ssh_stable = _ssh_v2 or _ssh
                                    if _ssh_stable:
                                        analysis["source_snapshot_hash_stable"] = analysis.get("source_snapshot_hash_stable") or _ssh_stable
                                        analysis["results"]["source_snapshot_hash_stable"] = analysis["results"].get("source_snapshot_hash_stable") or _ssh_stable
                                except Exception:
                                    pass
                        except Exception:
                            pass

                    if _ref:
                        analysis["snapshot_store_ref"] = analysis.get("snapshot_store_ref") or _ref
                        if isinstance(analysis["results"], dict):
                            analysis["results"]["snapshot_store_ref"] = analysis["results"].get("snapshot_store_ref") or _ref
                            # PATCH A5 (ADD): v2 snapshot ref for convenience
                            try:
                                if _ssh_v2:
                                    analysis["results"]["snapshot_store_ref_v2"] = analysis["results"].get("snapshot_store_ref_v2") or f"gsheet:Snapshots:{_ssh_v2}"
                            except Exception:
                                pass
                    # =============================================================
                    # PATCH SS4B (ADDITIVE): prefer Sheets snapshot ref when available
                    # =============================================================
                    try:
                        if _gs_ref:
                            analysis["snapshot_store_ref"] = _gs_ref
                            if isinstance(analysis.get("results"), dict):
                                analysis["results"]["snapshot_store_ref"] = _gs_ref
                    except Exception:
                        pass

        except Exception:
            pass
        # =====================================================================

        payload_for_sheets = _shrink_for_sheets(analysis)
        payload_json = _try_make_sheet_json(payload_for_sheets)

        # =====================================================================
        # PATCH A5 (BUGFIX, REQUIRED): never write invalid JSON to Sheets
        # - Previous hard truncation produced non-JSON (prefix + random suffix),
        #   causing history loaders (json.loads) to skip the row entirely.
        # - This wrapper guarantees valid JSON even when we must truncate.
        # =====================================================================
        if isinstance(payload_json, str) and len(payload_json) > SHEETS_CELL_LIMIT:
            try:
                payload_json = json.dumps(
                    {
                        "_sheet_write": {
                            "truncated": True,
                            "mode": "hard_truncation_wrapper",
                            "note": "Payload exceeded Google Sheets single-cell limit; stored preview only.",
                        },
                        # keep a preview for debugging/UI; still parseable JSON
                        "preview": payload_json[: max(0, SHEETS_CELL_LIMIT - 600)],
                        "analysis_id": analysis_id,
                        "timestamp": analysis.get("timestamp", datetime.now().isoformat()),
                        "question": (analysis.get("question", "") or "")[:200],
                    },
                    ensure_ascii=False,
                    default=str,
                )
            except Exception:
                # ultra-safe fallback: still valid JSON
                payload_json = '{"_sheet_write":{"truncated":true,"mode":"hard_truncation_wrapper","note":"json.dumps failed"}}'
        # =====================================================================
        # =====================================================================
        # PATCH HF_PERSIST1 (ADDITIVE): Persist full payload to HistoryFull when History cell is wrapped/truncated
        # Why:
        # - Evolution rebuild requires schema/anchors which may be lost in a sheets-safe wrapper
        # - HistoryFull stores the full JSON keyed by analysis_id for later rehydration
        # Behavior:
        # - If payload_json indicates truncation/wrapper OR is very large, write full payload to HistoryFull
        # - Attach a pointer full_store_ref to both analysis and the wrapper object (when possible)
        # =====================================================================
        try:
            is_truncated = False
            try:
                if isinstance(payload_json, str) and ('"_sheet_write"' in payload_json or '"_sheets_safe"' in payload_json):
                    # quick signal; parse if possible
                    try:
                        _pj = json.loads(payload_json)
                        sw = _pj.get("_sheet_write") if isinstance(_pj, dict) else None
                        if isinstance(sw, dict) and sw.get("truncated") is True:
                            is_truncated = True
                        if _pj.get("_sheets_safe") is True:
                            is_truncated = True
                    except Exception:
                        # if we can't parse and it's huge, treat as truncated risk
                        if len(payload_json) > 45000:
                            is_truncated = True
                elif isinstance(payload_json, str) and len(payload_json) > 45000:
                    is_truncated = True
            except Exception:
                pass

            if is_truncated:
                full_payload_json = ""
                try:
                    full_payload_json = json.dumps(analysis, ensure_ascii=False, default=str)
                except Exception:
                    full_payload_json = ""

                if full_payload_json:
                    ok_full = write_full_history_payload_to_sheet(analysis_id, full_payload_json, worksheet_title="HistoryFull")
                    if ok_full:
                        ref = f"gsheet:HistoryFull:{analysis_id}"
                        try:
                            analysis["full_store_ref"] = ref
                        except Exception:
                            pass
                        # If payload_json is a wrapper dict, embed ref too
                        try:
                            _pj2 = json.loads(payload_json)
                            if isinstance(_pj2, dict):
                                _pj2["full_store_ref"] = ref
                                sw2 = _pj2.get("_sheet_write")
                                if isinstance(sw2, dict):
                                    sw2["full_store_ref"] = ref
                                    _pj2["_sheet_write"] = sw2
                                payload_json = json.dumps(_pj2, ensure_ascii=False, default=str)
                        except Exception:
                            pass
        except Exception:
            pass
        # =====================================================================


        row = [
            analysis_id,
            analysis.get("timestamp", datetime.now().isoformat()),
            (analysis.get("question", "") or "")[:100],
            str(analysis.get("final_confidence", "")),
            payload_json,
        ]
        sheet.append_row(row, value_input_option="RAW")

        try:
            st.session_state["last_analysis"] = analysis
        except Exception:
            pass

        return True

    except Exception as e:
        st.warning(f"⚠️ Failed to save to Google Sheets: {e}")
        if "analysis_history" not in st.session_state:
            st.session_state.analysis_history = []
        st.session_state.analysis_history.append(analysis)
        try:
            st.session_state["last_analysis"] = analysis
        except Exception:
            pass
        return False


def normalize_unit_tag(unit_str: str) -> str:
    """
    Canonical unit tags used for drift=0 comparisons.
    """
    u = (unit_str or "").strip()
    if not u:
        return ""
    ul = u.lower().replace(" ", "")

    # energy units
    if ul == "twh":
        return "TWh"
    if ul == "gwh":
        return "GWh"
    if ul == "mwh":
        return "MWh"
    if ul == "kwh":
        return "kWh"
    if ul == "wh":
        return "Wh"

    # magnitudes
    if ul in ("t", "trillion", "tn"):
        return "T"
    if ul in ("b", "bn", "billion"):
        return "B"
    if ul in ("m", "mn", "mio", "million"):
        return "M"
    if ul in ("k", "thousand", "000"):
        return "K"

    # percent
    if ul in ("%", "pct", "percent"):
        return "%"

    return u


def unit_family(unit_tag: str) -> str:
    """
    Unit family classifier for gating.
    """
    ut = (unit_tag or "").strip()

    if ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
        return "energy"
    if ut == "%":
        return "percent"
    if ut in ("T", "B", "M", "K"):
        return "magnitude"

    return ""


def canonicalize_numeric_candidate(candidate: dict) -> dict:


    """


    Additive: attach canonical numeric fields to a candidate dict.


    Safe to call multiple times.



    PATCH AI4 (ADDITIVE): anchor integrity


    - Ensures anchor_hash + candidate_id are present when possible (derived if missing).


    - Does not change extraction behavior; only enriches fields.


    """


    import hashlib



    if not isinstance(candidate, dict):


        return {}



    # ---------- numeric value ----------


    v_raw = candidate.get("value_norm")


    v = None


    if v_raw is not None:


        try:


            v = float(v_raw)


        except Exception:


            v = None


    if v is None:


        try:


            v0 = candidate.get("value")


            if v0 is None:


                return candidate


            v = float(v0)


        except Exception:


            return candidate



    # ---------- unit normalization ----------


    try:


        ut = normalize_unit_tag(candidate.get("unit_tag") or candidate.get("unit") or "")


    except Exception:


        ut = str(candidate.get("unit_tag") or candidate.get("unit") or "").strip()



    try:


        fam = normalize_unit_family(ut)


    except Exception:


        fam = ""



    # If candidate already has base_unit/multiplier_to_base, respect them


    base_unit = candidate.get("base_unit")


    mult = candidate.get("multiplier_to_base")



    try:


        mult = float(mult) if mult is not None else None


    except Exception:


        mult = None



    # Minimal deterministic mapping (extend as needed)


    if (not base_unit) or (mult is None):


        base_unit = ""


        mult = 1.0



        # percents


        if ut in ("%", "pct"):


            base_unit, mult = "%", 1.0



        # energy


        elif ut == "MWh":


            base_unit, mult = "Wh", 1e6


        elif ut == "kWh":


            base_unit, mult = "Wh", 1e3


        elif ut == "Wh":


            base_unit, mult = "Wh", 1.0



        # power


        elif ut == "GW":


            base_unit, mult = "W", 1e9


        elif ut == "MW":


            base_unit, mult = "W", 1e6


        elif ut == "kW":


            base_unit, mult = "W", 1e3


        elif ut == "W":


            base_unit, mult = "W", 1.0



        # mass


        elif ut in ("Mt", "million_tonnes", "million_tons"):


            base_unit, mult = "t", 1e6


        elif ut in ("kt", "kilo_tonnes", "kilo_tons"):


            base_unit, mult = "t", 1e3


        elif ut in ("t", "tonne", "tonnes", "ton", "tons"):


            base_unit, mult = "t", 1.0



        # count-ish


        elif ut in ("vehicles", "units", "count"):


            base_unit, mult = ut, 1.0



        else:


            # unknown unit: treat as-is


            base_unit, mult = (ut or str(candidate.get("unit") or "").strip()), 1.0



    # Only set defaults to avoid overriding existing enriched fields


    candidate.setdefault("unit_tag", ut)


    candidate.setdefault("unit_family", fam)


    candidate.setdefault("base_unit", base_unit)


    candidate.setdefault("multiplier_to_base", mult)



    # value_norm: if already present, do not overwrite


    if candidate.get("value_norm") is None:


        try:


            candidate["value_norm"] = float(v) * float(mult)


        except Exception:


            pass



    # ---------- anchor integrity ----------


    def _sha1(s: str) -> str:


        try:


            return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()


        except Exception:


            return ""



    ah = candidate.get("anchor_hash") or candidate.get("anchor")


    if not ah:


        # attempt deterministic derive if fields exist


        src = candidate.get("source_url") or candidate.get("url") or ""


        ctx = candidate.get("context_snippet") or candidate.get("context") or ""


        if isinstance(ctx, str):


            ctx = ctx.strip()[:240]


        else:


            ctx = ""


        raw = candidate.get("raw")


        if raw is None:


            raw = f"{candidate.get('value')}{candidate.get('unit') or ''}"


        ah = _sha1(f"{src}|{str(raw)[:120]}|{ctx}") if (src or ctx) else ""


        if ah:


            candidate["anchor_hash"] = ah



    if not candidate.get("candidate_id") and ah:


        candidate["candidate_id"] = str(ah)[:16]



    return candidate

def rebuild_metrics_from_snapshots(
    prev_response: dict,
    baseline_sources_cache: list,
    web_context: dict = None
) -> dict:
    """
    Deterministic rebuild using cached snapshots only.
    If sources unchanged, rebuilt metrics converge with analysis.

    Behavior:
      1) Primary: anchor_hash match via prev_response.metric_anchors
      2) Fallback: schema-first deterministic selection when anchor missing
         using metric_schema_frozen + context match + deterministic tie-break.

    NOTE: Dead/unreachable legacy code previously below an early return has been removed
    (explicitly approved).
    """
    import re
    import hashlib

    # =========================
    # PATCH RMS0 (ADDITIVE): typing imports for Dict/Any/List used below
    # - Prevents NameError if typing symbols are not imported globally.
    # =========================
    from typing import Dict, Any, List
    # =========================

    prev_response = prev_response if isinstance(prev_response, dict) else {}

    # =========================
    # PATCH RMS0.1 (ADDITIVE): accept anchors stored under alternate keys
    # - Backward compatible: does not change existing behavior if metric_anchors exists.
    # =========================
    prev_anchors = (
        prev_response.get("metric_anchors")
        or prev_response.get("anchors")
        or {}
    )
    # =========================

    if not isinstance(prev_anchors, dict):
        prev_anchors = {}

    rebuilt: Dict[str, Any] = {}

    # ---------- schema + canonical lookup ----------
    metric_schema = prev_response.get("metric_schema_frozen") or {}
    if not isinstance(metric_schema, dict):
        metric_schema = {}

    # =========================
    # PATCH RB2 (ADDITIVE): ensure baseline_sources_cache is a full list (rehydrate from snapshot store if needed)
    # - Handles cases where history rows store only a summarized baseline_sources_cache, but full snapshots exist
    #   in the Snapshots sheet (referenced by snapshot_store_ref / source_snapshot_hash).
    # =========================
    try:
        if (not isinstance(baseline_sources_cache, list)) or (isinstance(baseline_sources_cache, dict) and baseline_sources_cache.get("_summary") is True):
            # Prefer already-rehydrated cache on prev_response["results"]["baseline_sources_cache"]
            _maybe = (prev_response.get("results", {}) or {}).get("baseline_sources_cache")
            if isinstance(_maybe, list) and _maybe:
                baseline_sources_cache = _maybe
            else:
                store_ref = prev_response.get("snapshot_store_ref") or (prev_response.get("results", {}) or {}).get("snapshot_store_ref")
                source_hash = prev_response.get("source_snapshot_hash") or (prev_response.get("results", {}) or {}).get("source_snapshot_hash")
                if (not store_ref) and source_hash:
                    store_ref = f"gsheet:Snapshots:{source_hash}"
                if isinstance(store_ref, str) and store_ref.startswith("gsheet:Snapshots:"):
                    _hash = store_ref.split(":")[-1]
                    _full = load_full_snapshots_from_sheet(_hash)
                    if isinstance(_full, list) and _full:
                        baseline_sources_cache = _full
    except Exception:
        pass

    prev_can = prev_response.get("primary_metrics_canonical") or {}
    if not isinstance(prev_can, dict):
        prev_can = {}

    # =========================
    # PATCH RMS0.2 (ADDITIVE): compute full metric key universe
    # - Important: some metrics may not have anchors yet; we still must rebuild them
    #   (otherwise evolution "misses" metrics and diffs become unstable).
    # =========================
    metric_key_universe = set()
    try:
        metric_key_universe.update(list(prev_can.keys()))
        metric_key_universe.update(list(prev_anchors.keys()))
    except Exception:
        metric_key_universe = set(prev_can.keys()) if isinstance(prev_can, dict) else set()
    # =========================

    # ---------- deterministic candidate id (tie-breaker) ----------
    def _candidate_id(c: dict) -> str:
        try:
            url = str(c.get("source_url") or c.get("url") or "")
            ah = str(c.get("anchor_hash") or "")
            vn = c.get("value_norm")
            bu = str(c.get("base_unit") or c.get("unit") or c.get("unit_tag") or "")
            mk = str(c.get("measure_kind") or "")
            vn_s = ""
            if vn is not None:
                try:
                    vn_s = f"{float(vn):.12g}"
                except Exception:
                    vn_s = str(vn)
            s = f"{url}|{ah}|{vn_s}|{bu}|{mk}"
            return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()
        except Exception:
            return ""

    # =====================================================================
    # PATCH RMS_E0 (ADDITIVE): small evidence extraction helper
    # - Ensures we consistently carry anchor/evidence fields onto rebuilt metrics.
    # - Purely additive; never affects selection logic.
    # =====================================================================
    def _extract_evidence_fields(c: dict) -> dict:
        if not isinstance(c, dict):
            return {}
        ctx = (c.get("context_snippet") or c.get("context") or "").strip()
        return {
            "raw": c.get("raw"),
            "candidate_id": c.get("candidate_id") or _candidate_id(c),
            "context_snippet": ctx[:240] if isinstance(ctx, str) else None,
            "measure_kind": c.get("measure_kind"),
            "measure_assoc": c.get("measure_assoc"),
            "start_idx": c.get("start_idx"),
            "end_idx": c.get("end_idx"),
            # optional passthroughs if upstream provides them
            "fingerprint": c.get("fingerprint"),
        }
    # =====================================================================

    # =====================================================================
    # PATCH RMS_E1 (ADDITIVE): anchor metadata getter
    # - Pull anchor_confidence (and any other safe fields) from prev_anchors entry.
    # - Helps diff/UI show confidence without recomputing.
    # =====================================================================
    def _anchor_meta(anchor_obj) -> dict:
        if isinstance(anchor_obj, dict):
            out = {}
            if anchor_obj.get("anchor_confidence") is not None:
                try:
                    out["anchor_confidence"] = float(anchor_obj.get("anchor_confidence"))
                except Exception:
                    pass
            # optional passthroughs if present
            if anchor_obj.get("source_url"):
                out["anchor_source_url"] = anchor_obj.get("source_url")
            if anchor_obj.get("raw"):
                out["anchor_raw"] = anchor_obj.get("raw")
            if anchor_obj.get("candidate_id"):
                out["anchor_candidate_id"] = anchor_obj.get("candidate_id")
            return out
        return {}
    # =====================================================================

    # ---------- collect candidates + anchor map ----------
    anchor_to_candidate: Dict[str, Dict[str, Any]] = {}
    all_candidates: List[Dict[str, Any]] = []

    for src in baseline_sources_cache or []:
        if not isinstance(src, dict):
            continue
        src_url = src.get("url") or src.get("source_url") or ""

        # =================================================================
        # PATCH RMS_E2 (ADDITIVE): capture source fingerprint on candidates
        # - Helps later debugging and “same source” proofs.
        # =================================================================
        src_fp = src.get("fingerprint")
        # =================================================================

        for c in (src.get("extracted_numbers") or []):
            if not isinstance(c, dict):
                continue

            # canonicalize if available (safe if repeated)
            try:
                c = canonicalize_numeric_candidate(dict(c))
            except Exception:
                c = dict(c)

            # ensure stable url carried through
            if not c.get("source_url"):
                c["source_url"] = src_url

            # =============================================================
            # PATCH RMS_E2 (ADDITIVE): attach fingerprint if missing
            # =============================================================
            if src_fp and not c.get("fingerprint"):
                c["fingerprint"] = src_fp
            # =============================================================

            ah = c.get("anchor_hash")
            if ah:
                if ah not in anchor_to_candidate:
                    anchor_to_candidate[ah] = c
                else:
                    old = anchor_to_candidate[ah]
                    if old.get("is_junk") and not c.get("is_junk"):
                        anchor_to_candidate[ah] = c

            all_candidates.append(c)

    # ---------- schema-first helpers ----------
    def _schema_for_key(metric_key: str) -> dict:
        d = metric_schema.get(metric_key)
        return d if isinstance(d, dict) else {}

    def _expected_from_schema(metric_key: str):
        d = _schema_for_key(metric_key)

        unit_family_s = str(d.get("unit_family") or "").strip().lower()
        dim_s = str(d.get("dimension") or "").strip().lower()
        unit_s = str(d.get("unit") or "").strip()
        name_l = str(d.get("name") or "").lower()

        expected_family = ""
        if unit_family_s in ("percent", "currency", "energy"):
            expected_family = unit_family_s
        else:
            ut = normalize_unit_tag(unit_s)
            if ut == "%":
                expected_family = "percent"
            elif ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
                expected_family = "energy"
            elif dim_s == "currency":
                expected_family = "currency"

        currencyish = (unit_family_s == "currency" or dim_s == "currency")

        expected_kind = None
        if expected_family == "percent":
            if any(k in name_l for k in ["growth", "cagr", "increase", "decrease", "yoy", "qoq", "mom", "rate"]):
                expected_kind = "growth_pct"
            else:
                expected_kind = "share_pct"
        if currencyish or expected_family == "currency":
            expected_kind = "money"
        if expected_kind is None and any(k in name_l for k in [
            "units", "unit sales", "vehicle sales", "vehicles sold", "sold",
            "deliveries", "shipments", "registrations", "volume"
        ]):
            expected_kind = "count_units"

        kw = d.get("keywords")
        schema_keywords = [str(x).strip() for x in kw] if isinstance(kw, list) else []
        schema_keywords = [x for x in schema_keywords if x]

        return expected_family, currencyish, expected_kind, schema_keywords, unit_s

    def _ctx_match_score(tokens: List[str], ctx: str) -> float:
        fn = globals().get("calculate_context_match")
        if callable(fn):
            try:
                return float(fn(tokens, ctx))
            except Exception:
                pass

        c = (ctx or "").lower()
        toks = [t.lower() for t in (tokens or []) if t and len(t) >= 2]
        if not toks:
            return 0.0
        hit = sum(1 for t in toks if t in c)
        return hit / max(1, len(toks))

    def _currency_evidence(raw: str, ctx: str) -> bool:
        r = (raw or "")
        c = (ctx or "").lower()
        if any(s in r for s in ["$", "S$", "€", "£"]):
            return True
        if any(code in c for code in [" usd", "sgd", " eur", " gbp", " aud", " cad", " jpy", " cny", " rmb"]):
            return True
        if any(k in c for k in ["revenue", "turnover", "valuation", "market size", "market value", "profit", "earnings", "ebitda"]):
            return True
        return False

    def _is_yearish_value(v) -> bool:
        try:
            iv = int(float(v))
            return 1900 <= iv <= 2099
        except Exception:
            return False

    # =========================
    # PATCH RMS_BASE (ADDITIVE): helper to overlay rebuilt fields onto prior canonical metric
    # - Keeps metric identity fields (name/canonical_key/dimension/etc.) stable for diffing.
    # - Only overwrites value-ish/source-ish fields with rebuilt candidate data.
    # =========================
    def _overlay_base(metric_key: str, patch: dict) -> dict:
        base = {}
        try:
            if isinstance(prev_can.get(metric_key), dict):
                base = dict(prev_can.get(metric_key) or {})
        except Exception:
            base = {}
        out = dict(base)
        try:
            if isinstance(patch, dict):
                out.update(patch)
        except Exception:
            pass
        return out
    # =========================


    # =====================================================================
    # PATCH FIX41AFC53 (ADDITIVE): anchor candidate resolution parity + anchor-source lock
    #
    # Problem addressed:
    # - Evolution snapshots may contain candidates whose `anchor_hash` differs from Analysis anchors
    #   (e.g., due to extractor hash drift or context normalization differences).
    # - When direct `anchor_hash` lookup fails, Evolution falls back to global scoring and can pick
    #   unrelated values from injected sources, causing blank/incorrect "current" values after
    #   unit gates.
    #
    # Solution (additive, non-invasive):
    # - Build a secondary index by candidate_id (derived from anchor_hash prefix when missing).
    # - Attempt to resolve each prev anchor using:
    #     (1) candidate_id match
    #     (2) same-source fuzzy match using (raw/value_norm/unit_family + token overlap with anchor context)
    # - Enforce a soft anchor-source lock: if an anchor has a source_url, prefer candidates from that
    #   exact source_url; do not allow cross-source replacement unless no same-source candidates exist.
    #
    # Notes:
    # - Fastpath/hashing remain untouched.
    # - Existing anchor_hash path remains authoritative when it hits.
    # =====================================================================

    def _cand_candidate_id(cand: dict) -> str:
        try:
            if isinstance(cand, dict):
                cid = str(cand.get("candidate_id") or "").strip()
                if cid:
                    return cid
                ah2 = str(cand.get("anchor_hash") or "").strip()
                if ah2 and len(ah2) >= 16:
                    return ah2[:16]
        except Exception:
            pass
        return ""

    def _tokset(s: str) -> set:
        import re
        s = (s or "").lower()
        s = re.sub(r"[^a-z0-9%$]+", " ", s)
        return set([t for t in s.split() if len(t) >= 3])

    def _floatish(v):
        try:
            if v is None:
                return None
            if isinstance(v, (int, float)):
                return float(v)
            s = str(v).strip()
            if not s:
                return None
            return float(s)
        except Exception:
            return None

    # secondary indexes
    _cand_by_candidate_id = {}
    _cands_by_url = {}
    for _c in (all_candidates or []):
        if not isinstance(_c, dict):
            continue
        _cid = _cand_candidate_id(_c)
        if _cid and _cid not in _cand_by_candidate_id:
            _cand_by_candidate_id[_cid] = _c
        _u = str(_c.get("source_url") or "").strip()
        if _u:
            _cands_by_url.setdefault(_u, []).append(_c)

    def _resolve_anchor_candidate(metric_key: str, anchor_obj) -> dict:
        """Best-effort anchor candidate resolve when direct anchor_hash lookup misses."""
        try:
            a = _safe_anchor_obj(anchor_obj) if ' _safe_anchor_obj' else (anchor_obj if isinstance(anchor_obj, dict) else {})
        except Exception:
            a = anchor_obj if isinstance(anchor_obj, dict) else {}
        if not isinstance(a, dict):
            a = {}

        a_url = str(a.get("source_url") or "").strip()
        a_cid = str(a.get("candidate_id") or "").strip()
        a_ah = str(a.get("anchor_hash") or a.get("anchor") or "").strip()
        a_ctx = str(a.get("context_snippet") or a.get("context") or "").strip()
        a_raw = str(a.get("raw") or a.get("anchor_raw") or "").strip()

        # (1) candidate_id direct
        if a_cid and a_cid in _cand_by_candidate_id:
            c0 = _cand_by_candidate_id.get(a_cid) or {}
            if isinstance(c0, dict):
                if (not a_url) or (str(c0.get("source_url") or "").strip() == a_url):
                    return c0

        # (2) anchor_hash prefix as candidate_id
        if (not a_cid) and a_ah and len(a_ah) >= 16:
            cid2 = a_ah[:16]
            if cid2 and cid2 in _cand_by_candidate_id:
                c1 = _cand_by_candidate_id.get(cid2) or {}
                if isinstance(c1, dict):
                    if (not a_url) or (str(c1.get("source_url") or "").strip() == a_url):
                        return c1

        # (3) same-source fuzzy
        pool = []
        if a_url and a_url in _cands_by_url:
            pool = _cands_by_url.get(a_url) or []
        else:
            # if anchor source missing, allow global (but very conservative)
            pool = list(all_candidates or [])

        expected_family, expected_unit_tag, expected_dim = _expected_from_schema(metric_key)

        # Compute anchor numeric target if present
        a_val = _floatish(a.get("value_norm"))
        if a_val is None:
            a_val = _floatish(a.get("value"))

        a_toks = _tokset(a_ctx)
        if a_raw:
            a_toks |= _tokset(a_raw)

        best = None
        best_score = -1e9
        for c in (pool or []):
            if not isinstance(c, dict):
                continue
            if c.get("is_junk") is True:
                continue

            # enforce source lock if we have anchor url
            if a_url and str(c.get("source_url") or "").strip() != a_url:
                continue

            # family/eligibility: prefer expected family if known
            fam = str(c.get("unit_family") or "").strip().lower()
            fam_score = 0.0
            if expected_family and fam == expected_family:
                fam_score = 5.0
            elif expected_family and fam and fam != expected_family:
                fam_score = -2.0

            # numeric closeness (if we have an anchor numeric target)
            cv = _floatish(c.get("value_norm"))
            if cv is None:
                cv = _floatish(c.get("value"))
            num_score = 0.0
            if a_val is not None and cv is not None:
                # relative closeness
                denom = max(1e-9, abs(a_val))
                rel = abs(cv - a_val) / denom
                num_score = 3.0 if rel <= 0.05 else (1.5 if rel <= 0.2 else (-1.0 if rel >= 2.0 else 0.0))

            # token overlap with anchor context
            c_ctx = str(c.get("context_snippet") or c.get("context") or "").strip()
            ctoks = _tokset(c_ctx)
            inter = len(a_toks & ctoks) if a_toks and ctoks else 0
            tok_score = min(6.0, inter * 0.75)

            # raw match bonus
            raw_bonus = 0.0
            if a_raw:
                try:
                    if a_raw in str(c.get("raw") or ""):
                        raw_bonus = 2.0
                except Exception:
                    pass

            score = fam_score + num_score + tok_score + raw_bonus
            if score > best_score:
                best_score = score
                best = c

        if isinstance(best, dict) and best_score >= 3.0:
            return best or {}

        return {}
    # =====================================================================

# ---------- 1) primary rebuild by anchor ----------
    rebuilt_by_anchor = set()

    for metric_key, anchor in prev_anchors.items():
        ah = None
        if isinstance(anchor, dict):
            ah = anchor.get("anchor_hash") or anchor.get("anchor")
        elif isinstance(anchor, str):
            ah = anchor

        # ================================================================
        # PATCH FIX41AFC53 (ADDITIVE): attempt anchor resolve when direct anchor_hash lookup misses
        # ================================================================
        c = None
        if ah and ah in anchor_to_candidate:
            c = anchor_to_candidate[ah]
        else:
            try:
                c = _resolve_anchor_candidate(metric_key, anchor)
            except Exception:
                c = None

        if isinstance(c, dict) and c:

            # =========================
            # PATCH RMS1 (ADDITIVE): overlay rebuilt candidate onto base canonical metric
            # - Keeps canonical identity fields intact for downstream diffs/UI.
            # =========================
            rebuilt[metric_key] = _overlay_base(metric_key, {
                "value": c.get("value"),
                "unit": c.get("unit"),
                "value_norm": c.get("value_norm"),
                "base_unit": c.get("base_unit"),
                "unit_tag": c.get("unit_tag"),
                "unit_family": c.get("unit_family"),
                "anchor_hash": ah,
                "source_url": c.get("source_url"),
                "context_snippet": (c.get("context_snippet") or c.get("context") or "")[:240],
                "measure_kind": c.get("measure_kind"),
                "measure_assoc": c.get("measure_assoc"),
                "rebuild_method": "anchor",

                # =============================================================
                # PATCH RMS_E3 (ADDITIVE): attach evidence + anchor metadata
                # - candidate_id used as stable ID for UI/debugging
                # - anchor_confidence helps diff/UI set match_confidence
                # =============================================================
                **_extract_evidence_fields(c),
                **_anchor_meta(anchor),
                # =============================================================
            })
            # =========================

            rebuilt_by_anchor.add(metric_key)

    # ---------- 2) fallback rebuild when anchor missing ----------
    # NOTE: existing loop only iterated prev_anchors.keys(); we keep it as-is,
    # and then add an extra additive loop to cover metrics without anchors. (PATCH RMS2)
    for metric_key in prev_anchors.keys():
        if metric_key in rebuilt_by_anchor:
            continue

        expected_family, currencyish, expected_kind, schema_keywords, schema_unit = _expected_from_schema(metric_key)

        # conservative fallback if schema is thin
        if not expected_family and metric_key in prev_can and isinstance(prev_can.get(metric_key), dict):
            pm = prev_can.get(metric_key) or {}
            ut = normalize_unit_tag(pm.get("unit") or schema_unit or "")
            if ut == "%":
                expected_family = "percent"
            elif ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
                expected_family = "energy"

        # tokens for context scoring
        tokens = []
        if schema_keywords:
            tokens = schema_keywords
        else:
            # fallback to build_metric_keywords(schema_name)
            schema_name = ""
            try:
                schema_name = str(_schema_for_key(metric_key).get("name") or "")
            except Exception:
                schema_name = ""
            fn_bmk = globals().get("build_metric_keywords")
            if callable(fn_bmk):
                try:
                    tokens = fn_bmk(schema_name or metric_key) or []
                except Exception:
                    tokens = []
            else:
                tokens = []

        best = None
        best_key = None
        best_score = -1.0

        for c in all_candidates:
            if not isinstance(c, dict):
                continue

            # fallback skips junk (anchor path already handled above)
            if c.get("is_junk") is True:
                continue

            ctx = (c.get("context") or c.get("context_snippet") or "").strip()
            if not ctx:
                continue

            # stop timeline years contaminating non-year metrics
            if expected_family not in ("percent", "energy") and not (currencyish or expected_family == "currency"):
                if (c.get("unit_tag") in ("", None)) and _is_yearish_value(c.get("value")):
                    continue
            # =====================================================================
            # PATCH FIX41AFC5 (ADDITIVE): hard-reject year-only + unitless candidates (evolution rebuild parity)
            # Why:
            #   - Prevent "2024"/"2025" from being selected as metric values (especially count/magnitude_other)
            #   - Applies regardless of expected_family, but only when the candidate is unitless/non-percent.
            # Determinism:
            #   - Pure filtering; stable ordering; no refetch.
            # =====================================================================
            try:
                _vnorm = c.get("value_norm", None)
                if _vnorm is None:
                    _vnorm = c.get("value", None)
                _is_year = _is_yearish_value(_vnorm)
                _cand_ut0 = (c.get("unit_tag") or normalize_unit_tag(c.get("unit") or "") or "").strip()
                _cand_fam0 = (c.get("unit_family") or unit_family(_cand_ut0) or "").strip().lower()
                _mk0 = str(c.get("measure_kind") or "").strip().lower()
                _is_pct0 = bool(c.get("is_percent") or c.get("has_percent") or (_cand_ut0 == "%") or (_cand_fam0 == "percent"))
                _has_curr0 = bool(str(c.get("currency_symbol") or c.get("currency") or "").strip())
                _has_unit_ev0 = bool(_cand_ut0 or _cand_fam0 or _is_pct0 or _has_curr0)
                # year-only guard (unitless, non-percent, non-currency)
                if _is_year and (not _has_unit_ev0) and (not _is_pct0) and (not _has_curr0) and (_mk0 in ("magnitude_other", "count_units", "count", "number", "")):
                    try:
                        _fix41afc5_dbg["rejected_year_only"] = int(_fix41afc5_dbg.get("rejected_year_only", 0) or 0) + 1
                    except Exception:
                        pass
                    continue
                # magnitude_other guard (unitless, non-percent, non-currency)
                if (_mk0 == "magnitude_other") and (not _has_unit_ev0) and (not _is_pct0) and (not _has_curr0):
                    try:
                        _fix41afc5_dbg["rejected_magnitude_other_unitless"] = int(_fix41afc5_dbg.get("rejected_magnitude_other_unitless", 0) or 0) + 1
                    except Exception:
                        pass
                    continue
            except Exception:
                pass

            cand_ut = c.get("unit_tag") or normalize_unit_tag(c.get("unit") or "")
            cand_fam = (c.get("unit_family") or unit_family(cand_ut) or "").strip().lower()
            mk = c.get("measure_kind")

            # unit-family gating
            if expected_family == "percent":
                if cand_fam != "percent" and cand_ut != "%":
                    continue
            elif expected_family == "energy":
                if cand_fam != "energy":
                    continue
            elif currencyish or expected_family == "currency":
                if cand_fam not in ("currency", "magnitude"):
                    continue
                if not _currency_evidence(c.get("raw", ""), ctx):
                    continue
                if mk == "count_units":
                    continue

            # measure-kind gating (only if candidate provides it)
            if expected_kind and mk and mk != expected_kind:
                continue

            # normalize value for ranking
            try:
                c2 = canonicalize_numeric_candidate(dict(c))
            except Exception:
                c2 = c

            val_norm = c2.get("value_norm")
            if val_norm is None:
                try:
                    val_norm = float(c2.get("value"))
                except Exception:
                    continue

            ctx_score = _ctx_match_score(tokens, ctx)
            if ctx_score <= 0.0:
                continue

            url = str(c2.get("source_url") or c2.get("url") or "")
            cid = c2.get("candidate_id") or _candidate_id({**c2, "value_norm": val_norm})

            # deterministic tie-break (max)
            key = (
                float(ctx_score),
                float(val_norm),
                url,
                str(cid),
            )

            if best_key is None or key > best_key:
                best_key = key
                best_score = float(ctx_score)
                best = {**c2, "value_norm": val_norm, "candidate_id": cid}

        if best:
            # =========================
            # PATCH RMS1 (ADDITIVE): overlay onto base canonical metric
            # =========================
            rebuilt[metric_key] = _overlay_base(metric_key, {
                "value": best.get("value"),
                "unit": best.get("unit") or best.get("unit_tag"),
                "value_norm": best.get("value_norm"),
                "base_unit": best.get("base_unit"),
                "unit_tag": best.get("unit_tag"),
                "unit_family": best.get("unit_family"),
                "anchor_hash": best.get("anchor_hash"),
                "source_url": best.get("source_url") or best.get("url"),
                "context_snippet": (best.get("context_snippet") or best.get("context") or "")[:240],
                "measure_kind": best.get("measure_kind"),
                "measure_assoc": best.get("measure_assoc"),
                "rebuild_method": "schema_fallback",
                "fallback_ctx_score": round(best_score, 6),
                "candidate_id": best.get("candidate_id"),

                # =============================================================
                # PATCH RMS_E4 (ADDITIVE): attach standardized evidence fields
                # - Ensures candidate_id/raw/context are always present when possible.
                # - Adds anchor_confidence derived from fallback_ctx_score.
                # =============================================================
                **_extract_evidence_fields(best),
                "anchor_confidence": float(min(100.0, max(0.0, best_score) * 100.0)) if best_score is not None else 0.0,
                # =============================================================
            })
            # =========================

    # =========================
    # PATCH RMS2 (ADDITIVE): ensure metrics without anchors are also rebuilt
    # - Your existing fallback loop only iterates prev_anchors.keys().
    # - This loop covers the remaining canonical metrics (prev_can keys) that are missing
    #   from prev_anchors, using the SAME schema-first logic (copied, not refactored).
    # - Additive: does not alter prior behavior for anchored metrics.
    # =========================
    for metric_key in (metric_key_universe or set()):
        if metric_key in rebuilt:
            continue

        expected_family, currencyish, expected_kind, schema_keywords, schema_unit = _expected_from_schema(metric_key)

        if not expected_family and metric_key in prev_can and isinstance(prev_can.get(metric_key), dict):
            pm = prev_can.get(metric_key) or {}
            ut = normalize_unit_tag(pm.get("unit") or schema_unit or "")
            if ut == "%":
                expected_family = "percent"
            elif ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
                expected_family = "energy"

        tokens = []
        if schema_keywords:
            tokens = schema_keywords
        else:
            schema_name = ""
            try:
                schema_name = str(_schema_for_key(metric_key).get("name") or "")
            except Exception:
                schema_name = ""
            fn_bmk = globals().get("build_metric_keywords")
            if callable(fn_bmk):
                try:
                    tokens = fn_bmk(schema_name or metric_key) or []
                except Exception:
                    tokens = []
            else:
                tokens = []

        best = None
        best_key = None
        best_score = -1.0

        for c in all_candidates:
            if not isinstance(c, dict):
                continue
            if c.get("is_junk") is True:
                continue

            ctx = (c.get("context") or c.get("context_snippet") or "").strip()
            if not ctx:
                continue

            if expected_family not in ("percent", "energy") and not (currencyish or expected_family == "currency"):
                if (c.get("unit_tag") in ("", None)) and _is_yearish_value(c.get("value")):
                    continue

            cand_ut = c.get("unit_tag") or normalize_unit_tag(c.get("unit") or "")
            cand_fam = (c.get("unit_family") or unit_family(cand_ut) or "").strip().lower()
            mk = c.get("measure_kind")

            if expected_family == "percent":
                if cand_fam != "percent" and cand_ut != "%":
                    continue
            elif expected_family == "energy":
                if cand_fam != "energy":
                    continue
            elif currencyish or expected_family == "currency":
                if cand_fam not in ("currency", "magnitude"):
                    continue
                if not _currency_evidence(c.get("raw", ""), ctx):
                    continue
                if mk == "count_units":
                    continue

            if expected_kind and mk and mk != expected_kind:
                continue

            try:
                c2 = canonicalize_numeric_candidate(dict(c))
            except Exception:
                c2 = c

            val_norm = c2.get("value_norm")
            if val_norm is None:
                try:
                    val_norm = float(c2.get("value"))
                except Exception:
                    continue

            ctx_score = _ctx_match_score(tokens, ctx)
            if ctx_score <= 0.0:
                continue

            url = str(c2.get("source_url") or c2.get("url") or "")
            cid = c2.get("candidate_id") or _candidate_id({**c2, "value_norm": val_norm})

            key = (
                float(ctx_score),
                float(val_norm),
                url,
                str(cid),
            )

            if best_key is None or key > best_key:
                best_key = key
                best_score = float(ctx_score)
                best = {**c2, "value_norm": val_norm, "candidate_id": cid}

        if best:
            rebuilt[metric_key] = _overlay_base(metric_key, {
                "value": best.get("value"),
                "unit": best.get("unit") or best.get("unit_tag"),
                "value_norm": best.get("value_norm"),
                "base_unit": best.get("base_unit"),
                "unit_tag": best.get("unit_tag"),
                "unit_family": best.get("unit_family"),
                "anchor_hash": best.get("anchor_hash"),
                "source_url": best.get("source_url") or best.get("url"),
                "context_snippet": (best.get("context_snippet") or best.get("context") or "")[:240],
                "measure_kind": best.get("measure_kind"),
                "measure_assoc": best.get("measure_assoc"),
                "rebuild_method": "schema_fallback_no_anchor",
                "fallback_ctx_score": round(best_score, 6),
                "candidate_id": best.get("candidate_id"),

                # =============================================================
                # PATCH RMS_E5 (ADDITIVE): attach standardized evidence fields
                # =============================================================
                **_extract_evidence_fields(best),
                "anchor_confidence": float(min(100.0, max(0.0, best_score) * 100.0)) if best_score is not None else 0.0,
                # =============================================================
            })
        else:
            # stable placeholder (do not fabricate)
            if isinstance(prev_can.get(metric_key), dict):
                rebuilt[metric_key] = _overlay_base(metric_key, {
                    "rebuild_method": "not_found_in_snapshots",

                    # =============================================================
                    # PATCH RMS_E6 (ADDITIVE): keep evidence fields present for stable shape
                    # =============================================================
                    "anchor_hash": None,
                    "source_url": None,
                    "context_snippet": None,
                    "raw": None,
                    "candidate_id": None,
                    "anchor_confidence": 0.0,
                    # =============================================================
                })
    # =========================

    # =====================================================================
    # PATCH RMS_FALLBACK1 (ADDITIVE): never return empty rebuild when we have a baseline universe
    # Why:
    #   - Source-anchored evolution is snapshot-gated; if snapshots exist but rebuild fails
    #     (missing anchors/schema mismatch/edge cases), returning {} causes evolution to hard-fail.
    #   - For determinism + drift-0 testing, we prefer a safe fallback that preserves the
    #     canonical metric universe from the previous analysis while emitting an explicit flag.
    #
    # Behavior:
    #   - If 'rebuilt' is empty/non-dict, fall back to prev_response['primary_metrics_canonical'].
    #   - Marks each metric with '_rebuild_fallback_used': True (additive field).
    #   - DOES NOT fabricate new values; it reuses previous canonical values only.
    # =====================================================================
    try:
        if not isinstance(rebuilt, dict) or not rebuilt:
            prev_universe = {}
            if isinstance(prev_response, dict):
                prev_universe = prev_response.get("primary_metrics_canonical") or {}
            if isinstance(prev_universe, dict) and prev_universe:
                rebuilt = {}
                for ck in sorted(prev_universe.keys()):
                    m = prev_universe.get(ck)
                    if isinstance(m, dict):
                        mm = dict(m)
                        mm["_rebuild_fallback_used"] = True
                        # Ensure ES7 fields exist (pure enrichment)
                        mm.setdefault("canonical_key", ck)
                        mm.setdefault("anchor_used", False)
                        mm.setdefault("anchor_confidence", 0.0)
                        rebuilt[ck] = mm
                # Add top-level marker (additive)
                try:
                    rebuilt["_rebuild_status"] = "fallback_prev_primary_metrics_canonical"
                except Exception:
                    pass
    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH FIX41AFC5 (ADDITIVE): attach eligibility-hardening debug counters
    # =====================================================================
    try:
        if isinstance(rebuilt, dict):
            rebuilt.setdefault("_fix41afc5_debug", {})
            if isinstance(rebuilt.get("_fix41afc5_debug"), dict):
                rebuilt["_fix41afc5_debug"].update(dict(_fix41afc5_dbg))
    except Exception:
        pass
    # =====================================================================

    return rebuilt




# =====================================================================
# PATCH RMS_MIN1 (ADDITIVE): Minimal schema-driven rebuild from snapshots
# ---------------------------------------------------------------------
# Goal:
#   - Provide a deterministic, evolution-safe metric rebuild that uses ONLY:
#       (a) baseline_sources_cache snapshots (and their extracted_numbers)
#       (b) frozen metric schema (metric_schema_frozen)
#   - No re-fetch, no LLM inference, no heuristic "best guess" beyond schema fields.
#
# Contract:
#   - Returns a dict shaped like primary_metrics_canonical:
#       { canonical_key: { ...metric fields... } }
#   - Deterministic tie-break ordering.
# =====================================================================


# =====================================================================
# PATCH F (deterministic): Explicit candidate exclusion in rebuild stage
#   - Enforce that ANY candidate flagged as junk is excluded from:
#       * candidate indexing
#       * candidate scoring
#       * final metric assignment
#   - Additionally, suppress "year-like" unitless tokens (e.g., 2024/2025) for
#     non-year metrics (currency/percent/rate/ratio/growth/etc.) to prevent
#     year fixation during evolution.
#   - Purely deterministic: no LLM, no refetch, no heuristics outside schema cues.
# =====================================================================

def _candidate_disallowed_for_metric(_cand: dict, _spec: dict = None) -> bool:
    """Return True if a snapshot candidate must not be used to assign a metric value."""
    if not isinstance(_cand, dict):
        return True

    # 1) Hard exclusion: explicit junk flags / reasons from extraction phase
    if _cand.get("is_junk") is True:
        return True
    jr = str(_cand.get("junk_reason") or "").strip().lower()
    if jr:
        # If a junk_reason exists, treat it as non-selectable deterministically.
        return True

    # 2) Deterministic anti-year-fixation: unitless year-like tokens are disallowed
    #    for most numeric metrics (unless schema clearly indicates a "year" metric).
    try:
        v = _cand.get("value_norm", _cand.get("value"))
        unitish = str(_cand.get("base_unit") or _cand.get("unit_tag") or _cand.get("unit") or "").strip()
        if unitish == "" and isinstance(v, (int, float)):
            if abs(float(v) - round(float(v))) < 1e-9:
                vi = int(round(float(v)))
                if 1900 <= vi <= 2100:
                    if isinstance(_spec, dict):
                        nm = str(_spec.get("name") or "").lower()
                        cid = str(_spec.get("canonical_id") or _spec.get("canonical_key") or "").lower()
                        kws = _spec.get("keywords") or []
                        kws_s = " ".join([str(k).lower() for k in kws]) if isinstance(kws, list) else str(kws).lower()

                        # Allow explicit year metrics
                        if ("year" in nm) or ("year" in cid) or ("founded" in nm) or ("since" in nm) or ("year" in kws_s):
                            return False

                        uf = str(_spec.get("unit_family") or "").lower().strip()
                        ut = str(_spec.get("unit_tag") or _spec.get("unit") or "").lower().strip()

                        # For common non-year metric families, exclude year-like tokens.
                        if uf in ("currency", "percent", "rate", "ratio", "growth", "share"):
                            return True
                        if "%" in ut:
                            return True
                        if any(w in nm for w in ("cagr", "revenue", "growth", "market", "sales", "profit", "margin", "volume")):
                            return True

                    # Default: unitless year-like token is not a valid metric value.
                    return True
    except Exception:
        pass

    return False

def rebuild_metrics_from_snapshots_schema_only(
    prev_response: dict,
    baseline_sources_cache: list,
    web_context: dict = None
) -> dict:
    """Schema-driven deterministic rebuild from cached snapshots only.

    This is intentionally minimal:
      - It does NOT attempt free-form metric discovery.
      - It ONLY populates metrics declared in the frozen schema.
      - Candidate selection is driven by schema fields (keywords + unit family/tag).
      - Deterministic sorting ensures stable output ordering.

    Returns:
      Dict[str, Dict] shaped like primary_metrics_canonical.
    """
    import re

# =====================================================================
    # =====================================================================
    # PATCH FIX33 (ADDITIVE): enforce unit-required eligibility in schema-only rebuild
    # Why:
    #   - When anchors are not used (anchor_used:false), schema-only rebuild can still
    #     select unit-less year tokens (e.g., 2024/2025) for currency/percent metrics.
    #   - This patch hard-rejects candidates with no token-level unit evidence when
    #     the schema (or canonical_key suffix) implies a unit is required.
    #   - Also optionally emits compact debug metadata for top candidates/rejections.
    # Determinism:
    #   - Pure filtering + stable ordering; no refetch; no randomness.
    # =====================================================================

    def _fix33_schema_unit_required(spec_unit_family: str, spec_unit_tag: str, canonical_key: str) -> bool:
        uf = str(spec_unit_family or "").strip().lower()
        ut = str(spec_unit_tag or "").strip().lower()
        ck = str(canonical_key or "").strip().lower()
        if uf in {"currency", "percent", "rate", "ratio"}:
            return True
        if ut in {"%", "percent"}:
            return True
        # Canonical-key suffix conventions (backstop)
        if ck.endswith("__currency") or ck.endswith("__percent") or ck.endswith("__rate") or ck.endswith("__ratio"):
            return True
        return False

    def _fix33_candidate_has_unit_evidence(c: dict) -> bool:
        if not isinstance(c, dict):
            return False
        # Any explicit unit/currency/% evidence is enough to qualify as "has unit".
        if str(c.get("unit_tag") or "").strip():
            return True
        if str(c.get("unit_family") or "").strip():
            return True
        if str(c.get("base_unit") or "").strip():
            return True
        if str(c.get("unit") or "").strip():
            return True
        if str(c.get("currency_symbol") or c.get("currency") or "").strip():
            return True
        if bool(c.get("is_percent") or c.get("has_percent")):
            return True
        mk = str(c.get("measure_kind") or "").strip().lower()
        if mk in {"money", "percent", "percentage", "rate", "ratio"}:
            return True
        toks = c.get("unit_tokens") or c.get("unit_evidence_tokens") or []
        if isinstance(toks, (list, tuple)) and len(toks) > 0:
            return True
        return False

    _fix33_dbg = False
    try:
        _fix33_dbg = bool((web_context or {}).get("debug_evolution") or ((prev_response or {}).get("debug") or {}).get("debug_evolution"))
    except Exception:
        _fix33_dbg = False


    # -------------------------
    # Resolve frozen schema (supports multiple storage locations)
    # -------------------------
    schema = None
    try:
        if isinstance(prev_response, dict):
            if isinstance(prev_response.get("metric_schema_frozen"), dict):
                schema = prev_response.get("metric_schema_frozen")
            elif isinstance(prev_response.get("primary_response"), dict) and isinstance(prev_response["primary_response"].get("metric_schema_frozen"), dict):
                schema = prev_response["primary_response"].get("metric_schema_frozen")
            elif isinstance(prev_response.get("results"), dict) and isinstance(prev_response["results"].get("metric_schema_frozen"), dict):
                schema = prev_response["results"].get("metric_schema_frozen")
    except Exception:
        schema = None

    if not isinstance(schema, dict) or not schema:
        return {}

    # -------------------------
    # Collect candidates from snapshots (no re-fetch)
    # -------------------------
    candidates = []
    if isinstance(baseline_sources_cache, list):
        for src in baseline_sources_cache:
            if not isinstance(src, dict):
                continue
            nums = src.get("extracted_numbers")
            if not isinstance(nums, list) or not nums:
                continue
            for n in nums:
                if not isinstance(n, dict):
                    continue
                # Filter junk deterministically (strict rebuild exclusion)
                if _candidate_disallowed_for_metric(n, None):
                    continue
                # Normalize a few fields to ensure stable downstream access
                c = dict(n)
                if not c.get("source_url"):
                    c["source_url"] = src.get("url", "") or src.get("source_url", "") or ""
                candidates.append(c)

    # Deterministic candidate ordering (no set/dict iteration surprises)
    def _cand_sort_key(c: dict):
        return (
            str(c.get("source_url") or ""),
            str(c.get("anchor_hash") or ""),
            int(c.get("start_idx") or 0),
            str(c.get("raw") or ""),
            str(c.get("unit_tag") or ""),
            str(c.get("unit_family") or ""),
            float(c.get("value_norm") or c.get("value") or 0.0),
        )

    candidates.sort(key=_cand_sort_key)

    if not candidates:
        return {}

    # -------------------------
    # Deterministic schema-driven selection
    # -------------------------
    def _norm_text(s: str) -> str:
        return re.sub(r"\s+", " ", (s or "").lower()).strip()

    out = {}

    for canonical_key in sorted(schema.keys()):
        spec = schema.get(canonical_key) or {}
        if not isinstance(spec, dict):
            continue

        spec_keywords = spec.get("keywords") or []
        if not isinstance(spec_keywords, list):
            spec_keywords = []
        spec_keywords_norm = [str(k).lower().strip() for k in spec_keywords if str(k).strip()]

        spec_unit_tag = str(spec.get("unit_tag") or spec.get("unit") or "").strip()
        spec_unit_family = str(spec.get("unit_family") or "").strip()

        # Score candidates by schema keyword hits, then filter by unit constraints if present.
        best = None
        best_key = None

        # ============================================================
        # PATCH FIX33 (ADDITIVE): per-metric debug collectors
        # ============================================================
        _fix33_top = []
        _fix33_rej = {}

        for c in candidates:
            # PATCH F: strict candidate exclusion at scoring time
            if _candidate_disallowed_for_metric(c, spec):
                continue
            # =====================================================================
            # PATCH AI2 (ADDITIVE): guard against year-only candidates on currency-like metrics
            # Why:
            # - Some sources contain many years (e.g., 2023, 2024) that can outscore true values.
            # - For currency-ish metrics, suppress candidates that look like bare years unless context clearly indicates money.
            # Determinism:
            # - Pure filter; does not invent candidates or refetch content.
            # =====================================================================
            try:
                def _ai2_is_year_only(c: dict):
                    """Return True if candidate is a likely standalone year (1900-2100) with no unit."""
                    try:
                        c = c if isinstance(c, dict) else {}
                        # Prefer canonical numeric
                        v = c.get("value_norm")
                        if v is None:
                            v = c.get("value")
                        try:
                            iv = int(float(v))
                        except Exception:
                            return False
                        if iv < 1900 or iv > 2100:
                            return False
                        # Must be truly 4-digit (avoid 2023.5 etc)
                        try:
                            if abs(float(v) - float(iv)) > 1e-9:
                                return False
                        except Exception:
                            pass

                        # If the candidate itself signals time/year, do not treat as "junk year".
                        u = str(c.get("base_unit") or c.get("unit") or "").strip().lower()
                        ut = str(c.get("unit_tag") or "").strip().lower()
                        uf = str(c.get("unit_family") or "").strip().lower()
                        if "year" in u or "year" in ut or "year" in uf or "time" in uf:
                            return False

                        raw = str(c.get("raw") or "").strip()
                        sval = str(iv)

                        # -------------------------------------------------------------
                        # PATCH E (ADDITIVE): strict handling when raw contains context
                        # - Some extractors store a wider raw window (e.g. includes '$721m ... in 2023')
                        # - Currency symbols elsewhere in raw should NOT make a year candidate non-year.
                        # - Only treat as non-year if the currency symbol is directly attached to the year.
                        # -------------------------------------------------------------
                        try:
                            if re.search(r"(\$|usd|eur|gbp|aud|cad|sgd)\s*"+re.escape(sval)+r"\b", raw.lower()):
                                return False
                        except Exception:
                            pass

                        # If raw is basically just the year token (allow brackets/punctuation), it's year-only.
                        try:
                            raw2 = re.sub(r"[\s,.;:()\[\]{}<>]", "", raw)
                            if raw2 == sval:
                                return True
                        except Exception:
                            pass

                        # If raw contains multiple numbers, it's likely context; still treat as year-only
                        # when this candidate has no unit.
                        try:
                            nums = re.findall(r"\d{2,}", raw)
                            if len(nums) >= 2:
                                return True
                        except Exception:
                            pass

                        # If raw contains month names, likely a date; treat as year-only (we suppress dates too).
                        try:
                            if re.search(r"\b(jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)\b", raw.lower()):
                                return True
                        except Exception:
                            pass

                        return True
                    except Exception:
                        return False
                def _ai2_schema_currencyish(sd: dict) -> bool:
                    try:
                        if not isinstance(sd, dict):
                            return False
                        u = str(sd.get('unit') or sd.get('base_unit') or '').lower()
                        if any(x in u for x in ('usd','sgd','eur','gbp','jpy','$','€','£')):
                            return True
                        # heuristic keywords on definition (safe, schema-driven-ish)
                        nm = str(sd.get('name') or '').lower()
                        if any(x in nm for x in ('revenue','sales','cost','price','capex','opex','investment','spend','spending','expenditure','value')):
                            return True
                        return False
                    except Exception:
                        return False

                _sd = locals().get('schema_def')
                if _ai2_schema_currencyish(_sd) and _ai2_is_year_only(c):
                    continue

                # =====================================================================
                # PATCH YEAR3 (ADDITIVE): suppress year-only candidates for percent/CAGR-like metrics too
                # Why: year tokens (e.g., 2025) can outrank true percent values when unit evidence is weak.
                # Safe: only suppress when candidate has no explicit unit and looks like a bare year.
                # =====================================================================
                try:
                    if _ai2_is_year_only(c):
                        _sd_name = str((_sd or {}).get('name') or '').lower()
                        _sd_ckey = str((_sd or {}).get('canonical_key') or ckey or '').lower()
                        _sd_unit_tag = str((_sd or {}).get('unit_tag') or '').lower()
                        _sd_unit_family = str((_sd or {}).get('unit_family') or '').lower()
                        if ('cagr' in _sd_name) or ('cagr' in _sd_ckey) or (_sd_unit_tag in ('percent','pct')) or (_sd_unit_family in ('percent','ratio','rate')):
                            continue
                except Exception:
                    pass
                # =====================================================================
            except Exception:
                pass
            # =====================================================================
            ctx = _norm_text(c.get("context_snippet") or "")
            if not ctx:
                continue

            # Keyword hits: schema-driven (no external heuristics)
            hits = 0
            if spec_keywords_norm:
                for kw in spec_keywords_norm:
                    if kw and kw in ctx:
                        hits += 1

            if spec_keywords_norm and hits == 0:
                continue

            # Unit constraints (only if schema declares them)
            if spec_unit_family:
                if str(c.get("unit_family") or "").strip() != spec_unit_family:
                    # allow a unit_tag-only match when family is missing in candidate
                    if not (spec_unit_tag and str(c.get("unit_tag") or "").strip() == spec_unit_tag):
                        continue

            if spec_unit_tag:
                # if a tag is specified, prefer exact tag matches
                if str(c.get("unit_tag") or "").strip() != spec_unit_tag:
                    # allow family match when tag differs
                    if not (spec_unit_family and str(c.get("unit_family") or "").strip() == spec_unit_family):
                        continue

            # =====================================================================
            # PATCH FIX41AFC5 (ADDITIVE): reject year-only candidates early (schema-only rebuild parity)
            # =====================================================================
            try:
                _vnorm = c.get("value_norm", None)
                if _vnorm is None:
                    _vnorm = c.get("value", None)
                _is_year = _is_yearish_value(_vnorm)
                _mk0 = str(c.get("measure_kind") or "").strip().lower()
                _cand_ut0 = str(c.get("unit_tag") or "").strip()
                _cand_fam0 = str(c.get("unit_family") or "").strip().lower()
                _is_pct0 = bool(c.get("is_percent") or c.get("has_percent") or (_cand_ut0 == "%") or (_cand_fam0 == "percent"))
                _has_curr0 = bool(str(c.get("currency_symbol") or c.get("currency") or "").strip())
                _has_unit_ev0 = bool(_cand_ut0 or _cand_fam0 or _is_pct0 or _has_curr0 or str(c.get("base_unit") or c.get("unit") or "").strip())
                if _is_year and (not _has_unit_ev0) and (not _is_pct0) and (not _has_curr0) and (_mk0 in ("magnitude_other", "count_units", "count", "number", "")):
                    try:
                        _fix41afc5_dbg2["rejected_year_only"] = int(_fix41afc5_dbg2.get("rejected_year_only", 0) or 0) + 1
                    except Exception:
                        pass
                    continue
            except Exception:
                pass

            # =====================================================================
            # PATCH FIX33 (ADDITIVE): hard-reject unit-less candidates when unit is required
            # =====================================================================
            try:
                _req = _fix33_schema_unit_required(spec_unit_family, spec_unit_tag, canonical_key)
                _has_unit_ev = _fix33_candidate_has_unit_evidence(c)
                if _req and not _has_unit_ev:
                    # Track rejection (debug)
                    if _fix33_dbg:
                        try:
                            _fix33_rej["missing_unit_required"] = int(_fix33_rej.get("missing_unit_required", 0) or 0) + 1
                        except Exception:
                            pass
                    continue

                # Track top candidates (debug)
                if _fix33_dbg:
                    try:
                        _fix33_top.append({
                            "raw": c.get("raw"),
                            "value_norm": c.get("value_norm"),
                            "unit_tag": c.get("unit_tag"),
                            "unit_family": c.get("unit_family"),
                            "base_unit": c.get("base_unit") or c.get("unit"),
                            "measure_kind": c.get("measure_kind"),
                            "hits": hits,
                            "has_unit_ev": bool(_has_unit_ev),
                            "source_url": c.get("source_url"),
                            "anchor_hash": c.get("anchor_hash"),
                        })
                    except Exception:
                        pass
            except Exception:
                pass

            # Deterministic tie-break:
            #   (-hits, then stable candidate identity tuple)
            tie = (-hits,) + _cand_sort_key(c)
            if best is None or tie < best_key:
                best = c
                best_key = tie


        # =====================================================================
        # PATCH FIX41AFC46 (ADDITIVE): Anchor / same-source rescue to prevent
        # cross-source hijack when an injected or unrelated URL contains a
        # numerically-eligible but semantically-wrong candidate.
        #
        # Why:
        # - We observed 'Current' selecting 170.0 from an injected GlobeNewswire
        #   page for Units Sold (2024) instead of 17.8M from ev-volumes.
        # - Root cause is usually anchor/hash non-compatibility + permissive
        #   fallback that roams across the entire candidate pool.
        #
        # What this does (non-fastpath only, pure selection-time override):
        # - If a preferred URL exists from prev metric_anchors or prev metric
        #   source_url, and best candidate comes from a different URL, attempt to
        #   re-select the best eligible candidate from the preferred URL.
        # - Also attempts anchor-hash compatibility matches (legacy vs v2).
        # - If a preferred-URL candidate is found, it replaces `best`.
        # - If none found, selection remains unchanged (backward compatible).
        # =====================================================================
        try:
            _ck = str(spec.get("canonical_key") or "")
            _prev_anchors = (prev_response.get("metric_anchors") or {}) if isinstance(prev_response, dict) else {}
            _prev_pm = (prev_response.get("primary_metrics_canonical") or {}) if isinstance(prev_response, dict) else {}
            _pa = _prev_anchors.get(_ck) if isinstance(_prev_anchors, dict) else None
            _pm = _prev_pm.get(_ck) if isinstance(_prev_pm, dict) else None

            def _fix41afc46__get_pref_url():
                # Prefer anchor URL; else prev metric's source_url; else blank.
                if isinstance(_pa, dict) and str(_pa.get("source_url") or "").strip():
                    return str(_pa.get("source_url")).strip()
                if isinstance(_pm, dict) and str(_pm.get("source_url") or "").strip():
                    return str(_pm.get("source_url")).strip()
                return ""

            def _fix41afc46__anchor_hashes(a):
                if not isinstance(a, dict):
                    return set()
                hs = set()
                for k in ("anchor_hash", "anchor_hash_v2", "anchor_hash_stable", "cur_anchor_hash", "prev_anchor_hash"):
                    v = a.get(k)
                    if v is not None and str(v).strip():
                        hs.add(str(v).strip())
                # Some historic anchor_hash values are stored as "None" string.
                hs = {h for h in hs if h.lower() != "none"}
                return hs

            def _fix41afc46__cand_hashes(c):
                if not isinstance(c, dict):
                    return set()
                hs = set()
                for k in ("anchor_hash", "anchor_hash_v2", "anchor_hash_stable"):
                    v = c.get(k)
                    if v is not None and str(v).strip():
                        hs.add(str(v).strip())
                # Support legacy "candidate_id"+"suffix" anchor form when present
                cid = str(c.get("candidate_id") or "").strip()
                if cid and len(cid) >= 8:
                    hs.add(cid)
                return hs

            _pref_url = _fix41afc46__get_pref_url()
            _best_url = str(best.get("source_url") or best.get("url") or "").strip()
            if _pref_url and _best_url and _pref_url != _best_url:
                _pa_hashes = _fix41afc46__anchor_hashes(_pa)
                _pm_hashes = _fix41afc46__anchor_hashes(_pm)
                _want_hashes = set()
                _want_hashes |= _pa_hashes
                _want_hashes |= _pm_hashes

                _best_pref = None
                _best_pref_key = None

                for _c in candidates:
                    if not isinstance(_c, dict):
                        continue
                    _cu = str(_c.get("source_url") or _c.get("url") or "").strip()
                    if _cu != _pref_url:
                        continue
                    # Must still respect all existing disallow/eligibility logic.
                    if _candidate_disallowed_for_metric(_c, spec):
                        continue

                    # If we have anchor hashes, require a compatible match OR a value+unit match.
                    _cand_hs = _fix41afc46__cand_hashes(_c)
                    _hash_ok = (bool(_want_hashes) and bool(_cand_hs.intersection(_want_hashes)))

                    _val_ok = False
                    try:
                        _pv = None
                        if isinstance(_pm, dict):
                            _pv = _pm.get("value_norm", _pm.get("value"))
                        if _pv is None:
                            _pv = prev_val_norm  # may exist in outer scope in some implementations
                        if _pv is not None and _c.get("value_norm") is not None:
                            if abs(float(_c.get("value_norm")) - float(_pv)) <= max(1e-9, abs(float(_pv))*0.005):
                                _val_ok = True
                    except Exception:
                        pass

                    if _want_hashes and not (_hash_ok or _val_ok):
                        continue

                    _k = _cand_sort_key(_c)
                    if _best_pref is None or _k < _best_pref_key:
                        _best_pref = _c
                        _best_pref_key = _k

                if isinstance(_best_pref, dict):
                    # Swap-in preferred source candidate.
                    best = _best_pref
                    best_key = _best_pref_key
                    # Annotate (additive) that a rescue occurred for downstream debug
                    try:
                        best.setdefault("_fix41afc46_rescued", True)
                        best.setdefault("_fix41afc46_preferred_url", _pref_url)
                    except Exception:
                        pass
        except Exception:
            pass
        # =====================================================================
        # PATCH FIX41AFC46 END
        # =====================================================================

        if not isinstance(best, dict):
            continue

        # Emit a minimal canonical metric row (schema-driven, deterministic)
        metric = {
            "name": spec.get("name") or spec.get("canonical_id") or canonical_key,
            "value": best.get("value"),
            "unit": best.get("unit") or spec.get("unit") or "",
            "unit_tag": best.get("unit_tag") or spec.get("unit_tag") or "",
            "unit_family": best.get("unit_family") or spec.get("unit_family") or "",
            "base_unit": best.get("base_unit") or best.get("unit_tag") or spec.get("unit_tag") or "",
            "multiplier_to_base": best.get("multiplier_to_base") if best.get("multiplier_to_base") is not None else 1.0,
            "value_norm": best.get("value_norm") if best.get("value_norm") is not None else best.get("value"),
            "canonical_id": spec.get("canonical_id") or spec.get("canonical_key") or canonical_key,
            "canonical_key": canonical_key,
            "dimension": spec.get("dimension") or "",
            "original_name": spec.get("name") or "",
            "geo_scope": "unknown",
            "geo_name": "",
            "is_proxy": False,
            "proxy_type": "",
            "provenance": {
                "method": "schema_keyword_match",
                "best_candidate": {
                    "raw": best.get("raw"),
                    "source_url": best.get("source_url"),
                    "context_snippet": best.get("context_snippet"),
                    "anchor_hash": best.get("anchor_hash"),
                    "start_idx": best.get("start_idx"),
                    "end_idx": best.get("end_idx"),
                },
            },
        }

# ============================================================
        # ============================================================
        # PATCH FIX33 (ADDITIVE): selection debug (top candidates + rejection counts)
        # ============================================================
        try:
            if _fix33_dbg and isinstance(metric, dict):
                try:
                    _fix33_top_sorted = sorted(
                        _fix33_top,
                        key=lambda d: (-(int(d.get("hits") or 0)), str(d.get("value_norm") or ""), str(d.get("raw") or "")),
                    )
                except Exception:
                    _fix33_top_sorted = _fix33_top
                metric.setdefault("provenance", {})
                metric["provenance"]["fix33_top_candidates"] = list(_fix33_top_sorted[:10])
                metric["provenance"]["fix33_rejected_reason_counts"] = dict(_fix33_rej or {})
        except Exception:
            pass

        out[canonical_key] = metric

    return out



# ===================== PATCH RMS_AWARE1 (ADDITIVE) =====================
def rebuild_metrics_from_snapshots_with_anchors(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    Anchor-aware deterministic rebuild (analysis-aligned):
      - Uses ONLY snapshots/cache + frozen schema + prior metric_anchors (if present)
      - No re-fetch
      - No heuristic matching outside anchor_hash + schema dimension checks
      - Deterministic ordering and selection

    Strategy:
      1) Load metric_anchors (canonical_key -> {anchor_hash, ...}) from prev_response (any common nesting).
      2) Flatten snapshot candidates (extracted_numbers) from baseline_sources_cache.
      3) For each canonical_key with an anchor_hash:
           pick candidate with matching anchor_hash (and compatible unit family if inferable).
      4) Build primary_metrics_canonical-like dict.

    Returns: dict {canonical_key: metric_obj}
    """
    import re

    if not isinstance(prev_response, dict):
        return {}

    # 1) Pull anchors from any common location
    metric_anchors = (
        prev_response.get("metric_anchors")
        or (prev_response.get("primary_response") or {}).get("metric_anchors")
        or (prev_response.get("results") or {}).get("metric_anchors")
    )
    if not isinstance(metric_anchors, dict) or not metric_anchors:
        return {}

    # 2) Pull frozen schema (for name/dimension hints; optional but preferred)
    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )

    # Flatten candidates from baseline_sources_cache (list of source dicts with extracted_numbers)
    if isinstance(baseline_sources_cache, dict) and isinstance(baseline_sources_cache.get("snapshots"), list):
        sources = baseline_sources_cache.get("snapshots", [])
    elif isinstance(baseline_sources_cache, list):
        sources = baseline_sources_cache
    else:
        sources = []

    candidates = []
    for s in sources:
        if not isinstance(s, dict):
            continue
        url = s.get("source_url") or s.get("url") or ""
        xs = s.get("extracted_numbers")
        if isinstance(xs, list) and xs:
            for c in xs:
                if not isinstance(c, dict):
                    continue
                if _candidate_disallowed_for_metric(c, None):
                    continue
                c2 = dict(c)
                c2.setdefault("source_url", url)
                candidates.append(c2)

    # Deterministic sort key (stable across runs)
    def _cand_sort_key(c: dict):
        try:
            return (
                str(c.get("anchor_hash") or ""),
                str(c.get("source_url") or ""),
                int(c.get("start_idx") or 0),
                str(c.get("raw") or ""),
                str(c.get("unit") or ""),
                float(c.get("value_norm") or 0.0),
            )
        except Exception:
            return ("", "", 0, "", "", 0.0)

    candidates.sort(key=_cand_sort_key)

    # Unit family inference (lightweight; used only as a compatibility guard)
    def _unit_family(unit: str) -> str:
        u = (unit or "").strip().lower()
        if u in ("%", "percent", "percentage"):
            return "percent"
        if any(x in u for x in ("usd", "$", "eur", "gbp", "jpy", "cny", "aud", "sgd")):
            return "currency"
        if any(x in u for x in ("unit", "units", "vehicle", "vehicles", "kwh", "mwh", "gwh", "twh", "ton", "tons")):
            return "quantity"
        return ""

    rebuilt = {}

    # 3) Anchor_hash match first (no schema-free guessing)
    for canonical_key, a in metric_anchors.items():
        if not isinstance(a, dict):
            continue
        ah = a.get("anchor_hash") or a.get("anchor") or ""
        if not ah:
            continue

        sch = metric_schema.get(canonical_key) if isinstance(metric_schema, dict) else None
        name = (sch or {}).get("name") or a.get("name") or canonical_key
        expected_dim = ((sch or {}).get("dimension") or (sch or {}).get("unit_family") or "").strip().lower()

        best = None

        # =====================================================================
        # PATCH AI_CAND3 (ADDITIVE): pick best candidate among same anchor_hash
        # =====================================================================
        same = [c for c in candidates if isinstance(c, dict) and (c.get("anchor_hash") or "") == ah and c.get("is_junk") is not True]
        best = _pick_best_candidate(
            same,
            expected_dim=expected_dim,
            expected_unit_family=str((schema.get(ckey) or {}).get("unit_family") or ""),
            expected_base_unit=str((schema.get(ckey) or {}).get("base_unit") or ""),
        )
        if not best:
            continue
        # =====================================================================

        rebuilt[canonical_key] = {
            "canonical_key": canonical_key,
            "name": name,
            "value": best.get("value"),
            "unit": best.get("unit") or "",
            "value_norm": best.get("value_norm"),
            "source_url": best.get("source_url") or "",
            "anchor_hash": best.get("anchor_hash") or "",
            "anchor_used": bool(best.get("_fix41afc54_anchor_used") or best.get("_fix41afc45_anchor_used")),
            "anchor_resolve_method": str(best.get("_fix41afc54_anchor_method") or ("fix41afc45" if best.get("_fix41afc45_anchor_used") else "")),
            "evidence": [{
                "source_url": best.get("source_url") or "",
                "raw": best.get("raw") or "",
                "context_snippet": (best.get("context") or best.get("context_window") or "")[:400],
                "anchor_hash": best.get("anchor_hash") or "",
                "method": "anchor_hash_rebuild",
            }],
        }

    return rebuilt
# =================== END PATCH RMS_AWARE1 (ADDITIVE) ===================



def get_history(limit: int = MAX_HISTORY_ITEMS) -> List[Dict]:
    """Load analysis history from Google Sheet"""
    sheet = get_google_sheet()
    if not sheet:
        # Fallback to session state
        return st.session_state.get('analysis_history', [])

    try:
        # ============================================================
        # PATCH GH_KEY1 (ADDITIVE): Use the actual worksheet title as cache key
        # Why:
        # - Your sheet names are: 'Sheet1', 'Snapshots', 'HistoryFull'
        # - There is no worksheet called 'History'
        # - Using cache_key='History' can cache empty reads under the wrong key.
        # ============================================================
        _ws_title = getattr(sheet, "title", "") or "Sheet1"
        _cache_key = f"History::{_ws_title}"
        # ============================================================

        # Get all rows (skip header)
        values = []
        try:
            values = sheets_get_all_values_cached(sheet, cache_key=_cache_key)
        except Exception:
            values = []

        # ============================================================
        # PATCH GH_FALLBACK1 (ADDITIVE): One direct-read retry if cached read is empty
        # Why:
        # - If a prior transient read/429 produced an empty cached value,
        #   evolution may temporarily see no history even though rows exist.
        # ============================================================
        if not values or len(values) < 2:
            try:
                direct = sheet.get_all_values()
                if direct and len(direct) >= 2:
                    values = direct
            except Exception:
                pass
        # ============================================================

        all_rows = values[1:] if values and len(values) >= 2 else []

        # ============================================================
        # PATCH GH_RL1 (ADDITIVE): Rate-limit fallback for History reads
        # ============================================================
        try:
            if (not all_rows) and globals().get("_SHEETS_LAST_READ_ERROR"):
                if ("RESOURCE_EXHAUSTED" in str(_SHEETS_LAST_READ_ERROR)
                    or "Quota exceeded" in str(_SHEETS_LAST_READ_ERROR)
                    or "429" in str(_SHEETS_LAST_READ_ERROR)):
                    return st.session_state.get('analysis_history', [])
        except Exception:
            pass
        # ============================================================

        # Parse and return most recent
        history = []
        for row in all_rows[-limit:]:
            if len(row) >= 5:
                raw_cell = row[4]
                try:
                    data = json.loads(raw_cell)
                    data['_sheet_id'] = row[0]  # Keep track of sheet row ID

                    # (your existing GH2 / ES1G / GH1 / GH3 logic unchanged)
                    # ...
                    history.append(data)

                except json.JSONDecodeError:
                    # (your existing GH1 rescue logic unchanged)
                    continue

        # (your existing GH3 sort unchanged)
        return history

    except Exception as e:
        st.warning(f"⚠️ Failed to load from Google Sheets: {e}")
        return st.session_state.get('analysis_history', [])


def get_analysis_by_id(analysis_id: str) -> Optional[Dict]:
    """Get a specific analysis by ID"""
    sheet = get_google_sheet()
    if not sheet:
        return None

    try:
        # Find row with matching ID
        cell = sheet.find(analysis_id)
        if cell:
            row = sheet.row_values(cell.row)
            if len(row) >= 5:
                return json.loads(row[4])
    except Exception as e:
        st.warning(f"⚠️ Failed to find analysis: {e}")

    return None

def delete_from_history(analysis_id: str) -> bool:
    """Delete an analysis from history"""
    sheet = get_google_sheet()
    if not sheet:
        return False

    try:
        cell = sheet.find(analysis_id)
        if cell:
            sheet.delete_rows(cell.row)
            return True
    except Exception as e:
        st.warning(f"⚠️ Failed to delete: {e}")

    return False

def clear_history() -> bool:
    """Clear all history (keep headers)"""
    sheet = get_google_sheet()
    if not sheet:
        st.session_state.analysis_history = []
        return True

    try:
        # Get row count
        all_rows = sheets_get_all_values_cached(sheet, cache_key="History")
        if len(all_rows) > 1:
            # Delete all rows except header
            sheet.delete_rows(2, len(all_rows))
        return True
    except Exception as e:
        st.warning(f"⚠️ Failed to clear history: {e}")
        return False

def format_history_label(analysis: Dict) -> str:
    """Format a history item for dropdown display"""
    timestamp = analysis.get('timestamp', '')
    question = analysis.get('question', 'Unknown query')[:40]
    confidence = analysis.get('final_confidence', '')

    try:
        dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
        now = datetime.now()
        delta = now - dt.replace(tzinfo=None)

        if delta.total_seconds() < 3600:
            time_str = f"{int(delta.total_seconds() / 60)}m ago"
        elif delta.total_seconds() < 86400:
            time_str = f"{int(delta.total_seconds() / 3600)}h ago"
        elif delta.days == 1:
            time_str = "Yesterday"
        elif delta.days < 7:
            time_str = f"{delta.days}d ago"
        else:
            time_str = dt.strftime("%b %d")
    except:
        time_str = timestamp[:10] if timestamp else "Unknown"

    conf_str = f" ({confidence:.0f}%)" if isinstance(confidence, (int, float)) else ""
    return f"{time_str}: {question}...{conf_str}"

def get_history_options() -> List[Tuple[str, int]]:
    """Get formatted history options for dropdown"""
    history = get_history()
    options = []
    for i, analysis in enumerate(reversed(history)):  # Most recent first
        label = format_history_label(analysis)
        actual_index = len(history) - 1 - i
        options.append((label, actual_index))
    return options

# =========================================================
# 1. CONFIGURATION & API KEY VALIDATION
# =========================================================

def load_api_keys():
    """Load and validate API keys from secrets or environment"""

    # =====================================================================
    # PATCH FIX41AFC5 (ADDITIVE): debug counters for schema-only rebuild eligibility hardening
    # =====================================================================
    _fix41afc5_dbg2 = {"rejected_year_only": 0, "rejected_unitless": 0, "rejected_magnitude_other_unitless": 0}
    # =====================================================================

    # =====================================================================
    # PATCH FIX41AFC5 (ADDITIVE): debug counters for rebuild eligibility hardening
    # =====================================================================
    _fix41afc5_dbg = {"rejected_year_only": 0, "rejected_unitless": 0, "rejected_magnitude_other_unitless": 0}
    # =====================================================================
    try:
        PERPLEXITY_KEY = st.secrets.get("PERPLEXITY_API_KEY") or os.getenv("PERPLEXITY_API_KEY", "")
        GEMINI_KEY = st.secrets.get("GEMINI_API_KEY") or os.getenv("GEMINI_API_KEY", "")
        SERPAPI_KEY = st.secrets.get("SERPAPI_KEY") or os.getenv("SERPAPI_KEY", "")
        SCRAPINGDOG_KEY = st.secrets.get("SCRAPINGDOG_KEY") or os.getenv("SCRAPINGDOG_KEY", "")
    except Exception:
        PERPLEXITY_KEY = os.getenv("PERPLEXITY_API_KEY", "")
        GEMINI_KEY = os.getenv("GEMINI_API_KEY", "")
        SERPAPI_KEY = os.getenv("SERPAPI_KEY", "")
        SCRAPINGDOG_KEY = os.getenv("SCRAPINGDOG_KEY", "")

    # Validate critical keys
    if not PERPLEXITY_KEY or len(PERPLEXITY_KEY) < 10:
        st.error("❌ PERPLEXITY_API_KEY is missing or invalid")
        st.stop()

    if not GEMINI_KEY or len(GEMINI_KEY) < 10:
        st.error("❌ GEMINI_API_KEY is missing or invalid")
        st.stop()

    return PERPLEXITY_KEY, GEMINI_KEY, SERPAPI_KEY, SCRAPINGDOG_KEY

PERPLEXITY_KEY, GEMINI_KEY, SERPAPI_KEY, SCRAPINGDOG_KEY = load_api_keys()
PERPLEXITY_URL = "https://api.perplexity.ai/chat/completions"

# Configure Gemini
genai.configure(api_key=GEMINI_KEY)
gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')

# =========================================================
# 2. PYDANTIC MODELS
# =========================================================

class MetricDetail(BaseModel):
    """Individual metric with name, value, and unit"""
    name: str = Field(..., description="Metric name")
    value: Union[float, int, str] = Field(..., description="Metric value")
    unit: str = Field(default="", description="Unit of measurement")
    model_config = ConfigDict(extra='ignore')

class TopEntityDetail(BaseModel):
    """Entity in top_entities list"""
    name: str = Field(..., description="Entity name")
    share: Optional[str] = Field(None, description="Market share")
    growth: Optional[str] = Field(None, description="Growth rate")
    model_config = ConfigDict(extra='ignore')

class TrendForecastDetail(BaseModel):
    """Trend forecast item"""
    trend: str = Field(..., description="Trend description")
    direction: Optional[str] = Field(None, description="Direction indicator")
    timeline: Optional[str] = Field(None, description="Timeline")
    model_config = ConfigDict(extra='ignore')

class VisualizationData(BaseModel):
    chart_labels: List[str] = Field(default_factory=list)
    chart_values: List[Union[float, int]] = Field(default_factory=list)
    chart_title: Optional[str] = Field("Trend Analysis")
    chart_type: Optional[str] = Field("line")
    x_axis_label: Optional[str] = None
    y_axis_label: Optional[str] = None
    model_config = ConfigDict(extra='ignore')

class ComparisonBar(BaseModel):
    """Comparison bar chart data"""
    title: str = Field("Comparison", description="Chart title")
    categories: List[str] = Field(default_factory=list)
    values: List[Union[float, int]] = Field(default_factory=list)
    model_config = ConfigDict(extra='ignore')

class BenchmarkTable(BaseModel):
    """Benchmark table row"""
    category: str
    value_1: Union[float, int, str] = Field(default=0, description="Numeric value or string")
    value_2: Union[float, int, str] = Field(default=0, description="Numeric value or string")
    model_config = ConfigDict(extra='ignore')

class Action(BaseModel):
    """Investment/action recommendation"""
    recommendation: str = Field("Neutral", description="Buy/Hold/Sell/Neutral")
    confidence: str = Field("Medium", description="High/Medium/Low")
    rationale: str = Field("", description="Reasoning")
    model_config = ConfigDict(extra='ignore')

class LLMResponse(BaseModel):
    """Complete LLM response schema"""
    executive_summary: str = Field(..., description="High-level summary")
    primary_metrics: Dict[str, MetricDetail] = Field(default_factory=dict)
    key_findings: List[str] = Field(default_factory=list)
    top_entities: List[TopEntityDetail] = Field(default_factory=list)
    trends_forecast: List[TrendForecastDetail] = Field(default_factory=list)
    visualization_data: Optional[VisualizationData] = None
    comparison_bars: Optional[ComparisonBar] = None
    benchmark_table: Optional[List[BenchmarkTable]] = None
    sources: List[str] = Field(default_factory=list)
    confidence: Union[float, int] = Field(default=75)
    freshness: Optional[str] = Field(None)
    action: Optional[Action] = None
    model_config = ConfigDict(extra='ignore')

# =========================================================
# 3. PROMPTS
# =========================================================

RESPONSE_TEMPLATE = """
{
  "executive_summary": "3-4 sentence high-level answer",
  "primary_metrics": {
    "metric_1": {"name": "Key Metric 1", "value": 25.5, "unit": "%"},
    "metric_2": {"name": "Key Metric 2", "value": 623, "unit": "$B"}
  },
  "key_findings": [
    "Finding 1 with quantified impact",
    "Finding 2 explaining drivers"
  ],
  "top_entities": [
    {"name": "Entity 1", "share": "25%", "growth": "15%"}
  ],
  "trends_forecast": [
    {"trend": "Trend description", "direction": "↑", "timeline": "2025-2027"}
  ],
  "visualization_data": {
    "chart_labels": ["2023", "2024", "2025"],
    "chart_values": [100, 120, 145],
    "chart_title": "Market Growth",
    "chart_type": "line"
  },
  "comparison_bars": {
    "title": "Market Share",
    "categories": ["A", "B", "C"],
    "values": [45, 30, 25]
  },
  "benchmark_table": [
    {"category": "Company A", "value_1": 25.5, "value_2": 623}
  ],
  "sources": ["source1.com"],
  "confidence": 87,
  "freshness": "Dec 2024"
}
"""



SYSTEM_PROMPT = f"""You are a professional market research analyst.

CRITICAL RULES:
1. Return ONLY valid JSON. NO markdown, NO code blocks, NO extra text.
2. NO citation references like [1][2] inside strings.
3. Use double quotes for all keys and string values.
4. NO trailing commas in arrays or objects.
5. Escape internal quotes with backslash.
6. If the prompt includes "Query Structure", you MUST follow it:
   - Treat "MAIN QUESTION" as the primary topic and address it FIRST.
   - Treat "SIDE QUESTIONS" as secondary topics and address them AFTER the main topic.
   - Do NOT let a side question replace the main question just because it is more specific.
   - In executive_summary, clearly separate: "Main:" then "Side:" when side questions exist.


NUMERIC FIELD RULES (IMPORTANT):
- In benchmark_table: value_1 and value_2 MUST be numbers (never "N/A", "null", or text)
- If data unavailable, use 0 for benchmark_table values
- In primary_metrics: values can be numbers or strings with units (e.g., "25.5" or "25.5 billion")
- In top_entities: share and growth can be strings (e.g., "25%")

REQUIRED FIELDS (provide substantive data):

**executive_summary** - MUST be 4-6 complete sentences covering:
  • Sentence 1: Direct answer with specific quantitative data (market size, revenue, units, etc.)
  • Sentence 2: Major players or regional breakdown with percentages/numbers
  • Sentence 3: Key growth drivers or market dynamics
  • Sentence 4: Future outlook with projected CAGR, timeline, or target values
  • Sentence 5 (optional): Challenge, risk, or competitive dynamic

  BAD (too short): "The EV market is growing rapidly due to government policies."

  GOOD: "The global electric vehicle market reached 14.2 million units sold in 2023, representing 18% of total auto sales. China dominates with 60% market share, followed by Europe (25%) and North America (10%). Growth is driven by battery cost reductions (down 89% since 2010), expanding charging infrastructure, and stricter emission regulations in over 20 countries. The market is projected to grow at 21% CAGR through 2030, reaching 40 million units annually. However, supply chain constraints for lithium and cobalt remain key challenges."

- primary_metrics (3+ metrics with numbers)
- key_findings (3+ findings with quantitative details)
- top_entities (3+ companies/countries with market share %)
- trends_forecast (2+ trends with timelines)
- visualization_data (MUST have chart_labels and chart_values)
- benchmark_table (if included, value_1 and value_2 must be NUMBERS, not "N/A")

Even if web data is sparse, use your knowledge to provide complete, detailed analysis.

Output ONLY this JSON structure:
{RESPONSE_TEMPLATE}
"""

EVOLUTION_PROMPT_TEMPLATE = """You are a market research analyst performing an UPDATE ANALYSIS.

You have been given a PREVIOUS ANALYSIS from {time_ago}. Your task is to:
1. Search for CURRENT data on the same metrics and entities
2. Identify what has CHANGED vs what has STAYED THE SAME
3. Provide updated values where data has changed
4. Flag any metrics/entities that are no longer relevant or have new entries

PREVIOUS ANALYSIS:
==================
Question: {previous_question}
Timestamp: {previous_timestamp}

Previous Executive Summary:
{previous_summary}

Previous Key Metrics:
{previous_metrics}

Previous Top Entities:
{previous_entities}

Previous Key Findings:
{previous_findings}
==================

CRITICAL RULES:
1. Return ONLY valid JSON. NO markdown, NO code blocks.
2. For EACH metric, indicate if it INCREASED, DECREASED, or stayed UNCHANGED
3. Keep the SAME metric names as previous analysis for easy comparison
4. If a metric is no longer available, mark it as "discontinued"
5. If there's a NEW important metric, add it with status "new"


REQUIRED OUTPUT FORMAT:
{{
  "executive_summary": "Updated 4-6 sentence summary noting key changes since last analysis",
  "analysis_delta": {{
    "time_since_previous": "{time_ago}",
    "overall_trend": "improving/declining/stable",
    "major_changes": ["Change 1", "Change 2"],
    "data_freshness": "Q4 2024"
  }},
  "primary_metrics": {{
    "metric_key": {{
      "name": "Same metric name as before",
      "previous_value": 100,
      "current_value": 110,
      "unit": "$B",
      "change_pct": 10.0,
      "direction": "increased/decreased/unchanged",
      "status": "updated/discontinued/new"
    }}
  }},
  "key_findings": [
    "[UNCHANGED] Finding that remains true",
    "[UPDATED] Finding with new data",
    "[NEW] Completely new finding",
    "[REMOVED] Finding no longer relevant - reason"
  ],
  "top_entities": [
    {{
      "name": "Company A",
      "previous_share": "25%",
      "current_share": "27%",
      "previous_rank": 1,
      "current_rank": 1,
      "change": "increased",
      "status": "updated"
    }}
  ],
  "trends_forecast": [
    {{"trend": "Trend description", "direction": "↑", "timeline": "2025-2027", "confidence": "high/medium/low"}}
  ],
  "visualization_data": {{
    "chart_labels": ["Previous", "Current"],
    "chart_values": [100, 110],
    "chart_title": "Market Size Evolution"
  }},
  "sources": ["source1.com", "source2.com"],
  "confidence": 85,
  "freshness": "Dec 2024",
  "drift_summary": {{
    "metrics_changed": 2,
    "metrics_unchanged": 3,
    "entities_reshuffled": 1,
    "findings_updated": 4,
    "overall_stability_pct": 75
  }}
}}

NOW, search for CURRENT information to UPDATE the previous analysis.
Focus on finding CHANGES to the metrics and entities listed above.

User Question: {query}
"""

# =========================================================
# 4. MODEL LOADING
# =========================================================

@st.cache_resource(show_spinner="🔧 Loading AI models...")
def load_models():
    """Load and cache sentence transformer and classifier"""
    try:
        classifier = pipeline(
            "zero-shot-classification",
            model="facebook/bart-large-mnli",
            device=-1
        )
        embedder = SentenceTransformer("all-MiniLM-L6-v2")
        return classifier, embedder
    except Exception as e:
        st.error(f"❌ Model loading failed: {e}")
        st.stop()

domain_classifier, embedder = load_models()

# =========================================================
# 5. JSON REPAIR FUNCTIONS
# =========================================================

def repair_llm_response(data: dict) -> dict:
    """
    Repair common LLM JSON structure issues:

    - Convert primary_metrics from list -> dict (stable keys)
    - Normalize MetricDetail fields so currency+unit do NOT get lost:
        "29.8 S$B" / "S$29.8B" / "S$29.8 billion" -> value=29.8, unit="S$B"
        "$204.7B" -> value=204.7, unit="$B"
        "9.8%" -> value=9.8, unit="%"
    - Ensure top_entities and trends_forecast are lists
    - Fix visualization_data legacy keys (labels/values)
    - Fix benchmark_table numeric values
    - Remove 'action' block entirely (no longer used)
    - Add minimal required fields if missing

    NOTE: This function is intentionally conservative: it normalizes obvious formatting
    without trying to "invent" missing values.
    """
    if not isinstance(data, dict):
        return {}

    def _to_list(x):
        if x is None:
            return []
        if isinstance(x, list):
            return x
        if isinstance(x, dict):
            return list(x.values())
        return []

    def _coerce_number(s: str):
        try:
            return float(str(s).replace(",", "").strip())
        except Exception:
            return None

    def _normalize_metric_item(item: dict) -> dict:
        """
        Normalize a single metric dict in-place-ish and return it.

        Goal: preserve currency + magnitude in `unit`, keep `value` numeric when possible.
        """
        if not isinstance(item, dict):
            return {"name": "N/A", "value": "N/A", "unit": ""}

        name = item.get("name")
        if not isinstance(name, str) or not name.strip():
            name = "N/A"
        item["name"] = name

        raw_val = item.get("value")
        raw_unit = item.get("unit")

        unit = (raw_unit or "")
        if not isinstance(unit, str):
            unit = str(unit)

        # If already numeric and unit looks okay, keep as-is
        if isinstance(raw_val, (int, float)) and isinstance(unit, str):
            item["unit"] = unit.strip()
            return item

        # Try to parse string value forms like:
        # "S$29.8B", "29.8 S$B", "$ 204.7 billion", "9.8%", "12 percent"
        if isinstance(raw_val, str):
            txt = raw_val.strip()

            # Also allow unit to carry the number sometimes (rare but happens)
            # e.g. value="29.8", unit="S$B" is already fine.
            # But if unit is empty and txt contains unit, we extract.
            # Percent detection
            if re.search(r'(%|\bpercent\b)', txt, flags=re.I):
                num = _coerce_number(re.sub(r'[^0-9\.\-\,]+', '', txt))
                if num is not None:
                    item["value"] = num
                    item["unit"] = "%"
                    return item

            # Currency detection
            currency = ""
            # Normalize currency tokens in either value or unit
            combo = f"{txt} {unit}".strip()

            if re.search(r'\bSGD\b', combo, flags=re.I) or "S$" in combo.upper():
                currency = "S$"
            elif re.search(r'\bUSD\b', combo, flags=re.I) or "$" in combo:
                currency = "$"

            # Magnitude detection
            # Accept: T/B/M/K, or words
            mag = ""
            if re.search(r'\btrillion\b', combo, flags=re.I):
                mag = "T"
            elif re.search(r'\bbillion\b', combo, flags=re.I):
                mag = "B"
            elif re.search(r'\bmillion\b', combo, flags=re.I):
                mag = "M"
            elif re.search(r'\bthousand\b', combo, flags=re.I):
                mag = "K"
            else:
                m = re.search(r'([TBMK])\b', combo.replace(" ", ""), flags=re.I)
                if m:
                    mag = m.group(1).upper()

            # Extract numeric
            num = _coerce_number(re.sub(r'[^0-9\.\-\,]+', '', txt))
            if num is not None:
                # If unit was present and meaningful (and already includes %), keep it
                if unit.strip() == "%":
                    item["value"] = num
                    item["unit"] = "%"
                    return item

                # Build unit as currency+magnitude when any found
                # If neither found, keep existing unit (may be e.g. "years", "points")
                if currency or mag:
                    item["value"] = num
                    item["unit"] = f"{currency}{mag}".strip()
                    return item

                # No currency/mag detected: keep unit if provided; else blank
                item["value"] = num
                item["unit"] = unit.strip()
                return item

            # If we can’t parse into a number, at least preserve the original text
            item["value"] = txt
            item["unit"] = unit.strip()
            return item

        # Non-string, non-numeric (None, dict, list, etc.)
        if raw_val is None or raw_val == "":
            item["value"] = "N/A"
        else:
            item["value"] = str(raw_val)

        item["unit"] = unit.strip()
        return item

    # -------------------------
    # primary_metrics normalization
    # -------------------------
    metrics = data.get("primary_metrics")

    # list -> dict
    if isinstance(metrics, list):
        new_metrics = {}
        for i, item in enumerate(metrics):
            if not isinstance(item, dict):
                continue
            item = _normalize_metric_item(item)

            raw_name = item.get("name", f"metric_{i+1}")
            key = re.sub(r'[^a-z0-9_]', '', str(raw_name).lower().replace(" ", "_")).strip("_")
            if not key:
                key = f"metric_{i+1}"

            original_key = key
            j = 1
            while key in new_metrics:
                key = f"{original_key}_{j}"
                j += 1

            new_metrics[key] = item

        data["primary_metrics"] = new_metrics

    elif isinstance(metrics, dict):
        # Normalize each metric dict entry
        cleaned = {}
        for k, v in metrics.items():
            if isinstance(v, dict):
                cleaned[str(k)] = _normalize_metric_item(v)
            else:
                # If someone stored a scalar, wrap it
                cleaned[str(k)] = _normalize_metric_item({"name": str(k), "value": v, "unit": ""})
        data["primary_metrics"] = cleaned

    else:
        data["primary_metrics"] = {}

    # -------------------------
    # list-like fields
    # -------------------------
    data["top_entities"] = _to_list(data.get("top_entities"))
    data["trends_forecast"] = _to_list(data.get("trends_forecast"))
    data["key_findings"] = _to_list(data.get("key_findings"))

    # Ensure strings in key_findings
    data["key_findings"] = [str(x) for x in data["key_findings"] if x is not None and str(x).strip()]

    # -------------------------
    # visualization_data legacy keys
    # -------------------------
    if isinstance(data.get("visualization_data"), dict):
        viz = data["visualization_data"]
        if "labels" in viz and "chart_labels" not in viz:
            viz["chart_labels"] = viz.pop("labels")
        if "values" in viz and "chart_values" not in viz:
            viz["chart_values"] = viz.pop("values")

        # Coerce chart_labels/values types gently
        if "chart_labels" in viz and not isinstance(viz["chart_labels"], list):
            viz["chart_labels"] = [str(viz["chart_labels"])]
        if "chart_values" in viz and not isinstance(viz["chart_values"], list):
            viz["chart_values"] = [viz["chart_values"]]

    # -------------------------
    # benchmark_table numeric cleaning
    # -------------------------
    if isinstance(data.get("benchmark_table"), list):
        cleaned_table = []
        for row in data["benchmark_table"]:
            if not isinstance(row, dict):
                continue

            if "category" not in row:
                row["category"] = "Unknown"

            for key in ["value_1", "value_2"]:
                if key not in row:
                    row[key] = 0
                    continue

                val = row.get(key)
                if isinstance(val, str):
                    val_upper = val.upper().strip()
                    if val_upper in ["N/A", "NA", "NULL", "NONE", "", "-", "—"]:
                        row[key] = 0
                    else:
                        try:
                            cleaned = re.sub(r'[^\d.-]', '', val)
                            row[key] = float(cleaned) if '.' in cleaned else int(cleaned) if cleaned else 0
                        except Exception:
                            row[key] = 0
                elif isinstance(val, (int, float)):
                    pass
                else:
                    row[key] = 0

            cleaned_table.append(row)

        data["benchmark_table"] = cleaned_table

    # -------------------------
    # Remove action block entirely
    # -------------------------
    data.pop("action", None)

    # -------------------------
    # Minimal required top-level fields
    # -------------------------
    if not isinstance(data.get("executive_summary"), str) or not data.get("executive_summary", "").strip():
        data["executive_summary"] = "No executive summary provided."

    if not isinstance(data.get("sources"), list):
        data["sources"] = []

    if "confidence" not in data:
        data["confidence"] = 60

    if not isinstance(data.get("freshness"), str) or not data.get("freshness", "").strip():
        data["freshness"] = "Current"

    return data


def validate_numeric_fields(data: dict, context: str = "LLM Response") -> None:
    """
    Guardrail logger (and gentle coercer) for numeric lists used in charts/tables.

    We keep this lightweight: warn when strings appear where numbers are expected,
    and attempt to coerce when safe.
    """
    if not isinstance(data, dict):
        return

    # Check benchmark_table
    if "benchmark_table" in data and isinstance(data["benchmark_table"], list):
        for i, row in enumerate(data["benchmark_table"]):
            if isinstance(row, dict):
                for key in ["value_1", "value_2"]:
                    val = row.get(key)
                    if isinstance(val, str):
                        st.warning(
                            f"⚠️ {context}: benchmark_table[{i}].{key} is string: '{val}' (coercing to 0 if invalid)"
                        )
                        try:
                            cleaned = re.sub(r"[^\d\.\-]", "", val)
                            row[key] = float(cleaned) if cleaned else 0
                        except Exception:
                            row[key] = 0

    # Check visualization_data chart_values
    viz = data.get("visualization_data")
    if isinstance(viz, dict):
        vals = viz.get("chart_values")
        if isinstance(vals, list):
            new_vals = []
            for j, v in enumerate(vals):
                if isinstance(v, (int, float)):
                    new_vals.append(v)
                elif isinstance(v, str):
                    try:
                        cleaned = re.sub(r"[^\d\.\-]", "", v)
                        new_vals.append(float(cleaned) if cleaned else 0.0)
                        st.warning(f"⚠️ {context}: visualization_data.chart_values[{j}] is string: '{v}' (coerced)")
                    except Exception:
                        new_vals.append(0.0)
                else:
                    new_vals.append(0.0)
            viz["chart_values"] = new_vals


def preclean_json(raw: str) -> str:
    """
    Remove markdown fences and common citation markers before JSON parsing.
    Conservative: tries not to destroy legitimate JSON content.
    """
    if not raw or not isinstance(raw, str):
        return ""

    text = raw.strip()

    # Remove leading/trailing code fences (```json ... ```)
    text = re.sub(r'^\s*```(?:json)?\s*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\s*```\s*$', '', text)

    text = text.strip()

    # Remove common citation formats the model may append
    # [web:1], [1], (1) etc. (but avoid killing array syntax by being specific)
    text = re.sub(r'\[web:\d+\]', '', text, flags=re.IGNORECASE)
    text = re.sub(r'(?<!")\[\d+\](?!")', '', text)   # not inside quotes
    text = re.sub(r'(?<!")\(\d+\)(?!")', '', text)   # not inside quotes

    return text.strip()


def parse_json_safely(json_str: str, context: str = "LLM") -> dict:
    """
    Parse JSON with aggressive error recovery:
    1) Pre-clean markdown/citations
    2) Extract the *first* JSON object
    3) Repair common issues (unquoted keys, trailing commas, True/False/Null)
    4) Try parsing; if it fails, attempt a small set of pragmatic fixes
    """
    if json_str is None:
        return {}
    if not isinstance(json_str, str):
        json_str = str(json_str)

    if not json_str.strip():
        return {}

    cleaned = preclean_json(json_str)

    # Extract first JSON object (most LLM outputs are one object)
    match = re.search(r'\{.*\}', cleaned, flags=re.DOTALL)
    if not match:
        st.warning(f"⚠️ No JSON object found in {context} response")
        return {}

    json_content = match.group(0)

    # Structural repairs
    try:
        # Fix unquoted keys: {key: -> {"key":
        json_content = re.sub(
            r'([\{\,]\s*)([a-zA-Z_][a-zA-Z0-9_\-]*)(\s*):',
            r'\1"\2"\3:',
            json_content
        )

        # Remove trailing commas
        json_content = re.sub(r',\s*([\]\}])', r'\1', json_content)

        # Fix boolean/null capitalization
        json_content = re.sub(r':\s*True\b', ': true', json_content)
        json_content = re.sub(r':\s*False\b', ': false', json_content)
        json_content = re.sub(r':\s*Null\b', ': null', json_content)

    except Exception as e:
        st.warning(f"⚠️ {context}: Regex repair failed: {e}")

    # Attempt parse with a few passes
    attempts = 0
    last_err = None

    while attempts < 6:
        try:
            return json.loads(json_content)
        except json.JSONDecodeError as e:
            last_err = e
            msg = (e.msg or "").lower()

            # Pass 1: replace smart quotes
            if attempts == 0:
                json_content = (
                    json_content.replace("“", '"')
                                .replace("”", '"')
                                .replace("’", "'")
                )

            # Pass 2: single-quote keys/strings -> double quotes (limited)
            elif attempts == 1:
                # Only do this if it looks like single quotes dominate
                if json_content.count("'") > json_content.count('"'):
                    json_content = re.sub(r"\'", '"', json_content)

            # Pass 3: try removing control characters
            elif attempts == 2:
                json_content = re.sub(r"[\x00-\x1F\x7F]", "", json_content)

            # Pass 4: if unterminated string, try escaping a quote near the error
            elif "unterminated string" in msg or "unterminated" in msg:
                pos = e.pos
                # Try escaping a quote a bit before pos
                for i in range(pos - 1, max(0, pos - 200), -1):
                    if i < len(json_content) and json_content[i] == '"':
                        if i == 0 or json_content[i - 1] != "\\":
                            json_content = json_content[:i] + '\\"' + json_content[i + 1:]
                            break

            # Pass 5+: give up
            attempts += 1
            continue

    st.error(f"❌ Failed to parse JSON from {context}: {str(last_err)[:180] if last_err else 'unknown error'}")
    return {}




def parse_query_structure_safe(json_str: str, user_question: str) -> Dict:
    """
    Parse LLM-derived query structure with guaranteed deterministic fallback.
    Never raises, never returns empty dict.
    """
    parsed = parse_json_safely(json_str, context="LLM Query Structure")

    if isinstance(parsed, dict) and parsed:
        # Minimal schema validation
        if "main" in parsed or "category" in parsed:
            return parsed

    # 🔒 Deterministic fallback (NO LLM)
    return {
        "category": "unknown",
        "category_confidence": 0.0,
        "main": user_question,
        "side": []
    }


def extract_json_object(text: str) -> Optional[Dict]:
    """
    Best-effort extraction of the first JSON object from a string.
    Returns dict or None.
    """
    if not text or not isinstance(text, str):
        return None

    # Common cleanup
    cleaned = text.strip()
    cleaned = cleaned.replace("```json", "").replace("```", "").strip()

    # Fast path
    try:
        obj = json.loads(cleaned)
        if isinstance(obj, dict):
            return obj
    except Exception:
        pass

    # Regex: first {...} block (non-greedy)
    try:
        m = re.search(r"\{.*\}", cleaned, flags=re.DOTALL)
        if not m:
            return None
        candidate = m.group(0)
        obj = json.loads(candidate)
        if isinstance(obj, dict):
            return obj
    except Exception:
        return None

    return None


# =========================================================
# 6. WEB SEARCH FUNCTIONS
#   SERPAPI STABILITY CONFIGURATION
# =========================================================

# Fixed parameters to prevent geo/personalization variance

SERPAPI_STABILITY_CONFIG = {
    "gl": "us",                    # Fixed country
    "hl": "en",                    # Fixed language
    "google_domain": "google.com", # Fixed domain
    "nfpr": "1",                   # No auto-query correction
    "safe": "active",              # Consistent safe search
    "device": "desktop",           # Fixed device type
    "no_cache": "false",           # Allow Google caching (more stable)
}

# Preferred domains for consistent sourcing (sorted by priority)
PREFERRED_SOURCE_DOMAINS = [
    "statista.com", "reuters.com", "bloomberg.com", "imf.org", "wsj.com", "bcg.com", "opec.org",
    "worldbank.org", "mckinsey.com", "deloitte.com", "spglobal.com", "ft.com", "pwc.com", "semiconductors.org",
    "ft.com", "economist.com", "wsj.com", "forbes.com", "cnbc.com", "kpmg.com", "eia.org"
]

# Search results cache
_search_cache: Dict[str, Tuple[List[Dict], datetime]] = {}
SEARCH_CACHE_TTL_HOURS = 24

def get_search_cache_key(query: str) -> str:
    """Generate stable cache key for search query"""
    normalized = re.sub(r'\s+', ' ', query.lower().strip())
    normalized = re.sub(r'\b(today|current|latest|now|recent)\b', '', normalized)
    return hashlib.md5(normalized.encode()).hexdigest()[:16]

def get_cached_search_results(query: str) -> Optional[List[Dict]]:
    """
    Get cached search results if still valid.

    IMPORTANT:
    - Never treat cached empty results as valid.
      Returning [] here "poisons" the pipeline for hours and makes SerpAPI look broken.
    """
    try:
        cache_key = get_search_cache_key(query)
        if cache_key in _search_cache:
            cached_results, cached_time = _search_cache[cache_key]
            if datetime.now() - cached_time < timedelta(hours=SEARCH_CACHE_TTL_HOURS):
                # ✅ Do not reuse empty cache entries
                if isinstance(cached_results, list) and len(cached_results) == 0:
                    return None
                return cached_results
            # expired
            del _search_cache[cache_key]
    except Exception:
        return None
    return None


def cache_search_results(query: str, results: List[Dict]):
    """
    Cache search results.

    IMPORTANT:
    - Do NOT cache empty lists
    - Do NOT cache lists that contain no usable URLs
      (prevents "poisoned cache" that makes SerpAPI appear broken)
    """
    try:
        if not isinstance(query, str) or not query.strip():
            return
        if not isinstance(results, list) or not results:
            return

        # Require at least one usable url/link
        has_url = False
        for r in results:
            if isinstance(r, dict):
                u = (r.get("link") or r.get("url") or "").strip()
                if u:
                    has_url = True
                    break
            elif isinstance(r, str) and r.strip():
                has_url = True
                break

        if not has_url:
            return

        cache_key = get_search_cache_key(query)
        _search_cache[cache_key] = (results, datetime.now())
    except Exception:
        return


# =========================================================
# LLM RESPONSE CACHE - Prevents variance on identical inputs
# =========================================================
_llm_cache: Dict[str, Tuple[str, datetime]] = {}
LLM_CACHE_TTL_HOURS = 24  # Cache LLM responses for 24 hours

def get_llm_cache_key(query: str, web_context: Dict) -> str:
    """Generate cache key from query + source URLs"""
    # Include source URLs so cache invalidates if sources change
    source_urls = sorted(web_context.get("sources", [])[:5])
    cache_input = f"{query.lower().strip()}|{'|'.join(source_urls)}"
    return hashlib.md5(cache_input.encode()).hexdigest()[:20]

def get_cached_llm_response(query: str, web_context: Dict) -> Optional[str]:
    """Get cached LLM response if still valid"""
    cache_key = get_llm_cache_key(query, web_context)
    if cache_key in _llm_cache:
        cached_response, cached_time = _llm_cache[cache_key]
        if datetime.now() - cached_time < timedelta(hours=LLM_CACHE_TTL_HOURS):
            return cached_response
        del _llm_cache[cache_key]
    return None

def cache_llm_response(query: str, web_context: Dict, response: str):
    """Cache LLM response"""
    cache_key = get_llm_cache_key(query, web_context)
    _llm_cache[cache_key] = (response, datetime.now())


def sort_results_deterministically(results: List[Dict]) -> List[Dict]:
    """Sort results for consistent ordering"""
    def sort_key(r):
        link = r.get("link", "").lower()
        # Priority: preferred domains first, then alphabetical
        priority = 999
        for i, domain in enumerate(PREFERRED_SOURCE_DOMAINS):
            if domain in link:
                priority = i
                break
        return (priority, link)
    return sorted(results, key=sort_key)


def classify_source_reliability(source: str) -> str:
    """Classify source as High/Medium/Low quality"""
    source = source.lower() if isinstance(source, str) else ""

    high = ["gov", "imf", "worldbank", "central bank", "fed", "ecb", "reuters", "spglobal", "economist", "mckinsey", "bcg", "cognitive market research",
            "financial times", "wsj", "oecd", "bloomberg", "tradingeconomics", "deloitte", "hsbc", "imarc", "booz allen", "bakerinstitute.org", "wef",
           "kpmg", "semiconductors.org", "eu", "iea", "world bank", "opec", "jpmorgan", "citibank", "goldmansachs", "j.p. morgan", "oecd",
           "world bank", "sec", "federalreserve", "bls", "bea"]
    medium = ["wikipedia", "forbes", "cnbc", "yahoo", "ceic", "statista", "trendforce", "digitimes", "idc", "gartner", "marketwatch", "fortune", "investopedia"]
    low = ["blog", "medium.com", "wordpress", "ad", "promo"]

    for h in high:
        if h in source:
            return "✅ High"
    for m in medium:
        if m in source:
            return "⚠️ Medium"
    for l in low:
        if l in source:
            return "❌ Low"

    return "⚠️ Medium"

def source_quality_score(sources: List[str]) -> float:
    """Calculate average source quality (0-100)"""
    if not sources:
        return 50.0  # Lower default when no sources

    weights = {"✅ High": 100, "⚠️ Medium": 60, "❌ Low": 30}
    scores = [weights.get(classify_source_reliability(s), 60) for s in sources]
    return sum(scores) / len(scores) if scores else 50.0

@st.cache_data(ttl=3600, show_spinner=False)
def search_serpapi(query: str, num_results: int = 10) -> List[Dict]:
    """Search Google via SerpAPI with stability controls"""
    if not SERPAPI_KEY:
        return []

    # Check cache first (this is the ONLY cache we use - removed @st.cache_data to avoid conflicts)
    cached = get_cached_search_results(query)
    if cached:
        st.info("📦 Using cached search results")
        return cached

    # Aggressive query normalization for consistent searches
    query_normalized = query.lower().strip()

    # Remove temporal words that cause variance
    query_normalized = re.sub(r'\b(latest|current|today|now|recent|new|upcoming|this year|this month)\b', '', query_normalized)

    # Normalize whitespace
    query_normalized = re.sub(r'\s+', ' ', query_normalized).strip()

    # Add year for consistency
    if not re.search(r'\b20\d{2}\b', query_normalized):
        query_normalized = f"{query_normalized} 2024"

    # Build search terms
    query_lower = query_normalized
    industry_kw = ["industry", "market", "sector", "size", "growth", "players"]

    if any(kw in query_lower for kw in industry_kw):
        search_terms = f"{query_normalized} market size growth statistics"
        tbm, tbs = "", ""  # Organic results (more stable than news)
    else:
        search_terms = f"{query_normalized} finance economics data"
        tbm, tbs = "", ""  # Use organic for stability

    params = {
        "engine": "google",
        "q": search_terms,
        "api_key": SERPAPI_KEY,
        "num": num_results,
        "tbm": tbm,
        "tbs": tbs,
        **SERPAPI_STABILITY_CONFIG  # Add fixed location params
    }

    try:
        resp = requests.get("https://serpapi.com/search", params=params, timeout=10)
        resp.raise_for_status()
        data = resp.json()

        results = []

        # Prefer organic results (more stable than news)
        for item in data.get("organic_results", [])[:num_results]:
            results.append({
                "title": item.get("title", ""),
                "link": item.get("link", ""),
                "snippet": item.get("snippet", ""),
                "date": item.get("date", ""),
                "source": item.get("source", "")
            })

        # Fall back to news only if no organic results
        if not results:
            for item in data.get("news_results", [])[:num_results]:
                src = item.get("source", {})
                source_name = src.get("name", "") if isinstance(src, dict) else str(src)
                results.append({
                    "title": item.get("title", ""),
                    "link": item.get("link", ""),
                    "snippet": item.get("snippet", ""),
                    "date": item.get("date", ""),
                    "source": source_name
                })

        # Sort deterministically
        results = sort_results_deterministically(results)
        results = results[:num_results]

        # Cache results
        if results:
            cache_search_results(query, results)

        return results

    except Exception as e:
        st.warning(f"⚠️ SerpAPI error: {e}")
        return []



# =====================================================================
# PATCH INJ_DIAG_HELPERS (ADDITIVE): Injected-URL diagnostics helpers
# - Pure helpers (no control-flow changes)
# - Used to trace injected extra URLs across: UI -> intake -> scrape -> snapshots -> hashing -> rebuild
# =====================================================================
def _inj_diag_make_run_id(prefix: str = "run") -> str:
    """Short correlation id for a single analysis/evolution run."""
    try:
        import os, time, hashlib
        seed = f"{prefix}|{time.time()}|{os.getpid()}|{os.urandom(8).hex()}"
        return hashlib.sha256(seed.encode("utf-8")).hexdigest()[:12]
    except Exception:
        try:
            import random
            return f"{prefix}_{random.randint(100000,999999)}"
        except Exception:
            return f"{prefix}_unknown"


# =====================================================================
# PATCH INJ_URL_CANON_V1 (ADDITIVE): Canonicalize injected URLs
# - Strips common tracking/query parameters from injected URLs ONLY
# - Keeps scheme/host/path; preserves non-tracking query params (sorted)
# - Adds deterministic canonical form for stable admission/dedupe/hashing
# =====================================================================
def _canonicalize_injected_url(url: str) -> str:
    """Canonicalize injected URLs by stripping known tracking params.

    This is intentionally conservative and applied only to user-injected URLs
    (extra URLs), not to SERP-derived URLs.
    """
    try:
        from urllib.parse import urlsplit, urlunsplit, parse_qsl, urlencode
        u = str(url or "").strip()
        if not u:
            return ""
        if not (u.startswith("http://") or u.startswith("https://")):
            return u

        parts = urlsplit(u)
        # Normalize scheme/host case
        scheme = (parts.scheme or "").lower()
        netloc = (parts.netloc or "").lower()
        path = parts.path or ""
        fragment = ""  # drop fragments for stability

        # Tracking params to drop (exact match)
        drop_exact = {
            "guccounter", "guce_referrer", "guce_referrer_sig",
            "gclid", "fbclid", "msclkid", "mc_cid", "mc_eid",
            "ref", "ref_src",
        }
        # Drop prefixes (utm_*, etc.)
        drop_prefixes = ("utm_",)

        qs = []
        for k, v in parse_qsl(parts.query or "", keep_blank_values=True):
            kk = (k or "").strip()
            if not kk:
                continue
            k_lower = kk.lower()
            if k_lower in drop_exact:
                continue
            if any(k_lower.startswith(p) for p in drop_prefixes):
                continue
            qs.append((kk, v))

        # Sort query params for determinism
        qs_sorted = sorted(qs, key=lambda kv: (kv[0].lower(), str(kv[1])))

        query = urlencode(qs_sorted, doseq=True) if qs_sorted else ""
        return urlunsplit((scheme, netloc, path, query, fragment))
    except Exception:
        try:
            return str(url or "").strip()
        except Exception:
            return ""

def _inj_diag_norm_url_list(extra_urls: Any) -> list:
    """Normalize/dedupe injected URL list (http/https only) with canonicalization.

    NOTE: This is used for injected/extra URL diagnostics and admission wiring only.
    It canonicalizes by stripping known tracking params for stability.
    """
    out = []
    try:
        if extra_urls is None:
            return []
        items = extra_urls
        if isinstance(items, str):
            items = [u.strip() for u in items.splitlines()]
        if not isinstance(items, (list, tuple, set)):
            items = [str(items)]
        seen = set()
        for u in items:
            uu = str(u or "").strip()
            if not uu:
                continue
            if not (uu.startswith("http://") or uu.startswith("https://")):
                continue
            cu = _canonicalize_injected_url(uu) or uu
            if cu in seen:
                continue
            seen.add(cu)
            out.append(cu)
    except Exception:
        return []
    return out


def _inj_diag_set_hash(urls: list) -> str:
    """Stable sha256 of sorted URL list (for compact logging)."""
    try:
        import hashlib
        lst = [str(u or "").strip() for u in (urls or []) if str(u or "").strip()]
        lst = sorted(set(lst))
        payload = "|".join(lst)
        return hashlib.sha256(payload.encode("utf-8")).hexdigest()
    except Exception:
        return ""

def _inj_diag_hash_inputs_from_bsc(baseline_sources_cache: Any) -> list:
    """Extract deterministic URL inputs used by snapshot hashing (v1/v2 both include URL)."""
    urls = []
    try:
        if not isinstance(baseline_sources_cache, list):
            return []
        for sr in baseline_sources_cache:
            if not isinstance(sr, dict):
                continue
            u = (sr.get("source_url") or sr.get("url") or "").strip()
            if u:
                urls.append(u)
    except Exception:
        return []
    return sorted(set(urls))

# =====================================================================
# PATCH INJ_HASH_V1 (ADDITIVE): optional inclusion of injected URLs in snapshot hash inputs
# Default behavior is OFF to avoid disrupting locked fastpath.
#
# When enabled, injected URLs that were persisted (per diag_injected_urls.persisted*)
# but are missing from baseline_sources_cache will be added as *synthetic* source
# records (url-only) so that:
#   - source_snapshot_hash (v1/v2) reflects injected sources deterministically
#   - evolution rebuild sees the same snapshot pool and hash identity via persistence
#
# Safety:
#   - Does NOT modify fastpath logic.
#   - Does NOT change metric selection (synthetic records have no extracted_numbers).
#   - Only activates when INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH is True.
# =====================================================================
INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH = False  # ✅ default OFF (locked fastpath safe)
CODE_VERSION_INJ_HASH_V1 = "fix41r_inj_hash_optional_include"  # additive version marker

def _inj_hash_should_include() -> bool:
    """Single switch for inclusion; additive-only. Supports env override."""
    try:
        import os
        v = os.getenv("YUREEKA_INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH", "").strip().lower()
        if v in ("1", "true", "yes", "y", "on"):
            return True
        if v in ("0", "false", "no", "n", "off"):
            return False
    except Exception:
        pass
    return bool(globals().get("INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH", False))

# =====================================================================
# PATCH INJ_HASH_POLICY_ALIGN_V1 (Additive, policy-aligned)
# Goal:
#   - Align injected URL "new data" identity semantics with baseline sources:
#       If an injected URL is PERSISTED as a successful snapshot, it should
#       participate in snapshot hash inputs by default (unless explicitly disabled).
#   - Preserve existing safety switch INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH
#     and its env override for emergency forcing.
#
# Controls:
#   - Default behavior (policy-aligned): ON when persisted injected URLs exist.
#   - Explicit disable: env YUREEKA_EXCLUDE_INJECTED_URLS_FROM_SNAPSHOT_HASH=1
#   - Explicit force include: env YUREEKA_INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH=1
#
# Notes:
#   - Fastpath logic is NOT modified.
#   - This only affects hash identity input construction; metric selection remains unchanged.
# =====================================================================
INJECTED_URL_HASH_POLICY_ALIGN_WITH_BASELINE = True  # ✅ default ON (policy-aligned)

def _inj_hash_policy_explicit_disable() -> bool:
    try:
        import os
        v = os.getenv("YUREEKA_EXCLUDE_INJECTED_URLS_FROM_SNAPSHOT_HASH", "").strip().lower()
        return v in ("1", "true", "yes", "y", "on")
    except Exception:
        return False

def _inj_hash_policy_should_include(persisted_injected_urls) -> bool:
    """Policy-aligned include decision for injected URLs in hash identity.

    - If explicitly disabled via env, returns False.
    - If explicitly forced via existing switch/env, returns True.
    - Otherwise, when policy-align is enabled and persisted injected URLs exist, returns True.
    - Else, falls back to legacy _inj_hash_should_include().
    """
    try:
        if _inj_hash_policy_explicit_disable():
            return False
        # Respect existing forcing mechanism first
        if _inj_hash_should_include():
            return True
        if bool(globals().get("INJECTED_URL_HASH_POLICY_ALIGN_WITH_BASELINE", True)) and (persisted_injected_urls or []):
            return True
    except Exception:
        pass
    return _inj_hash_should_include()

def _inj_hash_add_synthetic_sources(
    baseline_sources_cache: Any,
    injected_persisted_urls: list,
    now_iso: str = ""
) -> tuple:
    """
    Return (bsc_augmented, added_urls, reasons_by_url) without mutating the original list.
    Synthetic records are url-only, deterministic, and safe for selection logic.
    """
    reasons = {}
    added = []
    try:
        bsc = list(baseline_sources_cache) if isinstance(baseline_sources_cache, list) else []
        inj = _inj_diag_norm_url_list(injected_persisted_urls or [])
        if not inj:
            return (bsc, added, reasons)

        existing = set(_inj_diag_hash_inputs_from_bsc(bsc))
        for u in inj:
            if u in existing:
                reasons[u] = "present_in_bsc"
                continue
            # Add synthetic source record (no numbers) so hash includes the URL deterministically
            added.append(u)
            reasons[u] = "added_synthetic_for_hash"
            bsc.append({
                "url": u,
                "source_url": u,
                "status": "fetched",
                "status_detail": "synthetic_injected_for_hash",
                "numbers_found": 0,
                "fetched_at": now_iso or "",
                "fingerprint": "",
                "extracted_numbers": [],
                "__inj_synthetic": True,
            })

        # Keep deterministic ordering identical to existing conventions
        bsc = sorted(bsc, key=lambda x: str((x or {}).get("url") or ""))
        return (bsc, added, reasons)
    except Exception:
        try:
            return (list(baseline_sources_cache) if isinstance(baseline_sources_cache, list) else [], [], {})
        except Exception:
            return ([], [], {})

# =====================================================================
# PATCH INJ_TRACE_V1_HELPERS (ADDITIVE): canonical injected-URL lifecycle trace builder
# Objective:
# - Emit ONE canonical diagnostic payload in a fixed location for every run:
#     results.debug.inj_trace_v1  (analysis outputs)
#     results.debug.inj_trace_v1  (evolution outputs; mirrored from output.debug)
# - Purely additive; does NOT alter fastpath logic or selection control flow.
# =====================================================================
def _inj_trace_v1_build(
    diag_injected_urls: dict,
    hash_inputs: list,
    stage: str = "analysis",
    path: str = "",
    rebuild_pool: list = None,
    rebuild_selected: list = None,
    hash_exclusion_reasons: dict = None,
) -> dict:
    try:
        d = diag_injected_urls if isinstance(diag_injected_urls, dict) else {}
        ui_raw = d.get("ui_raw") if isinstance(d.get("ui_raw"), (str, list)) else (d.get("extra_urls_ui_raw") or "")
        ui_norm = _inj_diag_norm_url_list(d.get("ui_norm") or d.get("extra_urls_ui_norm") or d.get("extra_urls_normalized") or [])
        intake_norm = _inj_diag_norm_url_list(d.get("intake_norm") or d.get("extra_urls_intake_norm") or d.get("extra_urls") or [])
        admitted_norm = _inj_diag_norm_url_list(d.get("admitted") or d.get("extra_urls_admitted") or [])
        persisted_norm = _inj_diag_norm_url_list(d.get("persisted") or d.get("persisted_norm") or [])

        attempted = d.get("attempted") if isinstance(d.get("attempted"), list) else []
        # Keep attempted minimal and stable
        attempted_min = []
        for a in attempted:
            if not isinstance(a, dict):
                continue
            attempted_min.append({
                "url": str(a.get("url") or ""),
                "status": str(a.get("status") or a.get("fetch_status") or ""),
                "reason": str(a.get("reason") or a.get("fail_reason") or ""),
                "content_len": a.get("content_len"),
            })

        hash_inputs_norm = _inj_diag_norm_url_list(hash_inputs or [])
        rebuild_pool_norm = _inj_diag_norm_url_list(rebuild_pool or [])
        rebuild_selected_norm = _inj_diag_norm_url_list(rebuild_selected or [])

        # Deterministic deltas (set-based; small lists)
        def _delta(a, b):
            try:
                return sorted(list(set(a or []) - set(b or [])))[:100]
            except Exception:
                return []

        deltas = {
            "ui_minus_intake": _delta(ui_norm, intake_norm),
            "intake_minus_admitted": _delta(intake_norm, admitted_norm),
            "admitted_minus_attempted": _delta(admitted_norm, [x.get("url") for x in attempted_min if isinstance(x, dict)]),
            "attempted_minus_persisted": _delta([x.get("url") for x in attempted_min if isinstance(x, dict)], persisted_norm),
            "persisted_minus_hash_inputs": _delta(persisted_norm, hash_inputs_norm),
            "hash_inputs_minus_rebuild_pool": _delta(hash_inputs_norm, rebuild_pool_norm) if rebuild_pool is not None else [],
            "rebuild_pool_minus_selected": _delta(rebuild_pool_norm, rebuild_selected_norm) if rebuild_selected is not None else [],
        }


        # === PATCH EVO_INJ_ADMISSION_REASON_CODES_V1 START ===
        # Purpose: make evolution/analysis admission & selection drops explain themselves with stable reason codes.
        # Purely additive: diagnostics only (does not alter fastpath, hashing, scrape, or rebuild behavior).
        admission_rejection_reasons = {}
        attempted_rejection_reasons = {}
        try:
            # Prefer explicit per-URL decisions if present (from other EVO admission tracing patches)
            _decisions = d.get("inj_admission_decisions") or d.get("admission_decisions") or {}
            if isinstance(_decisions, dict):
                for _u, _v in _decisions.items():
                    if not _u:
                        continue
                    if isinstance(_v, dict):
                        _decision = str(_v.get("decision") or "")
                        _reason = str(_v.get("reason_code") or _v.get("reason") or "")
                    else:
                        _decision = str(_v or "")
                        _reason = ""
                    if _decision.lower().startswith("reject"):
                        admission_rejection_reasons[str(_u)] = _reason or "rejected_by_merge"
        except Exception:
            pass

        # Heuristic reason coding for intake→admitted drops
        for _u in (deltas.get("intake_minus_admitted") or []):
            if not _u:
                continue
            if _u in admission_rejection_reasons:
                continue
            _rsn = ""
            try:
                if not str(_u).startswith(("http://", "https://")):
                    _rsn = "invalid_scheme"
                elif str(stage) == "evolution" and str(path).startswith(("fastpath", "fastpath_replay")):
                    # In fastpath/replay, extra URLs may be visible but not admitted into the scrape/hash universe by policy.
                    _rsn = "fastpath_replay_no_admission"
                else:
                    _rsn = "unknown_rejected_pre_admission"
            except Exception:
                _rsn = "unknown_rejected_pre_admission"
            admission_rejection_reasons[str(_u)] = _rsn

        # Heuristic reason coding for admitted→attempted drops
        _attempted_urls = [x.get("url") for x in attempted_min if isinstance(x, dict) and x.get("url")]
        for _u in (deltas.get("admitted_minus_attempted") or []):
            if not _u:
                continue
            if str(stage) == "evolution" and str(path).startswith(("fastpath", "fastpath_replay")):
                attempted_rejection_reasons[str(_u)] = "fastpath_replay_no_fetch"
            else:
                attempted_rejection_reasons[str(_u)] = "not_fetched_or_filtered_before_fetch"

        # Policy/context snapshot (small + stable)
        try:
            import os as _os
            policy = {
                "exclude_injected_from_hash_env": str(_os.getenv("YUREEKA_EXCLUDE_INJECTED_URLS_FROM_SNAPSHOT_HASH") or ""),
                "force_include_injected_in_hash_env": str(_os.getenv("YUREEKA_INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH") or ""),
                "evolution_calls_fetch_web_context": d.get("evolution_calls_fetch_web_context"),
                "evolution_fastpath_allows_injection": False,
            }
        except Exception:
            policy = {}
        # === PATCH EVO_INJ_ADMISSION_REASON_CODES_V1 END ===
        return {
            "run_id": str(d.get("run_id") or ""),
            "stage": str(stage or ""),
            "path": str(path or ""),
            "ui_raw": ui_raw,
            "ui_norm": ui_norm,
            "intake_norm": intake_norm,
            "admitted_norm": admitted_norm,
            "attempted": attempted_min,
            "persisted_norm": persisted_norm,
            "hash_inputs_norm": hash_inputs_norm,
            "rebuild_pool_norm": rebuild_pool_norm if rebuild_pool is not None else None,
            "rebuild_selected_norm": rebuild_selected_norm if rebuild_selected is not None else None,
            "counts": {
                "ui_norm": int(len(ui_norm)),
                "intake_norm": int(len(intake_norm)),
                "admitted_norm": int(len(admitted_norm)),
                "attempted": int(len(attempted_min)),
                "persisted_norm": int(len(persisted_norm)),
                "hash_inputs_norm": int(len(hash_inputs_norm)),
                "rebuild_pool_norm": int(len(rebuild_pool_norm)) if rebuild_pool is not None else None,
                "rebuild_selected_norm": int(len(rebuild_selected_norm)) if rebuild_selected is not None else None,
            },
            "set_hashes": {
                "ui_norm": _inj_diag_set_hash(ui_norm),
                "intake_norm": _inj_diag_set_hash(intake_norm),
                "admitted_norm": _inj_diag_set_hash(admitted_norm),
                "persisted_norm": _inj_diag_set_hash(persisted_norm),
                "hash_inputs_norm": _inj_diag_set_hash(hash_inputs_norm),
                "rebuild_pool_norm": _inj_diag_set_hash(rebuild_pool_norm) if rebuild_pool is not None else "",
                "rebuild_selected_norm": _inj_diag_set_hash(rebuild_selected_norm) if rebuild_selected is not None else "",
            },
            "deltas": deltas,
            "rejection_reasons": {
                "intake_minus_admitted": admission_rejection_reasons,
                "admitted_minus_attempted": attempted_rejection_reasons,
            },
            "policy": policy,
        }
    except Exception:
        return {"stage": str(stage or ""), "path": str(path or ""), "error": "inj_trace_build_failed"}
# =====================================================================
# PATCH INJ_TRACE_V1_ENRICH_FROM_ARTIFACTS (ADDITIVE)
# Purpose:
# - Populate inj_trace_v1 attempted/persisted fields from *real* artifacts when
#   the upstream diag_injected_urls payload is partial (common in baseline/no-injection
#   or fastpath replay scenarios).
# - Pure diagnostics only: does NOT alter control flow, hashing, scraping, or selection.
#
# Artifacts supported:
#   - baseline_sources_cache (BSC): list of per-url snapshot dicts
#   - scraped_meta: dict keyed by url with status/status_detail/clean_text_len
# =====================================================================

def _inj_trace_v1_enrich_diag_from_bsc(diag: dict, baseline_sources_cache: list) -> dict:
    """Add attempted/persisted evidence into diag_injected_urls from baseline_sources_cache."""
    try:
        d = diag if isinstance(diag, dict) else {}
        bsc = baseline_sources_cache if isinstance(baseline_sources_cache, list) else []
        # If attempted already present, do not overwrite (avoid clobbering richer traces).
        if not isinstance(d.get("attempted"), list) or not d.get("attempted"):
            attempted = []
            for row in bsc:
                if not isinstance(row, dict):
                    continue
                u = str(row.get("url") or row.get("source_url") or "").strip()
                if not u:
                    continue
                st = str(row.get("status") or row.get("fetch_status") or "").strip() or "unknown"
                rs = str(row.get("status_detail") or row.get("fail_reason") or "").strip()
                clen = row.get("clean_text_len") or row.get("content_len") or 0
                try:
                    clen_i = int(clen)
                except Exception:
                    clen_i = 0
                attempted.append({"url": u, "status": st, "reason": rs, "content_len": clen_i})
            if attempted:
                d["attempted"] = attempted

        # Persisted: if missing, derive from successful snapshot rows in BSC.
        if not isinstance(d.get("persisted"), (list, str)) or not d.get("persisted"):
            persisted = []
            for row in bsc:
                if not isinstance(row, dict):
                    continue
                u = str(row.get("url") or row.get("source_url") or "").strip()
                if not u:
                    continue
                st = str(row.get("status") or row.get("fetch_status") or "").lower().strip()
                if st in ("success", "ok", "fetched"):
                    persisted.append(u)
            if persisted:
                d["persisted"] = _inj_diag_norm_url_list(persisted)

        return d
    except Exception:
        return diag if isinstance(diag, dict) else {}

def _inj_trace_v1_enrich_diag_from_scraped_meta(diag: dict, scraped_meta: dict, extra_urls: list) -> dict:
    """Add attempted/persisted evidence into diag_injected_urls from scraped_meta (evolution-side)."""
    try:
        d = diag if isinstance(diag, dict) else {}
        sm = scraped_meta if isinstance(scraped_meta, dict) else {}
        xs = _inj_diag_norm_url_list(extra_urls or [])
        if not xs:
            return d

        # attempted rows for injected urls
        if not isinstance(d.get("attempted"), list) or not d.get("attempted"):
            attempted = []
            for u in xs:
                m = sm.get(u) if isinstance(sm.get(u), dict) else {}
                st = str(m.get("status") or m.get("fetch_status") or "").strip() or "unknown"
                rs = str(m.get("status_detail") or m.get("fail_reason") or "").strip()
                clen = m.get("clean_text_len") or m.get("content_len") or 0
                try:
                    clen_i = int(clen)
                except Exception:
                    clen_i = 0
                attempted.append({"url": u, "status": st, "reason": rs, "content_len": clen_i})
            d["attempted"] = attempted

        # persisted success urls (only for injected)
        if not isinstance(d.get("persisted"), (list, str)) or not d.get("persisted"):
            persisted = []
            for a in (d.get("attempted") or []):
                if not isinstance(a, dict):
                    continue
                st = str(a.get("status") or "").lower().strip()
                if st in ("success", "ok", "fetched"):
                    u = str(a.get("url") or "").strip()
                    if u:
                        persisted.append(u)
            d["persisted"] = _inj_diag_norm_url_list(persisted)

        return d
    except Exception:
        return diag if isinstance(diag, dict) else {}

# =====================================================================
# PATCH DIAG17 (ADDITIVE): derive injection attempted/persisted/failed from actual artifacts
# Goal:
# - Prevent stale/early-stage diagnostic lists (e.g., admitted_minus_attempted) from contradicting
#   final artifact state (source_results / baseline_sources_cache).
# - Diagnostics only: does NOT affect fastpath, hashing, scraping, or metric selection.
#
# Emitted at:
#   output.debug.inj_trace_diag17
#   output.results.debug.inj_trace_diag17
# =====================================================================
def _inj_trace_diag17_from_artifacts(extra_urls: list, source_results: Any, baseline_sources_cache: Any) -> dict:
    try:
        xs = _inj_diag_norm_url_list(extra_urls or [])
        if not xs:
            return {"method": "artifact_state_scan", "note": "no_extra_urls"}

        # Normalize source_results into a url->status/status_detail map
        sr_map = {}
        try:
            if isinstance(source_results, dict):
                # allow dict keyed by url
                for k, v in source_results.items():
                    u = str(k or "").strip()
                    if not u:
                        continue
                    vv = v if isinstance(v, dict) else {}
                    sr_map[u] = {
                        "status": str(vv.get("status") or vv.get("fetch_status") or "").strip(),
                        "status_detail": str(vv.get("status_detail") or vv.get("fail_reason") or "").strip(),
                    }
            elif isinstance(source_results, list):
                for row in source_results:
                    if not isinstance(row, dict):
                        continue
                    u = str(row.get("url") or row.get("source_url") or "").strip()
                    if not u:
                        continue
                    sr_map[u] = {
                        "status": str(row.get("status") or row.get("fetch_status") or "").strip(),
                        "status_detail": str(row.get("status_detail") or row.get("fail_reason") or "").strip(),
                    }
        except Exception:
            sr_map = {}

        # Normalize baseline_sources_cache into url->(status, extracted_numbers_count)
        bsc_map = {}
        try:
            bsc = baseline_sources_cache if isinstance(baseline_sources_cache, list) else []
            for row in bsc:
                if not isinstance(row, dict):
                    continue
                u = str(row.get("url") or row.get("source_url") or "").strip()
                if not u:
                    continue
                st = str(row.get("status") or row.get("fetch_status") or "").strip()
                nums = row.get("extracted_numbers") or []
                try:
                    ncnt = int(len(nums)) if isinstance(nums, list) else 0
                except Exception:
                    ncnt = 0
                bsc_map[u] = {"status": st, "numbers_count": ncnt}
        except Exception:
            bsc_map = {}

        attempted = []
        persisted = []
        failed = []
        missing = []

        def _is_success_status(s: str) -> bool:
            s2 = (s or "").lower().strip()
            return s2 in ("success", "ok", "fetched")

        def _is_fail_status(s: str) -> bool:
            s2 = (s or "").lower().strip()
            if not s2:
                return False
            return any(tok in s2 for tok in ("fail", "error", "blocked", "timeout"))

        for u in xs:
            # attempted if present in either artifact set
            in_sr = u in sr_map
            in_bsc = u in bsc_map
            if not (in_sr or in_bsc):
                missing.append(u)
                continue

            attempted.append(u)

            st_sr = sr_map.get(u, {}).get("status", "")
            st_bsc = bsc_map.get(u, {}).get("status", "")
            nums_cnt = bsc_map.get(u, {}).get("numbers_count", 0)

            # persisted if:
            # - BSC indicates fetched/success OR
            # - extracted_numbers exist
            if _is_success_status(st_bsc) or nums_cnt > 0 or _is_success_status(st_sr):
                persisted.append(u)
            else:
                # failed if SR/BSC looks failed OR explicit non-success and no numbers
                if _is_fail_status(st_sr) or _is_fail_status(st_bsc) or ((st_sr or st_bsc) and not (_is_success_status(st_sr) or _is_success_status(st_bsc)) and nums_cnt == 0):
                    failed.append(u)

        out = {
            "method": "artifact_state_scan",
            "extra_urls_norm": xs,
            "attempted_norm": sorted(set(attempted)),
            "persisted_norm": sorted(set(persisted)),
            "failed_norm": sorted(set(failed)),
            "missing_norm": sorted(set(missing)),
            "counts": {
                "extra_urls_norm": int(len(xs)),
                "attempted_norm": int(len(set(attempted))),
                "persisted_norm": int(len(set(persisted))),
                "failed_norm": int(len(set(failed))),
                "missing_norm": int(len(set(missing))),
            },
            "notes": {
                "attempted_definition": "url present in source_results or baseline_sources_cache",
                "persisted_definition": "success/fetched status OR extracted_numbers present",
                "failed_definition": "fail-like status OR non-success with zero extracted_numbers",
            }
        }
        return out
    except Exception:
        return {"method": "artifact_state_scan", "error": "diag17_failed"}
# =====================================================================

# =====================================================================

# =====================================================================

# =====================================================================

def scrape_url(url: str) -> Optional[str]:
    """
    Scrape webpage content.

    Priority:
      1) ScrapingDog (if SCRAPINGDOG_KEY is present)
      2) Safe fallback: direct requests + BeautifulSoup visible-text extraction

    Returns:
      - Clean visible text (<= 3000 chars) or None
    """
    import re

    url_s = (url or "").strip()
    if not url_s:
        return None

    def _clean_html_to_text(html: str) -> str:
        try:
            from bs4 import BeautifulSoup  # type: ignore
            soup = BeautifulSoup(html or "", "html.parser")
            for tag in soup(["script", "style", "nav", "footer", "header", "aside", "noscript", "form"]):
                try:
                    tag.decompose()
                except Exception:
                    pass
            txt = soup.get_text(separator="\n")
        except Exception:
            # fallback: strip tags
            txt = re.sub(r"(?is)<(script|style|noscript).*?>.*?</\1>", " ", html or "")
            txt = re.sub(r"(?is)<[^>]+>", " ", txt)
        # normalize whitespace
        lines = [ln.strip() for ln in (txt or "").splitlines() if ln.strip()]
        out = "\n".join(lines)
        out = re.sub(r"\n{3,}", "\n\n", out)
        return out.strip()

    def _direct_fetch(u: str) -> Optional[str]:
        try:
            headers = {
                "User-Agent": (
                    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/120.0.0.0 Safari/537.36"
                ),
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9",
                "Cache-Control": "no-cache",
                "Pragma": "no-cache",
            }
            resp = requests.get(u, headers=headers, timeout=12, allow_redirects=True)
            if resp.status_code >= 400:
                return None

            ctype = (resp.headers.get("Content-Type") or "").lower()
            if "application/pdf" in ctype:
                return None

            cleaned = _clean_html_to_text(resp.text or "")
            cleaned = cleaned.strip()
            if not cleaned:
                return None
            return cleaned[:3000]
        except Exception:
            return None

    # 1) ScrapingDog path (if configured)
    if globals().get("SCRAPINGDOG_KEY"):
        try:
            params = {"api_key": SCRAPINGDOG_KEY, "url": url_s, "dynamic": "false"}
            resp = requests.get("https://api.scrapingdog.com/scrape", params=params, timeout=15)
            if resp.status_code < 400:
                cleaned = _clean_html_to_text(resp.text or "").strip()
                if cleaned:
                    return cleaned[:3000]
        except Exception:
            pass  # fall through to direct fetch

    # 2) Safe fallback
    return _direct_fetch(url_s)


def fetch_web_context(
    query: str,
    num_sources: int = 3,
    *,
    fallback_mode: bool = False,
    fallback_urls: list = None,
    existing_snapshots: Any = None,   # <-- ADDITIVE
    # ============================================================
    # PATCH FWC_EXTRA_URLS1 (ADDITIVE)
    # ============================================================
    extra_urls: Any = None,
    # ============================================================
    # PATCH INJ_DIAG_FWC_ARGS (ADDITIVE): correlation + UI raw
    # ============================================================
    diag_run_id: str = "",
    diag_extra_urls_ui_raw: Any = None,
    # ============================================================
    # PATCH FWC_IDENTITY_ONLY1 (ADDITIVE): admission-only mode (no scraping)
    # ============================================================
    identity_only: bool = False,
    # ============================================================
    # PATCH FIX41AFC8 (ADDITIVE): force scrape extra_urls even if not admitted
    # - Default False: no behavior change.
    # - When True: normalized extra URLs will be appended to admitted list for scraping.
    # ============================================================
    force_scrape_extra_urls: bool = False,
    # ============================================================
    # PATCH FIX41AFC13 (ADDITIVE): force admit extra_urls into admitted list (pre-admission override)
    # - Default False: no behavior change.
    # - When True: normalized extra URLs will be appended to admitted list (not just scrape list),
    #   enabling deterministic admission of injected URLs when delta exists.
    # ============================================================
    force_admit_extra_urls: bool = False,
) -> dict:

    """
    Web context collector used by BOTH analysis + evolution.

    Enhancements:
    - Dashboard telemetry (sources found / HQ / admitted / scraped / success)
    - Keeps snapshot-friendly scraped_meta (fingerprint + extracted_numbers + numbers_found)
    - Uses scrape_url() which now has ScrapingDog + safe fallback scraper
    - Restores legacy contract: web_context["sources"] AND ["web_sources"]
    """
    import re
    from datetime import datetime, timezone

    def _now_iso() -> str:
        return datetime.now(timezone.utc).isoformat()

    def _is_probably_url(s: str) -> bool:
        if not s or not isinstance(s, str):
            return False
        t = s.strip()
        if " " in t:
            return False
        if re.match(r"^https?://", t, flags=re.I):
            return True
        if re.match(r"^[a-z0-9.-]+\.[a-z]{2,}(/.*)?$", t, flags=re.I):
            return True
        return False

    def _normalize_url(s: str) -> str:
        t = (s or "").strip()
        if not t:
            return ""
        if re.match(r"^https?://", t, flags=re.I):
            return t
        if re.match(r"^[a-z0-9.-]+\.[a-z]{2,}(/.*)?$", t, flags=re.I):
            return "https://" + t
        return ""


    out = {
        "query": query,
        "sources": [],        # ✅ legacy key many downstream blocks expect
        "web_sources": [],    # ✅ newer key used by evolution/snapshots
        "search_results": [],
        "scraped_meta": {},
        "scraped_content": {},
        "errors": [],
        "status": "ok",
        "status_detail": "",
        "fetched_at": _now_iso(),
        "debug_counts": {},   # ✅ telemetry for dashboard + JSON debugging
    }

    # ---- ADDITIVE: snapshot reuse lookup (Change #3) ----
    snap_lookup = {}
    if isinstance(existing_snapshots, dict):
        snap_lookup = existing_snapshots
    elif isinstance(existing_snapshots, list):
        for s in existing_snapshots:
            if isinstance(s, dict) and s.get("url"):
                snap_lookup[str(s.get("url")).strip()] = s

    extractor_fp = get_extractor_fingerprint()
    # ----------------------------------------------------


    q = (query or "").strip()
    if not q:
        out["status"] = "no_query"
        out["status_detail"] = "empty_query"
        return out

    # -----------------------------
    # 1) Search (SerpAPI) OR fallback_urls
    # -----------------------------
    search_results = []
    urls_raw = []

    if not fallback_mode:
        try:
            sr = search_serpapi(q, num_results=10) or []
            if isinstance(sr, list):
                search_results = sr
        except Exception as e:
            out["errors"].append(f"search_failed:{type(e).__name__}")
            search_results = []

        out["search_results"] = search_results

        # Extract urls from results
        for r in (search_results or []):
            if isinstance(r, dict):
                u = (r.get("link") or r.get("url") or "").strip()
                if _is_probably_url(u):
                    urls_raw.append(u)
            elif isinstance(r, str):
                if _is_probably_url(r):
                    urls_raw.append(r.strip())

    else:
        # Evolution fallback: use provided URLs
        if isinstance(fallback_urls, list):
            for u in fallback_urls:
                if isinstance(u, str) and _is_probably_url(u.strip()):
                    urls_raw.append(u.strip())

    # -----------------------------
    # 2) Compute "HQ" counts (like old version)
    # -----------------------------
    total_found = len(search_results) if not fallback_mode else len(urls_raw)
    hq_count = 0

    try:
        fn_rel = globals().get("classify_source_reliability")
        if callable(fn_rel) and not fallback_mode:
            for r in (search_results or []):
                if not isinstance(r, dict):
                    continue
                u = (r.get("link") or "").strip()
                if not u:
                    continue
                label = fn_rel(u) or ""
                if "✅" in str(label):
                    hq_count += 1
    except Exception:
        hq_count = 0

    # -----------------------------
    # 3) Sanitize + normalize + dedupe
    # -----------------------------
    normed = []
    seen = set()
    for u in (urls_raw or []):
        nu = _normalize_url(u)
        if not nu:
            continue
        if nu in seen:
            continue
        seen.add(nu)
        normed.append(nu)

    # admitted for scraping (top N)
    try:
        n = int(num_sources or 3)
    except Exception:
        n = 3
    n = max(1, min(12, n))
    admitted = normed[:n] if not fallback_mode else normed  # fallback_mode typically wants all

    # =====================================================================
    # PATCH FIX41AFC8 (ADDITIVE): Force-scrape normalized extra URLs even if admission filters drop them
    #
    # Why:
    # - In evolution injection scenarios, extra URLs may be deliberately outside the normal
    #   admitted universe (domain allowlists, heuristics, etc.), but the user's intent is
    #   to attempt a fetch so the run can either persist a snapshot or fail with a concrete reason.
    #
    # Behavior:
    # - When force_scrape_extra_urls=True and normalized extras exist, append them into the
    #   admitted list (deduped, stable order) so downstream scraping attempts occur.
    #
    # Safety:
    # - Default is False (no change for normal runs).
    # - Never raises.
    # =====================================================================
    try:
        if bool(force_scrape_extra_urls):
            _fx8_extras = []
            if "_extras" in locals() and isinstance(_extras, list):
                _fx8_extras = [u for u in _extras if isinstance(u, str) and u.strip()]
            if _fx8_extras and isinstance(admitted, list):
                _seen = set([u for u in admitted if isinstance(u, str)])
                for _u in _fx8_extras:
                    if _u not in _seen:
                        admitted.append(_u)
                        _seen.add(_u)
                # breadcrumb for diagnostics
                try:
                    out.setdefault("debug_counts", {})
                    if isinstance(out.get("debug_counts"), dict):
                        out["debug_counts"].setdefault("fix41afc8", {})
                        if isinstance(out["debug_counts"].get("fix41afc8"), dict):
                            out["debug_counts"]["fix41afc8"].update({
                                "force_scrape_extra_urls": True,
                                "force_scrape_extra_urls_count": int(len(_fx8_extras)),
                            })
                except Exception:
                    pass
    except Exception:
        pass


    # ============================================================
    # PATCH FWC_EXTRA_URLS2 (ADDITIVE)
    # ============================================================
    try:
        _extras_in = extra_urls or []
        _extras = []
        _canon_map = {}
        if isinstance(_extras_in, str):
            _extras_in = [u.strip() for u in _extras_in.splitlines()]
        if isinstance(_extras_in, (list, tuple)):
            for u in _extras_in:
                u = str(u or "").strip()
                if not u:
                    continue
                if not (u.startswith("http://") or u.startswith("https://")):
                    continue
                _canon = _canonicalize_injected_url(u) or u
                _extras.append(_canon)
                try:
                    _canon_map[u] = _canon
                except Exception:
                    pass
        _seen = set()
        merged = []
        for u in _extras + (admitted or []):
            if u in _seen:
                continue
            _seen.add(u)
            merged.append(u)
        admitted = merged
        out.setdefault("debug", {})
        if isinstance(out.get("debug"), dict):
            out["debug"].setdefault("fwc_extra_urls", {})
            out["debug"]["fwc_extra_urls"]["extra_urls_count"] = int(len(_extras))
            out["debug"]["fwc_extra_urls"]["admitted_count_after_merge"] = int(len(admitted or []))
            out["debug"]["fwc_extra_urls"]["extra_urls"] = _extras[:20]
    except Exception:
        pass


    # =====================================================================
    # PATCH INJ_DIAG_FWC_STAGE (ADDITIVE): injected-URL stage checkpoints (A1-A3)
    # Records: UI->intake->admitted, and later enriches with scrape outcomes.
    # =====================================================================
    try:
        _diag_run = str(diag_run_id or "") or _inj_diag_make_run_id("analysis")
        out["diag_run_id"] = out.get("diag_run_id") or _diag_run

        _ui_raw = diag_extra_urls_ui_raw if diag_extra_urls_ui_raw is not None else extra_urls
        _ui_norm = _inj_diag_norm_url_list(_ui_raw)
        _intake_norm = list(_extras or []) if "_extras" in locals() and isinstance(_extras, list) else _inj_diag_norm_url_list(extra_urls)

        out["diag_injected_urls"] = {
            "run_id": _diag_run,
            "ui_raw": _ui_raw if isinstance(_ui_raw, (str, list, tuple)) else str(_ui_raw or ""),
            "ui_norm": _ui_norm,
            "intake_norm": _intake_norm,
            "admitted": list(admitted or []),
            "attempted": [],
            "persisted": [],
            "hash_inputs": [],
            "rebuild_pool": [],
            "rebuild_selected": [],
            "set_hashes": {
                "ui_norm": _inj_diag_set_hash(_ui_norm),
                "intake_norm": _inj_diag_set_hash(_intake_norm),
                "admitted": _inj_diag_set_hash(list(admitted or [])),
            },
            "canon_map": dict(_canon_map) if "_canon_map" in locals() else {},
            "deltas": {
                "ui_minus_intake": sorted(list(set(_ui_norm) - set(_intake_norm))),
                "intake_minus_admitted": sorted(list(set(_intake_norm) - set(list(admitted or [])))),
            },
        }
    except Exception:
        pass
    # =====================================================================


    # =====================================================================
    # PATCH FIX41AFC13 (ADDITIVE): Pre-admission override for extra_urls (injection lane)
    #
    # Goal:
    # - When force_admit_extra_urls is True, ensure normalized extra_urls are INCLUDED in the
    #   admitted list itself (not only the scrape list). This prevents injected URLs from dying
    #   at "intake_minus_admitted" and allows deterministic fetch/persist behavior.
    #
    # Safety:
    # - Default flag False => no behavior change.
    # - Never raises.
    # =====================================================================
    try:
        if force_admit_extra_urls:
            _fix41afc13_extra = _inj_diag_norm_url_list(extra_urls) if extra_urls else []
            if _fix41afc13_extra:
                _fix41afc13_before = list(admitted or [])
                _fix41afc13_set = set(_inj_diag_norm_url_list(_fix41afc13_before))
                _fix41afc13_added = []
                for _u in _fix41afc13_extra:
                    if _u and _u not in _fix41afc13_set:
                        _fix41afc13_before.append(_u)
                        _fix41afc13_set.add(_u)
                        _fix41afc13_added.append(_u)
                if _fix41afc13_added:
                    admitted = _fix41afc13_before
                    out.setdefault("debug_counts", {})
                    if isinstance(out.get("debug_counts"), dict):
                        out["debug_counts"].update({
                            "forced_admit_extra_urls_count": int(len(_fix41afc13_added)),
                        })
                    out.setdefault("debug", {})
                    if isinstance(out.get("debug"), dict):
                        out["debug"].setdefault("fix41afc13", {})
                        if isinstance(out["debug"].get("fix41afc13"), dict):
                            out["debug"]["fix41afc13"].update({
                                "forced_admit_applied": True,
                                "forced_admit_added": list(_fix41afc13_added),
                                "forced_admit_total_extra": int(len(_fix41afc13_extra)),
                            })
    except Exception:
        pass
    # =====================================================================

    out["sources"] = admitted
    out["web_sources"] = admitted

    # Telemetry before scrape
    out["debug_counts"].update({
        "total_found": int(total_found),
        "high_quality": int(hq_count),
        "admitted_for_scraping": int(len(admitted)),
        "fallback_mode": bool(fallback_mode),
    })

    # Dashboard info (restored)
    try:
        if not fallback_mode:
            st.info(
                f"🔍 Sources Found: **{out['debug_counts']['total_found']} total** | "
                f"**{out['debug_counts']['high_quality']} high-quality** | "
                f"Scraping **{out['debug_counts']['admitted_for_scraping']}**"
            )
        else:
            st.info(
                f"🧩 Fallback Sources: **{out['debug_counts']['admitted_for_scraping']}** (no SerpAPI search)"
            )
    except Exception:
        pass

    if not admitted:
        out["status"] = "no_sources"
        out["status_detail"] = "empty_sources_after_filter"
        return out

    # ============================================================
    # PATCH FWC_IDENTITY_ONLY2 (ADDITIVE): identity-only early return
    # ============================================================
    try:
        if bool(identity_only):
            out["status"] = out.get("status") or "ok"
            out["status_detail"] = out.get("status_detail") or "identity_only"
            return out
    except Exception:
        pass




    # -----------------------------
    # 4) Scrape + extract numbers (snapshot-friendly scraped_meta)
    # -----------------------------
    fn_fp = globals().get("fingerprint_text")
    fn_extract = globals().get("extract_numbers_with_context") or globals().get("extract_numeric_candidates") or globals().get("extract_numbers_from_text")

    scraped_attempted = 0
    scraped_ok_text = 0
    scraped_ok_numbers = 0
    scraped_failed = 0

    # optional progress bar
    progress = None
    try:
        progress = st.progress(0)
    except Exception:
        progress = None

    for i, url in enumerate(admitted):
        scraped_attempted += 1

        meta = {
            "url": url,
            "fetched_at": _now_iso(),
            "status": "failed",
            "status_detail": "",
            "content_type": "",
            "content_len": 0,
            "clean_text_len": 0,
            "fingerprint": None,
            "numbers_found": 0,
            "extracted_numbers": [],
            "content": "",
            "clean_text": "",
        }

        try:
            text = scrape_url(url)  # ✅ ScrapingDog + fallback inside scrape_url
            if not text or not str(text).strip():
                meta["status"] = "failed"
                meta["status_detail"] = "failed:no_text"
                scraped_failed += 1
                out["scraped_meta"][url] = meta
            else:
                cleaned = str(text).strip()
                meta["status"] = "fetched"
                meta["status_detail"] = "success"
                meta["content"] = cleaned
                meta["clean_text"] = cleaned
                meta["content_len"] = len(cleaned)
                meta["clean_text_len"] = len(cleaned)

                # fingerprint
                try:
                    if callable(fn_fp):
                        meta["fingerprint"] = fn_fp(cleaned)
                    else:
                        meta["fingerprint"] = fingerprint_text(cleaned) if callable(globals().get("fingerprint_text")) else None
                except Exception:
                    meta["fingerprint"] = None

                # ---- ADDITIVE: reuse extracted_numbers when unchanged (Change #3) ----
                meta["extractor_fingerprint"] = extractor_fp
                prev = snap_lookup.get(url) if isinstance(snap_lookup, dict) else None
                if isinstance(prev, dict):
                    if prev.get("fingerprint") == meta.get("fingerprint") and prev.get("extractor_fingerprint") == extractor_fp:
                        prev_nums = prev.get("extracted_numbers")
                        if isinstance(prev_nums, list) and prev_nums:
                            meta["extracted_numbers"] = prev_nums
                            meta["numbers_found"] = len(prev_nums)
                            meta["reused_snapshot"] = True

                            out["scraped_meta"][url] = meta
                            out["scraped_content"][url] = cleaned

                            scraped_ok_text += 1
                            if meta["numbers_found"] > 0:
                                scraped_ok_numbers += 1

                            if progress:
                                try:
                                    progress.progress((i + 1) / max(1, len(admitted)))
                                except Exception:
                                    pass

                            continue
                meta["reused_snapshot"] = False
                # ---------------------------------------------------------------

                # numeric extraction (analysis-aligned if fn exists)
                nums = []
                try:
                    if callable(fn_extract):
                        nums = fn_extract(cleaned, url=url) if "url" in fn_extract.__code__.co_varnames else fn_extract(cleaned)
                except Exception:
                    nums = []

                if isinstance(nums, list):
                    meta["extracted_numbers"] = nums
                    meta["numbers_found"] = len(nums)

                    # ---- ADDITIVE: stable IDs + ordering (Change #2 / Part 1) ----
                    urlv = meta.get("url") or url
                    fpv = meta.get("fingerprint") or ""

                    for n in (meta["extracted_numbers"] or []):
                        if isinstance(n, dict):
                            if "extracted_number_id" not in n:
                                n["extracted_number_id"] = make_extracted_number_id(urlv, fpv, n)
                            if not n.get("source_url"):
                                n["source_url"] = urlv

                    meta["extracted_numbers"] = sort_snapshot_numbers(meta["extracted_numbers"])
                    meta["numbers_found"] = len(meta["extracted_numbers"])
                    # --------------------------------------------------------------

                out["scraped_meta"][url] = meta
                out["scraped_content"][url] = cleaned

                scraped_ok_text += 1
                if meta["numbers_found"] > 0:
                    scraped_ok_numbers += 1

        except Exception as e:
            meta["status"] = "failed"
            meta["status_detail"] = f"failed:exception:{type(e).__name__}"
            scraped_failed += 1
            out["scraped_meta"][url] = meta
            out["errors"].append(meta["status_detail"])

        if progress:
            try:
                progress.progress((i + 1) / max(1, len(admitted)))
            except Exception:
                pass


    # =====================================================================
    # PATCH INJ_DIAG_FWC_POSTSCRAPE (ADDITIVE): finalize scrape outcomes (A3)
    # =====================================================================
    try:
        d = out.get("diag_injected_urls")
        if isinstance(d, dict):
            _inj = set(d.get("intake_norm") or [])
            sm = out.get("scraped_meta") or {}
            attempted = []
            persisted = []
            if isinstance(sm, dict):
                for u in sorted(_inj):
                    meta = sm.get(u) or {}
                    status = (meta.get("status") or "")
                    status_detail = (meta.get("status_detail") or "")
                    content = meta.get("clean_text") or meta.get("content") or ""
                    attempted.append({
                        "url": u,
                        "attempted": bool(u in (admitted or [])),
                        "fetch_status": "success" if (str(status_detail).startswith("success") or status == "fetched") else ("failed" if meta else "skipped"),
                        "fail_reason": (str(status_detail) or str(status) or "")[:80],
                        "content_len": int(len(content) if isinstance(content, str) else 0),
                        "numbers_found": int(meta.get("numbers_found") or 0),
                    })
                    if str(status_detail).startswith("success") or status == "fetched":
                        persisted.append(u)
            d["attempted"] = attempted
            d["persisted"] = persisted
            d.setdefault("set_hashes", {})
            if isinstance(d["set_hashes"], dict):
                d["set_hashes"]["persisted"] = _inj_diag_set_hash(persisted)
    except Exception:
        pass
    # =====================================================================

    out["debug_counts"].update({
        "scraped_attempted": int(scraped_attempted),
        "scraped_ok_text": int(scraped_ok_text),
        "scraped_ok_numbers": int(scraped_ok_numbers),
        "scraped_failed": int(scraped_failed),
    })

    # Dashboard scrape summary
    try:
        st.info(
            f"🧽 Scrape Results: **{out['debug_counts']['scraped_ok_text']} ok-text** | "
            f"**{out['debug_counts']['scraped_ok_numbers']} ok-numbers** | "
            f"**{out['debug_counts']['scraped_failed']} failed**"
        )
    except Exception:
        pass

    # status summarization

    # =====================================================================
    # PATCH FWC_EXTRA_URLS_TRACE2 (ADDITIVE): trace how injected URLs were handled
    # Why:
    # - When scenario B "extra URLs" are provided, it can be unclear whether they:
    #     (a) were normalized/deduped
    #     (b) were admitted into the scrape list
    #     (c) were successfully scraped
    #     (d) actually entered the snapshot-hash pool used by analysis/evolution
    # - This patch records a deterministic, non-invasive trace in web_context only.
    # =====================================================================
    try:
        if not isinstance(out.get("debug_counts"), dict):
            out["debug_counts"] = {}
        _dbg_counts = out["debug_counts"]

        _extra_trace = {
            "extra_urls_requested": list(extra_urls or []) if isinstance(extra_urls, list) else [],
            "extra_urls_normalized": list(_extras or []) if "_extras" in locals() and isinstance(_extras, list) else [],
            "extra_urls_admitted": [],
            "extra_urls_scraped": [],
            "extra_urls_in_hash_pool": [],
            "notes": [],
        }

        # Which extras actually made it into the final admitted URL list?
        try:
            _admitted_urls = []
            if "admitted" in locals() and isinstance(admitted, list):
                _admitted_urls = [u for u in admitted if isinstance(u, str) and u.strip()]
            _extra_set = set(_extra_trace["extra_urls_normalized"])
            _extra_trace["extra_urls_admitted"] = [u for u in _admitted_urls if u in _extra_set]
        except Exception:
            pass

        # How did each extra URL scrape?
        try:
            sm = out.get("scraped_meta") or {}
            if isinstance(sm, dict):
                for u in _extra_trace["extra_urls_normalized"]:
                    meta = sm.get(u) or {}
                    if isinstance(meta, dict) and meta:
                        content = meta.get("clean_text") or meta.get("content") or ""
                        fp = meta.get("fingerprint")
                        _extra_trace["extra_urls_scraped"].append({
                            "url": u,
                            "status": meta.get("status"),
                            "status_detail": meta.get("status_detail"),
                            "fingerprint": (fp[:16] if isinstance(fp, str) else fp),
                            "numbers_found": meta.get("numbers_found"),
                            "content_len": (len(content) if isinstance(content, str) else 0),
                            "content_type": meta.get("content_type") or "",
                        })
        except Exception:
            pass

        # Approximate "hash pool" membership (non-invasive):
        # we mark extras whose scrape produced a non-empty fingerprint + some text.
        try:
            sm = out.get("scraped_meta") or {}
            if isinstance(sm, dict):
                for row in (_extra_trace.get("extra_urls_scraped") or []):
                    u = row.get("url")
                    meta = sm.get(u) or {}
                    content = meta.get("clean_text") or meta.get("content") or ""
                    fp = meta.get("fingerprint")
                    if isinstance(fp, str) and fp and isinstance(content, str) and len(content) >= 200:
                        _extra_trace["extra_urls_in_hash_pool"].append(u)
        except Exception:
            pass

        out["extra_urls_debug"] = _extra_trace
        _dbg_counts["extra_urls_trace"] = {
            "requested": len(_extra_trace.get("extra_urls_requested") or []),
            "normalized": len(_extra_trace.get("extra_urls_normalized") or []),
            "admitted": len(_extra_trace.get("extra_urls_admitted") or []),
            "scraped": len(_extra_trace.get("extra_urls_scraped") or []),
            "in_hash_pool": len(_extra_trace.get("extra_urls_in_hash_pool") or []),
        }
    except Exception:
        pass
    # =====================================================================
    if scraped_ok_text == 0:
        out["status"] = "failed"
        out["status_detail"] = "no_usable_text"
    elif scraped_ok_numbers == 0:
        out["status"] = "partial"
        out["status_detail"] = "text_ok_numbers_empty"
    else:
        out["status"] = "success"
        out["status_detail"] = "ok"

    return out



def fingerprint_text(text: str) -> str:
    """Stable short fingerprint for fetched content (for debugging + determinism checks)."""
    if not text:
        return ""
    normalized = re.sub(r"\s+", " ", text.strip().lower())
    return hashlib.md5(normalized.encode("utf-8")).hexdigest()[:12]

def unit_clean_first_letter(unit: str) -> str:
    """Normalize units to first letter (T/B/M/K/%), ignoring $ and spaces."""
    if not unit:
        return ""
    u = unit.replace("$", "").replace(" ", "").strip().upper()
    return u[0] if u else ""

# =========================================================
# 7. LLM QUERY FUNCTIONS
# =========================================================

def query_perplexity(query: str, web_context: Dict, query_structure: Optional[Dict[str, Any]] = None) -> Optional[str]:
    """
    Query Perplexity and return a validated JSON string (LLMResponse-compatible).
    Removes 'action' and excludes None fields from output JSON.
    """
    if not PERPLEXITY_KEY:
        st.error("❌ PERPLEXITY_KEY not set.")
        return None

    query_structure = query_structure or {}
    structure_txt = ""
    ordering_contract = ""

    try:
        structure_txt, ordering_contract = build_query_structure_prompt(query_structure)
    except Exception:
        structure_txt = ""
        ordering_contract = ""

    # Web context: show top sources + snippets
    sources = (web_context.get("sources", []) if isinstance(web_context, dict) else []) or []
    search_results = (web_context.get("search_results", []) if isinstance(web_context, dict) else []) or []
    search_count = int(web_context.get("search_count", len(search_results)) if isinstance(web_context, dict) else 0)

    context_section = "WEB CONTEXT:\n"
    for url in sources[:6]:
        content = (web_context.get("scraped_content", {}) or {}).get(url) if isinstance(web_context, dict) else None
        if content:
            context_section += f"\n{url}:\n{str(content)[:800]}...\n"
        else:
            context_section += f"\n{url}\n"

    enhanced_query = (
        f"{context_section}\n"
        f"{SYSTEM_PROMPT}\n\n"
        f"User Question: {query}\n\n"
        f"{structure_txt}\n\n"
        f"{ordering_contract}\n"
        f"Web search returned {search_count} results.\n"
        f"Return ONLY valid JSON matching the template and include all required fields."
    )

    headers = {
        "Authorization": f"Bearer {PERPLEXITY_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "sonar",
        "temperature": 0.0,
        "max_tokens": 2400,
        "top_p": 1.0,
        "messages": [{"role": "user", "content": enhanced_query}]
    }

    try:
        resp = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=45)
        resp.raise_for_status()
        data = resp.json()

        if "choices" not in data:
            raise Exception("No 'choices' in Perplexity response")

        content = data["choices"][0]["message"]["content"]
        if not content or not content.strip():
            raise Exception("Empty Perplexity response")

        parsed = parse_json_safely(content, "Perplexity")
        if not parsed:
            return create_fallback_response(query, search_count, web_context)

        repaired = repair_llm_response(parsed)

        # Ensure action is removed even if present
        repaired.pop("action", None)

        validate_numeric_fields(repaired, "Perplexity")

        try:
            llm_obj = LLMResponse.model_validate(repaired)

            # Ensure action not present (belt + suspenders)
            if hasattr(llm_obj, "action"):
                llm_obj.action = None

            # Merge web sources
            if isinstance(web_context, dict) and web_context.get("sources"):
                existing = llm_obj.sources or []
                merged = list(dict.fromkeys(existing + web_context["sources"]))
                llm_obj.sources = merged[:10]
                llm_obj.freshness = "Current (web-enhanced)"

            result = llm_obj.model_dump_json(exclude_none=True)
            cache_llm_response(query, web_context, result)
            return result

        except ValidationError as e:
            st.warning(f"⚠️ Pydantic validation failed: {e}")
            return create_fallback_response(query, search_count, web_context)

    except Exception as e:
        st.error(f"❌ Perplexity API error: {e}")
        return create_fallback_response(query, search_count, web_context)


def query_perplexity_raw(prompt: str, max_tokens: int = 400, timeout: int = 30) -> str:
    """
    Raw Perplexity call that returns text only.
    IMPORTANT: Does NOT attempt to validate as LLMResponse.
    """
    headers = {
        "Authorization": f"Bearer {PERPLEXITY_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "sonar",
        "temperature": 0.0,
        "top_p": 1.0,
        "max_tokens": max_tokens,
        "messages": [{"role": "user", "content": prompt}],
    }

    resp = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=timeout)
    resp.raise_for_status()
    data = resp.json()
    return (data.get("choices", [{}])[0].get("message", {}) or {}).get("content", "") or ""

def create_fallback_response(query: str, search_count: int, web_context: Dict) -> str:
    """Create fallback response matching schema, excluding None fields and removing action."""
    fallback = LLMResponse(
        executive_summary=f"Analysis of '{query}' completed with {search_count} web sources. Schema validation used fallback structure.",
        primary_metrics={
            "sources": MetricDetail(name="Web Sources", value=search_count, unit="sources"),
            "quality": MetricDetail(name="Data Quality", value=70, unit="%")
        },
        key_findings=[
            f"Web search found {search_count} relevant sources.",
            "Primary model output required fallback due to format issues.",
            "Manual review of raw data recommended for accuracy."
        ],
        top_entities=[
            TopEntityDetail(name="Source 1", share="N/A", growth="N/A")
        ],
        trends_forecast=[
            TrendForecastDetail(trend="Schema validation used fallback", direction="⚠️", timeline="Now")
        ],
        visualization_data=VisualizationData(
            chart_labels=["Attempt"],
            chart_values=[search_count],
            chart_title="Search Results"
        ),
        sources=web_context.get("sources", []),
        confidence=60,
        freshness="Current (fallback)",
        action=None
    )

    return fallback.model_dump_json(exclude_none=True)


# =========================================================
# 7B. ANCHORED EVOLUTION QUERY
# =========================================================

def _ensure_metric_labels(metric_changes: list) -> list:
    """
    Backward/forward compatible label normalization:
    - guarantees a non-empty display label
    - adds aliases so different UIs render correctly: metric_name, metric, label
    """
    import re

    def _prettify(s: str) -> str:
        s = str(s or "").strip()
        if not s:
            return ""
        s = s.replace("__", " ").replace("_", " ")
        s = re.sub(r"\s+", " ", s).strip()
        return s[:120]

    out = []
    for row in (metric_changes or []):
        if not isinstance(row, dict):
            continue

        name = row.get("name")
        if isinstance(name, str):
            name = name.strip()
        else:
            name = ""

        # try to derive a label if name missing (canonical_key or metric_definition.name)
        if not name:
            md = row.get("metric_definition") if isinstance(row.get("metric_definition"), dict) else {}
            name = (md.get("name") or "").strip() if isinstance(md.get("name"), str) else ""
        if not name:
            ckey = row.get("canonical_key")
            name = _prettify(ckey) if ckey else "Unknown Metric"

        # write canonical label + aliases
        row["name"] = name
        row.setdefault("metric_name", name)
        row.setdefault("metric", name)
        row.setdefault("label", name)

        out.append(row)

    return out


def format_previous_metrics(metrics: Dict) -> str:
    """Format previous metrics for prompt"""
    if not metrics:
        return "No previous metrics available"

    lines = []
    for key, m in metrics.items():
        if isinstance(m, dict):
            lines.append(f"- {m.get('name', key)}: {m.get('value', 'N/A')} {m.get('unit', '')}")
    return "\n".join(lines) if lines else "No metrics"

def format_previous_entities(entities: List) -> str:
    """Format previous entities for prompt"""
    if not entities:
        return "No previous entities available"

    lines = []
    for i, e in enumerate(entities, 1):
        if isinstance(e, dict):
            lines.append(f"{i}. {e.get('name', 'Unknown')}: {e.get('share', 'N/A')} share, {e.get('growth', 'N/A')} growth")
    return "\n".join(lines) if lines else "No entities"

def format_previous_findings(findings: List) -> str:
    """Format previous findings for prompt"""
    if not findings:
        return "No previous findings available"

    lines = [f"- {f}" for f in findings if f]
    return "\n".join(lines) if lines else "No findings"

def calculate_time_ago(timestamp_str: str) -> str:
    """Calculate human-readable time difference"""
    try:
        prev_time = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
        delta = datetime.now() - prev_time.replace(tzinfo=None)

        hours = delta.total_seconds() / 3600
        if hours < 24:
            return f"{hours:.1f} hours ago"
        elif hours < 168:  # 7 days
            return f"{hours/24:.1f} days ago"
        elif hours < 720:  # 30 days
            return f"{hours/168:.1f} weeks ago"
        else:
            return f"{hours/720:.1f} months ago"
    except:
        return "unknown time ago"

def query_perplexity_anchored(query: str, previous_data: Dict, web_context: Dict, temperature: float = 0.1) -> str:
    """
    Query Perplexity with previous analysis as anchor.
    This produces an evolution-aware response that tracks changes.
    """

    prev_response = previous_data.get("primary_response", {})
    prev_timestamp = previous_data.get("timestamp", "")
    prev_question = previous_data.get("question", query)

    time_ago = calculate_time_ago(prev_timestamp)

    # Build the anchored prompt
    anchored_prompt = EVOLUTION_PROMPT_TEMPLATE.format(
        time_ago=time_ago,
        previous_question=prev_question,
        previous_timestamp=prev_timestamp,
        previous_summary=prev_response.get("executive_summary", "No previous summary"),
        previous_metrics=format_previous_metrics(prev_response.get("primary_metrics", {})),
        previous_entities=format_previous_entities(prev_response.get("top_entities", [])),
        previous_findings=format_previous_findings(prev_response.get("key_findings", [])),
        query=query
    )

    # Add web context if available
    if web_context.get("summary"):
        anchored_prompt = f"CURRENT WEB RESEARCH:\n{web_context['summary']}\n\n{anchored_prompt}"

    # API request
    headers = {
        "Authorization": f"Bearer {PERPLEXITY_KEY}",
        "Content-Type": "application/json"
    }


    payload = {
        "model": "sonar",
        "temperature": 0.0,      # DETERMINISTIC
        "max_tokens": 2500,
        "top_p": 1.0,            # DETERMINISTIC
        "messages": [{"role": "user", "content": anchored_prompt}]
    }

    try:
        resp = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=60)
        resp.raise_for_status()
        data = resp.json()

        if "choices" not in data:
            raise Exception("No choices in response")

        content = data["choices"][0]["message"]["content"]
        if not content:
            raise Exception("Empty response")

        # Parse JSON
        parsed = parse_json_safely(content, "Perplexity-Anchored")
        if not parsed:
            return create_anchored_fallback(query, previous_data, web_context)

        # Add sources from web context
        if web_context.get("sources"):
            existing = parsed.get("sources", [])
            parsed["sources"] = list(dict.fromkeys(existing + web_context["sources"]))[:10]

        return json.dumps(parsed)

    except Exception as e:
        st.error(f"❌ Anchored query error: {e}")
        return create_anchored_fallback(query, previous_data, web_context)

def create_anchored_fallback(query: str, previous_data: Dict, web_context: Dict) -> str:
    """Create fallback for anchored evolution query"""
    prev_response = previous_data.get("primary_response", {})

    fallback = {
        "executive_summary": f"Evolution analysis for '{query}' - model returned invalid format. Showing previous data.",
        "analysis_delta": {
            "time_since_previous": calculate_time_ago(previous_data.get("timestamp", "")),
            "overall_trend": "unknown",
            "major_changes": ["Unable to determine changes - API error"],
            "data_freshness": "Unknown"
        },
        "primary_metrics": prev_response.get("primary_metrics", {}),
        "key_findings": ["[UNCHANGED] " + f for f in prev_response.get("key_findings", [])[:3]],
        "top_entities": prev_response.get("top_entities", []),
        "trends_forecast": prev_response.get("trends_forecast", []),
        "sources": web_context.get("sources", []),
        "confidence": 50,
        "freshness": "Fallback",
        "drift_summary": {
            "metrics_changed": 0,
            "metrics_unchanged": len(prev_response.get("primary_metrics", {})),
            "entities_reshuffled": 0,
            "findings_updated": 0,
            "overall_stability_pct": 100
        }
    }
    return json.dumps(fallback)

# =========================================================
# 8. VALIDATION & SCORING
# =========================================================


def parse_number_with_unit(val_str: str) -> float:
    """
    Parse a numeric string into a comparable base scale.
    Returns a float in "millions" for currency/volume-like values.
    Percentages are returned as their numeric value (e.g., "9.8%" -> 9.8).

    Handles:
      - $58.3B, 58.3B, S$29.8B, 29.8 S$B, USD 21.18 B
      - 58.3 billion, 58.3 bn, 58.3 million, 58.3 mn, 570 thousand
      - 570,000 (interpreted as an absolute count -> converted to millions)
      - 9.8% (kept as 9.8)
    """
    if val_str is None:
        return 0.0

    s = str(val_str).strip()
    if not s:
        return 0.0

    s_low = s.lower()

    # If it's a percentage, return the raw percent number (not millions)
    if "%" in s_low:
        m = re.search(r'(-?\d+(?:\.\d+)?)', s_low)
        if not m:
            return 0.0
        try:
            return float(m.group(1))
        except Exception:
            return 0.0

    # Normalize: remove commas and common currency tokens/symbols
    # (keep letters because we need bn/mn/b/m/k detection)
    s_low = s_low.replace(",", " ")
    for token in ["s$", "usd", "sgd", "us$", "$", "€", "£", "aud", "cad"]:
        s_low = s_low.replace(token, " ")

    # Collapse whitespace
    s_low = re.sub(r"\s+", " ", s_low).strip()

    # Extract the first number
    m = re.search(r'(-?\d+(?:\.\d+)?)', s_low)
    if not m:
        return 0.0

    try:
        num = float(m.group(1))
    except Exception:
        return 0.0

    # Look at the remaining text after the number for unit words/suffix
    tail = s_low[m.end():].strip()

    # Decide multiplier (base = millions)
    # billions -> *1000, millions -> *1, thousands -> *0.001
    multiplier = 1.0

    # Word-based units
    if re.search(r'\b(trillion|tn)\b', tail):
        multiplier = 1_000_000.0  # trillion -> million
    elif re.search(r'\b(billion|bn)\b', tail):
        multiplier = 1000.0
    elif re.search(r'\b(million|mn)\b', tail):
        multiplier = 1.0
    elif re.search(r'\b(thousand|k)\b', tail):
        multiplier = 0.001
    else:
        # Suffix-style units (possibly with spaces), e.g. "29.8 b", "21.18 b", "58.3m"
        # We only look at the very first letter-ish token in tail.
        t0 = tail[:4].strip()  # enough to catch "b", "m", "k"
        if t0.startswith("b"):
            multiplier = 1000.0
        elif t0.startswith("m"):
            multiplier = 1.0
        elif t0.startswith("k"):
            multiplier = 0.001
        else:
            # No unit detected. If it's a big integer like 570000 (jobs, people),
            # interpret as an absolute count and convert to millions.
            # (570000 -> 0.57 million)
            if abs(num) >= 10000 and float(num).is_integer():
                multiplier = 1.0 / 1_000_000.0
            else:
                multiplier = 1.0

    return num * multiplier


def numeric_consistency_with_sources(primary_data: dict, web_context: dict) -> float:
    """Compare primary numbers vs source numbers"""
    primary_metrics = primary_data.get("primary_metrics", {})
    primary_numbers = []

    for metric in primary_metrics.values():
        if isinstance(metric, dict):
            val = metric.get("value")
            num = parse_number_with_unit(str(val))
            if num > 0:
                primary_numbers.append(num)

    if not primary_numbers:
        return 50.0  # Neutral when no metrics to compare

    # Extract source numbers with same parsing
    source_numbers = []
    search_results = web_context.get("search_results", [])

    for result in search_results:
        snippet = str(result.get("snippet", ""))
        # Match patterns like "$58.3B", "123M", "456 billion"
        patterns = [
            r'\$?(\d+(?:\.\d+)?)\s*([BbMmKk])',  # $58.3B
            r'(\d+(?:\.\d+)?)\s*(billion|million|thousand)',  # 58.3 billion
        ]

        for pattern in patterns:
            matches = re.findall(pattern, snippet, re.IGNORECASE)
            for num, unit in matches:
                source_numbers.append(parse_number_with_unit(f"{num}{unit[0].upper()}"))

    if not source_numbers:
        return 50.0  # Neutral when no source numbers found

    # Check agreement (within 25% tolerance)
    agreements = 0
    for p_num in primary_numbers:
        for s_num in source_numbers:
            if abs(p_num - s_num) / max(p_num, s_num, 1) < 0.25:
                agreements += 1
                break

    # Scale: 0 agreements = 30%, all agreements = 95%
    agreement_ratio = agreements / len(primary_numbers)
    agreement_pct = 30.0 + (agreement_ratio * 65.0)
    return min(agreement_pct, 95.0)

def numeric_consistency_with_sources_v2(primary_data: dict, web_context: dict) -> float:
    """
    Stable numeric consistency (0-100):
    - Evidence text: search_results snippets + web_context summary + scraped_content
    - Unit-aware parsing via parse_number_with_unit()
    - Range-aware (supports min/max if metric has a 'range' dict)
    - Downweights proxy metrics (is_proxy=True) so they don't tank the score
    """

    try:
        # Prefer canonical metrics if available (has is_proxy, range, etc.)
        metrics = primary_data.get("primary_metrics_canonical") or primary_data.get("primary_metrics") or {}
        if not isinstance(metrics, dict) or not metrics:
            return 50.0

        # -----------------------------
        # Build evidence text corpus
        # -----------------------------
        texts = []

        # 1) snippets
        sr = (web_context or {}).get("search_results") or []
        if isinstance(sr, list):
            for r in sr:
                if isinstance(r, dict):
                    snip = r.get("snippet", "")
                    if isinstance(snip, str) and snip.strip():
                        texts.append(snip)

        # 2) summary
        summary = (web_context or {}).get("summary") or ""
        if isinstance(summary, str) and summary.strip():
            texts.append(summary)

        # 3) scraped_content
        scraped = (web_context or {}).get("scraped_content") or {}
        if isinstance(scraped, dict):
            for _, content in scraped.items():
                if isinstance(content, str) and content.strip():
                    texts.append(content)

        evidence_text = "\n".join(texts)
        if not evidence_text.strip():
            return 45.0  # no evidence stored

        # -----------------------------
        # Extract numeric candidates from evidence text
        # -----------------------------
        # Keep this broad; parse_number_with_unit will normalize.
        patterns = [
            r'\$?\s?\d{1,3}(?:,\d{3})*(?:\.\d+)?\s*[BbMmKk]\b',                 # 29.8B, 570K, 1.2M
            r'\$?\s?\d+(?:\.\d+)?\s*(?:billion|million|thousand|bn|mn)\b',      # 29.8 billion, 29.8 bn
            r'\b\d{1,3}(?:,\d{3})+(?:\.\d+)?\b',                               # 570,000
            r'\b\d+(?:\.\d+)?\s*%\b',                                          # 9.8%
        ]

        evidence_numbers = []
        lowered = evidence_text.lower()

        for pat in patterns:
            for m in re.findall(pat, lowered, flags=re.IGNORECASE):
                n = parse_number_with_unit(str(m))
                if n and n > 0:
                    evidence_numbers.append(n)

        # If nothing extracted, don’t penalize too hard
        if not evidence_numbers:
            return 50.0

        # -----------------------------
        # Verify each metric against evidence numbers (tolerance match)
        # -----------------------------
        def _metric_candidates(m: dict) -> list:
            """Return list of candidate numeric values for a metric (range-aware)."""
            out = []
            if not isinstance(m, dict):
                return out

            # Range support: check min/max if present
            rng = m.get("range") if isinstance(m.get("range"), dict) else None
            if rng:
                if rng.get("min") is not None:
                    out.append(rng.get("min"))
                if rng.get("max") is not None:
                    out.append(rng.get("max"))

            # Also check main value
            if m.get("value") is not None:
                out.append(m.get("value"))

            return out

        def _parse_metric_num(val, unit_hint: str = "") -> float:
            # build a value+unit string so parse_number_with_unit has a chance
            if val is None:
                return 0.0
            s = str(val)
            if unit_hint and unit_hint.lower() not in s.lower():
                s = f"{s} {unit_hint}"
            return parse_number_with_unit(s)

        def _is_supported(target: float, evidence_nums: list, rel_tol: float = 0.25) -> bool:
            # same tolerance approach as v1 (25%)
            if not target or target <= 0:
                return False
            for e in evidence_nums:
                if e <= 0:
                    continue
                if abs(target - e) / max(target, e, 1) < rel_tol:
                    return True
            return False

        supported_w = 0.0
        total_w = 0.0

        for _, m in metrics.items():
            if not isinstance(m, dict):
                continue

            unit = str(m.get("unit") or "").strip()

            # proxy weighting
            is_proxy = bool(m.get("is_proxy"))
            w = 0.5 if is_proxy else 1.0

            cands = _metric_candidates(m)
            if not cands:
                continue

            # parse candidates into numeric values
            parsed_targets = []
            for c in cands:
                n = _parse_metric_num(c, unit_hint=unit)
                if n and n > 0:
                    parsed_targets.append(n)

            if not parsed_targets:
                continue

            total_w += w

            # supported if ANY candidate matches evidence
            if any(_is_supported(t, evidence_numbers, rel_tol=0.25) for t in parsed_targets):
                supported_w += w

        if total_w <= 0:
            return 50.0

        ratio = supported_w / total_w
        # Map: keep a soft floor so one miss doesn't tank the whole run
        score = 30.0 + (ratio * 65.0)  # same scale as v1 (30..95)
        return min(max(score, 20.0), 95.0)

    except Exception:
        return 45.0



def source_consensus(web_context: dict) -> float:
    """
    Calculate source consensus based on proportion of high-quality sources.
    Returns continuous score 0-100 based on quality distribution.
    """
    reliabilities = web_context.get("source_reliability", [])

    if not reliabilities:
        return 50.0  # Neutral when no sources

    total = len(reliabilities)
    high_count = sum(1 for r in reliabilities if "✅" in str(r))
    medium_count = sum(1 for r in reliabilities if "⚠️" in str(r))
    low_count = sum(1 for r in reliabilities if "❌" in str(r))

    # Weighted score: High=100, Medium=60, Low=30
    weighted_sum = (high_count * 100) + (medium_count * 60) + (low_count * 30)
    consensus_score = weighted_sum / total

    # Bonus for having multiple high-quality sources
    if high_count >= 3:
        consensus_score = min(100, consensus_score + 10)
    elif high_count >= 2:
        consensus_score = min(100, consensus_score + 5)

    return round(consensus_score, 1)

def evidence_based_veracity(primary_data: dict, web_context: dict) -> dict:
    """
    Evidence-driven veracity scoring.
    Returns breakdown of component scores and overall score (0-100).
    """
    breakdown = {}

    # 1. SOURCE QUALITY (35% weight)
    sources = primary_data.get("sources", [])
    src_score = source_quality_score(sources)
    breakdown["source_quality"] = round(src_score, 1)

    # 2. NUMERIC CONSISTENCY (30% weight)
    num_score = numeric_consistency_with_sources_v2(primary_data, web_context)
    breakdown["numeric_consistency"] = round(num_score, 1)

    # 3. CITATION DENSITY (20% weight)
    # FIXED: Higher score when sources support findings, not penalize detail
    sources_count = len(sources)
    findings_count = len(primary_data.get("key_findings", []))
    metrics_count = len(primary_data.get("primary_metrics", {}))

    # Total claims = findings + metrics
    total_claims = findings_count + metrics_count

    if total_claims == 0:
        citations_score = 40.0  # Low score for no claims
    else:
        # Ratio of sources to claims - ideal is ~0.5-1.0 sources per claim
        ratio = sources_count / total_claims
        if ratio >= 1.0:
            citations_score = 90.0  # Well-supported
        elif ratio >= 0.5:
            citations_score = 70.0 + (ratio - 0.5) * 40  # 70-90 range
        elif ratio >= 0.25:
            citations_score = 50.0 + (ratio - 0.25) * 80  # 50-70 range
        else:
            citations_score = ratio * 200  # 0-50 range

    breakdown["citation_density"] = round(min(citations_score, 95.0), 1)

    # 4. SOURCE CONSENSUS (15% weight)
    consensus_score = source_consensus(web_context)
    breakdown["source_consensus"] = round(consensus_score, 1)

    # Calculate weighted total
    total_score = (
        breakdown["source_quality"] * 0.35 +
        breakdown["numeric_consistency"] * 0.30 +
        breakdown["citation_density"] * 0.20 +
        breakdown["source_consensus"] * 0.15
    )

    breakdown["overall"] = round(total_score, 1)

    return breakdown

def calculate_final_confidence(
    base_conf: float,
    evidence_score: float
) -> float:
    """
    Calculate final confidence score.

    Formula balances model confidence with evidence quality:
    - Evidence has higher weight (65%) as it's more objective
    - Model confidence (35%) is adjusted by evidence quality

    This ensures:
    - High model + High evidence → High final (~85-90%)
    - High model + Low evidence → Medium final (~55-65%)
    - Low model + High evidence → Medium-High final (~70-80%)
    - Low model + Low evidence → Low final (~40-50%)
    """

    # Normalize inputs to 0-100 range
    base_conf = max(0, min(100, base_conf))
    evidence_score = max(0, min(100, evidence_score))

    # 1. EVIDENCE COMPONENT (65% weight) - Primary driver
    evidence_component = evidence_score * 0.65

    # 2. MODEL COMPONENT (35% weight) - Adjusted by evidence quality
    # When evidence is weak, model confidence is discounted
    evidence_multiplier = 0.5 + (evidence_score / 200)  # Range: 0.5 to 1.0
    model_component = base_conf * evidence_multiplier * 0.35

    final = evidence_component + model_component

    # Ensure result is in valid range
    return round(max(0, min(100, final)), 1)

# =========================================================
# 8A. DETERMINISTIC DIFF ENGINE
# Pure Python computation - no LLM variance
# =========================================================

@dataclass
class MetricDiff:
    """Single metric change record"""
    name: str
    old_value: Optional[float]
    new_value: Optional[float]
    old_raw: str  # Original string representation
    new_raw: str
    unit: str
    change_pct: Optional[float]
    change_type: str  # 'increased', 'decreased', 'unchanged', 'added', 'removed'

@dataclass
class EntityDiff:
    """Single entity ranking change record"""
    name: str
    old_rank: Optional[int]
    new_rank: Optional[int]
    old_share: Optional[str]
    new_share: Optional[str]
    rank_change: Optional[int]  # Positive = moved up
    change_type: str  # 'moved_up', 'moved_down', 'unchanged', 'added', 'removed'

@dataclass
class FindingDiff:
    """Single finding change record"""
    old_text: Optional[str]
    new_text: Optional[str]
    similarity: float  # 0-100
    change_type: str  # 'retained', 'modified', 'added', 'removed'

@dataclass
class EvolutionDiff:
    """Complete diff between two analyses"""
    old_timestamp: str
    new_timestamp: str
    time_delta_hours: Optional[float]
    metric_diffs: List[MetricDiff]
    entity_diffs: List[EntityDiff]
    finding_diffs: List[FindingDiff]
    stability_score: float  # 0-100
    summary_stats: Dict[str, int]

# =========================================================
# CANONICAL METRIC REGISTRY & SEMANTIC FINDING HASH
# Add this section after the dataclass definitions (around line 1587)
# =========================================================

# ------------------------------------
# CANONICAL METRIC REGISTRY
# Removes LLM control over metric identity
# ------------------------------------

# Metric type definitions with aliases
            # =========================
# PATCH MR1 (ADDITIVE): de-ambiguate "sales" so unit-sales doesn't map to Revenue
# - Remove standalone "sales" from Revenue aliases (too ambiguous)
# - Add money-explicit revenue phrases instead ("sales revenue", "sales value", etc.)
# - Add a couple of volume-style aliases under units_sold ("sales volume", "volume sales")
            # =========================

METRIC_REGISTRY = {
    # Market Size metrics
    "market_size": {
        "canonical_name": "Market Size",
        "aliases": [
            "market size", "market value", "market cap", "total market",
            "global market", "market valuation", "industry size",
            "total addressable market", "tam", "market worth"
        ],
        "unit_type": "currency",
        "category": "size"
    },
    "market_size_current": {
        "canonical_name": "Current Market Size",
        "aliases": [
            "2024 market size", "2025 market size", "current market",
            "present market size", "today market", "current year market",
            "market size 2024", "market size 2025"
        ],
        "unit_type": "currency",
        "category": "size"
    },
    "market_size_projected": {
        "canonical_name": "Projected Market Size",
        "aliases": [
            "projected market", "forecast market", "future market",
            "2026 market", "2027 market", "2028 market", "2029 market", "2030 market",
            "market projection", "expected market size", "estimated market"
        ],
        "unit_type": "currency",
        "category": "size"
    },

    # Growth metrics
    "cagr": {
        "canonical_name": "CAGR",
        "aliases": [
            "cagr", "compound annual growth", "compound growth rate",
            "annual growth rate", "growth rate", "yearly growth"
        ],
        "unit_type": "percentage",
        "category": "growth"
    },
    "yoy_growth": {
        "canonical_name": "YoY Growth",
        "aliases": [
            "yoy growth", "year over year", "year-over-year",
            "annual growth", "yearly growth rate", "growth percentage"
        ],
        "unit_type": "percentage",
        "category": "growth"
    },

    # Revenue metrics
    "revenue": {
        "canonical_name": "Revenue",
        "aliases": [
            "revenue",
            # =========================
            # PATCH MR1 (CHANGED): removed ambiguous standalone alias "sales"
            # =========================
            # "sales",
            # =========================
            "total revenue", "annual revenue",
            "yearly revenue", "gross revenue",

            # =========================
            # PATCH MR1 (ADDITIVE): money-explicit sales phrasing (revenue-like)
            # =========================
            "sales revenue",
            "revenue from sales",
            "sales value",
            "value of sales",
            "sales (value)",
            "turnover",  # common finance synonym
            # =========================
        ],
        "unit_type": "currency",
        "category": "financial"
    },

    # Market share
    "market_share": {
        "canonical_name": "Market Share",
        "aliases": [
            "market share", "share", "market portion", "market percentage",
            "share of market"
        ],
        "unit_type": "percentage",
        "category": "share"
    },

    # Volume metrics
    "units_sold": {
        "canonical_name": "Units Sold",
        "aliases": [
            "units sold", "unit sales", "volume", "units shipped",
            "shipments", "deliveries", "production volume",

            # =========================
            # PATCH MR1 (ADDITIVE): common unit-sales phrasing variants
            # =========================
            "sales volume",
            "volume sales",
            # =========================
        ],
        "unit_type": "count",
        "category": "volume"
    },

    # Pricing
    "average_price": {
        "canonical_name": "Average Price",
        "aliases": [
            "average price", "avg price", "mean price", "asp",
            "average selling price", "unit price"
        ],
        "unit_type": "currency",
        "category": "pricing"
    },

    # -------------------------
    # Country / Macro metrics
    # -------------------------
    "gdp": {
        "canonical_name": "GDP",
        "aliases": ["gdp", "gross domestic product", "economic output"],
        "unit_type": "currency",
        "category": "macro"
    },
    "gdp_per_capita": {
        "canonical_name": "GDP per Capita",
        "aliases": ["gdp per capita", "gdp/capita", "income per person", "per capita gdp"],
        "unit_type": "currency",
        "category": "macro"
    },
    "gdp_growth": {
        "canonical_name": "GDP Growth",
        "aliases": ["gdp growth", "economic growth", "growth rate of gdp", "real gdp growth"],
        "unit_type": "percentage",
        "category": "macro"
    },
    "population": {
        "canonical_name": "Population",
        "aliases": ["population", "population size", "number of people"],
        "unit_type": "count",
        "category": "macro"
    },
    "exports": {
        "canonical_name": "Exports",
        "aliases": ["exports", "export value", "total exports"],
        "unit_type": "currency",
        "category": "trade"
    },
    "imports": {
        "canonical_name": "Imports",
        "aliases": ["imports", "import value", "total imports"],
        "unit_type": "currency",
        "category": "trade"
    },
    "inflation": {
        "canonical_name": "Inflation",
        "aliases": ["inflation", "cpi", "consumer price index", "inflation rate"],
        "unit_type": "percentage",
        "category": "macro"
    },
    "interest_rate": {
        "canonical_name": "Interest Rate",
        "aliases": ["interest rate", "policy rate", "benchmark rate", "central bank rate"],
        "unit_type": "percentage",
        "category": "macro"
    }
}

            # =========================
# END PATCH MR1
            # =========================

# Year extraction pattern
YEAR_PATTERN = re.compile(r'(20\d{2})')

# ------------------------------------
# DETERMINISTIC QUESTION SIGNALS
# Drives metric table templates (no LLM)
# ------------------------------------

QUESTION_CATEGORY_TEMPLATES = {
    "country": [
        "gdp",
        "gdp_per_capita",
        "gdp_growth",
        "population",
        "exports",
        "imports",
        "inflation",
        "interest_rate",
    ],
    "industry": [
        "market_size_current",
        "market_size_projected",
        "cagr",
        "revenue",
        "market_share",
        "units_sold",
        "average_price",
    ],
}

def get_expected_metric_ids_for_category(category: str) -> List[str]:
    """
    Domain-agnostic mapping from a template/category string to expected metric IDs.

    Backward compatible:
      - accepts legacy categories like 'country', 'industry', 'company', 'generic'
      - also accepts template IDs like 'ENTITY_OVERVIEW_MARKET_LIGHT_V1', etc.

    NOTE:
    - This function returns a *default* set for a given template/category.
    - The profiler (classify_question_signals) can override/compose expected_metric_ids dynamically.
    """
    c_raw = (category or "unknown").strip()
    c = c_raw.lower().strip()

    # -------------------------
    # New generalized templates
    # -------------------------
    if c in {"entity_overview_country_light_v1", "entity_overview_country_v1"}:
        return [
            "population",
            "gdp_nominal",
            "gdp_per_capita",
            "gdp_growth",
            "inflation",
            "currency",
            "unemployment",
            "exports",
            "imports",
            "top_industries",
        ]

    if c in {"entity_overview_market_light_v1"}:
        return [
            "market_size_current",
            "market_size_projected",
            "cagr",
            "key_trends",
            "top_players",
        ]

    if c in {"entity_overview_market_heavy_v1"}:
        return [
            "market_size_current",
            "market_size_projected",
            "cagr",
            "key_trends",
            "top_players",
            "key_regions",
            "segments",
            "market_share",
            "revenue",
            "units_sold",
            "average_price",
        ]

    if c in {"entity_overview_company_light_v1", "entity_overview_company_v1"}:
        return [
            "revenue",
            "growth",
            "gross_margin",
            "operating_margin",
            "net_income",
            "market_cap",
            "valuation_multiple",
        ]

    if c in {"entity_overview_product_light_v1", "entity_overview_product_v1"}:
        return [
            "average_price",
            "units_sold",
            "market_share",
            "growth",
            "key_trends",
        ]

    if c in {"entity_overview_topic_v1", "generic_v1"}:
        return []

    # -------------------------
    # Legacy categories (still supported)
    # -------------------------
    if c == "country":
        return get_expected_metric_ids_for_category("ENTITY_OVERVIEW_COUNTRY_LIGHT_V1")

    if c == "industry":
        # legacy industry defaults to light market
        return get_expected_metric_ids_for_category("ENTITY_OVERVIEW_MARKET_LIGHT_V1")

    if c == "company":
        return get_expected_metric_ids_for_category("ENTITY_OVERVIEW_COMPANY_LIGHT_V1")

    if c == "generic":
        return []

    # fallback
    return []


def classify_question_signals(query: str) -> Dict[str, Any]:
    """
    Deterministically classify query and return:
      - category: high-level bucket used for templates (country | industry | company | generic)
      - expected_metric_ids: list[str]
      - signals: list[str] (debuggable reasons)
      - years: list[int]
      - regions: list[str]
      - intents: list[str] (market_size, growth_forecast, competitive_landscape, pricing, regulation, consumer_demand, supply_chain, investment, macro_outlook)
    """
    q_raw = (query or "").strip()
    q = q_raw.lower().strip()
    signals: List[str] = []

    if not q:
        return {
            "category": "generic",
            "expected_metric_ids": [],
            "signals": ["empty_query"],
            "years": [],
            "regions": [],
            "intents": []
        }

    # -------------------------
    # 1) Extract years (deterministic)
    # -------------------------
    years: List[int] = []
    try:
        year_matches = re.findall(r"\b(19|20)\d{2}\b", q_raw)
        # The regex above returns the first group; re-run with a non-capturing group to capture full year strings.
        year_matches_full = re.findall(r"\b(?:19|20)\d{2}\b", q_raw)
        years = sorted({int(y) for y in year_matches_full})
        if years:
            signals.append(f"years:{','.join(map(str, years[:8]))}")
    except Exception:
        years = []

    # -------------------------
    # 2) Extract regions/countries (best-effort deterministic; spaCy if available)
    # -------------------------
    regions: List[str] = []
    try:
        nlp = _try_spacy_nlp()
        if nlp:
            doc = nlp(q_raw)
            gpes = [ent.text.strip() for ent in getattr(doc, "ents", []) if ent.label_ in ("GPE", "LOC")]
            regions = []
            for g in gpes:
                if g and g.lower() not in [x.lower() for x in regions]:
                    regions.append(g)
            if regions:
                signals.append(f"regions_spacy:{','.join(regions[:6])}")
    except Exception:
        pass

    # Fallback: very lightweight region tokens
    if not regions:
        region_tokens = [
            "singapore", "malaysia", "indonesia", "thailand", "vietnam", "philippines",
            "china", "india", "japan", "korea", "australia",
            "usa", "united states", "europe", "uk", "united kingdom",
            "asean", "southeast asia", "sea", "global", "worldwide"
        ]
        hits = [t for t in region_tokens if t in q]
        if hits:
            # Keep original casing loosely (title-case single words)
            regions = [h.title() if " " not in h else h.upper() if h in ("usa", "uk") else h.title() for h in hits[:6]]
            signals.append(f"regions_kw:{','.join(hits[:6])}")

    # -------------------------
    # 3) Intent detection (domain-agnostic)
    # -------------------------
    intent_patterns: Dict[str, List[str]] = {
        "market_size": ["market size", "tam", "total addressable market", "how big", "size of the market", "market value"],
        "growth_forecast": ["cagr", "forecast", "projection", "by 20", "growth rate", "expected to", "outlook", "trend"],
        "competitive_landscape": ["key players", "competitors", "market share", "top companies", "leading players", "who are the players"],
        "pricing": ["pricing", "price", "asp", "average selling price", "cost", "margins"],
        "consumer_demand": ["demand", "users", "penetration", "adoption", "consumer", "customer", "behavior"],
        "supply_chain": ["supply", "capacity", "production", "manufacturing", "inventory", "shipment", "lead time"],
        "regulation": ["regulation", "policy", "law", "compliance", "tax", "tariff", "subsidy"],
        "investment": ["investment", "capex", "funding", "valuation", "roi", "profit", "ebitda"],
        "macro_outlook": ["gdp", "inflation", "interest rate", "policy rate", "exports", "imports", "currency", "exchange rate", "per capita"],
    }

    intents: List[str] = []
    for intent, pats in intent_patterns.items():
        if any(p in q for p in pats):
            intents.append(intent)

    # Small disambiguation: "by 2030" etc. strongly suggests forecast if years exist
    if years and "growth_forecast" not in intents and any(yr >= 2025 for yr in years):
        intents.append("growth_forecast")

    if intents:
        signals.append(f"intents:{','.join(intents[:10])}")

    # -------------------------
    # 4) Category decision (template driver)
    # -------------------------
    # Keep it coarse: country vs industry vs company vs generic
    country_kw = [
        "gdp", "per capita", "population", "exports", "imports",
        "inflation", "cpi", "interest rate", "policy rate", "central bank",
        "currency", "exchange rate"
    ]
    company_kw = ["revenue", "earnings", "profit", "ebitda", "guidance", "quarter", "fy", "10-k", "10q", "balance sheet"]
    industry_kw = [
        "market", "industry", "sector", "tam", "cagr", "market size", "market share",
        "key players", "competitors", "pricing", "forecast", "outlook"
    ]

    country_hits = [k for k in country_kw if k in q]
    company_hits = [k for k in company_kw if k in q]
    industry_hits = [k for k in industry_kw if k in q]

    # If macro intent is present, strongly bias to country
    if "macro_outlook" in intents and (regions or country_hits):
        category = "country"
        signals.append("category_rule:macro_outlook_bias_country")
    elif company_hits and not industry_hits:
        category = "company"
        signals.append(f"category_rule:company_keywords:{','.join(company_hits[:5])}")
    elif industry_hits and not country_hits:
        category = "industry"
        signals.append(f"category_rule:industry_keywords:{','.join(industry_hits[:5])}")
    elif industry_hits and country_hits:
        # tie-break: if market sizing/competitive signals exist -> industry; if macro_outlook -> country
        if "macro_outlook" in intents:
            category = "country"
            signals.append("category_rule:mixed_signals_macro_wins")
        else:
            category = "industry"
            signals.append("category_rule:mixed_signals_default_to_industry")
    else:
        category = "generic"
        signals.append("category_rule:no_template_keywords")

    # -------------------------
    # 5) Expected metric IDs (category + intent)
    # -------------------------
    expected_metric_ids: List[str] = []
    try:
        expected_metric_ids = get_expected_metric_ids_for_category(category) or []
    except Exception:
        expected_metric_ids = []

    # Add a few intent-driven metric IDs (only if your registry supports them)
    intent_metric_suggestions = {
        "market_size": ["market_size", "market_size_2024", "market_size_2025"],
        "growth_forecast": ["cagr", "forecast_period", "market_size_2030"],
        "competitive_landscape": ["market_share", "top_players"],
        "pricing": ["avg_price", "asp"],
        "consumer_demand": ["users", "penetration", "arpu"],
        "supply_chain": ["capacity", "shipments"],
        "investment": ["capex", "profit", "ebitda"],
        "macro_outlook": ["gdp", "inflation", "interest_rate", "exchange_rate"],
    }

    for intent in intents:
        for mid in intent_metric_suggestions.get(intent, []):
            if mid not in expected_metric_ids:
                expected_metric_ids.append(mid)

    return {
        "category": category,
        "expected_metric_ids": expected_metric_ids,
        "signals": signals,
        "years": years,
        "regions": regions,
        "intents": intents
    }


    def _contains_any(needle_list: List[str]) -> bool:
        return any(k in q for k in needle_list)

    # -------------------------
    # Determine intents
    # -------------------------
    intents: List[str] = []
    for intent, kws in intent_triggers.items():
        if _contains_any(kws):
            intents.append(intent)

    if intents:
        signals.append("intents:" + ",".join(sorted(set(intents))))

    # -------------------------
    # Determine entity_kind (best-effort heuristic)
    # -------------------------
    is_marketish = _contains_any(market_entity_kw) or any(i in intents for i in ["size", "growth", "forecast", "share", "segments", "players", "regions"])
    is_companyish = _contains_any(company_entity_kw) and not _contains_any(country_entity_kw)
    is_countryish = _contains_any(country_entity_kw) and not is_companyish
    is_productish = _contains_any(product_entity_kw) and not (is_marketish or is_countryish or is_companyish)

    if is_countryish:
        entity_kind = "country"
        signals.append("entity_kind:country")
    elif is_companyish:
        entity_kind = "company"
        signals.append("entity_kind:company")
    elif is_productish:
        entity_kind = "product"
        signals.append("entity_kind:product")
    elif is_marketish:
        entity_kind = "market"
        signals.append("entity_kind:market")
    else:
        entity_kind = "topic_general"
        signals.append("entity_kind:topic_general")

    # -------------------------
    # Determine scope
    # -------------------------
    is_comparative = _contains_any(comparative_kw)
    is_forecasty = _contains_any(forecast_kw) or bool(YEAR_PATTERN.findall(q_raw))

    # Broad overview should win when user explicitly asks for general explainer
    # BUT: if they also mention measurable intents (size/growth/forecast/etc.), treat as metrics_light.
    is_broad_phrase = _contains_any(broad_phrases)

    if is_comparative:
        scope = "comparative"
        signals.append("scope:comparative")
    elif is_forecasty and any(i in intents for i in ["forecast", "growth", "size"]):
        scope = "forecast_specific"
        signals.append("scope:forecast_specific")
    elif is_broad_phrase and not intents:
        scope = "broad_overview"
        signals.append("scope:broad_overview")
    else:
        # metrics light vs heavy
        heavy_asks = ["segments", "share", "volume", "regions", "players"]
        heavy_requested = any(i in intents for i in heavy_asks)
        if heavy_requested:
            scope = "metrics_heavy"
            signals.append("scope:metrics_heavy")
        else:
            scope = "metrics_light"
            signals.append("scope:metrics_light")

    # -------------------------
    # Map entity_kind -> category (backward compatible)
    # -------------------------
    if entity_kind == "country":
        category = "country"
    elif entity_kind == "company":
        category = "company"
    elif entity_kind in {"market", "product"}:
        category = "industry"
    else:
        category = "generic"

    # -------------------------
    # Choose generalized template + tiers
    # -------------------------
    # Tier meanings:
    #  1 = high extractability (size/growth/forecast)
    #  2 = medium (players/regions/basic segments)
    #  3 = low (granular channels, detailed splits) -> only if explicitly asked
    if category == "country":
        metric_template_id = "ENTITY_OVERVIEW_COUNTRY_LIGHT_V1" if scope != "metrics_heavy" else "ENTITY_OVERVIEW_COUNTRY_LIGHT_V1"
        metric_tiers_enabled = [1]
    elif category == "company":
        metric_template_id = "ENTITY_OVERVIEW_COMPANY_LIGHT_V1"
        metric_tiers_enabled = [1]
    elif category == "industry":
        if scope in {"metrics_heavy", "comparative"}:
            metric_template_id = "ENTITY_OVERVIEW_MARKET_HEAVY_V1"
            metric_tiers_enabled = [1, 2]
        else:
            metric_template_id = "ENTITY_OVERVIEW_MARKET_LIGHT_V1"
            metric_tiers_enabled = [1]
    else:
        metric_template_id = "ENTITY_OVERVIEW_TOPIC_V1"
        metric_tiers_enabled = []

    # -------------------------
    # Build expected_metric_ids dynamically from intents (domain-agnostic)
    # -------------------------
    # Slot -> metric id mapping (kept generic; avoids tourism specialization)
    # If you later add more canonical IDs, expand these mappings.
    market_slot_to_id = {
        "size_current": "market_size_current",
        "size_projected": "market_size_projected",
        "growth_cagr": "cagr",
        "growth_yoy": "growth",
        "share_key": "market_share",
        "volume_current": "units_sold",
        "price_avg": "average_price",
        "players_top": "top_players",
        "regions_key": "key_regions",
        "segments_basic": "segments",
        "trends": "key_trends",
        "revenue": "revenue",
    }

    company_slot_to_id = {
        "revenue": "revenue",
        "growth": "growth",
        "gross_margin": "gross_margin",
        "operating_margin": "operating_margin",
        "net_income": "net_income",
        "market_cap": "market_cap",
        "valuation_multiple": "valuation_multiple",
        "trends": "key_trends",
    }

    country_slot_to_id = {
        "population": "population",
        "gdp_nominal": "gdp_nominal",
        "gdp_per_capita": "gdp_per_capita",
        "gdp_growth": "gdp_growth",
        "inflation": "inflation",
        "currency": "currency",
        "unemployment": "unemployment",
        "exports": "exports",
        "imports": "imports",
        "top_industries": "top_industries",
        "trends": "key_trends",
    }

    # Determine slots from intents
    slots: List[str] = []
    if entity_kind == "country":
        # For countries: macro defaults if broad, otherwise macro intents
        if scope == "broad_overview":
            slots = ["population", "gdp_nominal", "gdp_per_capita", "gdp_growth", "inflation", "currency", "top_industries"]
        else:
            # If user asks for macro (or didn’t specify), still give a tight macro set
            slots = ["population", "gdp_nominal", "gdp_growth", "inflation", "currency"]
            if "macro" in intents:
                slots += ["unemployment", "exports", "imports"]

        mapper = country_slot_to_id

    elif entity_kind == "company":
        slots = ["revenue", "growth", "gross_margin", "operating_margin", "net_income", "market_cap", "valuation_multiple"]
        mapper = company_slot_to_id

    elif entity_kind in {"market", "product"}:
        # Tier 1 core (always when metrics_* scope)
        if scope == "broad_overview":
            slots = ["trends", "players_top"]
        else:
            slots = ["size_current", "growth_cagr"]
            if "forecast" in intents:
                slots.append("size_projected")
            if "trends" in intents:
                slots.append("trends")
            # Tier 2 (only when explicitly asked or heavy scope)
            if scope in {"metrics_heavy", "comparative"}:
                if "players" in intents:
                    slots.append("players_top")
                if "regions" in intents:
                    slots.append("regions_key")
                if "segments" in intents:
                    slots.append("segments_basic")
                if "share" in intents:
                    slots.append("share_key")
                if "volume" in intents:
                    slots.append("volume_current")
                if "price" in intents:
                    slots.append("price_avg")
            else:
                # metrics_light: include players/trends only if asked
                if "players" in intents:
                    slots.append("players_top")
                if "regions" in intents:
                    slots.append("regions_key")

        mapper = market_slot_to_id

    else:
        # topic_general
        slots = []
        mapper = {}

    expected_metric_ids = []
    for s in slots:
        mid = mapper.get(s)
        if mid:
            expected_metric_ids.append(mid)

    # If still empty but template provides defaults, use template defaults
    if not expected_metric_ids:
        expected_metric_ids = get_expected_metric_ids_for_category(metric_template_id)

    # De-dup while preserving order
    seen = set()
    expected_metric_ids = [x for x in expected_metric_ids if not (x in seen or seen.add(x))]

    # -------------------------
    # Preferred source classes (generic)
    # -------------------------
    if category == "country":
        preferred_source_classes = ["official_stats", "government", "reputable_org", "reference"]
    elif category == "company":
        preferred_source_classes = ["official_filings", "investor_relations", "reputable_org", "news"]
    elif category == "industry":
        preferred_source_classes = ["industry_association", "reputable_org", "official_stats", "news", "research_portal"]
    else:
        preferred_source_classes = ["reference", "official_stats", "reputable_org"]

    # Attach year detection signal
    years = sorted(set(YEAR_PATTERN.findall(q_raw))) if YEAR_PATTERN.findall(q_raw) else []
    if years:
        signals.append("years_detected:" + ",".join(years))

    return {
        "category": category,
        "expected_metric_ids": expected_metric_ids,
        "signals": signals,
        "entity_kind": entity_kind,
        "scope": scope,
        "metric_template_id": metric_template_id,
        "metric_tiers_enabled": metric_tiers_enabled,
        "preferred_source_classes": preferred_source_classes,
        "intents": sorted(set(intents)),
    }


def get_canonical_metric_id(metric_name: str) -> Tuple[str, str]:
    """
    Map a metric name to its canonical ID and display name.

    Returns:
        Tuple of (canonical_id, canonical_display_name)

    Example:
        "2024 Market Size" -> ("market_size_2024", "Market Size (2024)")
        "Global Market Value" -> ("market_size", "Market Size")
        "CAGR 2024-2030" -> ("cagr_2024_2030", "CAGR (2024-2030)")
    """
    import re

    if not metric_name:
        return ("unknown", "Unknown Metric")

    name_lower = metric_name.lower().strip()
    name_normalized = re.sub(r"[^\w\s]", " ", name_lower)
    name_normalized = re.sub(r"\s+", " ", name_normalized).strip()

    # Extract years
    years = YEAR_PATTERN.findall(metric_name)
    year_suffix = "_".join(sorted(years)) if years else ""

    # =========================
    # PATCH CM1 (ADDITIVE): intent signals to prevent "sales" -> "revenue" mis-maps
    # =========================
    name_words = set(name_normalized.split())

    # Explicit money intent (strong)
    money_tokens = {
        "revenue", "turnover", "valuation", "valued", "value", "market", "capex", "opex",
        "profit", "earnings", "ebitda", "income",
        "usd", "sgd", "eur", "gbp", "aud", "cad", "jpy", "cny", "rmb"
    }
    # Currency symbols appear in raw text sometimes
    has_currency_symbol = any(sym in metric_name for sym in ["$", "€", "£", "S$"])

    has_money_intent = bool(name_words & money_tokens) or has_currency_symbol

    # Explicit unit/count intent (strong)
    unit_tokens = {
        "unit", "units", "deliveries", "shipments", "registrations", "vehicles",
        "sold", "salesvolume", "volume", "pcs", "pieces"
    }
    # normalize joined token cases like "sales volume"
    joined = name_normalized.replace(" ", "")
    has_unit_intent = bool(name_words & unit_tokens) or any(t in joined for t in ["salesvolume", "unitsold", "vehiclesold"])
    # =========================

    # Find best matching registry entry
    best_match_id = None
    best_match_score = 0.0

    # =========================
    # PATCH CM2 (ADDITIVE): helper to identify revenue-like registry targets
    # =========================
    def _is_revenue_like(metric_id: str, config: dict) -> bool:
        mid = (metric_id or "").lower()
        cname = str((config or {}).get("canonical_name") or "").lower()
        # treat "market value" / "valuation" as currency-like too
        if any(k in cname for k in ["revenue", "market value", "valuation", "market size", "turnover"]):
            return True
        if any(k in mid for k in ["revenue", "market_value", "market_size", "valuation"]):
            return True
        return False
    # =========================

    for metric_id, config in METRIC_REGISTRY.items():
        for alias in config["aliases"]:
            # Remove years from alias for comparison
            alias_no_year = YEAR_PATTERN.sub("", alias).strip().lower()
            alias_no_year = re.sub(r"[^\w\s]", " ", alias_no_year)
            alias_no_year = re.sub(r"\s+", " ", alias_no_year).strip()

            name_no_year = YEAR_PATTERN.sub("", name_normalized).strip()

            # ---- base score from your existing logic ----
            score = 0.0

            # Exact match
            if alias_no_year == name_no_year and alias_no_year:
                score = 1.0

            # Containment match
            elif alias_no_year and (alias_no_year in name_no_year or name_no_year in alias_no_year):
                score = len(alias_no_year) / max(len(name_no_year), 1)

            # Word overlap match
            else:
                alias_words = set(alias_no_year.split())
                name_words_local = set(name_no_year.split())
                if alias_words and name_words_local:
                    overlap = len(alias_words & name_words_local) / len(alias_words | name_words_local)
                    score = max(score, overlap)

            # =========================
            # PATCH CM3 (ADDITIVE): disambiguation penalties/guards
            # - Block "sales" -> revenue when no money intent
            # - Block unit-intent -> revenue-like
            # - Require explicit money intent for revenue-like (soft guard, not hard stop)
            # =========================
            if score > 0.0:
                revenue_like = _is_revenue_like(metric_id, config)

                # If target is revenue-like but name has strong unit intent, penalize heavily
                if revenue_like and has_unit_intent and not has_money_intent:
                    score *= 0.20  # strong downweight

                # If target is revenue-like but name has NO money intent at all, penalize
                if revenue_like and not has_money_intent:
                    score *= 0.55  # moderate downweight

                # If name includes the word "sales" but no money intent, avoid mapping to revenue-like
                if revenue_like and ("sales" in name_no_year.split()) and not has_money_intent:
                    score *= 0.60

                # Conversely: if target is NOT revenue-like but name has money intent, slight penalty
                if (not revenue_like) and has_money_intent and ("sales" in name_no_year.split()) and not has_unit_intent:
                    score *= 0.85
            # =========================

            if score > best_match_score:
                best_match_id = metric_id
                best_match_score = score

            if best_match_score == 1.0:
                break

        if best_match_score == 1.0:
            break

    # Build canonical ID and display name
    if best_match_id and best_match_score > 0.4:
        config = METRIC_REGISTRY[best_match_id]
        canonical_base = best_match_id
        display_name = config["canonical_name"]

        if year_suffix:
            canonical_id = f"{canonical_base}_{year_suffix}"
            if len(years) == 1:
                display_name = f"{display_name} ({years[0]})"
            else:
                display_name = f"{display_name} ({'-'.join(years)})"
        else:
            canonical_id = canonical_base

        return (canonical_id, display_name)

    # Fallback: create ID from normalized name
    fallback_id = re.sub(r"\s+", "_", name_normalized)
    if year_suffix:
        fallback_id = f"{fallback_id}_{year_suffix}" if year_suffix not in fallback_id else fallback_id

    return (fallback_id, metric_name)

# ------------------------------------
# GEO + PROXY TAGGING (DETERMINISTIC)
# ------------------------------------

import re
from typing import Dict, Any, Tuple, List, Optional

REGION_KEYWORDS = {
    "APAC": ["apac", "asia pacific", "asia-pacific"],
    "SOUTHEAST_ASIA": ["southeast asia", "asean", "sea "],  # note space to reduce false matches
    "ASIA": ["asia"],
    "EUROPE": ["europe", "eu", "emea"],
    "NORTH_AMERICA": ["north america"],
    "LATAM": ["latin america", "latam"],
    "MIDDLE_EAST": ["middle east", "mena"],
    "AFRICA": ["africa"],
    "OCEANIA": ["oceania", "australia", "new zealand"],
}

GLOBAL_KEYWORDS = ["global", "worldwide", "world", "international", "across the world"]

# Minimal country map (expand deterministically over time)
COUNTRY_KEYWORDS = {
    "Singapore": ["singapore", "sg"],
    "United States": ["united states", "usa", "u.s.", "us"],
    "United Kingdom": ["united kingdom", "uk", "u.k.", "britain", "england"],
    "China": ["china", "prc"],
    "Japan": ["japan"],
    "India": ["india"],
    "Indonesia": ["indonesia"],
    "Malaysia": ["malaysia"],
    "Thailand": ["thailand"],
    "Vietnam": ["vietnam"],
    "Philippines": ["philippines"],
}

def infer_geo_scope(*texts: str) -> Dict[str, str]:
    """
    Deterministically infer geography from text.
    Returns {"geo_scope": "local|regional|global|unknown", "geo_name": "<name or ''>"}.
    Priority: country > region > global.
    """
    combined = " ".join([t for t in texts if isinstance(t, str) and t.strip()]).lower()
    if not combined:
        return {"geo_scope": "unknown", "geo_name": ""}

    # 1) Country/local (most specific)
    for country, kws in COUNTRY_KEYWORDS.items():
        for kw in kws:
            if kw in combined:
                return {"geo_scope": "local", "geo_name": country}

    # 2) Region
    for region_name, kws in REGION_KEYWORDS.items():
        for kw in kws:
            if kw in combined:
                pretty = region_name.replace("_", " ").title()
                return {"geo_scope": "regional", "geo_name": pretty}

    # 3) Global
    for kw in GLOBAL_KEYWORDS:
        if kw in combined:
            return {"geo_scope": "global", "geo_name": "Global"}

    return {"geo_scope": "unknown", "geo_name": ""}


# ---- Proxy labeling ----
# "Proxy" = adjacent metric that can help approximate the target but isn't the target definition.
# You can expand these sets deterministically.

PROXY_PATTERNS = [
    # (pattern, proxy_type, reason_template)
    (r"\bapparel\b|\bfashion\b|\bclothing\b", "adjacent_category", "Uses apparel/fashion as an adjacent proxy for streetwear."),
    (r"\bfootwear\b|\bsneaker\b|\bshoes\b", "subsegment", "Uses footwear/sneakers as a subsegment proxy for the broader market."),
    (r"\bresale\b|\bsecondary market\b", "channel_proxy", "Uses resale/secondary-market measures as a channel proxy."),
    (r"\be-?commerce\b|\bonline sales\b|\bsocial commerce\b", "channel_proxy", "Uses e-commerce indicators as a channel proxy."),
    (r"\btourism\b|\bvisitor\b|\btravel retail\b", "demand_driver", "Uses tourism indicators as a demand-driver proxy."),
    (r"\bsearch interest\b|\bgoogle trends\b|\bweb traffic\b", "interest_proxy", "Uses interest/attention measures as a proxy."),
]

# These are words that signal "core market size" style metrics (usually non-proxy if they match the user topic).
CORE_MARKET_PATTERNS = [
    r"\bmarket size\b",
    r"\bmarket value\b",
    r"\brevenue\b",
    r"\bsales\b",
    r"\bcagr\b",
    r"\bgrowth\b",
    r"\bprojected\b|\bforecast\b",
]

def infer_proxy_label(
    metric_name: str,
    question_text: str = "",
    category_hint: str = "",
    *extra_context: str
) -> Dict[str, Any]:
    """
    Deterministically label a metric as proxy/non-proxy.

    Returns fields:
      is_proxy: bool
      proxy_type: str
      proxy_reason: str
      proxy_confidence: float (0-1)
      proxy_target: str (best-guess target topic)
    """
    name = (metric_name or "").lower().strip()
    q = (question_text or "").lower().strip()
    ctx = " ".join([c for c in extra_context if isinstance(c, str)]).lower()

    combined = " ".join([name, q, ctx]).strip()

    # Default: not proxy
    out = {
        "is_proxy": False,
        "proxy_type": "",
        "proxy_reason": "",
        "proxy_confidence": 0.0,
        "proxy_target": ""
    }

    if not combined:
        return out

    # Best-effort target topic extraction (very light heuristic)
    # If you already have question signals elsewhere, you can pass them in category_hint/question_text.
    # Here we just keep a short phrase if present.
    proxy_target = ""
    if "streetwear" in q:
        proxy_target = "streetwear"
    elif "semiconductor" in q:
        proxy_target = "semiconductors"
    elif "battery" in q:
        proxy_target = "batteries"
    out["proxy_target"] = proxy_target

    # If metric name itself looks like core market patterns AND includes the target keyword, treat as non-proxy.
    # (prevents incorrectly labeling "Singapore streetwear market size" as proxy)
    core_like = any(re.search(p, name) for p in CORE_MARKET_PATTERNS)
    if core_like:
        # If it explicitly contains the topic keyword, strongly non-proxy
        if proxy_target and proxy_target in name:
            return out
        # If it says "streetwear market" in name, non-proxy even if target not detected
        if "streetwear" in name:
            return out

    # Detect proxies using patterns.
    for pat, ptype, reason in PROXY_PATTERNS:
        if re.search(pat, combined):
            out["is_proxy"] = True
            out["proxy_type"] = ptype
            out["proxy_reason"] = reason
            # Confidence: stronger if pattern appears in metric name; weaker if only in context.
            if re.search(pat, name):
                out["proxy_confidence"] = 0.9
            elif re.search(pat, ctx):
                out["proxy_confidence"] = 0.7
            else:
                out["proxy_confidence"] = 0.6
            return out

    return out


def merge_group_geo(group: List[Dict[str, Any]]) -> Tuple[str, str]:
    """
    Choose the most frequent geo tag within a merged group deterministically.
    Returns (geo_scope, geo_name).
    """
    items = []
    for g in group:
        s = g.get("geo_scope", "unknown")
        n = g.get("geo_name", "")
        if s and s != "unknown":
            items.append((s, n))

    if not items:
        return "unknown", ""

    counts: Dict[str, int] = {}
    for s, n in items:
        k = f"{s}|{n}"
        counts[k] = counts.get(k, 0) + 1

    best_k = max(counts.items(), key=lambda kv: kv[1])[0]  # deterministic tie via insertion order after stable sort
    s, n = best_k.split("|", 1)
    return s, n


def merge_group_proxy(group: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Merge proxy labels for duplicates deterministically.
    If ANY member is proxy -> merged metric is proxy.
    Choose the highest-confidence proxy candidate.
    """
    best = None
    best_conf = -1.0

    for g in group:
        is_proxy = bool(g.get("is_proxy", False))
        conf = float(g.get("proxy_confidence", 0.0) or 0.0)
        if is_proxy and conf > best_conf:
            best_conf = conf
            best = g

    if best is None:
        return {
            "is_proxy": False,
            "proxy_type": "",
            "proxy_reason": "",
            "proxy_confidence": 0.0,
            "proxy_target": "",
        }

    return {
        "is_proxy": True,
        "proxy_type": best.get("proxy_type", ""),
        "proxy_reason": best.get("proxy_reason", ""),
        "proxy_confidence": float(best.get("proxy_confidence", 0.0) or 0.0),
        "proxy_target": best.get("proxy_target", ""),
    }

def canonicalize_metrics(
    metrics: Dict,
    merge_duplicates_to_range: bool = True,
    question_text: str = "",
    category_hint: str = ""
) -> Dict:
    """
    Convert metrics to canonical IDs, but NEVER merge across incompatible dimensions.

    Key fix:
      - Adds deterministic 'dimension' classification and incorporates it into canonical keys.
      - Prevents revenue vs unit-sales from merging just because the year matches.
      - Keeps your geo + proxy tagging behavior.

    Output:
      canonicalized[canonical_key] -> metric dict with:
        - canonical_id (base id)
        - canonical_key (dimension-safe id you should use everywhere downstream)
        - dimension (currency | unit_sales | percent | count | index | unknown)
        - name (dimension-corrected display name)
    """
    import re  # ========================= PATCH C0 (ADDITIVE): missing import =========================

    if not isinstance(metrics, dict):
        return {}

    # =========================
    # PATCH C1 (ADDITIVE): safe helpers for canonical numeric fields
    # - Prefer existing normalize_unit_tag/unit_family/canonicalize_numeric_candidate if present.
    # - Never breaks if those helpers are missing.
    # =========================
    def _safe_normalize_unit_tag(u: str) -> str:
        try:
            fn = globals().get("normalize_unit_tag")
            if callable(fn):
                return fn(u or "")
        except Exception:
            pass
        # minimal fallback (kept conservative)
        uu = (u or "").strip()
        ul = uu.lower().replace(" ", "")
        if ul in ("%", "pct", "percent"):
            return "%"
        if ul in ("twh",):
            return "TWh"
        if ul in ("gwh",):
            return "GWh"
        if ul in ("mwh",):
            return "MWh"
        if ul in ("kwh",):
            return "kWh"
        if ul in ("wh",):
            return "Wh"
        if ul in ("t", "trillion", "tn"):
            return "T"
        if ul in ("b", "bn", "billion"):
            return "B"
        if ul in ("m", "mn", "mio", "million"):
            return "M"
        if ul in ("k", "thousand", "000"):
            return "K"
        return uu

    def _safe_unit_family(unit_tag: str) -> str:
        try:
            fn = globals().get("unit_family")
            if callable(fn):
                return fn(unit_tag or "")
        except Exception:
            pass
        ut = (unit_tag or "").strip()
        if ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
            return "energy"
        if ut == "%":
            return "percent"
        if ut in ("T", "B", "M", "K"):
            return "magnitude"
        # currency not reliably derived here (handled elsewhere)
        return ""
    # =========================

    def infer_metric_dimension(metric_name: str, unit_raw: str) -> str:
        n = (metric_name or "").lower()
        u = (unit_raw or "").strip().lower()

        # Percent
        if "%" in u or "percent" in n or "share" in n or "cagr" in n:
            return "percent"

        # Currency signals
        currency_tokens = ["$", "s$", "usd", "sgd", "eur", "€", "gbp", "£", "jpy", "¥", "cny", "rmb", "aud", "cad"]
        if any(t in u for t in currency_tokens) or any(t in n for t in ["revenue", "market value", "valuation", "value (", "usd", "sgd", "eur"]):
            return "currency"

        # Unit sales / shipments
        unit_tokens = ["unit", "units", "sold", "sales volume", "shipments", "registrations", "deliveries", "vehicles", "pcs", "pieces", "volume"]
        if any(t in n for t in unit_tokens):
            return "unit_sales"

        # Pure counts
        if any(t in n for t in ["count", "number of", "install base", "installed base", "users", "subscribers"]) and "revenue" not in n:
            return "count"

        # Index / score
        if any(t in n for t in ["index", "score", "rating"]):
            return "index"

        return "unknown"

    def display_name_for_dimension(original_display: str, dim: str) -> str:
        if not original_display:
            return original_display

        od = original_display.strip()
        od_low = od.lower()

        if dim == "unit_sales":
            if "revenue" in od_low or "market value" in od_low or "valuation" in od_low:
                return re.sub(r"(?i)revenue|market value|valuation", "Unit Sales", od).strip()
            if od_low.startswith("sales"):
                return "Unit Sales" + od[len("Sales"):]
            if "sales" in od_low:
                return re.sub(r"(?i)sales", "Unit Sales", od).strip()
            return od

        if dim == "currency":
            if "unit sales" in od_low:
                return re.sub(r"(?i)unit sales", "Revenue", od).strip()
            return od

        if dim == "percent":
            if "unit sales" in od_low or "revenue" in od_low:
                return od
            return od

        return od

    candidates = []

    for key, metric in metrics.items():
        if not isinstance(metric, dict):
            continue

        original_name = metric.get("name", key)
        canonical_id, canonical_name = get_canonical_metric_id(original_name)

        # =========================
        # PATCH CM1 (ADDITIVE): registry-guided dimension hint
        # - If the canonical base metric is in METRIC_REGISTRY, use its unit_type
        #   as a strong prior for dimension classification.
        # - This reduces mislabel drift like "Revenue" being assigned as unit_sales
        #   (or vice-versa) purely from noisy LLM labels.
        #
        # NOTE (conflict fix, additive):
        # - Your prior code risked UnboundLocalError due to base_id scoping.
        # - We keep your legacy behavior, but guard it and define base_id upfront.
        # =========================

        registry_unit_type = ""

        # ---- PATCH CM1.A (ADDITIVE): define base_id upfront to prevent UnboundLocalError ----
        base_id = ""
        # -------------------------------------------------------------------------------

        try:
            # =========================
            # PATCH CM1.B (BUGFIX + ADDITIVE): registry base_id extraction
            # - canonical_id may contain underscores inside the base id (e.g., "market_size_2025")
            # - Find the LONGEST registry key that is a prefix of canonical_id.
            # =========================
            try:
                reg = globals().get("METRIC_REGISTRY")
                cid = str(canonical_id or "")
                if isinstance(reg, dict) and cid:
                    # choose the longest matching prefix key
                    for k in reg.keys():
                        ks = str(k)
                        if cid == ks or cid.startswith(ks + "_"):
                            if len(ks) > len(base_id):
                                base_id = ks

                    if base_id and isinstance(reg.get(base_id), dict):
                        registry_unit_type = (reg[base_id].get("unit_type") or "").strip().lower()
            except Exception:
                # keep safe defaults
                pass
            # =========================

            # -------------------------------------------------------------------
            # PATCH CM1.C (ADDITIVE): legacy code preserved, but guarded
            # - This block is redundant with CM1.B, but we keep it as requested.
            # - Guard prevents:
            #   (1) base_id undefined
            #   (2) overwriting registry_unit_type already computed above
            # -------------------------------------------------------------------
            if not registry_unit_type:
                reg = globals().get("METRIC_REGISTRY")
                if base_id and isinstance(reg, dict) and base_id in reg and isinstance(reg[base_id], dict):
                    registry_unit_type = (reg[base_id].get("unit_type") or "").strip().lower()
            # -------------------------------------------------------------------

        except Exception:
            registry_unit_type = ""

        # Map registry unit_type -> canonicalize_metrics dimension vocabulary
        # (keep it small + deterministic)
        if registry_unit_type:
            if registry_unit_type in ("currency",):
                registry_dim_hint = "currency"
            elif registry_unit_type in ("percentage", "percent"):
                registry_dim_hint = "percent"
            elif registry_unit_type in ("count",):
                # keep "unit_sales" vs "count" distinction:
                # registry says count; name-based inference decides "unit_sales" if it sees units/shipments/deliveries
                registry_dim_hint = "count"
            else:
                registry_dim_hint = ""
        else:
            registry_dim_hint = ""
        # =========================

        raw_unit = (metric.get("unit") or "").strip()

        # =========================
        # PATCH C2 (ADDITIVE): compute unit_tag/unit_family without changing existing unit behavior
        # - We keep your existing unit_norm logic for backwards compatibility.
        # - But we ALSO attach unit_tag + unit_family so downstream can gate deterministically.
        # =========================
        unit_tag = metric.get("unit_tag") or _safe_normalize_unit_tag(raw_unit)
        unit_family_tag = metric.get("unit_family") or _safe_unit_family(unit_tag)
        # =========================

        unit_norm = raw_unit.upper()  # keep original behavior (do not change)
        dim = infer_metric_dimension(str(original_name), raw_unit)

        # =========================
        # PATCH CM1 (ADDITIVE): apply registry hint as override / guardrail
        # - If registry says currency/percent, force that dimension.
        # - If registry says count, prevent accidental "currency"/"percent".
        # =========================
        if registry_dim_hint in ("currency", "percent"):
            dim = registry_dim_hint
        elif registry_dim_hint == "count":
            # Allow unit_sales if name clearly indicates it; else keep "count"
            if dim in ("currency", "percent"):
                dim = "count"
        # =========================

        canonical_key = f"{canonical_id}__{dim}"

        parsed_val = parse_to_float(metric.get("value"))
        value_for_sort = parsed_val if parsed_val is not None else str(metric.get("value", ""))

        stable_sort_key = (
            str(original_name).lower().strip(),
            dim,
            unit_norm,
            str(value_for_sort),
            str(key),
        )

        geo = infer_geo_scope(
            str(original_name),
            str(metric.get("context_snippet", "")),
            str(metric.get("source", "")),
            str(metric.get("source_url", "")),
        )

        proxy = infer_proxy_label(
            str(original_name),
            str(question_text),
            str(category_hint),
            str(metric.get("context_snippet", "")),
            str(metric.get("source", "")),
            str(metric.get("source_url", "")),
        )

        # =========================
        # PATCH C3 (ADDITIVE): canonicalize numeric fields on the candidate metric dict
        # - If canonicalize_numeric_candidate exists, it will attach:
        #   unit_tag/unit_family/base_unit/multiplier_to_base/value_norm
        # - If not, we attach minimal fields ourselves (still additive).
        # =========================
        metric_enriched = dict(metric)  # never mutate caller's dict
        try:
            fn_can = globals().get("canonicalize_numeric_candidate")
            if callable(fn_can):
                metric_enriched = fn_can(metric_enriched)
        except Exception:
            pass

        # Ensure minimal canonical fields exist (additive)
        metric_enriched.setdefault("unit_tag", unit_tag)
        metric_enriched.setdefault("unit_family", unit_family_tag)
        # =========================

        candidates.append({
            "canonical_id": canonical_id,
            "canonical_key": canonical_key,
            "canonical_name": display_name_for_dimension(canonical_name, dim),
            "original_name": original_name,

            # NOTE: store enriched metric
            "metric": metric_enriched,

            "unit": unit_norm,
            "parsed_val": parsed_val,
            "dimension": dim,
            "stable_sort_key": stable_sort_key,
            "geo_scope": geo["geo_scope"],
            "geo_name": geo["geo_name"],
            **proxy,
        })

    candidates.sort(key=lambda x: x["stable_sort_key"])

    grouped: Dict[str, List[Dict]] = {}
    for c in candidates:
        grouped.setdefault(c["canonical_key"], []).append(c)

    canonicalized: Dict[str, Dict] = {}

    for ckey, group in grouped.items():
        if len(group) == 1 or not merge_duplicates_to_range:
            g = group[0]
            m = g["metric"]

            # =========================
            # PATCH C4 (ADDITIVE): keep canonical numeric & semantic fields on output row
            # (only adds keys; does not remove/rename existing keys)
            # =========================
            out_row = {
                **m,
                "name": g["canonical_name"],
                "canonical_id": g["canonical_id"],
                "canonical_key": ckey,
                "dimension": g["dimension"],
                "original_name": g["original_name"],
                "geo_scope": g.get("geo_scope", "unknown"),
                "geo_name": g.get("geo_name", ""),
                "is_proxy": bool(g.get("is_proxy", False)),
                "proxy_type": g.get("proxy_type", ""),
                "proxy_reason": g.get("proxy_reason", ""),
                "proxy_confidence": float(g.get("proxy_confidence", 0.0) or 0.0),
                "proxy_target": g.get("proxy_target", ""),
            }
            # Ensure these exist if upstream provided them
            for k in ["anchor_hash", "source_url", "context_snippet", "measure_kind", "measure_assoc",
                      "unit_tag", "unit_family", "base_unit", "multiplier_to_base", "value_norm"]:
                if k in m and k not in out_row:
                    out_row[k] = m.get(k)
            canonicalized[ckey] = out_row
            # =========================
            continue

        # Merge duplicates within SAME dimension-safe canonical_key
        base = group[0]
        base_metric = dict(base["metric"])
        base_metric["name"] = base["canonical_name"]
        base_metric["canonical_id"] = base["canonical_id"]
        base_metric["canonical_key"] = ckey
        base_metric["dimension"] = base["dimension"]

        geo_scope, geo_name = merge_group_geo(group)
        base_metric["geo_scope"] = geo_scope
        base_metric["geo_name"] = geo_name

        merged_proxy = merge_group_proxy(group)
        base_metric.update(merged_proxy)

        vals = [g["parsed_val"] for g in group if g["parsed_val"] is not None]
        raw_vals = [str(g["metric"].get("value", "")) for g in group]
        orig_names = [g["original_name"] for g in group]

        units = [g["unit"] for g in group if g["unit"]]
        unit_base = units[0] if units else (base_metric.get("unit") or "")
        base_metric["unit"] = unit_base

        base_metric["original_names"] = orig_names
        base_metric["raw_values"] = raw_vals

        # =========================
        # PATCH C5 (ADDITIVE): optional canonical range using value_norm if present
        # - Keeps your existing "range" untouched.
        # - Adds "range_norm" when we can compute it.
        # =========================
        vals_norm = []
        for g in group:
            mm = g.get("metric") if isinstance(g, dict) else {}
            if isinstance(mm, dict) and mm.get("value_norm") is not None:
                try:
                    vals_norm.append(float(mm.get("value_norm")))
                except Exception:
                    pass
        # =========================

        # =====================================================================
        # PATCH ANCHOR_VAL1 (ADDITIVE): set metric value from selected evidence candidate
        # Why:
        # - Avoid median/aggregate drift between analysis and evolution.
        # - If we already chose a specific evidence candidate (candidate_id/anchor_hash),
        #   that candidate should determine the metric's reported value/value_norm/unit.
        # Determinism:
        # - Select the evidence row with highest confidence if present, else first.
        # - No re-fetching, no new extraction; uses existing evidence payload only.
        # =====================================================================
        _anchored_value_set = False
        try:
            _ev = base_metric.get("evidence")
            if isinstance(_ev, list) and _ev:
                # pick best evidence deterministically
                def _ev_score(e):
                    try:
                        c = e.get("confidence")
                        return float(c) if c is not None else 0.0
                    except Exception:
                        return 0.0
                _ev_sorted = sorted([e for e in _ev if isinstance(e, dict)], key=_ev_score, reverse=True)
                _best = _ev_sorted[0] if _ev_sorted else None

                if isinstance(_best, dict):
                    # Prefer canonical normalized fields if present
                    _bn = _best.get("value_norm")
                    _bu = _best.get("base_unit") or _best.get("unit")
                    _rawv = _best.get("raw") if _best.get("raw") is not None else _best.get("value")

                    if _bn is not None:
                        try:
                            base_metric["value_norm"] = float(_bn)
                        except Exception:
                            pass

                    # Preserve unit/base_unit
                    if _bu:
                        try:
                            base_metric["base_unit"] = str(_bu)
                        except Exception:
                            pass
                    if _best.get("unit"):
                        base_metric["unit"] = _best.get("unit")

                    # Preserve raw/value display from evidence (preferred)
                    if _rawv is not None:
                        base_metric["raw"] = _rawv
                        base_metric["value"] = _rawv

                    # Helpful debug: show that we anchored value from evidence
                    base_metric.setdefault("debug", {})
                    if isinstance(base_metric.get("debug"), dict):
                        base_metric["debug"]["value_origin"] = "evidence_best_candidate"
                        base_metric["debug"]["evidence_candidate_id"] = _best.get("candidate_id") or _best.get("anchor_hash")
                    _anchored_value_set = True
        except Exception:
            pass
        if vals and not _anchored_value_set:

            vals_sorted = sorted(vals)
            vmin, vmax = vals_sorted[0], vals_sorted[-1]
            vmed = vals_sorted[len(vals_sorted) // 2]
            base_metric["value"] = vmed
            base_metric["range"] = {
                "min": vmin,
                "max": vmax,
                "candidates": vals_sorted,
                "n": len(vals_sorted),
            }
        else:
            base_metric["range"] = {"min": None, "max": None, "candidates": [], "n": 0}

        if len(vals_norm) >= 2:
            vn = sorted(vals_norm)
            base_metric["range_norm"] = {
                "min": vn[0],
                "max": vn[-1],
                "candidates": vn,
                "n": len(vn),
                "unit": base_metric.get("base_unit") or base_metric.get("unit") or "",
            }
        # =========================

        canonicalized[ckey] = base_metric

    return canonicalized



def freeze_metric_schema(canonical_metrics: Dict) -> Dict:
    """
    Lock metric identity + expected schema for future evolution.

    Key fix:
      - Stores canonical_key (dimension-safe)
      - Stores dimension + unit family
      - Keywords include dimension hints to improve later matching
    """
    frozen = {}
    if not isinstance(canonical_metrics, dict):
        return frozen

    # =========================
    # PATCH F1 (ADDITIVE): prefer shared normalize_unit_tag/unit_family helpers if present
    # This improves consistency with extractor + attribution gating.
    # Falls back safely to old heuristics.
    # =========================
    def _normalize_unit_safe(u: str) -> str:
        try:
            fn = globals().get("normalize_unit_tag")
            if callable(fn):
                return fn(u or "")
        except Exception:
            pass
        return (u or "").strip()

    def _unit_family_safe(unit_raw: str, dim_hint: str = "") -> str:
        # 1) dimension-first (strongest signal)
        d = (dim_hint or "").strip().lower()
        if d in ("percent", "pct"):
            return "percent"
        if d in ("currency",):
            return "currency"
        if d in ("energy",):
            return "energy"
        if d in ("unit_sales", "count"):
            # You’ve been treating M/B/T as “magnitude” for counts; keep aligned.
            return "magnitude"
        if d in ("index", "score"):
            return "index"

        # 2) if you already have a unit-family helper in the codebase, use it
        try:
            fn = globals().get("unit_family")
            if callable(fn):
                uf = fn(_normalize_unit_safe(unit_raw))
                if isinstance(uf, str) and uf.strip():
                    return uf.strip().lower()
        except Exception:
            pass

        # 3) fallback to old heuristic (your original logic)
        u = (unit_raw or "").strip().lower()
        if not u:
            return "unknown"
        if "%" in u:
            return "percent"
        if any(t in u for t in ["$", "s$", "usd", "sgd", "eur", "€", "gbp", "£", "jpy", "¥", "cny", "rmb"]):
            return "currency"
        if any(t in u for t in ["b", "bn", "billion", "m", "mn", "million", "k", "thousand", "t", "trillion"]):
            return "magnitude"
        return "other"
    # =========================

    for ckey, m in canonical_metrics.items():
        if not isinstance(m, dict):
            continue

        dim = (m.get("dimension") or "").strip() or "unknown"
        name = m.get("name")
        unit = (m.get("unit") or "").strip()

        # =========================
        # PATCH F2 (ADDITIVE): compute unit_family using dimension-first logic
        # =========================
        uf = _unit_family_safe(unit, dim_hint=dim)
        # =========================

        # Keywords: name + dimension token to prevent cross-dimension matches later
        kws = extract_context_keywords(name or "") or []
        if dim and dim not in kws:
            kws.append(dim)
        if uf and uf not in kws:
            kws.append(uf)

        # =========================
        # PATCH F3 (ADDITIVE): preserve schema unit more safely
        # - Keep your existing behavior in 'unit' (backward compatible),
        #   BUT also add 'unit_tag' which is the canonicalized unit used downstream.
        # - This avoids the "SGD -> S" schema corruption that breaks currency gating.
        # =========================
        unit_tag = _normalize_unit_safe(unit)
        # Keep existing 'unit' output to avoid breaking consumers:
        unit_out = unit_clean_first_letter(unit.upper())
        # =========================

        frozen[ckey] = {
            "canonical_key": ckey,
            "canonical_id": m.get("canonical_id") or ckey.split("__", 1)[0],
            "dimension": dim,
            "name": name,

            # Existing field kept exactly (backward compatible)
            "unit": unit_out,

            # =========================
            # PATCH F3 (ADDITIVE): extra stable fields (non-breaking additions)
            # =========================
            "unit_tag": unit_tag,          # e.g., "%", "M", "B", "TWh"
            "unit_family": uf,             # e.g., "currency", "percent", "magnitude"
            # =========================

            "keywords": kws[:30],
        }

    return frozen


# =========================================================
# RANGE + SOURCE ATTRIBUTION (DETERMINISTIC, NO LLM)
# =========================================================

def stable_json_hash(obj: Any) -> str:
    import hashlib, json
    try:
        s = json.dumps(obj, sort_keys=True, ensure_ascii=False, separators=(",", ":"))
    except Exception:
        s = str(obj)
    return hashlib.sha256(s.encode("utf-8", errors="ignore")).hexdigest()

def make_extracted_number_id(source_url: str, fingerprint: str, n: Dict) -> str:
    payload = {
        "url": source_url or "",
        "fp": fingerprint or "",
        "start": n.get("start_idx"),
        "end": n.get("end_idx"),
        "value": n.get("value"),
        "unit": normalize_unit(n.get("unit") or ""),
        "raw": n.get("raw") or "",
        "ctx": " ".join((n.get("context_snippet") or "").split())[:240],
    }
    return stable_json_hash(payload)

def sort_snapshot_numbers(numbers: List[Dict]) -> List[Dict]:
    """
    Deterministic ordering for extracted_numbers in snapshots.

    Backward compatible + robust:
      - Uses start/end idx when present
      - Avoids hard dependency on normalize_unit() (may not exist)
      - Falls back to normalize_unit_tag() if available
    """

    # =========================
    # PATCH SS1 (ADDITIVE): safe unit normalizer
    # - Prefer normalize_unit() if it exists
    # - Else fall back to normalize_unit_tag() if present
    # - Else just return stripped unit
    # =========================
    _norm_unit_fn = globals().get("normalize_unit")
    _norm_tag_fn = globals().get("normalize_unit_tag")

    def _safe_norm_unit(u: str) -> str:
        u = (u or "").strip()
        try:
            if callable(_norm_unit_fn):
                return str(_norm_unit_fn(u) or "")
        except Exception:
            pass
        try:
            if callable(_norm_tag_fn):
                # normalize_unit_tag expects tags / unit-ish strings; still better than raw
                return str(_norm_tag_fn(u) or "")
        except Exception:
            pass
        return u
    # =========================

    def k(n: Dict[str, Any]):
        n = n or {}
        return (
            n.get("start_idx") if isinstance(n.get("start_idx"), int) else 10**18,
            n.get("end_idx") if isinstance(n.get("end_idx"), int) else 10**18,

            # stable identity ordering
            str(n.get("anchor_hash") or ""),

            # unit + value
            _safe_norm_unit(str(n.get("unit") or "")),
            str(n.get("unit_tag") or ""),
            str(n.get("value_norm") if n.get("value_norm") is not None else n.get("value")),

            # final tie-breakers
            str(n.get("raw") or ""),
            str(n.get("context_snippet") or n.get("context") or "")[:80],
        )

    return sorted((numbers or []), key=k)

def sort_evidence_records(records: List[Dict]) -> List[Dict]:
    """
    Deterministic ordering for evidence_records.

    Backward compatible:
      - Uses url + fingerprint (as you had)
      - Adds fetched_at as tie-breaker if present (non-breaking)
    """

    # =========================
    # PATCH SE1 (ADDITIVE): add fetched_at tie-breaker (optional)
    # =========================
    def k(r: Dict[str, Any]):
        r = r or {}
        return (
            str(r.get("url") or ""),
            str(r.get("fingerprint") or ""),
            str(r.get("fetched_at") or ""),
        )
    # =========================

    return sorted((records or []), key=k)

def sort_metric_anchors(anchors: List[Dict]) -> List[Dict]:
    # =========================
    # PATCH MA2 (ADDITIVE): canonical-first stable sort
    # - Prefer canonical_key (new)
    # - Fall back to metric_id/metric_name (legacy)
    # =========================
    return sorted(
        (anchors or []),
        key=lambda a: (
            str((a or {}).get("canonical_key") or ""),
            str((a or {}).get("metric_id") or ""),
            str((a or {}).get("metric_name") or ""),
            str((a or {}).get("source_url") or ""),
        ),
    )


def normalize_unit(unit: str) -> str:
    """
    Deterministic unit normalizer used across analysis/evolution.

    Goals:
    - Preserve domain units like TWh/GWh/MWh/kWh (do NOT collapse to T/M/etc.)
    - Normalize magnitude suffixes case-insensitively: b/m/t/k -> B/M/T/K
    - Normalize percent consistently to "%"
    - Avoid clever heuristics; only normalize when confidently recognized
    """
    if not unit:
        return ""

    u0 = str(unit).strip()
    if not u0:
        return ""

    ul = u0.strip().lower().replace(" ", "")

    # --- Domain energy units (short-circuit, must be first) ---
    # Normalize casing to canonical display forms
    if "twh" == ul or ul.endswith("twh"):
        return "TWh"
    if "gwh" == ul or ul.endswith("gwh"):
        return "GWh"
    if "mwh" == ul or ul.endswith("mwh"):
        return "MWh"
    if "kwh" == ul or ul.endswith("kwh"):
        return "kWh"
    if ul == "wh":
        return "Wh"

    # --- Percent ---
    if ul in ("%", "percent", "pct"):
        return "%"

    # --- Currency prefixes/symbols (do not try to infer currency codes here) ---
    # Keep currency detection elsewhere; unit here is for magnitude tags.
    # If unit is literally "usd"/"$" etc, strip to empty.
    if ul in ("$", "usd", "sgd", "eur", "gbp", "aud", "cad", "jpy", "cny", "rmb"):
        return ""

    # --- Magnitude tags (case-insensitive) ---
    # IMPORTANT: handle single-letter forms used by extractor ("m", "b", "t", "k")
    if ul in ("trillion", "tn", "t"):
        return "T"
    if ul in ("billion", "bn", "b"):
        return "B"
    if ul in ("million", "mn", "mio", "m"):
        return "M"
    if ul in ("thousand", "k", "000"):
        return "K"

    # Unknown: return original trimmed (preserve domain-specific tokens)
    return u0.strip()




def to_billions(value: float, unit_tag: str) -> Optional[float]:
    """Convert T/B/M tagged values into billions. Leaves % unchanged (returns None for % here)."""
    try:
        v = float(value)
    except Exception:
        return None

    if unit_tag == "T":
        return v * 1000.0
    if unit_tag == "B":
        return v
    if unit_tag == "M":
        return v / 1000.0
    return None


def build_metric_keywords(metric_name: str) -> List[str]:
    """Reuse your existing keyword extractor, but ensure we always have something."""
    kws = extract_context_keywords(metric_name) or []
    # Add simple fallback tokens (deterministic)
    for t in re.findall(r"[a-zA-Z]{4,}", str(metric_name).lower()):
        if t not in kws:
            kws.append(t)
    return kws[:25]


def extract_numbers_from_scraped_sources(
    scraped_content: Dict[str, str],
) -> List[Dict[str, Any]]:
    """
    Deterministically extract numeric candidates from all scraped source texts.
    Returns list of {url, value, unit_tag, raw, context}.
    """
    candidates: List[Dict[str, Any]] = []
    if not isinstance(scraped_content, dict):
        return candidates

    for url, content in scraped_content.items():
        if not content or not isinstance(content, str) or len(content) < 200:
            continue

        # =========================
        # PATCH 1 (ADDITIVE): pass source_url through (improves anchor stability)
        # =========================
        nums = extract_numbers_with_context(content, source_url=url)
        # =========================

        for n in nums:
            # =========================
            # PATCH 1 (ADDITIVE): prefer extractor-provided unit_tag if present; else normalize
            # =========================
            unit_tag = n.get("unit_tag")
            if not unit_tag:
                unit_tag = normalize_unit_tag(n.get("unit", ""))
            # =========================

            row = {
                "url": url,
                "value": n.get("value"),
                "unit_tag": unit_tag,
                "raw": n.get("raw", ""),
                "context": (n.get("context") or ""),
            }

            # =========================
            # PATCH 3 (ADDITIVE): preserve measure association tags if extractor provides them
            # =========================
            if "measure_kind" in n:
                row["measure_kind"] = n.get("measure_kind")
            if "measure_assoc" in n:
                row["measure_assoc"] = n.get("measure_assoc")
            # =========================

            # =========================
            # PATCH 1 (ADDITIVE): preserve extra fields if extractor provides them
            # (backwards compatible: we only add keys, never remove)
            # =========================
            for k in [
                "unit", "is_junk", "junk_reason", "anchor_hash",
                "start_idx", "end_idx", "context_snippet",
                "unit_family", "base_unit", "multiplier_to_base", "value_norm"
            ]:
                if k in n:
                    row[k] = n.get(k)
            # =========================

            # ============================================================
            # PATCH 9 (ADDITIVE): enforce canonical numeric fields uniformly
            # Why:
            #   - Some candidates may not carry unit_family/base_unit/value_norm yet
            #   - We want every candidate (analysis + evolution) to have the same
            #     canonical fields so diff + span logic is stable and drift-free.
            #
            # This is additive and safe to call multiple times.
            # ============================================================
            try:
                fn_can = globals().get("canonicalize_numeric_candidate")
                if callable(fn_can):
                    row = fn_can(row) or row
                else:
                    row = canonicalize_numeric_candidate(row) or row
            except Exception:
                pass

            # --- ADDITIVE: ensure canonical keys exist even if canonicalize failed ---
            row.setdefault("unit_family", unit_family(row.get("unit_tag", "") or ""))
            row.setdefault("base_unit", row.get("unit_tag", "") or "")
            row.setdefault("multiplier_to_base", 1.0)
            if row.get("value") is not None and row.get("value_norm") is None:
                try:
                    row["value_norm"] = float(row.get("value"))
                except Exception:
                    pass
            # ------------------------------------------------------------------------
            # ============================================================

            candidates.append(row)

    return candidates


def attribute_span_to_sources(
    metric_name: str,
    metric_unit: str,
    scraped_content: Dict[str, str],
    rel_tol: float = 0.08,
    # =========================
    # PATCH S1 (ADDITIVE): optional schema inputs (non-breaking)
    # - If provided, we enforce schema-first gating for drift stability.
    # - If not provided, we fall back to existing heuristic behavior.
    # =========================
    canonical_key: str = "",
    metric_schema: Dict[str, Any] = None,
    # =========================
) -> Dict[str, Any]:
    """
    Build a deterministic span (min/mid/max) for a metric, and attribute min/max to sources.
    Uses only scraped content + regex extractions (NO LLM).

    Schema-first behavior (when metric_schema/canonical_key provided):
      - Enforces unit_family and currency/count/percent gating from frozen schema
      - Uses measure_kind tags when available to avoid semantic leakage
      - Keeps deterministic tie-breaking
    """
    import re
    import hashlib

    unit_tag_hint = normalize_unit_tag(metric_unit)
    keywords = build_metric_keywords(metric_name)

    all_candidates = extract_numbers_from_scraped_sources(scraped_content)
    filtered: List[Dict[str, Any]] = []

    metric_l = (metric_name or "").lower()

    # =========================
    # PATCH S2 (ADDITIVE): resolve schema entry (if available)
    # =========================
    schema_entry = None
    if isinstance(metric_schema, dict) and canonical_key and isinstance(metric_schema.get(canonical_key), dict):
        schema_entry = metric_schema.get(canonical_key)
    # =========================

    # =========================
    # PATCH S3 (ADDITIVE): schema-derived expectations with safe fallbacks
    # =========================
    schema_unit_family = ""
    schema_dimension = ""
    schema_unit = ""
    if isinstance(schema_entry, dict):
        schema_unit_family = (schema_entry.get("unit_family") or "").strip().lower()
        schema_dimension = (schema_entry.get("dimension") or "").strip().lower()
        schema_unit = (schema_entry.get("unit") or "").strip()

    expected_family = ""
    if schema_unit_family in ("percent", "currency", "energy"):
        expected_family = schema_unit_family
    if not expected_family:
        ut = normalize_unit_tag(metric_unit)
        if ut == "%":
            expected_family = "percent"
        elif ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
            expected_family = "energy"
        else:
            expected_family = ""

    currencyish = False
    if schema_unit_family == "currency" or schema_dimension == "currency":
        currencyish = True
    if not currencyish:
        mu = (metric_unit or "").lower()
        if any(x in mu for x in ["usd", "sgd", "eur", "gbp", "$", "s$", "€", "£", "aud", "cad", "jpy", "cny", "rmb"]):
            currencyish = True
    if not currencyish and any(x in metric_l for x in ["revenue", "turnover", "valuation", "market value", "market size",
                                                       "profit", "earnings", "ebitda", "capex", "opex"]):
        currencyish = True
    # =========================

    # =========================
    # PATCH S4 (ADDITIVE): expected measure_kind (schema-first with fallback)
    # =========================
    expected_kind = None

    if expected_family == "percent":
        if any(k in metric_l for k in ["growth", "cagr", "increase", "decrease", "yoy", "qoq", "mom", "rate"]):
            expected_kind = "growth_pct"
        else:
            expected_kind = "share_pct"

    if currencyish:
        expected_kind = "money"

    if expected_kind is None and any(k in metric_l for k in [
        "units", "unit sales", "vehicle sales", "vehicles sold", "sold", "sales volume",
        "deliveries", "shipments", "registrations", "volume"
    ]):
        expected_kind = "count_units"
    # =========================

    # =========================
    # PATCH S5 (ADDITIVE): year-ish suppression helpers (unchanged behavior)
    # =========================
    metric_is_yearish = any(k in metric_l for k in ["year", "years", "fy", "fiscal", "calendar", "timeline", "target year"])

    def _looks_like_year_value(v) -> bool:
        try:
            iv = int(float(v))
            return 1900 <= iv <= 2099
        except Exception:
            return False

    def _ctx_has_year_range(ctx: str) -> bool:
        return bool(re.search(r"\b(19|20)\d{2}\s*(?:-|–|—|to)\s*(19|20)\d{2}\b", ctx or "", flags=re.I))
    # =========================

    # =========================
    # PATCH S6 (ADDITIVE): currency evidence check (used only when currencyish)
    # =========================
    def _has_currency_evidence(raw: str, ctx: str) -> bool:
        r = (raw or "")
        c = (ctx or "").lower()

        if any(s in r for s in ["$", "S$", "€", "£"]):
            return True
        if any(code in c for code in [" usd", "sgd", " eur", " gbp", " aud", " cad", " jpy", " cny", " rmb"]):
            return True

        strong_kw = [
            "revenue", "turnover", "valuation", "valued at", "market value", "market size",
            "sales value", "net profit", "operating profit", "gross profit",
            "ebitda", "earnings", "income", "capex", "opex"
        ]
        if any(k in c for k in strong_kw):
            return True
        return False
    # =========================

    # =========================================================================
    # PATCH S11 (ADDITIVE): deterministic candidate_id for tie-breaking
    # - Stable across runs, depends only on stable fields
    # - Used ONLY as final tie-breaker (won't change non-tie outcomes)
    # =========================================================================
    def _candidate_id(x: dict) -> str:
        try:
            url = str(x.get("url") or x.get("source_url") or "")
            ah = str(x.get("anchor_hash") or "")
            vn = x.get("value_norm")
            bu = str(x.get("base_unit") or x.get("unit") or x.get("unit_tag") or "")
            mk = str(x.get("measure_kind") or "")
            # normalize numeric string for stability
            vn_s = ""
            if vn is not None:
                try:
                    vn_s = f"{float(vn):.12g}"
                except Exception:
                    vn_s = str(vn)
            s = f"{url}|{ah}|{vn_s}|{bu}|{mk}"
            return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()
        except Exception:
            return ""
    # =========================================================================

    for c in all_candidates:
        ctx = c.get("context", "")
        if not ctx:
            continue

        if c.get("is_junk") is True:
            continue

        if not metric_is_yearish:
            if (c.get("unit_tag") in ("", None)) and _looks_like_year_value(c.get("value")):
                continue
            if _looks_like_year_value(c.get("value")) and _ctx_has_year_range(ctx):
                continue

        ctx_score = calculate_context_match(keywords, ctx)
        if ctx_score <= 0.0:
            continue

        cand_ut = c.get("unit_tag") or normalize_unit_tag(c.get("unit") or "")
        cand_fam = (c.get("unit_family") or unit_family(cand_ut) or "").strip().lower()

        if expected_family:
            if expected_family == "percent" and cand_fam != "percent":
                continue
            if expected_family == "currency":
                if cand_fam not in ("currency", "magnitude"):
                    continue
                if not _has_currency_evidence(c.get("raw", ""), ctx):
                    continue
            if expected_family == "energy" and cand_fam != "energy":
                continue

        if expected_kind:
            mk = c.get("measure_kind")
            if mk and mk != expected_kind:
                continue

        val_norm = None
        if expected_family == "percent" or unit_tag_hint == "%":
            if cand_ut != "%":
                continue
            val_norm = c.get("value")

        elif expected_family == "energy":
            val_norm = c.get("value_norm")
            if val_norm is None:
                val_norm = c.get("value")

        elif currencyish or expected_family == "currency":
            if c.get("measure_kind") == "count_units":
                continue
            if cand_ut not in ("T", "B", "M"):
                continue
            val_norm = to_billions(c.get("value"), cand_ut)
            if val_norm is None:
                continue

        else:
            try:
                val_norm = float(c.get("value"))
            except Exception:
                continue

        row = {
            **c,
            "unit_tag": cand_ut,
            "unit_family": cand_fam,
            "value_norm": val_norm,
            "ctx_score": float(ctx_score),
        }

        # =========================
        # PATCH S11 (ADDITIVE): attach candidate_id (safe extra field)
        # =========================
        row.setdefault("candidate_id", _candidate_id(row))
        # =========================

        filtered.append(row)

    if not filtered:
        return {
            "span": None,
            "source_attribution": None,
            "evidence": []
        }

    # Deterministic selection: value_norm then ctx_score then url then candidate_id
    # =========================================================================
    # PATCH S12 (ADDITIVE): candidate_id as final tie-breaker
    # =========================================================================
    def min_key(x):
        return (
            float(x["value_norm"]),
            -float(x["ctx_score"]),
            str(x.get("url", "")),
            str(x.get("candidate_id", "")),
        )

    def max_key(x):
        return (
            -float(x["value_norm"]),
            -float(x["ctx_score"]),
            str(x.get("url", "")),
            str(x.get("candidate_id", "")),
        )
    # =========================================================================

    min_item = sorted(filtered, key=min_key)[0]
    max_item = sorted(filtered, key=max_key)[0]

    vmin = float(min_item["value_norm"])
    vmax = float(max_item["value_norm"])
    vmid = (vmin + vmax) / 2.0

    if expected_family == "percent" or unit_tag_hint == "%":
        unit_out = "%"
    elif currencyish or expected_family == "currency":
        unit_out = "billion USD"
    elif expected_family == "energy":
        unit_out = "Wh"
    else:
        unit_out = metric_unit or (schema_unit or "")

    evidence = []
    for it in sorted(filtered, key=lambda x: (-float(x["ctx_score"]), str(x.get("url", "")), str(x.get("candidate_id", ""))))[:12]:
        evidence.append({
            "url": it.get("url"),
            "raw": it.get("raw"),
            "unit_tag": it.get("unit_tag"),
            "unit_family": it.get("unit_family"),
            "measure_kind": it.get("measure_kind"),
            "measure_assoc": it.get("measure_assoc"),
            "value_norm": it.get("value_norm"),
            "candidate_id": it.get("candidate_id"),
            # PATCH EVID_AH1 (ADDITIVE): carry anchor_hash for evolution matching
            "anchor_hash": it.get("anchor_hash"),
            # PATCH EVID_AH2 (ADDITIVE): carry stable span fields when present
            "start_idx": it.get("start_idx"),
            "end_idx": it.get("end_idx"),
            # PATCH EVID_AH3 (ADDITIVE): carry normalized value basis when present
            "value_norm": it.get("value_norm"),
            "base_unit": it.get("base_unit"),
            "multiplier_to_base": it.get("multiplier_to_base"),  # PATCH S11: exposed for transparency
            "context_snippet": (it.get("context") or "")[:220],
            "context_score": round(float(it.get("ctx_score", 0.0)) * 100, 1),
        })

    return {
        "span": {
            "min": round(vmin, 4),
            "mid": round(vmid, 4),
            "max": round(vmax, 4),
            "unit": unit_out
        },
        "source_attribution": {
            "min": {
                "url": min_item.get("url"),
                "raw": min_item.get("raw"),
                "measure_kind": min_item.get("measure_kind"),
                "measure_assoc": min_item.get("measure_assoc"),
                "value_norm": min_item.get("value_norm"),
                "candidate_id": min_item.get("candidate_id"),  # PATCH S11
                "context_snippet": (min_item.get("context") or "")[:220],
                "context_score": round(float(min_item.get("ctx_score", 0.0)) * 100, 1),
            },
            "max": {
                "url": max_item.get("url"),
                "raw": max_item.get("raw"),
                "measure_kind": max_item.get("measure_kind"),
                "measure_assoc": max_item.get("measure_assoc"),
                "value_norm": max_item.get("value_norm"),
                "candidate_id": max_item.get("candidate_id"),  # PATCH S11
                "context_snippet": (max_item.get("context") or "")[:220],
                "context_score": round(float(max_item.get("ctx_score", 0.0)) * 100, 1),
            }
        },
        "evidence": evidence
    }



def add_range_and_source_attribution_to_canonical_metrics(
    canonical_metrics: Dict[str, Any],
    web_context: dict,
    # =========================
    # PATCH R1 (ADDITIVE): optional schema-first inputs
    # If provided, attribution uses frozen schema to avoid semantic/unit leakage.
    # =========================
    metric_schema: Dict[str, Any] = None,
    # =========================
) -> Dict[str, Any]:
    """
    Enrich canonical metrics with deterministic range + source attribution.

    IMPORTANT:
    - canonical_metrics is expected to be keyed by canonical_key (dimension-safe),
      i.e. the output of canonicalize_metrics().
    - Schema-first mode (recommended): pass metric_schema=metric_schema_frozen so
      attribute_span_to_sources() can enforce unit_family / measure_kind gates.
    - Backward compatible: if metric_schema not provided, attribution falls back
      to existing heuristic behavior inside attribute_span_to_sources().
    """
    enriched: Dict[str, Any] = {}
    if not isinstance(canonical_metrics, dict):
        return enriched

    scraped = (web_context or {}).get("scraped_content") or {}
    if not isinstance(scraped, dict):
        scraped = {}

    # =========================
    # PATCH R2 (ADDITIVE): resolve schema dict safely
    # =========================
    schema = metric_schema if isinstance(metric_schema, dict) else {}
    # =========================

    for ckey, m in canonical_metrics.items():
        if not isinstance(m, dict):
            continue

        metric_name = m.get("name") or m.get("original_name") or str(ckey)
        metric_unit = m.get("unit") or ""

        # =========================
        # PATCH R3 (BUGFIX): schema-first wiring (no undefined prev_response/ckey)
        # - canonical_key is the dict key (ckey)
        # - metric_schema is the frozen schema dict (if provided)
        # =========================
        span_pack = attribute_span_to_sources(
            metric_name=metric_name,
            metric_unit=metric_unit,
            scraped_content=scraped,
            canonical_key=str(ckey),
            metric_schema=schema,
        )
        # =========================

        mm = dict(m)

        # Preserve old behavior: only add keys (don’t remove anything)
        if isinstance(span_pack, dict):
            if span_pack.get("span") is not None:
                mm["source_span"] = span_pack.get("span")
            if span_pack.get("source_attribution") is not None:
                mm["source_attribution"] = span_pack.get("source_attribution")
            if span_pack.get("evidence") is not None:
                mm["evidence"] = span_pack.get("evidence")

        enriched[ckey] = mm

    return enriched





# ------------------------------------
# SEMANTIC FINDING HASH
# Removes wording-based churn from findings comparison
# ------------------------------------

# Semantic components to extract from findings
FINDING_PATTERNS = {
    # Growth/decline patterns
    "growth": [
        r'(?:grow(?:ing|th)?|increas(?:e|ing)|expand(?:ing)?|ris(?:e|ing)|up)\s*(?:by|at|of)?\s*(\d+(?:\.\d+)?)\s*%?',
        r'(\d+(?:\.\d+)?)\s*%?\s*(?:growth|increase|expansion|rise)',
    ],
    "decline": [
        r'(?:declin(?:e|ing)|decreas(?:e|ing)|fall(?:ing)?|drop(?:ping)?|down)\s*(?:by|at|of)?\s*(\d+(?:\.\d+)?)\s*%?',
        r'(\d+(?:\.\d+)?)\s*%?\s*(?:decline|decrease|drop|fall)',
    ],

    # Value patterns
    "value": [
        r'\$\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)?',
        r'(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)',
    ],

    # Ranking patterns
    "rank": [
        r'(?:lead(?:ing|er)?|top|first|largest|biggest|#1|number one)',
        r'(?:second|#2|runner.?up)',
        r'(?:third|#3)',
    ],

    # Trend patterns
    "trend_up": [
        r'(?:bullish|optimistic|positive|strong|robust|accelerat)',
    ],
    "trend_down": [
        r'(?:bearish|pessimistic|negative|weak|slow(?:ing)?|decelerat)',
    ],

    # Entity patterns (will be filled dynamically)
    "entities": []
}

# Common stop words to remove
STOP_WORDS = {
    'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
    'should', 'may', 'might', 'must', 'shall', 'can', 'to', 'of', 'in',
    'for', 'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through',
    'during', 'before', 'after', 'above', 'below', 'between', 'under',
    'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where',
    'why', 'how', 'all', 'each', 'few', 'more', 'most', 'other', 'some',
    'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',
    'too', 'very', 'just', 'also', 'now', 'new'
}


def extract_semantic_components(finding: str) -> Dict[str, Any]:
    """
    Extract semantic components from a finding.

    Example:
        "The market is growing at 15% annually" ->
        {
            "direction": "up",
            "percentage": 15.0,
            "subject": "market",
            "timeframe": "annual",
            "entities": [],
            "keywords": ["market", "growing", "annually"]
        }
    """
    if not finding:
        return {}

    finding_lower = finding.lower()
    components = {
        "direction": None,
        "percentage": None,
        "value": None,
        "value_unit": None,
        "subject": None,
        "timeframe": None,
        "entities": [],
        "keywords": []
    }

    # Extract direction
    for pattern in FINDING_PATTERNS["growth"]:
        match = re.search(pattern, finding_lower)
        if match:
            components["direction"] = "up"
            if match.groups():
                try:
                    components["percentage"] = float(match.group(1))
                except:
                    pass
            break

    if not components["direction"]:
        for pattern in FINDING_PATTERNS["decline"]:
            match = re.search(pattern, finding_lower)
            if match:
                components["direction"] = "down"
                if match.groups():
                    try:
                        components["percentage"] = float(match.group(1))
                    except:
                        pass
                break

    # Extract trend sentiment
    if not components["direction"]:
        for pattern in FINDING_PATTERNS["trend_up"]:
            if re.search(pattern, finding_lower):
                components["direction"] = "up"
                break
        for pattern in FINDING_PATTERNS["trend_down"]:
            if re.search(pattern, finding_lower):
                components["direction"] = "down"
                break

    # Extract value
    for pattern in FINDING_PATTERNS["value"]:
        match = re.search(pattern, finding_lower)
        if match:
            try:
                components["value"] = float(match.group(1))
                if len(match.groups()) > 1 and match.group(2):
                    components["value_unit"] = match.group(2)[0].upper()
            except:
                pass
            break

    # Extract timeframe
    timeframe_patterns = {
        "annual": r'\b(?:annual(?:ly)?|year(?:ly)?|per year|yoy|y-o-y)\b',
        "quarterly": r'\b(?:quarter(?:ly)?|q[1-4])\b',
        "monthly": r'\b(?:month(?:ly)?|per month)\b',
        "2024": r'\b2024\b',
        "2025": r'\b2025\b',
        "2026": r'\b2026\b',
        "2030": r'\b2030\b',
    }
    for tf_name, tf_pattern in timeframe_patterns.items():
        if re.search(tf_pattern, finding_lower):
            components["timeframe"] = tf_name
            break

    # Extract subject keywords
    words = re.findall(r'\b[a-z]{3,}\b', finding_lower)
    keywords = [w for w in words if w not in STOP_WORDS]
    components["keywords"] = keywords[:10]  # Limit to top 10

    # Identify likely subject
    subject_candidates = ["market", "industry", "sector", "segment", "revenue", "sales", "demand", "supply"]
    for word in keywords:
        if word in subject_candidates:
            components["subject"] = word
            break

    return components


def compute_semantic_hash(finding: str) -> str:
    """
    Compute a semantic hash for a finding that's invariant to wording changes.

    Two findings with the same meaning should produce the same hash.

    Example:
        "The market is growing at 15% annually" -> "up_15.0_market_annual"
        "Annual growth rate stands at 15%" -> "up_15.0_market_annual"
    """
    components = extract_semantic_components(finding)

    # Build hash components in consistent order
    hash_parts = []

    # Direction
    if components.get("direction"):
        hash_parts.append(components["direction"])

    # Percentage (rounded to avoid float issues)
    if components.get("percentage") is not None:
        hash_parts.append(f"{components['percentage']:.1f}")

    # Value with unit
    if components.get("value") is not None:
        val_str = f"{components['value']:.1f}"
        if components.get("value_unit"):
            val_str += components["value_unit"]
        hash_parts.append(val_str)

    # Subject
    if components.get("subject"):
        hash_parts.append(components["subject"])

    # Timeframe
    if components.get("timeframe"):
        hash_parts.append(components["timeframe"])

    # If we have enough components, use them for hash
    if len(hash_parts) >= 2:
        return "_".join(hash_parts)

    # Fallback: use sorted keywords
    keywords = sorted(components.get("keywords", []))[:5]
    if keywords:
        return "_".join(keywords)

    # Last resort: normalized text hash
    normalized = re.sub(r'\s+', ' ', finding.lower().strip())
    return hashlib.md5(normalized.encode()).hexdigest()[:12]


def compute_semantic_finding_diffs(old_findings: List[str], new_findings: List[str]) -> List[FindingDiff]:
    """
    Compute finding diffs using semantic hashing instead of text similarity.

    This ensures that findings with the same meaning but different wording
    are recognized as the same finding.
    """
    diffs = []
    matched_new_indices = set()

    # Compute hashes for all findings
    old_hashes = [(f, compute_semantic_hash(f), extract_semantic_components(f)) for f in old_findings if f]
    new_hashes = [(f, compute_semantic_hash(f), extract_semantic_components(f)) for f in new_findings if f]

    # Match by semantic hash
    for old_text, old_hash, old_components in old_hashes:
        best_match_idx = None
        best_match_score = 0

        for i, (new_text, new_hash, new_components) in enumerate(new_hashes):
            if i in matched_new_indices:
                continue

            # Exact hash match = same finding
            if old_hash == new_hash:
                best_match_idx = i
                best_match_score = 100
                break

            # Component-based similarity
            score = compute_component_similarity(old_components, new_components)
            if score > best_match_score:
                best_match_score = score
                best_match_idx = i

        if best_match_idx is not None and best_match_score >= 60:
            matched_new_indices.add(best_match_idx)
            new_text = new_hashes[best_match_idx][0]

            if best_match_score >= 90:
                change_type = 'retained'
            else:
                change_type = 'modified'

            diffs.append(FindingDiff(
                old_text=old_text,
                new_text=new_text,
                similarity=best_match_score,
                change_type=change_type
            ))
        else:
            # Finding removed
            diffs.append(FindingDiff(
                old_text=old_text,
                new_text=None,
                similarity=0,
                change_type='removed'
            ))

    # Find added findings
    for i, (new_text, new_hash, new_components) in enumerate(new_hashes):
        if i not in matched_new_indices:
            diffs.append(FindingDiff(
                old_text=None,
                new_text=new_text,
                similarity=0,
                change_type='added'
            ))

    return diffs


def compute_component_similarity(comp1: Dict, comp2: Dict) -> float:
    """
    Compute similarity between two finding component dictionaries.
    Returns a score from 0-100.
    """
    if not comp1 or not comp2:
        return 0

    score = 0
    weights = {
        "direction": 25,
        "percentage": 25,
        "value": 20,
        "subject": 15,
        "timeframe": 10,
        "keywords": 5
    }

    # Direction match
    if comp1.get("direction") and comp2.get("direction"):
        if comp1["direction"] == comp2["direction"]:
            score += weights["direction"]
    elif not comp1.get("direction") and not comp2.get("direction"):
        score += weights["direction"] * 0.5  # Both neutral

    # Percentage match (within 2% tolerance)
    if comp1.get("percentage") is not None and comp2.get("percentage") is not None:
        diff = abs(comp1["percentage"] - comp2["percentage"])
        if diff <= 2:
            score += weights["percentage"]
        elif diff <= 5:
            score += weights["percentage"] * 0.5

    # Value match (within 10% tolerance)
    if comp1.get("value") is not None and comp2.get("value") is not None:
        v1, v2 = comp1["value"], comp2["value"]
        # Normalize by unit
        if comp1.get("value_unit") == comp2.get("value_unit"):
            if v1 > 0 and v2 > 0:
                ratio = min(v1, v2) / max(v1, v2)
                if ratio >= 0.9:
                    score += weights["value"]
                elif ratio >= 0.8:
                    score += weights["value"] * 0.5

    # Subject match
    if comp1.get("subject") and comp2.get("subject"):
        if comp1["subject"] == comp2["subject"]:
            score += weights["subject"]

    # Timeframe match
    if comp1.get("timeframe") and comp2.get("timeframe"):
        if comp1["timeframe"] == comp2["timeframe"]:
            score += weights["timeframe"]

    # Keyword overlap
    kw1 = set(comp1.get("keywords", []))
    kw2 = set(comp2.get("keywords", []))
    if kw1 and kw2:
        overlap = len(kw1 & kw2) / len(kw1 | kw2)
        score += weights["keywords"] * overlap

    return score


# ------------------------------------
# UPDATED METRIC DIFF COMPUTATION
# Using canonical IDs
# ------------------------------------

def compute_metric_diffs_canonical(old_metrics: Dict, new_metrics: Dict) -> List[MetricDiff]:
    """
    Compute metric diffs using canonical IDs for stable matching.
    Range-aware via get_metric_value_span + spans_overlap.
    """
    diffs: List[MetricDiff] = []

    old_canonical = canonicalize_metrics(old_metrics)
    new_canonical = canonicalize_metrics(new_metrics)

    matched_new_ids = set()

    # Match by canonical ID
    for old_id, old_m in old_canonical.items():
        old_name = old_m.get("name", old_id)

        old_span = get_metric_value_span(old_m)
        old_raw = str(old_m.get("value", ""))
        old_unit = old_span.get("unit") or old_m.get("unit", "")
        old_val = old_span.get("mid")

        # -------------------------
        # Direct canonical ID match
        # -------------------------
        if old_id in new_canonical:
            new_m = new_canonical[old_id]
            matched_new_ids.add(old_id)

            new_raw = str(new_m.get("value", ""))
            new_span = get_metric_value_span(new_m)
            new_val = new_span.get("mid")
            new_unit = new_span.get("unit") or new_m.get("unit", old_unit)

            if spans_overlap(old_span, new_span, rel_tol=0.05):
                change_pct = 0.0
                change_type = "unchanged"
            else:
                change_pct = compute_percent_change(old_val, new_val)
                if change_pct is None or abs(change_pct) < 0.5:
                    change_type = "unchanged"
                elif change_pct > 0:
                    change_type = "increased"
                else:
                    change_type = "decreased"

            diffs.append(MetricDiff(
                name=old_name,
                old_value=old_val,
                new_value=new_val,
                old_raw=old_raw,
                new_raw=new_raw,
                unit=new_unit or old_unit,
                change_pct=change_pct,
                change_type=change_type
            ))
            continue  # important: don't fall into base-ID matching

        # -------------------------
        # Base ID match (strip years)
        # -------------------------
        base_id = re.sub(r'_\d{4}(?:_\d{4})*$', '', old_id)
        found = False

        for new_id, new_m in new_canonical.items():
            if new_id in matched_new_ids:
                continue

            new_base_id = re.sub(r'_\d{4}(?:_\d{4})*$', '', new_id)
            if base_id != new_base_id:
                continue

            matched_new_ids.add(new_id)
            found = True

            new_raw = str(new_m.get("value", ""))
            new_span = get_metric_value_span(new_m)
            new_val = new_span.get("mid")
            new_unit = new_span.get("unit") or new_m.get("unit", old_unit)

            if spans_overlap(old_span, new_span, rel_tol=0.05):
                change_pct = 0.0
                change_type = "unchanged"
            else:
                change_pct = compute_percent_change(old_val, new_val)
                if change_pct is None or abs(change_pct) < 0.5:
                    change_type = "unchanged"
                elif change_pct > 0:
                    change_type = "increased"
                else:
                    change_type = "decreased"

            diffs.append(MetricDiff(
                name=old_name,
                old_value=old_val,
                new_value=new_val,
                old_raw=old_raw,
                new_raw=new_raw,
                unit=new_unit or old_unit,
                change_pct=change_pct,
                change_type=change_type
            ))
            break

        if not found:
            diffs.append(MetricDiff(
                name=old_name,
                old_value=old_val,
                new_value=None,
                old_raw=old_raw,
                new_raw="",
                unit=old_unit,
                change_pct=None,
                change_type="removed"
            ))

    # Added metrics
    for new_id, new_m in new_canonical.items():
        if new_id in matched_new_ids:
            continue

        new_name = new_m.get("name", new_id)
        new_raw = str(new_m.get("value", ""))
        new_span = get_metric_value_span(new_m)
        new_val = new_span.get("mid")
        new_unit = new_span.get("unit") or new_m.get("unit", "")

        diffs.append(MetricDiff(
            name=new_name,
            old_value=None,
            new_value=new_val,
            old_raw="",
            new_raw=new_raw,
            unit=new_unit,
            change_pct=None,
            change_type="added"
        ))

    return diffs


# ------------------------------------
# NUMERIC PARSING (DETERMINISTIC)
# ------------------------------------

def parse_to_float(value: Any) -> Optional[float]:
    """
    Deterministically parse any value to float.
    Returns None if unparseable.
    """
    if value is None:
        return None
    if isinstance(value, (int, float)):
        return float(value)
    if not isinstance(value, str):
        return None

    # Clean string
    cleaned = value.strip().upper()
    cleaned = re.sub(r'[,$]', '', cleaned)

    # Handle empty/NA
    if cleaned in ['', 'N/A', 'NA', 'NULL', 'NONE', '-', '—']:
        return None

    # Extract multiplier
    multiplier = 1.0
    if 'TRILLION' in cleaned or cleaned.endswith('T'):
        multiplier = 1_000_000
        cleaned = re.sub(r'T(?:RILLION)?', '', cleaned)
    elif 'BILLION' in cleaned or cleaned.endswith('B'):
        multiplier = 1_000
        cleaned = re.sub(r'B(?:ILLION)?', '', cleaned)
    elif 'MILLION' in cleaned or cleaned.endswith('M'):
        multiplier = 1
        cleaned = re.sub(r'M(?:ILLION)?', '', cleaned)
    elif 'THOUSAND' in cleaned or cleaned.endswith('K'):
        multiplier = 0.001
        cleaned = re.sub(r'K(?:THOUSAND)?', '', cleaned)

    # Handle percentages
    if '%' in cleaned:
        cleaned = cleaned.replace('%', '')
        # Don't apply multiplier to percentages
        multiplier = 1.0

    try:
        return float(cleaned.strip()) * multiplier
    except (ValueError, TypeError):
        return None

def get_metric_value_span(metric: Dict) -> Dict[str, Any]:
    """
    Return a numeric span for a metric to support range-aware canonical metrics.

    Output:
      {
        "min": float|None,
        "max": float|None,
        "mid": float|None,   # representative value (median if range, else parsed value)
        "unit": str,         # normalized (upper/stripped), preserves %/$ units if present
        "is_range": bool
      }
    """
    if not isinstance(metric, dict):
        return {"min": None, "max": None, "mid": None, "unit": "", "is_range": False}

    unit = (metric.get("unit") or "").strip()

    # If metric already has a range object, prefer it
    r = metric.get("range")
    if isinstance(r, dict):
        vmin = r.get("min")
        vmax = r.get("max")
        # ensure numeric
        try:
            vmin = float(vmin) if vmin is not None else None
        except Exception:
            vmin = None
        try:
            vmax = float(vmax) if vmax is not None else None
        except Exception:
            vmax = None

        # Representative = median of candidates if provided, else midpoint of min/max
        candidates = r.get("candidates")
        nums = []
        if isinstance(candidates, list):
            for c in candidates:
                try:
                    nums.append(float(c))
                except Exception:
                    pass
        if nums:
            nums_sorted = sorted(nums)
            mid = nums_sorted[len(nums_sorted) // 2]
        else:
            mid = None
            if vmin is not None and vmax is not None:
                mid = (vmin + vmax) / 2.0
            elif vmin is not None:
                mid = vmin
            elif vmax is not None:
                mid = vmax

        return {
            "min": vmin,
            "max": vmax,
            "mid": mid,
            "unit": unit,
            "is_range": True
        }

    # Non-range metric: parse single numeric value
    val = parse_to_float(metric.get("value"))
    return {
        "min": val,
        "max": val,
        "mid": val,
        "unit": unit,
        "is_range": False
    }


def spans_overlap(a: Dict[str, Any], b: Dict[str, Any], rel_tol: float = 0.05) -> bool:
    """
    Decide whether two spans overlap "enough" to be considered stable.
    rel_tol provides a small widening to avoid false drift from rounding.
    """
    if not a or not b:
        return False
    a_min, a_max = a.get("min"), a.get("max")
    b_min, b_max = b.get("min"), b.get("max")

    if a_min is None or a_max is None or b_min is None or b_max is None:
        return False

    # Widen spans slightly
    a_pad = max(abs(a_max), abs(a_min), 1.0) * rel_tol
    b_pad = max(abs(b_max), abs(b_min), 1.0) * rel_tol

    a_min2, a_max2 = a_min - a_pad, a_max + a_pad
    b_min2, b_max2 = b_min - b_pad, b_max + b_pad

    return not (a_max2 < b_min2 or b_max2 < a_min2)


def compute_percent_change(old_val: Optional[float], new_val: Optional[float]) -> Optional[float]:
    """
    Compute percent change. Returns None if either value is None or old is 0.
    """
    if old_val is None or new_val is None:
        return None
    if old_val == 0:
        return None if new_val == 0 else float('inf')
    return round(((new_val - old_val) / abs(old_val)) * 100, 2)

# ------------------------------------
# NAME MATCHING (DETERMINISTIC)
# ------------------------------------

def normalize_name(name: str) -> str:
    """Normalize name for matching"""
    if not name:
        return ""
    n = name.lower().strip()
    n = re.sub(r'[^\w\s]', '', n)
    n = re.sub(r'\s+', ' ', n)
    return n

def name_similarity(name1: str, name2: str) -> float:
    """Compute similarity ratio between two names (0-1)"""
    n1 = normalize_name(name1)
    n2 = normalize_name(name2)
    if not n1 or not n2:
        return 0.0
    if n1 == n2:
        return 1.0
    # Check containment
    if n1 in n2 or n2 in n1:
        return 0.9
    # Sequence matcher
    return difflib.SequenceMatcher(None, n1, n2).ratio()

def find_best_match(name: str, candidates: List[str], threshold: float = 0.7) -> Optional[str]:
    """Find best matching name from candidates"""
    best_match = None
    best_score = threshold
    for candidate in candidates:
        score = name_similarity(name, candidate)
        if score > best_score:
            best_score = score
            best_match = candidate
    return best_match

# =========================================================
# DETERMINISTIC QUERY STRUCTURE EXTRACTION
# - Classify query into a known category (country / industry / etc.)
# - Extract main question + "side questions" deterministically
# - Optional: spaCy dependency parse (if installed)
# - Optional: embedding similarity (if sentence-transformers/sklearn installed)
# =========================================================

SIDE_CONNECTOR_PATTERNS = [
    r"\bimpact of\b",
    r"\beffect of\b",
    r"\binfluence of\b",
    r"\brole of\b",
    r"\bdriven by\b",
    r"\bcaused by\b",
    r"\bdue to\b",
    r"\bincluding\b",
    r"\bincluding but not limited to\b",
    r"\bwith a focus on\b",
    r"\bespecially\b",
    r"\bnotably\b",
    r"\bplus\b",
    r"\bas well as\b",
    r"\band also\b",
    r"\bvs\b",
    r"\bversus\b",
]

QUESTION_CATEGORIES = {
    "country": {
        "signals": [
            "gdp", "gdp per capita", "population", "inflation", "interest rate",
            "exports", "imports", "trade balance", "currency", "fx", "central bank",
            "unemployment", "fiscal", "budget", "debt", "sovereign", "country"
        ],
        "template_sections": [
            "GDP & GDP per capita", "Growth rates", "Population & demographics",
            "Key industries", "Exports & imports", "Currency & FX trends",
            "Interest rates & inflation", "Risks & outlook"
        ],
    },
    "industry": {
        "signals": [
            "market size", "tam", "cagr", "industry", "sector", "market",
            "key players", "competitive landscape", "drivers", "challenges",
            "regulation", "technology trends", "forecast"
        ],
        "template_sections": [
            "Total Addressable Market (TAM) / Market size", "Growth rates (CAGR/YoY)",
            "Key players", "Key drivers", "Challenges & risks",
            "Technology trends", "Regulatory / environmental factors", "Outlook"
        ],
    },
    "company": {
        "signals": [
            "revenue", "earnings", "profit", "margins", "guidance",
            "business model", "segments", "customers", "competitors",
            "valuation", "multiple", "pe ratio", "cash flow"
        ],
        "template_sections": [
            "Business overview", "Revenue / profitability", "Segments",
            "Competitive position", "Key risks", "Guidance / outlook"
        ],
    },
    "unknown": {
        "signals": [],
        "template_sections": [],
    }
}

def _normalize_q(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip())

def _cleanup_clause(text: str) -> str:
    t = _normalize_q(text)
    t = re.sub(r"^[,;:\-\s]+", "", t)
    t = re.sub(r"[,;:\-\s]+$", "", t)
    return t

def detect_query_category(query: str) -> Dict[str, Any]:
    """
    Deterministically classify query category using keyword signals.
    Returns: {"category": "...", "confidence": 0-1, "matched_signals": [...]}
    """
    q = (query or "").lower()
    best_cat = "unknown"
    best_hits = 0
    best_matched = []

    for cat, cfg in QUESTION_CATEGORIES.items():
        if cat == "unknown":
            continue
        matched = [s for s in cfg["signals"] if s in q]
        hits = len(matched)
        if hits > best_hits:
            best_hits = hits
            best_cat = cat
            best_matched = matched

    # simple confidence: saturate after ~6 hits
    conf = min(best_hits / 6.0, 1.0) if best_hits > 0 else 0.0
    return {"category": best_cat, "confidence": round(conf, 2), "matched_signals": best_matched[:8]}

# =========================================================
# 3A+. LAYERED QUERY STRUCTURE PARSER (Deterministic -> NLP -> Embeddings -> LLM fallback)
# =========================================================

_QUERY_SPLIT_PATTERNS = [
    r"\bas well as\b",
    r"\balong with\b",
    r"\bin addition to\b",
    r"\band\b",
    r"\bplus\b",
    r"\bvs\.?\b",
    r"\bversus\b",
    r",",
    r";",
]

_COUNTRY_OVERVIEW_SIGNALS = [
    "in general", "overview", "tell me about", "general", "profile", "facts about",
    "economy", "population", "gdp", "currency", "exports", "imports",
]

def _normalize_q(q: str) -> str:
    q = (q or "").strip()
    q = re.sub(r"\s+", " ", q)
    return q

def _split_clauses_deterministic(q: str) -> List[str]:
    """
    Deterministically split a question into ordered clauses.

    Supports:
    - comma/connector splits (",", "and", "as well as", "in addition to", etc.)
    - multi-side enumerations like:
        "in addition to: 1. X 2. Y"
        "including: (1) X (2) Y"
        "as well as: • X • Y"
    """
    if not isinstance(q, str):
        return []

    s = q.strip()
    if not s:
        return []

    # Normalize whitespace early (keep original casing if present; upstream may lowercase already)
    s = re.sub(r"\s+", " ", s).strip()

    # --- Step A: If there's an enumeration intro, split head vs tail ---
    # Examples: "in addition to:", "including:", "plus:", "as well as:"
    enum_intro = re.search(
        r"\b(in addition to|in addition|including|in addition to the following|as well as|plus)\b\s*:?\s*",
        s,
        flags=re.IGNORECASE,
    )

    head = s
    tail = ""

    if enum_intro:
        # Split at the FIRST occurrence of the enum phrase
        idx = enum_intro.start()
        # head is everything before the phrase if it exists, otherwise keep whole string
        # but we usually want "Tell me about X in general" to remain in head.
        head = s[:idx].strip().rstrip(",")
        tail = s[enum_intro.end():].strip()

        # If head is empty (e.g., query begins with "In addition to:"), treat everything as head
        if not head:
            head = s
            tail = ""

    clauses: List[str] = []

    # --- Step B: Split head using your existing connector patterns ---
    if head:
        parts = [head]
        for pat in _QUERY_SPLIT_PATTERNS:
            next_parts = []
            for p in parts:
                next_parts.extend(re.split(pat, p, flags=re.IGNORECASE))
            parts = next_parts

        for p in parts:
            p = p.strip(" ,;:.").strip()
            if p:
                clauses.append(p)

    # --- Step C: If tail exists, split as enumerated items/bullets ---
    if tail:
        # Split on "1.", "1)", "(1)", "•", "-", "*"
        # Keep it robust: find item starts, then slice.
        item_start = re.compile(r"(?:^|\s)(?:\(?\d+\)?[\.\)]|[•\-\*])\s+", flags=re.IGNORECASE)
        starts = [m.start() for m in item_start.finditer(tail)]

        if starts:
            # Build slices using detected starts
            spans = []
            for i, st0 in enumerate(starts):
                st = st0
                # Move start to the start of token (strip leading whitespace)
                while st < len(tail) and tail[st].isspace():
                    st += 1
                en = starts[i + 1] if i + 1 < len(starts) else len(tail)
                spans.append((st, en))

            for st, en in spans:
                item = tail[st:en].strip(" ,;:.").strip()
                # Remove the leading bullet/number token again (safety)
                item = re.sub(r"^\(?\d+\)?[\.\)]\s+", "", item)
                item = re.sub(r"^[•\-\*]\s+", "", item)
                item = item.strip(" ,;:.").strip()
                if item:
                    clauses.append(item)
        else:
            # If tail doesn't look enumerated, fall back to normal splitter on tail
            parts = [tail]
            for pat in _QUERY_SPLIT_PATTERNS:
                next_parts = []
                for p in parts:
                    next_parts.extend(re.split(pat, p, flags=re.IGNORECASE))
                parts = next_parts

            for p in parts:
                p = p.strip(" ,;:.").strip()
                if p:
                    clauses.append(p)

    # Final cleanup + dedupe while preserving order
    out: List[str] = []
    seen = set()
    for c in clauses:
        c2 = c.strip()
        if not c2:
            continue
        if c2.lower() in seen:
            continue
        seen.add(c2.lower())
        out.append(c2)

    return out



def _dedupe_clauses(clauses: List[str]) -> List[str]:
    seen = set()
    out = []
    for c in clauses:
        c2 = c.strip().lower()
        if not c2 or c2 in seen:
            continue
        seen.add(c2)
        out.append(c.strip())
    return out

def _choose_main_and_side(clauses: List[str]) -> Tuple[str, List[str]]:
    """
    Pick 'main' as the first clause; side = remainder.
    Deterministic, stable across runs.
    """
    clauses = _dedupe_clauses(clauses)
    if not clauses:
        return "", []
    main = clauses[0]
    side = clauses[1:]
    return main, side

def _try_spacy_nlp():
    """
    Optional NLP layer. If spaCy is installed, use it; otherwise return None.
    """
    try:
        import spacy  # type: ignore
        # Avoid heavy model loading; prefer blank model with sentencizer if no model available.
        try:
            nlp = spacy.load("en_core_web_sm")  # common if installed
        except Exception:
            nlp = spacy.blank("en")
            if "sentencizer" not in nlp.pipe_names:
                nlp.add_pipe("sentencizer")
        return nlp
    except Exception:
        return None

def _nlp_refine_clauses(query: str, clauses: List[str]) -> Dict[str, Any]:
    """
    Use dependency/NER cues to:
      - detect country-overview questions
      - improve main-vs-side decision (coordination / 'as well as' patterns)
    Returns partial overrides: {"main":..., "side":[...], "hints":{...}}
    """
    nlp = _try_spacy_nlp()
    if not nlp:
        return {"hints": {"nlp_used": False}}

    doc = nlp(_normalize_q(query))
    # Named entities that look like places
    gpes = [ent.text for ent in getattr(doc, "ents", []) if ent.label_ in ("GPE", "LOC")]
    gpes_norm = [g.strip() for g in gpes if g and len(g.strip()) > 1]

    # Coordination hint: if query has "as well as" or "and", keep deterministic split,
    # but try to pick the more "general" clause as main when overview signals exist.
    overview_hit = any(sig in (query or "").lower() for sig in _COUNTRY_OVERVIEW_SIGNALS)
    hints = {
        "nlp_used": True,
        "gpe_entities": gpes_norm[:5],
        "overview_signal_hit": bool(overview_hit),
    }

    main, side = _choose_main_and_side(clauses)

    # If overview signals + place entity present, bias main to the overview clause
    if overview_hit and gpes_norm:
        # choose clause with strongest overview signal density
        def score_clause(c: str) -> int:
            c = c.lower()
            return sum(1 for sig in _COUNTRY_OVERVIEW_SIGNALS if sig in c)
        scored = sorted([(score_clause(c), c) for c in clauses], key=lambda x: (-x[0], x[1]))
        if scored and scored[0][0] > 0:
            main = scored[0][1]
            side = [c for c in clauses if c != main]

    return {"main": main, "side": side, "hints": hints}

def _embedding_category_vote(query: str) -> Dict[str, Any]:
    """
    Deterministic 'embedding-like' similarity using TF-IDF (no external model downloads).
    Produces a category suggestion + confidence based on similarity to category descriptors.
    """
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore
        from sklearn.metrics.pairwise import cosine_similarity  # type: ignore
    except Exception:
        return {"category": "unknown", "confidence": 0.0, "method": "tfidf_unavailable"}

    q = _normalize_q(query).lower()
    if not q:
        return {"category": "unknown", "confidence": 0.0, "method": "tfidf_empty"}

    # Build deterministic descriptors from your registry
    cat_texts = []
    cat_names = []
    for cat, cfg in (QUESTION_CATEGORIES or {}).items():
        if not isinstance(cfg, dict) or cat == "unknown":
            continue
        signals = " ".join(cfg.get("signals", [])[:50])
        sections = " ".join((cfg.get("template_sections", []) or [])[:50])
        descriptor = f"{cat} {signals} {sections}".strip()
        if descriptor:
            cat_names.append(cat)
            cat_texts.append(descriptor)

    if not cat_texts:
        return {"category": "unknown", "confidence": 0.0, "method": "tfidf_no_registry"}

    vec = TfidfVectorizer(ngram_range=(1, 2), min_df=1, max_features=8000)
    X = vec.fit_transform(cat_texts + [q])
    sims = cosine_similarity(X[-1], X[:-1]).flatten()

    best_idx = int(sims.argmax()) if sims.size else 0
    best_sim = float(sims[best_idx]) if sims.size else 0.0
    best_cat = cat_names[best_idx] if cat_names else "unknown"

    # Map cosine similarity (~0-1) into a conservative confidence
    conf = max(0.0, min(best_sim / 0.35, 1.0))  # 0.35 sim ~= "high"
    return {"category": best_cat, "confidence": round(conf, 2), "method": "tfidf"}

def _llm_fallback_query_structure(query: str, web_context: Optional[Dict] = None) -> Optional[Dict[str, Any]]:
    """
    Last resort: ask LLM to output ONLY a small JSON query-structure object.
    Guardrail: do NOT let the LLM invent extra side questions unless the user explicitly enumerated them.
    This path must NOT validate against LLMResponse.
    """
    try:
        q = str(query or "").strip()
        if not q:
            return None

        # --- Detect explicit enumeration / list structure in the USER query ---
        # If the user wrote a list (1., 2), bullets, etc.), it's reasonable to accept multiple sides.
        enum_patterns = [
            r"(^|\n)\s*\d+\s*[\.\)]\s+",     # 1.  / 2)
            r"(^|\n)\s*[-•*]\s+",           # - item / • item
            r"(^|\n)\s*[a-zA-Z]\s*[\.\)]\s+"  # a) / b. etc.
        ]
        has_explicit_enumeration = any(re.search(p, q, flags=re.MULTILINE) for p in enum_patterns)

        # Deterministic baseline (what the system already extracted)
        # We use this to clamp LLM hallucinations.
        det_clauses = _split_clauses_deterministic(_normalize_q(q))
        det_main, det_side = _choose_main_and_side(det_clauses)
        det_side = _dedupe_clauses([s.strip() for s in (det_side or []) if isinstance(s, str) and s.strip()])

        prompt = (
            "Extract a query structure.\n"
            "Return ONLY valid JSON with keys:\n"
            "  category: one of [country, industry, company, finance, market, unknown]\n"
            "  category_confidence: number 0-1\n"
            "  main: string (the main question/topic)\n"
            "  side: array of strings (side questions)\n"
            "No extra keys, no commentary.\n\n"
            f"Query: {q}"
        )

        raw = query_perplexity_raw(prompt, max_tokens=250, timeout=30)

        # Parse
        if isinstance(raw, dict):
            parsed = raw
        else:
            if raw is None:
                raw = ""
            if not isinstance(raw, str):
                raw = str(raw)
            parsed = parse_json_safely(raw, "LLM Query Structure")

        if not isinstance(parsed, dict) or parsed.get("main") is None:
            return None

        # --- Clean/normalize fields ---
        llm_main = str(parsed.get("main") or "").strip()
        llm_side = parsed.get("side") if isinstance(parsed.get("side"), list) else []
        llm_side = [str(s).strip() for s in llm_side if s is not None and str(s).strip()]

        # Reject "invented" side items that look like generic outline bullets
        # (Only apply this rejection when the user did NOT explicitly enumerate a list.)
        def _looks_like_outline_item(s: str) -> bool:
            s2 = s.lower().strip()
            bad_starts = (
                "overview", "key", "key stats", "statistics", "major statistics",
                "policies", "infrastructure", "recent trends", "post-covid", "covid",
                "challenges", "opportunities", "drivers", "headwinds",
                "background", "introduction"
            )
            return any(s2.startswith(b) for b in bad_starts)

        # --- Guardrail policy ---
        # If user didn't enumerate, do NOT accept LLM expansion of side questions.
        if not has_explicit_enumeration:
            # Keep deterministic sides only. (You can allow 1 LLM side if deterministic found none.)
            final_side = det_side
            if not final_side and llm_side:
                # Allow at most one side item as a fallback, but avoid outline-like additions.
                cand = llm_side[0]
                final_side = [] if _looks_like_outline_item(cand) else [cand]
        else:
            # User enumerated: accept multiple sides, but still de-dupe and keep deterministic items first
            merged = []
            for s in (det_side + llm_side):
                s = str(s).strip()
                if not s:
                    continue
                if s not in merged:
                    merged.append(s)
            final_side = merged

        # If LLM main is empty or fragment-y, keep deterministic main
        bad_prefixes = ("as well as", "as well", "and ", "also ", "plus ", "as for ")
        if not llm_main or any(llm_main.lower().startswith(p) for p in bad_prefixes):
            llm_main = (det_main or "").strip()

        # Return only allowed keys
        out = {
            "category": parsed.get("category", "unknown") or "unknown",
            "category_confidence": parsed.get("category_confidence", 0.0),
            "main": llm_main,
            "side": final_side,
        }
        return out

    except Exception:
        return None


def _split_side_candidates(query: str) -> List[str]:
    """
    Deterministic splitting into clause candidates.
    We keep it conservative to avoid over-splitting.
    """
    q = _normalize_q(query)
    # Pull quoted strings as strong side-topic candidates
    quoted = re.findall(r"['\"]([^'\"]{2,80})['\"]", q)
    q_wo_quotes = re.sub(r"['\"][^'\"]{2,80}['\"]", " ", q)

    # Split on major separators
    parts = re.split(r"[;]|(?:\s+-\s+)|(?:\s+—\s+)", q_wo_quotes)
    parts = [p for p in (_cleanup_clause(x) for x in parts) if p]

    # Further split on side connectors
    connector_re = "(" + "|".join(SIDE_CONNECTOR_PATTERNS) + ")"
    expanded = []
    for p in parts:
        # break into chunks but keep connector words in-place by splitting into sentences first
        sub = re.split(r"\.\s+|\?\s+|\!\s+", p)
        for s in sub:
            s = _cleanup_clause(s)
            if not s:
                continue
            # if contains connector, split into [before, after...] using first connector
            m = re.search(connector_re, s.lower())
            if m:
                idx = m.start()
                before = _cleanup_clause(s[:idx])
                after = _cleanup_clause(s[idx:])
                if before:
                    expanded.append(before)
                if after:
                    expanded.append(after)
            else:
                expanded.append(s)

    # Add quoted items as standalone candidates (often side topics)
    for qitem in quoted:
        cleaned = _cleanup_clause(qitem)
        if cleaned:
            expanded.append(cleaned)

    # De-dupe while preserving order (deterministic)
    seen = set()
    out = []
    for x in expanded:
        k = x.lower()
        if k not in seen:
            seen.add(k)
            out.append(x)
    return out

def _extract_spacy_side_topics(query: str) -> List[str]:
    """
    Optional: use spaCy dependency parse if available.
    Extracts objects of 'impact/effect/role/influence' patterns.
    """
    try:
        import spacy  # type: ignore
        try:
            nlp = spacy.load("en_core_web_sm")  # type: ignore
        except Exception:
            return []
    except Exception:
        return []

    doc = nlp(query)
    side = []

    triggers = {"impact", "effect", "influence", "role"}
    for token in doc:
        if token.lemma_.lower() in triggers:
            # Look for "of X" attached to trigger
            for child in token.children:
                if child.dep_ == "prep" and child.text.lower() == "of":
                    pobj = next((c for c in child.children if c.dep_ in ("pobj", "dobj", "obj")), None)
                    if pobj is not None:
                        # take subtree as phrase
                        phrase = " ".join(t.text for t in pobj.subtree)
                        phrase = _cleanup_clause(phrase)
                        if phrase and phrase.lower() not in (s.lower() for s in side):
                            side.append(phrase)
    return side[:5]

def _embedding_similarity(a: str, b: str) -> Optional[float]:
    """
    Optional: compute cosine similarity using:
      - sentence-transformers (preferred) OR
      - sklearn TF-IDF fallback
    Returns None if unavailable.
    """
    a = _normalize_q(a)
    b = _normalize_q(b)
    if not a or not b:
        return None

    # 1) sentence-transformers (if installed)
    try:
        from sentence_transformers import SentenceTransformer  # type: ignore
        import numpy as np  # type: ignore
        model = SentenceTransformer("all-MiniLM-L6-v2")
        emb = model.encode([a, b], normalize_embeddings=True)
        sim = float(np.dot(emb[0], emb[1]))
        return max(min(sim, 1.0), -1.0)
    except Exception:
        pass

    # 2) sklearn TF-IDF cosine similarity (deterministic)
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore
        from sklearn.metrics.pairwise import cosine_similarity  # type: ignore
        vec = TfidfVectorizer(stop_words="english")
        X = vec.fit_transform([a, b])
        sim = float(cosine_similarity(X[0], X[1])[0, 0])
        return max(min(sim, 1.0), -1.0)
    except Exception:
        return None

def extract_query_structure(query: str) -> Dict[str, Any]:
    """
    Layered query structure extraction:
      1) Deterministic clause split -> main/side
      2) Deterministic category from keyword signals (detect_query_category)
      3) Optional NLP refinement (spaCy if available)
      4) Deterministic similarity vote (TF-IDF)
      5) LLM fallback ONLY if confidence remains low
    """
    q = _normalize_q(query)
    clauses = _split_clauses_deterministic(q)
    main, side = _choose_main_and_side(clauses)

    # --- Layer 1: deterministic keyword category ---
    det_cat = detect_query_category(q)
    category = det_cat.get("category", "unknown")
    cat_conf = float(det_cat.get("confidence", 0.0))

    debug = {
        "deterministic": {
            "clauses": clauses,
            "main": main,
            "side": side,
            "category": category,
            "confidence": cat_conf,
            "matched_signals": det_cat.get("matched_signals", []),
        }
    }

    # --- Layer 2: NLP refinement (optional) ---
    nlp_out = _nlp_refine_clauses(q, clauses)
    if isinstance(nlp_out, dict):
        hints = nlp_out.get("hints", {})
        debug["nlp"] = hints or {"nlp_used": False}

        # Override main/side if NLP produced them (guard against fragment-y mains)
        nlp_main = (nlp_out.get("main") or "").strip()
        if nlp_main:
            bad_prefixes = ("as well as", "as well", "and ", "also ", "plus ", "as for ")
            if not any(nlp_main.lower().startswith(p) for p in bad_prefixes):
                main = nlp_main

        if isinstance(nlp_out.get("side"), list):
            side = nlp_out["side"]

        # If NLP detects a place + overview cue, bias to "country"
        gpes = (hints or {}).get("gpe_entities", []) if isinstance(hints, dict) else []
        overview_hit = (hints or {}).get("overview_signal_hit", False) if isinstance(hints, dict) else False
        if overview_hit and gpes and cat_conf < 0.45:
            category = "country"
            cat_conf = max(cat_conf, 0.55)

    # --- Layer 3: embedding-style category vote ---
    emb_vote = _embedding_category_vote(q)
    debug["similarity_vote"] = emb_vote

    emb_cat = emb_vote.get("category", "unknown")
    emb_conf = float(emb_vote.get("confidence", 0.0))

    if cat_conf < 0.40 and emb_cat != "unknown" and emb_conf >= 0.45:
        category = emb_cat
        cat_conf = max(cat_conf, min(0.75, emb_conf))

    # --- Layer 4: LLM fallback if still ambiguous ---
    if cat_conf < 0.30:
        llm = _llm_fallback_query_structure(q)
        debug["llm_fallback_used"] = bool(llm)

        if isinstance(llm, dict):
            category = llm.get("category", category) or category
            try:
                cat_conf = float(llm.get("category_confidence", cat_conf))
            except Exception:
                pass

            llm_main = (llm.get("main") or "").strip()
            llm_side = llm.get("side") if isinstance(llm.get("side"), list) else []

            det_main = (main or "").strip()
            det_side = side or []

            def _overview_score(s: str) -> int:
                if not s:
                    return 0
                s2 = s.lower()
                signals = [
                    "in general", "overview", "background", "basic facts",
                    "at a glance", "tell me about", "describe", "introduction"
                ]
                return sum(1 for sig in signals if sig in s2)

            def _is_bad_main(s: str) -> bool:
                if not s or len(s) < 8:
                    return True
                return s.lower().startswith(
                    ("as well as", "as well", "and ", "also ", "plus ", "as for ")
                )

            merged_side = []
            for s in det_side + llm_side:
                s = str(s).strip()
                if s and s not in merged_side:
                    merged_side.append(s)

            det_score = _overview_score(det_main)
            llm_score = _overview_score(llm_main)

            if llm_main and not _is_bad_main(llm_main):
                if not det_main or llm_score > det_score:
                    main = llm_main

            side = merged_side

    side = _dedupe_clauses([s.strip() for s in (side or []) if s.strip()])

    return {
        "category": category or "unknown",
        "category_confidence": round(max(0.0, min(cat_conf, 1.0)), 2),
        "main": (main or "").strip(),
        "side": side,
        "debug": debug,
    }


def format_query_structure_for_prompt(qs: Optional[Dict[str, Any]]) -> str:
    if not qs or not isinstance(qs, dict):
        return ""

    parts = []
    parts.append("STRUCTURED QUESTION (DETERMINISTIC):")
    parts.append(f"- Category: {qs.get('category','unknown')} (conf {qs.get('category_confidence','')})")
    parts.append(f"- Main (answer this FIRST): {qs.get('main','')}")
    side = qs.get("side") or []

    if side:
        parts.append("- Side questions (answer AFTER main, in this exact order):")
        for i, s in enumerate(side[:10], 1):
            parts.append(f"  {i}. {s}")

    tmpl = qs.get("template_sections") or []
    if tmpl:
        parts.append("- Recommended response sections (use as headings if helpful):")
        for t in tmpl[:10]:
            parts.append(f"  - {t}")

    # Hard behavioral instruction to the LLM (kept short and explicit)
    parts.append(
        "RESPONSE RULES:\n"
        "1) Start by answering the MAIN request with general context.\n"
        "2) Then answer EACH side question explicitly (label them).\n"
        "3) Metrics/findings can include both main + side, but do not ignore the main.\n"
        "4) If you provide tourism/industry metrics, ALSO provide basic country/overview facts when main is an overview."
    )

    return "\n".join(parts).strip()


# ------------------------------------
# METRIC DIFF COMPUTATION
# ------------------------------------

def compute_metric_diffs(old_metrics: Dict, new_metrics: Dict) -> List[MetricDiff]:
    """
    Compute deterministic diffs between metric dictionaries.
    Returns list of MetricDiff objects.
    """
    diffs = []
    matched_new_keys = set()

    # Build lookup for new metrics by normalized name
    new_by_name = {}
    for key, m in new_metrics.items():
        if isinstance(m, dict):
            name = m.get('name', key)
            new_by_name[normalize_name(name)] = (key, m)

    # Process old metrics
    for old_key, old_m in old_metrics.items():
        if not isinstance(old_m, dict):
            continue

        old_name = old_m.get('name', old_key)
        old_raw = str(old_m.get('value', ''))
        old_unit = old_m.get('unit', '')
        old_val = parse_to_float(old_m.get('value'))

        # Find matching new metric
        norm_name = normalize_name(old_name)
        match = new_by_name.get(norm_name)

        if not match:
            # Try fuzzy matching
            best = find_best_match(old_name, [m.get('name', k) for k, m in new_metrics.items() if isinstance(m, dict)])
            if best:
                for k, m in new_metrics.items():
                    if isinstance(m, dict) and m.get('name', k) == best:
                        match = (k, m)
                        break

        if match:
            new_key, new_m = match
            matched_new_keys.add(new_key)

            new_raw = str(new_m.get('value', ''))
            new_val = parse_to_float(new_m.get('value'))
            new_unit = new_m.get('unit', old_unit)

            change_pct = compute_percent_change(old_val, new_val)

            # Determine change type
            if change_pct is None:
                change_type = 'unchanged'
            elif abs(change_pct) < 0.5:  # Less than 0.5% change = unchanged
                change_type = 'unchanged'
            elif change_pct > 0:
                change_type = 'increased'
            else:
                change_type = 'decreased'

            diffs.append(MetricDiff(
                name=old_name,
                old_value=old_val,
                new_value=new_val,
                old_raw=old_raw,
                new_raw=new_raw,
                unit=new_unit or old_unit,
                change_pct=change_pct,
                change_type=change_type
            ))
        else:
            # Metric was removed
            diffs.append(MetricDiff(
                name=old_name,
                old_value=old_val,
                new_value=None,
                old_raw=old_raw,
                new_raw='',
                unit=old_unit,
                change_pct=None,
                change_type='removed'
            ))

    # Find added metrics
    for new_key, new_m in new_metrics.items():
        if new_key in matched_new_keys:
            continue
        if not isinstance(new_m, dict):
            continue

        new_name = new_m.get('name', new_key)
        new_raw = str(new_m.get('value', ''))
        new_val = parse_to_float(new_m.get('value'))
        new_unit = new_m.get('unit', '')

        diffs.append(MetricDiff(
            name=new_name,
            old_value=None,
            new_value=new_val,
            old_raw='',
            new_raw=new_raw,
            unit=new_unit,
            change_pct=None,
            change_type='added'
        ))

    return diffs

# ------------------------------------
# ENTITY DIFF COMPUTATION
# ------------------------------------

def compute_entity_diffs(old_entities: List, new_entities: List) -> List[EntityDiff]:
    """
    Compute deterministic diffs between entity rankings.
    """
    diffs = []

    # Build lookups with ranks
    old_lookup = {}
    for i, e in enumerate(old_entities):
        if isinstance(e, dict):
            name = normalize_name(e.get('name', ''))
            old_lookup[name] = {
                'rank': i + 1,
                'share': e.get('share'),
                'original_name': e.get('name', '')
            }

    new_lookup = {}
    for i, e in enumerate(new_entities):
        if isinstance(e, dict):
            name = normalize_name(e.get('name', ''))
            new_lookup[name] = {
                'rank': i + 1,
                'share': e.get('share'),
                'original_name': e.get('name', '')
            }

    # All unique names
    all_names = set(old_lookup.keys()) | set(new_lookup.keys())

    for norm_name in all_names:
        old_data = old_lookup.get(norm_name)
        new_data = new_lookup.get(norm_name)

        if old_data and new_data:
            # Entity exists in both
            rank_change = old_data['rank'] - new_data['rank']  # Positive = moved up

            if rank_change > 0:
                change_type = 'moved_up'
            elif rank_change < 0:
                change_type = 'moved_down'
            else:
                change_type = 'unchanged'

            diffs.append(EntityDiff(
                name=new_data['original_name'],
                old_rank=old_data['rank'],
                new_rank=new_data['rank'],
                old_share=old_data['share'],
                new_share=new_data['share'],
                rank_change=rank_change,
                change_type=change_type
            ))
        elif old_data:
            # Entity removed
            diffs.append(EntityDiff(
                name=old_data['original_name'],
                old_rank=old_data['rank'],
                new_rank=None,
                old_share=old_data['share'],
                new_share=None,
                rank_change=None,
                change_type='removed'
            ))
        else:
            # Entity added
            diffs.append(EntityDiff(
                name=new_data['original_name'],
                old_rank=None,
                new_rank=new_data['rank'],
                old_share=None,
                new_share=new_data['share'],
                rank_change=None,
                change_type='added'
            ))

    # Sort by new rank (added entities at end)
    diffs.sort(key=lambda x: x.new_rank if x.new_rank else 999)
    return diffs

# ------------------------------------
# FINDING DIFF COMPUTATION
# ------------------------------------

def compute_finding_diffs(old_findings: List[str], new_findings: List[str]) -> List[FindingDiff]:
    """
    Compute deterministic diffs between findings using text similarity.
    """
    diffs = []
    matched_new_indices = set()

    # Match old findings to new
    for old_f in old_findings:
        if not old_f:
            continue

        best_match_idx = None
        best_similarity = 0.5  # Minimum threshold

        for i, new_f in enumerate(new_findings):
            if i in matched_new_indices or not new_f:
                continue

            sim = name_similarity(old_f, new_f)  # Reuse name similarity for text
            if sim > best_similarity:
                best_similarity = sim
                best_match_idx = i

        if best_match_idx is not None:
            matched_new_indices.add(best_match_idx)
            similarity_pct = round(best_similarity * 100, 1)

            if similarity_pct >= 90:
                change_type = 'retained'
            else:
                change_type = 'modified'

            diffs.append(FindingDiff(
                old_text=old_f,
                new_text=new_findings[best_match_idx],
                similarity=similarity_pct,
                change_type=change_type
            ))
        else:
            # Finding removed
            diffs.append(FindingDiff(
                old_text=old_f,
                new_text=None,
                similarity=0,
                change_type='removed'
            ))

    # Find added findings
    for i, new_f in enumerate(new_findings):
        if i in matched_new_indices or not new_f:
            continue

        diffs.append(FindingDiff(
            old_text=None,
            new_text=new_f,
            similarity=0,
            change_type='added'
        ))

    return diffs

# =========================================================
# 8C. DETERMINISTIC SOURCE EXTRACTION
# Extract metrics/entities directly from web snippets - NO LLM
# =========================================================

def extract_metrics_from_sources(web_context: Dict) -> Dict:
    """
    Extract numeric metrics directly from web search snippets.
    100% deterministic - no LLM involved.
    """
    extracted = {}
    search_results = web_context.get("search_results", [])

    # Patterns to match common metric formats
    patterns = [
        # Market size patterns
        (r'\$\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)\b', 'market_size'),
        (r'market\s+size[:\s]+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'market_size'),
        (r'valued\s+at\s+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'market_size'),
        (r'worth\s+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'market_size'),

        # Growth rate patterns
        (r'CAGR[:\s]+of?\s*(\d+(?:\.\d+)?)\s*%', 'cagr'),
        (r'(\d+(?:\.\d+)?)\s*%\s*CAGR', 'cagr'),
        (r'grow(?:th|ing)?\s+(?:at\s+)?(\d+(?:\.\d+)?)\s*%', 'growth_rate'),

        # Revenue patterns
        (r'revenue[:\s]+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'revenue'),

        # Year-specific values
        (r'(?:in\s+)?20\d{2}[:\s]+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'year_value'),
    ]

    all_matches = []

    for result in search_results:
        snippet = result.get("snippet", "")
        title = result.get("title", "")
        source = result.get("source", "")
        text = f"{title} {snippet}".lower()

        for pattern, metric_type in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    value_str, unit = match[0], match[1] if len(match) > 1 else ''
                else:
                    value_str, unit = match, ''

                try:
                    value = float(value_str)

                    # Normalize unit
                    unit_lower = unit.lower() if unit else ''
                    if unit_lower in ['t', 'trillion']:
                        unit_normalized = 'T'
                        value_in_billions = value * 1000
                    elif unit_lower in ['b', 'billion']:
                        unit_normalized = 'B'
                        value_in_billions = value
                    elif unit_lower in ['m', 'million']:
                        unit_normalized = 'M'
                        value_in_billions = value / 1000
                    elif unit_lower == '%':
                        unit_normalized = '%'
                        value_in_billions = value  # Keep as-is for percentages
                    else:
                        unit_normalized = ''
                        value_in_billions = value

                    all_matches.append({
                        'type': metric_type,
                        'value': value,
                        'unit': unit_normalized,
                        'value_normalized': value_in_billions,
                        'source': source,
                        'raw': f"{value_str} {unit}".strip()
                    })
                except (ValueError, TypeError):
                    continue

    # Deduplicate and select best matches by type
    metrics_by_type = {}
    for match in all_matches:
        mtype = match['type']
        if mtype not in metrics_by_type:
            metrics_by_type[mtype] = []
        metrics_by_type[mtype].append(match)

    # For each type, take the most common value (mode) or median
    metric_counter = 0
    for mtype, matches in metrics_by_type.items():
        if not matches:
            continue

        # Group by similar values (within 10%)
        value_groups = []
        for m in matches:
            added = False
            for group in value_groups:
                if group and abs(m['value_normalized'] - group[0]['value_normalized']) / max(group[0]['value_normalized'], 0.001) < 0.1:
                    group.append(m)
                    added = True
                    break
            if not added:
                value_groups.append([m])

        # Take the largest group (most consensus)
        if value_groups:
            best_group = max(value_groups, key=len)
            representative = best_group[0]

            metric_counter += 1
            metric_key = f"extracted_{mtype}_{metric_counter}"

            # Map type to readable name
            type_names = {
                'market_size': 'Market Size',
                'cagr': 'CAGR',
                'growth_rate': 'Growth Rate',
                'revenue': 'Revenue',
                'year_value': 'Market Value'
            }

            extracted[metric_key] = {
                'name': type_names.get(mtype, mtype.replace('_', ' ').title()),
                'value': representative['value'],
                'unit': f"${representative['unit']}" if representative['unit'] in ['T', 'B', 'M'] else representative['unit'],
                'source_count': len(best_group),
                'sources': list(set(m['source'] for m in best_group))[:3]
            }

    return extracted


def extract_entities_from_sources(web_context: Dict) -> List[Dict]:
    """
    Extract company/entity names from web search snippets.
    100% deterministic - no LLM involved.
    """
    search_results = web_context.get("search_results", [])

    # Common market leaders that appear in financial contexts
    known_entities = [
        # Tech
        'apple', 'microsoft', 'google', 'alphabet', 'amazon', 'meta', 'facebook',
        'nvidia', 'tesla', 'intel', 'amd', 'qualcomm', 'broadcom', 'cisco',
        'ibm', 'oracle', 'salesforce', 'adobe', 'netflix', 'uber', 'airbnb',
        # Finance
        'jpmorgan', 'goldman sachs', 'morgan stanley', 'bank of america',
        'wells fargo', 'citigroup', 'blackrock', 'vanguard', 'fidelity',
        # Auto
        'toyota', 'volkswagen', 'ford', 'gm', 'general motors', 'honda',
        'bmw', 'mercedes', 'byd', 'nio', 'rivian', 'lucid',
        # Pharma
        'pfizer', 'johnson & johnson', 'roche', 'novartis', 'merck',
        'abbvie', 'eli lilly', 'astrazeneca', 'moderna', 'gilead',
        # Energy
        'exxon', 'chevron', 'shell', 'bp', 'totalenergies', 'conocophillips',
        # Consumer
        'walmart', 'costco', 'home depot', 'nike', 'starbucks', 'mcdonalds',
        'coca-cola', 'pepsi', 'procter & gamble', 'unilever',
        # Regions (for market share by region)
        'north america', 'europe', 'asia pacific', 'asia-pacific', 'apac',
        'china', 'united states', 'japan', 'germany', 'india', 'uk',
        'latin america', 'middle east', 'africa'
    ]

    entity_mentions = {}

    for result in search_results:
        snippet = result.get("snippet", "").lower()
        title = result.get("title", "").lower()
        text = f"{title} {snippet}"

        for entity in known_entities:
            if entity in text:
                # Try to extract market share if mentioned
                share_pattern = rf'{re.escape(entity)}[^.]*?(\d+(?:\.\d+)?)\s*%'
                share_match = re.search(share_pattern, text, re.IGNORECASE)

                share = None
                if share_match:
                    share = f"{share_match.group(1)}%"

                if entity not in entity_mentions:
                    entity_mentions[entity] = {'count': 0, 'shares': []}

                entity_mentions[entity]['count'] += 1
                if share:
                    entity_mentions[entity]['shares'].append(share)

    # Sort by mention count and build list
    sorted_entities = sorted(entity_mentions.items(), key=lambda x: x[1]['count'], reverse=True)

    entities = []
    for entity_name, data in sorted_entities[:10]:  # Top 10
        # Use most common share if available
        share = None
        if data['shares']:
            # Take the most common share value
            share_counts = Counter(data['shares'])
            share = share_counts.most_common(1)[0][0]

        entities.append({
            'name': entity_name.title(),
            'share': share,
            'growth': None,  # Can't reliably extract growth from snippets
            'mention_count': data['count']
        })

    return entities

# ------------------------------------
# STABILITY SCORE COMPUTATION
# ------------------------------------

def compute_stability_score(
    metric_diffs: List[MetricDiff],
    entity_diffs: List[EntityDiff],
    finding_diffs: List[FindingDiff]
) -> float:
    """
    Compute overall stability score (0-100).
    Higher = more stable (less change).
    """
    scores = []

    # Metric stability (40% weight)
    if metric_diffs:
        stable_metrics = sum(1 for m in metric_diffs if m.change_type == 'unchanged')
        small_change = sum(1 for m in metric_diffs if m.change_pct and abs(m.change_pct) < 10)
        metric_score = ((stable_metrics + small_change * 0.5) / len(metric_diffs)) * 100
        scores.append(('metrics', metric_score, 0.4))

    # Entity stability (35% weight)
    if entity_diffs:
        stable_entities = sum(1 for e in entity_diffs if e.change_type == 'unchanged')
        entity_score = (stable_entities / len(entity_diffs)) * 100
        scores.append(('entities', entity_score, 0.35))

    # Finding stability (25% weight)
    if finding_diffs:
        retained = sum(1 for f in finding_diffs if f.change_type in ['retained', 'modified'])
        finding_score = (retained / len(finding_diffs)) * 100
        scores.append(('findings', finding_score, 0.25))

    if not scores:
        return 100.0

    # Weighted average
    total_weight = sum(s[2] for s in scores)
    weighted_sum = sum(s[1] * s[2] for s in scores)
    return round(weighted_sum / total_weight, 1)

# ------------------------------------
# MAIN DIFF COMPUTATION
# ------------------------------------

def compute_evolution_diff(old_analysis: Dict, new_analysis: Dict) -> EvolutionDiff:
    """
    Main entry point: compute complete deterministic diff between two analyses.
    """
    old_response = old_analysis.get('primary_response', {})
    new_response = new_analysis.get('primary_response', {})

    # Timestamps
    old_ts = old_analysis.get('timestamp', '')
    new_ts = new_analysis.get('timestamp', '')

    # Calculate time delta
    time_delta = None
    try:
        old_dt = datetime.fromisoformat(old_ts.replace('Z', '+00:00'))
        new_dt = datetime.fromisoformat(new_ts.replace('Z', '+00:00'))
        time_delta = round((new_dt.replace(tzinfo=None) - old_dt.replace(tzinfo=None)).total_seconds() / 3600, 1)
    except:
        pass

    # Compute diffs using CANONICAL metric registry for stable matching
    metric_diffs = compute_metric_diffs_canonical(
        old_response.get('primary_metrics', {}),
        new_response.get('primary_metrics', {})
    )

    entity_diffs = compute_entity_diffs(
        old_response.get('top_entities', []),
        new_response.get('top_entities', [])
    )

    # Use SEMANTIC finding comparison (stable across wording changes)
    finding_diffs = compute_semantic_finding_diffs(
        old_response.get('key_findings', []),
        new_response.get('key_findings', [])
    )

    # Compute stability
    stability = compute_stability_score(metric_diffs, entity_diffs, finding_diffs)

    # Summary stats
    summary_stats = {
        'metrics_increased': sum(1 for m in metric_diffs if m.change_type == 'increased'),
        'metrics_decreased': sum(1 for m in metric_diffs if m.change_type == 'decreased'),
        'metrics_unchanged': sum(1 for m in metric_diffs if m.change_type == 'unchanged'),
        'metrics_added': sum(1 for m in metric_diffs if m.change_type == 'added'),
        'metrics_removed': sum(1 for m in metric_diffs if m.change_type == 'removed'),
        'entities_moved_up': sum(1 for e in entity_diffs if e.change_type == 'moved_up'),
        'entities_moved_down': sum(1 for e in entity_diffs if e.change_type == 'moved_down'),
        'entities_unchanged': sum(1 for e in entity_diffs if e.change_type == 'unchanged'),
        'entities_added': sum(1 for e in entity_diffs if e.change_type == 'added'),
        'entities_removed': sum(1 for e in entity_diffs if e.change_type == 'removed'),
        'findings_retained': sum(1 for f in finding_diffs if f.change_type == 'retained'),
        'findings_modified': sum(1 for f in finding_diffs if f.change_type == 'modified'),
        'findings_added': sum(1 for f in finding_diffs if f.change_type == 'added'),
        'findings_removed': sum(1 for f in finding_diffs if f.change_type == 'removed'),
    }

    return EvolutionDiff(
        old_timestamp=old_ts,
        new_timestamp=new_ts,
        time_delta_hours=time_delta,
        metric_diffs=metric_diffs,
        entity_diffs=entity_diffs,
        finding_diffs=finding_diffs,
        stability_score=stability,
        summary_stats=summary_stats
    )

# ------------------------------------
# LLM EXPLANATION (ONLY INTERPRETS DIFFS)
# ------------------------------------

def generate_diff_explanation_prompt(diff: EvolutionDiff, query: str) -> str:
    """
    Generate prompt for LLM to EXPLAIN computed diffs (not discover them).
    """
    # Build metric changes text
    metric_changes = []
    for m in diff.metric_diffs:
        if m.change_type == 'increased':
            metric_changes.append(f"- {m.name}: {m.old_raw} → {m.new_raw} ({m.change_pct:+.1f}%) INCREASED")
        elif m.change_type == 'decreased':
            metric_changes.append(f"- {m.name}: {m.old_raw} → {m.new_raw} ({m.change_pct:+.1f}%) DECREASED")
        elif m.change_type == 'added':
            metric_changes.append(f"- {m.name}: NEW metric added with value {m.new_raw}")
        elif m.change_type == 'removed':
            metric_changes.append(f"- {m.name}: REMOVED (was {m.old_raw})")

    # Build entity changes text
    entity_changes = []
    for e in diff.entity_diffs:
        if e.change_type == 'moved_up':
            entity_changes.append(f"- {e.name}: Rank {e.old_rank} → {e.new_rank} (moved UP {e.rank_change} positions)")
        elif e.change_type == 'moved_down':
            entity_changes.append(f"- {e.name}: Rank {e.old_rank} → {e.new_rank} (moved DOWN {abs(e.rank_change)} positions)")
        elif e.change_type == 'added':
            entity_changes.append(f"- {e.name}: NEW entrant at rank {e.new_rank}")
        elif e.change_type == 'removed':
            entity_changes.append(f"- {e.name}: DROPPED OUT (was rank {e.old_rank})")

    # Build findings changes text
    finding_changes = []
    for f in diff.finding_diffs:
        if f.change_type == 'added':
            finding_changes.append(f"- NEW: {f.new_text}")
        elif f.change_type == 'removed':
            finding_changes.append(f"- REMOVED: {f.old_text}")
        elif f.change_type == 'modified':
            finding_changes.append(f"- MODIFIED: '{f.old_text[:50]}...' → '{f.new_text[:50]}...'")

    prompt = f"""You are a market analyst explaining changes between two analysis snapshots.

    QUERY: {query}
    TIME ELAPSED: {diff.time_delta_hours:.1f} hours
    STABILITY SCORE: {diff.stability_score:.0f}%

    COMPUTED METRIC CHANGES:
    {chr(10).join(metric_changes) if metric_changes else "No significant metric changes"}

    COMPUTED ENTITY RANKING CHANGES:
    {chr(10).join(entity_changes) if entity_changes else "No ranking changes"}

    COMPUTED FINDING CHANGES:
    {chr(10).join(finding_changes) if finding_changes else "No finding changes"}

    SUMMARY STATS:
    - Metrics: {diff.summary_stats['metrics_increased']} increased, {diff.summary_stats['metrics_decreased']} decreased, {diff.summary_stats['metrics_unchanged']} unchanged
    - Entities: {diff.summary_stats['entities_moved_up']} moved up, {diff.summary_stats['entities_moved_down']} moved down
    - Findings: {diff.summary_stats['findings_added']} new, {diff.summary_stats['findings_removed']} removed

    YOUR TASK: Provide a 3-5 sentence executive interpretation of these changes.
    - What is the overall trend (improving/declining/stable)?
    - What are the most significant changes and why might they have occurred?
    - What should stakeholders pay attention to?

    Return ONLY a JSON object:
    {{
        "trend": "improving/declining/stable",
        "headline": "One sentence summary of key change",
        "interpretation": "3-5 sentence detailed interpretation",
        "watch_items": ["Item 1 to monitor", "Item 2 to monitor"]
    }}
    """
    return prompt

def get_llm_explanation(diff: EvolutionDiff, query: str) -> Dict:
    """
    Ask LLM to explain the computed diffs (not discover them).
    """
    prompt = generate_diff_explanation_prompt(diff, query)

    headers = {
        "Authorization": f"Bearer {PERPLEXITY_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "sonar",
        "temperature": 0.0,  # Deterministic
        "max_tokens": 500,
        "top_p": 1.0,
        "messages": [{"role": "user", "content": prompt}]
    }

    try:
        resp = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        content = data["choices"][0]["message"]["content"]

        parsed = parse_json_safely(content, "Explanation")
        if parsed:
            return parsed
    except Exception as e:
        st.warning(f"LLM explanation failed: {e}")

    # Fallback
    return {
        "trend": "stable" if diff.stability_score >= 70 else "changing",
        "headline": f"Analysis shows {diff.stability_score:.0f}% stability over {diff.time_delta_hours:.0f} hours",
        "interpretation": "Unable to generate detailed interpretation.",
        "watch_items": []
    }


# =========================================================
# 8B. EVOLUTION DASHBOARD RENDERING
# =========================================================

def render_evolution_results(diff: EvolutionDiff, explanation: Dict, query: str):
    """Render deterministic evolution results"""

    st.header("📈 Evolution Analysis")
    st.markdown(f"**Query:** {query}")

    # Overview metrics
    col1, col2, col3, col4 = st.columns(4)

    if diff.time_delta_hours:
        if diff.time_delta_hours < 24:
            time_str = f"{diff.time_delta_hours:.1f}h"
        else:
            time_str = f"{diff.time_delta_hours/24:.1f}d"
        col1.metric("Time Elapsed", time_str)
    else:
        col1.metric("Time Elapsed", "Unknown")

    col2.metric("Stability", f"{diff.stability_score:.0f}%")

    trend = explanation.get('trend', 'stable')
    trend_icon = {'improving': '📈', 'declining': '📉', 'stable': '➡️'}.get(trend, '➡️')
    col3.metric("Trend", f"{trend_icon} {trend.title()}")

    # Stability indicator
    if diff.stability_score >= 80:
        col4.success("🟢 Highly Stable")
    elif diff.stability_score >= 60:
        col4.warning("🟡 Moderate Changes")
    else:
        col4.error("🔴 Significant Drift")

    # Headline
    st.info(f"**{explanation.get('headline', 'Analysis complete')}**")

    st.markdown("---")

    # Interpretation

    # =====================================================================
    # PATCH FIX39 (ADDITIVE): enforce unit-required gate at render time
    # =====================================================================
    try:
        # best effort: use schema carried on diff (if any) else global latest schema
        schema = getattr(diff, "metric_schema_frozen", None)
        if not isinstance(schema, dict):
            schema = {}
        _fix39_sanitize_evolutiondiff_object(diff, schema)
    except Exception:
        pass

    st.subheader("📋 Interpretation")
    st.markdown(explanation.get('interpretation', 'No interpretation available'))

    # Watch items
    watch_items = explanation.get('watch_items', [])
    if watch_items:
        st.markdown("**🔔 Watch Items:**")
        for item in watch_items:
            st.markdown(f"- {item}")

    st.markdown("---")

    # Metric Changes Table
    st.subheader("💰 Metric Changes")
    if diff.metric_diffs:
        metric_rows = []

        def _fmt_currency_first(raw: str, unit: str) -> str:
            """
            Formats evolution metrics as:
            - S$29.8B
            - $120M
            - 29.8%
            """
            raw = (raw or "").strip()
            unit = (unit or "").strip()

            if not raw or raw == "-":
                return "-"

            # If already currency-first, trust it
            if raw.startswith("S$") or raw.startswith("$"):
                return raw

            # Percent case
            if unit == "%":
                return f"{raw}%"

            # Detect currency from unit
            currency = ""
            scale = unit.replace(" ", "")

            if scale.upper().startswith("SGD"):
                currency = "S$"
                scale = scale[3:]
            elif scale.upper().startswith("USD"):
                currency = "$"
                scale = scale[3:]
            elif scale.startswith("S$"):
                currency = "S$"
                scale = scale[2:]
            elif scale.startswith("$"):
                currency = "$"
                scale = scale[1:]

            # Human-readable units
            if unit.lower().endswith("billion"):
                return f"{currency}{raw} billion".strip()
            if unit.lower().endswith("million"):
                return f"{currency}{raw} million".strip()

            # Compact units (B/M/K)
            if scale.upper() in {"B", "M", "K"}:
                return f"{currency}{raw}{scale}".strip()

            # Fallback
            return f"{currency}{raw} {unit}".strip()

        for m in diff.metric_diffs:
            icon = {
                'increased': '📈', 'decreased': '📉', 'unchanged': '➡️',
                'added': '🆕', 'removed': '❌'
            }.get(m.change_type, '•')

            change_str = f"{m.change_pct:+.1f}%" if m.change_pct is not None else "-"

            prev_raw = m.old_raw or "-"
            curr_raw = m.new_raw or "-"

            metric_rows.append({
                "": icon,
                "Metric": m.name,
                "Previous": _fmt_currency_first(prev_raw, getattr(m, "unit", "") or ""),
                "Current":  _fmt_currency_first(curr_raw, getattr(m, "unit", "") or ""),
                "Change": change_str,
                "Status": m.change_type.replace('_', ' ').title()
            })

        st.dataframe(pd.DataFrame(metric_rows), hide_index=True, use_container_width=True)
    else:
        st.info("No metrics to compare")

    st.markdown("---")

    # Entity Changes Table
    st.subheader("🏢 Entity Ranking Changes")
    if diff.entity_diffs:
        entity_rows = []
        for e in diff.entity_diffs:
            icon = {
                'moved_up': '⬆️', 'moved_down': '⬇️', 'unchanged': '➡️',
                'added': '🆕', 'removed': '❌'
            }.get(e.change_type, '•')

            rank_str = f"{e.rank_change:+d}" if e.rank_change else "-"

            entity_rows.append({
                "": icon,
                "Entity": e.name,
                "Old Rank": e.old_rank or "-",
                "New Rank": e.new_rank or "-",
                "Rank Δ": rank_str,
                "Old Share": e.old_share or "-",
                "New Share": e.new_share or "-"
            })
        st.dataframe(pd.DataFrame(entity_rows), hide_index=True, use_container_width=True)
    else:
        st.info("No entities to compare")

    st.markdown("---")

    # Finding Changes
    st.subheader("🔍 Finding Changes")
    if diff.finding_diffs:
        added = [f for f in diff.finding_diffs if f.change_type == 'added']
        removed = [f for f in diff.finding_diffs if f.change_type == 'removed']
        modified = [f for f in diff.finding_diffs if f.change_type == 'modified']

        if added:
            st.markdown("**🆕 New Findings:**")
            for f in added:
                st.success(f"• {f.new_text}")

        if removed:
            st.markdown("**❌ Removed Findings:**")
            for f in removed:
                st.error(f"• ~~{f.old_text}~~")

        if modified:
            st.markdown("**✏️ Modified Findings:**")
            for f in modified:
                st.warning(f"• {f.new_text} *(similarity: {f.similarity:.0f}%)*")
    else:
        st.info("No findings to compare")

    st.markdown("---")

    # Summary Stats
    st.subheader("📊 Change Summary")
    stats = diff.summary_stats

    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("**Metrics:**")
        st.write(f"📈 {stats['metrics_increased']} increased")
        st.write(f"📉 {stats['metrics_decreased']} decreased")
        st.write(f"➡️ {stats['metrics_unchanged']} unchanged")

    with col2:
        st.markdown("**Entities:**")
        st.write(f"⬆️ {stats['entities_moved_up']} moved up")
        st.write(f"⬇️ {stats['entities_moved_down']} moved down")
        st.write(f"🆕 {stats['entities_added']} new")

    with col3:
        st.markdown("**Findings:**")
        st.write(f"✅ {stats['findings_retained']} retained")
        st.write(f"✏️ {stats['findings_modified']} modified")
        st.write(f"🆕 {stats['findings_added']} new")


# =========================================================
# 8D. SOURCE-ANCHORED EVOLUTION
# Re-fetch the SAME sources from previous analysis for true stability
# Enhanced fetch_url_content function to use scrapingdog as fallback
# =========================================================

def _extract_pdf_text_from_bytes(pdf_bytes: bytes, max_pages: int = 6, max_chars: int = 7000) -> Optional[str]:
    """
    Extract readable text from PDF bytes deterministically.
    Limits pages/chars for speed and consistent output.
    """
    try:
        reader = PdfReader(io.BytesIO(pdf_bytes))
        texts = []
        for i, page in enumerate(reader.pages[:max_pages]):
            t = page.extract_text() or ""
            t = t.replace("\x00", " ").strip()
            if t:
                texts.append(t)
        joined = "\n".join(texts).strip()
        if len(joined) < 200:
            return None
        return joined[:max_chars]
    except Exception:
        return None



def fetch_url_content(url: str) -> Optional[str]:
    """Fetch content from a specific URL with ScrapingDog fallback"""

    def extract_text(html: str) -> Optional[str]:
        """Extract clean text from HTML"""
        soup = BeautifulSoup(html, 'html.parser')
        for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
            tag.decompose()
        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        clean_text = ' '.join(line for line in lines if line)
        return clean_text[:5000] if len(clean_text) > 200 else None

    # Try 1: Direct request
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        }
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()

        if 'captcha' not in resp.text.lower():
            content = extract_text(resp.text)
            if content:
                return content
    except:
        pass

    # Try 2: ScrapingDog API
    if SCRAPINGDOG_KEY:
        try:
            api_url = "https://api.scrapingdog.com/scrape"
            params = {"api_key": SCRAPINGDOG_KEY, "url": url, "dynamic": "false"}
            resp = requests.get(api_url, params=params, timeout=30)
            if resp.status_code == 200:
                content = extract_text(resp.text)
                if content:
                    return content
        except:
            pass

    return None



def fetch_url_content_with_status(url: str, timeout: int = 25):
    """
    Fetch URL content and return (text, status_detail).

    status_detail:
      - "success"
      - "success_pdf"
      - "http_<code>"
      - "exception:<TypeName>"
      - "empty"
      - "success_scrapingdog"

    Hardened:
      - Uses browser-like headers for direct fetch
      - Falls back to ScrapingDog when blocked/empty and SCRAPINGDOG_KEY is available
      - Avoids returning binary garbage as "text"
    """
    import re
    import requests

    def _normalize_url(s: str) -> str:
        t = (s or "").strip()
        if not t:
            return ""
        if re.match(r"^https?://", t, flags=re.I):
            return t
        if re.match(r"^[a-z0-9.-]+\.[a-z]{2,}(/.*)?$", t, flags=re.I):
            return "https://" + t
        return ""

    url = _normalize_url(url)
    if not url:
        return None, "empty"

    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
    }

    # ---------- 1) Direct fetch ----------
    try:
        resp = requests.get(url, timeout=timeout, headers=headers, allow_redirects=True)

        ct = (resp.headers.get("content-type", "") or "").lower()

        if resp.status_code >= 400:
            # If blocked, try ScrapingDog fallback (optional)
            if resp.status_code in (401, 403, 429) and globals().get("SCRAPINGDOG_KEY"):
                txt = _fetch_via_scrapingdog(url, timeout=timeout)
                if txt and txt.strip():
                    return txt, "success_scrapingdog"
            return None, f"http_{resp.status_code}"

        # PDF handling
        if "application/pdf" in ct or url.lower().endswith(".pdf"):
            try:
                import io
                import pdfplumber  # type: ignore
                with pdfplumber.open(io.BytesIO(resp.content)) as pdf:
                    out = []
                    for page in pdf.pages[:20]:
                        t = page.extract_text() or ""
                        if t.strip():
                            out.append(t)
                text = "\n".join(out).strip()
                if not text:
                    return None, "empty"
                return text, "success_pdf"
            except Exception as e:
                return None, f"exception:{type(e).__name__}"

        # Text/HTML
        text = resp.text or ""
        # If empty or suspiciously short, attempt ScrapingDog (optional)
        if (not text.strip() or len(text.strip()) < 300) and globals().get("SCRAPINGDOG_KEY"):
            txt = _fetch_via_scrapingdog(url, timeout=timeout)
            if txt and txt.strip():
                return txt, "success_scrapingdog"

        if not text.strip():
            return None, "empty"

        return text, "success"

    except Exception as e:
        # ScrapingDog as last resort for network-y issues
        try:
            if globals().get("SCRAPINGDOG_KEY"):
                txt = _fetch_via_scrapingdog(url, timeout=timeout)
                if txt and txt.strip():
                    return txt, "success_scrapingdog"
        except Exception:
            pass
        return None, f"exception:{type(e).__name__}"


def _fetch_via_scrapingdog(url: str, timeout: int = 25) -> str:
    """
    Internal helper used by fetch_url_content_with_status.
    Returns raw HTML text from ScrapingDog (or "" on failure).
    """
    import requests

    key = globals().get("SCRAPINGDOG_KEY")
    if not key:
        return ""

    params = {"api_key": key, "url": url, "dynamic": "false"}
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        resp = requests.get("https://api.scrapingdog.com/scrape", params=params, headers=headers, timeout=timeout)
        if resp.status_code >= 400:
            return ""
        return resp.text or ""
    except Exception:
        return ""

def get_extractor_fingerprint() -> str:
    """
    Bump this string whenever you change extraction or normalization behavior.
    Used to decide whether cached extracted_numbers are still valid.
    """
    return "extract_v2_normunits_2026-01-02"



def extract_numbers_from_text(text: str) -> List[Dict]:
    """
    Backward-compatible wrapper.

    v7_34 tightening:
    - Delegate to extract_numbers_with_context() so junk suppression is applied consistently.
    """
    try:
        return extract_numbers_with_context(text or "", source_url="", max_results=600) or []
    except Exception:
        return []


def _parse_iso_dt(ts: Optional[str]) -> Optional[datetime]:
    if not ts:
        return None
    try:
        ts2 = ts.replace("Z", "+00:00")
        dt = datetime.fromisoformat(ts2)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception:
        return None

def now_utc() -> datetime:
    """Timezone-aware UTC now (prevents naive/aware datetime bugs)."""
    return datetime.now(timezone.utc)


def _normalize_number_to_parse_base(value: float, unit: str) -> float:
    u = (unit or "").strip().upper()
    if u == "T":
        return value * 1_000_000
    if u == "B":
        return value * 1_000
    if u == "M":
        return value * 1
    if u == "K":
        return value * 0.001
    if u == "%":
        return value
    return value

def run_source_anchored_evolution(previous_data: dict, web_context: dict = None) -> dict:
    """
    Backward-compatible entrypoint used by the Streamlit Evolution UI.

    Enhancements:
      - Accept optional web_context so evolution can reuse same-run analysis upstream artifacts.
      - ALWAYS returns a dict with required keys (even on crash).
    """
    fn = globals().get("compute_source_anchored_diff")

    def _fail(msg: str) -> dict:
        return {
            "status": "failed",
            "message": msg,
            "sources_checked": 0,
            "sources_fetched": 0,
            "numbers_extracted_total": 0,
            "stability_score": 0.0,
            "summary": {
                "total_metrics": 0,
                "metrics_found": 0,
                "metrics_increased": 0,
                "metrics_decreased": 0,
                "metrics_unchanged": 0,
            },
            "metric_changes": [],
            "source_results": [],
            "interpretation": "Evolution failed.",
        }

    if not callable(fn):
        return _fail("compute_source_anchored_diff() is not defined, so source-anchored evolution cannot run.")

    try:
        # Support both old signature (previous_data) and new signature (previous_data, web_context)
        try:
            out = fn(previous_data, web_context=web_context)
        except TypeError:
            out = fn(previous_data)
    except Exception as e:
        return _fail(f"compute_source_anchored_diff crashed: {e}")

    if not isinstance(out, dict):
        return _fail("compute_source_anchored_diff returned a non-dict payload.")

    # Renderer-required defaults
    out.setdefault("status", "success")
    out.setdefault("message", "")
    out.setdefault("sources_checked", 0)
    out.setdefault("sources_fetched", 0)
    out.setdefault("numbers_extracted_total", 0)
    out.setdefault("stability_score", 0.0)
    out.setdefault("summary", {})
    out["summary"].setdefault("total_metrics", len(out.get("metric_changes") or []))
    out["summary"].setdefault("metrics_found", 0)
    out["summary"].setdefault("metrics_increased", 0)
    out["summary"].setdefault("metrics_decreased", 0)
    out["summary"].setdefault("metrics_unchanged", 0)
    out.setdefault("metric_changes", [])
    out.setdefault("source_results", [])
    out.setdefault("interpretation", "")
    # =====================================================================
    # PATCH FIX39 (ADDITIVE): sanitize evolution output before publish/render
    # =====================================================================
    try:
        _fix39_sanitize_metric_change_rows(out)
    except Exception:
        pass

    return out
# =========================================================
# ROBUST EVOLUTION HELPERS (DETERMINISTIC)
# =========================================================

NON_DATA_CONTEXT_HINTS = [
    "table of contents", "cookie", "privacy", "terms", "copyright",
    "subscribe", "newsletter", "login", "sign in", "nav", "footer"
]


def _truncate_json_safely_for_sheets(json_str: str, max_chars: int = 45000) -> str:
    """
# =====================================================================
# PATCH FIX41G (ADDITIVE): Normalize web_context and capture force_rebuild
# Ensures the UI flag reaches the fastpath gate and is recorded in output.
# =====================================================================
if web_context is None or not isinstance(web_context, dict):
    web_context = {}
_fix41_force_rebuild_seen = False
try:
    _fix41_force_rebuild_seen = bool(web_context.get("force_rebuild"))
except Exception:
    _fix41_force_rebuild_seen = False
# =====================================================================

    PATCH TS1 (ADDITIVE): JSON-safe truncation wrapper
    - Ensures json.loads always succeeds for any returned value.
    - Stores a preview when oversized.
    """
    import json

    s = "" if json_str is None else str(json_str)
    if len(s) <= max_chars:
        return s

    preview_len = max(0, int(max_chars) - 700)
    wrapper = {
        "_sheets_safe": True,
        "_sheet_write": {
            "truncated": True,
            "mode": "json_wrapper",
            "note": "Payload exceeded cell limit; stored preview only.",
        },
        "preview": s[:preview_len],
    }
    try:
        return json.dumps(wrapper, ensure_ascii=False, default=str)
    except Exception:
        return '{"_sheets_safe":true,"_sheet_write":{"truncated":true,"mode":"json_wrapper","note":"json.dumps failed"}}'


def _truncate_for_sheets(s: str, max_chars: int = 45000) -> str:
    """Hard cap to stay under Google Sheets 50k/cell limit."""
    if s is None:
        return ""
    s = str(s)
    if len(s) <= max_chars:
        return s
    head = s[: int(max_chars * 0.75)]
    tail = s[- int(max_chars * 0.20):]
    return head + "\n...\n[TRUNCATED FOR GOOGLE SHEETS]\n...\n" + tail



def _summarize_heavy_fields_for_sheets(obj: dict) -> dict:
    """
    Summarize fields that commonly exceed the per-cell limit while keeping debug utility.
    Only used for Sheets serialization; does NOT modify your in-memory analysis dict.
    """
    if not isinstance(obj, dict):
        return {"_type": str(type(obj)), "value": str(obj)[:500]}

    out = dict(obj)

    # Common bloat fields
    if "scraped_meta" in out:
        sm = out.get("scraped_meta")
        if isinstance(sm, dict):
            compact = {}
            for url, meta in list(sm.items())[:12]:
                if isinstance(meta, dict):
                    compact[url] = {
                        "status": meta.get("status"),
                        "status_detail": meta.get("status_detail"),
                        "numbers_found": meta.get("numbers_found"),
                        "fingerprint": meta.get("fingerprint"),
                        "clean_text_len": meta.get("clean_text_len"),
                    }
            out["scraped_meta"] = {"_summary": True, "count": len(sm), "sample": compact}
        else:
            out["scraped_meta"] = {"_summary": True, "type": str(type(sm))}

    for big_key in ("source_results", "baseline_sources_cache", "baseline_sources_cache_compact"):
        if big_key in out:
            sr = out.get(big_key)
            if isinstance(sr, list):
                sample = []
                for item in sr[:2]:
                    if isinstance(item, dict):
                        item2 = dict(item)
                        if isinstance(item2.get("extracted_numbers"), list):
                            item2["extracted_numbers"] = {"_summary": True, "count": len(item2["extracted_numbers"])}
                        sample.append(item2)
                out[big_key] = {"_summary": True, "count": len(sr), "sample": sample}
            else:
                out[big_key] = {"_summary": True, "type": str(type(sr))}

    # If you store full scraped_content anywhere, summarize it too
    if "scraped_content" in out:
        sc = out.get("scraped_content")
        if isinstance(sc, dict):
            out["scraped_content"] = {"_summary": True, "count": len(sc), "keys_sample": list(sc.keys())[:10]}
        else:
            out["scraped_content"] = {"_summary": True, "type": str(type(sc))}

    # =====================================================================
    # PATCH SS2 (ADDITIVE, REQUIRED): summarize nested heavy fields under out["results"]
    # Why:
    # - Your biggest payload is typically results.baseline_sources_cache (full snapshots)
    # - The previous summarizer only handled top-level keys, so Sheets payload still exceeded limits
    # - This keeps the saved JSON smaller AND keeps json.loads(get_history) working reliably
    # =====================================================================
    try:
        r = out.get("results")
        if isinstance(r, dict):
            r2 = dict(r)

            for big_key in ("baseline_sources_cache", "source_results"):
                if big_key in r2:
                    sr = r2.get(big_key)
                    if isinstance(sr, list):
                        sample = []
                        for item in sr[:2]:
                            if isinstance(item, dict):
                                item2 = dict(item)
                                if isinstance(item2.get("extracted_numbers"), list):
                                    item2["extracted_numbers"] = {
                                        "_summary": True,
                                        "count": len(item2["extracted_numbers"])
                                    }
                                sample.append(item2)
                        r2[big_key] = {"_summary": True, "count": len(sr), "sample": sample}
                    else:
                        r2[big_key] = {"_summary": True, "type": str(type(sr))}

            out["results"] = r2
    except Exception:
        pass
    # =====================================================================

    return out



def make_sheet_safe_json(obj: dict, max_chars: int = 45000) -> str:
    """
    Serialize sheet-safe JSON under the cell limit.

    NOTE / CONFLICT:
      - The prior implementation used _truncate_for_sheets() on the JSON string, which can produce
        invalid JSON (cut mid-string). Invalid JSON rows are skipped by get_history() (json.loads fails),
        so evolution can't pick them up.
      - This patch preserves summarization but replaces raw string truncation with a JSON wrapper
        that is ALWAYS valid JSON.

    Output behavior:
      - If JSON fits: returns full compact JSON string.
      - If too large: returns a valid JSON wrapper with a preview + metadata.
    """
    import json

    # Keep existing behavior: summarize heavy fields
    compact = _summarize_heavy_fields_for_sheets(obj if isinstance(obj, dict) else {"value": obj})
    if isinstance(compact, dict):
        compact["_sheets_safe"] = True

    # Try to serialize
    try:
        s = json.dumps(compact, ensure_ascii=False, default=str)
    except Exception:
        # ultra-safe fallback (still return valid JSON)
        try:
            s = json.dumps({"_sheets_safe": True, "_sheet_write": {"error": "json.dumps failed"}}, ensure_ascii=False)
        except Exception:
            return '{"_sheets_safe":true,"_sheet_write":{"error":"json.dumps failed"}}'

    # If it fits, return as-is
    if isinstance(s, str) and len(s) <= int(max_chars or 45000):
        return s

    # =========================
    # PATCH SS1 (BUGFIX, REQUIRED): valid JSON wrapper when oversized
    # - Never return mid-string truncations that break json.loads in get_history().
    # =========================
    try:
        preview_len = max(0, int(max_chars or 45000) - 700)  # leave room for wrapper fields
        wrapper = {
            "_sheets_safe": True,
            "_sheet_write": {
                "truncated": True,
                "mode": "sheets_safe_wrapper",
                "note": "Payload exceeded cell limit; stored preview only. Full snapshots must be stored separately if needed.",
            },
            # Keep a preview for UI/debugging
            "preview": s[:preview_len],
        }

        # Optional: carry minimal identity fields for convenience (additive)
        if isinstance(obj, dict):
            wrapper["question"] = (obj.get("question") or "")[:200]
            wrapper["timestamp"] = obj.get("timestamp")
            wrapper["code_version"] = obj.get("code_version") or (obj.get("primary_response") or {}).get("code_version")

            # =========================
            # PATCH SS1B (ADDITIVE, REQUIRED FOR SNAPSHOT REHYDRATION):
            # Carry snapshot pointers even when the payload is wrapped.
            # Without these fields, evolution cannot rehydrate full snapshots
            # from the Snapshots worksheet (or local fallback) and will fail
            # the snapshot gate with "No valid snapshots".
            # =========================
            try:
                _ssh = obj.get("source_snapshot_hash") or (obj.get("results") or {}).get("source_snapshot_hash")
                _ref = obj.get("snapshot_store_ref") or (obj.get("results") or {}).get("snapshot_store_ref")
                if _ssh:
                    wrapper["source_snapshot_hash"] = _ssh
                if _ref:
                    wrapper["snapshot_store_ref"] = _ref
            except Exception:
                pass

        return json.dumps(wrapper, ensure_ascii=False, default=str)
    except Exception:
        return '{"_sheets_safe":true,"_sheet_write":{"truncated":true,"mode":"sheets_safe_wrapper","note":"wrapper failed"}}'


# =====================================================================
# PATCH ES1D (ADDITIVE): external snapshot store (local file-based)
# Purpose:
#   - Store full baseline_sources_cache outside Google Sheets when rows
#     are too large (Sheets wrapper / preview mode).
#   - Allow deterministic rehydration for evolution (no refetch).
# =====================================================================
def _snapshot_store_dir() -> str:
    import os
    d = os.path.join(os.getcwd(), "snapshot_store")
    try:
        os.makedirs(d, exist_ok=True)
    except Exception:
        pass
    return d

def store_full_snapshots_local(baseline_sources_cache: list, source_snapshot_hash: str) -> str:
    """
    Store full snapshots deterministically by hash. Returns a store ref string (path).
    Additive-only helper.
    """
    import os, json
    if not source_snapshot_hash:
        return ""
    if not isinstance(baseline_sources_cache, list) or not baseline_sources_cache:
        return ""

    path = os.path.join(_snapshot_store_dir(), f"{source_snapshot_hash}.json")
    try:
        # write-once semantics (deterministic)
        if os.path.exists(path) and os.path.getsize(path) > 0:
            return path
    except Exception:
        pass

    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(baseline_sources_cache, f, ensure_ascii=False, default=str)
        return path
    except Exception:
        return ""

def load_full_snapshots_local(snapshot_store_ref: str) -> list:
    """
    Load full snapshots from a store ref string (path). Returns [] if not available.
    """
    import json, os
    try:
        if not snapshot_store_ref or not isinstance(snapshot_store_ref, str):
            return []
        if not os.path.exists(snapshot_store_ref):
            return []
        with open(snapshot_store_ref, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data if isinstance(data, list) else []
    except Exception:
        return []

# =====================================================================
# PATCH ES1E (ADDITIVE): deterministic source_snapshot_hash helper
# =====================================================================
def compute_source_snapshot_hash(baseline_sources_cache: list) -> str:
    import hashlib
    pairs = []
    for sr in (baseline_sources_cache or []):
        if not isinstance(sr, dict):
            continue
        u = (sr.get("source_url") or sr.get("url") or "").strip()
        fp = (sr.get("fingerprint") or sr.get("content_fingerprint") or "").strip()
        if u:
            pairs.append((u, fp))
    pairs.sort()
    sig = "|".join([f"{u}#{fp}" for (u, fp) in pairs])
    return hashlib.sha256(sig.encode("utf-8")).hexdigest() if sig else ""
# =====================================================================
# =====================================================================
# PATCH SS6 (ADDITIVE): build full baseline_sources_cache from evidence_records
# Why:
# - Sheets-safe summarization may replace baseline_sources_cache/extracted_numbers
#   with summary dicts. However, evidence_records often remains available and is
#   already deterministic, snapshot-derived data.
# - This helper reconstructs the minimal snapshot shape needed for
#   source-anchored evolution WITHOUT re-fetching or heuristic matching.
# =====================================================================

            # =========================
# PATCH A (ADD): Snapshot hash v2 (stable, content-weighted)
# - Keeps v1 compute_source_snapshot_hash() for backward compatibility.
# - v2 includes url + status + fingerprint + (anchor_hash,value_norm,unit_tag) tuples (bounded) for stronger identity.
            # =========================
def compute_source_snapshot_hash_v2(baseline_sources_cache: list, max_items_per_source: int = 120) -> str:
    import hashlib
    import json

    try:
        sources = baseline_sources_cache if isinstance(baseline_sources_cache, list) else []
        parts = []
        for s in sources:
            if not isinstance(s, dict):
                continue
            url = str(s.get("url") or "")
            status = str(s.get("status") or "")
            status_detail = str(s.get("status_detail") or "")
            fingerprint = str(s.get("fingerprint") or "")

            nums = s.get("extracted_numbers") or s.get("numbers") or []
            # Sometimes stored in summarized form
            if isinstance(nums, dict) and nums.get("_summary") and isinstance(nums.get("count"), int):
                # no details available; just use summary
                num_tuples = [("summary_count", int(nums.get("count")))]
            else:
                num_list = nums if isinstance(nums, list) else []
                num_tuples = []
                for n in num_list[: int(max_items_per_source or 120)]:
                    if not isinstance(n, dict):
                        continue
                    ah = str(n.get("anchor_hash") or "")
                    vn = n.get("value_norm")
                    ut = str(n.get("unit_tag") or n.get("unit") or "")
                    # Use JSON for float stability + None handling
                    num_tuples.append((ah, vn, ut))
                # Deterministic order
                num_tuples = sorted(num_tuples, key=lambda t: (t[0], str(t[1]), t[2]))

            parts.append({
                "url": url,
                "status": status,
                "status_detail": status_detail,
                "fingerprint": fingerprint,
                "nums": num_tuples,
            })

        # Deterministic ordering of sources
        parts = sorted(parts, key=lambda d: (d.get("url",""), d.get("fingerprint",""), d.get("status","")))

        payload = json.dumps(parts, ensure_ascii=False, default=str, separators=(",", ":"))
        return hashlib.sha256(payload.encode("utf-8")).hexdigest()
    except Exception:
        # Ultra-safe fallback (still deterministic-ish)
        try:
            return hashlib.sha256(str(baseline_sources_cache).encode("utf-8")).hexdigest()
        except Exception:
            return "0"*64

def build_baseline_sources_cache_from_evidence_records(evidence_records):

    """

    Rebuild a minimal baseline_sources_cache from evidence_records deterministically.


    PATCH AI3 (ADDITIVE): anchor integrity

    - Ensures each candidate has anchor_hash + candidate_id (derived if missing)

    - Preserves analysis-aligned numeric normalization fields when present

    - Deterministic ordering by (source_url, fingerprint)

    """

    import hashlib


    if not isinstance(evidence_records, list) or not evidence_records:

        return []


    def _sha1(s: str) -> str:

        try:

            return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()

        except Exception:

            return ""


    def _ensure_anchor_fields(c: dict, source_url: str = "") -> dict:

        c = c if isinstance(c, dict) else {}

        ctx = c.get("context_snippet") or c.get("context") or ""

        if isinstance(ctx, str):

            ctx = ctx.strip()[:240]

        else:

            ctx = ""

        raw = c.get("raw")

        if raw is None:

            v = c.get("value_norm") if c.get("value_norm") is not None else c.get("value")

            u = c.get("base_unit") or c.get("unit") or ""

            raw = f"{v}{u}"

        raw = str(raw)[:120]


        ah = c.get("anchor_hash") or c.get("anchor") or ""

        if not ah:

            ah = _sha1(f"{source_url}|{raw}|{ctx}")

            if ah:

                c["anchor_hash"] = ah

        if not c.get("candidate_id") and ah:

            c["candidate_id"] = str(ah)[:16]

        if source_url and not c.get("source_url"):

            c["source_url"] = source_url

        if ctx and not c.get("context_snippet"):

            c["context_snippet"] = ctx

        return c


    by_url = {}

    for rec in evidence_records:

        if not isinstance(rec, dict):

            continue

        url = rec.get("source_url") or rec.get("url") or ""

        fp = rec.get("fingerprint") or ""

        # candidates may be stored under candidates or extracted_numbers depending on producer

        cand_list = rec.get("candidates")

        if not isinstance(cand_list, list):

            cand_list = rec.get("extracted_numbers")

        if not isinstance(cand_list, list):

            cand_list = []


        out_cands = []

        for c in cand_list:

            if not isinstance(c, dict):

                continue

            cc = _ensure_anchor_fields(dict(c), url)

            out_cands.append(cc)


        if not out_cands:

            continue


        key = (str(url), str(fp))

        by_url.setdefault(key, {"source_url": url, "fingerprint": fp, "extracted_numbers": []})

        by_url[key]["extracted_numbers"].extend(out_cands)


    rebuilt = list(by_url.values())


    # deterministic sort & per-source deterministic candidate order

    try:

        rebuilt.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))

        for s in rebuilt:

            if isinstance(s, dict) and isinstance(s.get("extracted_numbers"), list):

                s["extracted_numbers"] = sorted(

                    s["extracted_numbers"],

                    key=lambda c: (

                        str(c.get("anchor_hash") or ""),

                        str(c.get("candidate_id") or ""),

                        str(c.get("raw") or ""),

                        str(c.get("unit") or ""),

                    )

                )

    except Exception:

        pass


    # =====================================================================
    # PATCH FIX41AFC5 (ADDITIVE): attach eligibility-hardening debug counters
    # =====================================================================
    try:
        if isinstance(rebuilt, dict):
            rebuilt.setdefault("_fix41afc5_debug", {})
            if isinstance(rebuilt.get("_fix41afc5_debug"), dict):
                rebuilt["_fix41afc5_debug"].update(dict(_fix41afc5_dbg2))
    except Exception:
        pass
    # =====================================================================

    return rebuilt
def _sheets_now_ts():
    import time
    return time.time()

def _sheets_cache_get(key: str):
    try:
        item = _SHEETS_READ_CACHE.get(key)
        if not item:
            return None
        ts, val = item
        if (_sheets_now_ts() - ts) > _SHEETS_READ_CACHE_TTL_SEC:
            return None
        return val
    except Exception:
        return None

def _sheets_cache_set(key: str, val):
    try:
        _SHEETS_READ_CACHE[key] = (_sheets_now_ts(), val)
    except Exception:
        pass

def _is_sheets_rate_limit_error(err: Exception) -> bool:
    s = ""
    try:
        s = str(err) or ""
    except Exception:
        s = ""
    # Common markers seen via gspread/googleapiclient:
    markers = ["RESOURCE_EXHAUSTED", "Quota exceeded", "RATE_LIMIT_EXCEEDED", "429"]
    return any(m in s for m in markers)

def sheets_get_all_values_cached(ws, cache_key: str):
    """
    Cached wrapper for ws.get_all_values() with rate-limit fallback.
    cache_key should be stable for the worksheet (e.g., 'Snapshots', 'HistoryFull', 'History').
    """
    global _SHEETS_LAST_READ_ERROR
    key = f"get_all_values:{cache_key}"
    cached = _sheets_cache_get(key)
    if cached is not None:
        return cached
    try:
        # === PATCH SHEETS_CACHE1 (CONFLICT FIX, MINIMAL): call the underlying worksheet read ===
        # Previous draft accidentally recursed into itself and referenced an undefined variable.
        # This is a direct execution conflict fix (no behavior change intended beyond correctness).
        values = ws.get_all_values() if ws else []
        _sheets_cache_set(key, values)
        return values
    except Exception as e:
        _SHEETS_LAST_READ_ERROR = str(e)
        # Rate-limit fallback: return last cached value if we have one, else empty list
        if _is_sheets_rate_limit_error(e):
            stale = _SHEETS_READ_CACHE.get(key)
            if stale and isinstance(stale, tuple) and len(stale) == 2:
                return stale[1]
            return []
        raise

# =====================================================================
# PATCH SS2 (ADDITIVE): Google Sheets snapshot store (separate worksheet)
# Purpose:
#   - Persist full baseline_sources_cache inside the same Spreadsheet
#     but in a dedicated worksheet (tab), chunked across rows.
#   - Enables deterministic rehydration for evolution without refetch.
# Notes:
#   - Write-once semantics by source_snapshot_hash.
#   - Chunking and reassembly are deterministic (part_index ordering).
# =====================================================================
def get_google_spreadsheet():
    """Connect to Google Spreadsheet (cached connection if available)."""
    try:
        # If get_google_sheet() exists and already opened the spreadsheet as sheet.sheet1,
        # we re-open to obtain the Spreadsheet handle (additive; avoids refactoring).
        import streamlit as st
        from google.oauth2.service_account import Credentials
        import gspread

        SCOPES = [
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive",
        ]
        creds = Credentials.from_service_account_info(
            dict(st.secrets["gcp_service_account"]),
            scopes=SCOPES
        )
        client = gspread.authorize(creds)
        spreadsheet_name = st.secrets.get("google_sheets", {}).get("spreadsheet_name", "Yureeka_JSON")
        return client.open(spreadsheet_name)
    except Exception:
        return None

def _ensure_snapshot_worksheet(spreadsheet, title: str = "Snapshots"):
    """Ensure a worksheet tab exists for snapshot storage."""
    try:
        if not spreadsheet:
            return None
        try:
            ws = spreadsheet.worksheet(title)
            return ws
        except Exception:
            # Create with a reasonable default size; Sheets can expand.
            ws = spreadsheet.add_worksheet(title=title, rows=2000, cols=8)
            try:
                ws.append_row(
                    ["source_snapshot_hash", "part_index", "total_parts", "payload_part", "created_at", "code_version", "fingerprints_sig", "sha256"],
                    value_input_option="RAW",
                )
            except Exception:
                pass
            return ws
    except Exception:
        return None

def store_full_snapshots_to_sheet(baseline_sources_cache: list, source_snapshot_hash: str, worksheet_title: str = "Snapshots", chunk_chars: int = 45000) -> str:
    """
    Store full snapshots to a dedicated worksheet tab in chunked rows.
    Returns a ref string like: 'gsheet:Snapshots:<hash>'
    """
    import json, hashlib
    if not source_snapshot_hash:
        return ""
    if not isinstance(baseline_sources_cache, list) or not baseline_sources_cache:
        return ""

    try:
        ss = get_google_spreadsheet()
        ws = _ensure_snapshot_worksheet(ss, worksheet_title) if ss else None
        if not ws:
            return ""

        # Write-once: if hash already present, do not write again.
        try:
            # Find any existing rows for this hash (skip header)
            existing = ws.findall(source_snapshot_hash)
            if existing:
                return f"gsheet:{worksheet_title}:{source_snapshot_hash}"
        except Exception:
            # best effort; continue to attempt write
            pass

        payload = json.dumps(baseline_sources_cache, ensure_ascii=False, default=str)
        sha = hashlib.sha256(payload.encode("utf-8")).hexdigest()

        # deterministic chunking
        chunk_size = max(1000, int(chunk_chars or 45000))
        parts = [payload[i:i+chunk_size] for i in range(0, len(payload), chunk_size)]
        total = len(parts)

        # Optional fingerprints signature (stable)
        pairs = []
        for sr in baseline_sources_cache:
            if isinstance(sr, dict):
                u = (sr.get("source_url") or sr.get("url") or "").strip()
                fp = (sr.get("fingerprint") or sr.get("content_fingerprint") or "").strip()
                if u:
                    pairs.append((u, fp))
        pairs.sort()
        fingerprints_sig = "|".join([f"{u}#{fp}" for (u, fp) in pairs]) if pairs else ""

        from datetime import datetime
        created_at = datetime.utcnow().isoformat() + "Z"

        # Append rows in order (deterministic)
        code_version = ""
        try:
            # best effort: use global if exists
            code_version = globals().get("CODE_VERSION") or ""
        except Exception:
            code_version = ""

        # Use append_rows if available, else append_row in loop
        rows = []
        for idx, part in enumerate(parts):
            rows.append([source_snapshot_hash, idx, total, part, created_at, code_version, fingerprints_sig, sha])

        try:
            ws.append_rows(rows, value_input_option="RAW")
        except Exception:
            for r in rows:
                try:
                    ws.append_row(r, value_input_option="RAW")
                except Exception:
                    # partial failure: still return empty to avoid false pointer
                    return ""

        return f"gsheet:{worksheet_title}:{source_snapshot_hash}"
    except Exception:
        return ""

def load_full_snapshots_from_sheet(source_snapshot_hash: str, worksheet_title: str = "Snapshots") -> list:
    """Load and reassemble full snapshots list from a dedicated worksheet."""
    import json, hashlib
    if not source_snapshot_hash:
        return []
    try:
        ss = get_google_spreadsheet()
        ws = ss.worksheet(worksheet_title) if ss else None
        if not ws:
            return []

        # =====================================================================
        # PATCH SNAPLOAD1 (ADDITIVE): cache-safe snapshot read fallback
        # Why:
        # - If a prior read hit quota / partial failure and we cached [], evolution
        #   will permanently think "no snapshots exist" until cache clears.
        # Behavior:
        # - Try cached read first (fast)
        # - If empty/too small, do ONE direct read to bypass stale empty cache
        # =====================================================================
        values = []
        try:
            values = sheets_get_all_values_cached(ws, cache_key=worksheet_title)
        except Exception:
            values = []

        if not values or len(values) < 2:
            # Direct retry (best-effort)
            try:
                direct = ws.get_all_values()
                if direct and len(direct) >= 2:
                    values = direct
            except Exception:
                pass
        # =====================================================================
        # END PATCH SNAPLOAD1 (ADDITIVE)
        # =====================================================================

        if not values or len(values) < 2:
            return []

        header = values[0] or []
        # Expect at least: source_snapshot_hash, part_index, total_parts, payload_part
        try:
            col_h = header.index("source_snapshot_hash")
            col_i = header.index("part_index")
            col_t = header.index("total_parts")
            col_p = header.index("payload_part")
            col_sha = header.index("sha256") if "sha256" in header else None
        except Exception:
            # If headers are missing/misaligned, bail safely
            return []

        # Filter rows for this hash
        rows = []
        for r in values[1:]:
            try:
                if len(r) > col_h and r[col_h] == source_snapshot_hash:
                    rows.append(r)
            except Exception:
                continue

        if not rows:
            return []

        # Deterministic sort by part_index
        def _safe_int(x):
            try:
                return int(x)
            except Exception:
                return 0
        rows.sort(key=lambda r: _safe_int(r[col_i] if len(r) > col_i else 0))

        # Reassemble
        payload_parts = []
        for r in rows:
            if len(r) > col_p:
                payload_parts.append(r[col_p] or "")
        payload = "".join(payload_parts)

        # Optional integrity check
        try:
            if col_sha is not None and len(rows[0]) > col_sha:
                expected = rows[0][col_sha] or ""
                if expected:
                    actual = hashlib.sha256(payload.encode("utf-8")).hexdigest()
                    if actual != expected:
                        return []
        except Exception:
            pass

        try:
            data = json.loads(payload)
            return data if isinstance(data, list) else []
        except Exception:
            return []
    except Exception:
        return []

# =====================================================================
# PATCH HF4 (ADDITIVE): HistoryFull payload rehydration support
# Why:
# - Evolution may receive a sheets-safe wrapper that omits primary_response,
#   metric_schema_frozen, metric_anchors, etc.
# - When wrapper includes full_store_ref ("gsheet:HistoryFull:<analysis_id>"),
#   we can deterministically load the full analysis payload (no re-fetch).
# Notes:
# - Additive only. Safe no-op if sheet/tab not present.
# =====================================================================

# ===================== PATCH HF_WRITE1 (ADDITIVE) =====================

def write_full_history_payload_to_sheet(analysis_id: str, payload: str, worksheet_title: str = "HistoryFull", chunk_size: int = 20000) -> bool:
    """Write a full analysis payload (string) into HistoryFull as chunked rows keyed by analysis_id.

    Additive helper to support oversized History cells:
      - Ensures HistoryFull headers exist
      - Splits payload into chunks
      - Stores sha256 for integrity
    """
    import hashlib
    if not analysis_id or not payload:
        return False
    try:
        ss = get_google_spreadsheet()
        if not ss:
            return False
        try:
            ws = ss.worksheet(worksheet_title)
        except Exception:
            # Create sheet if missing (best-effort)
            try:
                ws = ss.add_worksheet(title=worksheet_title, rows=2000, cols=10)
            except Exception:
                ws = ss.worksheet(worksheet_title)

        # Ensure headers exist
        try:
            headers = ws.row_values(1)
            if (not headers) or (len(headers) < 5) or headers[0] != "analysis_id":
                _ = ws.update('A1:E1', [["analysis_id", "part_index", "total_parts", "payload_part", "sha256"]])
        except Exception:
            try:
                _ = ws.update('A1:E1', [["analysis_id", "part_index", "total_parts", "payload_part", "sha256"]])
            except Exception:
                pass

        # Remove existing rows for this analysis_id (optional; keep additive and safe: do not delete to avoid refactor)
        # We will append new parts; loader will read the latest by order if duplicates exist.

        sha = hashlib.sha256(payload.encode("utf-8", errors="ignore")).hexdigest()
        parts = [payload[i:i+chunk_size] for i in range(0, len(payload), chunk_size)]
        total = len(parts) if parts else 0
        if total == 0:
            return False

        rows = []
        for i, part in enumerate(parts):
            rows.append([analysis_id, str(i), str(total), part, sha])

        # Append in one batch if possible
        try:
            ws.append_rows(rows, value_input_option="RAW")
        except Exception:
            # Fallback to per-row append
            for r in rows:
                try:
                    ws.append_row(r, value_input_option="RAW")
                except Exception:
                    return False
        return True
    except Exception:
        return False

# =================== END PATCH HF_WRITE1 (ADDITIVE) ===================

def load_full_history_payload_from_sheet(analysis_id: str, worksheet_title: str = "HistoryFull") -> dict:
    """
    Load the full analysis JSON payload from the HistoryFull worksheet.

    PATCH HF_LOAD_V2 (ADDITIVE):
    - Supports payloads split across multiple rows (chunked writes).
    - Supports current HistoryFull headers:
        analysis_id, part_index, total_parts, payload_json, created_at, code_version, sha256
    - Backward compatible with older variants:
        id, part_index, total_parts, payload_part / data, sha256
    - Deterministic stitching (sort by part_index) + safe JSON parse.

    PATCH HF_LOAD_V3 (ADDITIVE):
    - Verifies chunk completeness when total_parts is available (0..total_parts-1).
    - Optionally verifies sha256 when present (stitched string).
    - Does not change failure mode: still returns {} on any failure.
    """
    try:
        ss = get_google_spreadsheet()
        if not ss:
            return {}

        try:
            ws = ss.worksheet(worksheet_title)
        except Exception:
            return {}

        # Read all rows (prefer cached getter if present)
        try:
            fn = globals().get("sheets_get_all_values_cached")
            rows = fn(ws, cache_key=worksheet_title) if callable(fn) else (ws.get_all_values() or [])
        except Exception:
            try:
                rows = ws.get_all_values() or []
            except Exception:
                return {}

        if not rows or len(rows) < 2:
            return {}

        header = rows[0] or []
        body = rows[1:] or []

        def _col(name: str):
            try:
                return header.index(name)
            except Exception:
                return None

        # -----------------------------------------------------------------
        # PATCH HF_LOAD_V2_COLS (ADDITIVE): map to real sheet headers + legacy
        # -----------------------------------------------------------------
        c_id = _col("analysis_id")
        if c_id is None:
            c_id = _col("id")
        if c_id is None:
            c_id = 0  # last-ditch fallback

        c_part = _col("part_index")
        c_total = _col("total_parts")

        # IMPORTANT: your sheet uses payload_json
        c_payload = _col("payload_json")
        if c_payload is None:
            c_payload = _col("payload_part")
        if c_payload is None:
            c_payload = _col("data")
        if c_payload is None:
            c_payload = len(header) - 1  # last-ditch fallback

        c_sha = _col("sha256")
        # -----------------------------------------------------------------

        target_id = str(analysis_id).strip()
        if not target_id:
            return {}

        # (pidx:int|None, total:int|None, chunk:str, sha:str)
        parts_with_sha = []

        for r in body:
            try:
                if not r:
                    continue

                rid = r[c_id] if c_id < len(r) else ""
                rid = str(rid).strip()
                if rid != target_id:
                    continue

                chunk = r[c_payload] if c_payload < len(r) else ""
                chunk = chunk or ""
                if not isinstance(chunk, str):
                    chunk = str(chunk)

                # part_index (optional)
                pidx = None
                if c_part is not None and c_part < len(r):
                    try:
                        pidx = int(str(r[c_part]).strip())
                    except Exception:
                        pidx = None

                # total_parts (optional)
                tparts = None
                if c_total is not None and c_total < len(r):
                    try:
                        tparts = int(str(r[c_total]).strip())
                    except Exception:
                        tparts = None

                sha = ""
                if c_sha is not None and c_sha < len(r):
                    sha = str(r[c_sha] or "").strip()

                # keep even tiny chunks; concatenation is deterministic
                if chunk.strip() == "":
                    continue

                parts_with_sha.append((pidx, tparts, chunk, sha))
            except Exception:
                continue

        if not parts_with_sha:
            return {}

        # -----------------------------------------------------------------
        # PATCH HF_LOAD_V2_BUCKET (ADDITIVE): if duplicates exist, pick best sha bucket
        # Score = (unique part_index count, total payload length)
        # -----------------------------------------------------------------
        parts = []          # list[(pidx, chunk)]
        chosen_sha = ""     # sha bucket selected (if any)
        chosen_total = None # total_parts inferred for chosen bucket (if any)
        try:
            if any(s for _, _, _, s in parts_with_sha):
                buckets = {}
                for pidx, tparts, chunk, sha in parts_with_sha:
                    key = sha or "__no_sha__"
                    buckets.setdefault(key, []).append((pidx, tparts, chunk))

                def _score(items):
                    idxs = [i for i, _, _ in items if i is not None]
                    uniq = len(set(idxs)) if idxs else 0
                    total_len = sum(len(c or "") for _, _, c in items)
                    return (uniq, total_len)

                best_key = sorted(buckets.keys(), key=lambda k: _score(buckets[k]), reverse=True)[0]
                chosen_sha = "" if best_key == "__no_sha__" else best_key
                best_items = buckets[best_key]

                # Infer total_parts for this bucket (mode / max)
                try:
                    totals = [tp for _, tp, _ in best_items if isinstance(tp, int) and tp > 0]
                    chosen_total = max(totals) if totals else None
                except Exception:
                    chosen_total = None

                parts = [(pidx, chunk) for (pidx, _tparts, chunk) in best_items]
            else:
                parts = [(pidx, chunk) for pidx, _tparts, chunk, _sha in parts_with_sha]
                # Infer total_parts (mode / max) even without sha
                try:
                    totals = [tp for _, tp, _, _ in parts_with_sha if isinstance(tp, int) and tp > 0]
                    chosen_total = max(totals) if totals else None
                except Exception:
                    chosen_total = None
        except Exception:
            parts = [(pidx, chunk) for pidx, _tparts, chunk, _sha in parts_with_sha]
            try:
                totals = [tp for _, tp, _, _ in parts_with_sha if isinstance(tp, int) and tp > 0]
                chosen_total = max(totals) if totals else None
            except Exception:
                chosen_total = None
        # -----------------------------------------------------------------

        # Sort parts deterministically by part_index; None last
        def _sort_key(t):
            pidx, _ = t
            return (pidx is None, pidx if pidx is not None else 0)

        parts.sort(key=_sort_key)

        # -----------------------------------------------------------------
        # PATCH HF_LOAD_V3_COMPLETE (ADDITIVE): completeness check when total_parts known
        # - If total_parts is known and we have part_index values, require 0..total-1.
        # - If incomplete, return {} (do not attempt parse on partial payload).
        # -----------------------------------------------------------------
        try:
            if isinstance(chosen_total, int) and chosen_total > 0:
                idxs = [p for (p, _c) in parts if isinstance(p, int)]
                if idxs:
                    uniq = sorted(set(idxs))
                    expected = list(range(0, chosen_total))
                    if uniq != expected:
                        return {}
        except Exception:
            pass
        # -----------------------------------------------------------------

        # Stitch chunks
        full_json_str = "".join([chunk for _, chunk in parts]).strip()
        if not full_json_str:
            return {}

        # -----------------------------------------------------------------
        # PATCH HF_LOAD_V3_SHA (ADDITIVE): verify sha256 when present
        # - If chosen_sha exists, compare against sha256(stitched_bytes).
        # - If mismatch, return {} (treat as corrupted / wrong bucket).
        # -----------------------------------------------------------------
        try:
            if isinstance(chosen_sha, str) and chosen_sha:
                import hashlib
                digest = hashlib.sha256(full_json_str.encode("utf-8", errors="ignore")).hexdigest()
                if str(digest).lower() != str(chosen_sha).lower():
                    return {}
        except Exception:
            pass
        # -----------------------------------------------------------------

        import json
        try:
            obj = json.loads(full_json_str)
            if isinstance(obj, dict):
                # -----------------------------------------------------------------
                # PATCH HF_LOAD_V3_META (ADDITIVE): optional debug stamp (harmless)
                # Only attaches when parse succeeded.
                # -----------------------------------------------------------------
                try:
                    obj["_rehydration_debug"] = {
                        "worksheet": str(worksheet_title or ""),
                        "analysis_id": str(target_id),
                        "parts_used": int(len(parts)),
                        "total_parts_expected": int(chosen_total) if isinstance(chosen_total, int) else None,
                        "sha_verified": bool(chosen_sha),
                    }
                except Exception:
                    pass
                # -----------------------------------------------------------------
                return obj
            return {}
        except Exception:
            # PATCH HF_LOAD_V2_SALVAGE (ADDITIVE): common salvage for leading/trailing junk
            try:
                # Try to isolate first "{" and last "}" if accidental prefix/suffix exists
                a = full_json_str.find("{")
                b = full_json_str.rfind("}")
                if a != -1 and b != -1 and b > a:
                    obj2 = json.loads(full_json_str[a:b+1])
                    if isinstance(obj2, dict):
                        # (keep same meta stamp behavior)
                        try:
                            obj2["_rehydration_debug"] = {
                                "worksheet": str(worksheet_title or ""),
                                "analysis_id": str(target_id),
                                "parts_used": int(len(parts)),
                                "total_parts_expected": int(chosen_total) if isinstance(chosen_total, int) else None,
                                "sha_verified": bool(chosen_sha),
                                "salvaged": True,
                            }
                        except Exception:
                            pass
                        return obj2
            except Exception:
                pass
            return {}

    except Exception:
        return {}

def fingerprint_text(text: str) -> str:
    """Stable short fingerprint for fetched content (deterministic)."""
    if not text:
        return ""
    normalized = re.sub(r"\s+", " ", text.strip().lower())
    return hashlib.md5(normalized.encode("utf-8")).hexdigest()[:12]

def attach_source_snapshots_to_analysis(analysis: dict, web_context: dict) -> dict:
    """
    Attach stable source snapshots (from web_context.scraped_meta) into analysis.

    Enhancements (v7_34 patch):
    - Ensures scraped_meta.extracted_numbers is always list-like
    - Adds RANGE capture per canonical metric using admitted snapshots:
        primary_metrics_canonical[ckey]["value_range"] = {min,max,n,examples}
      This restores earlier "range vs point estimate" behavior in a compatible way.
    """
    import re
    from datetime import datetime, timezone

    def _now_iso() -> str:
        return datetime.now(timezone.utc).isoformat()

    def _fingerprint(text: str) -> str:
        try:
            fn = globals().get("fingerprint_text")
            if callable(fn):
                return fn(text)
        except Exception:
            pass
        try:
            import hashlib
            t = re.sub(r"\s+", " ", (text or "").strip().lower())
            return hashlib.md5(t.encode("utf-8", errors="ignore")).hexdigest()[:12]
        except Exception:
            return ""

    # =========================================================================
    # PATCH N1 (ADDITIVE): stable anchor_hash fallback helper for snapshots
    # - Does NOT change existing behavior if anchor_hash already present.
    # =========================================================================
    def _sha1(s: str) -> str:
        try:
            import hashlib
            return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()
        except Exception:
            return ""
    # =========================================================================

    # =========================================================================
    # PATCH N2 (ADDITIVE): optional canonicalizer hook for snapshot numbers
    # - Ensures unit_tag/unit_family/base_unit/value_norm are present when possible.
    # - No behavior change if helper missing.
    # =========================================================================
    _canon_fn = globals().get("canonicalize_numeric_candidate")
    def _maybe_canonicalize(n: dict) -> dict:
        try:
            if callable(_canon_fn):
                return _canon_fn(dict(n))
        except Exception:
            pass
        return dict(n)
    # =========================================================================

    def _parse_num(value, unit_hint=""):
        try:
            fn = globals().get("parse_human_number")
            if callable(fn):
                return fn(str(value), unit_hint)
        except Exception:
            pass
        # fallback
        try:
            s = str(value).strip().replace(",", "")
            if not s:
                return None
            return float(re.findall(r"-?\d+(?:\.\d+)?", s)[0])
        except Exception:
            return None

    def _unit_family_from_metric(mdef: dict) -> str:
        # prefer metric schema
        uf = (mdef or {}).get("unit_family") or ""
        uf = str(uf).lower().strip()
        if uf in ("percent", "pct"):
            return "PCT"
        if uf in ("currency",):
            return "CUR"
        if uf in ("magnitude", "unit_sales", "other"):
            return "MAG"
        return "OTHER"

    def _cand_unit_family(cunit: str, craw: str) -> str:
        u = (cunit or "").strip()
        r = (craw or "")
        uu = u.upper()
        ru = r.upper()

        # Percent
        if uu == "%" or "%" in ru:
            return "PCT"

        # Energy
        if any(x in (u or "").lower() for x in ["twh", "gwh", "mwh", "kwh"]) or any(x in (r or "").lower() for x in ["twh", "gwh", "mwh", "kwh"]):
            return "ENERGY"

        # Currency (symbol/code presence)
        #if any(x in ru for x in ["$", "USD", "SGD", "EUR", "GBP", "S$"]) or uu in ("USD", "SGD", "EUR", "GBP"):
        #    return "CUR"

        if re.search(r"(\$|S\$|€|£)\s*\d", r) or any(x in ru for x in ["USD", "SGD", "EUR", "GBP"]) or uu in ("USD","SGD","EUR","GBP"):
            return "CUR"


        # Magnitude (case-insensitive)
        if uu in ("K", "M", "B", "T") or (u or "").lower() in ("k", "m", "b", "t"):
            return "MAG"

        return "OTHER"

    def _tokenize(s: str):
        return [t for t in re.findall(r"[a-z0-9]+", (s or "").lower()) if len(t) > 2]

    def _safe_norm_unit_tag(x: str) -> str:
        try:
            fn = globals().get("normalize_unit_tag")
            if callable(fn):
                return fn(x or "")
        except Exception:
            pass
        return (x or "").strip()


    # -----------------------------
    # Build baseline_sources_cache from scraped_meta (snapshot-friendly)
    # -----------------------------
    baseline_sources_cache = []
    scraped_meta = (web_context or {}).get("scraped_meta") or {}
    if isinstance(scraped_meta, dict):
        for url, meta in scraped_meta.items():
            if not isinstance(meta, dict):
                continue
            nums = meta.get("extracted_numbers") or []
            if nums is None or not isinstance(nums, list):
                nums = []

            content = meta.get("content") or meta.get("clean_text") or (web_context.get("scraped_content", {}) or {}).get(url, "") or ""

            baseline_sources_cache.append({
                "url": url,
                "status": "fetched" if str(meta.get("status_detail", "")).startswith("success") or meta.get("status") == "fetched" else "failed",
                "status_detail": meta.get("status_detail") or meta.get("status") or "",
                "numbers_found": int(meta.get("numbers_found") or (len(nums) if isinstance(nums, list) else 0)),
                "fetched_at": meta.get("fetched_at") or _now_iso(),
                "fingerprint": meta.get("fingerprint") or _fingerprint(content),

                # =====================================================================
                # PATCH N1 (+ N2) (ADDITIVE): preserve full candidate record in snapshots
                # - This is critical for:
                #   * range gating (metric-aware)
                #   * schema-first attribution
                #   * evolution rebuild (anchor_hash + value_norm + unit_family)
                # - Backward compatible: only adds keys; existing keys unchanged.
                # =====================================================================
                "extracted_numbers": [
                    (lambda nn: {
                        "value": nn.get("value"),
                        "unit": nn.get("unit"),
                        "raw": nn.get("raw"),
                        "context_snippet": (nn.get("context_snippet") or nn.get("context") or "")[:240],

                        # keep existing anchor_hash if present; else stable fallback
                        "anchor_hash": (
                            nn.get("anchor_hash")
                            or _sha1(
                                f"{url}|{str(nn.get('raw') or '')}|{(nn.get('context_snippet') or nn.get('context') or '')[:240]}"
                            )
                        ),

                        "source_url": nn.get("source_url") or url,

                        # ---- Additive: junk tagging & deterministic offsets ----
                        "is_junk": nn.get("is_junk"),
                        "junk_reason": nn.get("junk_reason"),
                        "start_idx": nn.get("start_idx"),
                        "end_idx": nn.get("end_idx"),

                        # ---- Additive: normalized unit fields (if already present or canonicalized) ----
                        "unit_tag": nn.get("unit_tag"),
                        "unit_family": nn.get("unit_family"),
                        "base_unit": nn.get("base_unit"),
                        "multiplier_to_base": nn.get("multiplier_to_base"),
                        "value_norm": nn.get("value_norm"),

                        # ---- Additive: semantic association tags (if present) ----
                        "measure_kind": nn.get("measure_kind"),
                        "measure_assoc": nn.get("measure_assoc"),
                    })(_maybe_canonicalize(n))
                    for n in nums
                    if isinstance(n, dict)
                ]
                # =====================================================================
            })

    if baseline_sources_cache:

        # ---- ADDITIVE: stable ordering of snapshots (Change #2) ----
        for s in (baseline_sources_cache or []):
            if isinstance(s, dict) and isinstance(s.get("extracted_numbers"), list):

                # =========================================================================
                # PATCH N3 (ADDITIVE): guard sort_snapshot_numbers if not defined
                # =========================================================================
                try:
                    if "sort_snapshot_numbers" in globals() and callable(globals()["sort_snapshot_numbers"]):
                        s["extracted_numbers"] = sort_snapshot_numbers(s["extracted_numbers"])
                    else:
                        # safe fallback: anchor_hash then raw
                        s["extracted_numbers"] = sorted(
                            s["extracted_numbers"],
                            key=lambda x: (str((x or {}).get("anchor_hash") or ""), str((x or {}).get("raw") or ""))
                        )
                except Exception:
                    pass
                # =========================================================================

                s["numbers_found"] = len(s["extracted_numbers"])

        baseline_sources_cache = sorted(
            baseline_sources_cache,
            key=lambda x: str((x or {}).get("url") or "")
        )
        # -----------------------------------------------------------

        # =====================================================================
        # PATCH INJ_HASH_V1_APPLY (ADDITIVE): optionally include injected URLs in snapshot hash identity
        # - Adds *synthetic* url-only source records for injected URLs that were
        #   persisted (per diag) but are missing from baseline_sources_cache.
        # - Default OFF; only activates when INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH is enabled.
        # - Does NOT alter fastpath logic or metric selection (synthetic has no numbers).
        # =====================================================================
        _inj_hash_added = []
        _inj_hash_reasons = {}
        try:
            _diag_local = {}
            if isinstance(web_context, dict):
                _diag_local = web_context.get("diag_injected_urls") or web_context.get("extra_urls_debug") or {}
            _persisted_for_hash = []
            if isinstance(_diag_local, dict):
                _persisted_for_hash = _inj_diag_norm_url_list(
                    _diag_local.get("persisted_norm") or _diag_local.get("persisted") or []
                )
            _incl_inj_hash = _inj_hash_policy_should_include(_persisted_for_hash)
            if _incl_inj_hash and _persisted_for_hash:
                _bsc_aug, _inj_hash_added, _inj_hash_reasons = _inj_hash_add_synthetic_sources(
                    baseline_sources_cache,
                    _persisted_for_hash,
                    now_iso=_now_iso(),
                )
                baseline_sources_cache = _bsc_aug
        except Exception:
            _inj_hash_added = []
            _inj_hash_reasons = {}
        # =====================================================================

        analysis["baseline_sources_cache"] = baseline_sources_cache
        analysis.setdefault("results", {})
        if isinstance(analysis["results"], dict):

            # =====================================================================
            # PATCH INJ_DIAG_ATTACH_SNAPSHOTS (ADDITIVE): propagate injected-URL trace into analysis
            # - Captures persisted snapshot URLs + exact hash input URL set (A4-A5)
            # - Does NOT alter any gating/selection logic.
            # =====================================================================
            try:
                _diag = {}
                if isinstance(web_context, dict):
                    _diag = web_context.get("diag_injected_urls") or web_context.get("extra_urls_debug") or {}

                _inj_urls = []
                if isinstance(_diag, dict):
                    _inj_urls = _inj_diag_norm_url_list(
                        _diag.get("intake_norm")
                        or _diag.get("extra_urls_normalized")
                        or _diag.get("extra_urls")
                        or []
                    )

                _snap_urls = _inj_diag_hash_inputs_from_bsc(baseline_sources_cache)
                _hash_inputs = _snap_urls

                _h_v1 = ""
                _h_v2 = ""
                try:
                    _h_v1 = compute_source_snapshot_hash(baseline_sources_cache)
                except Exception:
                    _h_v1 = ""
                try:
                    _h_v2 = compute_source_snapshot_hash_v2(baseline_sources_cache)
                except Exception:
                    _h_v2 = ""

                analysis.setdefault("results", {})
                if isinstance(analysis.get("results"), dict):
                    analysis["results"].setdefault("debug", {})
                    if isinstance(analysis["results"].get("debug"), dict):
                        analysis["results"]["debug"].setdefault("inj_diag", {})
                        analysis["results"]["debug"]["inj_diag"].update({
                            "run_id": str((web_context or {}).get("diag_run_id") or _diag.get("run_id") or ""),
                            "injected_urls": _inj_urls[:50],
                            "snapshot_pool_urls_count": int(len(_snap_urls)),
                            "snapshot_pool_urls_hash": _inj_diag_set_hash(_snap_urls),
                            "hash_input_urls_count": int(len(_hash_inputs)),
                            "hash_input_urls_hash": _inj_diag_set_hash(_hash_inputs),
                            "injected_in_snapshot_pool": sorted(list(set(_inj_urls) & set(_snap_urls)))[:50],
                            "injected_in_hash_inputs": sorted(list(set(_inj_urls) & set(_hash_inputs)))[:50],
                            "computed_hash_v1": _h_v1,
                            "computed_hash_v2": _h_v2,
                        })


                        # =====================================================================
                        # PATCH INJ_TRACE_V1_EMIT_ANALYSIS (ADDITIVE): always emit canonical trace
                        # Location: analysis.results.debug.inj_trace_v1
                        # =====================================================================
                        try:

                            # =====================================================================
                            # PATCH INJ_TRACE_V1_ENRICH_ANALYSIS_ARTIFACTS (ADDITIVE)
                            # Ensure inj_trace_v1 shows attempted/persisted evidence even when
                            # upstream diag_injected_urls is partial (e.g., baseline/no-injection).
                            # Diagnostics only.
                            # =====================================================================
                            try:
                                if isinstance(_diag, dict):
                                    _diag = _inj_trace_v1_enrich_diag_from_bsc(_diag, baseline_sources_cache)
                            except Exception:
                                pass
                            # =====================================================================

                            _trace = _inj_trace_v1_build(
                                diag_injected_urls=_diag if isinstance(_diag, dict) else {},
                                hash_inputs=_hash_inputs,
                                stage="analysis",
                                path="analysis",
                                rebuild_pool=None,
                                rebuild_selected=None,
                                hash_exclusion_reasons=(_inj_hash_reasons if isinstance(locals().get('_inj_hash_reasons'), dict) else {}),
                            )
                            analysis["results"]["debug"].setdefault("inj_trace_v1", {})
                            # Do not overwrite if already present; only fill/merge
                            if isinstance(analysis["results"]["debug"].get("inj_trace_v1"), dict):
                                analysis["results"]["debug"]["inj_trace_v1"].update(_trace)
                        except Exception:
                            pass
                        # =====================================================================

            except Exception:
                pass
            # =====================================================================

        analysis["results"]["baseline_sources_cache"] = baseline_sources_cache


    # -----------------------------
    # RANGE capture for canonical metrics
    # -----------------------------
    pmc = analysis.get("primary_response", {}).get("primary_metrics_canonical") if isinstance(analysis.get("primary_response"), dict) else analysis.get("primary_metrics_canonical")
    schema = analysis.get("primary_response", {}).get("metric_schema_frozen") if isinstance(analysis.get("primary_response"), dict) else analysis.get("metric_schema_frozen")

    # Support both placements (your JSON seems to store these at top-level primary_response)
    if pmc is None and isinstance(analysis.get("primary_response"), dict):
        pmc = analysis["primary_response"].get("primary_metrics_canonical")
    if schema is None and isinstance(analysis.get("primary_response"), dict):
        schema = analysis["primary_response"].get("metric_schema_frozen")

    if isinstance(pmc, dict) and isinstance(schema, dict) and baseline_sources_cache:
        # flatten candidates
        all_cands = []
        for sr in baseline_sources_cache:
            for n in (sr.get("extracted_numbers") or []):
                if isinstance(n, dict):
                    all_cands.append(n)

        for ckey, m in pmc.items():
            if not isinstance(m, dict):
                continue
            mdef = schema.get(ckey) or {}
            uf = _unit_family_from_metric(mdef)
            keywords = mdef.get("keywords") or []

            kw_tokens = []
            for k in (keywords or []):
                kw_tokens.extend(_tokenize(str(k)))

            kw_tokens.extend(_tokenize(m.get("name") or m.get("original_name") or ""))
            kw_tokens = list(dict.fromkeys([t for t in kw_tokens if len(t) > 2]))[:40]

            vals = []
            examples = []

            for cand in all_cands:
                craw = str(cand.get("raw") or "")
                cunit = str(cand.get("unit") or "")
                ctx = str(cand.get("context_snippet") or cand.get("context") or "")

                # family gate
                cf = _cand_unit_family(cunit, craw)
                if uf == "PCT" and cf != "PCT":
                    continue
                if uf == "CUR" and cf != "CUR":
                    continue
                # MAG: allow MAG/OTHER but avoid CUR/PCT
                if uf == "MAG" and cf in ("CUR", "PCT"):
                    continue

                # NEW (additive): metric-aware magnitude gate
                if uf == "MAG":

                    cand_tag = _safe_norm_unit_tag(cunit or craw)
                    exp_tag = _safe_norm_unit_tag((mdef.get("unit") or "") or (m.get("unit") or ""))


                    if exp_tag in ("K", "M", "B", "T"):
                        if cand_tag != exp_tag:
                            continue
                    else:
                        if cand_tag not in ("K", "M", "B", "T"):
                            continue

                # token overlap gate
                c_tokens = set(_tokenize(ctx))
                if kw_tokens:
                    overlap = sum(1 for t in kw_tokens if t in c_tokens)
                    if overlap < max(1, min(3, len(kw_tokens) // 8)):
                        continue

                v = _parse_num(cand.get("value"), cunit) or _parse_num(craw, cunit)
                if v is None:
                    continue

                vals.append(float(v))
                if len(examples) < 5:
                    examples.append({
                        "raw": craw[:32],
                        "source_url": cand.get("source_url"),
                        "context_snippet": ctx[:180]
                    })

            if len(vals) >= 2:
                vmin = min(vals)
                vmax = max(vals)
                if abs(vmax - vmin) > max(1e-9, abs(vmin) * 0.02):
                    m["value_range"] = {
                        "min": vmin,
                        "max": vmax,
                        "n": len(vals),
                        "examples": examples,
                        "method": "snapshot_candidates"
                    }
                    try:
                        unit_disp = m.get("unit") or ""
                        m["value_range_display"] = f"{vmin:g}–{vmax:g} {unit_disp}".strip()
                    except Exception:
                        pass
    # =====================================================================
    # PATCH V1 (ADDITIVE): analysis & schema version stamping
    # - Pure metadata, NO logic impact
    # - Allows downstream drift attribution:
    #     * pipeline changes vs source changes
    # =====================================================================
    analysis.setdefault("analysis_pipeline_version", "v7_41_endstate_wip_1")
    analysis.setdefault("metric_identity_version", "canon_v2_dim_safe")
    analysis.setdefault("schema_freeze_version", 1)
    # =====================================================================

    # =========================
    # VERSION STAMP (ADDITIVE)
    # =========================
    analysis.setdefault("code_version", CODE_VERSION)
    # =========================
# ============================================================
    # PATCH FIX41AFC36C START — apply defensive backfill to attached snapshot artifacts
    # ============================================================
    try:
        if isinstance(analysis, dict):
            if isinstance(analysis.get("baseline_sources_cache"), (dict, list)):
                _fix41afc36_backfill_in_sources_cache(analysis.get("baseline_sources_cache"))
            if isinstance(analysis.get("results"), dict) and isinstance(analysis["results"].get("source_results"), list):
                for _sr in analysis["results"]["source_results"]:
                    if isinstance(_sr, dict) and isinstance(_sr.get("extracted_numbers"), list):
                        _fix41afc36_backfill_candidates_list(_sr.get("extracted_numbers"))
    except Exception:
        pass
    # ============================================================
    # PATCH FIX41AFC36C END
# ============================================================




    return analysis





# ============================================================
# PATCH FIX41AFC36B START — defensive backfill for snapshot caches (analysis + evolution)
# ============================================================
def _fix41afc36_backfill_candidates_list(_lst):
    try:
        if not isinstance(_lst, list):
            return _lst
        for _c in _lst:
            if not isinstance(_c, dict):
                continue
            uf = str(_c.get("unit_family") or "").strip().lower()
            raw = str(_c.get("raw") or _c.get("raw_disp") or "").lower()
            unit = str(_c.get("unit") or "").lower()
            utag = str(_c.get("unit_tag") or "").lower()
            base = str(_c.get("base_unit") or "").lower()
            ctx = str(_c.get("context") or "").lower()
            blob = " ".join([raw, unit, utag, base, ctx])

            if not uf:
                # ============================================================
                # PATCH FIX41AFC40A START
                # Unit-token authoritative unit_family backfill:
                # Prefer explicit unit/unit_tag/base_unit/raw tokens over nearby context
                # (prevents "17.8 million units" being mislabeled as percent due to nearby "%").
                # ============================================================
                try:
                    _explicit = " ".join([unit, utag, base, raw]).strip().lower()
                    _has_pct = ("%" in _explicit) or ("percent" in _explicit) or ("percentage" in _explicit)
                    _has_ccy = any(t in _explicit for t in ["$", "usd", "us$", "eur", "sgd", "gbp", "aud", "cad", "jpy", "cny", "rmb", "inr", "krw", "chf"])
                    _has_mag = any(t in _explicit for t in ["million", "billion", "trillion", "mn", "bn", "tn"]) or any(t in (" " + _explicit + " ") for t in [" k ", " m ", " b ", " t "])
                    if _has_pct:
                        uf = "percent"
                    elif _has_ccy:
                        uf = "currency"
                    elif _has_mag:
                        uf = "magnitude"
                except Exception:
                    pass
                # PATCH FIX41AFC40A END
                if "%" in blob or "percent" in blob or "percentage" in blob:
                    uf = "percent"

                # ============================================================
                # PATCH FIX41AFC42A START — enforce explicit unit token precedence (post-inference)
                # ============================================================
                try:
                    _fix41afc42_apply_unit_family_lock(_c)
                    uf = str(_c.get("unit_family") or uf or "").strip().lower()
                except Exception:
                    pass
                # ============================================================
                # PATCH FIX41AFC42A END
                # ============================================================

                # ============================================================
                # PATCH FIX41AFC42_SYNTAXFIX START — repair truncated token lists to valid Python
                # ============================================================
                if any(t in blob for t in ["$", "usd", "us$", "eur", "sgd", "gbp", "aud", "cad", "jpy", "cny", "rmb", "inr", "krw", "chf"]):
                    uf = "currency"
                elif ("million" in blob) or ("billion" in blob) or ("trillion" in blob) or any((unit + " " + utag + " " + base).strip() == t for t in ["k","m","b","t"]):
                    uf = "magnitude"
                # ============================================================
                # PATCH FIX41AFC42_SYNTAXFIX END
                # ============================================================
                elif any(t in blob for t in ["kwh", "mwh", "gwh", "twh", "wh", "mw", "gw", "tw"]):
                    uf = "energy"
                else:
                    uf = ""
                _c["unit_family"] = uf


                # ============================================================
                # PATCH FIX41AFC47A START — clear context-bleed unit_family for unitless candidates
                # - Prevents unitless numbers near '%' text from being labeled as percent/currency.
                # ============================================================
                try:
                    _fix41afc47_ctx_bleed_guard(_c)
                except Exception:
                    pass
                # ============================================================
                # PATCH FIX41AFC47A END
                # ============================================================

            unit_ev = any(str(_c.get(k) or "").strip() for k in ["unit", "unit_tag", "base_unit"])
            v = _c.get("value_norm", None)
            if v is None:
                v = _c.get("value", None)
            try:
                if isinstance(v, str) and v.strip():
                    v = float(v.strip())
            except Exception:
                v = None
            try:
                if (not unit_ev) and v is not None:
                    iv = int(round(float(v)))
                    if 1900 <= iv <= 2100 and abs(float(v) - iv) < 1e-9:
                        _c["is_junk"] = True
                        _c["junk_reason"] = (_c.get("junk_reason") or "bare_year_fix41afc36")
            except Exception:
                pass
        return _lst
    except Exception:
        return _lst


# ============================================================
# PATCH FIX41AFC42_HELPERS START — unit_family precedence hard lock (explicit unit tokens win)
# ============================================================
def _fix41afc42_apply_unit_family_lock(_c: dict) -> dict:
    """Enforce unit_family based on explicit unit tokens (unit/unit_tag/base_unit/raw), and lock it.
    This prevents context-derived '%' proximity from overwriting magnitude/currency/energy tokens.
    Additive-only: safe to call repeatedly; respects existing locks unless they are empty/invalid.
    """
    try:
        if not isinstance(_c, dict):
            return _c

        # Respect existing lock unless unit_family is empty
        _locked = bool(_c.get("unit_family_locked_fix41afc42"))
        _uf = str(_c.get("unit_family") or "").strip().lower()

        raw = str(_c.get("raw") or _c.get("raw_disp") or "").strip()
        unit = str(_c.get("unit") or "").strip()
        utag = str(_c.get("unit_tag") or "").strip()
        base = str(_c.get("base_unit") or "").strip()
        blob = " ".join([raw, unit, utag, base]).lower()

        def _has_any(tokens):
            return any(t in blob for t in tokens)

        # Explicit token families (authoritative)
        energy_tokens = ["twh", "gwh", "mwh", "kwh"]
        magnitude_tokens = [" thousand", " million", " billion", " trillion", "k", "m", "b", "t"]
        percent_tokens = ["%", " percent", " percentage", " pct"]
        currency_tokens = ["$", "€", "£", "¥", "usd", "eur", "sgd", "gbp", "jpy", "cny", "rmb", "aud", "cad", "inr", "chf", "hkd", "nzd"]

        forced = ""
        reason = ""

        # Energy first (distinct)
        if _has_any(energy_tokens):
            forced, reason = "energy", "explicit_energy_token"

        # Percent explicit
        if not forced and _has_any(percent_tokens):
            forced, reason = "percent", "explicit_percent_token"

        # Currency explicit (symbols/codes)
        if not forced and _has_any(currency_tokens):
            forced, reason = "currency", "explicit_currency_token"

        # Magnitude explicit (K/M/B/T or million/billion words). Avoid mis-firing on currency codes.
        if not forced:
            # word forms are safest; also single-letter tokens when present in unit fields
            if any(w in blob for w in [" thousand", " million", " billion", " trillion"]):
                forced, reason = "magnitude", "explicit_magnitude_word"
            else:
                # token from unit/unit_tag/base only (avoid random 'm' in text)
                toks = " ".join([unit, utag, base]).strip().lower()
                if toks in {"k", "m", "b", "t"} or toks.endswith("k") or toks.endswith("m") or toks.endswith("b") or toks.endswith("t"):
                    forced, reason = "magnitude", "explicit_magnitude_token"

        # Apply if:
        # - not locked, OR
        # - locked but unit_family is empty (repair)
        if forced and ((not _locked) or (not _uf)):
            _c["unit_family"] = forced
            _c["unit_family_locked_fix41afc42"] = True
            _c["unit_family_lock_reason_fix41afc42"] = reason
        return _c
    except Exception:
        return _c

# ============================================================
# PATCH FIX41AFC42_HELPERS END
# ============================================================


# ============================================================
# PATCH FIX41AFC47_HELPERS START — prevent context-bleed from promoting unitless numbers into percent/currency
# ============================================================
def _fix41afc47_has_explicit_percent_token(_c: dict) -> bool:
    try:
        if not isinstance(_c, dict):
            return False
        raw = str(_c.get("raw") or _c.get("raw_disp") or "").lower()
        unit = str(_c.get("unit") or "").lower()
        utag = str(_c.get("unit_tag") or "").lower()
        base = str(_c.get("base_unit") or "").lower()
        blob = " ".join([raw, unit, utag, base])
        if "%" in blob:
            return True
        if "percent" in blob or "percentage" in blob or " pct" in (" " + blob + " "):
            return True
        return False
    except Exception:
        return False

def _fix41afc47_has_explicit_currency_token(_c: dict) -> bool:
    try:
        if not isinstance(_c, dict):
            return False
        raw = str(_c.get("raw") or _c.get("raw_disp") or "").lower()
        unit = str(_c.get("unit") or "").lower()
        utag = str(_c.get("unit_tag") or "").lower()
        base = str(_c.get("base_unit") or "").lower()
        blob = " ".join([raw, unit, utag, base])
        # Currency symbols/codes are considered explicit unit tokens
        return any(t in blob for t in ["$", "€", "£", "¥", "usd", "us$", "eur", "sgd", "gbp", "aud", "cad", "jpy", "cny", "rmb", "inr", "krw", "chf", "hkd", "nzd"])
    except Exception:
        return False

def _fix41afc47_ctx_bleed_guard(_c: dict) -> dict:
    """If unit_family was inferred from *context* (e.g., nearby %), ensure it does not
    'infect' unitless numbers. Percent/currency families must have explicit tokens on the candidate itself.
    Additive-only: safe to call repeatedly; records a debug flag when it clears a context-bleed family.
    """
    try:
        if not isinstance(_c, dict):
            return _c
        uf = str(_c.get("unit_family") or "").strip().lower()
        if uf == "percent" and not _fix41afc47_has_explicit_percent_token(_c):
            _c["unit_family"] = ""
            # release any earlier lock that might have come from context inference
            if _c.get("unit_family_locked_fix41afc42"):
                _c["unit_family_locked_fix41afc42"] = False
            _c["unit_family_cleared_fix41afc47"] = "ctx_bleed_no_explicit_percent_token"
        if uf == "currency" and not _fix41afc47_has_explicit_currency_token(_c):
            _c["unit_family"] = ""
            if _c.get("unit_family_locked_fix41afc42"):
                _c["unit_family_locked_fix41afc42"] = False
            _c["unit_family_cleared_fix41afc47"] = "ctx_bleed_no_explicit_currency_token"
        return _c
    except Exception:
        return _c

# ============================================================
# PATCH FIX41AFC47_HELPERS END
# ============================================================

def _fix41afc36_backfill_in_sources_cache(_cache):
    try:
        if isinstance(_cache, dict):
            if "sources" in _cache and isinstance(_cache.get("sources"), dict):
                for _u, _s in _cache["sources"].items():
                    if isinstance(_s, dict) and isinstance(_s.get("extracted_numbers"), list):
                        _fix41afc36_backfill_candidates_list(_s.get("extracted_numbers"))
            else:
                for _u, _s in _cache.items():
                    if isinstance(_s, dict) and isinstance(_s.get("extracted_numbers"), list):
                        _fix41afc36_backfill_candidates_list(_s.get("extracted_numbers"))
        elif isinstance(_cache, list):
            for _s in _cache:
                if isinstance(_s, dict) and isinstance(_s.get("extracted_numbers"), list):
                    _fix41afc36_backfill_candidates_list(_s.get("extracted_numbers"))
        return _cache
    except Exception:
        return _cache
# ============================================================
# PATCH FIX41AFC36B END
# ============================================================

def normalize_unit(unit: str) -> str:
    """Normalize unit to one of: T/B/M/%/'' (deterministic)."""
    if not unit:
        return ""
    u = unit.strip().upper().replace("USD", "").replace("$", "").replace(" ", "")
    if u in ["TRILLION", "T"]:
        return "T"
    if u in ["BILLION", "B"]:
        return "B"
    if u in ["MILLION", "M"]:
        return "M"
    if u in ["PERCENT", "%"]:
        return "%"
    if u in ["K", "THOUSAND"]:
        return "K"
    return u

def normalize_currency_prefix(raw: str) -> bool:
    """True if looks like a currency number ($/USD)."""
    if not raw:
        return False
    s = raw.strip().upper()
    return s.startswith("$") or " USD" in s or s.startswith("USD")

def is_likely_junk_context(ctx: str) -> bool:
    """
    Returns True if a context snippet strongly indicates the number is coming from
    HTML/JS/CSS/asset junk (srcset resize params, scripts, svg path data, etc.)
    rather than real narrative/tabular data.
    """
    import re

    c = (ctx or "").strip()
    if not c:
        return True

    cl = c.lower()

    # Too much binary / garbled text (common when PDF bytes leak through)
    non_print = sum(1 for ch in c if ord(ch) < 9 or (13 < ord(ch) < 32))
    if non_print > 0:
        return True

    # Lots of replacement chars / unusual glyphs → decode garbage
    bad_glyphs = c.count("\ufffd")
    if bad_glyphs >= 1:
        return True

    # Very long uninterrupted “code-ish” context
    if len(c) > 260 and ("{" in c and "}" in c) and ("function" in cl or "var " in cl or "const " in cl):
        return True

    # Hard “asset / markup / script” indicators
    hard_hints = [
        "srcset=", "resize=", "quality=", "offsc", "offscreencanvas", "createelement(\"canvas\")",
        "willreadfrequently", "function(", "webpack", "window.", "document.", "var ", "const ",
        "<script", "</script", "<style", "</style", "text/javascript", "application/javascript",
        "og:image", "twitter:image", "meta property=", "content=\"width=device-width",
        "/wp-content/", ".jpg", ".jpeg", ".png", ".svg", ".webp", ".css", ".js", ".woff", ".woff2",
        "data:image", "base64,", "viewbox", "path d=", "d=\"m", "aria-label=", "class=\""
    ]
    if any(h in cl for h in hard_hints):
        return True

    # SVG path command patterns like "h4.16v-2.56"
    if re.search(r"(?:^|[^a-z0-9])[a-z]\d+(?:\.\d+)?[a-z]-?\d", cl):
        return True

    # Image resize query param like "...jpg?resize=770%2C513..."
    if re.search(r"resize=\d+%2c\d+", cl):
        return True

    # Phone / tracking / footer junk often has lots of separators and few letters
    letters = sum(1 for ch in c if ch.isalpha())
    if len(c) >= 120 and letters / max(1, len(c)) < 0.08:
        return True

    return False


def parse_human_number(value_str: str, unit: str) -> Optional[float]:
    """
    Parse number + unit into a comparable float scale.
    - For T/B/M: returns value in billions (B) to compare apples-to-apples.
    - For %: returns numeric percent.
    """
    if value_str is None:
        return None

    s = str(value_str).strip()
    if not s:
        return None

    # remove currency symbols/commas/space
    s = s.replace("$", "").replace(",", "").strip()

    # handle parentheses for negatives e.g. (12.3)
    if s.startswith("(") and s.endswith(")"):
        s = "-" + s[1:-1].strip()

    try:
        v = float(s)
    except Exception:
        return None

    u = normalize_unit(unit)

    # Normalize magnitudes into BILLIONS for currency-like units
    if u == "T":
        return v * 1000.0
    if u == "B":
        return v
    if u == "M":
        return v / 1000.0
    if u == "K":
        return v / 1_000_000.0

    # Percent: keep as percent number
    if u == "%":
        return v

    # Unknown unit: leave as-is (still useful for ratio filtering)
    return v

def build_prev_numbers(prev_metrics: Dict) -> Dict[str, Dict]:
    """
    Build previous metric lookup keyed by metric_name string.
    Stores:
      - parsed numeric value (for matching)
      - normalized unit (for gating)
      - raw display string INCLUDING currency/magnitude (for dashboards + evolution JSON)
      - raw_value/raw_unit for debugging
    """
    def _format_raw_display(value: Any, unit: str) -> str:
        v = "" if value is None else str(value).strip()
        u = (unit or "").strip()

        if not v:
            return ""

        # Currency prefix handling (SGD/USD keywords OR symbol prefixes)
        currency = ""
        u_nospace = u.replace(" ", "")

        if u_nospace.upper().startswith("SGD"):
            currency = "S$"
            u_tail = u_nospace[3:]
        elif u_nospace.upper().startswith("USD"):
            currency = "$"
            u_tail = u_nospace[3:]
        elif u_nospace.startswith("S$"):
            currency = "S$"
            u_tail = u_nospace[2:]
        elif u_nospace.startswith("$"):
            currency = "$"
            u_tail = u_nospace[1:]
        else:
            u_tail = u_nospace

        # Percent special case
        if u_tail == "%":
            return f"{v}%"

        # Word scales
        if "billion" in u.lower():
            return f"{currency}{v} billion".strip()
        if "million" in u.lower():
            return f"{currency}{v} million".strip()

        # Compact suffix (B/M/K/T)
        if u_tail.upper() in {"T", "B", "M", "K"}:
            return f"{currency}{v}{u_tail.upper()}".strip()

        # Fallback
        return f"{currency}{v} {u}".strip()

    prev_numbers: Dict[str, Dict] = {}
    for key, metric in (prev_metrics or {}).items():
        if not isinstance(metric, dict):
            continue

        metric_name = metric.get("name", key)
        raw_value = metric.get("value", "")
        raw_unit = metric.get("unit", "")

        val = parse_human_number(str(raw_value), raw_unit)
        if val is None:
            continue

        prev_numbers[metric_name] = {
            "value": val,
            "unit": normalize_unit(raw_unit),
            "raw": _format_raw_display(raw_value, raw_unit),   # ✅ now includes currency + unit
            "raw_value": raw_value,
            "raw_unit": raw_unit,
            "keywords": extract_context_keywords(metric_name),
        }

    return prev_numbers

def _extract_baseline_cache(previous_data: dict) -> list:
    """
    Pull prior source snapshots from any known places v7.x stores them.
    Returns a list of source_result-like dicts, or [].
    """
    pd = previous_data or {}
    pr = (pd.get("primary_response") or {}) if isinstance(pd.get("primary_response"), dict) else {}

    for obj in [
        pd.get("baseline_sources_cache"),
        (pd.get("results") or {}).get("baseline_sources_cache") if isinstance(pd.get("results"), dict) else None,
        (pd.get("results") or {}).get("source_results") if isinstance(pd.get("results"), dict) else None,
        pd.get("source_results"),
        pr.get("baseline_sources_cache"),
        (pr.get("results") or {}).get("source_results") if isinstance(pr.get("results"), dict) else None,
    ]:
        if isinstance(obj, list) and obj:
            return obj

    return []


def _extract_query_from_previous(previous_data: dict) -> str:
    """
    Try to recover the original user query/topic from the saved analysis object.
    v7.27 commonly uses 'question'.
    """
    pd = previous_data or {}
    if isinstance(pd.get("question"), str) and pd["question"].strip():
        return pd["question"].strip()

    pr = pd.get("primary_response") or {}
    if isinstance(pr, dict):
        if isinstance(pr.get("question"), str) and pr["question"].strip():
            return pr["question"].strip()
        if isinstance(pr.get("query"), str) and pr["query"].strip():
            return pr["query"].strip()

    meta = pd.get("meta") or {}
    if isinstance(meta, dict) and isinstance(meta.get("question"), str) and meta["question"].strip():
        return meta["question"].strip()

    return ""

def _build_source_snapshots_from_web_context(web_context: dict) -> list:
    """
    Convert fetch_web_context() output (scraped_meta) into evolution snapshots.

    Preferred inputs:
      - web_context["scraped_meta"][url]["extracted_numbers"] (analysis-aligned)

    Safety-net hard gates (small set):
      1) homepage-like URLs downweighted + tagged
      2) nav/chrome/junk context downweighted
      3) year-only suppression (e.g., raw == "2024" and no unit/context)
      4) light topic gate (requires minimal overlap with query tokens)
    """
    import hashlib
    from datetime import datetime
    from urllib.parse import urlparse
    import re

    def _sha1(s: str) -> str:
        return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()

    def _now() -> str:
        try:
            return datetime.utcnow().isoformat() + "+00:00"
        except Exception:
            return datetime.now().isoformat()

    def _is_homepage_url(u: str) -> bool:
        try:
            p = urlparse((u or "").strip())
            path = (p.path or "").strip()
            if path in ("", "/"):
                return True
            low = path.lower().rstrip("/")
            if low in ("/index", "/index.html", "/index.htm", "/home", "/default", "/default.aspx"):
                return True
            return False
        except Exception:
            return False



    def _tokenize(s: str) -> list:
        toks = re.findall(r"[a-z0-9]+", (s or "").lower())
        stop = {"the","and","or","of","in","to","for","by","from","with","on","at","as","a","an","is","are","this","that"}
        return [t for t in toks if len(t) >= 4 and t not in stop]

    def _looks_like_year_only(n: dict) -> bool:
        try:
            raw = str(n.get("raw") or "").strip()
            # =====================================================================
            # PATCH YEAR2 (ADDITIVE): handle years stored as numeric/float strings
            # - If raw is empty or looks like '2024.0', normalize to '2024' for checks.
            # =====================================================================
            try:
                if (not raw) or (raw and raw.replace('.', '', 1).isdigit() and '.' in raw):
                    vraw = n.get('value_norm') if n.get('value_norm') is not None else n.get('value')
                    iv = int(float(str(vraw).strip())) if vraw is not None else None
                    if iv is not None and 1900 <= iv <= 2105:
                        raw = str(iv)
            except Exception:
                pass
            # =====================================================================
            unit = str(n.get("unit") or "").strip()
            ctx = str(n.get("context") or n.get("context_snippet") or "").strip()
            # exactly 4 digits year and nothing else
            if re.fullmatch(r"(19|20)\d{2}", raw) and not unit:
                # if context is empty or super short, treat as junk
                if len(ctx) < 12:
                    return True
            return False
        except Exception:
            return False

    def _is_chrome_ctx(ctx: str) -> bool:
        if not ctx:
            return False
        low = ctx.lower()
        for h in globals().get("NON_DATA_CONTEXT_HINTS", []) or []:
            if h in low:
                return True
        return False

    if not isinstance(web_context, dict):
        return []

    scraped_meta = web_context.get("scraped_meta") or {}
    if not isinstance(scraped_meta, dict) or not scraped_meta:
        return []

    query = (web_context.get("query") or "")
    q_toks = set(_tokenize(query))

    out = []

    for url, meta in scraped_meta.items():
        if not isinstance(meta, dict):
            continue

        url_s = str(url or meta.get("url") or "").strip()
        if not url_s:
            continue

        extracted = meta.get("extracted_numbers") or []
        if not isinstance(extracted, list):
            extracted = []

        fp = meta.get("fingerprint") or meta.get("extract_hash") or meta.get("content_fingerprint")
        if fp and not isinstance(fp, str):
            fp = str(fp)
        if not fp and isinstance(meta.get("clean_text"), str):
            fp = _sha1(meta["clean_text"][:200000])

        status_detail = meta.get("status_detail") or meta.get("status") or ""
        fetched_ok = str(status_detail).startswith("success") or meta.get("status") == "fetched"

        is_homepage = _is_homepage_url(url_s)

        cleaned_numbers = []
        for n in extracted:
            if not isinstance(n, dict):
                continue

            # ---- Hard gate: year-only suppression ----
            if _looks_like_year_only(n):
                continue

            value = n.get("value")
            raw = n.get("raw")
            unit = n.get("unit")
            ctx = n.get("context") or n.get("context_snippet") or ""

            # normalize context
            ctx_s = ctx if isinstance(ctx, str) else ""
            ctx_s = ctx_s.strip()

            # ---- Hard gate: chrome/nav rejection (soft) ----
            chrome_ctx = _is_chrome_ctx(ctx_s)

            # ---- Light topic gate (soft): require some overlap with query tokens ----
            # This is intentionally mild: it *downweights* rather than drops everything.
            ctx_toks = set(_tokenize(ctx_s))
            tok_overlap = len(q_toks.intersection(ctx_toks)) if q_toks and ctx_toks else 0

            # quality scoring (small + interpretable)
            quality = 1.0
            reasons = []

            if is_homepage:
                quality *= 0.25
                reasons.append("homepage_like")

            if chrome_ctx:
                quality *= 0.40
                reasons.append("chrome_context")

            if q_toks and tok_overlap == 0:
                quality *= 0.55
                reasons.append("topic_miss")

            # cap/trim context snippet for JSON size
            ctx_snip = ctx_s[:240]

            cleaned_numbers.append({
                "value": value,
                "unit": unit,
                "raw": raw,
                "source_url": n.get("source_url") or url_s,
                "context_snippet": ctx_snip,
                "anchor_hash": n.get("anchor_hash") or _sha1(f"{url_s}|{ctx_snip}|{raw}|{unit}"),
                # Debug fields for tuning:
                "quality_score": round(float(quality), 3),
                "quality_reasons": reasons,
                "topic_overlap": tok_overlap,
            })

        out.append({
            "url": url_s,
            "status": "fetched_extracted" if cleaned_numbers else ("fetched" if fetched_ok else "failed"),
            "status_detail": status_detail,
            "numbers_found": len(cleaned_numbers),
            "fingerprint": fp or "",
            "fetched_at": meta.get("fetched_at") or _now(),
            "is_homepage_like": bool(is_homepage),
            "extracted_numbers": cleaned_numbers,
        })

    return out



def _build_source_snapshots_from_baseline_cache(baseline_cache: list) -> list:
    """
    Normalize prior cached source_results (from previous run) into a consistent schema.

    Tightening:
      - Detect domain-only/homepage URLs and label them (same as web_context snapshots)
      - Keep backward compatible fields; only add new fields.
    """
    from urllib.parse import urlparse

    def _is_homepage_url(u: str) -> bool:
        try:
            p = urlparse((u or "").strip())
            path = (p.path or "").strip()
            if path in ("", "/"):
                return True
            low = path.lower().rstrip("/")
            if low in ("/index", "/index.html", "/index.htm", "/home", "/default", "/default.aspx"):
                return True
            return False
        except Exception:
            return False

    out = []
    if not isinstance(baseline_cache, list):
        return out

    for sr in baseline_cache:
        if not isinstance(sr, dict):
            continue

        url = sr.get("url") or sr.get("source_url")
        if not url:
            continue
        url_s = str(url).strip()
        if not url_s:
            continue

        extracted = sr.get("extracted_numbers") or []
        if not isinstance(extracted, list):
            extracted = []

        cleaned = []
        for n in extracted:
            if not isinstance(n, dict):
                continue
            cleaned.append({
                "value": n.get("value"),
                "unit": n.get("unit"),
                "raw": n.get("raw"),
                "source_url": n.get("source_url") or url_s,
                "context": (n.get("context") or n.get("context_snippet") or "")[:220]
                if isinstance((n.get("context") or n.get("context_snippet")), str) else "",
            })

        fp = sr.get("fingerprint")
        if fp and not isinstance(fp, str):
            fp = str(fp)

        # --- homepage labeling (tightening #3) ---
        is_homepage = bool(sr.get("is_homepage")) or _is_homepage_url(url_s)
        quality_score = sr.get("quality_score")
        if quality_score is None:
            quality_score = 0.15 if is_homepage else 1.0

        skip_reason = sr.get("skip_reason") or ("homepage_url_low_signal" if is_homepage else "")

        host = sr.get("host") or ""
        path = sr.get("path") or ""
        if not host and not path:
            try:
                p = urlparse(url_s)
                host = p.netloc or ""
                path = p.path or ""
            except Exception:
                pass

        out.append({
            "url": url_s,
            "status": sr.get("status") or "",
            "status_detail": sr.get("status_detail") or "",
            "numbers_found": int(sr.get("numbers_found") or len(cleaned)),
            "fingerprint": fp,
            "fetched_at": sr.get("fetched_at"),
            "extracted_numbers": cleaned,

            # NEW debug fields (safe additions)
            "is_homepage": bool(is_homepage),
            "quality_score": float(quality_score),
            "skip_reason": skip_reason,
            "host": host,
            "path": path,
        })

    return out


def _merge_snapshots_prefer_cached_when_unchanged(current_snaps: list, cached_snaps: list) -> list:
    """
    Policy merge:
      - If current fingerprint matches cached fingerprint for same URL:
        reuse cached snapshot (even if live fetch worked)  ✅ point A
      - Else prefer current (fresh).
      - Add cached snapshots not present in current.
      - Also: if current numbers_found is 0 but cached has >0, reuse cached.
    """
    if not isinstance(current_snaps, list):
        current_snaps = []
    if not isinstance(cached_snaps, list):
        cached_snaps = []

    cached_by_url = {}
    for s in cached_snaps:
        if isinstance(s, dict) and s.get("url"):
            cached_by_url[str(s["url"])] = s

    merged = []
    seen = set()

    for cs in current_snaps:
        if not isinstance(cs, dict) or not cs.get("url"):
            continue
        url = str(cs["url"])
        seen.add(url)

        cached = cached_by_url.get(url)
        if not cached:
            merged.append(cs)
            continue

        cur_fp = cs.get("fingerprint")
        old_fp = cached.get("fingerprint")

        cur_nf = int(cs.get("numbers_found") or 0)
        old_nf = int(cached.get("numbers_found") or 0)

        # If current extraction is empty but cached had numbers, reuse cached.
        if cur_nf == 0 and old_nf > 0:
            merged.append(cached)
            continue

        # Fingerprint unchanged -> reuse cached even if live fetch worked.
        if cur_fp and old_fp and str(cur_fp) == str(old_fp):
            merged.append(cached)
        else:
            merged.append(cs)

    for url, cached in cached_by_url.items():
        if url not in seen:
            merged.append(cached)

    return merged


def _safe_parse_current_analysis(query: str, web_context: dict) -> dict:
    """
    Run the same analysis pipeline used in v7.27 to produce primary_response, but safely.
    Returns dict with at least {primary_response:{primary_metrics:{}}} or {} on failure.
    """
    import json
    qp = globals().get("query_perplexity")
    if not callable(qp):
        return {}

    try:
        raw = qp(query, web_context)
        if not raw or not isinstance(raw, str):
            return {}
        obj = json.loads(raw)
        if not isinstance(obj, dict):
            return {}
        return {"primary_response": obj}
    except Exception:
        return {}


def diff_metrics_by_name_BASE(prev_response: dict, cur_response: dict):
    """
    Canonical-first diff with:
      - HARD STOP when prev canonical_key is missing in current (no name fallback)
      - Row-level metric_definition sourced from PREVIOUS (original new analysis) schema:
          prev_response['metric_schema_frozen'][canonical_key] (preferred)
          else prev_response['primary_metrics_canonical'][canonical_key]
      - Backward compatible: still returns 'name' (non-empty) and existing fields.

    Returns:
      metric_changes, unchanged, increased, decreased, found
    """
    import re

    # Defaults (used unless schema provides overrides)
    ABS_EPS = 1e-9
    REL_EPS = 0.0005

    def norm_name(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def parse_num(v, unit=""):
        fn = globals().get("parse_human_number")
        if callable(fn):
            try:
                return fn(str(v), unit)
            except Exception:
                return None
        try:
            return float(str(v).replace(",", "").strip())
        except Exception:
            return None

    # =========================================================================
    # PATCH D1 (ADDITIVE): canonical numeric extractor
    # - Prefer value_norm/base_unit when present (analysis/evolution alignment)
    # - Fall back to existing parse_num(value, unit) when canonical fields missing
    # =========================================================================
    def get_canonical_value_and_unit(m: dict):
        """
        Returns: (val: float|None, unit: str)
        Priority:
          1) value_norm (float-like) + base_unit (if present)
          2) parse_num(value, unit)
        """
        m = m if isinstance(m, dict) else {}

        # 1) canonical path
        if m.get("value_norm") is not None:
            try:
                v = float(m.get("value_norm"))
                u = str(m.get("base_unit") or m.get("unit") or "").strip()

                # ============================================================
                # PATCH FIX41AFC50A START — unit_cmp fallback from unit_tag/family
                # Some rebuilt metrics omit base_unit/unit but carry unit_tag or unit_family.
                # Without this, diffing sees cur_unit_cmp="" and triggers unit_mismatch blocks.
                if not u:
                    try:
                        ut = str(m.get("unit_tag") or "").strip()
                        uf = str(m.get("unit_family") or "").strip().lower()
                        if ut:
                            u = ut
                        elif uf == "percent":
                            u = "%"
                        elif uf in ("currency", "money"):
                            u = str(m.get("currency") or "$")  # best-effort
                        elif uf == "magnitude":
                            u = str(m.get("unit") or "M").strip() or "M"
                    except Exception:
                        pass
                # PATCH FIX41AFC50A END
                # ============================================================

                return v, u
            except Exception:
                pass

        # 2) legacy parse path
        u = str(m.get("unit") or "").strip()
        v = parse_num(m.get("value"), u)
        return v, u
    # =========================================================================

    # =========================================================================
    # PATCH D0 (ADDITIVE): anchor helpers (drift=0 stability)
    # NOTE (IMPORTANT):
    # - Anchor_hash equality should NOT force "unchanged" if numeric values differ.
    #   It means "we matched the same evidence anchor" (identity/matching), not
    #   that the metric's value necessarily didn't change.
    # - This patch keeps anchor_same, but uses it only for match_confidence +
    #   diagnostics, not as a classification override.
    # =========================================================================
    def _get_anchor_hash_from_metric(m: dict):
        try:
            if isinstance(m, dict):
                ah = m.get("anchor_hash") or m.get("anchor") or m.get("anchorHash")
                return str(ah) if ah else None
        except Exception:
            pass
        return None

    def _get_prev_anchor_hash(prev_resp: dict, ckey: str, pm: dict):
        # 1) direct on metric row
        ah = _get_anchor_hash_from_metric(pm)
        if ah:
            return ah

        # 2) prev_response.metric_anchors[ckey].anchor_hash
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            pass

        return None

    def _get_cur_anchor_hash(cur_resp: dict, ckey: str, cm: dict):
        # 1) direct on metric row (evolution rebuild puts anchor_hash here)
        ah = _get_anchor_hash_from_metric(cm)
        if ah:
            return ah

        # 2) cur_response.metric_anchors[ckey].anchor_hash (if present)
        try:
            ma = (cur_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            pass

        return None
    # =========================================================================

    # =========================================================================
    # PATCH MA2 (ADDITIVE): wire metric_anchors into row fields
    # - Populate context_snippet/source_url from prev_response.metric_anchors[ckey] when available
    # - Output enrichment only
    # =========================================================================
    def _get_prev_anchor_obj(prev_resp: dict, ckey: str):
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                return a if isinstance(a, dict) else {}
        except Exception:
            pass
        return {}

    def _anchor_meta(prev_resp: dict, cur_resp: dict, ckey: str, pm: dict, cm: dict):
        """
        Returns: (source_url, context_snippet, anchor_confidence)
        Priority:
          1) prev_response.metric_anchors[ckey] (baseline anchoring is authoritative)
          2) current metric row fields (if present)
          3) prev metric row fields (if present)
        """
        a = _get_prev_anchor_obj(prev_resp, ckey)

        src = a.get("source_url") or a.get("url")
        ctx = a.get("context_snippet") or a.get("context")
        conf = a.get("anchor_confidence")

        if not src:
            try:
                src = (cm or {}).get("source_url") or (cm or {}).get("url")
            except Exception:
                src = None
        if not ctx:
            try:
                ctx = (cm or {}).get("context_snippet") or (cm or {}).get("context")
            except Exception:
                ctx = None

        if not src:
            try:
                src = (pm or {}).get("source_url") or (pm or {}).get("url")
            except Exception:
                src = None
        if not ctx:
            try:
                ctx = (pm or {}).get("context_snippet") or (pm or {}).get("context")
            except Exception:
                ctx = None

        try:
            if isinstance(ctx, str):
                ctx = ctx.strip()[:220] or None
            else:
                ctx = None
        except Exception:
            ctx = None

        try:
            conf = float(conf) if conf is not None else None
        except Exception:
            conf = None

        return src, ctx, conf
    # =========================================================================

    def prettify_ckey(ckey: str) -> str:
        ckey = str(ckey or "").strip()
        if not ckey:
            return "Unknown Metric"
        parts = ckey.split("__", 1)
        left = parts[0].replace("_", " ").strip()
        right = parts[1].replace("_", " ").strip() if len(parts) > 1 else ""
        left = " ".join(w.capitalize() for w in left.split())
        return f"{left} ({right})" if right else left

    def get_metric_definition(prev_resp: dict, ckey: str) -> dict:
        """
        Pull authoritative definition from the ORIGINAL analysis run (prev_response).
        """
        prev_resp = prev_resp if isinstance(prev_resp, dict) else {}

        schema = prev_resp.get("metric_schema_frozen")
        if isinstance(schema, dict):
            d = schema.get(ckey)
            if isinstance(d, dict) and d:
                out = dict(d)
                out.setdefault("canonical_key", ckey)
                return out

        prev_can = prev_resp.get("primary_metrics_canonical")
        if isinstance(prev_can, dict):
            d = prev_can.get(ckey)
            if isinstance(d, dict) and d:
                out = {
                    "canonical_key": ckey,
                    "canonical_id": d.get("canonical_id"),
                    "dimension": d.get("dimension"),
                    "name": d.get("name") or d.get("original_name"),
                    "unit": d.get("unit"),
                    "geo_scope": d.get("geo_scope"),
                    "geo_name": d.get("geo_name"),
                    "keywords": d.get("keywords"),
                }
                return {k: v for k, v in out.items() if v not in (None, "", [], {})}

        return {"canonical_key": ckey, "name": prettify_ckey(ckey)}

    def get_display_name(prev_resp: dict, prev_can_obj: dict, cur_can_obj: dict, ckey: str) -> str:
        schema = prev_resp.get("metric_schema_frozen")
        if isinstance(schema, dict):
            sm = schema.get(ckey)
            if isinstance(sm, dict):
                v = sm.get("name")
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(prev_can_obj, dict):
            for k in ("name", "original_name"):
                v = prev_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(cur_can_obj, dict):
            for k in ("name", "original_name"):
                v = cur_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        return prettify_ckey(ckey)

    # =========================================================================
    # PATCH D3 (ADDITIVE): schema-driven tolerances (optional)
    # - If schema provides abs_eps/rel_eps use them, else default.
    # =========================================================================
    def get_eps_for_metric(prev_resp: dict, ckey: str):
        ae = ABS_EPS
        re_ = REL_EPS
        try:
            schema = (prev_resp or {}).get("metric_schema_frozen")
            if isinstance(schema, dict):
                d = schema.get(ckey)
                if isinstance(d, dict):
                    if d.get("abs_eps") is not None:
                        try:
                            ae = float(d.get("abs_eps"))
                        except Exception:
                            pass
                    if d.get("rel_eps") is not None:
                        try:
                            re_ = float(d.get("rel_eps"))
                        except Exception:
                            pass
        except Exception:
            pass
        return ae, re_
    # =========================================================================

    prev_response = prev_response if isinstance(prev_response, dict) else {}
    cur_response = cur_response if isinstance(cur_response, dict) else {}

    prev_can = prev_response.get("primary_metrics_canonical")
    cur_can = cur_response.get("primary_metrics_canonical")

    # =========================
    # Path A: canonical-first
    # =========================
    if isinstance(prev_can, dict) and isinstance(cur_can, dict) and prev_can:
        metric_changes = []
        unchanged = increased = decreased = found = 0

        for ckey, pm in prev_can.items():
            pm = pm if isinstance(pm, dict) else {}
            cm = cur_can.get(ckey)
            cm = cm if isinstance(cm, dict) else {}

            display_name = get_display_name(prev_response, pm, cm, ckey)
            definition = get_metric_definition(prev_response, ckey)

            prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

            # ✅ HARD STOP: canonical key missing in current => not_found (no name fallback)
            if ckey not in cur_can or not isinstance(cur_can.get(ckey), dict):
                # PATCH MA2 (ADDITIVE): fill row fields from metric_anchors where possible
                _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, {})

                metric_changes.append({
                    "name": display_name,
                    "previous_value": prev_raw,
                    "current_value": "N/A",
                    "change_pct": None,
                    "change_type": "not_found",
                    "match_confidence": 0.0,
                    "context_snippet": _ctx,
                    "source_url": _src,
                    "anchor_used": False,
            "eligibility_pass_reason_fix41afc28": _fix41afc28_candidate_pass_reason(spec, best),  # not applicable when current metric missing
                    "canonical_key": ckey,
                    "metric_definition": definition,
                    "anchor_confidence": _aconf,
                })
                continue

            found += 1

            cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

            # =========================================================================
            # PATCH D0 (ADDITIVE): anchor identity (do NOT force unchanged)
            # =========================================================================
            prev_ah = _get_prev_anchor_hash(prev_response, ckey, pm)
            cur_ah = _get_cur_anchor_hash(cur_response, ckey, cm)
            anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))
            # =========================================================================

            # PATCH D2 (ADDITIVE): use canonical values for diff when available
            prev_val, prev_unit_cmp = get_canonical_value_and_unit(pm)
            cur_val, cur_unit_cmp = get_canonical_value_and_unit(cm)

            # PATCH D3 (ADDITIVE): metric-specific tolerances (schema overrides)
            abs_eps, rel_eps = get_eps_for_metric(prev_response, ckey)

            change_type = "unknown"
            change_pct = None

            # =========================================================================
            # PATCH D0B (ADDITIVE, REQUIRED): numeric-first classification even if anchors match
            # Why:
            # - anchor_same means "we matched the same evidence anchor"
            # - It MUST NOT short-circuit classification to "unchanged" when values differ.
            # - This fixes the exact bug you observed: prev_value_norm != cur_value_norm
            #   while change_type incorrectly says "unchanged".
            # =========================================================================
            if prev_val is not None and cur_val is not None:
                if abs(prev_val - cur_val) <= max(abs_eps, abs(prev_val) * rel_eps):
                    change_type = "unchanged"
                    change_pct = 0.0
                    unchanged += 1
                elif cur_val > prev_val:
                    change_type = "increased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    increased += 1
                else:
                    change_type = "decreased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    decreased += 1
            # If we cannot compare numerically, fall back:
            # - If anchors match, treat as unchanged ONLY as a last resort (formatting issue)
            elif anchor_same:
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            # =========================================================================

            # PATCH D4 (ADDITIVE): unit mismatch flag (debug only)
            unit_mismatch = False
            try:
                if prev_unit_cmp and cur_unit_cmp and str(prev_unit_cmp) != str(cur_unit_cmp):
                    unit_mismatch = True
            except Exception:
                unit_mismatch = False
            # =====================================================================
            # PATCH FIX41AFC23B (ADDITIVE): treat missing/empty units as mismatch when schema expects a unit family
            # =====================================================================
            try:
                if not unit_mismatch:
                    _sch = (prev_response.get("metric_schema_frozen") or {}).get(ckey) or {}
                    _exp_family = (_sch.get("unit_family") or _sch.get("dimension") or "").lower()
                    _exp_unit = str(_sch.get("unit") or _sch.get("unit_tag") or "").strip()
                    _cur_u = str(cur_unit_cmp or "").strip()
                    _prev_u = str(prev_unit_cmp or "").strip()

                    if _prev_u and not _cur_u:
                        if _exp_family in ("magnitude", "unit_sales") or (_sch.get("dimension") == "unit_sales") or (_exp_unit in ("M", "B", "K", "T")):
                            unit_mismatch = True

                    if not unit_mismatch and (_sch.get("dimension") == "percent" or _exp_family == "percent"):
                        if not _cur_u or _cur_u != "%":
                            unit_mismatch = True

                    if not unit_mismatch and (_sch.get("dimension") == "currency" or _exp_family == "currency"):
                        if not _cur_u:
                            unit_mismatch = True
            except Exception:
                pass
            # =====================================================================
            # END PATCH FIX41AFC23B
            # =====================================================================

            # PATCH MA2 (ADDITIVE): fill row fields from metric_anchors where possible
            _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, cm)

            # PATCH D0C (ADDITIVE): match_confidence reflects anchor identity
            match_conf = 92.0
            try:
                if anchor_same:
                    match_conf = 98.0
            except Exception:
                match_conf = 92.0

            metric_changes.append({
                "name": display_name,
                "previous_value": prev_raw,
                "current_value": cur_raw,
                "change_pct": change_pct,
                "change_type": change_type,
                "match_confidence": float(match_conf),

                "context_snippet": _ctx,
                "source_url": _src,

                # anchor identity (matching), not classification
                "anchor_used": bool(anchor_same),
                "prev_anchor_hash": prev_ah,
                "cur_anchor_hash": cur_ah,

                "canonical_key": ckey,
                "metric_definition": definition,

                "anchor_confidence": _aconf,

                # expose canonical comparison basis for debugging/convergence
                "prev_value_norm": prev_val,
                "cur_value_norm": cur_val,
                "prev_unit_cmp": prev_unit_cmp,
                "cur_unit_cmp": cur_unit_cmp,
                "unit_mismatch": bool(unit_mismatch),
                "abs_eps_used": abs_eps,
                "rel_eps_used": rel_eps,
            })

        # PATCH FIX41AFC38_UNITCMP_BACKFILL START

        try:

            for _row in metric_changes:

                if not isinstance(_row, dict):

                    continue

                _md = _row.get("metric_definition") or {}

                if not isinstance(_md, dict):

                    _md = {}

                _uf = (_md.get("unit_family") or "").lower()

                if (_row.get("cur_unit_cmp") or "") == "" and _row.get("cur_value_norm") is not None and (_row.get("current_value") not in ["", None, "N/A"]):

                    if _uf == "percent":

                        _row["cur_unit_cmp"] = "%"

                        _row["cur_unit_cmp_backfilled_fix41afc38"] = True

                        if _row.get("unit_mismatch") is True and str(_row.get("prev_unit_cmp","")) .strip() in ["%", "percent", ""]:

                            _row["unit_mismatch"] = False

                    elif _uf == "currency":

                        _ut = _md.get("unit_tag") or _md.get("unit") or ""

                        if _ut:

                            _row["cur_unit_cmp"] = str(_ut)

                            _row["cur_unit_cmp_backfilled_fix41afc38"] = True

                            if _row.get("unit_mismatch") is True and str(_row.get("prev_unit_cmp","")) .strip() in ["", str(_ut)]:

                                _row["unit_mismatch"] = False

        except Exception:

            pass

        # PATCH FIX41AFC38_UNITCMP_BACKFILL END


        return metric_changes, unchanged, increased, decreased, found

    # =========================
    # Path B: legacy name fallback
    # =========================
    prev_metrics = prev_response.get("primary_metrics") or {}
    cur_metrics = cur_response.get("primary_metrics") or {}
    if not isinstance(prev_metrics, dict):
        prev_metrics = {}
    if not isinstance(cur_metrics, dict):
        cur_metrics = {}

    prev_index = {}
    for k, m in prev_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            prev_index[norm_name(name)] = (name, m)

    cur_index = {}
    for k, m in cur_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            cur_index[norm_name(name)] = (name, m)

    metric_changes = []
    unchanged = increased = decreased = found = 0

    for nk, (display_name, pm) in prev_index.items():
        prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

        if nk not in cur_index:
            metric_changes.append({
                "name": display_name or "Unknown Metric",
                "previous_value": prev_raw,
                "current_value": "N/A",
                "change_pct": None,
                "change_type": "not_found",
                "match_confidence": 0.0,
                "context_snippet": None,
                "source_url": None,
                "anchor_used": False,
            })
            continue

        found += 1
        _, cm = cur_index[nk]
        cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

        prev_val, _prev_unit_cmp = get_canonical_value_and_unit(pm)
        cur_val, _cur_unit_cmp = get_canonical_value_and_unit(cm)

        prev_ah = _get_anchor_hash_from_metric(pm)
        cur_ah = _get_anchor_hash_from_metric(cm)
        anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))

        change_type = "unknown"
        change_pct = None

        # PATCH D0B mirrors canonical path: numeric-first, anchor fallback only if numeric missing
        if prev_val is not None and cur_val is not None:
            if abs(prev_val - cur_val) <= max(ABS_EPS, abs(prev_val) * REL_EPS):
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            elif cur_val > prev_val:
                change_type = "increased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                increased += 1
            else:
                change_type = "decreased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                decreased += 1
        elif anchor_same:
            change_type = "unchanged"
            change_pct = 0.0
            unchanged += 1

        metric_changes.append({
            "name": display_name or "Unknown Metric",
            "previous_value": prev_raw,
            "current_value": cur_raw,
            "change_pct": change_pct,
            "change_type": change_type,
            "match_confidence": 90.0 if anchor_same else 80.0,
            "context_snippet": None,
            "source_url": None,

            "anchor_used": bool(anchor_same),
            "prev_anchor_hash": prev_ah,
            "cur_anchor_hash": cur_ah,

            "prev_value_norm": prev_val,
            "cur_value_norm": cur_val,
        })

    return metric_changes, unchanged, increased, decreased, found







# =====================================================================
# PATCH WRAP_DIFF_METRICS_BY_NAME (ADDITIVE): preserve original as diff_metrics_by_name_BASE
# and define the patched version below.
# =====================================================================

def diff_metrics_by_name(prev_response: dict, cur_response: dict):
    """
    Canonical-first diff with:
      - HARD STOP when prev canonical_key is missing in current (no name fallback)
      - Row-level metric_definition sourced from PREVIOUS (original new analysis) schema:
          prev_response['metric_schema_frozen'][canonical_key] (preferred)
          else prev_response['primary_metrics_canonical'][canonical_key]
      - Backward compatible: still returns 'name' (non-empty) and existing fields.

    Returns:
      metric_changes, unchanged, increased, decreased, found
    """
    import re

    # Defaults (used unless schema provides overrides)
    ABS_EPS = 1e-9
    REL_EPS = 0.0005

    def norm_name(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def parse_num(v, unit=""):
        fn = globals().get("parse_human_number")
        if callable(fn):
            try:
                return fn(str(v), unit)
            except Exception:
                return None
        try:
            return float(str(v).replace(",", "").strip())
        except Exception:
            return None

    # =========================================================================
    # PATCH D1 (ADDITIVE): canonical numeric extractor
    # - Prefer value_norm/base_unit when present (analysis/evolution alignment)
    # - Fall back to existing parse_num(value, unit) when canonical fields missing
    # =========================================================================
    def get_canonical_value_and_unit(m: dict):
        """
        Returns: (val: float|None, unit: str)
        Priority:
          1) value_norm (float-like) + base_unit (if present)
          2) parse_num(value, unit)
        """
        m = m if isinstance(m, dict) else {}

        # 1) canonical path
        if m.get("value_norm") is not None:
            try:
                v = float(m.get("value_norm"))
                u = str(m.get("base_unit") or m.get("unit") or "").strip()
                return v, u
            except Exception:
                pass

        # 2) legacy parse path
        u = str(m.get("unit") or "").strip()
        v = parse_num(m.get("value"), u)
        return v, u
    # =========================================================================

    # =========================================================================
    # PATCH D0 (ADDITIVE): anchor helpers (drift=0 stability)
    # NOTE (IMPORTANT):
    # - Anchor_hash equality should NOT force "unchanged" if numeric values differ.
    #   It means "we matched the same evidence anchor" (identity/matching), not
    #   that the metric's value necessarily didn't change.
    # - This patch keeps anchor_same, but uses it only for match_confidence +
    #   diagnostics, not as a classification override.
    # =========================================================================
    def _get_anchor_hash_from_metric(m: dict):
        try:
            if isinstance(m, dict):
                ah = m.get("anchor_hash") or m.get("anchor") or m.get("anchorHash")
                return str(ah) if ah else None
        except Exception:
            pass
        return None

    def _get_prev_anchor_hash(prev_resp: dict, ckey: str, pm: dict):
        # 1) direct on metric row
        ah = _get_anchor_hash_from_metric(pm)
        if ah:
            return ah

        # 2) prev_response.metric_anchors[ckey].anchor_hash
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            pass

        return None

    def _get_cur_anchor_hash(cur_resp: dict, ckey: str, cm: dict):
        # 1) direct on metric row (evolution rebuild puts anchor_hash here)
        ah = _get_anchor_hash_from_metric(cm)
        if ah:
            return ah

        # 2) cur_response.metric_anchors[ckey].anchor_hash (if present)
        try:
            ma = (cur_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            pass

        return None
    # =========================================================================

    # =========================================================================
    # PATCH MA2 (ADDITIVE): wire metric_anchors into row fields
    # - Populate context_snippet/source_url from prev_response.metric_anchors[ckey] when available
    # - Output enrichment only
    # =========================================================================
    def _get_prev_anchor_obj(prev_resp: dict, ckey: str):
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                return a if isinstance(a, dict) else {}
        except Exception:
            pass
        return {}

    def _anchor_meta(prev_resp: dict, cur_resp: dict, ckey: str, pm: dict, cm: dict):
        """
        Returns: (source_url, context_snippet, anchor_confidence)
        Priority:
          1) prev_response.metric_anchors[ckey] (baseline anchoring is authoritative)
          2) current metric row fields (if present)
          3) prev metric row fields (if present)
        """
        a = _get_prev_anchor_obj(prev_resp, ckey)

        src = a.get("source_url") or a.get("url")
        ctx = a.get("context_snippet") or a.get("context")
        conf = a.get("anchor_confidence")

        if not src:
            try:
                src = (cm or {}).get("source_url") or (cm or {}).get("url")
            except Exception:
                src = None
        if not ctx:
            try:
                ctx = (cm or {}).get("context_snippet") or (cm or {}).get("context")
            except Exception:
                ctx = None

        if not src:
            try:
                src = (pm or {}).get("source_url") or (pm or {}).get("url")
            except Exception:
                src = None
        if not ctx:
            try:
                ctx = (pm or {}).get("context_snippet") or (pm or {}).get("context")
            except Exception:
                ctx = None

        try:
            if isinstance(ctx, str):
                ctx = ctx.strip()[:220] or None
            else:
                ctx = None
        except Exception:
            ctx = None

        try:
            conf = float(conf) if conf is not None else None
        except Exception:
            conf = None

        return src, ctx, conf
    # =========================================================================

    def prettify_ckey(ckey: str) -> str:
        ckey = str(ckey or "").strip()
        if not ckey:
            return "Unknown Metric"
        parts = ckey.split("__", 1)
        left = parts[0].replace("_", " ").strip()
        right = parts[1].replace("_", " ").strip() if len(parts) > 1 else ""
        left = " ".join(w.capitalize() for w in left.split())
        return f"{left} ({right})" if right else left

    def get_metric_definition(prev_resp: dict, ckey: str) -> dict:
        """
        Pull authoritative definition from the ORIGINAL analysis run (prev_response).
        """
        prev_resp = prev_resp if isinstance(prev_resp, dict) else {}

        schema = prev_resp.get("metric_schema_frozen")
        if isinstance(schema, dict):
            d = schema.get(ckey)
            if isinstance(d, dict) and d:
                out = dict(d)
                out.setdefault("canonical_key", ckey)
                return out

        prev_can = prev_resp.get("primary_metrics_canonical")
        if isinstance(prev_can, dict):
            d = prev_can.get(ckey)
            if isinstance(d, dict) and d:
                out = {
                    "canonical_key": ckey,
                    "canonical_id": d.get("canonical_id"),
                    "dimension": d.get("dimension"),
                    "name": d.get("name") or d.get("original_name"),
                    "unit": d.get("unit"),
                    "geo_scope": d.get("geo_scope"),
                    "geo_name": d.get("geo_name"),
                    "keywords": d.get("keywords"),
                }
                return {k: v for k, v in out.items() if v not in (None, "", [], {})}

        return {"canonical_key": ckey, "name": prettify_ckey(ckey)}

    def get_display_name(prev_resp: dict, prev_can_obj: dict, cur_can_obj: dict, ckey: str) -> str:
        schema = prev_resp.get("metric_schema_frozen")
        if isinstance(schema, dict):
            sm = schema.get(ckey)
            if isinstance(sm, dict):
                v = sm.get("name")
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(prev_can_obj, dict):
            for k in ("name", "original_name"):
                v = prev_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(cur_can_obj, dict):
            for k in ("name", "original_name"):
                v = cur_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        return prettify_ckey(ckey)

    # =========================================================================
    # PATCH D3 (ADDITIVE): schema-driven tolerances (optional)
    # - If schema provides abs_eps/rel_eps use them, else default.
    # =========================================================================
    def get_eps_for_metric(prev_resp: dict, ckey: str):
        ae = ABS_EPS
        re_ = REL_EPS
        try:
            schema = (prev_resp or {}).get("metric_schema_frozen")
            if isinstance(schema, dict):
                d = schema.get(ckey)
                if isinstance(d, dict):
                    if d.get("abs_eps") is not None:
                        try:
                            ae = float(d.get("abs_eps"))
                        except Exception:
                            pass
                    if d.get("rel_eps") is not None:
                        try:
                            re_ = float(d.get("rel_eps"))
                        except Exception:
                            pass
        except Exception:
            pass
        return ae, re_
    # =========================================================================

    prev_response = prev_response if isinstance(prev_response, dict) else {}
    cur_response = cur_response if isinstance(cur_response, dict) else {}

    # =====================================================================
    # PATCH AI_ANCHMAP1 (ADDITIVE): normalize metric_anchors shape (list -> dict)
    # Why:
    # - Some pipelines persist metric_anchors as a list of records:
    #     [{"canonical_key": ..., "anchor_hash": ..., ...}, ...]
    # - Diff expects a dict mapping canonical_key -> anchor object.
    # Determinism:
    # - Pure reshaping; no new anchors invented.
    # =====================================================================
    def _coerce_metric_anchors_to_dict(resp: dict):
        try:
            if not isinstance(resp, dict):
                return resp
            ma = resp.get("metric_anchors")
            if isinstance(ma, dict) or ma is None:
                return resp
            if isinstance(ma, list):
                out = {}
                for a in ma:
                    if not isinstance(a, dict):
                        continue
                    ck = a.get("canonical_key") or a.get("ckey") or a.get("metric_key")
                    if not ck:
                        continue
                    if ck not in out:
                        out[str(ck)] = a
                resp["metric_anchors"] = out
            return resp
        except Exception:
            return resp

    prev_response = _coerce_metric_anchors_to_dict(prev_response)
    cur_response = _coerce_metric_anchors_to_dict(cur_response)
    # =====================================================================

    prev_can = prev_response.get("primary_metrics_canonical")
    cur_can = cur_response.get("primary_metrics_canonical")

    # =========================
    # Path A: canonical-first
    # =========================
    if isinstance(prev_can, dict) and isinstance(cur_can, dict) and prev_can:
        metric_changes = []
        unchanged = increased = decreased = found = 0

        for ckey, pm in prev_can.items():
            pm = pm if isinstance(pm, dict) else {}
            cm = cur_can.get(ckey)
            cm = cm if isinstance(cm, dict) else {}

            display_name = get_display_name(prev_response, pm, cm, ckey)
            definition = get_metric_definition(prev_response, ckey)

            prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

            # ✅ HARD STOP: canonical key missing in current => not_found (no name fallback)
            if ckey not in cur_can or not isinstance(cur_can.get(ckey), dict):
                # PATCH MA2 (ADDITIVE): fill row fields from metric_anchors where possible
                _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, {})

                metric_changes.append({
                    "name": display_name,
                    "previous_value": prev_raw,
                    "current_value": "N/A",
                    "change_pct": None,
                    "change_type": "not_found",
                    "match_confidence": 0.0,
                    "context_snippet": _ctx,
                    "source_url": _src,
                    "anchor_used": False,  # not applicable when current metric missing
                    "canonical_key": ckey,
                    "metric_definition": definition,
                    "anchor_confidence": _aconf,
                })
                continue

            found += 1

            cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

            # =========================================================================
            # PATCH D0 (ADDITIVE): anchor identity (do NOT force unchanged)
            # =========================================================================
            prev_ah = _get_prev_anchor_hash(prev_response, ckey, pm)
            cur_ah = _get_cur_anchor_hash(cur_response, ckey, cm)
            anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))
            # =========================================================================

            # PATCH D2 (ADDITIVE): use canonical values for diff when available
            prev_val, prev_unit_cmp = get_canonical_value_and_unit(pm)
            cur_val, cur_unit_cmp = get_canonical_value_and_unit(cm)

            # PATCH D3 (ADDITIVE): metric-specific tolerances (schema overrides)
            abs_eps, rel_eps = get_eps_for_metric(prev_response, ckey)

            change_type = "unknown"
            change_pct = None

            # =========================================================================
            # PATCH D0B (ADDITIVE, REQUIRED): numeric-first classification even if anchors match
            # Why:
            # - anchor_same means "we matched the same evidence anchor"
            # - It MUST NOT short-circuit classification to "unchanged" when values differ.
            # - This fixes the exact bug you observed: prev_value_norm != cur_value_norm
            #   while change_type incorrectly says "unchanged".
            # =========================================================================
            if prev_val is not None and cur_val is not None:
                if abs(prev_val - cur_val) <= max(abs_eps, abs(prev_val) * rel_eps):
                    change_type = "unchanged"
                    change_pct = 0.0
                    unchanged += 1
                elif cur_val > prev_val:
                    change_type = "increased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    increased += 1
                else:
                    change_type = "decreased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    decreased += 1
            # If we cannot compare numerically, fall back:
            # - If anchors match, treat as unchanged ONLY as a last resort (formatting issue)
            elif anchor_same:
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            # =========================================================================

            # PATCH D4 (ADDITIVE): unit mismatch flag (debug only)
            unit_mismatch = False
            try:
                if prev_unit_cmp and cur_unit_cmp and str(prev_unit_cmp) != str(cur_unit_cmp):
                    unit_mismatch = True
            except Exception:
                unit_mismatch = False

            # PATCH MA2 (ADDITIVE): fill row fields from metric_anchors where possible
            _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, cm)

            # PATCH D0C (ADDITIVE): match_confidence reflects anchor identity
            match_conf = 92.0
            try:
                if anchor_same:
                    match_conf = 98.0
            except Exception:
                match_conf = 92.0

            metric_changes.append({
                "name": display_name,
                "previous_value": prev_raw,
                "current_value": cur_raw,
                "change_pct": change_pct,
                "change_type": change_type,
                "match_confidence": float(match_conf),

                "context_snippet": _ctx,
                "source_url": _src,

                # anchor identity (matching), not classification
                "anchor_used": bool(anchor_same),
                "prev_anchor_hash": prev_ah,
                "cur_anchor_hash": cur_ah,

                "canonical_key": ckey,
                "metric_definition": definition,

                "anchor_confidence": _aconf,

                # expose canonical comparison basis for debugging/convergence
                "prev_value_norm": prev_val,
                "cur_value_norm": cur_val,
                "prev_unit_cmp": prev_unit_cmp,
                "cur_unit_cmp": cur_unit_cmp,
                "unit_mismatch": bool(unit_mismatch),
                "abs_eps_used": abs_eps,
                "rel_eps_used": rel_eps,
            })

        return metric_changes, unchanged, increased, decreased, found

    # =========================
    # Path B: legacy name fallback
    # =========================
    prev_metrics = prev_response.get("primary_metrics") or {}
    cur_metrics = cur_response.get("primary_metrics") or {}
    if not isinstance(prev_metrics, dict):
        prev_metrics = {}
    if not isinstance(cur_metrics, dict):
        cur_metrics = {}

    prev_index = {}
    for k, m in prev_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            prev_index[norm_name(name)] = (name, m)

    cur_index = {}
    for k, m in cur_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            cur_index[norm_name(name)] = (name, m)

    metric_changes = []
    unchanged = increased = decreased = found = 0

    for nk, (display_name, pm) in prev_index.items():
        prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

        if nk not in cur_index:
            metric_changes.append({
                "name": display_name or "Unknown Metric",
                "previous_value": prev_raw,
                "current_value": "N/A",
                "change_pct": None,
                "change_type": "not_found",
                "match_confidence": 0.0,
                "context_snippet": None,
                "source_url": None,
                "anchor_used": False,
            })
            continue

        found += 1
        _, cm = cur_index[nk]
        cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

        prev_val, _prev_unit_cmp = get_canonical_value_and_unit(pm)
        cur_val, _cur_unit_cmp = get_canonical_value_and_unit(cm)

        prev_ah = _get_anchor_hash_from_metric(pm)
        cur_ah = _get_anchor_hash_from_metric(cm)
        anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))

        change_type = "unknown"
        change_pct = None

        # PATCH D0B mirrors canonical path: numeric-first, anchor fallback only if numeric missing
        if prev_val is not None and cur_val is not None:
            if abs(prev_val - cur_val) <= max(ABS_EPS, abs(prev_val) * REL_EPS):
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            elif cur_val > prev_val:
                change_type = "increased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                increased += 1
            else:
                change_type = "decreased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                decreased += 1
        elif anchor_same:
            change_type = "unchanged"
            change_pct = 0.0
            unchanged += 1

        metric_changes.append({
            "name": display_name or "Unknown Metric",
            "previous_value": prev_raw,
            "current_value": cur_raw,
            "change_pct": change_pct,
            "change_type": change_type,
            "match_confidence": 90.0 if anchor_same else 80.0,
            "context_snippet": None,
            "source_url": None,

            "anchor_used": bool(anchor_same),
            "prev_anchor_hash": prev_ah,
            "cur_anchor_hash": cur_ah,

            "prev_value_norm": prev_val,
            "cur_value_norm": cur_val,
        })

    return metric_changes, unchanged, increased, decreased, found

# =====================================================================
# PATCH DIFF_V2 (ADDITIVE): upgrade diff_metrics_by_name to anchor-first + value_norm-aware
# Why:
# - Enforce drift=0 when the same anchor_hash matches on both sides.
# - Prefer canonical numeric fields (value_norm/base_unit) to avoid unit/scale parsing drift.
# - Support schema-driven tolerances (abs_eps/rel_eps) when present.
# Notes:
# - Preserves prior implementation under diff_metrics_by_name_LEGACY.
# - No behavior change unless this upgraded function is called.
# =====================================================================
try:
    diff_metrics_by_name_LEGACY = diff_metrics_by_name  # type: ignore
except Exception:
    diff_metrics_by_name_LEGACY = None

def diff_metrics_by_name(prev_response: dict, cur_response: dict):
    """
    Canonical-first diff with:
      - HARD STOP when prev canonical_key is missing in current (no name fallback)
      - Row-level metric_definition sourced from PREVIOUS (original new analysis) schema:
          prev_response['metric_schema_frozen'][canonical_key] (preferred)
          else prev_response['primary_metrics_canonical'][canonical_key]
      - Backward compatible: still returns 'name' (non-empty) and existing fields.

    Returns:
      metric_changes, unchanged, increased, decreased, found
    """
    import re

    # Defaults (used unless schema provides overrides)
    ABS_EPS = 1e-9
    REL_EPS = 0.0005

    def norm_name(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def parse_num(v, unit=""):
        fn = globals().get("parse_human_number")
        if callable(fn):
            try:
                return fn(str(v), unit)
            except Exception:
                return None
        try:
            return float(str(v).replace(",", "").strip())
        except Exception:
            return None

    # =========================================================================
    # PATCH D1 (ADDITIVE): canonical numeric extractor
    # - Prefer value_norm/base_unit when present (analysis/evolution alignment)
    # - Fall back to existing parse_num(value, unit) when canonical fields missing
    # =========================================================================
    def get_canonical_value_and_unit(m: dict):
        """
        Returns: (val: float|None, unit: str)
        Priority:
          1) value_norm (float-like) + base_unit (if present)
          2) parse_num(value, unit)
        """
        m = m if isinstance(m, dict) else {}

        # 1) canonical path
        if m.get("value_norm") is not None:
            try:
                v = float(m.get("value_norm"))
                u = str(m.get("base_unit") or m.get("unit") or "").strip()
                return v, u
            except Exception:
                pass

        # 2) legacy parse path
        u = str(m.get("unit") or "").strip()
        v = parse_num(m.get("value"), u)
        return v, u
    # =========================================================================

    # =========================================================================
    # PATCH D0 (ADDITIVE): anchor helpers (drift=0 stability)
    # NOTE (IMPORTANT):
    # - Anchor_hash equality should NOT force "unchanged" if numeric values differ.
    #   It means "we matched the same evidence anchor" (identity/matching), not
    #   that the metric's value necessarily didn't change.
    # - This patch keeps anchor_same, but uses it only for match_confidence +
    #   diagnostics, not as a classification override.
    # =========================================================================
    def _get_anchor_hash_from_metric(m: dict):
        try:
            if isinstance(m, dict):
                ah = m.get("anchor_hash") or m.get("anchor") or m.get("anchorHash")
                return str(ah) if ah else None
        except Exception:
            pass
        return None

    def _get_prev_anchor_hash(prev_resp: dict, ckey: str, pm: dict):
        # 1) direct on metric row
        ah = _get_anchor_hash_from_metric(pm)
        if ah:
            return ah

        # 2) prev_response.metric_anchors[ckey].anchor_hash
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            pass

        return None

    def _get_cur_anchor_hash(cur_resp: dict, ckey: str, cm: dict):
        # 1) direct on metric row (evolution rebuild puts anchor_hash here)
        ah = _get_anchor_hash_from_metric(cm)
        if ah:
            return ah

        # 2) cur_response.metric_anchors[ckey].anchor_hash (if present)
        try:
            ma = (cur_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            pass

        return None
    # =========================================================================

    # =========================================================================
    # PATCH MA2 (ADDITIVE): wire metric_anchors into row fields
    # - Populate context_snippet/source_url from prev_response.metric_anchors[ckey] when available
    # - Output enrichment only
    # =========================================================================
    def _get_prev_anchor_obj(prev_resp: dict, ckey: str):
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                return a if isinstance(a, dict) else {}
        except Exception:
            pass
        return {}

    def _anchor_meta(prev_resp: dict, cur_resp: dict, ckey: str, pm: dict, cm: dict):
        """
        Returns: (source_url, context_snippet, anchor_confidence)
        Priority:
          1) prev_response.metric_anchors[ckey] (baseline anchoring is authoritative)
          2) current metric row fields (if present)
          3) prev metric row fields (if present)
        """
        a = _get_prev_anchor_obj(prev_resp, ckey)

        src = a.get("source_url") or a.get("url")
        ctx = a.get("context_snippet") or a.get("context")
        conf = a.get("anchor_confidence")

        if not src:
            try:
                src = (cm or {}).get("source_url") or (cm or {}).get("url")
            except Exception:
                src = None
        if not ctx:
            try:
                ctx = (cm or {}).get("context_snippet") or (cm or {}).get("context")
            except Exception:
                ctx = None

        if not src:
            try:
                src = (pm or {}).get("source_url") or (pm or {}).get("url")
            except Exception:
                src = None
        if not ctx:
            try:
                ctx = (pm or {}).get("context_snippet") or (pm or {}).get("context")
            except Exception:
                ctx = None

        try:
            if isinstance(ctx, str):
                ctx = ctx.strip()[:220] or None
            else:
                ctx = None
        except Exception:
            ctx = None

        try:
            conf = float(conf) if conf is not None else None
        except Exception:
            conf = None

        return src, ctx, conf
    # =========================================================================

    def prettify_ckey(ckey: str) -> str:
        ckey = str(ckey or "").strip()
        if not ckey:
            return "Unknown Metric"
        parts = ckey.split("__", 1)
        left = parts[0].replace("_", " ").strip()
        right = parts[1].replace("_", " ").strip() if len(parts) > 1 else ""
        left = " ".join(w.capitalize() for w in left.split())
        return f"{left} ({right})" if right else left

    def get_metric_definition(prev_resp: dict, ckey: str) -> dict:
        """
        Pull authoritative definition from the ORIGINAL analysis run (prev_response).
        """
        prev_resp = prev_resp if isinstance(prev_resp, dict) else {}

        schema = prev_resp.get("metric_schema_frozen")
        if isinstance(schema, dict):
            d = schema.get(ckey)
            if isinstance(d, dict) and d:
                out = dict(d)
                out.setdefault("canonical_key", ckey)
                return out

        prev_can = prev_resp.get("primary_metrics_canonical")
        if isinstance(prev_can, dict):
            d = prev_can.get(ckey)
            if isinstance(d, dict) and d:
                out = {
                    "canonical_key": ckey,
                    "canonical_id": d.get("canonical_id"),
                    "dimension": d.get("dimension"),
                    "name": d.get("name") or d.get("original_name"),
                    "unit": d.get("unit"),
                    "geo_scope": d.get("geo_scope"),
                    "geo_name": d.get("geo_name"),
                    "keywords": d.get("keywords"),
                }
                return {k: v for k, v in out.items() if v not in (None, "", [], {})}

        return {"canonical_key": ckey, "name": prettify_ckey(ckey)}

    def get_display_name(prev_resp: dict, prev_can_obj: dict, cur_can_obj: dict, ckey: str) -> str:
        schema = prev_resp.get("metric_schema_frozen")
        if isinstance(schema, dict):
            sm = schema.get(ckey)
            if isinstance(sm, dict):
                v = sm.get("name")
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(prev_can_obj, dict):
            for k in ("name", "original_name"):
                v = prev_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(cur_can_obj, dict):
            for k in ("name", "original_name"):
                v = cur_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        return prettify_ckey(ckey)

    # =========================================================================
    # PATCH D3 (ADDITIVE): schema-driven tolerances (optional)
    # - If schema provides abs_eps/rel_eps use them, else default.
    # =========================================================================
    def get_eps_for_metric(prev_resp: dict, ckey: str):
        ae = ABS_EPS
        re_ = REL_EPS
        try:
            schema = (prev_resp or {}).get("metric_schema_frozen")
            if isinstance(schema, dict):
                d = schema.get(ckey)
                if isinstance(d, dict):
                    if d.get("abs_eps") is not None:
                        try:
                            ae = float(d.get("abs_eps"))
                        except Exception:
                            pass
                    if d.get("rel_eps") is not None:
                        try:
                            re_ = float(d.get("rel_eps"))
                        except Exception:
                            pass
        except Exception:
            pass
        return ae, re_
    # =========================================================================

    prev_response = prev_response if isinstance(prev_response, dict) else {}
    cur_response = cur_response if isinstance(cur_response, dict) else {}

    # =====================================================================
    # PATCH AI_ANCHMAP1 (ADDITIVE): normalize metric_anchors shape (list -> dict)
    # Why:
    # - Some pipelines persist metric_anchors as a list of records:
    #     [{"canonical_key": ..., "anchor_hash": ..., ...}, ...]
    # - Diff expects a dict mapping canonical_key -> anchor object.
    # Determinism:
    # - Pure reshaping; no new anchors invented.
    # =====================================================================
    def _coerce_metric_anchors_to_dict(resp: dict):
        try:
            if not isinstance(resp, dict):
                return resp
            ma = resp.get("metric_anchors")
            if isinstance(ma, dict) or ma is None:
                return resp
            if isinstance(ma, list):
                out = {}
                for a in ma:
                    if not isinstance(a, dict):
                        continue
                    ck = a.get("canonical_key") or a.get("ckey") or a.get("metric_key")
                    if not ck:
                        continue
                    if ck not in out:
                        out[str(ck)] = a
                resp["metric_anchors"] = out
            return resp
        except Exception:
            return resp

    prev_response = _coerce_metric_anchors_to_dict(prev_response)
    cur_response = _coerce_metric_anchors_to_dict(cur_response)
    # =====================================================================

    prev_can = prev_response.get("primary_metrics_canonical")
    cur_can = cur_response.get("primary_metrics_canonical")

    # =========================
    # Path A: canonical-first
    # =========================
    if isinstance(prev_can, dict) and isinstance(cur_can, dict) and prev_can:
        metric_changes = []
        unchanged = increased = decreased = found = 0

        for ckey, pm in prev_can.items():
            pm = pm if isinstance(pm, dict) else {}
            cm = cur_can.get(ckey)
            cm = cm if isinstance(cm, dict) else {}

            display_name = get_display_name(prev_response, pm, cm, ckey)
            definition = get_metric_definition(prev_response, ckey)

            prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

            # ✅ HARD STOP: canonical key missing in current => not_found (no name fallback)
            if ckey not in cur_can or not isinstance(cur_can.get(ckey), dict):
                # PATCH MA2 (ADDITIVE): fill row fields from metric_anchors where possible
                _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, {})

                metric_changes.append({
                    "name": display_name,
                    "previous_value": prev_raw,
                    "current_value": "N/A",
                    "change_pct": None,
                    "change_type": "not_found",
                    "match_confidence": 0.0,
                    "context_snippet": _ctx,
                    "source_url": _src,
                    "anchor_used": False,  # not applicable when current metric missing
                    "canonical_key": ckey,
                    "metric_definition": definition,
                    "anchor_confidence": _aconf,
                })
                continue

            found += 1

            cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

            # =========================================================================
            # PATCH D0 (ADDITIVE): anchor identity (do NOT force unchanged)
            # =========================================================================
            prev_ah = _get_prev_anchor_hash(prev_response, ckey, pm)
            cur_ah = _get_cur_anchor_hash(cur_response, ckey, cm)
            anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))
            # =========================================================================

            # PATCH D2 (ADDITIVE): use canonical values for diff when available
            prev_val, prev_unit_cmp = get_canonical_value_and_unit(pm)
            cur_val, cur_unit_cmp = get_canonical_value_and_unit(cm)

            # PATCH D3 (ADDITIVE): metric-specific tolerances (schema overrides)
            abs_eps, rel_eps = get_eps_for_metric(prev_response, ckey)

            change_type = "unknown"
            change_pct = None

            # =========================================================================
            # PATCH D0B (ADDITIVE, REQUIRED): numeric-first classification even if anchors match
            # Why:
            # - anchor_same means "we matched the same evidence anchor"
            # - It MUST NOT short-circuit classification to "unchanged" when values differ.
            # - This fixes the exact bug you observed: prev_value_norm != cur_value_norm
            #   while change_type incorrectly says "unchanged".
            # =========================================================================
            if prev_val is not None and cur_val is not None:
                if abs(prev_val - cur_val) <= max(abs_eps, abs(prev_val) * rel_eps):
                    change_type = "unchanged"
                    change_pct = 0.0
                    unchanged += 1
                elif cur_val > prev_val:
                    change_type = "increased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    increased += 1
                else:
                    change_type = "decreased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    decreased += 1
            # If we cannot compare numerically, fall back:
            # - If anchors match, treat as unchanged ONLY as a last resort (formatting issue)
            elif anchor_same:
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            # =========================================================================

            # PATCH D4 (ADDITIVE): unit mismatch flag (debug only)
            unit_mismatch = False
            try:
                if prev_unit_cmp and cur_unit_cmp and str(prev_unit_cmp) != str(cur_unit_cmp):
                    unit_mismatch = True
            except Exception:
                unit_mismatch = False

            # PATCH MA2 (ADDITIVE): fill row fields from metric_anchors where possible
            _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, cm)

            # PATCH D0C (ADDITIVE): match_confidence reflects anchor identity
            match_conf = 92.0
            try:
                if anchor_same:
                    match_conf = 98.0
            except Exception:
                match_conf = 92.0

            # =====================================================================
            # PATCH FIX41AFC40B START
            # Post-selection unit compare backfill (dashboard-safe):
            # If we have an expected schema unit family (percent/currency/magnitude) but cur_unit_cmp is blank,
            # synthesize a comparable unit token from candidate/schema so unit_mismatch does not fail-open/blank.
            # =====================================================================
            try:
                if (not str(cur_unit_cmp or "").strip()):
                    exp_fam = str((definition or {}).get("unit_family") or "").strip().lower()
                    cand_ut = str(cm.get("unit_tag") or cm.get("unit") or cm.get("base_unit") or "").strip()
                    cand_raws = " ".join([str(cur_raw or ""), cand_ut]).lower()
                    if exp_fam == "percent" or "%" in cand_raws or "percent" in cand_raws:
                        cur_unit_cmp = "%"
                    elif exp_fam == "currency":
                        cur_unit_cmp = str((definition or {}).get("unit_tag") or (definition or {}).get("unit") or cand_ut or "").strip()
                    elif exp_fam == "magnitude":
                        cur_unit_cmp = str((definition or {}).get("unit_tag") or cand_ut or "").strip()
                    # If mismatch was solely due to blank cur_unit_cmp, clear it when units now match
                    if bool(unit_mismatch) and str(prev_unit_cmp or "").strip() and str(cur_unit_cmp or "").strip():
                        if str(prev_unit_cmp).strip() == str(cur_unit_cmp).strip():
                            unit_mismatch = False
            except Exception:
                pass
            # PATCH FIX41AFC40B END

            # =====================================================================
            # PATCH FIX41AFC20A (ADDITIVE): HARD unit-family mismatch eligibility gate
            # =====================================================================
            cur_value_blocked_reason = ""
            # =====================================================================
            # PATCH FIX41AFC69B START
            # If schema expects magnitude/unit_sales style units but candidate carries no unit evidence,
            # hard-block BEFORE the generic unit_mismatch gate. This prevents schema-based unit synthesis
            # (FIX41AFC40B) from making a unitless raw number look compatible.
            # =====================================================================
            try:
                _exp_fam_69b = str((definition or {}).get("unit_family") or "").strip().lower()
                _exp_tag_69b = str((definition or {}).get("unit_tag") or "").strip()
                if (not cur_value_blocked_reason) and _exp_fam_69b in ("magnitude", "unit_sales", "units", "count", "count_units"):
                    if str(cur_raw or "").strip():
                        if not _fix41afc69_candidate_has_unit_evidence(cm, expected_unit_tag=_exp_tag_69b, expected_unit_family=_exp_fam_69b):
                            cur_raw = ""
                            cur_value_norm = None
                            cur_unit_cmp = ""
                            cur_unit_family = ""
                            # Ensure downstream gates don't overwrite the reason
                            unit_mismatch = False
                            cur_value_blocked_reason = "unit_missing_evidence_hard_block"
            except Exception:
                pass
            # =====================================================================
            # PATCH FIX41AFC69B END
            # =====================================================================
            try:
                if bool(unit_mismatch):
                    cur_raw = ""
                    cur_value_norm = None
                    cur_unit_cmp = ""
                    cur_unit_family = ""
                    cur_value_blocked_reason = "unit_mismatch_hard_block"
            except Exception:
                pass

            # PATCH FIX41AFC35 START
            # Hard-block bare 4-digit year tokens being emitted as "current" for non-year metrics.
            # This is dashboard-safety and parity with analysis-side year suppression.
            try:
                if not cur_value_blocked_reason:
                    _uf_exp = ""
                    try:
                        _uf_exp = str((metric_def or {}).get("unit_family") or "")
                    except Exception:
                        _uf_exp = ""
                    if _uf_exp.lower() != "year":
                        _raw_tok = str(cur_raw).strip()
                        _is_year_like = False
                        try:
                            _vnum = float(cur_val_norm) if cur_val_norm is not None else None
                            if _vnum is not None and 1900.0 <= _vnum <= 2100.0 and re.fullmatch(r"\d{4}", _raw_tok or ""):
                                _is_year_like = True
                        except Exception:
                            _is_year_like = False
                        # Only block when there's no explicit unit evidence carried through
                        if _is_year_like and (not str(cur_unit_cmp or "").strip()):
                            cur_raw = ""
                            cur_val_norm = None
                            cur_unit_cmp = ""
                            cur_value_blocked_reason = "bare_year_hard_block"
            except Exception:
                pass
            # PATCH FIX41AFC35 END
            # =====================================================================
            metric_changes.append({
                "name": display_name,
                "previous_value": prev_raw,
                "current_value": cur_raw,
                "cur_value_blocked_reason": (cur_value_blocked_reason or ""),
                "change_pct": change_pct,
                "change_type": change_type,
                "match_confidence": float(match_conf),

                "context_snippet": _ctx,
                "source_url": _src,

                # anchor identity (matching), not classification
                "anchor_used": bool(anchor_same),
                "prev_anchor_hash": prev_ah,
                "cur_anchor_hash": cur_ah,

                # ============================================================
                # PATCH FIX41AFC50B START — selection trace (diagnostic only)
                # Captures raw fields for prev/cur metrics to debug blanks & scaling.
                "fix41afc50_trace": {
                    "prev_metric": {
                        "value": (pm or {}).get("value"),
                        "value_norm": (pm or {}).get("value_norm"),
                        "unit": (pm or {}).get("unit"),
                        "unit_tag": (pm or {}).get("unit_tag"),
                        "unit_family": (pm or {}).get("unit_family"),
                        "base_unit": (pm or {}).get("base_unit"),
                        "multiplier_to_base": (pm or {}).get("multiplier_to_base"),
                        "candidate_id": (pm or {}).get("candidate_id"),
                        "anchor_hash": (pm or {}).get("anchor_hash"),
                        "source_url": (pm or {}).get("source_url") or (pm or {}).get("url"),
                        "context_snippet": (pm or {}).get("context_snippet"),
                    },
                    "cur_metric": {
                        "value": (cm or {}).get("value"),
                        "value_norm": (cm or {}).get("value_norm"),
                        "unit": (cm or {}).get("unit"),
                        "unit_tag": (cm or {}).get("unit_tag"),
                        "unit_family": (cm or {}).get("unit_family"),
                        "base_unit": (cm or {}).get("base_unit"),
                        "multiplier_to_base": (cm or {}).get("multiplier_to_base"),
                        "candidate_id": (cm or {}).get("candidate_id"),
                        "anchor_hash": (cm or {}).get("anchor_hash"),
                        "source_url": (cm or {}).get("source_url") or (cm or {}).get("url"),
                        "context_snippet": (cm or {}).get("context_snippet"),
                    },
                },
                # PATCH FIX41AFC50B END
                # ============================================================

                "canonical_key": ckey,
                "metric_definition": definition,

                "anchor_confidence": _aconf,

                # expose canonical comparison basis for debugging/convergence
                "prev_value_norm": prev_val,
                "cur_value_norm": cur_val,
                "prev_unit_cmp": prev_unit_cmp,
                "cur_unit_cmp": cur_unit_cmp,
                "unit_mismatch": bool(unit_mismatch),
                "abs_eps_used": abs_eps,
                "rel_eps_used": rel_eps,
            })

        return metric_changes, unchanged, increased, decreased, found

    # =========================
    # Path B: legacy name fallback
    # =========================
    prev_metrics = prev_response.get("primary_metrics") or {}
    cur_metrics = cur_response.get("primary_metrics") or {}
    if not isinstance(prev_metrics, dict):
        prev_metrics = {}
    if not isinstance(cur_metrics, dict):
        cur_metrics = {}

    prev_index = {}
    for k, m in prev_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            prev_index[norm_name(name)] = (name, m)

    cur_index = {}
    for k, m in cur_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            cur_index[norm_name(name)] = (name, m)

    metric_changes = []
    unchanged = increased = decreased = found = 0

    for nk, (display_name, pm) in prev_index.items():
        prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

        if nk not in cur_index:
            metric_changes.append({
                "name": display_name or "Unknown Metric",
                "previous_value": prev_raw,
                "current_value": "N/A",
                "change_pct": None,
                "change_type": "not_found",
                "match_confidence": 0.0,
                "context_snippet": None,
                "source_url": None,
                "anchor_used": False,
            })
            continue

        found += 1
        _, cm = cur_index[nk]
        cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

        prev_val, _prev_unit_cmp = get_canonical_value_and_unit(pm)
        cur_val, _cur_unit_cmp = get_canonical_value_and_unit(cm)

        prev_ah = _get_anchor_hash_from_metric(pm)
        cur_ah = _get_anchor_hash_from_metric(cm)
        anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))

        change_type = "unknown"
        change_pct = None

        # PATCH D0B mirrors canonical path: numeric-first, anchor fallback only if numeric missing
        if prev_val is not None and cur_val is not None:
            if abs(prev_val - cur_val) <= max(ABS_EPS, abs(prev_val) * REL_EPS):
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            elif cur_val > prev_val:
                change_type = "increased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                increased += 1
            else:
                change_type = "decreased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                decreased += 1
        elif anchor_same:
            change_type = "unchanged"
            change_pct = 0.0
            unchanged += 1

        metric_changes.append({
            "name": display_name or "Unknown Metric",
            "previous_value": prev_raw,
            "current_value": cur_raw,
            "change_pct": change_pct,
            "change_type": change_type,
            "match_confidence": 90.0 if anchor_same else 80.0,
            "context_snippet": None,
            "source_url": None,

            "anchor_used": bool(anchor_same),
            "prev_anchor_hash": prev_ah,
            "cur_anchor_hash": cur_ah,

            "prev_value_norm": prev_val,
            "cur_value_norm": cur_val,
        })

    return metric_changes, unchanged, increased, decreased, found
# =====================================================================

def _fallback_match_from_snapshots(prev_numbers: dict, snapshots: list, anchors_by_name: dict):
    """
    When current analysis is missing, fall back to cached extracted_numbers only.
    If there is no snapshot candidate, return not_found ✅.

    Tightening implemented:
      1) Reject obvious year mismatches:
         - If metric name or prev_raw includes a year (e.g., 2024), require candidate context to contain it.
         - Also reject candidates that are a bare year if metric is not a year metric.
      2) Unit-family gating:
         - percent vs currency vs magnitude vs other (GW/TWh/tons/etc)
      3) Domain/homepage handling:
         - Downweight homepage sources heavily unless anchored (or if no non-homepage pool exists)

    Debugging enhancements:
      - Each metric row includes match_debug with:
        method, pool sizes, required years, unit families, best score, reject counts, top alternatives (small).
    """
    import re

    ABS_EPS = 1e-9
    REL_EPS = 0.0005

    def norm_unit(u: str) -> str:
        fn = globals().get("normalize_unit")
        if callable(fn):
            try:
                return fn(u)
            except Exception:
                pass
        return (u or "").strip()

    def parse_num(v, unit=""):
        fn = globals().get("parse_human_number")
        if callable(fn):
            try:
                return fn(str(v), unit)
            except Exception:
                return None
        try:
            return float(str(v).replace(",", "").strip())
        except Exception:
            return None

    def metric_tokens(name: str):
        toks = re.findall(r"[a-z0-9]+", (name or "").lower())
        stop = {"the","and","or","of","in","to","for","by","from","with","on","at","as"}
        return [t for t in toks if len(t) > 3 and t not in stop][:24]

    def unit_family(unit: str, raw: str = "", ctx: str = "") -> str:
        u = (norm_unit(unit) or "").strip().upper()
        blob = f"{raw or ''} {ctx or ''}".upper()

        # percent
        if u == "%" or "%" in blob:
            return "percent"

        # currency
        if any(x in blob for x in ["USD", "SGD", "EUR", "GBP", "S$", "$", "€", "£"]):
            return "currency"
        if any(x in u for x in ["USD", "SGD", "EUR", "GBP"]) or u.startswith("$") or u.startswith("S$"):
            return "currency"

        # magnitude suffix
        if u in ("K", "M", "B", "T") or any(x in blob for x in [" BILLION", " MILLION", " TRILLION", " BN", " MN"]):
            return "magnitude"

        # otherwise: other units like GW, TWh, tons, units, etc
        return "other"

    def required_years(metric_name: str, prev_raw: str) -> list:
        years = set()
        for s in [metric_name or "", prev_raw or ""]:
            for y in re.findall(r"\b(19\d{2}|20\d{2})\b", str(s)):
                years.add(y)
        return sorted(years)

    def year_ok(req_years: list, ctx: str) -> bool:
        if not req_years:
            return True
        c = (ctx or "").lower()
        return any(y.lower() in c for y in req_years)

    def is_bare_year(raw: str, unit: str) -> bool:
        r = (raw or "").strip()
        if unit and norm_unit(unit) not in ("", None):
            # If there is a unit, don't treat as bare year
            return False
        return bool(re.match(r"^(19\d{2}|20\d{2})$", r))

    def ctx_score(tokens, ctx: str) -> float:
        c = (ctx or "").lower()
        if not tokens:
            return 0.0
        hit = sum(1 for t in tokens if t in c)
        return hit / max(1, len(tokens))

    # Flatten candidates from snapshots ONLY, keep snapshot metadata
    candidates = []
    for sr in (snapshots or []):
        if not isinstance(sr, dict):
            continue
        url = sr.get("url")
        if not url:
            continue
        is_home = bool(sr.get("is_homepage"))
        qs = sr.get("quality_score", 1.0)
        try:
            qs = float(qs)
        except Exception:
            qs = 1.0

        for n in (sr.get("extracted_numbers") or []):
            if not isinstance(n, dict):
                continue
            candidates.append({
                "url": url,
                "value": n.get("value"),
                "unit": norm_unit(n.get("unit") or ""),
                "raw": n.get("raw") or "",
                "context": n.get("context") or "",
                "is_homepage": is_home,
                "quality_score": qs,
            })

    # Pre-split pools for tightening #3
    non_home = [c for c in candidates if not c.get("is_homepage")]
    home = [c for c in candidates if c.get("is_homepage")]

    out_changes = []
    for metric_name, prev in (prev_numbers or {}).items():
        prev_raw = prev.get("raw") or prev.get("value") or "N/A"
        prev_unit = norm_unit(prev.get("unit") or "")
        prev_val = prev.get("value")
        toks = prev.get("keywords") or metric_tokens(metric_name)

        req_years = required_years(metric_name, str(prev_raw))
        prev_fam = unit_family(prev_unit, str(prev_raw), "")

        anchor = anchors_by_name.get(metric_name) or {}
        anchor_url = anchor.get("source_url") if isinstance(anchor, dict) else None

        # Pool policy:
        # - anchored: use anchor_url pool if exists
        # - else: use non-homepage pool when available; only fall back to homepage if necessary
        pool_policy = "non_home_preferred"
        pool = non_home if non_home else candidates
        if anchor_url:
            anchored_pool = [c for c in candidates if c.get("url") == anchor_url]
            if anchored_pool:
                pool = anchored_pool
                pool_policy = "anchored_url"
            else:
                pool_policy = "anchored_url_not_present"

        reject_counts = {"year_mismatch": 0, "unit_mismatch": 0, "bare_year_reject": 0}
        best = None
        best_score = -1e9
        top_alts = []  # store a few near-misses for debugging

        for c in pool:
            ctx = c.get("context", "") or ""
            raw = c.get("raw", "") or ""
            unit = c.get("unit", "") or ""

            # (1) year gating: if required years exist, require them in context
            if not year_ok(req_years, ctx):
                reject_counts["year_mismatch"] += 1
                continue

            # reject bare-year candidates unless the metric itself is a year metric
            # (prevents "2024" being selected as a value for percent/currency/etc)
            if is_bare_year(str(raw), unit) and prev_fam != "other":
                reject_counts["bare_year_reject"] += 1
                continue

            # (2) unit-family gating
            cand_fam = unit_family(unit, raw, ctx)
            if prev_fam != cand_fam:
                reject_counts["unit_mismatch"] += 1
                continue

            score = ctx_score(toks, ctx)

            # bonus for numeric closeness
            cv = parse_num(c.get("value"), unit) or parse_num(raw, unit)
            if prev_val is not None and cv is not None:
                if abs(prev_val - cv) <= max(ABS_EPS, abs(prev_val) * REL_EPS):
                    score += 0.25

            # (3) homepage penalty unless anchored
            if c.get("is_homepage") and not anchor_url:
                score -= 0.35

            # quality_score weighting
            try:
                score *= max(0.1, min(1.0, float(c.get("quality_score", 1.0))))
            except Exception:
                pass

            # keep top alternatives for debugging
            if len(top_alts) < 5:
                top_alts.append({
                    "raw": raw[:60],
                    "unit": unit,
                    "url": c.get("url"),
                    "score": float(score),
                    "is_homepage": bool(c.get("is_homepage")),
                    "ctx": (ctx or "")[:120],
                })

            if score > best_score:
                best_score = score
                best = c

        # sort alt candidates by score desc
        try:
            top_alts.sort(key=lambda x: x.get("score", 0.0), reverse=True)
        except Exception:
            pass

        if not best:
            out_changes.append({
                "name": metric_name,
                "previous_value": prev_raw,
                "current_value": "N/A",
                "change_pct": None,
                "change_type": "not_found",
                "match_confidence": 0.0,
                "context_snippet": None,
                "source_url": None,
                "anchor_used": bool(anchor_url),

                # NEW debug payload
                "match_debug": {
                    "method": "snapshots_only",
                    "pool_policy": pool_policy,
                    "pool_size": int(len(pool)),
                    "req_years": req_years,
                    "prev_unit": prev_unit,
                    "prev_unit_family": prev_fam,
                    "reject_counts": reject_counts,
                    "top_alternatives": top_alts[:3],
                }
            })
            continue

        cur_raw = best.get("raw") or best.get("value")
        cv = parse_num(best.get("value"), best.get("unit")) or parse_num(cur_raw, best.get("unit"))

        change_type = "unknown"
        change_pct = None
        if prev_val is not None and cv is not None:
            if abs(prev_val - cv) <= max(ABS_EPS, abs(prev_val) * REL_EPS):
                change_type = "unchanged"
                change_pct = 0.0
            elif cv > prev_val:
                change_type = "increased"
                change_pct = ((cv - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
            else:
                change_type = "decreased"
                change_pct = ((cv - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0

        conf = max(0.0, min(60.0, best_score * 60.0))

        out_changes.append({
            "name": metric_name,
            "previous_value": prev_raw,
            "current_value": cur_raw,
            "change_pct": change_pct,
            "change_type": change_type,
            "match_confidence": float(conf),
            "context_snippet": (best.get("context") or "")[:200] if isinstance(best.get("context"), str) else None,
            "source_url": best.get("url"),
            "anchor_used": bool(anchor_url),

            # NEW debug payload
            "match_debug": {
                "method": "snapshots_only",
                "pool_policy": pool_policy,
                "pool_size": int(len(pool)),
                "req_years": req_years,
                "prev_unit": prev_unit,
                "prev_unit_family": prev_fam,
                "best_unit": best.get("unit"),
                "best_unit_family": unit_family(best.get("unit") or "", best.get("raw") or "", best.get("context") or ""),
                "best_score": float(best_score),
                "best_is_homepage": bool(best.get("is_homepage")),
                "reject_counts": reject_counts,
                "top_alternatives": top_alts[:3],
            }
        })

    return out_changes


def compute_source_anchored_diff_BASE(previous_data: dict, web_context: dict = None) -> dict:
    """
    Tight source-anchored evolution:
      - Prefer snapshots from analysis (baseline_sources_cache)
      - Optionally reconstruct snapshots from web_context.scraped_meta
      - If no valid snapshots: return not_found (no heuristic junk)

    Always returns a dict.
    """
    import re
    from datetime import datetime, timezone

    def _now():
        return datetime.now(timezone.utc).isoformat()

    def _safe_int(x, default=0):
        try:
            return int(x)
        except Exception:
            return default

    def _fingerprint(text: str):
        try:
            fn = globals().get("fingerprint_text")
            if callable(fn):
                return fn(text or "")
        except Exception:
            pass
        try:
            return fingerprint_text(text or "")
        except Exception:
            return None

    # ============================================================
    # PATCH CSR_UNWRAP1 (ADDITIVE): robust nested retrieval helpers
    # Why:
    # - Some runs store rebuild essentials under primary_response or results.primary_response
    # - Evolution may look only at top-level keys, causing schema=0 / anchors=0
    # ============================================================
    def _get_nested(d, path, default=None):
        try:
            x = d
            for k in path:
                if not isinstance(x, dict):
                    return default
                x = x.get(k)
            return x if x is not None else default
        except Exception:
            return default

    def _first_present(d, paths, default=None):
        for p in paths:
            v = _get_nested(d, p, None)
            if v is not None:
                return v
        return default
    # ============================================================

    # =====================================================================
    # PATCH HF5 (ADDITIVE): rehydrate previous_data from HistoryFull if wrapper
    # Why:
    # - Some UI/Sheets paths provide a summarized wrapper that lacks primary_response,
    #   metric_schema_frozen, metric_anchors, baseline_sources_cache, etc.
    # - If a full_store_ref pointer exists, load the full payload deterministically.
    #
    # NOTE:
    # - Do NOT write to `output` here (output not built yet). We stash flags
    #   and attach them after `output = {...}` is created.
    # =====================================================================
    _prev_rehydrated = False
    _prev_rehydrated_ref = ""

    try:
        if isinstance(previous_data, dict):
            _pr = previous_data.get("primary_response")

            # Determine if we are missing rebuild essentials
            _need = (
                (not isinstance(_pr, dict))
                or (not _pr)
                or (not isinstance(_pr.get("metric_schema_frozen"), dict))
            )

            if _need:
                # Explicit line (requested): simplest location first
                ref = previous_data.get("full_store_ref", "")  # <-- requested line

                # Then fall back to other known wrapper locations (more robust)
                _ref = (
                    ref
                    or (previous_data.get("results") or {}).get("full_store_ref")
                    or (isinstance(_pr, dict) and _pr.get("full_store_ref"))
                    or ""
                )

                # Last-ditch deterministic fallback: if wrapper carries _sheet_id
                if (not _ref) and isinstance(previous_data.get("_sheet_id"), str) and previous_data.get("_sheet_id"):
                    _ref = f"gsheet:HistoryFull:{previous_data.get('_sheet_id')}"

                if isinstance(_ref, str) and _ref.startswith("gsheet:"):
                    parts = _ref.split(":")
                    _ws_title = parts[1] if len(parts) > 1 and parts[1] else "HistoryFull"
                    _aid = parts[2] if len(parts) > 2 else ""
                    full = load_full_history_payload_from_sheet(_aid, worksheet_title=_ws_title) if _aid else {}
                    if isinstance(full, dict) and full:
                        previous_data = full
                        _prev_rehydrated = True
                        _prev_rehydrated_ref = _ref
    except Exception:
        pass
    # =====================================================================

    # ---------- Pull baseline snapshots (VALID only) ----------
    snapshot_origin = "none"
    baseline_sources_cache = []

    try:
        if isinstance(previous_data, dict):
            # 1) results.baseline_sources_cache (preferred)
            r = previous_data.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache"), list):
                baseline_sources_cache = r.get("baseline_sources_cache") or []
                if baseline_sources_cache:
                    snapshot_origin = "analysis_results_cache"

            # 2) top-level baseline_sources_cache
            if not baseline_sources_cache and isinstance(previous_data.get("baseline_sources_cache"), list):
                baseline_sources_cache = previous_data.get("baseline_sources_cache") or []
                if baseline_sources_cache:
                    snapshot_origin = "analysis_top_level_cache"
    except Exception:
        baseline_sources_cache = []

    # =====================================================================
    # PATCH ES1B (ADDITIVE): broaden snapshot discovery (legacy storage shapes)
    # =====================================================================
    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            pr = previous_data.get("primary_response") or {}
            if isinstance(pr, dict):
                # A) primary_response.results.baseline_sources_cache
                r2 = pr.get("results")
                if (not baseline_sources_cache) and isinstance(r2, dict) and isinstance(r2.get("baseline_sources_cache"), list):
                    baseline_sources_cache = r2.get("baseline_sources_cache") or []
                    if baseline_sources_cache:
                        snapshot_origin = "primary_response_results_cache"

                # B) primary_response.baseline_sources_cache
                if (not baseline_sources_cache) and isinstance(pr.get("baseline_sources_cache"), list):
                    baseline_sources_cache = pr.get("baseline_sources_cache") or []
                    if baseline_sources_cache:
                        snapshot_origin = "primary_response_top_level_cache"

                # C) primary_response.results.source_results (reconstruct minimal snapshot shape)
                if (not baseline_sources_cache) and isinstance(r2, dict) and isinstance(r2.get("source_results"), list):
                    rebuilt_sr = []
                    for sr in (r2.get("source_results") or []):
                        if not isinstance(sr, dict):
                            continue
                        u = sr.get("source_url") or sr.get("url")
                        ex = sr.get("extracted_numbers")
                        if u and isinstance(ex, list) and ex:
                            rebuilt_sr.append({
                                "source_url": u,
                                "extracted_numbers": ex,
                                "clean_text": sr.get("clean_text") or sr.get("content") or "",
                                "fingerprint": sr.get("fingerprint"),
                                "fetched_at": sr.get("fetched_at"),
                            })
                    rebuilt_sr.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))
                    if rebuilt_sr:
                        baseline_sources_cache = rebuilt_sr
                        snapshot_origin = "primary_response_source_results_rebuild"

        # D) previous_data.results.source_results fallback
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            r3 = previous_data.get("results")
            if isinstance(r3, dict) and isinstance(r3.get("source_results"), list):
                rebuilt_sr2 = []
                for sr in (r3.get("source_results") or []):
                    if not isinstance(sr, dict):
                        continue
                    u = sr.get("source_url") or sr.get("url")
                    ex = sr.get("extracted_numbers")
                    if u and isinstance(ex, list) and ex:
                        rebuilt_sr2.append({
                            "source_url": u,
                            "extracted_numbers": ex,
                            "clean_text": sr.get("clean_text") or sr.get("content") or "",
                            "fingerprint": sr.get("fingerprint"),
                            "fetched_at": sr.get("fetched_at"),
                        })
                rebuilt_sr2.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))
                if rebuilt_sr2:
                    baseline_sources_cache = rebuilt_sr2
                    snapshot_origin = "analysis_source_results_rebuild"
    except Exception:
        pass

    # =====================================================================
    # PATCH SS6C (ADDITIVE): evidence_records fallback for snapshots (evolution-time)
    # =====================================================================
    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            _er = None
            if isinstance(previous_data.get("results"), dict):
                _er = previous_data["results"].get("evidence_records")
            if _er is None:
                _er = previous_data.get("evidence_records")
            _rebuilt = build_baseline_sources_cache_from_evidence_records(_er)
            if isinstance(_rebuilt, list) and _rebuilt:
                baseline_sources_cache = _rebuilt
                snapshot_origin = "evidence_records_rebuild"
    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH ES1C (ADDITIVE): validate snapshot shape & prepare debug metadata
    # =====================================================================
    _snapshot_debug = None
    try:
        _raw_len = int(len(baseline_sources_cache)) if isinstance(baseline_sources_cache, list) else 0
        _kept = []
        if isinstance(baseline_sources_cache, list):
            for s in baseline_sources_cache:
                if not isinstance(s, dict):
                    continue
                u = s.get("source_url") or s.get("url")
                ex = s.get("extracted_numbers")
                if u and isinstance(ex, list):
                    _kept.append(s)
        _kept.sort(key=lambda d: (str(d.get("source_url") or d.get("url") or ""), str(d.get("fingerprint") or "")))
        baseline_sources_cache = _kept
        _snapshot_debug = {
            "origin": snapshot_origin,
            "raw_count": _raw_len,
            "valid_count": int(len(baseline_sources_cache)),
            "example_urls": [x.get("source_url") or x.get("url") for x in (baseline_sources_cache[:3] if isinstance(baseline_sources_cache, list) else [])],
            "prev_keys": sorted(list(previous_data.keys()))[:40] if isinstance(previous_data, dict) else [],
        }
    except Exception:
        pass
    # =====================================================================

    # 3) reconstruct from web_context.scraped_meta (if provided)
    if (not baseline_sources_cache) and isinstance(web_context, dict):
        try:
            scraped_meta = web_context.get("scraped_meta") or {}
            rebuilt = []
            if isinstance(scraped_meta, dict):
                for url, meta in scraped_meta.items():
                    if not isinstance(meta, dict):
                        continue
                    content = meta.get("clean_text") or meta.get("content") or ""
                    fp = meta.get("fingerprint") or _fingerprint(content)
                    if not fp or len(content or "") < 800:
                        continue
                    nums = meta.get("extracted_numbers") or []
                    if not isinstance(nums, list):
                        nums = []
                    rebuilt.append({
                        "url": url,
                        "status": meta.get("status") or "fetched",
                        "status_detail": meta.get("status_detail") or "",
                        "numbers_found": _safe_int(meta.get("numbers_found"), default=len(nums)),
                        "fetched_at": meta.get("fetched_at") or _now(),
                        "fingerprint": fp,
                        "content_type": meta.get("content_type") or "",
                        "extracted_numbers": [
                            {
                                "value": n.get("value"),
                                "unit": n.get("unit"),
                                "raw": n.get("raw"),
                                "context_snippet": (n.get("context_snippet") or n.get("context") or "")[:200],
                                "anchor_hash": n.get("anchor_hash"),
                                "is_junk": n.get("is_junk"),
                                "junk_reason": n.get("junk_reason"),
                                "unit_tag": n.get("unit_tag"),
                                "unit_family": n.get("unit_family"),
                                "base_unit": n.get("base_unit"),
                                "multiplier_to_base": n.get("multiplier_to_base"),
                                "value_norm": n.get("value_norm"),
                                "start_idx": n.get("start_idx"),
                                "end_idx": n.get("end_idx"),
                                "source_url": n.get("source_url") or url,
                            }
                            for n in nums if isinstance(n, dict)
                        ]
                    })
            if rebuilt:
                baseline_sources_cache = rebuilt
                snapshot_origin = "web_context_scraped_meta"
        except Exception:
            pass

    # Also count invalid snapshots for debug (if present)
    invalid_count = 0
    try:
        if isinstance(previous_data, dict):
            r = previous_data.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache_invalid"), list):
                invalid_count = len(r.get("baseline_sources_cache_invalid") or [])
    except Exception:
        invalid_count = 0

    # ---------- Prepare stable default output ----------
    output = {
        "status": "success",
        "message": "",
        "sources_checked": 0,
        "sources_fetched": 0,
        "numbers_extracted_total": 0,
        "stability_score": 0.0,
        "summary": {
            "total_metrics": 0,
            "metrics_found": 0,
            "metrics_increased": 0,
            "metrics_decreased": 0,
            "metrics_unchanged": 0,
        },
        "metric_changes": [],
        "source_results": [],
        "interpretation": "",
        "snapshot_origin": snapshot_origin,
        "valid_snapshot_count": len(baseline_sources_cache or []),
        "invalid_snapshot_count": int(invalid_count),
        "generated_at": _now(),
    }

    # Attach debug flags (rehydration + snapshot_debug)
    try:
        if _prev_rehydrated:
            output["previous_data_rehydrated"] = True
            output["previous_data_full_store_ref"] = _prev_rehydrated_ref
    except Exception:
        pass
    try:
        if isinstance(_snapshot_debug, dict) and _snapshot_debug:
            output["snapshot_debug"] = _snapshot_debug
    except Exception:
        pass

    # =====================================================================
    # PATCH SS6 (ADDITIVE, REQUIRED): last-chance snapshot rehydration
    # =====================================================================
    try:
        if not baseline_sources_cache and isinstance(previous_data, dict):
            _ref = previous_data.get("snapshot_store_ref") or (previous_data.get("results") or {}).get("snapshot_store_ref")
            _hash = previous_data.get("source_snapshot_hash") or (previous_data.get("results") or {}).get("source_snapshot_hash")

            # ============================================================
            # PATCH FIX41I_SS6_STABLE (ADDITIVE): prefer v2/stable snapshot refs & hashes
            # Why:
            # - Analysis now emits stable/v2 snapshot hashes (source_snapshot_hash_v2 / _stable) and
            #   snapshot_store_ref_v2 pointing at the same Snapshots row key.
            # - Evolution must prefer these fields to keep fastpath alignment intact.
            # ============================================================
            try:
                _ref_v2 = previous_data.get("snapshot_store_ref_v2") or previous_data.get("snapshot_store_ref_stable")
                if (not _ref_v2) and isinstance(previous_data.get("results"), dict):
                    _ref_v2 = (previous_data.get("results") or {}).get("snapshot_store_ref_v2") or (previous_data.get("results") or {}).get("snapshot_store_ref_stable")
                if isinstance(_ref_v2, str) and _ref_v2:
                    _ref = _ref_v2  # prefer stable ref
            except Exception:
                pass

            try:
                _hash_stable = previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2")
                if (not _hash_stable) and isinstance(previous_data.get("results"), dict):
                    _hash_stable = (previous_data.get("results") or {}).get("source_snapshot_hash_stable") or (previous_data.get("results") or {}).get("source_snapshot_hash_v2")
                if isinstance(_hash_stable, str) and _hash_stable:
                    _hash = _hash_stable  # prefer stable hash
            except Exception:
                pass
            # ============================================================

            if isinstance(_ref, str) and _ref.startswith("gsheet:"):
                parts = _ref.split(":")
                _ws_title = parts[1] if len(parts) > 1 and parts[1] else "Snapshots"
                _h = parts[2] if len(parts) > 2 else ""
                baseline_sources_cache = load_full_snapshots_from_sheet(_h, worksheet_title=_ws_title) if _h else []
                if baseline_sources_cache:
                    output["snapshot_origin"] = "sheet_snapshot_store_ref"

            if not baseline_sources_cache and isinstance(_hash, str) and _hash:
                baseline_sources_cache = load_full_snapshots_from_sheet(_hash, worksheet_title="Snapshots")
                if baseline_sources_cache:
                    output["snapshot_origin"] = "sheet_source_snapshot_hash"

            if not baseline_sources_cache and isinstance(_ref, str) and _ref and not _ref.startswith("gsheet:"):
                baseline_sources_cache = load_full_snapshots_local(_ref)
                if baseline_sources_cache:
                    output["snapshot_origin"] = "local_snapshot_store_ref"

            if isinstance(baseline_sources_cache, list):
                output["valid_snapshot_count"] = len(baseline_sources_cache)
    except Exception:
        pass
    # =====================================================================

    # If no valid snapshots, return "not_found"
    if not baseline_sources_cache:
        output["status"] = "failed"
        output["message"] = "No valid snapshots available for source-anchored evolution. (No re-fetch / no heuristic matching performed.)"
        output["interpretation"] = "Snapshot-gated: evolution refused to fabricate matches without valid cached source text."
        return output


    # =====================================================================
    # PATCH FIX41AFC10 (ADDITIVE): Fetch + attach injected URL snapshots when injection delta exists
    #
    # Goal:
    # - We already bypass fastpath when injected URL delta exists.
    # - However, evolution can still be snapshot-gated and never actually fetch the injected URL.
    # - This patch performs a **targeted fetch** for injected URLs that are not already present in
    #   the baseline snapshot universe, then appends them to `baseline_sources_cache` so they can
    #   participate in downstream admission / hashing / metric rebuild deterministically.
    #
    # Non-negotiables:
    # - No change to normal (no-injection) behavior
    # - Additive only; no refactors
    # =====================================================================
    try:
        _fix41afc10_injected_norm = []
        try:
            # Robust extraction order (same intent as FIX41AFC9 lineage)
            _fix41afc10_injected_norm = _inj_diag_norm_url_list(
                (web_context or {}).get("extra_urls")
                or (web_context or {}).get("diag_extra_urls_ui")
                or []
            )
        except Exception:
            _fix41afc10_injected_norm = []

        # If UI raw textarea is present, merge its parsed URLs (newline/comma separated)
        try:
            _fix41afc10_ui_raw = str((web_context or {}).get("diag_extra_urls_ui_raw") or (web_context or {}).get("extra_urls_ui_raw") or "")
            if _fix41afc10_ui_raw.strip():
                _fix41afc10_from_raw = _inj_diag_norm_url_list(_fix41afc10_ui_raw)
                if _fix41afc10_from_raw:
                    _fix41afc10_injected_norm = _inj_diag_norm_url_list((_fix41afc10_injected_norm or []) + _fix41afc10_from_raw)
        except Exception:
            pass

        # Only run when there is any injected URL present at all (do not affect normal runs)
        if _fix41afc10_injected_norm:
            # Build baseline URL set from existing snapshots
            _fix41afc10_base_set = set()
            try:
                for _s in (baseline_sources_cache or []):
                    if isinstance(_s, dict) and _s.get("url"):
                        _fix41afc10_base_set.add(_normalize_url(_s.get("url")) or str(_s.get("url")))
            except Exception:
                pass

            _fix41afc10_missing = [u for u in _fix41afc10_injected_norm if (u and (u not in _fix41afc10_base_set))]
            if _fix41afc10_missing:
                _fix41afc10_attempted = []
                _fix41afc10_persisted = []
                for _u in _fix41afc10_missing:
                    try:
                        _fix41afc10_attempted.append(_u)

                        _txt = None
                        try:
                            _txt = fetch_url_content(_u)
                        except Exception:
                            _txt = None

                        _status = "failed"
                        _detail = "no_text"
                        _nums = []
                        if isinstance(_txt, str) and _txt.strip():
                            _status = "success"
                            _detail = "fetched"
                            try:
                                _nums = extract_numbers_with_context(_txt, source_url=_u) or []
                            except Exception:
                                _nums = []
                        # fingerprint (best-effort; used only for determinism / debugging)
                        _fp = ""
                        try:
                            if isinstance(_txt, str) and _txt:
                                _fp = hashlib.sha256(_txt.encode("utf-8", errors="ignore")).hexdigest()
                        except Exception:
                            _fp = ""

                        _snap = {
                            "url": _u,
                            "status": _status,
                            "status_detail": _detail,
                            "numbers_found": len(_nums or []),
                            "fetched_at": _now(),
                            "fingerprint": _fp,
                            "extracted_numbers": _nums or [],
                        }

                        # Append snapshot (even if failed) so lifecycle is visible + deterministic
                        try:
                            if isinstance(baseline_sources_cache, list):
                                baseline_sources_cache.append(_snap)
                                _fix41afc10_persisted.append(_u)
                        except Exception:
                            pass
                    except Exception:
                        # never break evolution on injected fetch attempts
                        pass

                # Emit explicit debug for closure visibility
                try:
                    if isinstance(output.get("debug"), dict):
                        output.setdefault("debug", {})
                        output["debug"].setdefault("fix35", {})
                        output["debug"]["fix35"]["injected_fetch_attempted_count"] = len(_fix41afc10_attempted)
                        output["debug"]["fix35"]["injected_fetch_attempted"] = _fix41afc10_attempted
                        output["debug"]["fix35"]["injected_fetch_snapshot_appended_count"] = len(_fix41afc10_persisted)
                        output["debug"]["fix35"]["injected_fetch_snapshot_appended"] = _fix41afc10_persisted
                except Exception:
                    pass
    except Exception:
        pass

    # ---------- Use your existing deterministic metric diff helper ----------
    prev_response = (previous_data or {}).get("primary_response", {}) or {}

    # =====================================================================
    # PATCH HF6 (ADDITIVE): tolerate previous_data being the primary_response itself
    # =====================================================================
    try:
        if (not isinstance(prev_response, dict) or not prev_response) and isinstance(previous_data, dict):
            if isinstance(previous_data.get("primary_metrics_canonical"), dict) or isinstance(previous_data.get("metric_schema_frozen"), dict):
                prev_response = previous_data
    except Exception:
        pass
    # =====================================================================

    prev_metrics = prev_response.get("primary_metrics_canonical") or prev_response.get("primary_metrics") or {}

    # ============================================================
    # PATCH CSR_INPUTS1 (ADDITIVE): normalize prev schema/anchors/canon
    # (safe alias for prior `prev_analysis` usage)
    # ============================================================
    prev_analysis = previous_data  # PATCH CSR_INPUTS1_ALIAS (ADDITIVE)
    try:
        prev_schema = _first_present(prev_analysis, [
            ("metric_schema_frozen",),
            ("primary_response", "metric_schema_frozen"),
            ("results", "metric_schema_frozen"),
            ("results", "primary_response", "metric_schema_frozen"),
        ], default=None)

        prev_canon = _first_present(prev_analysis, [
            ("primary_metrics_canonical",),
            ("primary_response", "primary_metrics_canonical"),
            ("results", "primary_metrics_canonical"),
            ("results", "primary_response", "primary_metrics_canonical"),
        ], default=None)

        prev_anchors = _first_present(prev_analysis, [
            ("metric_anchors",),
            ("primary_response", "metric_anchors"),
            ("results", "metric_anchors"),
            ("results", "primary_response", "metric_anchors"),
        ], default=None)

        if (not isinstance(prev_schema, dict) or not prev_schema) and isinstance(prev_canon, dict) and prev_canon:
            try:
                fn = globals().get("freeze_metric_schema")
                if callable(fn):
                    prev_schema = fn(prev_canon)
            except Exception:
                pass
    except Exception:
        pass
    # ============================================================

    # Ensure schema/anchors are available inside prev_response (additive copies)
    try:
        if isinstance(prev_response, dict) and not isinstance(prev_response.get("metric_schema_frozen"), dict):
            if isinstance(previous_data.get("metric_schema_frozen"), dict):
                prev_response["metric_schema_frozen"] = previous_data.get("metric_schema_frozen")
    except Exception:
        pass
    try:
        if isinstance(prev_response, dict) and not isinstance(prev_response.get("metric_anchors"), dict):
            if isinstance(previous_data.get("metric_anchors"), dict):
                prev_response["metric_anchors"] = previous_data.get("metric_anchors")
    except Exception:
        pass

    # Build a minimal current metrics dict from snapshots:
    current_metrics = {}

    # Prefer metric_anchors to rebuild current_metrics (snapshot-gated)
    def _get_metric_anchors(prev: dict) -> dict:
        if not isinstance(prev, dict):
            return {}
        a = prev.get("metric_anchors")
        if isinstance(a, dict) and a:
            return a
        pr = prev.get("primary_response")
        if isinstance(pr, dict):
            a2 = pr.get("metric_anchors")
            if isinstance(a2, dict) and a2:
                return a2
        res = prev.get("results")
        if isinstance(res, dict):
            a3 = res.get("metric_anchors")
            if isinstance(a3, dict) and a3:
                return a3
        return {}

    def _canonicalize_candidate(n: dict) -> dict:
        try:
            fn = globals().get("canonicalize_numeric_candidate")
            if callable(fn):
                return fn(dict(n))
        except Exception:
            pass
        return dict(n)

    def _build_anchor_to_candidate_map(snapshots: list) -> dict:
        m = {}
        for sr in snapshots or []:
            if not isinstance(sr, dict):
                continue
            for n in (sr.get("extracted_numbers") or []):
                if not isinstance(n, dict):
                    continue
                nn = _canonicalize_candidate(n)
                ah = nn.get("anchor_hash")
                if not ah:
                    continue
                if ah not in m:
                    m[ah] = nn
        return m

    try:
        metric_anchors = _get_metric_anchors(previous_data)
        anchor_to_candidate = _build_anchor_to_candidate_map(baseline_sources_cache)

        if isinstance(metric_anchors, dict) and metric_anchors:
            for ckey, a in metric_anchors.items():
                if not isinstance(a, dict):
                    continue
                ah = a.get("anchor_hash") or a.get("anchor")
                if not ah:
                    continue
                cand = anchor_to_candidate.get(ah)
                if not isinstance(cand, dict):
                    continue

                base = prev_metrics.get(ckey) if isinstance(prev_metrics, dict) else None
                out_row = dict(base) if isinstance(base, dict) else {}
                out_row.update({
                    "canonical_key": ckey,
                    "anchor_hash": ah,
                    "anchor_used": True,
            "eligibility_pass_reason_fix41afc28": _fix41afc28_candidate_pass_reason(spec, c),
                            "eligibility_pass_reason_fix41afc28": _fix41afc28_candidate_pass_reason(spec, _cand),
                    "anchor_confidence": a.get("anchor_confidence"),
                    "source_url": cand.get("source_url") or a.get("source_url"),
                    "raw": cand.get("raw"),
                    "value": cand.get("value"),
                    "unit": cand.get("unit"),
                    "value_norm": cand.get("value_norm"),
                    "context_snippet": cand.get("context_snippet") or cand.get("context") or "",
                    "candidate_id": cand.get("candidate_id") or a.get("candidate_id"),
                    "fix36_origin": "anchor_mapping",  # PATCH FIX36 (ADD): per-metric provenance
                })
                current_metrics[ckey] = out_row
    except Exception:
        pass

    # ============================================================
    # PATCH FIX36 (ADDITIVE): set current_metrics_origin when anchor mapping populated any metrics
    # ============================================================
    try:
        if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
            if output["debug"]["fix35"].get("current_metrics_origin") in (None, "", "unknown"):
                # If any current metric was filled via anchor mapping, stamp origin
                if isinstance(current_metrics, dict) and any(isinstance(v, dict) and v.get("anchor_used") for v in current_metrics.values()):
                    output["debug"]["fix35"]["current_metrics_origin"] = "anchor_mapping"
    except Exception:
        pass
    # ============================================================

    # Rebuild fallback only if anchors didn't produce metrics
    if not isinstance(current_metrics, dict) or not current_metrics:
        try:
            # =====================================================================
            # PATCH FIX41AFC14 (ADDITIVE): Augment baseline_sources_cache with injected URL delta BEFORE schema-only rebuild
            #
            # Problem:
            # - With fastpath bypassed, evolution may fall back to schema-only rebuild *without* running any
            #   fetch cycle. In that case, injected URLs never enter baseline_sources_cache, so rebuild cannot
            #   see them and injection remains inert (attempted=0, persisted=0).
            #
            # Goal:
            # - If injected URLs are present AND introduce a delta vs the current snapshot universe,
            #   run fetch_web_context() once (normal mode) with force_admit/force_scrape enabled,
            #   then merge any successful injected snapshots into baseline_sources_cache, and only then
            #   call the rebuild function.
            #
            # Safety:
            # - No effect when no injection or no delta.
            # - Does NOT alter hashing logic; it only ensures the snapshot pool reflects successfully fetched injected sources.
            # - Never raises; on any failure, proceeds with the original baseline_sources_cache.
            # =====================================================================
            try:
                _fx14_wc = web_context if isinstance(web_context, dict) else {}
                _fx14_extra_raw = []
                if isinstance(_fx14_wc.get("extra_urls"), (list, tuple)) and _fx14_wc.get("extra_urls"):
                    _fx14_extra_raw = list(_fx14_wc.get("extra_urls") or [])
                elif isinstance(_fx14_wc.get("diag_extra_urls_ui"), (list, tuple)) and _fx14_wc.get("diag_extra_urls_ui"):
                    _fx14_extra_raw = list(_fx14_wc.get("diag_extra_urls_ui") or [])
                elif isinstance(_fx14_wc.get("diag_extra_urls_ui_raw"), str) and (_fx14_wc.get("diag_extra_urls_ui_raw") or "").strip():
                    _raw = str(_fx14_wc.get("diag_extra_urls_ui_raw") or "")
                    _parts = []
                    for _line in _raw.splitlines():
                        _line = (_line or "").strip()
                        if not _line:
                            continue
                        for _p in _line.split(","):
                            _p = (_p or "").strip()
                            if _p:
                                _parts.append(_p)
                    _fx14_extra_raw = _parts

                _fx14_inj = _inj_diag_norm_url_list(_fx14_extra_raw) if _fx14_extra_raw else []
                _fx14_base_urls = []
                if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                    for _r in baseline_sources_cache:
                        if isinstance(_r, dict) and isinstance(_r.get("source_url"), str) and _r.get("source_url"):
                            _fx14_base_urls.append(_r.get("source_url"))
                _fx14_base_set = set(_inj_diag_norm_url_list(_fx14_base_urls)) if _fx14_base_urls else set()
                _fx14_delta = sorted(list(set(_fx14_inj) - _fx14_base_set)) if _fx14_inj else []

                if _fx14_delta:
                    _fx14_q = str((prev_response or {}).get("question") or (previous_data or {}).get("question") or "").strip()
                    _fx14_prev_snap = baseline_sources_cache
                    _fx14_fwc = fetch_web_context(
                        _fx14_q or "evolution_injection_fetch_pre_rebuild",
                        num_sources=int(min(12, max(1, len(_fx14_base_set) + len(_fx14_inj)))),
                        fallback_mode=True,
                        fallback_urls=list(_inj_diag_norm_url_list(_fx14_base_urls)),
                        existing_snapshots=_fx14_prev_snap,
                        extra_urls=list(_fx14_inj),
                        diag_run_id=str((_fx14_wc or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                        diag_extra_urls_ui_raw=(_fx14_wc or {}).get("diag_extra_urls_ui_raw"),
                        identity_only=False,
                        force_scrape_extra_urls=True,
                        force_admit_extra_urls=True,
                    ) or {}

                    try:
                        if isinstance(_fx14_wc, dict):
                            _fx14_wc["evolution_calls_fetch_web_context"] = True
                    except Exception:
                        pass

                    _fx14_sm = _fx14_fwc.get("scraped_meta")
                    _fx14_bsc_new = None
                    try:
                        _fn_bsc = globals().get("_fix24_baseline_sources_cache_from_scraped_meta")
                        if callable(_fn_bsc) and isinstance(_fx14_sm, dict):
                            _fx14_bsc_new = _fn_bsc(_fx14_sm)
                    except Exception:
                        _fx14_bsc_new = None

                    if isinstance(_fx14_bsc_new, list) and _fx14_bsc_new:
                        # Merge: keep original order for existing snapshots, append new unique ones
                        _merged = list(baseline_sources_cache or [])
                        _seen = set(_inj_diag_norm_url_list(_fx14_base_urls))
                        _added = []
                        for _row in _fx14_bsc_new:
                            if not isinstance(_row, dict):
                                continue
                            _u = _row.get("source_url") or ""
                            _nu = (_inj_diag_norm_url(_u) if isinstance(_u, str) else "")
                            if _nu and _nu not in _seen:
                                _merged.append(_row)
                                _seen.add(_nu)
                                _added.append(_nu)
                        if _added:
                            baseline_sources_cache = _merged

                        # Debug
                        try:
                            output.setdefault("debug", {})
                            if isinstance(output.get("debug"), dict):
                                output["debug"].setdefault("fix41afc14", {})
                                if isinstance(output["debug"].get("fix41afc14"), dict):
                                    output["debug"]["fix41afc14"].update({
                                        "inj_delta_count": int(len(_fx14_delta)),
                                        "inj_delta": list(_fx14_delta),
                                        "merged_added_count": int(len(_added)),
                                        "merged_added": list(_added),
                                        "baseline_sources_cache_count_after_merge": int(len(baseline_sources_cache or [])),
                                    })
                        except Exception:
                            pass
            except Exception:
                pass
            # =====================================================================
            fn_rebuild = globals().get("rebuild_metrics_from_snapshots_schema_only") or globals().get("rebuild_metrics_from_snapshots")
            if callable(fn_rebuild):
                current_metrics = fn_rebuild(prev_response, baseline_sources_cache, web_context=web_context)
                # PATCH FIX36 (ADD): provenance for rebuild fallback
                try:
                    if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                        if output["debug"]["fix35"].get("current_metrics_origin") in (None, "", "unknown"):
                            output["debug"]["fix35"]["current_metrics_origin"] = "schema_only_rebuild"
                except Exception:
                    pass
                # =====================================================================
                # PATCH FIX41AFC18 (ADDITIVE): schema-preserve guard on rebuild when injected URLs are present
                # Intent:
                #   - Keep drift-0 for existing metrics even when an injected URL forces rebuild
                #   - Only allow a rebuilt metric to replace the previous metric if it carries
                #     evidence anchors and passes basic schema/unit sanity checks
                #   - Never changes fastpath logic; only post-rebuild metric selection safety
                # =====================================================================
                try:
                    _fix41afc18_prev = prev_response.get("primary_metrics_canonical") if isinstance(prev_response, dict) else None
                    _fix41afc18_schema = prev_response.get("metric_schema_frozen") if isinstance(prev_response, dict) else None

                    # Recover injected-delta set (normalized) from earlier patches, if available
                    _fix41afc18_inj_delta = []
                    try:
                        if isinstance(output.get("debug"), dict):
                            _d15 = output["debug"].get("fix41afc15") or output["debug"].get("fix41afc16") or output["debug"].get("fix41afc14") or {}
                            if isinstance(_d15, dict):
                                _fix41afc18_inj_delta = list(_d15.get("inj_delta") or [])
                    except Exception:
                        _fix41afc18_inj_delta = []
                    _fix41afc18_inj_set = set()
                    try:
                        _norm_fn = globals().get("_inj_diag_norm_url_list")
                        if callable(_norm_fn) and _fix41afc18_inj_delta:
                            _fix41afc18_inj_set = set(_norm_fn(_fix41afc18_inj_delta))
                        else:
                            _fix41afc18_inj_set = set([str(u).strip() for u in (_fix41afc18_inj_delta or []) if str(u).strip()])
                    except Exception:
                        _fix41afc18_inj_set = set([str(u).strip() for u in (_fix41afc18_inj_delta or []) if str(u).strip()])

                    def _fix41afc18_has_evidence(m: dict) -> bool:
                        if not isinstance(m, dict):
                            return False
                        ev = m.get("evidence")
                        if isinstance(ev, list) and len(ev) > 0:
                            return True
                        # fallback: anchor_hash present
                        ah = m.get("anchor_hash") or m.get("anchorHash") or m.get("anchor")
                        return bool(ah and str(ah) not in ("None", "none", ""))

                    def _fix41afc18_unit_ok(prev_m: dict, cur_m: dict) -> bool:
                        try:
                            # Prefer schema unit_family/unit_tag if available
                            ck = str(cur_m.get("canonical_key") or "")
                            schema_row = _fix41afc18_schema.get(ck) if isinstance(_fix41afc18_schema, dict) else None
                            if isinstance(schema_row, dict):
                                exp_fam = str(schema_row.get("unit_family") or "")
                                exp_tag = str(schema_row.get("unit_tag") or schema_row.get("unit") or "")
                                cur_fam = str(cur_m.get("unit_family") or "")
                                cur_tag = str(cur_m.get("unit_tag") or cur_m.get("unit") or "")
                                # If schema expects a family/tag, require match when present
                                if exp_fam and cur_fam and exp_fam != cur_fam:
                                    return False
                                if exp_tag and cur_tag and exp_tag != cur_tag:
                                    return False
                            # Fallback: if both prev and cur have unit_family, require equality
                            pf = str(prev_m.get("unit_family") or "")
                            cf = str(cur_m.get("unit_family") or "")
                            if pf and cf and pf != cf:
                                return False
                            return True
                        except Exception:
                            return True

                    def _fix41afc18_from_injected_source(cur_m: dict) -> bool:
                        try:
                            su = str(cur_m.get("source_url") or cur_m.get("source") or "")
                            if not su:
                                return False
                            # normalize by simple strip only (avoid heavy deps)
                            su = su.strip()
                            # if we have inj set, treat any exact/startswith match as injected
                            for iu in _fix41afc18_inj_set:
                                if not iu:
                                    continue
                                if su == iu or su.startswith(iu) or iu.startswith(su):
                                    return True
                            return False
                        except Exception:
                            return False

                    def _fix41afc18_value(prev_m: dict):
                        for k in ("value_norm", "value"):
                            v = prev_m.get(k) if isinstance(prev_m, dict) else None
                            if isinstance(v, (int, float)):
                                return float(v)
                        return None

                    _fix41afc18_replaced = 0
                    _fix41afc18_preserved = 0
                    _fix41afc18_added = 0
                    _fix41afc18_notes = []

                    # Only apply guard when injected delta is present (no-change case remains locked)
                    if _fix41afc18_inj_set and isinstance(_fix41afc18_prev, dict) and isinstance(current_metrics, dict):
                        for _ck, _prev_m in _fix41afc18_prev.items():
                            if not isinstance(_ck, str) or not _ck:
                                continue
                            _cur_m = current_metrics.get(_ck)
                            if not isinstance(_cur_m, dict):
                                # If rebuild dropped a metric, preserve previous
                                current_metrics[_ck] = _prev_m
                                _fix41afc18_preserved += 1
                                _fix41afc18_notes.append({"canonical_key": _ck, "action": "preserve_prev_metric_missing_in_rebuild"})
                                continue

                            # If rebuild metric lacks evidence, preserve previous (schema-driven drift lock)
                            if not _fix41afc18_has_evidence(_cur_m):
                                current_metrics[_ck] = _prev_m
                                _fix41afc18_preserved += 1
                                _fix41afc18_notes.append({"canonical_key": _ck, "action": "preserve_prev_no_evidence"})
                                continue

                            # If unit sanity fails, preserve previous
                            if not _fix41afc18_unit_ok(_prev_m, _cur_m):
                                current_metrics[_ck] = _prev_m
                                _fix41afc18_preserved += 1
                                _fix41afc18_notes.append({"canonical_key": _ck, "action": "preserve_prev_unit_mismatch"})
                                continue

                            # If the replacement comes only from injected source and is wildly different, preserve prev
                            try:
                                pv = _fix41afc18_value(_prev_m)
                                cv = _fix41afc18_value(_cur_m)
                                if pv is not None and cv is not None and pv != 0:
                                    rel = abs(cv - pv) / max(abs(pv), 1e-9)
                                    if _fix41afc18_from_injected_source(_cur_m) and rel >= 0.50:
                                        current_metrics[_ck] = _prev_m
                                        _fix41afc18_preserved += 1
                                        _fix41afc18_notes.append({"canonical_key": _ck, "action": "preserve_prev_suspicious_injected_delta", "prev": pv, "cur": cv, "rel": rel})
                                        continue
                            except Exception:
                                pass

                            # Otherwise accept rebuilt metric (explicit replace)
                            _fix41afc18_replaced += 1

                        # If rebuild produced new metrics not in schema, keep them but track (non-breaking)
                        for _ck2, _cur_m2 in list(current_metrics.items()):
                            if not isinstance(_ck2, str) or not _ck2:
                                continue
                            if _ck2 not in _fix41afc18_prev:
                                _fix41afc18_added += 1

                    # Emit debug for traceability
                    try:
                        if isinstance(output.get("debug"), dict):
                            output["debug"].setdefault("fix41afc18", {})
                            if isinstance(output["debug"].get("fix41afc18"), dict):
                                output["debug"]["fix41afc18"].update({
                                    "inj_delta_present": bool(_fix41afc18_inj_set),
                                    "inj_delta_count": int(len(_fix41afc18_inj_set)),
                                    "rebuild_metrics_replaced_count": int(_fix41afc18_replaced),
                                    "rebuild_metrics_preserved_count": int(_fix41afc18_preserved),
                                    "rebuild_metrics_added_count": int(_fix41afc18_added),
                                    "notes_sample": _fix41afc18_notes[:10],
                                })
                    except Exception:
                        pass
                except Exception:
                    pass
                # =====================================================================
        except Exception:
            current_metrics = {}

    if not isinstance(current_metrics, dict) or not current_metrics:
        output["status"] = "failed"
        output["message"] = "Valid snapshots exist, but metric rebuild returned empty. No re-fetch / no heuristic matching performed."
        output["source_results"] = baseline_sources_cache[:50]
        output["sources_checked"] = len(baseline_sources_cache)
        output["sources_fetched"] = len(baseline_sources_cache)
        output["interpretation"] = "Snapshot-ready but metric rebuild not implemented or returned empty; add/verify rebuild_metrics_from_snapshots* hooks."
        return output

    # Diff using existing diff helper if present
    metric_changes = []
    try:
        fn_diff = globals().get("diff_metrics_by_name")
        if callable(fn_diff):
            # ============================================================
            # PATCH FIX38 (ADDITIVE): ensure schema is wired into diff layer + emit lookup provenance
            # - Some runs showed schema_unit_family=None in diff rows, preventing unit-required mismatch logic.
            # - We attach metric_schema_frozen to prev_response if missing, and post-process diff rows
            #   to populate schema_unit_family + schema_lookup_source.
            # ============================================================
            _fix38_schema = None
            _fix38_schema_src = ""
            try:
                # Prefer schema already on prev_response
                if isinstance(prev_response, dict) and isinstance(prev_response.get("metric_schema_frozen"), dict) and prev_response.get("metric_schema_frozen"):
                    _fix38_schema = prev_response.get("metric_schema_frozen")
                    _fix38_schema_src = "prev_response.metric_schema_frozen"
                # Else try common containers on previous_data
                elif isinstance(previous_data, dict):
                    if isinstance(previous_data.get("metric_schema_frozen"), dict) and previous_data.get("metric_schema_frozen"):
                        _fix38_schema = previous_data.get("metric_schema_frozen")
                        _fix38_schema_src = "previous_data.metric_schema_frozen"
                    elif isinstance(previous_data.get("primary_response"), dict) and isinstance(previous_data["primary_response"].get("metric_schema_frozen"), dict) and previous_data["primary_response"].get("metric_schema_frozen"):
                        _fix38_schema = previous_data["primary_response"].get("metric_schema_frozen")
                        _fix38_schema_src = "previous_data.primary_response.metric_schema_frozen"
                # Attach if missing
                if isinstance(_fix38_schema, dict) and _fix38_schema and isinstance(prev_response, dict):
                    if not (isinstance(prev_response.get("metric_schema_frozen"), dict) and prev_response.get("metric_schema_frozen")):
                        prev_response["metric_schema_frozen"] = _fix38_schema
            except Exception:
                pass

            # Record schema wiring debug
            try:
                if isinstance(output.get("debug"), dict):
                    output["debug"].setdefault("fix38", {})
                    output["debug"]["fix38"]["schema_attached"] = bool(isinstance(_fix38_schema, dict) and _fix38_schema)
                    output["debug"]["fix38"]["schema_source"] = _fix38_schema_src
            except Exception:
                pass

            cur_resp_for_diff = {"primary_metrics_canonical": current_metrics}
            metric_changes, unchanged, increased, decreased, found = fn_diff(prev_response, cur_resp_for_diff)

            # ============================================================
            # PATCH FIX38 (ADDITIVE): populate schema_unit_family on diff rows (if missing)
            # ============================================================
            try:
                if isinstance(metric_changes, list) and metric_changes and isinstance(_fix38_schema, dict) and _fix38_schema:
                    bad = {}
                    for row in metric_changes:
                        if not isinstance(row, dict):
                            continue
                        ckey = row.get("canonical_key") or row.get("canonical") or row.get("key") or ""
                        if not ckey:
                            continue
                        if row.get("schema_unit_family") in (None, "", "None"):
                            md = _fix38_schema.get(ckey) if isinstance(_fix38_schema.get(ckey), dict) else None
                            uf = ""
                            if isinstance(md, dict):
                                uf = (md.get("unit_family") or md.get("unit") or "").strip()
                            if uf:
                                row["schema_unit_family"] = uf
                                row["fix38_schema_lookup"] = _fix38_schema_src or "attached_schema"
                        # Track any remaining year-like current with missing unit-family for diagnosis
                        try:
                            cv = row.get("cur_value_norm")
                            cu = (row.get("cur_unit_cmp") or "").strip()
                            if isinstance(cv, (int, float)) and 1900 <= float(cv) <= 2100 and not cu:
                                bad[ckey] = {"cur_value_norm": cv, "cur_unit_cmp": cu, "schema_unit_family": row.get("schema_unit_family")}
                        except Exception:
                            pass
                    if bad:
                        output.setdefault("debug", {}).setdefault("fix38", {})["bad_year_currents_sample"] = dict(list(bad.items())[:10])
            except Exception:
                pass
        else:
            metric_changes, unchanged, increased, decreased, found = ([], 0, 0, 0, 0)
    except Exception:
        metric_changes, unchanged, increased, decreased, found = ([], 0, 0, 0, 0)

    # ============================================================
    # PATCH FIX36 (ADDITIVE): attach per-row provenance from current_metrics
    # - Adds row['fix36_origin'] when available so we can see which path produced 'Current'
    # ============================================================
    try:
        if isinstance(metric_changes, list) and isinstance(current_metrics, dict):
            for r in metric_changes:
                if not isinstance(r, dict):
                    continue
                ck = r.get("canonical_key") or r.get("canonical") or ""
                if ck and isinstance(current_metrics.get(ck), dict):
                    if current_metrics[ck].get("fix36_origin"):
                        r["fix36_origin"] = current_metrics[ck].get("fix36_origin")
    except Exception:
        pass
    # ============================================================

    output["metric_changes"] = metric_changes or []

    # =====================================================================
    # PATCH FIX41AFC20C (ADDITIVE): Injection lifecycle diagnostics derived from actual state
    # =====================================================================
    try:
        if isinstance(output.get("debug"), dict):
            output["debug"].setdefault("inj_trace_v2_state", {})
            _inj_urls = []
            try:
                _inj = (output.get("debug", {}).get("inj_trace_v1") or {})
                if isinstance(_inj, dict):
                    _inj_urls = list(_inj.get("ui_norm") or []) or list(_inj.get("intake_norm") or [])
            except Exception:
                _inj_urls = []

            _pool = None
            try:
                _pool = locals().get("baseline_sources_cache_current") or output.get("baseline_sources_cache_current")
            except Exception:
                _pool = None
            if _pool is None:
                _pool = locals().get("baseline_sources_cache") or output.get("baseline_sources_cache") or output.get("source_results")

            attempted = set()
            persisted = set()
            failed = set()

            for sr in (_pool or []):
                if not isinstance(sr, dict):
                    continue
                u = str(sr.get("url") or sr.get("source_url") or "")
                if not u:
                    continue
                attempted.add(u)
                if isinstance(sr.get("extracted_numbers"), list) and len(sr.get("extracted_numbers") or []) > 0:
                    persisted.add(u)
                st = str(sr.get("status") or "")
                if st and st.lower() not in ("fetched", "success", "ok"):
                    failed.add(u)

            inj_set = set(_inj_urls or [])
            attempted_scoped = sorted([u for u in attempted if (not inj_set or u in inj_set)])
            persisted_scoped = sorted([u for u in persisted if (not inj_set or u in inj_set)])
            failed_scoped = sorted([u for u in failed if (not inj_set or u in inj_set)])

            output["debug"]["inj_trace_v2_state"]["ui_norm"] = _inj_urls
            output["debug"]["inj_trace_v2_state"]["attempted_norm"] = attempted_scoped
            output["debug"]["inj_trace_v2_state"]["persisted_norm"] = persisted_scoped
            output["debug"]["inj_trace_v2_state"]["failed_norm"] = failed_scoped
            output["debug"]["inj_trace_v2_state"]["method"] = "artifact_state_pool_scan"
    except Exception:
        pass
    # =====================================================================


    output["summary"]["total_metrics"] = len(output["metric_changes"])
    output["summary"]["metrics_found"] = int(found or 0)
    output["summary"]["metrics_increased"] = int(increased or 0)
    output["summary"]["metrics_decreased"] = int(decreased or 0)
    output["summary"]["metrics_unchanged"] = int(unchanged or 0)

    total = max(1, len(output["metric_changes"]))
    output["stability_score"] = (output["summary"]["metrics_unchanged"] / total) * 100.0

    output["source_results"] = baseline_sources_cache[:50]
    output["sources_checked"] = len(baseline_sources_cache)
    output["sources_fetched"] = len(baseline_sources_cache)

    try:
        total_nums = 0
        for sr in baseline_sources_cache or []:
            if isinstance(sr, dict) and isinstance(sr.get("extracted_numbers"), list):
                total_nums += len(sr.get("extracted_numbers") or [])
        output["numbers_extracted_total"] = int(total_nums)
    except Exception:
        pass

    output["message"] = "Source-anchored evolution completed (snapshot-gated, analysis-aligned)."
    output["interpretation"] = "Evolution used cached source snapshots only; no brute-force candidate harvesting."

    return output


# ===================== PATCH RMS_CORE1 (ADDITIVE) =====================




# =====================================================================
# PATCH WRAP_COMPUTE_SOURCE_ANCHORED_DIFF (ADDITIVE): preserve original as compute_source_anchored_diff_BASE
# and define the patched version below.
# =====================================================================

def compute_source_anchored_diff(previous_data: dict, web_context: dict = None) -> dict:
    """
    Tight source-anchored evolution:
      - Prefer snapshots from analysis (baseline_sources_cache)
      - Optionally reconstruct snapshots from web_context.scraped_meta
      - If no valid snapshots: return not_found (no heuristic junk)

    Always returns a dict.
    """
    import re
    from datetime import datetime, timezone

    def _now():
        return datetime.now(timezone.utc).isoformat()

    def _safe_int(x, default=0):
        try:
            return int(x)
        except Exception:
            return default

    def _fingerprint(text: str):
        try:
            fn = globals().get("fingerprint_text")
            if callable(fn):
                return fn(text or "")
        except Exception:
            pass
        try:
            return fingerprint_text(text or "")
        except Exception:
            return None

    # ============================================================
    # PATCH CSR_UNWRAP1 (ADDITIVE): robust nested retrieval helpers
    # Why:
    # - Some runs store rebuild essentials under primary_response or results.primary_response
    # - Evolution may look only at top-level keys, causing schema=0 / anchors=0
    # ============================================================
    def _get_nested(d, path, default=None):
        try:
            x = d
            for k in path:
                if not isinstance(x, dict):
                    return default
                x = x.get(k)
            return x if x is not None else default
        except Exception:
            return default

    def _first_present(d, paths, default=None):
        for p in paths:
            v = _get_nested(d, p, None)
            if v is not None:
                return v
        return default
    # ============================================================

    # =====================================================================
    # PATCH FIX41U (ADDITIVE): Evolution-side injected-URL diag prewire + replay visibility
    # Objective:
    # - Ensure compute_source_anchored_diff can ALWAYS populate web_context.diag_injected_urls
    #   even when the caller only supplies:
    #     * web_context["extra_urls"]
    #     * web_context["diag_extra_urls_ui_raw"]
    #     * web_context["diag_run_id"]
    # - Additionally, if the evolution UI did NOT supply extra_urls, record what the baseline
    #   analysis run had (if any) as "replayed_from_analysis_norm" for diagnostics only.
    # Safety:
    # - Purely additive diagnostics. Does NOT alter fastpath logic or hashing inputs.
    # =====================================================================
    def _fix41u_extract_injected_from_prev(prev: dict) -> dict:
        try:
            if not isinstance(prev, dict):
                return {}
            cand_paths = [
                ["results","debug","inj_trace_v1"],
                ["primary_response","results","debug","inj_trace_v1"],
                ["results","primary_response","results","debug","inj_trace_v1"],
                ["debug","inj_trace_v1"],
            ]
            for p in cand_paths:
                v = _get_nested(prev, p, None)
                if isinstance(v, dict) and v:
                    return v
        except Exception:
            pass
        return {}

    try:
        if web_context is None or not isinstance(web_context, dict):
            web_context = {}

        # Pull what the caller provided (Evolution UI should pass these)
        _fix41u_ui_raw = ""
        try:
            _fix41u_ui_raw = str(web_context.get("diag_extra_urls_ui_raw") or web_context.get("extra_urls_ui_raw") or "")
        except Exception:
            _fix41u_ui_raw = ""

        _fix41u_extra_urls = []
        try:
            _fix41u_extra_urls = _inj_diag_norm_url_list(web_context.get("extra_urls") or [])
        except Exception:
            _fix41u_extra_urls = []

        # If Evolution UI did not supply extras, capture what baseline analysis had (diagnostic only)
        _fix41u_prev_trace = _fix41u_extract_injected_from_prev(previous_data)
        _fix41u_replayed = []
        try:
            if not _fix41u_extra_urls and isinstance(_fix41u_prev_trace, dict):
                _fix41u_replayed = _inj_diag_norm_url_list(_fix41u_prev_trace.get("ui_norm") or [])
        except Exception:
            _fix41u_replayed = []

        # Ensure diag container exists
        web_context.setdefault("diag_injected_urls", {})
        if isinstance(web_context.get("diag_injected_urls"), dict):
            _d = web_context["diag_injected_urls"]
            # run_id continuity
            try:
                _d.setdefault("run_id", str(web_context.get("diag_run_id") or ""))
            except Exception:
                pass
            # UI-provided inputs (preferred truth)
            _d.setdefault("ui_raw", _fix41u_ui_raw)
            _d.setdefault("ui_norm", list(_fix41u_extra_urls))
            _d.setdefault("intake_norm", list(_fix41u_extra_urls))
            # Replay visibility (diagnostic only; NOT used for hashing unless caller also provided extras)
            if _fix41u_replayed:
                _d.setdefault("replayed_from_analysis_norm", list(_fix41u_replayed))
    except Exception:
        pass
    # =====================================================================
    # END PATCH FIX41U
    # =====================================================================


    # =====================================================================
    # PATCH HF5 (ADDITIVE): rehydrate previous_data from HistoryFull if wrapper
    # Why:
    # - Some UI/Sheets paths provide a summarized wrapper that lacks primary_response,
    #   metric_schema_frozen, metric_anchors, baseline_sources_cache, etc.
    # - If a full_store_ref pointer exists, load the full payload deterministically.
    #
    # NOTE:
    # - Do NOT write to `output` here (output not built yet). We stash flags
    #   and attach them after `output = {...}` is created.
    # =====================================================================
    _prev_rehydrated = False
    _prev_rehydrated_ref = ""

    try:
        if isinstance(previous_data, dict):
            _pr = previous_data.get("primary_response")

            # Determine if we are missing rebuild essentials
            _need = (
                (not isinstance(_pr, dict))
                or (not _pr)
                or (not isinstance(_pr.get("metric_schema_frozen"), dict))
            )

            if _need:
                # Explicit line (requested): simplest location first
                ref = previous_data.get("full_store_ref", "")  # <-- requested line

                # Then fall back to other known wrapper locations (more robust)
                _ref = (
                    ref
                    or (previous_data.get("results") or {}).get("full_store_ref")
                    or (isinstance(_pr, dict) and _pr.get("full_store_ref"))
                    or ""
                )

                # Last-ditch deterministic fallback: if wrapper carries _sheet_id
                if (not _ref) and isinstance(previous_data.get("_sheet_id"), str) and previous_data.get("_sheet_id"):
                    _ref = f"gsheet:HistoryFull:{previous_data.get('_sheet_id')}"

                if isinstance(_ref, str) and _ref.startswith("gsheet:"):
                    parts = _ref.split(":")
                    _ws_title = parts[1] if len(parts) > 1 and parts[1] else "HistoryFull"
                    _aid = parts[2] if len(parts) > 2 else ""
                    full = load_full_history_payload_from_sheet(_aid, worksheet_title=_ws_title) if _aid else {}
                    if isinstance(full, dict) and full:
                        previous_data = full
                        _prev_rehydrated = True
                        _prev_rehydrated_ref = _ref
    except Exception:
        pass
    # =====================================================================

    # ---------- Pull baseline snapshots (VALID only) ----------
    snapshot_origin = "none"
    baseline_sources_cache = []

    try:
        if isinstance(previous_data, dict):
            # 1) results.baseline_sources_cache (preferred)
            r = previous_data.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache"), list):
                baseline_sources_cache = r.get("baseline_sources_cache") or []
                if baseline_sources_cache:
                    snapshot_origin = "analysis_results_cache"

            # 2) top-level baseline_sources_cache
            if not baseline_sources_cache and isinstance(previous_data.get("baseline_sources_cache"), list):
                baseline_sources_cache = previous_data.get("baseline_sources_cache") or []
                if baseline_sources_cache:
                    snapshot_origin = "analysis_top_level_cache"
    except Exception:
        baseline_sources_cache = []

    # =====================================================================
    # PATCH ES1B (ADDITIVE): broaden snapshot discovery (legacy storage shapes)
    # =====================================================================
    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            pr = previous_data.get("primary_response") or {}
            if isinstance(pr, dict):
                # A) primary_response.results.baseline_sources_cache
                r2 = pr.get("results")
                if (not baseline_sources_cache) and isinstance(r2, dict) and isinstance(r2.get("baseline_sources_cache"), list):
                    baseline_sources_cache = r2.get("baseline_sources_cache") or []
                    if baseline_sources_cache:
                        snapshot_origin = "primary_response_results_cache"

                # B) primary_response.baseline_sources_cache
                if (not baseline_sources_cache) and isinstance(pr.get("baseline_sources_cache"), list):
                    baseline_sources_cache = pr.get("baseline_sources_cache") or []
                    if baseline_sources_cache:
                        snapshot_origin = "primary_response_top_level_cache"

                # C) primary_response.results.source_results (reconstruct minimal snapshot shape)
                if (not baseline_sources_cache) and isinstance(r2, dict) and isinstance(r2.get("source_results"), list):
                    rebuilt_sr = []
                    for sr in (r2.get("source_results") or []):
                        if not isinstance(sr, dict):
                            continue
                        u = sr.get("source_url") or sr.get("url")
                        ex = sr.get("extracted_numbers")
                        if u and isinstance(ex, list) and ex:
                            rebuilt_sr.append({
                                "source_url": u,
                                "extracted_numbers": ex,
                                "clean_text": sr.get("clean_text") or sr.get("content") or "",
                                "fingerprint": sr.get("fingerprint"),
                                "fetched_at": sr.get("fetched_at"),
                            })
                    rebuilt_sr.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))
                    if rebuilt_sr:
                        baseline_sources_cache = rebuilt_sr
                        snapshot_origin = "primary_response_source_results_rebuild"

        # D) previous_data.results.source_results fallback
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            r3 = previous_data.get("results")
            if isinstance(r3, dict) and isinstance(r3.get("source_results"), list):
                rebuilt_sr2 = []
                for sr in (r3.get("source_results") or []):
                    if not isinstance(sr, dict):
                        continue
                    u = sr.get("source_url") or sr.get("url")
                    ex = sr.get("extracted_numbers")
                    if u and isinstance(ex, list) and ex:
                        rebuilt_sr2.append({
                            "source_url": u,
                            "extracted_numbers": ex,
                            "clean_text": sr.get("clean_text") or sr.get("content") or "",
                            "fingerprint": sr.get("fingerprint"),
                            "fetched_at": sr.get("fetched_at"),
                        })
                rebuilt_sr2.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))
                if rebuilt_sr2:
                    baseline_sources_cache = rebuilt_sr2
                    snapshot_origin = "analysis_source_results_rebuild"
    except Exception:
        pass

    # =====================================================================
    # PATCH SS6C (ADDITIVE): evidence_records fallback for snapshots (evolution-time)
    # =====================================================================
    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            _er = None
            if isinstance(previous_data.get("results"), dict):
                _er = previous_data["results"].get("evidence_records")
            if _er is None:
                _er = previous_data.get("evidence_records")
            _rebuilt = build_baseline_sources_cache_from_evidence_records(_er)
            if isinstance(_rebuilt, list) and _rebuilt:
                baseline_sources_cache = _rebuilt
                snapshot_origin = "evidence_records_rebuild"
    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH ES1C (ADDITIVE): validate snapshot shape & prepare debug metadata
    # =====================================================================
    _snapshot_debug = None
    try:
        _raw_len = int(len(baseline_sources_cache)) if isinstance(baseline_sources_cache, list) else 0
        _kept = []
        if isinstance(baseline_sources_cache, list):
            for s in baseline_sources_cache:
                if not isinstance(s, dict):
                    continue
                u = s.get("source_url") or s.get("url")
                ex = s.get("extracted_numbers")
                if u and isinstance(ex, list):
                    _kept.append(s)
        _kept.sort(key=lambda d: (str(d.get("source_url") or d.get("url") or ""), str(d.get("fingerprint") or "")))
        baseline_sources_cache = _kept
        _snapshot_debug = {
            "origin": snapshot_origin,
            "raw_count": _raw_len,
            "valid_count": int(len(baseline_sources_cache)),
            "example_urls": [x.get("source_url") or x.get("url") for x in (baseline_sources_cache[:3] if isinstance(baseline_sources_cache, list) else [])],
            "prev_keys": sorted(list(previous_data.keys()))[:40] if isinstance(previous_data, dict) else [],
        }
    except Exception:
        pass
    # =====================================================================

    # 3) reconstruct from web_context.scraped_meta (if provided)
    if (not baseline_sources_cache) and isinstance(web_context, dict):
        try:
            scraped_meta = web_context.get("scraped_meta") or {}
            rebuilt = []
            if isinstance(scraped_meta, dict):
                for url, meta in scraped_meta.items():
                    if not isinstance(meta, dict):
                        continue
                    content = meta.get("clean_text") or meta.get("content") or ""
                    fp = meta.get("fingerprint") or _fingerprint(content)
                    if not fp or len(content or "") < 800:
                        continue
                    nums = meta.get("extracted_numbers") or []
                    if not isinstance(nums, list):
                        nums = []
                    rebuilt.append({
                        "url": url,
                        "status": meta.get("status") or "fetched",
                        "status_detail": meta.get("status_detail") or "",
                        "numbers_found": _safe_int(meta.get("numbers_found"), default=len(nums)),
                        "fetched_at": meta.get("fetched_at") or _now(),
                        "fingerprint": fp,
                        "content_type": meta.get("content_type") or "",
                        "extracted_numbers": [
                            {
                                "value": n.get("value"),
                                "unit": n.get("unit"),
                                "raw": n.get("raw"),
                                "context_snippet": (n.get("context_snippet") or n.get("context") or "")[:200],
                                "anchor_hash": n.get("anchor_hash"),
                                "is_junk": n.get("is_junk"),
                                "junk_reason": n.get("junk_reason"),
                                "unit_tag": n.get("unit_tag"),
                                "unit_family": n.get("unit_family"),
                                "base_unit": n.get("base_unit"),
                                "multiplier_to_base": n.get("multiplier_to_base"),
                                "value_norm": n.get("value_norm"),
                                "start_idx": n.get("start_idx"),
                                "end_idx": n.get("end_idx"),
                                "source_url": n.get("source_url") or url,
                            }
                            for n in nums if isinstance(n, dict)
                        ]
                    })
            if rebuilt:
                baseline_sources_cache = rebuilt
                snapshot_origin = "web_context_scraped_meta"
        except Exception:
            pass

    # Also count invalid snapshots for debug (if present)
    invalid_count = 0
    try:
        if isinstance(previous_data, dict):
            r = previous_data.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache_invalid"), list):
                invalid_count = len(r.get("baseline_sources_cache_invalid") or [])
    except Exception:
        invalid_count = 0

    # ---------- Prepare stable default output ----------
    output = {
        "status": "success",
        "message": "",
        "sources_checked": 0,
        "sources_fetched": 0,
        "numbers_extracted_total": 0,
        "stability_score": 0.0,
        "summary": {
            "total_metrics": 0,
            "metrics_found": 0,
            "metrics_increased": 0,
            "metrics_decreased": 0,
            "metrics_unchanged": 0,
        },
        "metric_changes": [],
        "source_results": [],
        "interpretation": "",
        "snapshot_origin": snapshot_origin,
        "valid_snapshot_count": len(baseline_sources_cache or []),
        "invalid_snapshot_count": int(invalid_count),
        "generated_at": _now(),
    }

    # =====================================================================
    # PATCH FIX35 (ADDITIVE): emit origin + hash debugging for process-of-elimination
    # - Always stamp CODE_VERSION into output
    # - Create output['debug'] container (non-breaking)
    # - Track fastpath eligibility + reason in a deterministic way
    # =====================================================================
    try:
        output["code_version"] = CODE_VERSION
    except Exception:
        pass
    # ============================================================
    # PATCH FIX41AFC40C START
    # Runtime fingerprint + effective CODE_VERSION emission (debug-only).
    # Helps confirm the deployed file matches the intended patch version.
    # ============================================================
    try:
        if not isinstance(output.get("debug"), dict):
            output["debug"] = {}
        output["debug"].setdefault("fix41afc40_runtime", {})
        output["debug"]["fix41afc40_runtime"]["runtime_file"] = __file__ if "__file__" in globals() else ""
        output["debug"]["fix41afc40_runtime"]["code_version_effective"] = CODE_VERSION
    except Exception:
        pass
    # PATCH FIX41AFC40C END
    try:
        if not isinstance(output.get("debug"), dict):
            output["debug"] = {}
        output["debug"].setdefault("fix35", {})
        output["debug"]["fix35"]["current_metrics_origin"] = "unknown"
        output["debug"]["fix35"]["fastpath_eligible"] = False
        output["debug"]["fix35"]["fastpath_reason"] = ""
    except Exception:
        pass

    # Attach debug flags (rehydration + snapshot_debug)
    try:
        if _prev_rehydrated:
            output["previous_data_rehydrated"] = True
            output["previous_data_full_store_ref"] = _prev_rehydrated_ref
    except Exception:
        pass
    try:
        if isinstance(_snapshot_debug, dict) and _snapshot_debug:
            output["snapshot_debug"] = _snapshot_debug
    except Exception:
        pass

    # =====================================================================
    # PATCH SS6 (ADDITIVE, REQUIRED): last-chance snapshot rehydration
    # =====================================================================
    try:
        if not baseline_sources_cache and isinstance(previous_data, dict):
            _ref = previous_data.get("snapshot_store_ref") or (previous_data.get("results") or {}).get("snapshot_store_ref")
            _hash = previous_data.get("source_snapshot_hash") or (previous_data.get("results") or {}).get("source_snapshot_hash")

            # ============================================================
            # PATCH FIX41I_SS6_STABLE (ADDITIVE): prefer v2/stable snapshot refs & hashes
            # Why:
            # - Analysis now emits stable/v2 snapshot hashes (source_snapshot_hash_v2 / _stable) and
            #   snapshot_store_ref_v2 pointing at the same Snapshots row key.
            # - Evolution must prefer these fields to keep fastpath alignment intact.
            # ============================================================
            try:
                _ref_v2 = previous_data.get("snapshot_store_ref_v2") or previous_data.get("snapshot_store_ref_stable")
                if (not _ref_v2) and isinstance(previous_data.get("results"), dict):
                    _ref_v2 = (previous_data.get("results") or {}).get("snapshot_store_ref_v2") or (previous_data.get("results") or {}).get("snapshot_store_ref_stable")
                if isinstance(_ref_v2, str) and _ref_v2:
                    _ref = _ref_v2  # prefer stable ref
            except Exception:
                pass

            try:
                _hash_stable = previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2")
                if (not _hash_stable) and isinstance(previous_data.get("results"), dict):
                    _hash_stable = (previous_data.get("results") or {}).get("source_snapshot_hash_stable") or (previous_data.get("results") or {}).get("source_snapshot_hash_v2")
                if isinstance(_hash_stable, str) and _hash_stable:
                    _hash = _hash_stable  # prefer stable hash
            except Exception:
                pass
            # ============================================================

            if isinstance(_ref, str) and _ref.startswith("gsheet:"):
                parts = _ref.split(":")
                _ws_title = parts[1] if len(parts) > 1 and parts[1] else "Snapshots"
                _h = parts[2] if len(parts) > 2 else ""
                baseline_sources_cache = load_full_snapshots_from_sheet(_h, worksheet_title=_ws_title) if _h else []
                if baseline_sources_cache:
                    output["snapshot_origin"] = "sheet_snapshot_store_ref"

            if not baseline_sources_cache and isinstance(_hash, str) and _hash:
                baseline_sources_cache = load_full_snapshots_from_sheet(_hash, worksheet_title="Snapshots")
                if baseline_sources_cache:
                    output["snapshot_origin"] = "sheet_source_snapshot_hash"

            if not baseline_sources_cache and isinstance(_ref, str) and _ref and not _ref.startswith("gsheet:"):
                baseline_sources_cache = load_full_snapshots_local(_ref)
                if baseline_sources_cache:
                    output["snapshot_origin"] = "local_snapshot_store_ref"

            if isinstance(baseline_sources_cache, list):
                output["valid_snapshot_count"] = len(baseline_sources_cache)
    except Exception:
        pass
    # =====================================================================

    # If no valid snapshots, return "not_found"
    if not baseline_sources_cache:
        output["status"] = "failed"
        output["message"] = "No valid snapshots available for source-anchored evolution. (No re-fetch / no heuristic matching performed.)"
        output["interpretation"] = "Snapshot-gated: evolution refused to fabricate matches without valid cached source text."
        return output

    # ---------- Use your existing deterministic metric diff helper ----------
    prev_response = (previous_data or {}).get("primary_response", {}) or {}

    # =====================================================================
    # PATCH HF6 (ADDITIVE): tolerate previous_data being the primary_response itself
    # =====================================================================
    try:
        if (not isinstance(prev_response, dict) or not prev_response) and isinstance(previous_data, dict):
            if isinstance(previous_data.get("primary_metrics_canonical"), dict) or isinstance(previous_data.get("metric_schema_frozen"), dict):
                prev_response = previous_data
    except Exception:
        pass
    # =====================================================================

    prev_metrics = prev_response.get("primary_metrics_canonical") or prev_response.get("primary_metrics") or {}

    # ============================================================
    # PATCH CSR_INPUTS1 (ADDITIVE): normalize prev schema/anchors/canon
    # (safe alias for prior `prev_analysis` usage)
    # ============================================================
    prev_analysis = previous_data  # PATCH CSR_INPUTS1_ALIAS (ADDITIVE)
    try:
        prev_schema = _first_present(prev_analysis, [
            ("metric_schema_frozen",),
            ("primary_response", "metric_schema_frozen"),
            ("results", "metric_schema_frozen"),
            ("results", "primary_response", "metric_schema_frozen"),
        ], default=None)

        prev_canon = _first_present(prev_analysis, [
            ("primary_metrics_canonical",),
            ("primary_response", "primary_metrics_canonical"),
            ("results", "primary_metrics_canonical"),
            ("results", "primary_response", "primary_metrics_canonical"),
        ], default=None)

        prev_anchors = _first_present(prev_analysis, [
            ("metric_anchors",),
            ("primary_response", "metric_anchors"),
            ("results", "metric_anchors"),
            ("results", "primary_response", "metric_anchors"),
        ], default=None)

        if (not isinstance(prev_schema, dict) or not prev_schema) and isinstance(prev_canon, dict) and prev_canon:
            try:
                fn = globals().get("freeze_metric_schema")
                if callable(fn):
                    prev_schema = fn(prev_canon)
            except Exception:
                pass
    except Exception:
        pass
    # ============================================================

    # Ensure schema/anchors are available inside prev_response (additive copies)
    try:
        if isinstance(prev_response, dict) and not isinstance(prev_response.get("metric_schema_frozen"), dict):
            if isinstance(previous_data.get("metric_schema_frozen"), dict):
                prev_response["metric_schema_frozen"] = previous_data.get("metric_schema_frozen")
    except Exception:
        pass
    try:
        if isinstance(prev_response, dict) and not isinstance(prev_response.get("metric_anchors"), dict):
            if isinstance(previous_data.get("metric_anchors"), dict):
                prev_response["metric_anchors"] = previous_data.get("metric_anchors")
    except Exception:
        pass

    # =====================================================================
    # PATCH FIX31 (ADDITIVE): authoritative fast-path when sources + data unchanged
    #
    # Principle:
    #   If the source snapshot inputs are proven unchanged, do NOT perform any
    #   anchor-based selection or rebuild "gymnastics". Reuse the already
    #   processed + schema-gated metrics from the previous analysis payload and
    #   publish directly.
    #
    # Implementation notes:
    #   - We compute a stable hash from baseline_sources_cache[*].extracted_numbers
    #     using a reduced, order-independent projection.
    #   - If it matches previous_data/source_snapshot_hash AND a prior processed
    #     canonical metrics dict exists, we set current_metrics to prev_metrics
    #     and force anchors to be ignored by short-circuiting _get_metric_anchors().
    #   - This is purely additive and does not remove legacy paths.
    # =====================================================================
    _fix31_authoritative_reuse = False
    try:
        import json as _fix31_json
        import hashlib as _fix31_hashlib

        def _fix31_stable_dumps(obj):
            try:
                return _fix31_json.dumps(obj, sort_keys=True, ensure_ascii=False, separators=(",", ":"))
            except Exception:
                # last resort
                return str(obj)

        def _fix31_snapshot_fingerprint(bsc):
            # Reduced projection: stable across benign field additions/ordering
            rows = []
            for sr in (bsc or []):
                if not isinstance(sr, dict):
                    continue
                u = sr.get("source_url") or sr.get("url") or ""
                nums = []
                for n in (sr.get("extracted_numbers") or []):
                    if not isinstance(n, dict):
                        continue
                    nums.append({
                        "anchor_hash": n.get("anchor_hash") or "",
                        "value_norm": n.get("value_norm"),
                        "unit_tag": n.get("unit_tag") or "",
                        "unit": n.get("unit") or n.get("unit_norm") or "",
                        "currency": n.get("currency") or n.get("currency_symbol") or "",
                        "is_percent": bool(n.get("is_percent") or n.get("has_percent")),
                        "is_junk": bool(n.get("is_junk")),
                    })
                # order-independent for candidates
                nums = sorted(nums, key=lambda x: (_fix31_stable_dumps(x)))
                rows.append({"source_url": u, "extracted_numbers": nums})
            rows = sorted(rows, key=lambda r: r.get("source_url") or "")
            payload = _fix31_stable_dumps(rows).encode("utf-8", errors="ignore")
            return _fix31_hashlib.sha256(payload).hexdigest()

        # =========================
        # PATCH FIX37 (ADD): stable snapshot hash for fastpath alignment
        # - Use the SAME hash function as analysis (compute_source_snapshot_hash_v2) whenever possible.
        # - Falls back to legacy compute_source_snapshot_hash, then to the reduced fingerprint.
        # =========================
        def _fix37_snapshot_hash_stable(bsc):
            try:
                if isinstance(bsc, list) and bsc:
                    try:
                        _h2 = compute_source_snapshot_hash_v2(bsc)
                        if _h2:
                            return str(_h2)
                    except Exception:
                        pass
                    try:
                        _h1 = compute_source_snapshot_hash(bsc)
                        if _h1:
                            return str(_h1)
                    except Exception:
                        pass
            except Exception:
                pass
            return _fix31_snapshot_fingerprint(bsc)

        _prev_hash = None
        _prev_hash_stable = None
        if isinstance(previous_data, dict):
            # =========================
            # PATCH FIX37 (ADD): prefer stable hash keys when available
            # =========================
            _prev_hash_stable = previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2")
            try:
                if not _prev_hash_stable and isinstance(previous_data.get("results"), dict):
                    _prev_hash_stable = (previous_data.get("results") or {}).get("source_snapshot_hash_stable") or (previous_data.get("results") or {}).get("source_snapshot_hash_v2")
            except Exception:
                pass
            _prev_hash = _prev_hash_stable or previous_data.get("source_snapshot_hash")
            # PATCH FIX41I_FASTPATH_PREF (ADDITIVE): explicit preferred hash (stable/v2 first)
            _prev_hash_pref = _prev_hash_stable or previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2") or _prev_hash

            try:
                if not _prev_hash and isinstance(previous_data.get("results"), dict):
                    _prev_hash = (previous_data.get("results") or {}).get("source_snapshot_hash")
            except Exception:
                pass

# PATCH FIX36 (ADDITIVE): populate explicit fastpath ineligibility reasons
        # - Record current/previous hashes even on mismatch
        # - Explain which prerequisite failed (no_prev_hash / no_prev_metrics / no_snapshots / hash_mismatch)
        # ============================================================
        _fix36_cur_hash = None
        _fix36_reason = ""

        # =====================================================================
        # PATCH FIX41AFC15 (ADDITIVE): Pre-hash merge of injected URL delta into baseline_sources_cache
        #
        # Why:
        # - In your latest evolution JSON, fastpath was correctly bypassed due to injected delta,
        #   but the current snapshot universe (baseline_sources_cache) still did NOT include the
        #   injected URL, so the stable hash still matched and downstream logic treated the run
        #   as "no delta" (no fetch, no rebuild, no injected lifecycle).
        #
        # Goal:
        # - BEFORE computing the current stable hash / fastpath eligibility, deterministically
        #   append placeholder snapshot rows for any injected URLs missing from the current
        #   baseline_sources_cache universe. This makes the hash differ (as it should when the
        #   source universe changes), forcing the normal rebuild/fetch pathways without changing
        #   the hashing algorithm itself.
        #
        # Safety:
        # - Purely additive.
        # - No effect when no injected URLs are present OR all injected URLs already exist in
        #   baseline_sources_cache.
        # =====================================================================
        try:
            _fx15_wc = web_context if isinstance(web_context, dict) else {}
            _fx15_extra = []
            # Prefer already-wired list fields
            if isinstance(_fx15_wc.get("extra_urls"), (list, tuple)):
                _fx15_extra = list(_fx15_wc.get("extra_urls") or [])
            elif isinstance(_fx15_wc.get("diag_extra_urls_ui"), (list, tuple)) and _fx15_wc.get("diag_extra_urls_ui"):
                _fx15_extra = list(_fx15_wc.get("diag_extra_urls_ui") or [])
            elif isinstance(_fx15_wc.get("diag_extra_urls_ui_raw"), str) and (_fx15_wc.get("diag_extra_urls_ui_raw") or "").strip():
                _raw = str(_fx15_wc.get("diag_extra_urls_ui_raw") or "")
                _parts = []
                for _line in _raw.splitlines():
                    _line = (_line or "").strip()
                    if not _line:
                        continue
                    for _p in _line.split(","):
                        _p = (_p or "").strip()
                        if _p:
                            _parts.append(_p)
                _fx15_extra = _parts

            _fx15_inj_norm = _inj_diag_norm_url_list(_fx15_extra) if _fx15_extra else []
            if _fx15_inj_norm and isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                _fx15_base_urls = []
                for _r in (baseline_sources_cache or []):
                    if not isinstance(_r, dict):
                        continue
                    _u = _r.get("source_url") or _r.get("url") or ""
                    if isinstance(_u, str) and _u:
                        _fx15_base_urls.append(_u)
                _fx15_base_set = set(_inj_diag_norm_url_list(_fx15_base_urls)) if _fx15_base_urls else set()
                _fx15_delta = [u for u in _fx15_inj_norm if u and u not in _fx15_base_set]
                if _fx15_delta:
                    # Append stable placeholders so the snapshot hash changes deterministically.
                    for _u in _fx15_delta:
                        baseline_sources_cache.append({
                            "source_url": _u,
                            "url": _u,
                            "status": "injected_pending",
                            "status_detail": "injected_url_placeholder_pre_hash",
                            "snapshot_text": "",
                            "extracted_numbers": [],
                            "numbers_found": 0,
                            "injected": True,
                            "injected_reason": "prehash_placeholder",
                        })
                    # Also ensure downstream sees a consistent universe via web_context["extra_urls"].
                    try:
                        if isinstance(_fx15_wc, dict):
                            _fx15_wc.setdefault("extra_urls", [])
                            if isinstance(_fx15_wc.get("extra_urls"), list):
                                # keep original order; append unique normalized
                                _seen = set(_inj_diag_norm_url_list(_fx15_wc.get("extra_urls") or []))
                                for _u in _fx15_delta:
                                    if _u not in _seen:
                                        _fx15_wc["extra_urls"].append(_u)
                                        _seen.add(_u)
                    except Exception:
                        pass
                    # Debug
                    try:
                        output.setdefault("debug", {})
                        if isinstance(output.get("debug"), dict):
                            output["debug"].setdefault("fix41afc15", {})
                            if isinstance(output["debug"].get("fix41afc15"), dict):
                                output["debug"]["fix41afc15"].update({
                                    "inj_norm_count": int(len(_fx15_inj_norm)),
                                    "inj_norm": list(_fx15_inj_norm),
                                    "inj_delta_count": int(len(_fx15_delta)),
                                    "inj_delta": list(_fx15_delta),
                                    "baseline_sources_cache_count_after_placeholder": int(len(baseline_sources_cache or [])),
                                })
                    except Exception:
                        pass
        except Exception:
            pass

        # =====================================================================
        # PATCH FIX41AFC16 (ADDITIVE): If injected URL placeholders exist, actually fetch+extract them
        #
        # Observed gap (from evolution JSON):
        #   - Injected URLs were present in ui/intake/hash_inputs, but remained:
        #       status = "injected_pending" / status_detail = "injected_url_placeholder_pre_hash"
        #   - So they never produced snapshot_text / extracted_numbers, and thus could not
        #     influence metric rebuild beyond a "hash universe" delta.
        #
        # Goal:
        #   - When injection is present, attempt to fetch+extract the injected URLs (delta-only),
        #     and update their baseline_sources_cache rows in-place so downstream rebuild sees them
        #     like normal fetched sources (or explicit failed reasons).
        #
        # Safety:
        #   - Purely additive.
        #   - No effect when no injected URLs are present.
        #   - Only touches rows that are injected placeholders (status == injected_pending) OR
        #     URLs that are injected_delta (not already in baseline).
        # =====================================================================
        try:
            _fx16_wc = web_context if isinstance(web_context, dict) else {}
            _fx16_extra = []
            if isinstance(_fx16_wc.get("extra_urls"), (list, tuple)) and _fx16_wc.get("extra_urls"):
                _fx16_extra = list(_fx16_wc.get("extra_urls") or [])
            elif isinstance(_fx16_wc.get("diag_extra_urls_ui"), (list, tuple)) and _fx16_wc.get("diag_extra_urls_ui"):
                _fx16_extra = list(_fx16_wc.get("diag_extra_urls_ui") or [])
            elif isinstance(_fx16_wc.get("diag_extra_urls_ui_raw"), str) and (_fx16_wc.get("diag_extra_urls_ui_raw") or "").strip():
                _raw = str(_fx16_wc.get("diag_extra_urls_ui_raw") or "")
                _parts = []
                for _line in _raw.splitlines():
                    _line = (_line or "").strip()
                    if not _line:
                        continue
                    for _p in _line.split(","):
                        _p = (_p or "").strip()
                        if _p:
                            _parts.append(_p)
                _fx16_extra = _parts

            _fx16_inj_norm = _inj_diag_norm_url_list(_fx16_extra) if _fx16_extra else []
            _fx16_base_urls = []
            if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                for _r in (baseline_sources_cache or []):
                    if not isinstance(_r, dict):
                        continue
                    _u = _r.get("source_url") or _r.get("url") or ""
                    if isinstance(_u, str) and _u:
                        _fx16_base_urls.append(_u)
            _fx16_base_set = set(_inj_diag_norm_url_list(_fx16_base_urls)) if _fx16_base_urls else set()
            _fx16_delta = [u for u in _fx16_inj_norm if u and u not in _fx16_base_set]

            # Identify placeholder rows that should be fetched
            _fx16_targets = []
            if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                for _r in (baseline_sources_cache or []):
                    if not isinstance(_r, dict):
                        continue
                    _u = _r.get("source_url") or _r.get("url") or ""
                    _u_norm = _inj_diag_norm_url_list([_u])[0] if isinstance(_u, str) and _u else ""
                    if not _u_norm:
                        continue
                    if _r.get("status") == "injected_pending":
                        _fx16_targets.append((_u_norm, _r, "placeholder_row"))
                    elif _u_norm in _fx16_delta:
                        _fx16_targets.append((_u_norm, _r, "delta_row"))

            # Also cover the case where placeholders were not appended (defensive)
            for _u in (_fx16_delta or []):
                if not isinstance(baseline_sources_cache, list):
                    continue
                if any((_inj_diag_norm_url_list([(_r.get("source_url") or _r.get("url") or "")])[0] if isinstance(_r, dict) else "") == _u for _r in (baseline_sources_cache or [])):
                    continue
                baseline_sources_cache.append({
                    "source_url": _u,
                    "url": _u,
                    "status": "injected_pending",
                    "status_detail": "injected_url_placeholder_pre_hash",
                    "snapshot_text": "",
                    "extracted_numbers": [],
                    "numbers_found": 0,
                    "injected": True,
                    "injected_reason": "fx16_defensive_placeholder",
                })
                _fx16_targets.append((_u, baseline_sources_cache[-1], "defensive_placeholder"))

            # Fetch+extract for targets (best-effort)
            _fx16_fetched = []
            _fx16_failed = []
            if _fx16_targets:
                for (_u_norm, _row, _why) in _fx16_targets:
                    # Skip if row already has text/numbers (idempotent)
                    try:
                        if isinstance(_row.get("snapshot_text"), str) and _row.get("snapshot_text").strip():
                            continue
                        if isinstance(_row.get("extracted_numbers"), list) and len(_row.get("extracted_numbers") or []) > 0:
                            continue
                    except Exception:
                        pass

                    _txt = None
                    _detail = ""
                    try:
                        _txt, _detail = fetch_url_content_with_status(_u_norm, timeout=25)
                    except Exception as _e:
                        _txt, _detail = None, f"exception:{type(_e).__name__}"

                    if _txt and isinstance(_txt, str) and len(_txt.strip()) >= 200:
                        try:
                            _nums = extract_numbers_with_context(_txt, source_url=_u_norm) or []
                        except Exception:
                            _nums = []
                        _row.update({
                            "status": "fetched",
                            "status_detail": (_detail or "success"),
                            "snapshot_text": _txt[:7000],
                            "extracted_numbers": _nums,
                            "numbers_found": int(len(_nums or [])),
                            "injected": True,
                            "injected_reason": _row.get("injected_reason") or "fx16_fetch_and_extract",
                        })
                        _fx16_fetched.append({"url": _u_norm, "why": _why, "numbers_found": int(len(_nums or [])), "status_detail": (_detail or "success")})
                    else:
                        _row.update({
                            "status": "failed",
                            "status_detail": (_detail or "failed:no_text"),
                            "snapshot_text": "",
                            "extracted_numbers": [],
                            "numbers_found": 0,
                            "injected": True,
                            "injected_reason": _row.get("injected_reason") or "fx16_fetch_failed",
                        })
                        _fx16_failed.append({"url": _u_norm, "why": _why, "status_detail": (_detail or "failed:no_text")})

            # Emit debug
            try:
                output.setdefault("debug", {})
                if isinstance(output.get("debug"), dict):
                    output["debug"].setdefault("fix41afc16", {})
                    if isinstance(output["debug"].get("fix41afc16"), dict):
                        output["debug"]["fix41afc16"].update({
                            "inj_norm_count": int(len(_fx16_inj_norm or [])),
                            "inj_delta_count": int(len(_fx16_delta or [])),
                            "fetch_target_count": int(len(_fx16_targets or [])),
                            "fetched_count": int(len(_fx16_fetched or [])),
                            "failed_count": int(len(_fx16_failed or [])),
                            "fetched": list(_fx16_fetched or []),
                            "failed": list(_fx16_failed or []),
                        })
            except Exception:
                pass
        except Exception:
            pass


        # =====================================================================
        # PATCH FIX41AFC17 (ADDITIVE): Pin fetched injected snapshots into canonical snapshot plumbing
        #
        # Observed gap (from evolution JSON after FIX41AFC16):
        #   - Injected URL reaches intake/admitted/attempted/hash_inputs, but snapshot_debug remains empty
        #     (origin none / raw_count 0), and downstream consumers appear to miss the fetched snapshot_text.
        #
        # Goal:
        #   - Ensure the same snapshot-carrier fields used by New Analysis are populated for Evolution,
        #     so that attach_source_snapshots_to_analysis() (and any downstream rebuild plumbing) can
        #     “see” the injected (and other) snapshots deterministically.
        #
        # Safety:
        #   - Purely additive wiring.
        #   - No effect if baseline_sources_cache is missing.
        #   - Does not alter hashing logic; only ensures snapshots are attached consistently.
        # =====================================================================
        try:
            if isinstance(baseline_sources_cache, list) and baseline_sources_cache and isinstance(web_context, dict):
                # Provide canonical aliases for current pool (additive; downstream may read any of these)
                web_context.setdefault("current_baseline_sources_cache", baseline_sources_cache)
                web_context.setdefault("baseline_sources_cache_current", baseline_sources_cache)
                web_context.setdefault("current_source_results", baseline_sources_cache)

                # Mirror into output for downstream consumers/debug (additive)
                try:
                    output.setdefault("baseline_sources_cache_current", baseline_sources_cache)
                except Exception:
                    pass
                try:
                    output.setdefault("baseline_sources_cache", baseline_sources_cache)
                except Exception:
                    pass
                try:
                    output.setdefault("results", {})
                    if isinstance(output.get("results"), dict):
                        output["results"].setdefault("baseline_sources_cache_current", baseline_sources_cache)
                        output["results"].setdefault("baseline_sources_cache", baseline_sources_cache)
                except Exception:
                    pass

                # Call the same snapshot attach helper used by analysis if present (best-effort)
                try:
                    _att_fn = globals().get("attach_source_snapshots_to_analysis")
                    if callable(_att_fn):
                        _att_fn(output, web_context)
                except Exception:
                    pass

                # Small debug breadcrumb
                try:
                    output.setdefault("debug", {})
                    if isinstance(output.get("debug"), dict):
                        output["debug"].setdefault("fix41afc17", {})
                        if isinstance(output["debug"].get("fix41afc17"), dict):
                            output["debug"]["fix41afc17"].update({
                                "attached_pool_count": int(len(baseline_sources_cache)),
                                "attach_called": bool(callable(globals().get("attach_source_snapshots_to_analysis"))),
                            })
                except Exception:
                    pass
        except Exception:
            pass
        # =====================================================================

        try:
            if not (isinstance(baseline_sources_cache, list) and baseline_sources_cache):
                _fix36_reason = "no_snapshots"
            elif not (isinstance(prev_metrics, dict) and prev_metrics):
                _fix36_reason = "no_prev_metrics"
            else:
                _fix36_cur_hash = _fix37_snapshot_hash_stable(baseline_sources_cache)
                if not (isinstance(_prev_hash, str) and _prev_hash):
                    _fix36_reason = "no_prev_hash"
                elif _fix36_cur_hash != _prev_hash:
                    _fix36_reason = "hash_mismatch"
                else:
                    _fix36_reason = "hash_match_and_prev_metrics_present"

            # =====================================================================
            # PATCH EVO_FASTPATH_BYPASS_ON_INJECTED_URL_DELTA_V1 (ADDITIVE)
            # Intent:
            #   If the Evolution UI supplies injected URLs that are NOT already part of the
            #   baseline source universe, bypass fastpath eligibility even when hashes match.
            #   This does NOT weaken fastpath for the locked/no-injection case; it only prevents
            #   "observed but inert" injections from being ignored when they materially change
            #   the intended source universe.
            #
            #   Key rule:
            #     - If injected_delta (normalized_injected_urls - normalized_baseline_urls) is non-empty
            #       and fastpath would otherwise be eligible, force _fix36_reason to a bypass reason so
            #       fastpath_eligible becomes False and rebuild path can run.
            # =====================================================================
            try:
                _evo_wc = web_context if isinstance(web_context, dict) else {}
                _evo_diag = _evo_wc.get("diag_injected_urls") if isinstance(_evo_wc.get("diag_injected_urls"), dict) else {}
                _evo_extra_urls = []
                # Prefer already-normalized intake/ui lists if present
                for _k in ("intake", "ui_norm", "ui", "extra_urls"):
                    _v = _evo_diag.get(_k)
                    if isinstance(_v, (list, tuple)) and _v:
                        _evo_extra_urls = list(_v)
                        break
                # Fall back to raw web_context extras if diag not populated
                if not _evo_extra_urls:
                    _v2 = _evo_wc.get("extra_urls")
                    if isinstance(_v2, (list, tuple)) and _v2:
                        _evo_extra_urls = list(_v2)


                # =====================================================================
                # PATCH EVO_FASTPATH_BYPASS_INJ_DELTA_V2 (ADDITIVE):
                #   Robustly recover injected/extra URLs for bypass detection even when
                #   diag_injected_urls is not populated yet (common on replay/fastpath).
                #   Sources (in order):
                #     - web_context["extra_urls"] if list
                #     - web_context["diag_extra_urls_ui"] if list
                #     - web_context["diag_extra_urls_ui_raw"] if str (newline/space separated)
                #   This is diagnostic-only: we ONLY use this to decide whether to bypass
                #   fastpath when hashes otherwise match, so injected URLs can be admitted
                #   via the rebuild path and become first-class inputs.
                # =====================================================================
                try:
                    if not _evo_extra_urls:
                        _v3 = _evo_wc.get("diag_extra_urls_ui")
                        if isinstance(_v3, (list, tuple)) and _v3:
                            _evo_extra_urls = list(_v3)
                    if not _evo_extra_urls:
                        _raw = _evo_wc.get("diag_extra_urls_ui_raw")
                        if isinstance(_raw, str) and _raw.strip():
                            # Split on newlines first; also allow commas/spaces as separators
                            _parts = []
                            for _line in _raw.splitlines():
                                _line = (_line or "").strip()
                                if not _line:
                                    continue
                                # allow comma-separated within a line
                                for _p in _line.split(","):
                                    _p = (_p or "").strip()
                                    if _p:
                                        _parts.append(_p)
                            if _parts:
                                _evo_extra_urls = _parts
                except Exception:
                    pass

                # =====================================================================
                # PATCH FIX41AFC2 (ADDITIVE): Ensure rebuild/fetch path receives injected URLs
                #   If we recovered injected URLs from Streamlit diagnostic fields (e.g.,
                #   diag_extra_urls_ui_raw) and web_context["extra_urls"] is empty, wire the
                #   recovered list into web_context["extra_urls"] so downstream admission/
                #   fetch/persist logic can see the same universe deterministically.
                #   No effect on no-injection runs.
                # =====================================================================
                try:
                    if isinstance(_evo_wc, dict):
                        _wc_extra = _evo_wc.get("extra_urls")
                        if (not isinstance(_wc_extra, (list, tuple)) or not _wc_extra) and isinstance(_evo_extra_urls, list) and _evo_extra_urls:
                            _evo_wc["extra_urls"] = list(_evo_extra_urls)
                except Exception:
                    pass

                _evo_inj_set = set(_inj_diag_norm_url_list(_evo_extra_urls)) if _evo_extra_urls else set()

                # Baseline universe = urls present in baseline_sources_cache (the same object used for hashing)
                _evo_base_urls = []
                if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                    for _row in baseline_sources_cache:
                        if isinstance(_row, dict) and isinstance(_row.get("source_url"), str) and _row.get("source_url"):
                            _evo_base_urls.append(_row.get("source_url"))
                _evo_base_set = set(_inj_diag_norm_url_list(_evo_base_urls)) if _evo_base_urls else set()

                _evo_inj_delta = sorted(list(_evo_inj_set - _evo_base_set)) if _evo_inj_set else []
                # =====================================================================
                # PATCH FIX41AFC2 (ADDITIVE): Latch bypass decision for later fastpath checks
                #   We persist a simple boolean flag in locals so the downstream FIX31
                #   authoritative reuse check can be disabled without refactoring.
                # =====================================================================
                _fix41af_inj_delta_present = bool(_evo_inj_delta)



                # Only bypass when hashes match and we would otherwise take fastpath
                if _evo_inj_delta and _fix36_reason == "hash_match_and_prev_metrics_present":
                    _fix36_reason = "hash_match_but_injected_urls_present_bypass_fastpath"
                    try:
                        if isinstance(output.get("debug"), dict) and isinstance(output.get("debug", {}).get("fix35"), dict):
                            output["debug"]["fix35"]["fastpath_bypass_injected_delta"] = _evo_inj_delta
                            output["debug"]["fix35"]["fastpath_bypass_injected_delta_count"] = len(_evo_inj_delta)
                    except Exception:
                        pass
            except Exception:
                # Never break evolution on diagnostics / bypass checks
                pass
            if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                # Preserve any earlier reason, but fill if empty
                if not output["debug"]["fix35"].get("fastpath_reason"):
                    output["debug"]["fix35"]["fastpath_reason"] = _fix36_reason
                output["debug"]["fix35"]["fastpath_eligible"] = bool(_fix36_reason == "hash_match_and_prev_metrics_present")
                if _fix36_cur_hash:
                    output["debug"]["fix35"]["source_snapshot_hash_current"] = _fix36_cur_hash
                    output["debug"]["fix35"]["source_snapshot_hash_current_alg"] = "fix37_stable_v2_preferred"
                if isinstance(_prev_hash, str) and _prev_hash:
                    output["debug"]["fix35"]["source_snapshot_hash_previous"] = (_prev_hash_pref if isinstance(locals().get("_prev_hash_pref"), str) and locals().get("_prev_hash_pref") else _prev_hash)
                # PATCH FIX37 (ADD): also expose stable-hash candidate if available
                try:
                    if isinstance(_prev_hash_stable, str) and _prev_hash_stable:
                        output["debug"]["fix35"]["source_snapshot_hash_previous_stable"] = _prev_hash_stable
                except Exception:
                    pass
        except Exception:
            pass
        # ============================================================

        # Only attempt fast-path if we have snapshots AND prior canonical metrics to reuse
        if isinstance(baseline_sources_cache, list) and baseline_sources_cache and isinstance(prev_metrics, dict) and prev_metrics:
            # ============================================================
            # PATCH FIX38 (ADDITIVE): align FIX31 authoritative reuse with FIX37 stable hash
            # - Previously FIX31 compared a v1 fingerprint against prev source_snapshot_hash,
            #   which could mismatch even when data was unchanged.
            # - We now prefer the same stable/v2 hash used by analysis & FIX37 debug.
            # ============================================================
            _cur_hash_v1 = _fix31_snapshot_fingerprint(baseline_sources_cache)
            try:
                _cur_hash = _fix37_snapshot_hash_stable(baseline_sources_cache)
            except Exception:
                _cur_hash = _cur_hash_v1

            # Prefer stable/v2 previous hash if present
            _prev_hash_pref = previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2") or _prev_hash

            # =====================================================================
            # PATCH FIX42 (ADDITIVE): prefer "current" snapshot pool when provided
            #
            # Goal:
            #   When hashes are unequal, rebuild should run on the SAME snapshot pool
            #   that "new analysis" just produced, not on the stale baseline cache
            #   embedded in the previous analysis payload.
            #
            # Where it comes from:
            #   - web_context["current_baseline_sources_cache"]  (preferred)
            #   - web_context["baseline_sources_cache_current"]
            #   - web_context["current_source_results"]         (fallback alias)
            #
            # Policy (additive, fastpath-safe):
            #   - If force_rebuild is asserted by UI/web_context, always use current pool.
            #   - Else, only switch to current pool if its stable hash != previous hash.
            #   - If hashes match, we keep existing behavior (but either pool is equivalent).
            # =====================================================================
            _fix42_used_current_pool = False
            _fix42_reason = ""
            _fix42_cur_pool_hash = None
            try:
                if isinstance(web_context, dict):
                    _cur_pool = (
                        web_context.get("current_baseline_sources_cache")
                        or web_context.get("baseline_sources_cache_current")
                        or web_context.get("current_source_results")
                        or web_context.get("current_source_results_cache")
                        or None
                    )
                    if isinstance(_cur_pool, list) and _cur_pool:
                        _force = bool(
                            web_context.get("force_rebuild")
                            or web_context.get("forced_rebuild")
                            or web_context.get("force_full_rebuild")
                            or web_context.get("force_metric_rebuild")
                        )
                        try:
                            _fix42_cur_pool_hash = _fix37_snapshot_hash_stable(_cur_pool)
                        except Exception:
                            _fix42_cur_pool_hash = None

                        _prev_hash_for_compare = _prev_hash_pref if (isinstance(_prev_hash_pref, str) and _prev_hash_pref) else _prev_hash
                        _hash_mismatch = bool(
                            (isinstance(_prev_hash_for_compare, str) and _prev_hash_for_compare and isinstance(_fix42_cur_pool_hash, str) and _fix42_cur_pool_hash)
                            and (_fix42_cur_pool_hash != _prev_hash_for_compare)
                        )

                        if _force or _hash_mismatch:
                            baseline_sources_cache = _cur_pool
                            snapshot_origin = (snapshot_origin or "analysis_cache") + "|fix42_current_pool"
                            _fix42_used_current_pool = True
                            _fix42_reason = "forced_rebuild" if _force else "hash_mismatch_use_current_pool"
            except Exception:
                pass

            # Attach FIX42 diagnostics (non-breaking)
            try:
                if _fix42_used_current_pool:
                    output["snapshot_origin"] = snapshot_origin
                if isinstance(output.get("debug"), dict):
                    output["debug"].setdefault("fix42", {})
                    output["debug"]["fix42"]["used_current_pool"] = bool(_fix42_used_current_pool)
                    output["debug"]["fix42"]["reason"] = _fix42_reason
                    if isinstance(_fix42_cur_pool_hash, str) and _fix42_cur_pool_hash:
                        output["debug"]["fix42"]["current_pool_hash_stable"] = _fix42_cur_pool_hash
            except Exception:
                pass
            # =====================================================================


            # =====================================================================
            # PATCH FIX41AFC2 (ADDITIVE): Enforce fastpath bypass on injected URL delta
            #   If an injected URL delta exists, we MUST NOT take FIX31 authoritative
            #   reuse (fastpath replay), even if hashes match. We do this additively by
            #   temporarily blanking _prev_hash_pref so the existing hash-match check
            #   remains unchanged for normal runs.
            # =====================================================================
            _fix41af_prev_hash_pref_saved = None
            try:
                if bool(locals().get("_fix41af_inj_delta_present")):
                    _fix41af_prev_hash_pref_saved = _prev_hash_pref
                    _prev_hash_pref = ""
                    try:
                        if isinstance(output.get("debug"), dict) and isinstance(output.get("debug", {}).get("fix35"), dict):
                            output["debug"]["fix35"]["fastpath_reason"] = "hash_match_but_injected_urls_present_bypass_fastpath"
                            output["debug"]["fix35"]["fastpath_eligible"] = False
                    except Exception:
                        pass
            except Exception:
                pass

            if isinstance(_prev_hash_pref, str) and _prev_hash_pref and _cur_hash == _prev_hash_pref:
                _fix31_authoritative_reuse = True
                # =====================================================================
                # PATCH FIX41AFC2 (ADDITIVE): Restore _prev_hash_pref after bypass guard
                # =====================================================================
                try:
                    if _fix41af_prev_hash_pref_saved is not None:
                        _prev_hash_pref = _fix41af_prev_hash_pref_saved
                except Exception:
                    pass

                try:
                    output["rebuild_skipped"] = True
                    output["rebuild_skipped_reason"] = "fix31_sources_unchanged_reuse_prev_metrics"
                    output["source_snapshot_hash_current"] = _cur_hash
                    output["source_snapshot_hash_previous"] = (_prev_hash_cmp if " _prev_hash_cmp" in locals() else _prev_hash)
                    try:
                        if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                            output["debug"]["fix35"]["fastpath_eligible"] = True
                            output["debug"]["fix35"]["fastpath_reason"] = "hash_match_and_prev_metrics_present"
                            output["debug"]["fix35"]["source_snapshot_hash_current"] = _cur_hash
                            output["debug"]["fix35"]["source_snapshot_hash_previous"] = (_prev_hash_pref if isinstance(locals().get("_prev_hash_pref"), str) and locals().get("_prev_hash_pref") else _prev_hash)
                            output["debug"]["fix35"]["current_metrics_origin"] = "reuse_processed_metrics_fastpath"
                    except Exception:
                        pass
                except Exception:
                    pass
    except Exception:
        _fix31_authoritative_reuse = False
        try:
            if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                if not output["debug"]["fix35"].get("fastpath_reason"):
                    output["debug"]["fix35"]["fastpath_reason"] = "fastpath_not_taken_or_exception"
        except Exception:
            pass
    # =====================================================================

    # Build a minimal current metrics dict from snapshots:
    current_metrics = {}
    # ============================================================
    # PATCH FIX31 (ADDITIVE): assign authoritative reused metrics now
    # ============================================================
    try:

        # =====================================================================
        # PATCH FIX41AFC2 (ADDITIVE): Ensure _prev_hash_pref restored if bypass guard blanked it
        #   (covers the case where hash-match condition was false and the inline restore
        #   inside the if-body did not execute).
        # =====================================================================
        try:
            if locals().get("_fix41af_prev_hash_pref_saved") is not None and not _prev_hash_pref:
                _prev_hash_pref = locals().get("_fix41af_prev_hash_pref_saved")
        except Exception:
            pass

        if _fix31_authoritative_reuse and isinstance(prev_metrics, dict) and prev_metrics:
            current_metrics = dict(prev_metrics)
            try:
                output["snapshot_origin"] = (output.get("snapshot_origin") or "") + "|fix31_reuse_prev_metrics"
            except Exception:
                pass
    except Exception:
        pass
    # ============================================================


    # Prefer metric_anchors to rebuild current_metrics (snapshot-gated)
    def _get_metric_anchors(prev: dict) -> dict:
        # ============================================================
        # PATCH FIX31 (ADDITIVE): if authoritative reuse is active, ignore anchors entirely
        # so the reused, schema-gated metrics remain untouched.
        # ============================================================
        try:
            if _fix31_authoritative_reuse:
                return {}
        except Exception:
            pass
        # ============================================================

        if not isinstance(prev, dict):
            return {}
        a = prev.get("metric_anchors")
        if isinstance(a, dict) and a:
            return a
        pr = prev.get("primary_response")
        if isinstance(pr, dict):
            a2 = pr.get("metric_anchors")
            if isinstance(a2, dict) and a2:
                return a2
        res = prev.get("results")
        if isinstance(res, dict):
            a3 = res.get("metric_anchors")
            if isinstance(a3, dict) and a3:
                return a3
        return {}

    def _canonicalize_candidate(n: dict) -> dict:
        try:
            fn = globals().get("canonicalize_numeric_candidate")
            if callable(fn):
                return fn(dict(n))
        except Exception:
            pass
        return dict(n)

    def _build_anchor_to_candidate_map(snapshots: list) -> dict:
        m = {}
        for sr in snapshots or []:
            if not isinstance(sr, dict):
                continue
            for n in (sr.get("extracted_numbers") or []):
                if not isinstance(n, dict):
                    continue
                nn = _canonicalize_candidate(n)
                ah = nn.get("anchor_hash")
                if not ah:
                    continue
                if ah not in m:
                    m[ah] = nn
        return m

    try:
        metric_anchors = _get_metric_anchors(previous_data)
        anchor_to_candidate = _build_anchor_to_candidate_map(baseline_sources_cache)

        if isinstance(metric_anchors, dict) and metric_anchors:
            for ckey, a in metric_anchors.items():
                if not isinstance(a, dict):
                    continue
                ah = a.get("anchor_hash") or a.get("anchor")
                if not ah:
                    continue
                cand = anchor_to_candidate.get(ah)
                if not isinstance(cand, dict):
                    continue

                base = prev_metrics.get(ckey) if isinstance(prev_metrics, dict) else None
                out_row = dict(base) if isinstance(base, dict) else {}
                out_row.update({
                    "canonical_key": ckey,
                    "anchor_hash": ah,
                    "anchor_used": True,
                    "anchor_confidence": a.get("anchor_confidence"),
                    "source_url": cand.get("source_url") or a.get("source_url"),
                    "raw": cand.get("raw"),
                    "value": cand.get("value"),
                    "unit": cand.get("unit"),
                    "value_norm": cand.get("value_norm"),
                    "context_snippet": cand.get("context_snippet") or cand.get("context") or "",
                    "candidate_id": cand.get("candidate_id") or a.get("candidate_id"),
                })
                current_metrics[ckey] = out_row
    except Exception:
        pass

    # Rebuild fallback only if anchors didn't produce metrics
    if not isinstance(current_metrics, dict) or not current_metrics:
        try:
            fn_rebuild = globals().get("rebuild_metrics_from_snapshots_schema_only") or globals().get("rebuild_metrics_from_snapshots")
            if callable(fn_rebuild):
                current_metrics = fn_rebuild(prev_response, baseline_sources_cache, web_context=web_context)
        except Exception:
            current_metrics = {}

    if not isinstance(current_metrics, dict) or not current_metrics:
        output["status"] = "failed"
        output["message"] = "Valid snapshots exist, but metric rebuild returned empty. No re-fetch / no heuristic matching performed."
        output["source_results"] = baseline_sources_cache[:50]
        output["sources_checked"] = len(baseline_sources_cache)
        output["sources_fetched"] = len(baseline_sources_cache)
        output["interpretation"] = "Snapshot-ready but metric rebuild not implemented or returned empty; add/verify rebuild_metrics_from_snapshots* hooks."
        return output


    # =====================================================================
    # PATCH FIX41AFC19 (ADDITIVE): Anchor-first FIX16 rebuild override (schema parity)
    #
    # Why:
    # - Latest evo JSON shows current metrics can be selected from non-matching units
    #   (e.g., unit_sales metric receiving a unitless/negative number; percent metric
    #   receiving a magnitude unit like 'B'). This leads the dashboard "Current" column
    #   to display the wrong metric values even though injection plumbing is progressing.
    # - The new analysis pipeline already relies on FIX16-style hard eligibility gates +
    #   anchor_hash deterministic rebuild. Evolution must use the same selection rules
    #   when fastpath is NOT taken (hash mismatch or injection-triggered rebuild).
    #
    # What:
    # - Right before diffing, attempt an anchor-first rebuild using:
    #     rebuild_metrics_from_snapshots_schema_only_fix16(prev_response, pool, web_context)
    #   when available.
    # - If it returns a non-empty dict, it *overrides* the previously computed
    #   current_metrics (additive override only when rebuild succeeded).
    # - Emits explicit debug fields for traceability.
    #
    # Non-negotiables:
    # - Does NOT alter fastpath logic.
    # - Only activates when fastpath is not taken (i.e., not authoritative reuse).
    # =====================================================================
    try:
        _fix41afc19_applied = False
        _fix41afc19_reason = ""
        _fix41afc19_fn_name = ""
        _fix41afc19_rebuilt_count = 0

        # Only consider override when fastpath is not active
        if not bool(locals().get("_fix31_authoritative_reuse")):
            # Attempt to locate the "current" snapshot pool (post-injection merge/attach)
            _fix41afc19_pool = (
                locals().get("baseline_sources_cache_current")
                or (output.get("baseline_sources_cache_current") if isinstance(output, dict) else None)
                or (output.get("results", {}).get("baseline_sources_cache_current") if isinstance(output, dict) else None)
                or locals().get("baseline_sources_cache")
                or locals().get("baseline_sources_cache_prefetched")
                or None
            )

            # Prefer FIX16 schema-only rebuild if present
            _fix41afc19_fn = globals().get("rebuild_metrics_from_snapshots_schema_only_fix16")
            if callable(_fix41afc19_fn):
                _fix41afc19_fn_name = "rebuild_metrics_from_snapshots_schema_only_fix16"
            else:
                _fix41afc19_fn = None

            if callable(_fix41afc19_fn) and _fix41afc19_pool is not None:
                try:
                    _fix41afc19_rebuilt = _fix41afc19_fn(prev_response, _fix41afc19_pool, web_context=web_context)
                except TypeError:
                    # Backward-compat: older signature without web_context
                    _fix41afc19_rebuilt = _fix41afc19_fn(prev_response, _fix41afc19_pool)

                if isinstance(_fix41afc19_rebuilt, dict) and _fix41afc19_rebuilt:
                    current_metrics = dict(_fix41afc19_rebuilt)
                    _fix41afc19_applied = True
                    _fix41afc19_rebuilt_count = len(current_metrics)
                    _fix41afc19_reason = "override_current_metrics_with_fix16_anchor_rebuild"
    except Exception:
        pass

    # Emit debug for FIX41AFC19 (non-breaking)
    try:
        if isinstance(output.get("debug"), dict):
            output["debug"].setdefault("fix41afc19", {})
            output["debug"]["fix41afc19"]["applied"] = bool(locals().get("_fix41afc19_applied"))
            output["debug"]["fix41afc19"]["reason"] = locals().get("_fix41afc19_reason") or ""
            output["debug"]["fix41afc19"]["fn"] = locals().get("_fix41afc19_fn_name") or ""
            output["debug"]["fix41afc19"]["rebuilt_count"] = int(locals().get("_fix41afc19_rebuilt_count") or 0)
    except Exception:
        pass
    # =====================================================================
# Diff using existing diff helper if present
    metric_changes = []
    try:
        fn_diff = globals().get("diff_metrics_by_name")
        if callable(fn_diff):
            cur_resp_for_diff = {"primary_metrics_canonical": current_metrics}
            metric_changes, unchanged, increased, decreased, found = fn_diff(prev_response, cur_resp_for_diff)
        else:
            metric_changes, unchanged, increased, decreased, found = ([], 0, 0, 0, 0)
    except Exception:
        metric_changes, unchanged, increased, decreased, found = ([], 0, 0, 0, 0)

    output["metric_changes"] = metric_changes or []
    output["summary"]["total_metrics"] = len(output["metric_changes"])
    output["summary"]["metrics_found"] = int(found or 0)
    output["summary"]["metrics_increased"] = int(increased or 0)
    output["summary"]["metrics_decreased"] = int(decreased or 0)
    output["summary"]["metrics_unchanged"] = int(unchanged or 0)

    total = max(1, len(output["metric_changes"]))
    output["stability_score"] = (output["summary"]["metrics_unchanged"] / total) * 100.0

    output["source_results"] = baseline_sources_cache[:50]
    output["sources_checked"] = len(baseline_sources_cache)
    output["sources_fetched"] = len(baseline_sources_cache)

    try:
        total_nums = 0
        for sr in baseline_sources_cache or []:
            if isinstance(sr, dict) and isinstance(sr.get("extracted_numbers"), list):
                total_nums += len(sr.get("extracted_numbers") or [])
        output["numbers_extracted_total"] = int(total_nums)
    except Exception:
        pass

    output["message"] = "Source-anchored evolution completed (snapshot-gated, analysis-aligned)."
    output["interpretation"] = "Evolution used cached source snapshots only; no brute-force candidate harvesting."

    # =====================================================================
    # PATCH FIX35 (ADDITIVE): attach bad-current traces for unit-required metrics
    # - If a diff row shows a year-like integer as current for a unit-required metric,
    #   emit a compact trace: origin, schema unit_family, current fields, and top candidates.
    # =====================================================================
    try:
        if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
            bad_traces = {}
            # Build a flattened candidate pool once (from snapshots only)
            flat = []
            for sr in baseline_sources_cache or []:
                if isinstance(sr, dict):
                    for c in (sr.get("extracted_numbers") or []):
                        if isinstance(c, dict):
                            flat.append(c)

            def _is_yearlike(v):
                try:
                    iv = int(float(v))
                    return 1900 <= iv <= 2100
                except Exception:
                    return False

            def _schema_unit_required(md: dict, ckey: str = "") -> bool:
                uf = ((md or {}).get("unit_family") or (md or {}).get("unit") or "").strip().lower()
                if uf in {"currency", "percent", "rate", "ratio"}:
                    return True
                ck = (ckey or "").lower().strip()
                return ck.endswith("__currency") or ck.endswith("__percent") or ck.endswith("__rate") or ck.endswith("__ratio")

            def _cand_unit_evidence(c: dict) -> bool:
                if not isinstance(c, dict):
                    return False
                if (c.get("unit_tag") or c.get("unit") or c.get("unit_norm") or c.get("unit_raw") or "").strip():
                    return True
                if (c.get("currency") or c.get("currency_symbol") or "").strip():
                    return True
                if c.get("is_percent") or c.get("has_percent"):
                    return True
                if (c.get("base_unit") or "").strip():
                    return True
                if (c.get("unit_family") or "").strip():
                    return True
                if isinstance(c.get("unit_tokens"), list) and c.get("unit_tokens"):
                    return True
                return False

            schema = {}
            try:
                if isinstance(previous_data, dict):
                    pr = previous_data.get("primary_response") if isinstance(previous_data.get("primary_response"), dict) else previous_data
                    schema = (pr.get("metric_schema_frozen") or {}) if isinstance(pr, dict) else {}
            except Exception:
                schema = {}

            for row in output.get("metric_changes") or []:
                try:
                    ckey = row.get("canonical_key") or row.get("canonical") or ""
                    md = schema.get(ckey) if isinstance(schema, dict) else None
                    if not _schema_unit_required(md or {}, ckey):
                        continue

                    cur_val = row.get("current_value_norm")
                    cur_unit = (row.get("cur_unit_cmp") or row.get("current_unit") or "").strip()
                    if cur_val is None:
                        continue
                    if not _is_yearlike(cur_val):
                        continue
                    if cur_unit:
                        continue

                    kws = []
                    if isinstance(md, dict):
                        kws = md.get("keywords") or md.get("keyword_hints") or []
                    kws = [str(k).lower() for k in kws if str(k).strip()]

                    def _hit_score(c):
                        ctx = (c.get("context") or c.get("window") or c.get("context_window") or "").lower()
                        score = 0
                        for k in kws[:25]:
                            if k and k in ctx:
                                score += 1
                        if _cand_unit_evidence(c):
                            score += 5
                        if _is_yearlike(c.get("value_norm")) and not _cand_unit_evidence(c):
                            score -= 5
                        return score

                    top = sorted(flat, key=_hit_score, reverse=True)[:10]
                    bad_traces[ckey or row.get("name") or "unknown_metric"] = {
                        "current_value_norm": cur_val,
                        "cur_unit_cmp": cur_unit,
                        "schema_unit_family": (md or {}).get("unit_family") if isinstance(md, dict) else "",
                        "origin": output["debug"]["fix35"].get("current_metrics_origin", "unknown"),
                        "top_candidates": [
                            {
                                "raw": t.get("raw"),
                                "value_norm": t.get("value_norm"),
                                "unit_tag": t.get("unit_tag"),
                                "unit_family": t.get("unit_family"),
                                "base_unit": t.get("base_unit"),
                                "has_unit_evidence": bool(_cand_unit_evidence(t)),
                                "anchor_hash": t.get("anchor_hash"),
                            }
                            for t in top
                        ],
                    }
                except Exception:
                    continue

            if bad_traces:
                output["debug"]["fix35"]["bad_current_traces"] = bad_traces
                output["debug"]["fix35"]["bad_current_trace_count"] = len(bad_traces)
    except Exception:
        pass

    # =====================================================================
    # PATCH INJ_TRACE_V1_EMIT_EVOLUTION (ADDITIVE): always emit canonical trace
    # - Mirrors to output.results.debug.inj_trace_v1 for a fixed location across modes
    # - Does NOT affect fastpath decisioning
    # =====================================================================
    try:
        _wc_diag = {}
        if isinstance(web_context, dict):
            _wc_diag = web_context.get("diag_injected_urls") or {}
        _hash_inputs = _inj_diag_hash_inputs_from_bsc(baseline_sources_cache)

        # Determine path from existing fix35 origin stamp
        _path = ""
        try:
            origin = ""
            if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                origin = str(output["debug"]["fix35"].get("current_metrics_origin") or "")
            if "fastpath" in origin:
                _path = "fastpath"
            elif "rebuild" in origin:
                _path = "rebuild"
            else:
                _path = "unknown"
        except Exception:
            _path = "unknown"

        # Rebuild "selected" URLs: unique source_url from current metrics (if present)
        _selected = []
        try:
            cm = output.get("current_metrics")
            if isinstance(cm, dict):
                for v in cm.values():
                    if isinstance(v, dict):
                        u = (v.get("source_url") or "").strip()
                        if u:
                            _selected.append(u)
            _selected = sorted(set(_selected))
        except Exception:
            _selected = []

        # For evolution, rebuild_pool is effectively the hash input URL universe available via snapshots
        # =====================================================================
        # PATCH INJ_HASH_V1_EVO (ADDITIVE): compute per-URL hash exclusion reasons in evolution
        # - If injected URLs exist but are not in hash_inputs, we record the most likely reason:
        #     * excluded_by_flag_default_off  (when inclusion switch is OFF)
        #     * missing_from_hash_inputs      (when switch ON but still absent)
        # =====================================================================
        _evo_hash_reasons = {}
        try:
            _evo_persisted = []
            if isinstance(_wc_diag, dict):
                _evo_persisted = _inj_diag_norm_url_list(_wc_diag.get("persisted_norm") or _wc_diag.get("persisted") or [])
            _evo_inj = _inj_diag_norm_url_list(
                (_wc_diag.get("intake_norm") if isinstance(_wc_diag, dict) else []) or
                (_wc_diag.get("ui_norm") if isinstance(_wc_diag, dict) else []) or
                []
            )
            _evo_targets = _evo_persisted or _evo_inj
            _hashset = set(_inj_diag_norm_url_list(_hash_inputs or []))
            _incl = _inj_hash_policy_should_include(_evo_targets)
            for u in _evo_targets:
                if u in _hashset:
                    _evo_hash_reasons[u] = "present_in_hash_inputs"
                else:
                    _evo_hash_reasons[u] = ("excluded_by_policy_disable" if _inj_hash_policy_explicit_disable() else ("excluded_by_legacy_switch_default_off" if not _incl else "missing_from_hash_inputs"))
        except Exception:
            _evo_hash_reasons = {}
        # =====================================================================

        # =====================================================================
        # PATCH INJ_TRACE_V1_EVO_ADMISSION_ALIGN_V1 (ADDITIVE)
        # Goal:
        # - Evolution often bypasses fetch_web_context(), so "admitted" may be unset even
        #   when URLs are actually in the current scrape/hash universe.
        # - This patch makes inj_trace_v1 "admitted_norm" reflect the same practical
        #   universe used for scraping/hashing (without changing any control flow).
        #
        # Policy (diagnostics only):
        # - If diag.admitted is empty but hash_inputs are present, treat hash_inputs as
        #   admitted for trace purposes.
        # - Prefer any explicit FIX24 evo merge set if present (urls_after_merge_norm).
        # =====================================================================
        try:
            if isinstance(_wc_diag, dict):
                _ad = _inj_diag_norm_url_list(_wc_diag.get("admitted") or _wc_diag.get("extra_urls_admitted") or [])
                if not _ad:
                    _pref = []
                    try:
                        _pref = _inj_diag_norm_url_list(_wc_diag.get("urls_after_merge_norm") or [])
                    except Exception:
                        _pref = []
                    if not _pref:
                        _pref = _inj_diag_norm_url_list(_hash_inputs or [])
                    if _pref:
                        _wc_diag["admitted"] = list(_pref)
                        _wc_diag.setdefault("admission_reason", "trace_fallback_to_hash_inputs_or_urls_after_merge")
        except Exception:
            pass
        # =====================================================================

        # =====================================================================
        # PATCH INJ_TRACE_V1_EVO_REBUILD_SELECTED_FALLBACK_V1 (ADDITIVE)
        # Goal:
        # - In fastpath/replay or when current_metrics lacks source_url fields,
        #   rebuild_selected_norm can be empty, creating misleading pool_minus_selected.
        #
        # Diagnostics-only fallback:
        # - If rebuild_selected is empty but rebuild_pool/hash_inputs exists, treat
        #   selected as the full pool for trace purposes.
        # =====================================================================
        try:
            if (not _selected) and _hash_inputs:
                _selected = list(_inj_diag_norm_url_list(_hash_inputs))
                if isinstance(_wc_diag, dict):
                    _wc_diag.setdefault("rebuild_selected_reason", "trace_fallback_to_hash_inputs_no_current_metric_sources")
        except Exception:
            pass
        # =====================================================================


        # =====================================================================
        # PATCH FIX41AFC12 (ADDITIVE): Admission-gate override for injected URLs + post-fetch trace
        #
        # Why:
        # - inj_trace_v1 shows injected URLs at intake but missing from admitted (unknown_rejected_pre_admission).
        # - We must "pin" injection at the admission boundary for evolution (delta-only), and emit a post-fetch trace
        #   because inj_trace_v1 may be emitted before the fetch/persist stage completes.
        #
        # What:
        # - If injected URLs are present (from web_context.extra_urls OR diag ui fields) we force-add them into
        #   _wc_diag["admitted"] so the trace reflects admission override deterministically (delta-only).
        # - Additionally, emit inj_trace_v2_postfetch using best-effort enrichment from scraped_meta / cur_bsc if available.
        #
        # Safety:
        # - Purely additive; never raises; does not modify fastpath rules or hashing.
        # =====================================================================
        try:
            _fx12_wc = web_context if isinstance(web_context, dict) else {}
            _fx12_diag = _wc_diag if isinstance(locals().get("_wc_diag"), dict) else (_fx12_wc.get("diag_injected_urls") if isinstance(_fx12_wc.get("diag_injected_urls"), dict) else {})
            _fx12_inj_raw = []
            try:
                _fx12_inj_raw = list(_fx12_wc.get("extra_urls") or [])
            except Exception:
                _fx12_inj_raw = []
            if not _fx12_inj_raw and isinstance(_fx12_diag, dict):
                try:
                    _fx12_inj_raw = list(_fx12_diag.get("ui_norm") or _fx12_diag.get("intake_norm") or [])
                except Exception:
                    _fx12_inj_raw = []
            _fx12_inj = _inj_diag_norm_url_list(_fx12_inj_raw or [])
            if _fx12_inj and isinstance(_wc_diag, dict):
                _fx12_prev_ad = _inj_diag_norm_url_list(_wc_diag.get("admitted") or [])
                _fx12_forced = sorted(list(set(_fx12_inj) - set(_fx12_prev_ad)))
                if _fx12_forced:
                    _wc_diag["admitted"] = list(_inj_diag_stable_dedupe_order((_fx12_prev_ad or []) + _fx12_forced))
                    _wc_diag.setdefault("forced_admit_reasons", {})
                    if isinstance(_wc_diag.get("forced_admit_reasons"), dict):
                        for _u in _fx12_forced:
                            _wc_diag["forced_admit_reasons"][_u] = "forced_admit_injected_url_override"
                    try:
                        output.setdefault("debug", {})
                        if isinstance(output.get("debug"), dict):
                            output["debug"].setdefault("fix41afc12", {})
                            if isinstance(output["debug"].get("fix41afc12"), dict):
                                output["debug"]["fix41afc12"].update({
                                    "forced_admit_injected_count": int(len(_fx12_forced)),
                                    "forced_admit_injected_urls": list(_fx12_forced),
                                })
                    except Exception:
                        pass
        except Exception:
            pass
        # =====================================================================
        _trace = _inj_trace_v1_build(
            diag_injected_urls=_wc_diag if isinstance(_wc_diag, dict) else {},
            hash_inputs=_hash_inputs,
            stage="evolution",
            path=_path,
            rebuild_pool=_hash_inputs,
            rebuild_selected=_selected,
            hash_exclusion_reasons=_evo_hash_reasons,
        )

        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            output["debug"]["inj_trace_v1"] = _trace

        # Fixed location mirror: results.debug.inj_trace_v1
        output.setdefault("results", {})
        if isinstance(output.get("results"), dict):
            output["results"].setdefault("debug", {})
            if isinstance(output["results"].get("debug"), dict):
                output["results"]["debug"]["inj_trace_v1"] = _trace

                # =====================================================================
                # PATCH FIX41AFC12_POSTFETCH (ADDITIVE): inj_trace_v2_postfetch
                #
                # Emit a second trace after best-effort enrichment from scraped_meta / baseline cache so that
                # attempted/persisted deltas reflect the true post-fetch state (inj_trace_v1 may be earlier).
                # =====================================================================
                try:
                    _fx12_diag2 = dict(_wc_diag) if isinstance(_wc_diag, dict) else {}
                    try:
                        _sm = locals().get("scraped_meta")
                        if isinstance(_sm, dict):
                            _fx12_diag2 = _inj_trace_v1_enrich_diag_from_scraped_meta(_fx12_diag2, _sm, (_inj_extra_urls or []))
                    except Exception:
                        pass
                    try:
                        _cb = locals().get("cur_bsc") or locals().get("baseline_sources_cache") or locals().get("baseline_sources_cache_current")
                        if isinstance(_cb, list):
                            _fx12_diag2 = _inj_trace_v1_enrich_diag_from_bsc(_fx12_diag2, _cb)
                    except Exception:
                        pass
                    _trace2 = _inj_trace_v1_build(
                        diag_injected_urls=_fx12_diag2 if isinstance(_fx12_diag2, dict) else {},
                        hash_inputs=_hash_inputs,
                        stage="evolution",
                        path=str(_path or "evolution") + "_postfetch",
                        rebuild_pool=_hash_inputs,
                        rebuild_selected=_selected,
                        hash_exclusion_reasons=_evo_hash_reasons,
                    )
                    output["debug"]["inj_trace_v2_postfetch"] = _trace2
                    if isinstance(output.get("results"), dict) and isinstance(output["results"].get("debug"), dict):
                        output["results"]["debug"]["inj_trace_v2_postfetch"] = _trace2

                    # =====================================================================
                    # PATCH DIAG17_EMIT (ADDITIVE): artifact-derived attempted/persisted/failed (injection)
                    # - Derives from output.results.source_results + baseline_sources_cache (post-run state)
                    # - Diagnostics only; does not affect behavior
                    # =====================================================================
                    try:
                        _diag17_extra = []
                        try:
                            _diag17_extra = list((_fx12_wc.get("extra_urls") if isinstance(locals().get("_fx12_wc"), dict) else []) or (_inj_extra_urls or []) or [])
                        except Exception:
                            _diag17_extra = list((_inj_extra_urls or []) or [])
                        _diag17_sr = None
                        try:
                            _diag17_sr = (output.get("results", {}) or {}).get("source_results")
                        except Exception:
                            _diag17_sr = None
                        _diag17_bsc = None
                        try:
                            _diag17_bsc = locals().get("cur_bsc") or locals().get("baseline_sources_cache") or baseline_sources_cache
                        except Exception:
                            _diag17_bsc = baseline_sources_cache
                        _diag17 = _inj_trace_diag17_from_artifacts(_diag17_extra, _diag17_sr, _diag17_bsc)
                        output.setdefault("debug", {})
                        if isinstance(output.get("debug"), dict):
                            output["debug"]["inj_trace_diag17"] = _diag17
                        if isinstance(output.get("results"), dict):
                            output["results"].setdefault("debug", {})
                            if isinstance(output["results"].get("debug"), dict):
                                output["results"]["debug"]["inj_trace_diag17"] = _diag17
                    except Exception:
                        pass
                    # =====================================================================
                except Exception:
                    pass
                # =====================================================================
    except Exception:
        pass
    # =====================================================================

    return output
def rebuild_metrics_from_snapshots_schema_only(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    Minimal deterministic rebuild:
      - Uses ONLY: baseline_sources_cache + frozen schema (or derives schema from canonical metrics)
      - No re-fetch
      - No heuristic name fallback outside schema fields
      - Deterministic selection + ordering

    Returns a dict shaped like primary_metrics_canonical (by canonical_key).
    """
    import re

    if not isinstance(prev_response, dict):
        return {}

    # 1) Obtain frozen schema (required contract for schema-driven rebuild)
    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
    )

    # If schema is missing but canonical metrics exist, derive schema deterministically
    if not metric_schema:
        try:
            canon = (
                prev_response.get("primary_metrics_canonical")
                or (prev_response.get("primary_response") or {}).get("primary_metrics_canonical")
                or (prev_response.get("results") or {}).get("primary_metrics_canonical")
            )
            fn_freeze = globals().get("freeze_metric_schema")
            if canon and callable(fn_freeze):
                metric_schema = fn_freeze(canon)
        except Exception:
            metric_schema = None

    if not isinstance(metric_schema, dict) or not metric_schema:
        return {}

    # 2) Flatten candidates (must come from snapshots/cache, no re-fetch)
    candidates = []
    if isinstance(baseline_sources_cache, dict) and isinstance(baseline_sources_cache.get("snapshots"), list):
        source_entries = baseline_sources_cache.get("snapshots", [])
    elif isinstance(baseline_sources_cache, list):
        source_entries = baseline_sources_cache
    else:
        source_entries = []

    # Candidate normalization helpers
    def _norm(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def _unit_family_guess(unit: str) -> str:
        u = (unit or "").strip().lower()
        if u in ("%", "percent", "percentage"):
            return "percent"
        if any(x in u for x in ("usd", "$", "eur", "gbp", "jpy", "cny", "aud", "sgd")):
            return "currency"
        if any(x in u for x in ("unit", "units", "vehicle", "vehicles", "car", "cars", "kwh", "mwh", "gwh", "twh")):
            return "quantity"
        return ""

    # Prefer already-extracted numbers in snapshots; otherwise optionally extract from stored text if present.
    fn_extract = globals().get("extract_numbers_with_context")

    for s in source_entries:
        if not isinstance(s, dict):
            continue
        url = s.get("source_url") or s.get("url") or ""
        xs = s.get("extracted_numbers")
        if isinstance(xs, list) and xs:
            for c in xs:
                if isinstance(c, dict):
                    c2 = dict(c)
                    c2.setdefault("source_url", url)
                    candidates.append(c2)
            continue

        # Optional: if snapshot stores text, we can extract deterministically (no re-fetch).
        txt = s.get("text") or s.get("raw_text") or s.get("content_text") or ""
        if txt and callable(fn_extract):
            try:
                xs2 = fn_extract(txt, source_url=url)
                if isinstance(xs2, list):
                    for c in xs2:
                        if isinstance(c, dict):
                            c2 = dict(c)
                            c2.setdefault("source_url", url)
                            candidates.append(c2)
            except Exception:
                pass

    # Drop junk + enforce deterministic ordering
    def _cand_sort_key(c: dict):
        return (
            str(c.get("anchor_hash") or ""),
            str(c.get("source_url") or ""),
            int(c.get("start_idx") or 0),
            str(c.get("raw") or ""),
            str(c.get("unit") or ""),
            float(c.get("value_norm") or 0.0),
        )

    filtered = []
    for c in candidates:
        if not isinstance(c, dict):
            continue
        if c.get("is_junk") is True:
            continue
        filtered.append(c)

    filtered.sort(key=_cand_sort_key)

    # =====================================================================
    # PATCH FIX27 (ADDITIVE): Strict schema-only eligibility gate to prevent
    # bare-year tokens (e.g., "2024") from winning typed metrics (currency,
    # percent, unit_sales) due to keyword overlap.
    #
    # Key rules (token-level evidence):
    #   - If raw looks like a bare year (1900–2100) AND token has no unit
    #     evidence, reject for non-year metrics.
    #   - For currency metrics, require explicit currency evidence on the token.
    #   - For percent metrics, require explicit percent evidence on the token.
    #   - For unit/unit_sales metrics, require some unit evidence on the token.
    # NOTE: This is enforced inside schema-only selection, BEFORE scoring.
    # =====================================================================

    def _fix27_is_bare_year_token(raw_token: str, value_norm):
        try:
            s = (raw_token or "").strip()
            # Allow formats like "2024" or "2024.0"
            if re.fullmatch(r"\d{4}(?:\.0+)?", s):
                y = int(float(s))
                return 1900 <= y <= 2100
        except Exception:
            pass
        # fallback: numeric year-like
        try:
            if value_norm is not None:
                y = int(float(value_norm))
                if abs(float(value_norm) - float(y)) < 1e-9:
                    return 1900 <= y <= 2100
        except Exception:
            pass
        return False

    def _fix27_expected_kind(canonical_key: str, sch: dict) -> str:
        ck = (canonical_key or "").lower()
        dim = ((sch or {}).get("dimension") or (sch or {}).get("unit_family") or "").lower().strip()
        if ck.endswith("__percent") or "percent" in dim:
            return "percent"
        if ck.endswith("__currency") or "currency" in dim or "money" in dim:
            return "currency"
        if ck.endswith("__unit_sales") or "unit" in ck or "sales" in ck:
            return "unit"
        if "year" in dim or ck.endswith("__year"):
            return "year"
        return "other"

    def _fix27_has_currency_evidence(c: dict) -> bool:
        raw = (c.get("raw") or "")
        u = (c.get("unit") or "")
        bu = (c.get("base_unit") or "")
        ut = (c.get("unit_tag") or "")
        blob = f"{raw} {u} {bu} {ut}".lower()
        # Token-level currency signals
        if "$" in raw:
            return True
        for t in ("usd", "eur", "gbp", "sgd", "aud", "cad", "chf", "jpy", "cny", "rmb", "hkd", "inr", "krw"):
            if re.search(rf"\b{t}\b", blob):
                return True
        return False

    def _fix27_has_percent_evidence(c: dict) -> bool:
        raw = (c.get("raw") or "")
        u = (c.get("unit") or "")
        bu = (c.get("base_unit") or "")
        ut = (c.get("unit_tag") or "")
        blob = f"{raw} {u} {bu} {ut}".lower()
        return ("%" in raw) or ("%" in blob) or ("percent" in blob)

    def _fix27_has_any_unit_evidence(c: dict) -> bool:
        # Any unit evidence captured on the token (not schema defaults)
        return bool((c.get("unit") or "").strip() or (c.get("base_unit") or "").strip() or (c.get("unit_tag") or "").strip())
    # 3) Schema-driven selection
    rebuilt = {}
    for canonical_key, sch in metric_schema.items():
        if not isinstance(sch, dict):
            continue

        # Schema tokens/keywords
        name = sch.get("name") or canonical_key
        keywords = sch.get("keywords") or sch.get("keyword_hints") or []
        if isinstance(keywords, str):
            keywords = [keywords]
        kw_norm = [_norm(k) for k in keywords if k]

        # Expected family/dimension
        expected_dim = (sch.get("dimension") or sch.get("unit_family") or "").lower().strip()
        expected_kind = _fix27_expected_kind(canonical_key, sch)  # PATCH FIX27

        best = None
        best_score = None

        for c in filtered:
            ctx = _norm(c.get("context") or c.get("context_window") or "")
            raw = _norm(c.get("raw") or "")
            unit = c.get("unit") or ""
            fam = _unit_family_guess(unit)
            if expected_dim and fam and expected_dim not in (fam,):
                # strict family check when we can infer a family
                continue

            score = 0
            # keyword match only from schema-provided keywords
            for k in kw_norm:
                if k and (k in ctx or k in raw):
                    score += 10

            # Prefer candidates that have an anchor_hash (stability)
            if c.get("anchor_hash"):
                score += 1

            # Deterministic tie-breakers: earlier in list wins if equal score
            if best is None or score > best_score:
                best = c
                best_score = score

        if best is None or (best_score is not None and best_score <= 0):
            # No schema-consistent evidence found -> omit (or could mark proxy; keep minimal here)
            continue

        rebuilt[canonical_key] = {
            "canonical_key": canonical_key,
            "name": name,
            "value": best.get("value"),
            "unit": best.get("unit") or "",
            "value_norm": best.get("value_norm"),
            "source_url": best.get("source_url") or "",
            "anchor_hash": best.get("anchor_hash") or "",
            "evidence": [{
                "source_url": best.get("source_url") or "",
                "raw": best.get("raw") or "",
                "context_snippet": (best.get("context") or best.get("context_window") or "")[:400],
                "anchor_hash": best.get("anchor_hash") or "",
                "method": "schema_only_rebuild",
            }],
        }

    return rebuilt
# =================== END PATCH RMS_CORE1 (ADDITIVE) ===================





def extract_context_keywords(metric_name: str) -> List[str]:
    """
    General-purpose keyword extraction for matching metric names to page contexts.

    Goals:
    - Work for ANY topic (not tourism-specific)
    - Keep deterministic behavior
    - Extract years/quarters, key financial/stat terms, and meaningful tokens
    """
    if not metric_name:
        return []

    name = str(metric_name)
    n = name.lower()

    keywords: List[str] = []

    # Years (e.g., 2019, 2024)
    years = re.findall(r"\b(19\d{2}|20\d{2})\b", name)
    keywords.extend(years)

    # Quarters / time buckets
    q = re.findall(r"\bq[1-4]\b", n)
    keywords.extend([x.upper() for x in q])

    # Common metric concepts (broad, cross-industry)
    concept_phrases = [
        "market size", "revenue", "sales", "turnover", "profit", "operating profit",
        "ebit", "ebitda", "net income", "gross margin", "margin",
        "growth", "yoy", "cagr", "share", "penetration",
        "forecast", "projected", "projection", "estimate", "expected",
        "actual", "baseline", "target",
        "volume", "units", "shipments", "users", "subscribers", "visitors",
        "price", "asp", "arpu", "aov",
        "inflation", "gdp", "unemployment", "interest rate"
    ]
    for p in concept_phrases:
        if p in n:
            keywords.append(p)

    # Units / scales that help matching
    unit_hints = ["trillion", "billion", "million", "thousand", "%", "percent"]
    for u in unit_hints:
        if u in n:
            keywords.append(u)

    # Tokenize remaining meaningful words
    tokens = re.findall(r"[a-z0-9]+", n)
    stop = {
        "the","and","or","of","in","to","for","by","from","with","on","at","as",
        "total","overall","average","avg","number","rate","value","amount",
        "annual","year","years","monthly","month","daily","day","quarter","quarters"
    }
    for t in tokens:
        if t in stop:
            continue
        if len(t) <= 2:
            continue
        keywords.append(t)

    # De-dup, keep stable ordering
    seen = set()
    out = []
    for k in keywords:
        if k and k not in seen:
            seen.add(k)
            out.append(k)

    return out[:30]

def extract_numbers_with_context(text, source_url: str = "", max_results: int = 350):
    """
    Extract numeric candidates with context windows (analysis-aligned, hardened).

    Fixes / tightening:
    - ALWAYS returns a list (never None)  ✅ critical for snapshots & evolution
    - Strips HTML tags/scripts/styles if HTML-like
    - Nav/chrome/junk rejection (analytics, cookie banners, menus, footers, etc.)
    - Suppress year-only candidates (e.g., "2024") unless clearly a metric
    - Suppress ID-like long integers, phone-like patterns, DOI/ISBN-like contexts
    - Captures currency + scale + percent + common magnitude suffixes
    - Adds anchor_hash for stable matching
    """
    import re
    import hashlib

    if not text or not str(text).strip():
        return []

    raw = str(text)

    # ---------- helpers ----------
    def _sha1(s: str) -> str:
        return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()

    def _normalize_unit(u: str) -> str:
        u = (u or "").strip()
        if not u:
            return ""
        ul = u.lower().replace(" ", "")

        # Energy units (must come before magnitude)
        if "twh" in ul:
            return "TWh"
        if "gwh" in ul:
            return "GWh"
        if "mwh" in ul:
            return "MWh"
        if "kwh" in ul:
            return "kWh"
        if ul == "wh":
            return "Wh"

        # Magnitudes (case-insensitive; fix: accept single-letter suffixes)
        if ul in ("bn", "billion", "b"):
            return "B"
        if ul in ("mn", "mio", "million", "m"):
            return "M"
        if ul in ("k", "thousand", "000"):
            return "K"
        if ul in ("trillion", "tn", "t"):
            return "T"

        if ul in ("pct", "percent", "%"):
            return "%"

        return u

    def _looks_html(s: str) -> bool:
        sl = s.lower()
        return ("<html" in sl) or ("<div" in sl) or ("<p" in sl) or ("<script" in sl) or ("</" in sl)

    def _html_to_text(s: str) -> str:
        # Prefer BeautifulSoup if available
        try:
            from bs4 import BeautifulSoup  # type: ignore
            soup = BeautifulSoup(s, "html.parser")
            for tag in soup(["script", "style", "noscript", "svg", "canvas", "iframe", "header", "footer", "nav", "form"]):
                try:
                    tag.decompose()
                except Exception:
                    pass
            txt = soup.get_text(separator=" ", strip=True)
            txt = re.sub(r"\s+", " ", txt).strip()
            return txt
        except Exception:
            # fallback: cheap strip
            s2 = re.sub(r"(?is)<(script|style|noscript).*?>.*?</\1>", " ", s)
            s2 = re.sub(r"(?is)<[^>]+>", " ", s2)
            s2 = re.sub(r"\s+", " ", s2).strip()
            return s2

    def _is_phone_like(ctx: str, rawnum: str) -> bool:
        # strict phone pattern or phone keywords nearby
        if re.search(r"\b\d{3}-\d{3}-\d{4}\b", rawnum):
            return True
        c = (ctx or "").lower()
        if any(k in c for k in ["call", "phone", "tel:", "telephone", "contact us", "whatsapp"]):
            if re.search(r"\b\d{7,}\b", rawnum):
                return True
        return False

    def _is_id_like(val_str: str, ctx: str) -> bool:
        # very long digit strings typically IDs, unless explicitly monetary with symbols
        digits = re.sub(r"\D", "", val_str or "")
        if len(digits) >= 13:
            c = (ctx or "").lower()
            if any(k in c for k in ["isbn", "doi", "issn", "arxiv", "repec", "id:", "order", "invoice", "reference"]):
                return True
            # generic ID-like (too many digits)
            return True
        return False

    def _chrome_junk(ctx: str) -> bool:
        c = (ctx or "").lower()
        # common site chrome / analytics / cookie / nav junk
        bad = [
            "googleanalyticsobject", "gtag(", "googletagmanager", "analytics", "doubleclick",
            "cookie", "consent", "privacy", "terms", "copyright", "all rights reserved",
            "subscribe", "newsletter", "sign in", "login", "menu", "search", "breadcrumb",
            "share this", "follow us", "social media", "footer", "header", "nav", "sitemap"
        ]
        if any(b in c for b in bad):
            return True
        # css/js-like
        if any(b in c for b in ["function(", "var ", "const ", "let ", "webpack", "sourcemappingurl", ".css", "{", "};"]):
            return True
        # low alpha ratio
        if len(c) > 80:
            letters = sum(ch.isalpha() for ch in c)
            if letters / max(1, len(c)) < 0.18:
                return True
        return False

    def _year_only_suppression(num: float, unit: str, rawnum: str, ctx: str) -> bool:
        # suppress standalone 4-digit years like 2024 with no unit/currency
        if unit:
            return False
        s = (rawnum or "").strip()
        if re.fullmatch(r"\d{4}", s):
            year = int(s)
            if 1900 <= year <= 2099:
                c = (ctx or "").lower()
                allow_kw = ["cagr", "growth", "inflation", "gdp", "revenue", "market", "sales", "shipments", "capacity"]
                if not any(k in c for k in allow_kw):
                    return True
        return False

    # -------------------------------------------------------------------------
    # ADDITIVE (Patch A1): fix common "split year" artifact (e.g., "202 5" -> "2025")
    # Do this AFTER HTML->text and BEFORE regex extraction.
    # -------------------------------------------------------------------------

    # ---------- normalize to visible text ----------
    if _looks_html(raw):
        raw = _html_to_text(raw)

    # cap huge pages
    raw = raw[:250_000]

    # ---- ADDITIVE: fix common "split year" artifact (e.g., "202 5" -> "2025") ----
    raw = re.sub(r"\b((?:19|20)\d)\s+(\d)\b", r"\1\2", raw)
    # -----------------------------------------------------------------------------

    # -------------------------------------------------------------------------
    # PATCH A3 (ADDITIVE): year-range detector (tag-only, does NOT drop candidates)
    # -------------------------------------------------------------------------
    def _is_year_range_context(ctx: str) -> bool:
        return bool(re.search(r"\b(19|20)\d{2}\s*(?:-|–|—|to)\s*(19|20)\d{2}\b", ctx or "", flags=re.I))

    # -------------------------------------------------------------------------
    # ADDITIVE (Patch A2): non-destructive junk tagger
    # - We DO NOT filter here; we tag and downstream excludes by default.
    # -------------------------------------------------------------------------
    def _junk_tag(value: float, unit: str, raw_disp: str, ctx: str):
        """
        Non-destructive junk classifier.
        Returns (is_junk: bool, reason: str).
        """
        c = (ctx or "").lower()
        u = (unit or "").strip()

        # =========================
        # PATCH A3 (TAG ONLY): year-range endpoints are usually timeline metadata
        # =========================
        try:
            iv = int(float(value))
            if u == "" and 1900 <= iv <= 2099 and _is_year_range_context(ctx):
                return True, "year_range"
        except Exception:
            pass

        nav_hits = [
            "skip to content", "menu", "search", "login", "sign in", "sign up",
            "subscribe", "newsletter", "cookie", "privacy", "terms", "copyright",
            "all rights reserved", "back to top", "next", "previous", "page ",
            "home", "about", "contact", "sitemap", "breadcrumb"
        ]
        if any(h in c for h in nav_hits):
            try:
                if u == "" and abs(float(value)) <= 20:
                    return True, "nav_small_int"
            except Exception:
                pass

        if u == "":
            try:
                if abs(float(value)) <= 12:
                    if any(h in c for h in ["•", "–", "step", "chapter", "section", "item", "no."]):
                        return True, "enumeration_small_int"
            except Exception:
                pass

        if u == "":
            try:
                iv = int(abs(float(value)))
                if 190 <= iv <= 209:
                    if any(x in (raw_disp or "") for x in ["202", "203", "204", "205", "206", "207", "208", "209"]):
                        return True, "year_fragment_3digit"
            except Exception:
                pass

        # =====================================================================
        # PATCH FIX41AFC23A (ADDITIVE): hard junk for phone/contact numbers (prevents "+1-888-600-6441" tail capture)
        # - Tag-only (non-destructive): marks candidate as junk, downstream excludes by default.
        # - Handles full phone patterns AND "tail-4" captures when context shows a phone number.
        # =====================================================================
        try:
            _ctx_raw = (ctx or "")
            _ctx_l = _ctx_raw.lower()
            _raw_s = (raw_disp or "").strip()
            _digits = re.sub(r"\D", "", _raw_s)
            _contact_kw = any(k in _ctx_l for k in [
                "contact", "contact us", "tel", "telephone", "phone", "call", "hotline", "fax", "whatsapp",
                "press@", "media@", "investor relations"
            ])
            _phone_pat = (
                re.search(r"(?:\+?\d{1,3}[\s\-\.]*)?(?:\(?\d{2,4}\)?[\s\-\.]*)\d{3}[\s\-\.]*\d{4}\b", _raw_s)
                or re.search(r"(?:\+?\d{1,3}[\s\-\.]*)?(?:\(?\d{2,4}\)?[\s\-\.]*)\d{3}[\s\-\.]*\d{4}\b", _ctx_raw)
            )
            if _phone_pat:
                return True, "phone_number"
            # Tail-4 capture: candidate is 4 digits but context includes full phone number
            if u == "" and len(_digits) == 4:
                if _contact_kw and re.search(r"\b\d{3}[\s\-\.]\d{3}[\s\-\.]\d{4}\b", _ctx_raw):
                    return True, "phone_tail4"
            # Toll-free hint + contact context
            if u == "" and _contact_kw:
                if re.search(r"\b(?:800|888|877|866|855|844|833)\b", _ctx_raw) and len(_digits) in (4, 7, 10, 11):
                    return True, "phone_contact_context"
        except Exception:
            pass
        # =====================================================================
        # END PATCH FIX41AFC23A
        # =====================================================================
        return False, ""

    # -------------------------------------------------------------------------
    # PATCH M1 (ADDITIVE): semantic classifier for associations like "share" vs "units"
    # NOTE: moved OUTSIDE the loop for determinism + speed (no behavioral change).
    # Also emits a "measure_assoc" label that downstream can display easily.
    # -------------------------------------------------------------------------
    def _classify_measure(unit_tag: str, ctx: str):
        """
        Returns (measure_kind, measure_assoc):
          - measure_kind: stable internal tag (share_pct / growth_pct / count_units / money / etc.)
          - measure_assoc: human-meaning label ("share", "growth", "units", "money", "energy", etc.)
        """
        c = (ctx or "").lower()
        ut = (unit_tag or "").strip()

        if ut == "%":
            if any(k in c for k in ["market share", "share of", "share", "penetration", "portion", "contribution"]):
                return "share_pct", "share"
            if any(k in c for k in ["growth", "cagr", "increase", "decrease", "yoy", "mom", "qoq", "rate"]):
                return "growth_pct", "growth"
            return "percent_other", "percent"

        if ut in ("K", "M", "B", "T", ""):
            if any(k in c for k in ["units", "unit", "vehicles", "cars", "sold", "sales volume", "shipments", "deliveries", "registrations"]):
                return "count_units", "units"
            if any(k in c for k in ["revenue", "sales ($", "usd", "$", "market size", "valuation", "turnover"]):
                return "money", "money"
            return "magnitude_other", "magnitude"

        if ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
            return "energy", "energy"

        return "other", "other"
    # -------------------------------------------------------------------------

    # ---------- extraction pattern ----------
    # =========================
    # PATCH N1 (ADDITIVE, BUGFIX): currency tokens
    # - Fix US$ being parsed as S$ by matching US\$ first.
    # - Also accept "US$" as a single token (case-insensitive).
    # =========================
    pat = re.compile(
        r"(US\$|US\$(?!\w)|S\$|\$|USD|SGD|EUR|€|GBP|£)?\s*"
        # =========================
        # PATCH N2 (ADDITIVE, BUGFIX): avoid capturing negative year from year-range
        # - We'll still allow negatives generally, but we'll tag the special "2025-2030" case below.
        # (No behavior change for real negatives like -1.2% etc.)
        # =========================
        r"(-?\d{1,3}(?:,\d{3})*(?:\.\d+)?|-?\d+(?:\.\d+)?)(?!\d)\s*"
        # =========================
        # PATCH N3 (ADDITIVE, BUGFIX): capture 'tn' magnitude explicitly
        # - Keep your A5 safeguard: single-letter magnitudes only match if NOT followed by a letter.
        # =========================
        r"(TWh|GWh|MWh|kWh|Wh|tn|(?:T|B|M|K)(?![A-Za-z])|trillion|billion|million|bn|mn|%|percent)?",
        flags=re.I
    )

    out = []
    for m in pat.finditer(raw):
        cur = (m.group(1) or "").strip()
        num_s = (m.group(2) or "").strip()
        unit_s = (m.group(3) or "").strip()

        if not num_s:
            continue

        start = max(0, m.start() - 160)
        end = min(len(raw), m.end() + 160)
        ctx = raw[start:end].replace("\n", " ")
        ctx = re.sub(r"\s+", " ", ctx).strip()
        ctx_store = ctx[:240]

        # numeric parse
        try:
            val = float(num_s.replace(",", ""))
        except Exception:
            continue

        # normalize unit
        unit = _normalize_unit(unit_s)

        raw_disp = f"{cur} {num_s}{unit_s}".strip()
        raw_num_only = (cur + num_s).strip()

        if _chrome_junk(ctx_store):
            continue
        if _is_phone_like(ctx_store, raw_disp):
            continue
        if _is_id_like(raw_disp, ctx_store):
            continue
        if _year_only_suppression(val, unit, num_s, ctx_store):
            continue

        # =========================
        # PATCH N2b (ADDITIVE, BUGFIX): tag the "negative year from range" case as junk
        # Example: "CAGR 2025-2030" producing "-2030"
        # - Do NOT drop here (keep non-destructive policy); just tag.
        # =========================
        neg_year_from_range = False
        try:
            if num_s.startswith("-"):
                iv = int(abs(float(val)))
                if 1900 <= iv <= 2099:
                    # look immediately behind the match for a digit (the "2025" in "2025-2030")
                    if m.start() > 0 and raw[m.start() - 1].isdigit():
                        neg_year_from_range = True
        except Exception:
            neg_year_from_range = False
        # =========================

        anchor_hash = _sha1(f"{source_url}|{raw_disp}|{ctx_store}")
        is_junk, junk_reason = _junk_tag(val, unit, raw_disp, ctx_store)

        # =========================
        # PATCH N2c (ADDITIVE): override junk tagging reason when we confidently detect this bug
        # =========================
        if neg_year_from_range:
            is_junk = True
            junk_reason = "year_range_negative_endpoint"
        # =========================


            # =================================================================
            # PATCH YEAR_ONLY_V2 (ADDITIVE): suppress standalone years as datapoints
            # Why:
            # - Years (e.g., 2025) frequently appear in headings/ranges and should not
            #   compete with real metric values (currency, %, volumes) in evolution.
            # - We keep years only if there is strong metric context nearby.
            # Rules:
            # - If value is an integer-like 4-digit year in [1900..2100],
            #   unit is empty, and context lacks currency/%/magnitude cues => mark junk.
            # =================================================================
            try:
                if (not is_junk) and (not str(unit or "").strip()):
                    _v_int = None
                    try:
                        _v_int = int(float(val)) if val is not None else None
                    except Exception:
                        _v_int = None

                    if _v_int is not None and 1900 <= _v_int <= 2100:
                        _ctx = str(ctx_store or "")
                        _ctx_l = _ctx.lower()

                        # Strong numeric-metric cues that should keep the candidate
                        _keep_cue = False
                        try:
                            if re.search(r"[$€£¥]|\b(usd|sgd|eur|gbp|jpy|aud|cad|chf)\b", _ctx_l):
                                _keep_cue = True
                            elif re.search(r"%|\b(cagr|yoy|growth|increase|decrease)\b", _ctx_l):
                                _keep_cue = True
                            elif re.search(r"\b(million|billion|trillion|mn|bn|m|b)\b", _ctx_l):
                                _keep_cue = True
                            elif re.search(r"\b(kwh|mwh|gwh|twh|mw|gw|tw|kg|tonnes?|mt|kt)\b", _ctx_l):
                                _keep_cue = True
                            elif re.search(r"\b(revenue|sales|market\s*size|valuation|profit|earnings)\b", _ctx_l):
                                _keep_cue = True
                        except Exception:
                            _keep_cue = False

                        if not _keep_cue:
                            is_junk = True
                            junk_reason = "year_only"
            except Exception:
                pass
            # =================================================================
# semantic association tags
        measure_kind, measure_assoc = _classify_measure(unit, ctx_store)

        out.append({
            "value": val,
            "unit": unit,
            "raw": raw_disp,
            "source_url": source_url,
            "context": ctx_store,
            "context_snippet": ctx_store,
            "anchor_hash": anchor_hash,

            "is_junk": bool(is_junk),
            "junk_reason": junk_reason,
            "start_idx": int(m.start()),
            "end_idx": int(m.end()),

            "measure_kind": measure_kind,
            "measure_assoc": measure_assoc,
        })

        if len(out) >= int(max_results or 350):
            break
# ============================================================
    # PATCH FIX41AFC36A START — extraction hygiene: bare-year junk + unit_family backfill
    # ============================================================
    def _fix41afc36_infer_unit_family(_cand: dict) -> str:
        try:
            uf = (_cand.get("unit_family") or "").strip().lower()
            if uf:
                return uf
            raw = str(_cand.get("raw") or _cand.get("raw_disp") or "").lower()
            unit = str(_cand.get("unit") or "").lower()
            utag = str(_cand.get("unit_tag") or "").lower()
            base = str(_cand.get("base_unit") or "").lower()
            ctx = str(_cand.get("context") or "").lower()
            blob = " ".join([raw, unit, utag, base, ctx])

            if "%" in blob or "percent" in blob or "percentage" in blob:
                return "percent"
            if any(t in blob for t in ["$", "usd", "us$", "eur", "sgd", "gbp", "aud", "cad", "jpy", "cny", "rmb", "inr", "krw", "chf"]):
                return "currency"
            if any(t in blob for t in ["million", "billion", "trillion", " mn", " bn", " tn"]) or any(t in (unit + " " + utag + " " + base) for t in ["k", "m", "b", "t"]):
                return "magnitude"
            if any(t in blob for t in ["kwh", "mwh", "gwh", "twh", "wh", "mw", "gw", "tw"]):
                return "energy"
            return ""
        except Exception:
            return (_cand.get("unit_family") or "")

    def _fix41afc36_mark_bare_year_junk(_cand: dict) -> None:
        try:
            unit_ev = any(str(_cand.get(k) or "").strip() for k in ["unit", "unit_tag", "base_unit"])
            v = _cand.get("value_norm", None)
            if v is None:
                v = _cand.get("value", None)
            if isinstance(v, str) and v.strip():
                try:
                    v = float(v.strip())
                except Exception:
                    v = None
            if not unit_ev and v is not None:
                iv = int(round(float(v)))
                if 1900 <= iv <= 2100 and abs(float(v) - iv) < 1e-9:
                    _cand["is_junk"] = True
                    _cand["junk_reason"] = (_cand.get("junk_reason") or "bare_year_fix41afc36")
        except Exception:
            pass

    try:
        for _c in (out or []):
            if isinstance(_c, dict):
                _c["unit_family"] = _fix41afc36_infer_unit_family(_c)

# ============================================================
                # ============================================================
                # PATCH FIX41AFC42B START — explicit unit token precedence lock (extraction postpass)
                # ============================================================
                try:
                    _fix41afc42_apply_unit_family_lock(_c)
                except Exception:
                    pass
                # ============================================================
                # PATCH FIX41AFC42B END
                # ============================================================

                # ============================================================
                # PATCH FIX41AFC47B START — clear context-bleed unit_family post-lock (extraction postpass)
                # ============================================================
                try:
                    _fix41afc47_ctx_bleed_guard(_c)
                except Exception:
                    pass
                # ============================================================
                # PATCH FIX41AFC47B END
                # ============================================================
# ============================================================

                _fix41afc36_mark_bare_year_junk(_c)
    except Exception:
        pass
    # ============================================================
    # PATCH FIX41AFC36A END
# ============================================================



    # PATCH FIX41AFC38_EXTRACT_POSTPASS START



    try:



        _fix41afc38_backfill_unit_family_in_list(out)



    except Exception:



        pass



    # PATCH FIX41AFC38_EXTRACT_POSTPASS END




    return out



def extract_numbers_with_context_pdf(text):
    """
    PDF-specialized extractor wrapper.

    Tightening changes (v7.29+):
    - Inherit the year-only rejection from extract_numbers_with_context().
    - Keep boilerplate filters; prefer metric/table-like contexts.
    """
    import re

    if not text:
        return []

    base = extract_numbers_with_context(text) or []

    def _bad_pdf_context(ctx):
        c = (ctx or "").lower()
        bad = [
            "issn", "isbn", "doi", "catalogue", "legal notice",
            "all rights reserved", "reproduction is authorised",
            "printed by", "manuscript completed", "©", "copyright",
            "table of contents"
        ]
        return any(b in c for b in bad)

    def _good_pdf_context(ctx):
        c = (ctx or "").lower()
        # Lightweight heuristic: "table-ish" or "metric-ish"
        good = [
            "market", "revenue", "sales", "capacity", "generation", "growth",
            "cagr", "forecast", "projection", "increase", "decrease",
            "percent", "%", "billion", "million", "trillion", "usd", "eur", "gbp", "sgd"
        ]
        return any(g in c for g in good)

    filtered = []
    for n in base:
        if not isinstance(n, dict):
            continue
        ctx = n.get("context") or ""
        if _bad_pdf_context(ctx):
            continue
        filtered.append(n)

    # Prefer contexts that look "metric-like"
    preferred = [n for n in filtered if _good_pdf_context(n.get("context") or "")]

    # If we filtered too aggressively, fall back safely
    if preferred:
        return preferred
    if filtered:
        return filtered
    return base


def calculate_context_match(keywords: List[str], context: str) -> float:
    """Calculate how well keywords match the context (deterministic)."""
    if not context:
        return 0.0

    context_lower = context.lower()

    # If no keywords, give a small baseline (we'll rely more on value_score)
    if not keywords:
        return 0.25

    # Year keywords MUST match if present
    year_keywords = [kw for kw in keywords if re.fullmatch(r"20\d{2}", kw)]
    if year_keywords:
        if not any(y in context_lower for y in year_keywords):
            return 0.0

    matches = sum(1 for kw in keywords if kw.lower() in context_lower)

    # Instead of hard "matches < 2 = reject", scale smoothly:
    match_ratio = matches / max(len(keywords), 1)

    # If nothing matches, reject
    if matches == 0:
        return 0.0

    # Score between 0.35 and 1.0 depending on ratio
    return 0.35 + (match_ratio * 0.65)


def render_source_anchored_results(results, query: str):
    """Render source-anchored evolution results (guarded + backward compatible + tuned debug UI)."""
    import math
    import re
    from collections import Counter, defaultdict

    st.header("📈 Source-Anchored Evolution Analysis")
    st.markdown(f"**Query:** {query}")

    if not isinstance(results, dict):
        st.error("❌ Evolution returned an invalid result payload (not a dict).")
        st.write(results)
        return

    status = (results.get("status") or "").strip().lower()
    message = results.get("message") or ""

    def _safe_int(x, default=0) -> int:
        try:
            if x is None:
                return default
            return int(x)
        except Exception:
            return default

    def _safe_float(x, default=0.0) -> float:
        try:
            if x is None:
                return default
            return float(x)
        except Exception:
            return default

    def _fmt_pct(x, default="—") -> str:
        try:
            if x is None:
                return default
            v = float(x)
            if math.isnan(v):
                return default
            return f"{v:.0f}%"
        except Exception:
            return default

    def _fmt_change_pct(x) -> str:
        try:
            if x is None:
                return "-"
            v = float(x)
            if math.isnan(v):
                return "-"
            return f"{v:+.1f}%"
        except Exception:
            return "-"

    def _short(u: str, n: int = 95) -> str:
        if not u:
            return ""
        return (u[:n] + "…") if len(u) > n else u

    if status != "success":
        st.error(f"❌ {message or 'Evolution failed'}")
        sr = results.get("source_results") or []
        if isinstance(sr, list) and sr:
            st.subheader("🔗 Source Verification")
            for src in sr:
                if not isinstance(src, dict):
                    continue
                u = _short((src.get("url") or ""), 90)
                st.error(f"❌ {u} - {src.get('status_detail', 'Unknown error')}")
        return

    sources_checked = _safe_int(results.get("sources_checked"), 0)
    sources_fetched = _safe_int(results.get("sources_fetched"), 0)
    stability = _safe_float(results.get("stability_score"), 0.0)
    summary = results.get("summary") or {}
    if not isinstance(summary, dict):
        summary = {}

    metrics_inc = _safe_int(summary.get("metrics_increased"), 0)
    metrics_dec = _safe_int(summary.get("metrics_decreased"), 0)
    metrics_unch = _safe_int(summary.get("metrics_unchanged"), 0)

    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Sources Checked", sources_checked)
    col2.metric("Sources Fetched", sources_fetched)
    col3.metric("Stability", _fmt_pct(stability))
    if metrics_inc > metrics_dec:
        col4.success("📈 Trending Up")
    elif metrics_dec > metrics_inc:
        col4.error("📉 Trending Down")
    else:
        col4.info("➡️ Stable")

    if message:
        st.caption(message)

    st.markdown("---")

    # -------------------------
    # Source status
    # -------------------------
    st.subheader("🔗 Source Verification")
    src_results = results.get("source_results") or []
    if not isinstance(src_results, list):
        src_results = []

    # If everything failed, show breakdown
    if sources_checked > 0 and sources_fetched == 0 and src_results:
        reasons = []
        for s in src_results:
            if isinstance(s, dict):
                reasons.append((s.get("status_detail") or "unknown").split(":")[0])
        top = Counter(reasons).most_common(6)
        if top:
            st.warning("No sources were fetched successfully. Top failure types:")
            st.write({k: v for k, v in top})

    for src in src_results:
        if not isinstance(src, dict):
            continue
        url = src.get("url") or ""
        sstatus = src.get("status") or ""
        detail = src.get("status_detail") or ""
        ctype = src.get("content_type") or ""
        nfound = _safe_int(src.get("numbers_found"), 0)

        short = _short(url, 95)

        # show extra debug flags if present
        flags = []
        if src.get("snapshot_origin"):
            flags.append(f"origin={src.get('snapshot_origin')}")
        if src.get("is_homepage"):
            flags.append("homepage")
        if src.get("skip_reason"):
            flags.append(f"skip={src.get('skip_reason')}")
        if src.get("quality_score") is not None:
            try:
                flags.append(f"q={float(src.get('quality_score')):.2f}")
            except Exception:
                flags.append(f"q={src.get('quality_score')}")

        flag_txt = f" • {' • '.join(flags)}" if flags else ""

        if str(sstatus).startswith("fetched"):
            extra = f" ({nfound} nums)"
            if ctype:
                extra += f" • {ctype}"
            st.success(f"✅ {short}{extra}{flag_txt}")
        else:
            extra = f" - {detail}" if detail else ""
            if ctype:
                extra += f" • {ctype}"
            st.error(f"❌ {short}{extra}{flag_txt}")

    st.markdown("---")

    # -------------------------
    # Metric changes table
    # -------------------------
    st.subheader("💰 Metric Changes")
    rows = results.get("metric_changes") or []
    if not isinstance(rows, list) or not rows:
        st.info("No metric changes to display.")
        return

    table_rows = []
    for r in rows:
        if not isinstance(r, dict):
            continue

        metric_label = r.get("metric") or r.get("name") or ""
        status_label = r.get("status") or r.get("change_type") or ""

        table_rows.append({
            "Metric": metric_label,
            "Canonical Key": r.get("canonical_key", "") or "",
            "Match Stage": r.get("match_stage", "") or "",
            "Previous": r.get("previous_value", "") or "",
            "Current": r.get("current_value", "") or "",
            "Δ%": _fmt_change_pct(r.get("change_pct")),
            "Status": status_label,
            "Match": _fmt_pct(r.get("match_confidence")),
            "Score": ("" if r.get("match_score") is None else f"{_safe_float(r.get('match_score'), 0.0):.2f}"),
            "Anchor": "✅" if r.get("anchor_used") else "",
        })

    st.dataframe(table_rows, use_container_width=True)

    # -------------------------
    # Debug / tuning views
    # -------------------------
    # Aggregate rejection reasons across all metrics (quick tuning signal)
    agg_rej = Counter()
    for r in rows:
        if isinstance(r, dict) and isinstance(r.get("rejected_reason_counts"), dict):
            for k, v in r["rejected_reason_counts"].items():
                try:
                    agg_rej[k] += int(v or 0)
                except Exception:
                    pass

    if agg_rej:
        with st.expander("🧰 Tuning Summary (aggregate rejects across all metrics)"):
            st.write(dict(agg_rej.most_common(20)))

    # Full per-metric debug
    with st.expander("🧾 Per-metric match details (debug)"):
        for i, r in enumerate(rows, 1):
            if not isinstance(r, dict):
                continue

            metric_label = r.get("metric") or r.get("name") or f"metric_{i}"
            status_label = r.get("status") or r.get("change_type") or "unknown"

            canonical_key = r.get("canonical_key", "") or ""
            stage = r.get("match_stage", "") or ""
            conf = r.get("match_confidence", None)
            score = r.get("match_score", None)

            header = f"{i}. {metric_label} — {status_label}"
            meta_bits = []
            if canonical_key:
                meta_bits.append(f"ck={canonical_key}")
            if stage:
                meta_bits.append(f"stage={stage}")
            if conf is not None:
                meta_bits.append(f"conf={_fmt_pct(conf)}")
            if score is not None:
                try:
                    meta_bits.append(f"score={float(score):.2f}")
                except Exception:
                    meta_bits.append(f"score={score}")

            if meta_bits:
                header += f"  ({' • '.join(meta_bits)})"

            with st.expander(header):
                # Values
                st.write({
                    "previous_value": r.get("previous_value"),
                    "current_value": r.get("current_value"),
                    "change_pct": r.get("change_pct"),
                })

                # Candidate considered / rejects
                st.write("Candidates considered:", _safe_int(r.get("candidates_considered_count"), 0))

                rej = r.get("rejected_reason_counts")
                if isinstance(rej, dict) and rej:
                    # sort largest first
                    try:
                        rej_sorted = dict(sorted(((k, int(v or 0)) for k, v in rej.items()), key=lambda x: x[1], reverse=True))
                    except Exception:
                        rej_sorted = rej
                    st.write("Rejected reason counts:", rej_sorted)

                # Score breakdown (if present)
                sb = r.get("score_breakdown")
                if isinstance(sb, dict) and sb:
                    st.write("Score breakdown:", sb)

                # Matched candidate (new)
                mc = r.get("matched_candidate")
                if isinstance(mc, dict) and mc:
                    st.markdown("**Matched candidate**")
                    st.write({
                        "raw": mc.get("raw"),
                        "value": mc.get("value"),
                        "unit": mc.get("unit"),
                        "source_url": mc.get("source_url"),
                        "anchor_hash": mc.get("anchor_hash"),
                        "is_homepage": mc.get("is_homepage"),
                        "skip_reason": mc.get("skip_reason"),
                        "quality_score": mc.get("quality_score"),
                    })
                    ctx = mc.get("context_snippet")
                    if ctx:
                        st.write("Context:")
                        st.code(str(ctx))
                else:
                    # Backward-compatible fields
                    src = r.get("matched_source") or r.get("source_url")
                    ctx = r.get("matched_context") or r.get("context_snippet")
                    if src:
                        st.write("Source:", src)
                    if ctx:
                        st.write("Context:")
                        st.code(str(ctx))

                # Additional anchor hash compatibility
                if r.get("matched_anchor_hash"):
                    st.write("Matched Anchor Hash:", r.get("matched_anchor_hash"))

    st.markdown("---")


# =========================================================
# 9. DASHBOARD RENDERING
# =========================================================

def detect_x_label_dynamic(labels: list) -> str:
    """Enhanced X-axis detection with better region matching"""
    if not labels:
        return "Category"

    # Convert to lowercase for comparison
    label_texts = [str(l).lower().strip() for l in labels]
    all_text = ' '.join(label_texts)

    # 1. GEOGRAPHIC REGIONS (PRIORITY 1)
    region_keywords = [
        'north america', 'asia pacific', 'asia-pacific', 'apac', 'europe', 'emea',
        'latin america', 'latam', 'middle east', 'africa', 'oceania',
        'rest of world', 'row', 'china', 'usa', 'india', 'japan', 'germany'
    ]

    # Count how many labels contain region keywords
    region_matches = sum(
        1 for label in label_texts
        if any(keyword in label for keyword in region_keywords)
    )

    # If 40%+ of labels are regions → "Regions"
    if region_matches / len(labels) >= 0.4:
        return "Regions"

    # 2. YEARS (e.g., 2023, 2024, 2025)
    year_pattern = r'\b(19|20)\d{2}\b'
    year_count = sum(1 for label in label_texts if re.search(year_pattern, label))
    if year_count / len(labels) > 0.5:
        return "Years"

    # 3. QUARTERS (Q1, Q2, Q3, Q4)
    quarter_pattern = r'\bq[1-4]\b'
    quarter_count = sum(1 for label in label_texts if re.search(quarter_pattern, label, re.IGNORECASE))
    if quarter_count >= 2:
        return "Quarters"

    # 4. MONTHS
    months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']
    month_count = sum(1 for label in label_texts if any(month in label for month in months))
    if month_count >= 3:
        return "Months"

    # 5. COMPANIES (common suffixes)
    company_keywords = ['inc', 'corp', 'ltd', 'llc', 'gmbh', 'ag', 'sa', 'plc']
    company_count = sum(1 for label in label_texts if any(kw in label for kw in company_keywords))
    if company_count >= 2:
        return "Companies"

    # 6. PRODUCTS/SEGMENTS (if contains "segment", "product", "category")
    if any(word in all_text for word in ['segment', 'product line', 'category', 'type']):
        return "Segments"

    # Default
    return "Categories"

def detect_y_label_dynamic(values: list) -> str:
    """Fully dynamic Y-axis label based on magnitude + context"""
    if not values:
        return "Value"

    numeric_values = []
    for v in values:
        try:
            numeric_values.append(abs(float(v)))
        except (ValueError, TypeError):
            continue

    if not numeric_values:
        return "Value"

    avg_mag = np.mean(numeric_values)
    max_mag = max(numeric_values)

    # Non-overlapping ranges with clear boundaries
    # 1. BILLIONS (large market sizes)
    if max_mag > 100 or avg_mag > 50:
        return "USD B"

    # 2. MILLIONS (medium values)
    elif max_mag > 10 or avg_mag > 5:
        return "USD M"

    # 3. PERCENTAGES (typical 0-100 range, but also small decimals)
    elif max_mag <= 100 and avg_mag <= 50:
        # Check if values look like percentages (mostly 0-100)
        if all(0 <= v <= 100 for v in numeric_values):
            return "Percent %"
        else:
            return "USD K"

    # 4. Default
    else:
        return "Units"

# =========================================================
# 3A. QUESTION CATEGORIZATION + SIGNALS (DETERMINISTIC)
# =========================================================

def categorize_question_signals(query: str, qs: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Build a question_profile used for structured reporting.

    IMPORTANT:
      - category must follow query_structure if provided (single source of truth).
      - signals can be rich, but must not contradict the chosen category.
    """
    qs = qs or {}
    q = (query or "").strip()

    # Prefer category/main/side from query_structure when available
    category = (qs.get("category") or "").strip() or "unknown"
    main_q = (qs.get("main") or "").strip() or q
    side_qs = qs.get("side") if isinstance(qs.get("side"), list) else []

    # Deterministic signals (richer classifier)
    base = classify_question_signals(q) or {}

    # Force category + expected_metric_ids to match query_structure category
    # (but preserve other extracted info like years/regions/intents)
    signals: Dict[str, Any] = {}
    signals["category"] = category

    # Carry over extracted fields
    signals["years"] = base.get("years", []) or []
    signals["regions"] = base.get("regions", []) or []
    signals["intents"] = base.get("intents", []) or []

    # Keep raw signals for debugging
    raw_hits = list(base.get("signals") or [])
    signals["raw_signals"] = raw_hits

    def _signal_consistent_with_category(sig: str, cat: str) -> bool:
        s = (sig or "").lower()
        c = (cat or "").lower()
        if not s:
            return False

        # If final category is country, drop industry/company category-rule strings
        if c == "country":
            if "industry_keywords" in s or "mixed_signals_default_to_industry" in s or "company_keywords" in s:
                return False

        # If final category is industry, drop explicit country-rule strings
        if c == "industry":
            if "macro_outlook_bias_country" in s or "country_keywords" in s:
                return False

        return True

    signals["signals"] = [s for s in raw_hits if _signal_consistent_with_category(s, category)]

    # Expected metric IDs: always determined by the final category, then lightly enriched by intents (optional)
    expected_metric_ids: List[str] = []
    try:
        expected_metric_ids = get_expected_metric_ids_for_category(category) or []
    except Exception:
        expected_metric_ids = []

    # Optional: enrich with intent-based suggestions (won't remove anything)
    intent_metric_suggestions = {
        "market_size": ["market_size", "market_size_2024", "market_size_2025"],
        "growth_forecast": ["cagr", "market_size_2030"],
        "competitive_landscape": ["market_share", "top_players"],
        "pricing": ["avg_price", "asp"],
        "consumer_demand": ["users", "penetration", "arpu"],
        "supply_chain": ["capacity", "shipments"],
        "investment": ["capex", "profit", "ebitda"],
        "macro_outlook": ["gdp", "inflation", "interest_rate", "exchange_rate"],
    }

    intents = signals.get("intents") or []
    for intent in intents:
        for mid in intent_metric_suggestions.get(intent, []):
            if mid not in expected_metric_ids:
                expected_metric_ids.append(mid)

    signals["expected_metric_ids"] = expected_metric_ids

    profile: Dict[str, Any] = {
        "category": category,
        "signals": signals,
        "main_question": main_q,
        "side_questions": side_qs,
    }

    # Keep debug for traceability
    if qs.get("debug") is not None:
        profile["debug_query_structure"] = qs.get("debug")

    return profile




def render_dashboard(
    primary_json: str,
    final_conf: float,
    web_context: Dict,
    base_conf: float,
    user_question: str,
    veracity_scores: Optional[Dict] = None,
    source_reliability: Optional[List[str]] = None,
):
    """Render the analysis dashboard"""

    # -------------------------
    # Parse primary response
    # -------------------------

    # =========================
    # PATCH RD1 (ADDITIVE): safe preview helper
    # - Prevents slice errors when primary_json is dict/list/etc.
    # - Keeps original behavior for strings
    # =========================
    def _preview(x, limit: int = 1000) -> str:
        try:
            if isinstance(x, (dict, list)):
                s = json.dumps(x, ensure_ascii=False, indent=2, default=str)
            else:
                s = str(x)
        except Exception:
            s = repr(x)
        return s[:limit]
    # =========================

    try:
        # =========================
        # PATCH RD2 (ADDITIVE): accept dict/list directly
        # - If caller passes dict (primary_data), just use it
        # - If caller passes list, wrap it (keeps downstream dict access safe)
        # - Else try json.loads on string
        # =========================
        if isinstance(primary_json, dict):
            data = primary_json
        elif isinstance(primary_json, list):
            data = {"_list": primary_json}
        else:
            data = json.loads(primary_json)
        # =========================

    except Exception as e:
        st.error(f"❌ Cannot render dashboard: {e}")
        # =========================
        # PATCH RD1 (ADDITIVE): safe preview (no slicing crash)
        # =========================
        st.code(_preview(primary_json))
        # =========================
        return

    # -------------------------
    # Helper: metric value formatting (currency + compact units) + RANGE SUPPORT
    # -------------------------
    def _format_metric_value(m: Any) -> str:
        """
        Format metric values cleanly, with RANGE SUPPORT:
        - If value_range exists (min/max), show min–max using the same currency/unit rules
        - Otherwise show the point value as before
        """
        if not isinstance(m, dict):
            if m is None:
                return "N/A"
            return str(m)

        # -------------------------
        # Helper: format a single numeric endpoint (val+unit)
        # -------------------------
        def _format_point(val: Any, unit: str) -> str:
            if val is None or val == "":
                return "N/A"

            unit = (unit or "").strip()
            raw_val = str(val).strip()

            # Try parse numeric
            try:
                num = float(raw_val.replace(",", ""))
            except Exception:
                # If we can't parse as float, just glue value+unit neatly
                return f"{raw_val}{unit}".strip() if unit else raw_val

            # Normalize unit spacing
            unit = unit.replace(" ", "")
            currency_prefix = ""
            u_upper = unit.upper()

            # Common patterns: "S$B", "SGDB", "USD B", "$B"
            if u_upper.startswith("S$"):
                currency_prefix = "S$"
                unit = unit[2:]
            elif u_upper.startswith("SGD"):
                currency_prefix = "S$"
                unit = unit[3:]
            elif u_upper.startswith("USD"):
                currency_prefix = "$"
                unit = unit[3:]
            elif u_upper.startswith("$"):
                currency_prefix = "$"
                unit = unit[1:]

            unit = unit.strip()

            # Percent
            if unit == "%":
                return f"{num:.1f}%"

            # Compact units
            unit_upper = unit.upper()
            if unit_upper in ("B", "BILLION"):
                formatted = f"{num:.2f}".rstrip("0").rstrip(".") + "B"
                return f"{currency_prefix}{formatted}".strip()
            if unit_upper in ("M", "MILLION"):
                formatted = f"{num:.2f}".rstrip("0").rstrip(".") + "M"
                return f"{currency_prefix}{formatted}".strip()
            if unit_upper in ("K", "THOUSAND"):
                formatted = f"{num:.2f}".rstrip("0").rstrip(".") + "K"
                return f"{currency_prefix}{formatted}".strip()

            # Plain number formatting
            if abs(num) >= 1000:
                if float(num).is_integer():
                    formatted = f"{int(num):,}"
                else:
                    formatted = f"{num:,.2f}".rstrip("0").rstrip(".")
            else:
                formatted = f"{num:g}"

            # Unit glue
            if unit:
                formatted = f"{formatted} {unit}".strip()

            return f"{currency_prefix}{formatted}".strip()

        # -------------------------
        # RANGE: prefer value_range if present and meaningful
        # -------------------------
        unit = (m.get("unit") or "").strip()
        vr = m.get("value_range")

        if isinstance(vr, dict):
            vmin = vr.get("min")
            vmax = vr.get("max")
            if vmin is not None and vmax is not None:
                left = _format_point(vmin, unit)
                right = _format_point(vmax, unit)
                if left != "N/A" and right != "N/A" and left != right:
                    return f"{left}–{right}"

        # Precomputed range display (optional)
        vr_disp = m.get("value_range_display")
        if isinstance(vr_disp, str) and vr_disp.strip():
            return vr_disp.strip()

        # -------------------------
        # POINT VALUE fallback
        # -------------------------
        val = m.get("value")
        if val is None or val == "":
            return "N/A"

        return _format_point(val, unit)

    # -------------------------
    # Header + confidence row
    # -------------------------
    st.header("📊 Yureeka Market Report")
    st.markdown(f"**Question:** {user_question}")

    col1, col2, col3 = st.columns(3)
    col1.metric("Final Confidence", f"{float(final_conf):.1f}%")
    col2.metric("Base Model", f"{float(base_conf):.1f}%")
    if isinstance(veracity_scores, dict):
        col3.metric("Evidence", f"{float(veracity_scores.get('overall', 0) or 0):.1f}%")
    else:
        col3.metric("Evidence", "N/A")

    st.markdown("---")

    # -------------------------
    # Executive Summary
    # -------------------------
    st.subheader("📋 Executive Summary")
    st.markdown(f"**{data.get('executive_summary', 'No summary available')}**")

    # Optional: expand summary if side-questions exist
    side_questions = data.get("side_questions") or (data.get("question_profile", {}) or {}).get("side_questions", [])
    if side_questions:
        st.markdown("")
        st.markdown("**Also addressed:**")
        for sq in side_questions[:6]:
            if sq:
                st.markdown(f"- {sq}")

    st.markdown("---")

    # -------------------------
    # Key Metrics
    # -------------------------
    st.subheader("💰 Key Metrics")
    metrics = data.get("primary_metrics", {}) or {}

    question_category = data.get("question_category") or (data.get("question_profile", {}) or {}).get("category")
    question_signals = data.get("question_signals") or (data.get("question_profile", {}) or {}).get("signals", {})
    expected_ids = data.get("expected_metric_ids") or ((data.get("question_signals") or {}).get("expected_metric_ids") or [])

    metric_rows: List[Dict[str, str]] = []

    if question_category:
        metric_rows.append({"Metric": "Question Category", "Value": str(question_category)})
    if isinstance(question_signals, dict) and question_signals:
        metric_rows.append({"Metric": "Signals", "Value": ", ".join([str(x) for x in (question_signals.get("signals") or [])][:10])})
    if expected_ids:
        metric_rows.append({"Metric": "Expected Metrics", "Value": ", ".join([str(x) for x in expected_ids][:10])})

    # Render primary metrics
    if isinstance(metrics, dict) and metrics:
        for _, m in metrics.items():
            if isinstance(m, dict):
                name = m.get("name") or "Metric"
                metric_rows.append({"Metric": str(name), "Value": _format_metric_value(m)})

    # Display metrics table
    if metric_rows:
        try:
            import pandas as pd  # optional dependency in your environment
            df_metrics = pd.DataFrame(metric_rows)
            st.dataframe(df_metrics, use_container_width=True, hide_index=True)
        except Exception:
            for r in metric_rows:
                st.write(f"**{r.get('Metric','')}**: {r.get('Value','')}")

    st.markdown("---")

    # -------------------------
    # Key Findings
    # -------------------------
    st.subheader("🧠 Key Findings")
    kf = data.get("key_findings") or []
    if isinstance(kf, list) and kf:
        for item in kf[:12]:
            if item:
                st.markdown(f"- {item}")
    else:
        st.info("No key findings available.")

    st.markdown("---")

    # -------------------------
    # Trends / Forecast
    # -------------------------
    st.subheader("📈 Trends & Forecast")
    tf = data.get("trends_forecast") or []
    if isinstance(tf, list) and tf:
        for t in tf[:12]:
            if isinstance(t, dict):
                trend = t.get("trend") or ""
                direction = t.get("direction") or ""
                timeline = t.get("timeline") or ""
                st.markdown(f"- **{trend}** {direction} ({timeline})")
            elif t:
                st.markdown(f"- {t}")
    else:
        st.info("No trends forecast available.")

    st.markdown("---")

    # -------------------------
    # Sources / Web Context summary
    # -------------------------
    st.subheader("🔎 Sources & Evidence")
    sources = data.get("sources") or data.get("web_sources") or []
    if isinstance(sources, list) and sources:
        with st.expander(f"Show sources ({len(sources)})"):
            for s in sources[:50]:
                if s:
                    st.markdown(f"- {s}")
            if len(sources) > 50:
                st.markdown(f"... (+{len(sources)-50} more)")

    # Web context debug counters if present
    if isinstance(web_context, dict):
        dbg = web_context.get("debug_counts") or {}
        if isinstance(dbg, dict) and dbg:
            with st.expander("Collector debug counts"):
                st.json(dbg)

    # =====================================================================
    # PATCH UI_EXTRA_URLS_TRACE2 (ADDITIVE): show injected extra-URL trace (if any)
    # =====================================================================
    try:
        exdbg = {}
        if isinstance(web_context, dict):
            exdbg = web_context.get("extra_urls_debug") or {}
            # Back-compat: allow nested placement under debug_counts
            if (not exdbg) and isinstance(web_context.get("debug_counts"), dict):
                exdbg = (web_context.get("debug_counts") or {}).get("extra_urls_debug") or {}
        if isinstance(exdbg, dict) and exdbg:
            with st.expander("Extra URLs trace (injected sources)"):
                st.json(exdbg)
    except Exception:
        pass
    # =====================================================================

    # Source reliability badges (if provided)
    if isinstance(source_reliability, list) and source_reliability:
        with st.expander("Source reliability"):
            for line in source_reliability[:80]:
                st.write(line)



def render_native_comparison(baseline: Dict, compare: Dict):
    """Render a clean comparison between two analyses"""

    st.header("📊 Analysis Comparison")

    # Time info
    baseline_time = baseline.get('timestamp', '')
    compare_time = compare.get('timestamp', '')

    try:
        baseline_dt = datetime.fromisoformat(baseline_time.replace('Z', '+00:00'))
        compare_dt = datetime.fromisoformat(compare_time.replace('Z', '+00:00'))
        delta = compare_dt - baseline_dt
        if delta.days > 0:
            delta_str = f"{delta.days}d {delta.seconds // 3600}h"
        else:
            delta_str = f"{delta.seconds // 3600}h {(delta.seconds % 3600) // 60}m"
    except:
        delta_str = "Unknown"

    # Overview row
    col1, col2, col3 = st.columns(3)
    col1.metric("Baseline", baseline_time[:16] if baseline_time else "N/A")
    col2.metric("Current", compare_time[:16] if compare_time else "N/A")
    col3.metric("Time Delta", delta_str)

    st.markdown("---")

    # Extract metrics
    baseline_metrics = baseline.get('primary_response', {}).get('primary_metrics', {})
    compare_metrics = compare.get('primary_response', {}).get('primary_metrics', {})

    # Build metric diff table
    st.subheader("💰 Metric Changes")

    diff_rows = []
    stability_count = 0
    total_count = 0

    # Canonicalize metrics for stable matching
    baseline_canonical = canonicalize_metrics(baseline_metrics)
    compare_canonical = canonicalize_metrics(compare_metrics)

    # Build lookup by canonical ID
    baseline_by_id = {}
    compare_by_id = {}

    for cid, m in baseline_canonical.items():
        baseline_by_id[cid] = m

    for cid, m in compare_canonical.items():
        compare_by_id[cid] = m

    all_ids = set(baseline_by_id.keys()).intersection(compare_by_id.keys())

    for cid in sorted(all_ids):
        baseline_m = baseline_by_id.get(cid)
        compare_m = compare_by_id.get(cid)

        # Use canonical name for display, fallback to original
        display_name = cid
        if baseline_m and baseline_m.get('name'):
            display_name = baseline_m['name']


        if baseline_m and compare_m:
            old_val = baseline_m.get('value', 'N/A')
            new_val = compare_m.get('value', 'N/A')
            unit = compare_m.get('unit', baseline_m.get('unit', ''))

            old_num = parse_to_float(old_val)
            new_num = parse_to_float(new_val)

            if old_num is not None and new_num is not None and old_num != 0:
                change_pct = ((new_num - old_num) / abs(old_num)) * 100

                if abs(change_pct) < 1:
                    icon, reason = "➡️", "No change"
                    stability_count += 1
                elif abs(change_pct) < 5:
                    icon, reason = "➡️", "Minor change"
                    stability_count += 1
                elif change_pct > 0:
                    icon, reason = "📈", "Increased"
                else:
                    icon, reason = "📉", "Decreased"

                delta_str = f"{change_pct:+.1f}%"
            else:
                icon, delta_str, reason = "➡️", "-", "Non-numeric"
                stability_count += 1

            diff_rows.append({
                '': icon,
                'Metric': display_name,
                'Old': _fmt_currency_first(str(old_val), str(unit)),
                'New': _fmt_currency_first(str(new_val), str(unit)),
                'Δ': delta_str,
                'Reason': reason
            })
            total_count += 1

        elif baseline_m:
            old_val = baseline_m.get('value', 'N/A')
            unit = baseline_m.get('unit', '')
            diff_rows.append({
                '': '❌',
                'Metric': display_name,
                'Old': f"{old_val} {unit}".strip(),
                'New': '-',
                'Δ': '-',
                'Reason': 'Removed'
            })
            total_count += 1
        else:
            new_val = compare_m.get('value', 'N/A')
            unit = compare_m.get('unit', '')
            diff_rows.append({
                '': '🆕',
                'Metric': display_name,
                'Old': '-',
                'New': f"{new_val} {unit}".strip(),
                'Δ': '-',
                'Reason': 'New'
            })
            total_count += 1

    if diff_rows:
        st.dataframe(pd.DataFrame(diff_rows), hide_index=True, use_container_width=True)

        # Show canonical ID mapping for debugging
        with st.expander("🔧 Canonical ID Mapping (Debug)"):
            st.write("**How metrics were matched:**")

            baseline_canonical = canonicalize_metrics(baseline_metrics)
            compare_canonical = canonicalize_metrics(compare_metrics)

            col1, col2 = st.columns(2)

            with col1:
                st.write("**Baseline Metrics:**")
                for cid, m in baseline_canonical.items():
                    original = m.get('original_name', 'N/A')
                    canonical = m.get('name', 'N/A')
                    st.caption(f"`{cid}`")
                    st.write(f"  {original} → {canonical}")

            with col2:
                st.write("**Current Metrics:**")
                for cid, m in compare_canonical.items():
                    original = m.get('original_name', 'N/A')
                    canonical = m.get('name', 'N/A')
                    st.caption(f"`{cid}`")
                    st.write(f"  {original} → {canonical}")
    else:
        st.info("No metrics to compare")

    # Stability score
    stability_pct = (stability_count / total_count * 100) if total_count > 0 else 100

    st.markdown("---")
    st.subheader("📊 Stability Score")

    col1, col2, col3 = st.columns(3)
    col1.metric("Stable Metrics", f"{stability_count}/{total_count}")
    col2.metric("Stability", f"{stability_pct:.0f}%")

    if stability_pct >= 80:
        col3.success("🟢 Highly Stable")
    elif stability_pct >= 60:
        col3.warning("🟡 Moderate Changes")
    else:
        col3.error("🔴 Significant Drift")

    # Confidence comparison
    st.markdown("---")
    st.subheader("🎯 Confidence Change")

    col1, col2, col3 = st.columns(3)
    baseline_conf = baseline.get('final_confidence', 0)
    compare_conf = compare.get('final_confidence', 0)
    conf_change = compare_conf - baseline_conf if isinstance(baseline_conf, (int, float)) and isinstance(compare_conf, (int, float)) else 0

    col1.metric("Baseline", f"{baseline_conf:.1f}%" if isinstance(baseline_conf, (int, float)) else "N/A")
    col2.metric("Current", f"{compare_conf:.1f}%" if isinstance(compare_conf, (int, float)) else "N/A")
    col3.metric("Change", f"{conf_change:+.1f}%")

    # Download comparison
    st.markdown("---")
    comparison_output = {
        "comparison_timestamp": datetime.now().isoformat(),
        "baseline": baseline,
        "current": compare,
        "stability_score": stability_pct,
        "metrics_compared": total_count,
        "metrics_stable": stability_count
    }

    st.download_button(
        label="💾 Download Comparison Report",
        data=json.dumps(comparison_output, indent=2, ensure_ascii=False).encode('utf-8'),
        file_name=f"yureeka_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        mime="application/json"
    )

# =========================================================
# 10. MAIN APPLICATION
# =========================================================

# ==============================================================================
# PATCH FIX39 (ADDITIVE): Final publish/render unit-required hard gate
#
# Why:
# - Even if upstream selection is tightened, some paths (UI render, sheet publish,
#   legacy mappings) can still surface unit-less year-like integers (e.g., 2024/2025)
#   in the "Current" column for unit-required metrics (currency/percent/rate/ratio).
# - FIX39 enforces the invariant at the last mile: right before rendering/publishing.
#
# Behavior:
# - For each metric change row (dict-form) and each EvolutionDiff metric entry (object-form),
#   if schema indicates unit required (via unit_family or canonical_key suffix) AND
#   current value lacks token-level unit evidence, then:
#     * blank out Current/new_raw
#     * set unit_mismatch flag / change_type to "unit_mismatch" where possible
# - Purely additive; does not refactor upstream pipelines.
# ==============================================================================

def _fix39_schema_unit_required(metric_def: dict, canonical_key: str = "") -> bool:
    try:
        uf = str((metric_def or {}).get("unit_family") or (metric_def or {}).get("unit") or "").strip().lower()
        if uf in {"currency", "percent", "rate", "ratio"}:
            return True
    except Exception:
        pass
    ck = (canonical_key or "").strip().lower()
    if ck.endswith("__currency") or ck.endswith("__percent") or ck.endswith("__rate") or ck.endswith("__ratio"):
        return True
    # unit_tag explicit
    try:
        ut = str((metric_def or {}).get("unit_tag") or "").strip()
        if ut:
            # if schema explicitly wants a unit token, treat as required
            return True
    except Exception:
        pass
    return False

def _fix39_has_unit_evidence(metric_like: dict) -> bool:
    """Token-level unit evidence check (tolerant across shapes)."""
    try:
        m = metric_like if isinstance(metric_like, dict) else {}
        for k in ("unit", "unit_tag", "base_unit", "unit_family", "currency", "currency_symbol"):
            if str(m.get(k) or "").strip():
                return True
        if bool(m.get("is_percent") or m.get("has_percent")):
            return True
        # Some rows store comparator field
        if str(m.get("cur_unit_cmp") or "").strip():
            return True
        raw = str(m.get("raw") or m.get("value") or m.get("new_raw") or "")
        if raw and any(sym in raw for sym in ("$", "€", "£", "¥", "%")):
            return True
    except Exception:
        pass
    return False

def _fix39_sanitize_metric_change_rows(results_dict: dict) -> None:
    """Sanitize dict-based evolution results before publishing/rendering."""
    if not isinstance(results_dict, dict):
        return
    try:
        schema = results_dict.get("metric_schema_frozen") or results_dict.get("schema") or {}
        metric_changes = None
        # common nesting patterns
        if isinstance(results_dict.get("results"), dict) and isinstance(results_dict["results"].get("metric_changes"), list):
            metric_changes = results_dict["results"]["metric_changes"]
        elif isinstance(results_dict.get("metric_changes"), list):
            metric_changes = results_dict.get("metric_changes")

        if not isinstance(metric_changes, list):
            return

        bad = []
        for row in metric_changes:
            if not isinstance(row, dict):
                continue
            ck = row.get("canonical_key") or row.get("canonical") or row.get("key") or ""
            md = {}
            try:
                if isinstance(schema, dict) and ck in schema:
                    md = schema.get(ck) or {}
            except Exception:
                md = {}
            if _fix39_schema_unit_required(md, ck):
                # current fields may be in different keys
                cur_like = {
                    "unit": row.get("current_unit") or row.get("cur_unit") or row.get("unit") or "",
                    "unit_tag": row.get("current_unit_tag") or row.get("unit_tag") or "",
                    "unit_family": row.get("schema_unit_family") or "",
                    "cur_unit_cmp": row.get("cur_unit_cmp") or "",
                    "raw": row.get("current_value") or row.get("current_raw") or row.get("Current") or "",
                    "new_raw": row.get("new_raw") or "",
                    "currency_symbol": row.get("currency_symbol") or "",
                    "is_percent": row.get("is_percent") or False,
                }
                if not _fix39_has_unit_evidence(cur_like):
                    # invalidate
                    row["unit_mismatch"] = True
                    # prefer explicit fields if present
                    for k in ("current_value", "current_raw", "new_raw", "Current"):
                        if k in row:
                            row[k] = ""
                    if "current_value_norm" in row:
                        row["current_value_norm"] = None
                    if "cur_value_norm" in row:
                        row["cur_value_norm"] = None
                    # normalize change_type
                    if row.get("change_type") not in ("unit_mismatch", "invalid_current"):
                        row["change_type"] = "unit_mismatch"
                    bad.append(str(ck))
        # small debug marker
        dbg = results_dict.setdefault("debug", {})
        f39 = dbg.setdefault("fix39", {})
        f39["invalidated_count"] = len(bad)
        if bad:
            f39["invalidated_keys_sample"] = bad[:20]
    except Exception:
        return

def _fix39_sanitize_evolutiondiff_object(diff_obj, metric_schema_frozen: dict = None):
    """Sanitize object-based EvolutionDiff (used by Streamlit renderer)."""
    try:
        schema = metric_schema_frozen or {}
        mdiffs = getattr(diff_obj, "metric_diffs", None)
        if not mdiffs:
            return diff_obj
        bad = []
        for m in mdiffs:
            try:
                ck = getattr(m, "canonical_key", "") or getattr(m, "canonical", "") or ""
                md = schema.get(ck) if isinstance(schema, dict) else {}
                if _fix39_schema_unit_required(md or {}, ck):
                    unit = getattr(m, "unit", "") or ""
                    new_raw = getattr(m, "new_raw", None)
                    # basic evidence check: unit or symbol in new_raw
                    has_e = bool(str(unit).strip())
                    if not has_e:
                        s = str(new_raw or "")
                        if any(sym in s for sym in ("$", "€", "£", "¥", "%")):
                            has_e = True
                    if not has_e:
                        # invalidate
                        try: setattr(m, "new_raw", "")
                        except Exception: pass
                        try: setattr(m, "new_value", None)
                        except Exception: pass
                        try: setattr(m, "change_type", "unit_mismatch")
                        except Exception: pass
                        bad.append(str(ck))
            except Exception:
                continue
        try:
            dbg = getattr(diff_obj, "debug", None)
            if isinstance(dbg, dict):
                dbg.setdefault("fix39", {})["invalidated_count"] = len(bad)
        except Exception:
            pass
        return diff_obj
    except Exception:
        return diff_obj

def main():
    st.set_page_config(
        page_title="Yureeka Market Report",
        page_icon="💹",
        layout="wide"
    )

    st.title("💹 Yureeka Market Intelligence")

    # Info section
    col_info, col_status = st.columns([3, 1])
    with col_info:
        st.markdown("""
        **Yureeka** provides AI-powered market research and analysis for finance,
        economics, and business questions.
        Powered by evidence-based verification and real-time web search.

        *Currently in prototype stage.*
        """)

    # Create tabs
    tab1, tab2 = st.tabs(["🔍 New Analysis", "📈 Evolution Analysis"])

    # =====================
    # TAB 1: NEW ANALYSIS
    # =====================
    with tab1:
        query = st.text_input(
            "Enter your question about markets, industries, finance, or economics:",
            placeholder="e.g., What is the size of the global EV battery market?"
        )

        col_opt1, col_opt2 = st.columns(2)
        with col_opt1:
            use_web = st.checkbox(
                "Enable web search (recommended)",
                value=bool(SERPAPI_KEY),
                disabled=not SERPAPI_KEY
            )


            # ============================================================

            # PATCH UI_EXTRA_SOURCES_TAB1 (ADDITIVE)

            # - Add extra URL injection UI directly to TAB 1 (New Analysis)

            # - Does NOT alter behavior unless user supplies URLs

            # ============================================================

            extra_sources_text_tab1 = st.text_area(

                "Extra source URLs (optional, one per line)",

                placeholder="https://example.com/report\nhttps://another-source.com/page",

                help="Add these URLs to the admitted source list for this analysis run (useful for hash-mismatch tests).",

                height=90,

                key="ui_extra_sources_tab1",

            )

            # ============================================================



        if st.button("🔍 Analyze", type="primary") and query:
            if len(query.strip()) < 5:
                st.error("❌ Please enter a question with at least 5 characters")
                return

            query = query.strip()[:500]

            query_structure = extract_query_structure(query) or {}
            question_profile = categorize_question_signals(query, qs=query_structure)
            question_signals = question_profile.get("signals", {}) or {}

            web_context = {}
            if use_web:
                with st.spinner("🌐 Searching the web..."):

                    # ---- ADDITIVE: pass existing snapshots for reuse (Change #3 wiring) ----
                    existing_snapshots = None

                    # If you have an analysis dict already in scope, reuse its cache
                    try:
                        if isinstance(locals().get("analysis"), dict):
                            existing_snapshots = (
                                analysis.get("baseline_sources_cache")
                                or (analysis.get("results", {}) or {}).get("baseline_sources_cache")
                                or (analysis.get("results", {}) or {}).get("source_results")
                            )
                    except Exception:
                        existing_snapshots = None

                    # Optional: if you keep a prior analysis in session_state, reuse it
                    try:
                        prev = st.session_state.get("last_analysis")
                        if existing_snapshots is None and isinstance(prev, dict):
                            existing_snapshots = (
                                prev.get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("source_results")
                            )
                    except Exception:
                        pass

                                        # ============================================================
                    # PATCH UI_EXTRA_SOURCES2 (ADDITIVE): parse extra source URLs
                    # ============================================================
                    extra_urls = []
                    try:
                        for _l in str(extra_sources_text_tab1 or "").splitlines():
                            _u = _l.strip()
                            if not _u:
                                continue
                            if _u.startswith("http://") or _u.startswith("https://"):
                                extra_urls.append(_u)
                    except Exception:
                        extra_urls = []


                    # ============================================================
                    # PATCH INJ_DIAG_TAB1_CALL (ADDITIVE): correlate UI extra-URL input into fetch_web_context diagnostics
                    # ============================================================
                    _analysis_run_id = _inj_diag_make_run_id("analysis")
                    # ============================================================

                    web_context = fetch_web_context(
                        query,
                        num_sources=3,
                        existing_snapshots=existing_snapshots,
                        extra_urls=extra_urls,
                        diag_run_id=_analysis_run_id,
                        diag_extra_urls_ui_raw=(extra_sources_text_tab1 or ""),
                    )
                    # ----------------------------------------------------------------------

            if not web_context or not web_context.get("search_results"):
                st.info("💡 Using AI knowledge without web search")
                web_context = {
                    "search_results": [],
                    "scraped_content": {},
                    "summary": "",
                    "sources": [],
                    "source_reliability": []
                }

            with st.spinner("🤖 Analyzing query..."):
                primary_response = query_perplexity(query, web_context, query_structure=query_structure)

            if not primary_response:
                st.error("❌ Primary model failed to respond")
                return

            try:
                primary_data = json.loads(primary_response)
            except Exception as e:
                st.error(f"❌ Failed to parse primary response: {e}")
                st.code(primary_response[:1000])
                return

            with st.spinner("✅ Verifying evidence quality..."):
                veracity_scores = evidence_based_veracity(primary_data, web_context)

            base_conf = float(primary_data.get("confidence", 75))
            final_conf = calculate_final_confidence(base_conf, veracity_scores.get("overall", 0))

            # Optional: canonicalize + attribution + schema freeze (only if your codebase defines these)
            # Optional: canonicalize + attribution + schema freeze (only if your codebase defines these)
            try:
                # 1) canonicalize (unchanged)
                if primary_data.get("primary_metrics"):
                    primary_data["primary_metrics_canonical"] = canonicalize_metrics(
                        primary_data.get("primary_metrics", {}),
                        merge_duplicates_to_range=True,
                        question_text=query,
                        category_hint=str(primary_data.get("question_category", ""))
                    )

                # 2) freeze schema FIRST ✅ (so attribution can be schema-first)
                if primary_data.get("primary_metrics_canonical"):
                    primary_data["metric_schema_frozen"] = freeze_metric_schema(
                        primary_data["primary_metrics_canonical"]
                    )

                # 3) attribution using frozen schema ✅
                if primary_data.get("primary_metrics_canonical"):
                    primary_data["primary_metrics_canonical"] = add_range_and_source_attribution_to_canonical_metrics(
                        primary_data.get("primary_metrics_canonical", {}),
                        web_context,
                        metric_schema=(primary_data.get("metric_schema_frozen") or {}),
                    )

                # PATCH SV1/EG1 (ADDITIVE): validate frozen schema + enforce evidence gating (analysis-side)
                try:
                    fn = globals().get("apply_schema_validation_and_evidence_gating")
                    if callable(fn):
                        primary_data = fn(primary_data)
                except Exception:
                    pass

            except Exception:
                pass

            # Hash key findings (optional)
            try:
                if primary_data.get("key_findings"):
                    findings_with_hash = []
                    for finding in primary_data.get("key_findings", []):
                        if finding:
                            findings_with_hash.append({
                                "text": finding,
                                "semantic_hash": compute_semantic_hash(finding)
                            })
                    primary_data["key_findings_hashed"] = findings_with_hash
            except Exception:
                pass


            # Save baseline numeric cache if available (existing behavior)

            # Build output
            output = {
                "question": query,
                "question_profile": question_profile,
                "question_category": question_profile.get("category"),
                "question_signals": question_signals,
                "side_questions": question_profile.get("side_questions", []),
                "timestamp": now_utc().isoformat(),
                "primary_response": primary_data,
                "final_confidence": final_conf,
                "veracity_scores": veracity_scores,
                "web_sources": web_context.get("sources", []),
                "code_version": CODE_VERSION,
                }

            try:
                if isinstance(output.get("primary_response"), dict):
                    output["primary_response"]["code_version"] = CODE_VERSION
            except Exception:
                pass


            # ✅ NEW: attach analysis-aligned snapshots (from scraped_meta)
            # This is the stable cache evolution should reuse.
            try:
                output = attach_source_snapshots_to_analysis(output, web_context)
            except Exception:
                pass

            with st.spinner("💾 Saving to history..."):
                if add_to_history(output):
                    st.success("✅ Analysis saved to Google Sheets")
                else:
                    st.warning("⚠️ Saved to session only (Google Sheets unavailable)")

            json_bytes = json.dumps(output, indent=2, ensure_ascii=False).encode("utf-8")
            filename = f"yureeka_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

            st.download_button(
                label="💾 Download Analysis JSON",
                data=json_bytes,
                file_name=filename,
                mime="application/json"
            )

            render_dashboard(
            primary_data,
            final_conf,
            web_context,
            base_conf,
            query,
            veracity_scores,
            web_context.get("source_reliability", [])
            )


            with st.expander("🔧 Debug Information"):
                st.write("**Confidence Breakdown:**")
                st.json({
                    "base_confidence": base_conf,
                    "evidence_score": veracity_scores.get("overall", 0),
                    "final_confidence": final_conf,
                    "veracity_breakdown": veracity_scores
                })
                st.write("**Primary Model Response:**")
                st.json(primary_data)

    # =====================
    # TAB 2: EVOLUTION ANALYSIS
    # =====================
    with tab2:
        st.markdown("""
        ### 📈 Track the evolution of key metrics over time using **deterministic source-anchored analysis**.

        **How it works:**
        - Select a baseline from your history (stored in Google Sheets)
        - Re-fetches the **exact same sources** from that analysis
        - Extracts current numbers using regex (no LLM variance)
        - Computes deterministic diffs with context-aware matching
        """)

        with st.sidebar:
            st.subheader("📚 History")

            if st.button("🔄 Refresh"):
                st.cache_resource.clear()
                st.rerun()

            sheet = get_google_sheet()
            if sheet:
                st.success("✅ Google Sheets connected")
            else:
                st.warning("⚠️ Using session storage")

            # =====================================================================
            # PATCH FIX40 (ADDITIVE): Scenario B control — Force rebuild toggle
            # - Streamlit Cloud UI has no free-text question editing (dropdown-only).
            # - This toggle lets you intentionally bypass the unchanged fastpath so you
            #   can validate the rebuild path + FIX39 publish invariants.
            # - Pure UI flag; no logic changes unless explicitly enabled.
            # =====================================================================
            force_rebuild = st.checkbox(
                "🧪 Force rebuild (ignore snapshot fastpath)",
                value=False,
                key="fix41_force_rebuild_toggle",
                help="Debug only: forces evolution to rebuild even if sources+data are unchanged."
            )
            # =====================================================================

        # ✅ FIX: your codebase uses get_history(), not load_history()
        history = get_history()

        if not history:
            st.info("📭 No previous analyses found. Run an analysis in the 'New Analysis' tab first.")
            return

        baseline_options = [
            f"{i+1}. {h.get('question', 'N/A')}  ({h.get('timestamp', '')})"
            for i, h in enumerate(history)
        ]
        baseline_choice = st.selectbox("Select baseline analysis:", baseline_options)
        baseline_idx = int(baseline_choice.split(".")[0]) - 1
        baseline_data = history[baseline_idx]

        compare_method = st.selectbox(
            "Comparison method:",
            [
                "source-anchored evolution (re-fetch same sources)",
                "another saved analysis (deterministic)",
                "fresh analysis (volatile)"
            ]
        )

        # ============================================================
        # PATCH UI_EXTRA_SOURCES1 (ADDITIVE)
        # ============================================================
        extra_sources_text = st.text_area(
            "Extra source URLs (optional, one per line)",
            placeholder="https://example.com/report\nhttps://another-source.com/page",
            help="Adds these URLs to the admitted source list for this run. Useful to test hash-mismatch rebuilds.",
            height=110,
        )

        compare_data = None
        if "another saved analysis" in compare_method:
            compare_options = [
                f"{i+1}. {h.get('question', 'N/A')}  ({h.get('timestamp', '')})"
                for i, h in enumerate(history) if i != baseline_idx
            ]
            if compare_options:
                compare_choice = st.selectbox("Select comparison analysis:", compare_options)
                compare_idx = int(compare_choice.split(".")[0]) - 1
                compare_data = history[compare_idx]
            else:
                st.warning("No other saved analyses to compare with.")

        st.markdown("---")

        if st.button("🧬 Run Evolution Analysis", type="primary"):

            if "source-anchored evolution" in compare_method:
                evolution_query = baseline_data.get("question", "")
                if not evolution_query:
                    st.error("❌ No question found in baseline.")
                    return

                with st.spinner("🧬 Running source-anchored evolution..."):

                    try:


                        # ============================================================

                        # PATCH INJ_DIAG_EVO_UI (ADDITIVE): pass extra injected URLs + run_id into evolution

                        # ============================================================

                        _evo_run_id = _inj_diag_make_run_id("evo")

                        _extra_urls_evo = []

                        try:

                            for _l in str(extra_sources_text or "").splitlines():

                                _u = _l.strip()

                                if not _u:

                                    continue

                                if _u.startswith("http://") or _u.startswith("https://"):

                                    _extra_urls_evo.append(_u)

                        except Exception:

                            _extra_urls_evo = []


                        results = run_source_anchored_evolution(

                            baseline_data,

                            web_context={

                                "force_rebuild": bool(force_rebuild),

                                "extra_urls": _extra_urls_evo,

                                "diag_run_id": _evo_run_id,

                                "diag_extra_urls_ui_raw": (extra_sources_text or ""),

                            },

                        )

                        # ============================================================


                    except Exception as e:

                        st.error(f"❌ Evolution failed: {e}")

                        return


                interpretation = ""
                try:
                    if results and isinstance(results, dict):
                        interpretation = results.get("interpretation", "") or ""
                except Exception:
                    interpretation = ""

                evolution_output = {
                    "question": evolution_query,
                    "timestamp": datetime.now().isoformat(),
                    "analysis_type": "source_anchored",
                    "previous_timestamp": baseline_data.get("timestamp"),
                    "results": results,
                    "interpretation": {
                        "text": interpretation,
                        "authoritative": False,
                        "source": "llm_optional"
                    }
                }

                st.download_button(
                    label="💾 Download Evolution Report",
                    data=json.dumps(evolution_output, indent=2, ensure_ascii=False).encode("utf-8"),
                    file_name=f"yureeka_evolution_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json"
                )

                # ✅ FIX: guarded renderer to avoid stability_score=None formatting crashes
                render_source_anchored_results(results, evolution_query)

            elif "another saved analysis" in compare_method:
                if compare_data:
                    st.success("✅ Comparing two saved analyses (deterministic)")
                    render_native_comparison(baseline_data, compare_data)
                else:
                    st.error("❌ Please select a comparison analysis")

            else:
                st.warning("⚠️ Running fresh analysis - results may vary")

                query = baseline_data.get("question", "")
                if not query:
                    st.error("❌ No query found")
                    return

                with st.spinner("🌐 Fetching current data..."):
                    # ---- ADDITIVE: pass existing snapshots for reuse (Change #3 wiring) ----
                    existing_snapshots = None

                    try:
                        prev = st.session_state.get("last_analysis")
                        if isinstance(prev, dict):
                            existing_snapshots = (
                                prev.get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("source_results")
                            )
                    except Exception:
                        existing_snapshots = None

                    web_context = fetch_web_context(
                        query,
                        num_sources=3,
                        existing_snapshots=existing_snapshots,
                    )
                    # ----------------------------------------------------------------------


                if not web_context:
                    web_context = {
                        "search_results": [],
                        "scraped_content": {},
                        "summary": "",
                        "sources": [],
                        "source_reliability": []
                    }

                with st.spinner("🤖 Running analysis..."):
                    new_response = query_perplexity(query, web_context)

                if new_response:
                    try:
                        new_parsed = json.loads(new_response)
                        veracity = evidence_based_veracity(new_parsed, web_context)
                        base_conf = float(new_parsed.get("confidence", 75))
                        final_conf = calculate_final_confidence(base_conf, veracity.get("overall", 0))

                        compare_data = {
                            "question": query,
                            "timestamp": datetime.now().isoformat(),
                            "primary_response": new_parsed,
                            "final_confidence": final_conf,
                            "veracity_scores": veracity,
                            "web_sources": web_context.get("sources", [])
                        }

                        add_to_history(compare_data)
                        st.success("✅ Saved to history")

                        render_native_comparison(baseline_data, compare_data)
                    except Exception as e:
                        st.error(f"❌ Failed: {e}")
                else:
                    st.error("❌ Analysis failed")


# ======================================================================
# PATCH SV1/EG1 (ADDITIVE): Schema validation + Evidence gating (analysis)
# - Additive only: does not remove or refactor existing code.
# - Only applied in TAB 1 (New Analysis) via a small post-pass hook.
# - Does NOT alter evolution behavior (no changes to evolution functions).
# ======================================================================

def validate_metric_schema_frozen(metric_schema_frozen: dict) -> dict:
    """
    Validate frozen metric schema for internal consistency.
    Returns: {"ok": bool, "errors": [...], "warnings": [...], "by_key": {...}}
    """
    issues = {"ok": True, "errors": [], "warnings": [], "by_key": {}}

    def _add(kind: str, canonical_key: str, msg: str):
        issues["ok"] = issues["ok"] and (kind != "errors")
        issues[kind].append({"canonical_key": canonical_key, "message": msg})
        issues["by_key"].setdefault(canonical_key, {"errors": [], "warnings": []})
        issues["by_key"][canonical_key][kind].append(msg)

    if not isinstance(metric_schema_frozen, dict):
        _add("errors", "__schema__", "metric_schema_frozen missing or not a dict")
        return issues

    for canonical_key, spec in metric_schema_frozen.items():
        if not isinstance(spec, dict):
            _add("errors", canonical_key, "schema entry not a dict")
            continue

        dim = (spec.get("dimension") or spec.get("measure_kind") or "").lower().strip()
        unit = (spec.get("unit") or spec.get("unit_tag") or "").strip()
        unit_family = (spec.get("unit_family") or spec.get("unit_family_tag") or "").lower().strip()
        name = (spec.get("name") or canonical_key or "").lower()

        # Hard conflict: currency + percent
        if dim in ("currency", "revenue", "market_value", "value") and unit in ("%", "percent", "percentage"):
            _add("errors", canonical_key, "dimension=currency but unit is percent (%)")

        # Soft checks for percent metrics without percent unit
        if ("cagr" in name or dim in ("percent", "percentage", "growth_rate")) and unit and unit not in ("%", "percent", "percentage"):
            _add("warnings", canonical_key, f"percent-like metric but unit='{unit}' (expected '%')")

        # Common drift hazard: CAGR schema includes 'share'
        kw = " ".join([str(x) for x in (spec.get("keywords") or [])]).lower()
        if "cagr" in name and "share" in kw:
            _add("warnings", canonical_key, "CAGR schema keywords include 'share' (risk of mapping share% to CAGR)")

        # Unit family conflicts
        if dim == "currency" and unit_family and unit_family not in ("currency", "money"):
            _add("warnings", canonical_key, f"dimension=currency but unit_family='{unit_family}'")

    return issues


def _metric_evidence_list(metric: dict):
    ev = metric.get("evidence")
    if isinstance(ev, list):
        return ev
    return []


def _synthesize_evidence_from_examples(metric: dict, max_items: int = 5) -> list:
    """
    If metric has value_range.examples (from attribution pass), synthesize evidence records.
    This keeps JSON stable and makes evolution rebuild auditing possible.
    """
    examples = None
    vr = metric.get("value_range")
    if isinstance(vr, dict):
        examples = vr.get("examples")
    if not isinstance(examples, list) or not examples:
        return []

    # Try to use an existing anchor hash function if present
    anchor_fn = globals().get("compute_anchor_hash")
    out = []
    for ex in examples[:max_items]:
        if not isinstance(ex, dict):
            continue
        url = ex.get("source_url") or ex.get("url") or ""
        raw = ex.get("raw") or ""
        ctx = ex.get("context") or ex.get("context_window") or ex.get("snippet") or ""
        ah = ex.get("anchor_hash") or ""
        if not ah and callable(anchor_fn):
            try:
                ah = anchor_fn(url, ctx)
            except Exception:
                ah = ""
        out.append({
            "source_url": url,
            "raw": raw,
            "context_snippet": ctx[:500] if isinstance(ctx, str) else "",
            "anchor_hash": ah,
            "method": "value_range_examples",
        })
    return out


def ensure_metric_has_evidence(metric: dict) -> dict:
    """
    Evidence gating for a single metric:
    - If evidence already exists -> no change
    - Else synthesize from value_range.examples if available
    - Else mark as proxy (do not delete or zero the metric)
    """
    if not isinstance(metric, dict):
        return metric

    ev = _metric_evidence_list(metric)
    if ev:
        return metric

    synth = _synthesize_evidence_from_examples(metric)
    if synth:
        metric["evidence"] = synth
        return metric

    # No evidence at all: mark proxy (do not alter numeric payload)
    metric.setdefault("evidence", [])
    metric["is_proxy"] = True
    metric["proxy_type"] = "evidence_missing"
    metric["proxy_reason"] = "no_evidence_anchors_available"
    metric["proxy_confidence"] = float(metric.get("proxy_confidence") or 0.2)
    return metric


def enforce_evidence_gating(primary_metrics_canonical: dict) -> dict:
    """
    Apply evidence gating across canonical metrics.
    Returns the (mutated) dict for compatibility.
    """
    if not isinstance(primary_metrics_canonical, dict):
        return primary_metrics_canonical

    for k, m in list(primary_metrics_canonical.items()):
        if isinstance(m, dict):
            primary_metrics_canonical[k] = ensure_metric_has_evidence(m)

    return primary_metrics_canonical


def apply_schema_validation_and_evidence_gating(primary_data: dict) -> dict:
    """
    New Analysis post-pass hook:
    - validates metric_schema_frozen
    - evidence-gates primary_metrics_canonical
    - marks schema-conflict metrics as proxy (does not remove anything)
    """
    if not isinstance(primary_data, dict):
        return primary_data

    # Where schema is stored
    schema = (
        primary_data.get("metric_schema_frozen")
        or (primary_data.get("primary_response") or {}).get("metric_schema_frozen")
        or (primary_data.get("results") or {}).get("metric_schema_frozen")
        or {}
    )

    validation = validate_metric_schema_frozen(schema)
    primary_response = primary_data.setdefault("primary_response", {})
    primary_response["schema_validation"] = validation

    # Mark schema-conflict metrics as proxy (additive)
    pmc = primary_data.get("primary_metrics_canonical")
    if isinstance(pmc, dict) and validation.get("by_key"):
        for ck, iss in validation["by_key"].items():
            if ck in pmc and isinstance(pmc[ck], dict) and iss.get("errors"):
                pmc[ck]["is_proxy"] = True
                pmc[ck]["proxy_type"] = "schema_conflict"
                pmc[ck]["proxy_reason"] = "schema_validation_error"
                pmc[ck]["proxy_confidence"] = float(pmc[ck].get("proxy_confidence") or 0.15)
                pmc[ck]["schema_issues"] = {"errors": iss.get("errors", []), "warnings": iss.get("warnings", [])}

    # Evidence gating
    pmc2 = primary_data.get("primary_metrics_canonical")
    if isinstance(pmc2, dict):
        before = sum(1 for v in pmc2.values() if isinstance(v, dict) and _metric_evidence_list(v))
        enforce_evidence_gating(pmc2)
        after = sum(1 for v in pmc2.values() if isinstance(v, dict) and _metric_evidence_list(v))
        prox = sum(1 for v in pmc2.values() if isinstance(v, dict) and v.get("is_proxy"))
        primary_response["evidence_gating_summary"] = {
            "total_metrics": len(pmc2),
            "metrics_with_evidence_before": before,
            "metrics_with_evidence_after": after,
            "metrics_marked_proxy": prox,
        }

    return primary_data



# ===================== PATCH RMS_UNWRAP1 (ADDITIVE) =====================
def _normalize_prev_response_for_rebuild(previous_data):
    """Best-effort normalization of the loaded baseline object for rebuild dispatch.
    - If previous_data is a JSON string, parse it.
    - If it contains nested 'primary_response' as JSON string, parse it.
    - If it contains a top-level wrapper with 'data' or 'results', keep as dict.
    This is additive and only affects rebuild dispatch input normalization.
    """
    import json
    try:
        pd = previous_data
        if isinstance(pd, str) and pd.strip().startswith(("{","[")):
            try:
                pd = json.loads(pd)
            except Exception:
                pd = previous_data
        if isinstance(pd, dict):
            pr = pd.get("primary_response")
            if isinstance(pr, str) and pr.strip().startswith(("{","[")):
                try:
                    pd["primary_response"] = json.loads(pr)
                except Exception:
                    pass
            # Some callers store the main payload under 'data'
            d = pd.get("data")
            if isinstance(d, str) and d.strip().startswith(("{","[")):
                try:
                    pd["data"] = json.loads(d)
                except Exception:
                    pass
        return pd
    except Exception:
        return previous_data
# =================== END PATCH RMS_UNWRAP1 (ADDITIVE) ===================


if __name__ == "__main__":
    main()


# ===================== PATCH RMS_DISPATCH2 (ADDITIVE) =====================
def _get_metric_anchors_any(prev_response: dict) -> dict:
    """Best-effort retrieval of metric_anchors from any plausible location (additive helper)."""
    try:
        if not isinstance(prev_response, dict):
            return {}
        for path in (
            ("metric_anchors",),
            ("results", "metric_anchors"),
            ("primary_response", "metric_anchors"),
            ("primary_response", "results", "metric_anchors"),
        ):
            cur = prev_response
            ok = True
            for k in path:
                if isinstance(cur, dict) and k in cur:
                    cur = cur[k]
                else:
                    ok = False
                    break
            if ok and isinstance(cur, dict) and cur:
                return cur
        return {}
    except Exception:
        return {}

def _coerce_prev_response_any(previous_data):
    """Normalize previous_data into a dict-shaped 'prev_response' for rebuild dispatch (additive helper)."""
    try:
        return previous_data if isinstance(previous_data, dict) else {}
    except Exception:
        return {}
# =================== END PATCH RMS_DISPATCH2 (ADDITIVE) ===================

# =====================================================================
# PATCH FIX16 (ADDITIVE): close analysis↔evolution metric lock-down gaps
# Goals (deterministic, no re-architecture):
#   1) De-year schema keyword scoring for non-year metrics
#   2) Hard unit expectation gating (unitless years can't win currency/percent)
#   3) Absolute anchor priority when anchors exist
# Notes:
#   - Additive only: we define FIX16 rebuild functions and re-wire dispatch
#   - No refetch, no heuristics beyond schema/unit/anchors
# =====================================================================

def _fix16_is_year_token(s: str) -> bool:
    try:
        s2 = str(s or "").strip()
        return bool(re.fullmatch(r"(19\d{2}|20\d{2})", s2))
    except Exception:
        return False


def _fix16_metric_is_year_like(metric_spec: dict, canonical_key: str = "") -> bool:
    """Deterministic allow-list for metrics whose value is genuinely a year."""
    try:
        spec = metric_spec or {}
        blob = " ".join([
            str(canonical_key or ""),
            str(spec.get("name") or ""),
            str(spec.get("canonical_key") or spec.get("canonical_id") or ""),
            " ".join([str(x) for x in (spec.get("keywords") or spec.get("keyword_hints") or []) if x]),
            str(spec.get("dimension") or ""),
        ]).lower()
        # year-ish intents
        return any(k in blob for k in (" year", "year_", "founded", "since", "established", "launch year", "model year"))
    except Exception:
        return False


def _fix16_prune_year_keywords(keywords: list, metric_is_year_like: bool) -> list:
    """Remove YYYY tokens from keyword scoring unless the metric is year-like."""
    try:
        if metric_is_year_like:
            return list(keywords or [])
        out = []
        for k in (keywords or []):
            if _fix16_is_year_token(k):
                continue
            out.append(k)
        return out
    except Exception:
        return list(keywords or [])


def _fix16_expected_dimension(metric_spec: dict) -> str:
    try:
        spec = metric_spec or {}
        dim = (spec.get("dimension") or spec.get("unit_family") or spec.get("expected_unit_family") or "").strip().lower()
        return dim
    except Exception:
        return ""


def _fix16_candidate_has_any_unit(c: dict) -> bool:
    try:
        if not isinstance(c, dict):
            return False
        for k in ("base_unit", "unit", "unit_tag", "unit_family"):
            if str(c.get(k) or "").strip():
                return True
        # raw sometimes carries $ or % even if unit field blank
        raw = str(c.get("raw") or "")
        if "$" in raw or "%" in raw:
            return True
        return False
    except Exception:
        return False


def _fix16_unit_compatible(c: dict, expected_dim: str) -> bool:
    """Hard gate: if schema expects a unit family, candidate must be compatible."""
    try:
        if not expected_dim:
            return True
        if not isinstance(c, dict):
            return False

        # Treat these as requiring explicit unit-ness
        requires_unit = expected_dim in ("currency", "percent", "rate", "ratio")
        if requires_unit and not _fix16_candidate_has_any_unit(c):
            return False

        # If candidate has a unit_family, require match
        cand_fam = (c.get("unit_family") or "").strip().lower()
        if cand_fam:
            return cand_fam == expected_dim

        # Infer from unit/raw when unit_family missing
        u = (c.get("base_unit") or c.get("unit") or c.get("unit_tag") or "").strip().lower()
        raw = str(c.get("raw") or "").lower()

        if expected_dim == "percent":
            return ("%" in u) or ("percent" in u) or ("%" in raw)

        if expected_dim == "currency":
            # currency symbols/codes or magnitude suffix paired with a currency marker in raw
            if any(x in u for x in ("usd", "eur", "gbp", "jpy", "cny", "aud", "sgd", "$", "€", "£", "¥")):
                return True
            if "$" in raw or "usd" in raw or "sgd" in raw or "eur" in raw or "gbp" in raw:
                return True
            # if unit is magnitude only (M/B/K), require a currency marker in raw
            if u in ("m", "b", "k", "t", "mn", "bn", "million", "billion"):
                return ("$" in raw) or ("usd" in raw) or ("sgd" in raw) or ("eur" in raw) or ("gbp" in raw)
            return False

        # Quantity/rate/ratio are tricky; enforce only the unit-presence gate above.
        return True
    except Exception:
        return True


def _fix16_candidate_allowed(c: dict, metric_spec: dict, canonical_key: str = "") -> bool:
    """Compose fix15 exclusion + fix16 hard unit gate + year-token guard."""
    try:
        if not isinstance(c, dict):
            return False

        # Respect fix15 junk/year-only exclusion if present
        fn = globals().get("_candidate_disallowed_for_metric")
        if callable(fn):
            if fn(c, dict(metric_spec or {}, canonical_key=canonical_key)):
                return False

        expected_dim = _fix16_expected_dimension(metric_spec)
        if not _fix16_unit_compatible(c, expected_dim):
            return False

        # Extra deterministic guard: unitless year-like numbers should never compete
        # for non-year metrics even if upstream tagging missed them.
        if not _fix16_metric_is_year_like(metric_spec, canonical_key=canonical_key):
            v = c.get("value_norm")
            u = (c.get("base_unit") or c.get("unit") or "").strip()
            if u == "" and isinstance(v, (int, float)):
                iv = int(v)
                if 1900 <= iv <= 2100:
                    return False

        return True
    except Exception:
        return True


def rebuild_metrics_from_snapshots_with_anchors_fix16(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    FIX16 anchor-aware rebuild:
      - Absolute anchor priority when anchor_hash exists in prev_response.metric_anchors
      - Hard disallow junk/year-like unitless candidates for non-year metrics
      - Hard unit expectation gating for currency/percent/rate/ratio dimensions
    """
    import re

    if not isinstance(prev_response, dict):
        return {}

    metric_anchors = (
        prev_response.get("metric_anchors")
        or (prev_response.get("primary_response") or {}).get("metric_anchors")
        or (prev_response.get("results") or {}).get("metric_anchors")
        or {}
    )
    if not isinstance(metric_anchors, dict) or not metric_anchors:
        return {}

    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )

    # Build deterministic candidate index (anchor_hash -> best candidate)
    fn_idx = globals().get("_es_build_candidate_index_deterministic")
    cand_index = fn_idx(baseline_sources_cache) if callable(fn_idx) else {}

    rebuilt = {}

    for canonical_key, a in (metric_anchors or {}).items():
        if not isinstance(a, dict):
            continue
        ah = a.get("anchor_hash") or a.get("anchor") or ""
        if not ah:
            continue

        spec = (metric_schema.get(canonical_key) if isinstance(metric_schema, dict) else None) or {}
        spec = dict(spec)
        spec.setdefault("name", a.get("name") or canonical_key)
        spec.setdefault("canonical_key", canonical_key)

        # =====================================================================
        # PATCH FIX41AFC48B START — anchor candidate direct-resolve (candidate_id + hash)
        # Try candidate_id first (most stable), then anchor_hash.
        _cid_req = _safe_str(a.get("candidate_id"))
        c = None
        if _cid_req:
            c = cand_id_index.get(_cid_req)
            if not isinstance(c, dict):
                # prefix match for truncated IDs (common in some snapshots)
                _p16 = _cid_req[:16] if len(_cid_req) >= 16 else _cid_req
                _p12 = _cid_req[:12] if len(_cid_req) >= 12 else _cid_req
                c = cand_id_prefix_index.get(_p16) or cand_id_prefix_index.get(_p12)
            if isinstance(c, dict):
                dbg.setdefault("anchor_direct_resolve_fix41afc48", []).append(
                    {"canonical_key": canonical_key, "method": "candidate_id", "candidate_id": _cid_req}
                )
        if not isinstance(c, dict):
            c = cand_index.get(ah)
            if isinstance(c, dict):
                dbg.setdefault("anchor_direct_resolve_fix41afc48", []).append(
                    {"canonical_key": canonical_key, "method": "anchor_hash", "anchor_hash": ah}
                )
        # =====================================================================
        # PATCH FIX41AFC48B END — anchor candidate direct-resolve (candidate_id + hash)

        if not isinstance(c, dict):
            # =============================================================
            # PATCH FIX41AFC55 (ADDITIVE): prev-evidence same-source rescue
            # If anchor_hash/candidate_id lookup misses due to hash drift,
            # try to recover the exact candidate using prev evidence (raw/start_idx)
            # inside the preferred source URL.
            # =============================================================
            try:
                _pm = (primary_metrics_canonical or {}).get(canonical_key) if isinstance(primary_metrics_canonical, dict) else None
                _ev0 = None
                if isinstance(_pm, dict):
                    _evs = _pm.get("evidence") or []
                    if isinstance(_evs, list) and _evs:
                        _ev0 = _evs[0] if isinstance(_evs[0], dict) else None
                _pref_url = _safe_str(a.get("source_url")) or _safe_str((_pm or {}).get("source_url"))
                _c55 = _fix41afc55_resolve_by_prev_evidence(_pref_url, _ev0, candidates) if _pref_url and isinstance(_ev0, dict) else None
                if isinstance(_c55, dict):
                    c = _c55
                    dbg.setdefault("anchor_prev_evidence_rescue_fix41afc55", []).append(
                        {"canonical_key": canonical_key, "method": "prev_evidence", "preferred_url": _pref_url, "raw": _safe_str(_ev0.get("raw"))}
                    )
            except Exception:
                pass
            if not isinstance(c, dict):
                continue
            # =============================================================

        # FIX16 eligibility hard-gates
        if not _fix16_candidate_allowed(c, spec, canonical_key=canonical_key):
            continue

        # =================================================================
        # PATCH FIX41AFC55 (ADDITIVE): schema unit-required missing-unit hard gate (reachable)
        # Prevent unitless candidates (e.g., "170") from being selected for unit-tagged metrics.
        # (Previous FIX41AFC54 insertion landed after a continue and could be unreachable.)
        # =================================================================
        try:
            if _fix41afc33_schema_implies_unit_required(spec) and not _fix41afc54_candidate_has_unit_evidence(c):
                # As a last attempt, try same-source rescue via prev evidence (raw/start_idx)
                _pm = (primary_metrics_canonical or {}).get(canonical_key) if isinstance(primary_metrics_canonical, dict) else None
                _ev0 = None
                if isinstance(_pm, dict):
                    _evs = _pm.get("evidence") or []
                    if isinstance(_evs, list) and _evs and isinstance(_evs[0], dict):
                        _ev0 = _evs[0]
                _pref_url = _safe_str(a.get("source_url")) or _safe_str((_pm or {}).get("source_url"))
                _c55b = _fix41afc55_resolve_by_prev_evidence(_pref_url, _ev0, candidates) if _pref_url and isinstance(_ev0, dict) else None
                if isinstance(_c55b, dict) and _fix41afc54_candidate_has_unit_evidence(_c55b):
                    c = _c55b
                    dbg.setdefault("schema_unit_required_rescue_fix41afc55", []).append(
                        {"canonical_key": canonical_key, "method": "prev_evidence_rescue", "preferred_url": _pref_url}
                    )
                else:
                    continue
        except Exception:
            pass
        # =================================================================


            # =================================================================
            # PATCH FIX41AFC54 (ADDITIVE): schema unit-required missing-unit hard gate
            # Prevent unitless candidates (e.g., "170") from being selected for unit-tagged metrics.
            # =================================================================
            try:
                if _fix41afc33_schema_implies_unit_required(spec) and not _fix41afc54_candidate_has_unit_evidence(c):
                    continue
            except Exception:
                pass
            # =================================================================

        rebuilt[canonical_key] = {
            "canonical_key": canonical_key,
            "name": spec.get("name") or canonical_key,
            "value": c.get("value"),
            "unit": c.get("unit") or "",
            "value_norm": c.get("value_norm"),
            "source_url": c.get("source_url") or "",
            "anchor_hash": (ah if c.get("_anchor_compat_force_schema_anchor_fix41afc41") else (c.get("anchor_hash") or ah)),
            "anchor_hash_raw": c.get("anchor_hash") or "",  # PATCH FIX41AFC41B additive debug
            "evidence": [{
                "source_url": c.get("source_url") or "",
                "raw": c.get("raw") or "",
                "context_snippet": (c.get("context_snippet") or c.get("context") or c.get("context_window") or "")[:400],
                "anchor_hash": c.get("anchor_hash") or ah,
                "method": "anchor_hash_rebuild_fix16",
            }],
            "anchor_used": True,
        }

        return rebuilt


# =====================================================================
# PATCH FIX41AFC20B (ADDITIVE): schema-driven candidate eligibility parity v2
# Why:
# - Some percent/currency metrics can still accept magnitude-tagged candidates
#   (e.g., "2.0 B") when a weaker unit compatibility check passes.
# - New analysis uses schema-driven hard gates; evolution rebuild must match.
# What:
# - Add a lightweight eligibility wrapper that enforces:
#     * percent metrics must have explicit percent evidence
#     * currency metrics must have explicit currency evidence
#   and rejects magnitude-only tags (K/M/B/T) when dimension expects PCT/CUR.
# =====================================================================
def _metric_candidate_is_eligible_v2(metric_schema: dict, cand: dict) -> bool:
    try:
        spec = metric_schema or {}
        c = cand or {}
        raw = str(c.get("raw") or "")
        unit = str(c.get("unit") or "")
        unit_tag = str(c.get("unit_tag") or "")
        unit_family = str(c.get("unit_family") or "")
        measure_kind = str(c.get("measure_kind") or "")
        ctx = str(c.get("context_snippet") or c.get("context") or "")

        # Determine expected dimension using existing FIX16 helper when present
        exp_dim = ""
        try:
            fn = globals().get("_fix16_expected_dimension")
            if callable(fn):
                exp_dim = str(fn(spec) or "")
        except Exception:
            exp_dim = ""

        exp_dim_l = exp_dim.lower().strip()
        raw_u = (raw + " " + unit + " " + ctx).upper()

        # quick helpers
        def _has_pct():
            if "%" in raw_u:
                return True
            if "PERCENT" in raw_u or "PERCENTAGE" in raw_u:
                return True
            if unit_family.lower().strip() in ("percent", "pct"):
                return True
            if unit.strip() in ("%", "percent", "pct"):
                return True
            if measure_kind.lower().startswith("percent"):
                return True
            return False

        def _has_cur():
            # common currency signals (keep conservative; analysis has deeper parsing)
            if "$" in raw or "$" in ctx:
                return True
            for tok in ("USD", "EUR", "GBP", "SGD", "AUD", "CAD", "JPY", "CNY", "RMB", "HKD", "INR", "KRW", "CHF"):
                if tok in raw_u:
                    return True
            if unit_family.lower().strip() in ("currency", "cur"):
                return True
            if measure_kind.lower().startswith("currency"):
                return True
            return False

        # magnitude tag blacklist when expecting pct/cur
        mag_tag = (unit_tag or unit or "").strip().upper()
        is_mag = mag_tag in ("K", "M", "B", "T")

        if exp_dim_l == "percent":
            if not _has_pct():
                return False
            if is_mag and not _has_pct():
                return False
        if exp_dim_l == "currency":
            if not _has_cur():
                return False
            if is_mag and not _has_cur():
                return False

        return True
    except Exception:
        return True
# =====================================================================

# =====================================================================
# PATCH FIX41AFC45 (ADDITIVE): anchor fallback resolver (url + ctx overlap)
# Why:
# - anchor_hash can drift if derived from context windows; analysis anchors then fail to match
# - when direct anchor_hash lookup misses, recover the intended candidate deterministically
#   using the authoritative anchor's (source_url, context_snippet) against snapshot candidates.
# Notes:
# - Used only inside rebuild (non-fastpath).
# - Additive only: does not change extraction; only selection recovery when anchor lookup misses.
# =====================================================================
def _fix41afc45_anchor_fallback_by_url_ctx(
    source_url: str,
    anchor_ctx: str,
    candidates: list,
    spec: dict,
):
    try:
        import re as _re
        if not source_url or not isinstance(candidates, list) or not candidates:
            return None

        # token overlap scorer (deterministic, cheap)
        def _tok(s: str):
            return set(_re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).split())

        a = _tok(anchor_ctx or "")
        if not a:
            return None

        best = None
        best_score = -1
        best_tie = None

        for c in candidates:
            if not isinstance(c, dict):
                continue
            if (c.get("source_url") or "") != source_url:
                continue
            if c.get("is_junk") is True:
                continue

            # eligibility parity (reuse v2 if present)
            try:
                if not _metric_candidate_is_eligible_v2(spec, c):
                    continue
            except Exception:
                pass

            ctx = c.get("context_snippet") or c.get("context") or c.get("context_window") or ""
            t = _tok(ctx)
            if not t:
                continue
            overlap = len(a.intersection(t))
            if overlap <= 0:
                continue

            # Tie-breaker: prefer higher existing context_score, then stable sort key
            cs = 0.0
            try:
                cs = float(c.get("context_score") or 0.0)
            except Exception:
                cs = 0.0

            tie = (overlap, cs)
            if overlap > best_score:
                best = c
                best_score = overlap
                best_tie = tie
            elif overlap == best_score:
                # higher cs wins
                if best_tie is None or tie[1] > best_tie[1]:
                    best = c
                    best_tie = tie

        return best
    except Exception:
        return None



# =====================================================================
# PATCH FIX41AFC54 START
# Anchor resolution hardening (schema-only rebuild):
# - Adds candidate_id fallback when anchor_hash lookup misses (hash drift)
# - Adds preferred-source lock to prevent injected/unrelated URL hijack
# - Adds schema-unit-required missing-unit hard gate (selection-time)
# Additive-only: no refactors, safe no-ops when fields missing.
# =====================================================================

def _fix41afc54_candidate_has_unit_evidence(c: dict) -> bool:
    try:
        if not isinstance(c, dict):
            return False
        # Most common candidate fields in this pipeline
        if str(c.get('unit') or '').strip():
            return True
        if str(c.get('unit_tag') or '').strip():
            return True
        if str(c.get('unit_cmp') or '').strip():
            return True
        uf = str(c.get('unit_family') or '').strip()
        if uf:
            return True
        # percent/currency hints sometimes live in raw/context only
        raw = str(c.get('raw') or '')
        if '%' in raw:
            return True
        return False
    except Exception:
        return False


def _fix41afc54_resolve_anchor_candidate(
    canonical_key: str,
    anchor_obj: dict,
    candidates: list,
    cand_index: dict,
    spec: dict,
) -> tuple:
    """Return (candidate_or_None, method, preferred_url)"""
    try:
        if not isinstance(anchor_obj, dict):
            return (None, '', '')

        preferred_url = str(anchor_obj.get('source_url') or anchor_obj.get('url') or '').strip()
        ah = str(anchor_obj.get('anchor_hash') or '').strip()
        cid = str(anchor_obj.get('candidate_id') or '').strip()
        actx = str(anchor_obj.get('context_snippet') or '')

        # 1) anchor_hash -> cand_index
        if ah and isinstance(cand_index, dict):
            c = cand_index.get(ah)
            if isinstance(c, dict):
                if preferred_url and str(c.get('source_url') or '') != preferred_url:
                    # reject cross-source even if hash hits
                    c = None
                else:
                    try:
                        if _metric_candidate_is_eligible_v2(spec, c):
                            return (c, 'anchor_hash', preferred_url)
                    except Exception:
                        return (c, 'anchor_hash', preferred_url)

        # 2) candidate_id fallback (hash drift): match by explicit candidate_id OR anchor_hash prefix
        if cid and isinstance(candidates, list):
            pool = []
            for c in candidates:
                if not isinstance(c, dict):
                    continue
                if preferred_url and str(c.get('source_url') or '') != preferred_url:
                    continue
                c_cid = str(c.get('candidate_id') or '').strip()
                c_ah = str(c.get('anchor_hash') or '').strip()
                if (c_cid and c_cid == cid) or (c_ah and c_ah.startswith(cid)):
                    try:
                        if not _metric_candidate_is_eligible_v2(spec, c):
                            continue
                    except Exception:
                        pass
                    pool.append(c)
            if pool:
                # deterministic: by anchor_hash, start_idx, raw
                def _k(x: dict):
                    return (
                        str(x.get('anchor_hash') or ''),
                        int(x.get('start_idx') or 0),
                        str(x.get('raw') or ''),
                    )
                pool.sort(key=_k)
                return (pool[0], 'candidate_id', preferred_url)

        # 3) same-source + context overlap fallback (reuse FIX41AFC45 helper)
        if preferred_url and actx and isinstance(candidates, list):
            try:
                c = _fix41afc45_anchor_fallback_by_url_ctx(preferred_url, actx, candidates, spec)
                if isinstance(c, dict):
                    return (c, 'same_source_ctx', preferred_url)
            except Exception:
                pass

        return (None, '', preferred_url)
    except Exception:
        return (None, '', '')

# =====================================================================
# PATCH FIX41AFC54 END

# =====================================================================
# PATCH FIX41AFC55 START
# Anchor compatibility rescue using PREV evidence fields (raw/start_idx/url)
# Why:
# - candidate_id / anchor_hash may drift across hygiene changes
# - prev_response often contains canonical evidence with raw + start/end idx
# This resolver finds the matching candidate inside the preferred source pool
# using deterministic (url, raw, start_idx proximity) matching.
# Additive-only: does not modify hashing/fastpath; safe no-op when fields missing.
# =====================================================================

def _fix41afc55_resolve_by_prev_evidence(preferred_url: str, prev_ev: dict, candidates: list):
    try:
        if not preferred_url or not isinstance(prev_ev, dict) or not isinstance(candidates, list) or not candidates:
            return None
        raw0 = str(prev_ev.get("raw") or "").strip()
        si0 = prev_ev.get("start_idx")
        ei0 = prev_ev.get("end_idx")
        # Allow a small drift window (hygiene edits can shift indices slightly)
        try:
            si0 = int(si0) if si0 is not None else None
        except Exception:
            si0 = None
        try:
            ei0 = int(ei0) if ei0 is not None else None
        except Exception:
            ei0 = None

        pool = []
        for c in candidates:
            if not isinstance(c, dict):
                continue
            if (c.get("source_url") or "") != preferred_url:
                continue
            if c.get("is_junk") is True:
                continue
            if raw0 and str(c.get("raw") or "").strip() != raw0:
                continue
            # score by proximity to prev indices, then existing context_score, then stable tie
            si = c.get("start_idx")
            ei = c.get("end_idx")
            try:
                si = int(si) if si is not None else 0
            except Exception:
                si = 0
            try:
                ei = int(ei) if ei is not None else 0
            except Exception:
                ei = 0
            d = 0
            if si0 is not None:
                d += abs(si - si0)
            if ei0 is not None:
                d += abs(ei - ei0)
            cs = 0.0
            try:
                cs = float(c.get("context_score") or 0.0)
            except Exception:
                cs = 0.0
            tie = (
                d,
                -cs,  # prefer higher context_score
                str(c.get("anchor_hash") or ""),
                str(c.get("raw") or ""),
            )
            pool.append((tie, c))

        if not pool:
            return None
        pool.sort(key=lambda x: x[0])
        # If we have an index target, require drift to be reasonably small (avoid wrong raw duplicates)
        try:
            d0 = pool[0][0][0]
            if (si0 is not None or ei0 is not None) and d0 > 80:
                return None
        except Exception:
            pass
        return pool[0][1]
    except Exception:
        return None

# =====================================================================
# PATCH FIX41AFC55 END
# =====================================================================

# =====================================================================
def rebuild_metrics_from_snapshots_schema_only_fix16(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    FIX16 schema-only rebuild:
      - Removes YYYY tokens from keyword scoring for non-year metrics
      - Hard unit expectation gating
      - Applies fix15 junk/year exclusion + fix16 extra year-token disallow
      - Deterministic selection/tie-breaks
    """
    import re

    if not isinstance(prev_response, dict):
        return {}

    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )
    if not isinstance(metric_schema, dict) or not metric_schema:
        return {}

    # =====================================================================
    # PATCH FIX41AFC21D (ADDITIVE): Anchor-first selection parity in schema-only FIX17
    # Goal:
    #   - If an anchor exists for a canonical metric, prefer the anchored candidate
    #     before any keyword/context scoring (parity with analysis intent).
    # Notes:
    #   - Additive only; does not refactor existing scoring.
    #   - Safe if metric_anchors/candidate_index are missing.
    # =====================================================================
    metric_anchors_fix41afc21d = (
        prev_response.get("metric_anchors")
        or (prev_response.get("primary_response") or {}).get("metric_anchors")
        or (prev_response.get("results") or {}).get("metric_anchors")
        or {}
    )
    fn_idx_fix41afc21d = globals().get("_es_build_candidate_index_deterministic")
    cand_index_fix41afc21d = fn_idx_fix41afc21d(baseline_sources_cache) if callable(fn_idx_fix41afc21d) else {}
    dbg_fix41afc21d = prev_response.setdefault("_evolution_rebuild_debug", {})
    dbg_fix41afc21d.setdefault("schema_only_anchor_overrides_fix41afc21d", [])
    # =====================================================================
    # END PATCH FIX41AFC21D
    # =====================================================================

    # Flatten snapshot candidates (no re-fetch)
    if isinstance(baseline_sources_cache, dict) and isinstance(baseline_sources_cache.get("snapshots"), list):
        sources = baseline_sources_cache.get("snapshots", [])
    elif isinstance(baseline_sources_cache, list):
        sources = baseline_sources_cache
    else:
        sources = []

    candidates = []
    for s in sources:
        if not isinstance(s, dict):
            continue
        url = s.get("source_url") or s.get("url") or ""
        xs = s.get("extracted_numbers")
        if isinstance(xs, list) and xs:
            for c in xs:
                if not isinstance(c, dict):
                    continue
                c2 = dict(c)
                c2.setdefault("source_url", url)
                candidates.append(c2)

    def _norm(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def _cand_sort_key(c: dict):
        try:
            return (
                str(c.get("anchor_hash") or ""),
                str(c.get("source_url") or ""),
                int(c.get("start_idx") or 0),
                str(c.get("raw") or ""),
                str(c.get("unit") or ""),
                float(c.get("value_norm") or 0.0),
            )
        except Exception:
            return ("", "", 0, "", "", 0.0)

    # Deterministic global ordering of candidates
    candidates.sort(key=_cand_sort_key)

    # =================================================================
    # PATCH FIX41AFC65 START (ADDITIVE): snapshot global candidate pool
    # - Each metric iteration can start from the same full pool, then apply
    #   strict preferred-source locks without leaking across metrics.
    # =================================================================
    candidates_all_fix41afc65 = list(candidates) if isinstance(candidates, list) else []
    # =================================================================
    # PATCH FIX41AFC65 END
    # =================================================================

    rebuilt = {}

    for canonical_key, sch in metric_schema.items():
        if not isinstance(sch, dict):
            continue

        spec = dict(sch)
        spec.setdefault("canonical_key", canonical_key)
        spec.setdefault("name", sch.get("name") or canonical_key)

        metric_is_year_like = _fix16_metric_is_year_like(spec, canonical_key=canonical_key)

        keywords = sch.get("keywords") or sch.get("keyword_hints") or []
        if isinstance(keywords, str):
            keywords = [keywords]
        keywords = _fix16_prune_year_keywords(list(keywords), metric_is_year_like)
        kw_norm = [_norm(k) for k in keywords if k]

        expected_dim = _fix16_expected_dimension(spec)

        # =================================================================
        # PATCH FIX41AFC65 START (ADDITIVE): strict preferred-source lock (per-metric)
        # - Prevent injected / unrelated sources from winning anchored metrics
        # - Does NOT affect fastpath/hashing; rebuild-only scope
        # =================================================================
        # Start each metric from the global pool snapshot
        try:
            candidates = candidates_all_fix41afc65
        except Exception:
            pass

        _fx65_preferred_url = ""
        _fx65_preferred_norm = ""
        try:
            _a65 = (metric_anchors_fix41afc21d or {}).get(canonical_key) if isinstance(metric_anchors_fix41afc21d, dict) else None
            if isinstance(_a65, dict):
                _fx65_preferred_url = str(_a65.get("source_url") or _a65.get("url") or "").strip()
            if not _fx65_preferred_url and isinstance(prev_response, dict):
                _pmc65 = (prev_response.get("primary_metrics_canonical")
                          or (prev_response.get("primary_response") or {}).get("primary_metrics_canonical")
                          or {})
                if isinstance(_pmc65, dict):
                    _pm65 = _pmc65.get(canonical_key) or {}
                    if isinstance(_pm65, dict):
                        _fx65_preferred_url = str(_pm65.get("source_url") or _pm65.get("url") or "").strip()
        except Exception:
            _fx65_preferred_url = _fx65_preferred_url or ""

        try:
            _norm_fn65 = globals().get("_fix41afc60_norm_url")
            _fx65_preferred_norm = _norm_fn65(_fx65_preferred_url) if callable(_norm_fn65) else (_fx65_preferred_url or "").lower().rstrip("/")
        except Exception:
            _fx65_preferred_norm = (_fx65_preferred_url or "").lower().rstrip("/")

        _fx65_before = int(len(candidates or []))
        _fx65_after = _fx65_before
        _fx65_preferred_empty = False

        if _fx65_preferred_norm:
            try:
                def _fx65_cand_norm_url(c):
                    u = (c.get("source_url") or c.get("url") or "").strip()
                    if callable(_norm_fn65):
                        return _norm_fn65(u)
                    return (u or "").lower().rstrip("/")
                candidates = [c for c in (candidates or []) if isinstance(c, dict) and (_fx65_cand_norm_url(c) == _fx65_preferred_norm)]
                _fx65_after = int(len(candidates or []))
                _fx65_preferred_empty = (_fx65_after == 0)
            except Exception:
                pass

        try:
            dbg_fix41afc21d.setdefault("fix41afc65_preferred_lock", []).append({
                "canonical_key": canonical_key,
                "preferred_url": _fx65_preferred_url,
                "preferred_norm": _fx65_preferred_norm,
                "before": _fx65_before,
                "after": _fx65_after,
                "preferred_empty": bool(_fx65_preferred_empty),
            })
        except Exception:
            pass
        # =================================================================
        # PATCH FIX41AFC65 END
        # =================================================================

        best = None
        best_tie = None


        # =================================================================
        # PATCH FIX41AFC45 (ADDITIVE): apply anchor override (hash -> candidate),
        # with deterministic fallback (url + ctx overlap) when hash lookup misses.
        # =================================================================
        try:
            _a = (metric_anchors_fix41afc21d or {}).get(canonical_key) or {}
            _ah = _a.get("anchor_hash") if isinstance(_a, dict) else None
            _asrc = _a.get("source_url") if isinstance(_a, dict) else None
            _actx = _a.get("context_snippet") if isinstance(_a, dict) else ""

            _anchor_best = None
            if _ah and isinstance(cand_index_fix41afc21d, dict):
                _anchor_best = cand_index_fix41afc21d.get(_ah)

            if _anchor_best is None and _asrc:
                _anchor_best = _fix41afc45_anchor_fallback_by_url_ctx(
                    _asrc, _actx, candidates, spec
                )

            if isinstance(_anchor_best, dict):
                best = dict(_anchor_best)
                try:
                    best["_fix41afc45_anchor_used"] = True
                except Exception:
                    pass
                try:
                    dbg_fix41afc21d.setdefault("schema_only_anchor_overrides_fix41afc21d", []).append({
                        "canonical_key": canonical_key,
                        "used": True,
                        "mode": "anchor_hash" if (_ah and isinstance(cand_index_fix41afc21d, dict) and cand_index_fix41afc21d.get(_ah) is not None) else "anchor_fallback_url_ctx",
                        "anchor_hash": _ah or "",
                        "anchor_source_url": _asrc or "",
                    })
                except Exception:
                    pass
        except Exception:
            pass
        # =================================================================
        # END PATCH FIX41AFC45
        # ==============================================================
        # PATCH FIX41AFC54 (ADDITIVE): propagate anchor_used + resolve method
        # ==============================================================
        try:
            if isinstance(best, dict) and best.get("_fix41afc54_anchor_used") is True:
                rebuilt[canonical_key]["anchor_used"] = True
                rebuilt[canonical_key]["anchor_resolve_method"] = str(best.get("_fix41afc54_anchor_method") or "")
                if str(best.get("_fix41afc54_preferred_url") or "").strip():
                    rebuilt[canonical_key]["anchor_preferred_url"] = str(best.get("_fix41afc54_preferred_url") or "").strip()
        except Exception:
            pass
        # ==============================================================
        # END PATCH FIX41AFC54
        # ==============================================================


        # =================================================================
        # PATCH FIX41AFC54 (ADDITIVE): robust anchor override + source lock + best_tie sentinel
        # - If anchor exists, resolve via anchor_hash then candidate_id fallback then same-source ctx
        # - If resolved, set best + best_tie sentinel so scoring loop cannot overwrite
        # =================================================================
        try:
            _a54 = (metric_anchors_fix41afc21d or {}).get(canonical_key) or {}
            _cand54, _method54, _pref54 = _fix41afc54_resolve_anchor_candidate(
                canonical_key=canonical_key,
                anchor_obj=_a54,
                candidates=candidates,
                cand_index=cand_index_fix41afc21d,
                spec=spec,
            )
            if isinstance(_cand54, dict):
                best = dict(_cand54)
                try:
                    best['_fix41afc54_anchor_used'] = True
                    best['_fix41afc54_anchor_method'] = _method54 or 'anchor'
                    best['_fix41afc54_preferred_url'] = _pref54 or ''
                except Exception:
                    pass
                # Sentinel tie so later keyword scoring cannot override anchored best
                try:
                    best_tie = (-1000000000,) + _cand_sort_key(best)
                except Exception:
                    best_tie = (-1000000000,) + (str(best.get('anchor_hash') or ''),)

                try:
                    dbg_fix41afc21d.setdefault('schema_only_anchor_overrides_fix41afc21d', []).append({
                        'canonical_key': canonical_key,
                        'used': True,
                        'mode': 'fix41afc54_' + str(_method54 or ''),
                        'anchor_hash': str(_a54.get('anchor_hash') or ''),
                        'candidate_id': str(_a54.get('candidate_id') or ''),
                        'anchor_source_url': str(_pref54 or ''),
                    })
                except Exception:
                    pass
        except Exception:
            pass
        # =================================================================
        # END PATCH FIX41AFC54
        # =================================================================
        # =================================================================

        
        # =================================================================
        
# =================================================================
        # =================================================================
        # PATCH FIX41AFC59 START (ADDITIVE): Wire canonical selector into EVO schema-only rebuild
        #
        # Goal:
        #   Make Evolution downstream selection call the shared canonical selector,
        #   so anchors + preferred-source + eligibility parity are applied BEFORE
        #   the legacy keyword scoring scan.
        #
        # Notes:
        #   - Additive only: does not remove/modify existing scoring logic.
        #   - When selector returns a best candidate, we set best/best_tie in a way
        #     that preserves determinism and allows FIX41AFC57 to lock the winner.
        # =================================================================
        try:
            if globals().get("ENABLE_CANONICAL_SELECTOR_FIX41AFC58") is True and callable(globals().get("_select_current_metric_canonical_v1")):
                _prev_metric_fix41afc59 = None
                try:
                    _pmc = (prev_response.get("primary_metrics_canonical") or (prev_response.get("primary_response") or {}).get("primary_metrics_canonical") or {})
                    if isinstance(_pmc, dict):
                        _prev_metric_fix41afc59 = _pmc.get(canonical_key)
                except Exception:
                    _prev_metric_fix41afc59 = None

                # =====================================================================
                # PATCH FIX41AFC68 START (ADDITIVE): seed prev_metric from prev_response.results.metric_changes
                #
                # Why:
                # - In several evolution baselines, primary_metrics_canonical may be missing or
                #   incomplete, even though the diff payload persisted the metric's preferred
                #   source_url in results.metric_changes.
                # - When prev_metric is missing, the canonical selector cannot derive a preferred_url,
                #   allowing the legacy scorer to roam into injected/unrelated sources.
                #
                # What:
                # - If _prev_metric_fix41afc59 is empty, attempt to locate canonical_key in
                #   prev_response['results']['metric_changes'] and build a minimal prev_metric dict.
                # - This is strictly additive and only affects the canonical selector wiring path.
                # =====================================================================
                try:
                    if not isinstance(_prev_metric_fix41afc59, dict) or not (_prev_metric_fix41afc59.get("source_url") or _prev_metric_fix41afc59.get("url")):
                        _res68 = (prev_response or {}).get("results") or {}
                        _mc68 = _res68.get("metric_changes") or []
                        if isinstance(_mc68, list):
                            for _row68 in _mc68:
                                if not isinstance(_row68, dict):
                                    continue
                                if str(_row68.get("canonical_key") or "") != str(canonical_key):
                                    continue
                                _prev_metric_fix41afc59 = {
                                    "source_url": (_row68.get("source_url") or _row68.get("url") or "").strip(),
                                    "unit": (_row68.get("unit") or "").strip(),
                                    "unit_tag": (_row68.get("unit_tag") or _row68.get("unit") or "").strip(),
                                    "value": _row68.get("previous_value"),
                                    "value_norm": _row68.get("prev_value_norm"),
                                    "context_snippet": (_row68.get("context_snippet") or "").strip(),
                                }
                                try:
                                    dbg_fix41afc21d.setdefault("fix41afc68_prev_metric_seed", []).append({
                                        "canonical_key": canonical_key,
                                        "seeded": True,
                                        "source_url": _prev_metric_fix41afc59.get("source_url") or "",
                                    })
                                except Exception:
                                    pass
                                break
                except Exception:
                    pass
                # PATCH FIX41AFC68 END

                _sel_best_fix41afc59, _sel_meta_fix41afc59 = _select_current_metric_canonical_v1(
                    canonical_key=canonical_key,
                    metric_schema=spec,
                    candidates=candidates,
                    metric_anchors=(metric_anchors_fix41afc21d or {}),
                    prev_metric=(_prev_metric_fix41afc59 or {}),
                    web_context=(web_context or {}),
                    cand_index=(cand_index_fix41afc21d or {}),
                    prev_response=(prev_response or {}),
                )

                if isinstance(_sel_best_fix41afc59, dict):
                    best = dict(_sel_best_fix41afc59)
                    # mark as anchor-used when selector says so; this enables FIX41AFC57 lock.
                    try:
                        if isinstance(_sel_meta_fix41afc59, dict) and _sel_meta_fix41afc59.get("anchor_used") is True:
                            best["_fix41afc59_anchor_used"] = True
                    except Exception:
                        pass
                    # deterministic best_tie sentinel
                    try:
                        best_tie = (-1500000000,) + (str(best.get("anchor_hash") or ""), str(best.get("source_url") or ""))
                    except Exception:
                        best_tie = (-1500000000,)

                    # attach selector meta for downstream debug / dashboard
                    try:
                        dbg_fix41afc21d.setdefault("canonical_selector_fix41afc59", []).append({
                            "canonical_key": canonical_key,
                            "used": True,
                            "preferred_url": str(((_sel_meta_fix41afc59 or {}).get("preferred_url") or "")),
                            "anchor_used": bool((_sel_meta_fix41afc59 or {}).get("anchor_used")),
                            "anchor_resolve_method": str(((_sel_meta_fix41afc59 or {}).get("anchor_resolve_method") or "")),
                            "blocked_reason": str(((_sel_meta_fix41afc59 or {}).get("blocked_reason") or "")),
                        })
                    except Exception:
                        pass
        except Exception:
            pass
        # =================================================================
                # =====================================================================
        # PATCH FIX41AFC64 START (ADDITIVE): Preferred-source hard lock when canonical selector misses
        #
        # Why:
        # - We observed canonical selector returning no best candidate (e.g., due to eligibility gating),
        #   but the legacy fallback selector then 'roams' across the full candidate pool and latches onto
        #   injected/unrelated sources (e.g., GlobeNewswire). This produces nonsense candidates that are
        #   later hard-blocked, leaving the dashboard blank.
        #
        # What:
        # - If canonical selector emits a preferred_url AND fails to return a best candidate, we
        #   restrict the downstream (legacy) candidate pool to that preferred_url only.
        # - This is deterministic, additive, and preserves fastpath/hashing behavior.
        #
        # Effect:
        # - Prevents cross-source hijack when anchors are missing/unstable and eligibility is strict.
        # =====================================================================
        if not isinstance(_sel_best_fix41afc59, dict):
            try:
                _fx64_pref = ""
                _fx64_pref_norm = ""
                if isinstance(_sel_meta_fix41afc59, dict):
                    _fx64_pref = str(_sel_meta_fix41afc59.get("preferred_url") or "").strip()
                if _fx64_pref:
                    fn_norm = globals().get("_fix41afc60_norm_url")
                    if callable(fn_norm):
                        _fx64_pref_norm = fn_norm(_fx64_pref)
                    else:
                        _fx64_pref_norm = _fx64_pref.rstrip("/").lower()

                    _fx64_before = int(len(candidates or []))
                    try:
                        candidates = [
                            c for c in (candidates or [])
                            if isinstance(c, dict) and (
                                (callable(fn_norm) and fn_norm((c.get("source_url") or c.get("url") or "").strip()) == _fx64_pref_norm)
                                or ((not callable(fn_norm)) and ((c.get("source_url") or c.get("url") or "").strip().rstrip("/").lower() == _fx64_pref_norm))
                            )
                        ]
                    except Exception:
                        candidates = candidates

                    try:
                        dbg_fix41afc21d.setdefault("fix41afc64", []).append({
                            "canonical_key": canonical_key,
                            "preferred_url": _fx64_pref,
                            "preferred_norm": _fx64_pref_norm,
                            "candidates_before": _fx64_before,
                            "candidates_after": int(len(candidates or [])),
                            "selector_blocked_reason": (_sel_meta_fix41afc59.get("blocked_reason") if isinstance(_sel_meta_fix41afc59, dict) else ""),
                        })
                    except Exception:
                        pass
            except Exception:
                pass
        # ============================ END PATCH FIX41AFC64 ============================
# PATCH FIX41AFC59 END
        # =================================================================
# =================================================================

        # PATCH FIX41AFC57 START (ADDITIVE): Anchor-locked selection (prevent roam / override)
        # Problem:
        #   - Even when an anchor override resolves an eligible candidate from the preferred source,
        #     the subsequent keyword-scoring scan can still override it with a higher-scoring
        #     cross-source candidate (e.g., injected GlobeNewswire "170" / "$1.3B"), causing
        #     unit_mismatch hard-blocks and blank "Current" values.
        # Fix:
        #   - When an anchor override has been applied for this canonical_key, temporarily disable
        #     the scoring scan by swapping the candidate iterator to an empty list.
        #   - Restore the original candidates list immediately after the scan region.
        # Notes:
        #   - Additive-only: does not alter fastpath or hashing logic.
        #   - Deterministic: anchor-first candidate remains the winner when eligible.
        _fix41afc57_candidates_saved = candidates
        _fix41afc57_anchor_locked = False
        try:
            if isinstance(best, dict) and (
                best.get("_fix41afc54_anchor_used") is True
                or best.get("_fix41afc45_anchor_used") is True
                or best.get("_fix41afc54_anchor_method")
                or best.get("_fix41afc45_anchor_used")
            ):
                _fix41afc57_anchor_locked = True
        except Exception:
            _fix41afc57_anchor_locked = False

        
        # =================================================================
        # =================================================================
        # PATCH FIX41AFC59C START (ADDITIVE): Treat canonical-selector anchor_used as lock signal
        # =================================================================
        try:
            if isinstance(best, dict) and best.get("_fix41afc59_anchor_used") is True:
                _fix41afc57_anchor_locked = True
        except Exception:
            pass
        # =================================================================
        # PATCH FIX41AFC59C END
        # =================================================================

        if _fix41afc57_anchor_locked:
            try:
                candidates = []
                dbg_fix41afc21d.setdefault("fix41afc57_anchor_locked", []).append({
                    "canonical_key": canonical_key,
                    "locked": True,
                    "anchor_hash": str((metric_anchors_fix41afc21d or {}).get(canonical_key, {}).get("anchor_hash") or ""),
                    "preferred_url": str((metric_anchors_fix41afc21d or {}).get(canonical_key, {}).get("source_url") or ""),
                })
            except Exception:
                pass
        # =================================================================
        # PATCH FIX41AFC57 END
        # =================================================================

        for c in candidates:
            # FIX16 hard eligibility gates
            if not _fix16_candidate_allowed(c, spec, canonical_key=canonical_key):
                continue

            # =================================================================
            # PATCH FIX41AFC54 (ADDITIVE): schema unit-required missing-unit hard gate
            # Prevent unitless candidates (e.g., "170") from being selected for unit-tagged metrics.
            # =================================================================
            try:
                if _fix41afc33_schema_implies_unit_required(spec) and not _fix41afc54_candidate_has_unit_evidence(c):
                    continue
            except Exception:
                pass
            # =================================================================


            # =================================================================
            # PATCH FIX41AFC20B (ADDITIVE): eligibility parity wrapper (v2)
            # =================================================================
            try:
                if not _metric_candidate_is_eligible_v2(spec, c):
                    continue
            except Exception:
                pass
            # =================================================================

            # keyword relevance
            ctx = _norm(c.get("context_snippet") or c.get("context") or c.get("context_window") or "")
            raw = _norm(c.get("raw") or "")

            hits = 0
            for k in kw_norm:
                if k and (k in ctx or k in raw):
                    hits += 1

            # If there are no keyword hits at all, keep as weak fallback only if unit family matches strongly
            # but do not select zero-hit candidates over hit candidates.
            tie = (-hits,) + _cand_sort_key(c)
            if best is None or tie < best_tie:
                best = c
                best_tie = tie

        
        # =================================================================
        # PATCH FIX41AFC57_RESTORE START (ADDITIVE): restore candidate iterator after anchor lock
        try:
            candidates = _fix41afc57_candidates_saved
        except Exception:
            pass
        # PATCH FIX41AFC57_RESTORE END
        # =================================================================

        # PATCH FIX41AFC57B START (BUGFIX): fix indentation after restore block
        if not isinstance(best, dict):
            continue
        # PATCH FIX41AFC57B END

        # Require at least one keyword hit unless the schema has no keywords
        if kw_norm:
            if best_tie is not None and isinstance(best_tie, tuple):
                try:
                    if (-best_tie[0]) <= 0:
                        continue
                except Exception:
                    pass

        # =================================================================
        # PATCH FIX41AFC65 START (ADDITIVE): unit-evidence-required hard gate + safe carry-forward
        # =================================================================
        def _fix41afc65_unit_evidence_required(schema: dict) -> bool:
            try:
                fam = (schema or {}).get("unit_family") or ""
                unit_tag = (schema or {}).get("unit_tag") or ""
                if fam in ("percent", "currency"):
                    return True
                if fam == "magnitude" and str(unit_tag).strip():
                    return True
            except Exception:
                return False
            return False

        def _fix41afc65_has_unit_evidence(c: dict) -> bool:
            try:
                if not isinstance(c, dict):
                    return False
                u = (c.get("unit_cmp") or c.get("unit_norm") or c.get("unit") or "").strip()
                if u:
                    return True
                # explicit tokens
                if (c.get("currency") or c.get("currency_token")):
                    return True
                raw = str(c.get("raw") or "")
                if "%" in raw:
                    return True
                if (c.get("scale_token") or c.get("scale") or c.get("unit_tag")):
                    return True
                ctx = str(c.get("context_snippet") or c.get("context") or c.get("context_window") or "")
                if any(tok in ctx.lower() for tok in ["million", "mn", "mio", "billion", "bn", "trillion", "tn"]):
                    return True
            except Exception:
                return False
            return False

        # If best exists but lacks required unit evidence, drop it deterministically
        try:
            if isinstance(best, dict) and _fix41afc65_unit_evidence_required(spec) and (not _fix41afc65_has_unit_evidence(best)):
                dbg_fix41afc21d.setdefault("fix41afc65_unit_evidence_block", []).append({
                    "canonical_key": canonical_key,
                    "reason": "unit_evidence_missing_hard_block",
                    "picked_url": str(best.get("source_url") or best.get("url") or ""),
                    "picked_raw": str(best.get("raw") or ""),
                    "schema_unit_family": str(spec.get("unit_family") or ""),
                    "schema_unit_tag": str(spec.get("unit_tag") or ""),
                })
                best = None
                best_tie = None
        except Exception:
            pass

        # If no eligible best AND we had a preferred source lock, carry forward previous metric safely
        try:
            if best is None and _fx65_preferred_norm:
                _pmc65b = (prev_response.get("primary_metrics_canonical")
                           or (prev_response.get("primary_response") or {}).get("primary_metrics_canonical")
                           or {})
                _prevm65b = _pmc65b.get(canonical_key) if isinstance(_pmc65b, dict) else None
                if isinstance(_prevm65b, dict) and isinstance(_prevm65b.get("value_norm"), (int, float)):
                    best = {
                        "value": _prevm65b.get("value"),
                        "value_norm": _prevm65b.get("value_norm"),
                        "unit": _prevm65b.get("unit") or _prevm65b.get("unit_tag") or "",
                        "unit_cmp": _prevm65b.get("unit") or _prevm65b.get("unit_tag") or "",
                        "unit_tag": _prevm65b.get("unit_tag") or "",
                        "unit_family": _prevm65b.get("unit_family") or "",
                        "source_url": _fx65_preferred_url or (_prevm65b.get("source_url") or ""),
                        "anchor_hash": _prevm65b.get("anchor_hash") or "",
                        "raw": _prevm65b.get("raw") or "",
                        "_fix41afc65_carry_forward": True,
                        "_fix41afc65_blocked_reason": "carry_forward_preferred_source_no_eligible_candidate",
                    }
                    dbg_fix41afc21d.setdefault("fix41afc65_carry_forward", []).append({
                        "canonical_key": canonical_key,
                        "preferred_url": _fx65_preferred_url,
                        "prev_value_norm": _prevm65b.get("value_norm"),
                    })
        except Exception:
            pass
        # =================================================================
        # PATCH FIX41AFC65 END
        # =================================================================
        rebuilt[canonical_key] = {
            "canonical_key": canonical_key,
            "name": spec.get("name") or canonical_key,
            "value": best.get("value"),
            "unit": best.get("unit") or "",
            "value_norm": best.get("value_norm"),
            "source_url": best.get("source_url") or "",
            "anchor_hash": best.get("anchor_hash") or "",
            "evidence": [{
                "source_url": best.get("source_url") or "",
                "raw": best.get("raw") or "",
                "context_snippet": (best.get("context_snippet") or best.get("context") or best.get("context_window") or "")[:400],
                "anchor_hash": best.get("anchor_hash") or "",
                "method": "schema_only_rebuild_fix16",
            }],
            "anchor_used": False,
        }

        # ==============================================================
        # PATCH FIX41AFC45 (ADDITIVE): propagate anchor_used + anchor meta
        # ==============================================================
        try:
            if isinstance(best, dict) and best.get("_fix41afc45_anchor_used") is True:
                rebuilt[canonical_key]["anchor_used"] = True
                rebuilt[canonical_key]["anchor_hash"] = best.get("anchor_hash") or rebuilt[canonical_key].get("anchor_hash") or ""
                rebuilt[canonical_key]["anchor_source_url"] = best.get("source_url") or rebuilt[canonical_key].get("source_url") or ""
        except Exception:
            pass
        # ==============================================================
        # END PATCH FIX41AFC45

# ==============================================================
        # ==============================================================
        # PATCH FIX41AFC59B START (ADDITIVE): propagate canonical selector anchor_used/meta
        # ==============================================================
        try:
            if isinstance(best, dict) and best.get("_fix41afc59_anchor_used") is True:
                rebuilt[canonical_key]["anchor_used"] = True
                rebuilt[canonical_key]["anchor_resolve_method"] = (rebuilt[canonical_key].get("anchor_resolve_method") or "canonical_selector_fix41afc59")
        except Exception:
            pass
        # ==============================================================
        # PATCH FIX41AFC59B END
        # ==============================================================
# ==============================================================
        # ==============================================================
        # PATCH FIX41AFC54 (ADDITIVE): propagate anchor_used + resolve method (post-emit)
        # ==============================================================
        try:
            if isinstance(best, dict) and best.get("_fix41afc54_anchor_used") is True:
                rebuilt[canonical_key]["anchor_used"] = True
                rebuilt[canonical_key]["anchor_resolve_method"] = str(best.get("_fix41afc54_anchor_method") or "")
                if str(best.get("_fix41afc54_preferred_url") or "").strip():
                    rebuilt[canonical_key]["anchor_preferred_url"] = str(best.get("_fix41afc54_preferred_url") or "").strip()
        except Exception:
            pass
        # ==============================================================
        # END PATCH FIX41AFC54
        # ==============================================================

        # ==============================================================

    return rebuilt


# =====================================================================
# PATCH FIX16 (ADDITIVE): wire FIX16 rebuilds into the existing dispatch
# - Keep names identical so evolution uses these as the LAST definitions
# - We expose both functions while preserving older ones for reference
# =====================================================================

def rebuild_metrics_from_snapshots_with_anchors(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:  # noqa: F811
    return rebuild_metrics_from_snapshots_with_anchors_fix16(prev_response, baseline_sources_cache, web_context=web_context)


def rebuild_metrics_from_snapshots_schema_only(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:  # noqa: F811
    return rebuild_metrics_from_snapshots_schema_only_fix16(prev_response, baseline_sources_cache, web_context=web_context)

# =====================================================================
# END PATCH FIX16
# =====================================================================


# =====================================================================
# PATCH FIX17 (ADDITIVE): end the "circles" by making fallback explicit,
# tightening unit gating (even when schema dimension is missing),
# and logging anchor rejection reasons deterministically.
#
# Goals:
#   1) If anchors exist but are not used, record *why* (no silent fallback)
#   2) Enforce "unit-required" for currency/percent/rate/ratio even when
#      schema.dimension is blank by inferring expected_dim from schema/name/key
#   3) Make "bare year" candidates (1900-2100) absolutely ineligible for
#      non-year metrics, even if upstream tagging/unit_family misfires.
#
# Notes:
#   - Additive only: we introduce FIX17 helpers + rebuild functions,
#     then re-wire the public rebuild names at the end (last definition wins).
#   - No refetch. No heuristics beyond schema/keywords/units/anchors.
# =====================================================================

def _fix17_expected_dimension(metric_spec: dict, canonical_key: str = "") -> str:
    """
    Infer expected dimension when schema doesn't provide it.
    Deterministic rules:
      - if schema.dimension/unit_family present -> use it
      - else infer from canonical_key/name/keywords/unit strings
    """
    try:
        spec = metric_spec or {}
        dim = (spec.get("dimension") or spec.get("unit_family") or spec.get("expected_unit_family") or "").strip().lower()
        if dim:
            return dim

        blob = " ".join([
            str(canonical_key or ""),
            str(spec.get("canonical_key") or spec.get("canonical_id") or ""),
            str(spec.get("name") or ""),
            " ".join([str(x) for x in (spec.get("keywords") or spec.get("keyword_hints") or []) if x]),
            str(spec.get("unit") or spec.get("units") or ""),
        ]).lower()

        # canonical suffixes commonly used in your registry
        if "__currency" in blob or " currency" in blob or "usd" in blob or "sgd" in blob or "eur" in blob or "gbp" in blob:
            return "currency"
        if "__percent" in blob or " percent" in blob or "percentage" in blob or "%" in blob or "yoy" in blob or "cagr" in blob or "growth" in blob:
            return "percent"
        if "__rate" in blob or " rate" in blob:
            return "rate"
        if "__ratio" in blob or " ratio" in blob:
            return "ratio"

        return ""
    except Exception:
        return ""


def _fix17_candidate_has_explicit_unit(c: dict) -> bool:
    """
    Stricter than fix16: we only consider the candidate 'unit-bearing' if it has
    a unit/base_unit/unit_tag/unit_family OR raw contains explicit $/%/currency code.
    """
    try:
        if not isinstance(c, dict):
            return False

        for k in ("base_unit", "unit", "unit_tag", "unit_family"):
            if str(c.get(k) or "").strip():
                return True

        raw = str(c.get("raw") or "")
        raw_l = raw.lower()
        if any(sym in raw for sym in ("$", "€", "£", "¥", "%")):
            return True
        if any(code in raw_l for code in (" usd", "sgd", " eur", " gbp", " aud", " jpy", " cny", " cad")):
            return True

        return False
    except Exception:
        return False


def _fix17_is_bare_year_candidate(c: dict) -> bool:
    """Detect a naked year token candidate, regardless of measure_kind/unit_family."""
    try:
        if not isinstance(c, dict):
            return False
        v = c.get("value_norm")
        if not isinstance(v, (int, float)):
            return False
        iv = int(v)
        if not (1900 <= iv <= 2100):
            return False

        raw = str(c.get("raw") or "").strip()
        # If raw isn't a clean year token, treat it as not "bare year"
        if not re.fullmatch(r"(19\d{2}|20\d{2})", raw):
            return False

        # Must be unitless / not explicitly unit-bearing
        if _fix17_candidate_has_explicit_unit(c):
            return False

        return True
    except Exception:
        return False


def _fix17_metric_is_year_like(metric_spec: dict, canonical_key: str = "") -> bool:
    """Reuse fix16 year-like classifier if available; else fall back to fix16's rule."""
    try:
        fn = globals().get("_fix16_metric_is_year_like")
        if callable(fn):
            return bool(fn(metric_spec, canonical_key=canonical_key))
    except Exception:
        pass
    try:
        spec = metric_spec or {}
        blob = " ".join([
            str(canonical_key or ""),
            str(spec.get("name") or ""),
            str(spec.get("canonical_key") or spec.get("canonical_id") or ""),
            " ".join([str(x) for x in (spec.get("keywords") or spec.get("keyword_hints") or []) if x]),
        ]).lower()
        return any(k in blob for k in (" year", "year_", "founded", "since", "established", "launch year", "model year"))
    except Exception:
        return False


def _fix17_candidate_allowed_with_reason(c: dict, metric_spec: dict, canonical_key: str = "") -> tuple:
    """
    Returns: (allowed: bool, reason: str)
    reason is deterministic and suitable for debug logs.
    """
    try:
        if not isinstance(c, dict):
            return (False, "cand_not_dict")

        # Respect fix15 junk exclusion (if present) with a reason
        fn_dis = globals().get("_candidate_disallowed_for_metric")
        if callable(fn_dis):
            try:
                if fn_dis(c, dict(metric_spec or {}, canonical_key=canonical_key)):
                    # expose the most actionable indicator
                    if c.get("is_junk") is True:
                        return (False, "disallowed:is_junk")
                    if c.get("junk_reason"):
                        return (False, f"disallowed:junk_reason:{c.get('junk_reason')}")
                    return (False, "disallowed:fix15")
            except Exception:
                pass

        # Absolute bare-year ban for non-year metrics
        if _fix17_is_bare_year_candidate(c) and not _fix17_metric_is_year_like(metric_spec, canonical_key=canonical_key):
            return (False, "disallowed:bare_year_non_year_metric")

        expected_dim = _fix17_expected_dimension(metric_spec, canonical_key=canonical_key)

        # Hard "requires unit" gate even if schema didn't set dimension
        if expected_dim in ("currency", "percent", "rate", "ratio"):
            if not _fix17_candidate_has_explicit_unit(c):
                return (False, f"disallowed:missing_unit_for:{expected_dim}")

        # Reuse fix16 compatibility gate when available, but with expected_dim from fix17
        fn_u = globals().get("_fix16_unit_compatible")
        if callable(fn_u):
            if not fn_u(c, expected_dim):
                return (False, f"disallowed:unit_incompatible_for:{expected_dim}")

        return (True, "")
    except Exception:
        return (True, "")



# PATCH FIX41AFC28A START
def _fix41afc28_candidate_block_reasons(schema: dict, cand: dict) -> list:
    """Hard-block reasons for candidate eligibility (Evolution).

    Additive-only guardrail:
      - Block junk candidates even if earlier gates accidentally allow them.
      - Treat magnitude as unit-required when schema has unit_tag.
    Returns: list[str] reasons (empty => allowed by this patch)
    """
    import re
    reasons = []
    try:
        sch = schema if isinstance(schema, dict) else {}
        c = cand if isinstance(cand, dict) else {}
        # 1) Junk hard-block
        if bool(c.get("is_junk")):
            jr = c.get("junk_reason") or c.get("junk_tag") or "is_junk"
            reasons.append(f"junk_candidate_hard_block:{jr}")
        # 1b) Phone/contact pattern hard-block (extra safety even if is_junk not set)
        ctx = " ".join([
            str(c.get("raw") or ""),
            str(c.get("context_snippet") or c.get("context") or c.get("context_window") or ""),
        ]).lower()
        if any(k in ctx for k in ["phone", "tel", "telephone", "call", "contact", "email", "investor relations"]):
            if re.search(r"(\+?\d{1,2}[\s\-]?)?(\(?\d{2,4}\)?[\s\-]?)\d{3}[\s\-]?\d{4}", ctx):
                reasons.append("phone_like_contact_block")
            # Tail-4: protect against fragments like '6441' from '+1-888-600-6441'
            if re.search(r"\b\d{4}\b", str(c.get("raw") or "")) and re.search(r"\b\d{3}[\s\-]?\d{4}\b", ctx):
                reasons.append("phone_tail_fragment_block")

        # 2) Magnitude unit-required when schema has unit_tag (e.g., 'million vehicles')
        uf = str(sch.get("unit_family") or "").strip().lower()
        dim = str(sch.get("dimension") or "").strip().lower()
        sch_ut = str(sch.get("unit_tag") or "").strip()
        cand_ut = str(c.get("unit_tag") or c.get("unit") or "").strip()
        if sch_ut and (uf == "magnitude" or dim == "unit_sales"):
            if not cand_ut:
                reasons.append("missing_required_unit_tag")
    except Exception:
        # Fail-open: if helper errors, do not block
        return []
    return reasons



# PATCH FIX41AFC37_HELPERS START
def _fix41afc37_is_bare_year_no_unit(cand: dict) -> bool:
    """Return True if cand looks like a naked calendar year with no unit evidence."""
    try:
        vn = cand.get("value_norm", cand.get("value"))
        # accept float/int strings
        try:
            fv = float(vn)
        except Exception:
            return False
        iv = int(round(fv))
        if abs(fv - iv) > 1e-9:
            return False
        if iv < 1900 or iv > 2100:
            return False
        unit = (cand.get("unit") or "").strip()
        unit_tag = (cand.get("unit_tag") or "").strip()
        base_unit = (cand.get("base_unit") or "").strip()
        fam = (cand.get("unit_family") or "").strip()
        raw = (cand.get("raw") or "").strip()
        # Any explicit unit evidence defeats bare-year classification
        if unit or unit_tag or base_unit or fam:
            return False
        # If raw contains percent/currency symbols, not a naked year
        if any(tok in raw for tok in ["%", "$", "€", "£", "¥"]):
            return False
        return True
    except Exception:
        return False

def _fix41afc37_preselect_eligible(spec: dict, cand: dict, canonical_key: str = ""):
    """Pre-selection eligibility gate to skip ineligible candidates BEFORE scoring.

    Returns (ok: bool, reasons: list[str]).
    """
    reasons = []

    # PATCH FIX41AFC43_ALLOW_UNKNOWN_MAGNITUDE START
    # Treat schema dimension 'unknown' as eligible when unit_family implies magnitude (common for unit_sales/count metrics).
    try:
        _sdim = (spec.get("dimension") or "").strip().lower()
        _suf = (spec.get("unit_family") or "").strip().lower()
        if _sdim == "unknown" and _suf == "magnitude":
            # Normalize to a unit-sales like effective dimension for eligibility checks below (without mutating spec)
            spec = dict(spec)
            spec.setdefault("dimension_effective_fix41afc43", "unit_sales")
    except Exception:
        pass
    # PATCH FIX41AFC43_ALLOW_UNKNOWN_MAGNITUDE END
    try:
        # Always skip explicit junk
        if bool(cand.get("is_junk")):
            jr = cand.get("junk_reason") or "junk"
            reasons.append(f"junk:{jr}")
            return False, reasons
        # Skip naked years (most common '2023' creep vector)
        if _fix41afc37_is_bare_year_no_unit(cand):
            reasons.append("bare_year_no_unit")
            return False, reasons

        # Reuse existing hard-block logic if present (FIX41AFC28)
        fn_block = globals().get("_fix41afc28_candidate_block_reasons")
        if callable(fn_block):
            try:
                r28 = fn_block(spec, cand)
                if r28:
                    reasons.extend([f"block28:{x}" for x in list(r28)[:3]])
                    return False, reasons
            except Exception:
                pass

        # Schema-driven unit-required: if schema implies unit, require unit evidence
        dim = (spec or {}).get("dimension") or ""
        uf = (spec or {}).get("unit_family") or ""
        su = (spec or {}).get("unit") or ""
        sut = (spec or {}).get("unit_tag") or ""
        schema_implies_unit = bool(su or sut or uf in ("percent","currency","magnitude","energy") or dim in ("percent","currency","unit_sales","energy"))
        if schema_implies_unit:
            unit = (cand.get("unit") or "").strip()
            unit_tag = (cand.get("unit_tag") or "").strip()
            base_unit = (cand.get("base_unit") or "").strip()
            raw = (cand.get("raw") or "")
            ctx = (cand.get("context_snippet") or "")
            has_unit_evidence = bool(unit or unit_tag or base_unit) or ("%" in raw) or ("%" in ctx)
            if not has_unit_evidence:
                reasons.append("unit_required_missing")
                return False, reasons

        return True, reasons
    except Exception:
        return True, reasons
# PATCH FIX41AFC37_HELPERS END

# PATCH FIX41AFC38_HELPERS START
def _fix41afc38_guess_unit_family(cand: dict) -> str:
    """Deterministic unit_family guess from existing cand fields."""
    try:
        ut = (cand.get("unit_tag") or cand.get("unit") or cand.get("base_unit") or "").strip().lower()
        raw = (cand.get("raw") or cand.get("raw_disp") or "").lower()
        ctx = (cand.get("context") or cand.get("ctx") or "").lower()
        blob = " ".join([ut, raw, ctx])
        if "%" in blob or " percent" in blob or "percentage" in blob:
            return "percent"
        if any(tok in blob for tok in [" usd", " sgd", " eur", " gbp", " aud", " cad", " chf", " jpy", " cny", " rmb", "$", "€", "£", "¥"]):
            return "currency"
        if any(tok in blob for tok in [" million", " billion", " trillion", " mn", " bn", " tn"]):
            return "magnitude"
        if ut in ("k","m","b","t","mn","bn","tn"):
            return "magnitude"
        if any(tok in blob for tok in ["kwh","mwh","gwh","twh"]):
            return "energy"
    except Exception:
        pass
    return (cand.get("unit_family") or "").strip()

def _fix41afc38_magnitude_context_override(cand: dict) -> bool:
    """Correct magnitude-vs-percent mislabels like 17.8 million being tagged percent."""
    try:
        uf = (cand.get("unit_family") or "").strip().lower()
        ut = (cand.get("unit_tag") or cand.get("unit") or cand.get("base_unit") or "").lower()
        raw = (cand.get("raw") or cand.get("raw_disp") or "").lower()
        ctx = (cand.get("context") or cand.get("ctx") or "").lower()
        blob = " ".join([ut, raw, ctx])
        mag = ("million" in blob) or ("billion" in blob) or ("trillion" in blob) or (ut in ("k","m","b","t","mn","bn","tn"))
        unitsy = any(w in blob for w in [" unit", " units", " vehicle", " vehicles", " sales", " sold", " shipments"])
        if uf == "percent" and mag and unitsy and "%" not in blob:
            cand["unit_family"] = "magnitude"
            cand["_unit_family_overridden_fix41afc38"] = True
            return True
    except Exception:
        pass
    return False

def _fix41afc38_backfill_unit_family_in_list(cands):
    try:
        if not isinstance(cands, list):
            return
        for c in cands:
            if not isinstance(c, dict):
                continue
            if not (c.get("unit_family") or "").strip():
                uf = _fix41afc38_guess_unit_family(c)
                if uf:
                    c["unit_family"] = uf
                    c["_unit_family_backfilled_fix41afc38"] = True
            _fix41afc38_magnitude_context_override(c)
    except Exception:
        pass

def _fix41afc38_prev_value_norm(prev_response: dict, canonical_key: str):
    try:
        pmc = prev_response.get("primary_metrics_canonical") or {}
        if isinstance(pmc, dict) and canonical_key in pmc and isinstance(pmc[canonical_key], dict):
            return pmc[canonical_key].get("value_norm") or pmc[canonical_key].get("cur_value_norm") or pmc[canonical_key].get("value")
    except Exception:
        pass
    return None

def _fix41afc38_scale_sanity_adjust(cand: dict, spec: dict, prev_vn):
    """Deterministic rescale for unit_sales/magnitude when off by ~10x/100x vs prev."""
    try:
        if not isinstance(cand, dict) or not isinstance(spec, dict):
            return
        dim = (spec.get("dimension") or "").lower()
        uf = (spec.get("unit_family") or "").lower()
        if not (dim in ("unit_sales","units","volume") or uf == "magnitude"):
            return
        vn = cand.get("value_norm")
        if vn is None or prev_vn is None:
            return
        vn = float(vn)
        pv = float(prev_vn)
        if pv == 0:
            return
        ratio = vn / pv
        for f in (10.0, 100.0):
            if ratio > (0.8*f) and ratio < (1.2*f):
                cand["_fix41afc38_rescaled"] = True
                cand["_fix41afc38_rescale_factor"] = f
                cand["value_norm"] = vn / f
                cand["_fix41afc38_rescale_reason"] = "ratio_vs_prev"
                return
    except Exception:
        pass
# PATCH FIX41AFC38_HELPERS END

# PATCH FIX41AFC39_UNIT_OUT_BACKFILL START
def _fix41afc39_best_unit_out(cand: dict, spec: dict) -> str:
    """Best-effort deterministic unit string for output metrics (dashboard-safe).
    Prefers base_unit/unit_tag when cand['unit'] is empty.
    """
    try:
        c = cand if isinstance(cand, dict) else {}
        s = spec if isinstance(spec, dict) else {}
        dim = str(s.get("dimension") or "").lower()
        uf_exp = str(s.get("unit_family") or "").lower()

        u = str(c.get("unit") or "").strip()
        ut = str(c.get("unit_tag") or "").strip()
        bu = str(c.get("base_unit") or "").strip()
        raw = str(c.get("raw") or c.get("raw_disp") or "").lower()
        ctx = str(c.get("context_snippet") or c.get("context") or c.get("context_window") or c.get("ctx") or "").lower()
        blob = " ".join([u.lower(), ut.lower(), bu.lower(), raw, ctx])

        if dim == "percent" or uf_exp == "percent":
            if "%" in blob or " percent" in blob or "percentage" in blob:
                return "%"

        if dim == "currency" or uf_exp == "currency":
            for tok in ["usd", "sgd", "eur", "gbp", "aud", "cad", "chf", "jpy", "cny", "rmb", "$", "€", "£", "¥"]:
                if tok in blob:
                    if tok in ("$", "€", "£", "¥"):
                        return tok
                    return tok.upper()

        for v in (bu, ut, u):
            vv = str(v or "").strip()
            if vv:
                return vv

        # Context phrase normalization (aligns with analysis 'million units' style)
        if "trillion" in blob or " tn" in blob:
            return "trillion units" if "unit" in blob else "T"
        if "billion" in blob or " bn" in blob:
            return "billion units" if "unit" in blob else "B"
        if "million" in blob or " mn" in blob:
            return "million units" if "unit" in blob else "M"
        if "thousand" in blob or " k" in blob:
            return "thousand units" if "unit" in blob else "K"
    except Exception:
        pass
    return ""

def _fix41afc39_backfill_candidate_unit_fields(cand: dict):
    """Backfill cand['unit_family'] using context_snippet and set base_unit if missing."""
    try:
        if not isinstance(cand, dict):
            return
        if not (cand.get("unit_family") or "").strip():
            ut = (cand.get("unit_tag") or cand.get("unit") or cand.get("base_unit") or "").strip().lower()
            raw = (cand.get("raw") or cand.get("raw_disp") or "").lower()
            ctx = (cand.get("context_snippet") or cand.get("context") or cand.get("context_window") or cand.get("ctx") or "").lower()
            blob = " ".join([ut, raw, ctx])
            if "%" in blob or " percent" in blob or "percentage" in blob:
                cand["unit_family"] = "percent"
                cand["_unit_family_backfilled_fix41afc39"] = True
            elif any(tok in blob for tok in [" usd", " sgd", " eur", " gbp", " aud", " cad", " chf", " jpy", " cny", " rmb", "$", "€", "£", "¥"]):
                cand["unit_family"] = "currency"
                cand["_unit_family_backfilled_fix41afc39"] = True
            elif any(tok in blob for tok in [" million", " billion", " trillion", " mn", " bn", " tn"]) or ut in ("k","m","b","t","mn","bn","tn"):
                cand["unit_family"] = "magnitude"
                cand["_unit_family_backfilled_fix41afc39"] = True
        if not str(cand.get("base_unit") or "").strip():
            if str(cand.get("unit_tag") or "").strip():
                cand["base_unit"] = str(cand.get("unit_tag") or "")
                cand["_base_unit_backfilled_fix41afc39"] = True
        if not str(cand.get("unit") or "").strip() and str(cand.get("base_unit") or "").strip():
            cand["unit"] = str(cand.get("base_unit") or "")
            cand["_unit_backfilled_fix41afc39"] = True
    except Exception:
        pass
# PATCH FIX41AFC39_UNIT_OUT_BACKFILL END






def _fix41afc28_candidate_pass_reason(schema: dict, cand: dict) -> str:
    """Small debug string explaining why a candidate passed this patch."""
    try:
        sch = schema if isinstance(schema, dict) else {}
        c = cand if isinstance(cand, dict) else {}
        uf = str(sch.get("unit_family") or "").strip().lower()
        dim = str(sch.get("dimension") or "").strip().lower()
        sch_ut = str(sch.get("unit_tag") or "").strip()
        cand_ut = str(c.get("unit_tag") or c.get("unit") or "").strip()
        if bool(c.get("is_junk")):
            return "passed_unexpected_is_junk"  # should have been blocked
        if sch_ut and (uf == "magnitude" or dim == "unit_sales"):
            return "passed_required_unit_tag" if cand_ut else "passed_missing_unit_tag_unexpected"
        return "passed_fix41afc28"
    except Exception:
        return "passed_fix41afc28"
# PATCH FIX41AFC28A END

def rebuild_metrics_from_snapshots_with_anchors_fix17(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    FIX17 anchor-aware rebuild:
      - Absolute anchor priority
      - If anchor cannot be used, log deterministic rejection reason
      - Applies strict unit gating & bare-year exclusion
    """
    if not isinstance(prev_response, dict):
        return {}

    metric_anchors = (
        prev_response.get("metric_anchors")
        or (prev_response.get("primary_response") or {}).get("metric_anchors")
        or (prev_response.get("results") or {}).get("metric_anchors")
        or {}
    )
    if not isinstance(metric_anchors, dict) or not metric_anchors:
        return {}

    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )

    # Deterministic candidate index (anchor_hash -> best candidate)
    fn_idx = globals().get("_es_build_candidate_index_deterministic")
    cand_index = fn_idx(baseline_sources_cache) if callable(fn_idx) else {}

    # =====================================================================
    # PATCH FIX41AFC48A START — candidate_id index for anchor direct-resolve
    # Build a deterministic index: candidate_id -> candidate
    # This allows Evolution to resolve the exact anchored candidate even if
    # anchor_hash drifts due to context-window normalization changes.
    cand_id_index = {}
    cand_id_prefix_index = {}
    try:
        _bsc = baseline_sources_cache if isinstance(baseline_sources_cache, list) else []
        for _sr in _bsc:
            if not isinstance(_sr, dict):
                continue
            _nums = _sr.get("extracted_numbers") or []
            if not isinstance(_nums, list):
                continue
            for _c in _nums:
                if not isinstance(_c, dict):
                    continue
                _cid = str(_c.get("candidate_id") or "").strip()
                if _cid:
                    # First-wins for determinism (do not overwrite existing)
                    cand_id_index.setdefault(_cid, _c)
                    if len(_cid) >= 16:
                        cand_id_prefix_index.setdefault(_cid[:16], _c)
                    if len(_cid) >= 12:
                        cand_id_prefix_index.setdefault(_cid[:12], _c)
                # Also allow anchor_hash as a weak candidate_id fallback when stored that way
                _ah2 = str(_c.get("anchor_hash") or _c.get("anchor") or "").strip()
                if _ah2 and _ah2 not in cand_index:
                    # do not change cand_index itself (off-limits), but we can store as id prefix
                    if len(_ah2) >= 16:
                        cand_id_prefix_index.setdefault(_ah2[:16], _c)
    except Exception:
        cand_id_index = cand_id_index if isinstance(cand_id_index, dict) else {}
        cand_id_prefix_index = cand_id_prefix_index if isinstance(cand_id_prefix_index, dict) else {}
    # =====================================================================
    # PATCH FIX41AFC48A END — candidate_id index for anchor direct-resolve

    # =====================================================================
    # PATCH FIX41AFC51 START — anchor-resolve candidate payload enrichment (analysis parity)
    # Problem observed:
    # - Anchor signature/compat resolvers can return a *thin* candidate (missing context_snippet,
    #   unit_tag/unit_family/base_unit/multiplier/value_norm, etc.), which then cascades into
    #   blank "current" values and incorrect eligibility decisions.
    # Fix:
    # - Deterministically enrich the resolved candidate from the *actual* snapshot pool
    #   (baseline_sources_cache.extracted_numbers) using candidate_id / anchor_hash.
    # - Additive-only: never overwrite existing candidate fields; first-wins.
    # =====================================================================
    def _fix41afc51_enrich_candidate_payload(cand: dict, *, anchor_hash: str = "", anchor_dict: dict = None, canonical_key: str = ""):
        try:
            if not isinstance(cand, dict):
                return cand
            _ah = str(cand.get("anchor_hash") or anchor_hash or "").strip()
            _cid = str(cand.get("candidate_id") or "").strip()
            # also allow anchor dict to supply candidate_id
            try:
                if (not _cid) and isinstance(anchor_dict, dict):
                    _cid = str(anchor_dict.get("candidate_id") or anchor_dict.get("candidate") or "").strip()
            except Exception:
                pass

            # Only enrich when key evidence is missing
            _need = False
            for _k in ("context_snippet","unit_tag","unit_family","base_unit","multiplier_to_base","value_norm","candidate_id"):
                if cand.get(_k) is None or str(cand.get(_k) or "").strip() == "":
                    _need = True
                    break
            if not _need:
                return cand

            _src_c = None
            try:
                if _cid and isinstance(cand_id_index, dict) and _cid in cand_id_index and isinstance(cand_id_index[_cid], dict):
                    _src_c = cand_id_index[_cid]
                if _src_c is None and _cid and isinstance(cand_id_prefix_index, dict):
                    _p16 = _cid[:16] if len(_cid) >= 16 else ""
                    _p12 = _cid[:12] if len(_cid) >= 12 else ""
                    for _p in (_p16, _p12):
                        if _p and _p in cand_id_prefix_index and isinstance(cand_id_prefix_index[_p], dict):
                            _src_c = cand_id_prefix_index[_p]
                            break
                if _src_c is None and _ah and isinstance(cand_index, dict) and _ah in cand_index and isinstance(cand_index[_ah], dict):
                    _src_c = cand_index[_ah]
            except Exception:
                _src_c = None

            if not isinstance(_src_c, dict):
                # Deterministic scan (first-wins) as last resort
                try:
                    _bsc = baseline_sources_cache if isinstance(baseline_sources_cache, list) else []
                    for _sr in _bsc:
                        if not isinstance(_sr, dict):
                            continue
                        _nums = _sr.get("extracted_numbers") or []
                        if not isinstance(_nums, list):
                            continue
                        for _x in _nums:
                            if not isinstance(_x, dict):
                                continue
                            _xah = str(_x.get("anchor_hash") or _x.get("anchor") or "").strip()
                            _xcid = str(_x.get("candidate_id") or "").strip()
                            if (_cid and _xcid and _xcid == _cid) or (_ah and _xah and _xah == _ah):
                                _src_c = _x
                                raise StopIteration
                except StopIteration:
                    pass
                except Exception:
                    pass

            if isinstance(_src_c, dict):
                # Merge missing fields only
                for _k in ("raw","unit","unit_tag","unit_family","base_unit","multiplier_to_base","value_norm","candidate_id","context_snippet","source_url","url","anchor_hash"):
                    if cand.get(_k) is None or (isinstance(cand.get(_k), str) and not cand.get(_k).strip()):
                        if _k in _src_c and _src_c.get(_k) not in (None, ""):
                            cand[_k] = _src_c.get(_k)
                # Backfill unit fields if still missing (existing helper)
                try:
                    _bf = globals().get("_fix41afc39_backfill_candidate_unit_fields")
                    if callable(_bf):
                        _bf(cand)
                except Exception:
                    pass
                # Record debug
                try:
                    dbg.setdefault("anchor_payload_enrich_fix41afc51", []).append({
                        "canonical_key": canonical_key,
                        "anchor_hash": _ah,
                        "candidate_id": _cid,
                        "enriched": True,
                        "had_context": bool(str(cand.get("context_snippet") or "").strip()),
                        "unit_family": cand.get("unit_family"),
                    })
                except Exception:
                    pass
            else:
                try:
                    dbg.setdefault("anchor_payload_enrich_fix41afc51", []).append({
                        "canonical_key": canonical_key,
                        "anchor_hash": _ah,
                        "candidate_id": _cid,
                        "enriched": False,
                        "reason": "no_match_in_snapshot_pool"
                    })
                except Exception:
                    pass

        except Exception:
            pass
        return cand
    # PATCH FIX41AFC51 END
    # =====================================================================
    # PATCH FIX41AFC56 START — preferred-source rescue for anchored metrics (prevents injected-source hijack)
    #
    # Problem:
    #   When anchor_hash drifts (context-window hygiene changes), compat/signature resolvers may
    #   return a candidate from an injected/unrelated URL (e.g., GlobeNewswire), which then gets
    #   hard-blocked by unit gates and blanks the dashboard.
    #
    # Goal:
    #   If a metric is anchored and we know its preferred URL (from metric_anchors or prev canonical),
    #   then any non-preferred candidate MUST be replaced by the best eligible candidate from the
    #   preferred URL when available. This keeps selection aligned to Analysis and stabilizes Current.
    #
    # Notes:
    #   - Additive only; does not touch fastpath or hashing.
    #   - Applies only inside anchor-aware rebuild (FIX17).
    # =====================================================================

    def _fix41afc56_norm_url(u: str) -> str:
        try:
            u = (u or "").strip()
            # keep scheme+host+path, drop fragments/query for stability
            u = re.sub(r"[#?].*$", "", u)
            return u.rstrip("/")
        except Exception:
            return (u or "").strip().rstrip("/")

    def _fix41afc56_tokens(s: str):
        try:
            s = (s or "").lower()
            toks = re.findall(r"[a-z0-9]{3,}", s)
            # small stopword list (deterministic)
            stop = set(["the","and","for","with","from","that","this","into","than","then","are","was","were","has","have","had","its","their","global","sales"])
            return set([t for t in toks if t not in stop])
        except Exception:
            return set()

    def _fix41afc56_preferred_url_for_metric(prev_response: dict, canonical_key: str, anchor_dict: dict) -> str:
        try:
            # 1) anchor dict preferred source_url
            if isinstance(anchor_dict, dict):
                u = anchor_dict.get("source_url") or ""
                if u:
                    return _fix41afc56_norm_url(u)
            # 2) prev canonical metric source_url (analysis authoritative)
            pmc = (prev_response or {}).get("primary_metrics_canonical") or {}
            if isinstance(pmc, dict) and isinstance(pmc.get(canonical_key), dict):
                u = pmc[canonical_key].get("source_url") or ""
                if u:
                    return _fix41afc56_norm_url(u)
        except Exception:
            pass
        return ""

    def _fix41afc56_prev_value_norm(prev_response: dict, canonical_key: str):
        try:
            pmc = (prev_response or {}).get("primary_metrics_canonical") or {}
            if isinstance(pmc, dict) and isinstance(pmc.get(canonical_key), dict):
                v = pmc[canonical_key].get("value_norm")
                if v is None:
                    v = pmc[canonical_key].get("value")
                return float(v) if v is not None else None
        except Exception:
            return None
        return None

    def _fix41afc56_iter_candidates_for_url(baseline_sources_cache, preferred_url: str):
        pu = _fix41afc56_norm_url(preferred_url)
        if not pu:
            return
        _bsc = baseline_sources_cache if isinstance(baseline_sources_cache, list) else []
        for _sr in _bsc:
            if not isinstance(_sr, dict):
                continue
            u = _fix41afc56_norm_url(_sr.get("source_url") or _sr.get("url") or "")
            if not u or u != pu:
                continue
            nums = _sr.get("extracted_numbers") or []
            if not isinstance(nums, list):
                continue
            for _c in nums:
                if isinstance(_c, dict):
                    yield _c

    def _fix41afc56_best_candidate_same_source(spec: dict, anchor_dict: dict, prev_response: dict, baseline_sources_cache, canonical_key: str = ""):
        preferred_url = _fix41afc56_preferred_url_for_metric(prev_response, canonical_key, anchor_dict)
        if not preferred_url:
            return None
        anchor_ctx = ""
        try:
            if isinstance(anchor_dict, dict):
                anchor_ctx = anchor_dict.get("context_snippet") or anchor_dict.get("context") or ""
        except Exception:
            anchor_ctx = ""
        anchor_toks = _fix41afc56_tokens(anchor_ctx)
        prev_vn = _fix41afc56_prev_value_norm(prev_response, canonical_key)

        fn_v2 = globals().get("_metric_candidate_is_eligible_v2")
        best = None
        best_score = None

        for c0 in _fix41afc56_iter_candidates_for_url(baseline_sources_cache, preferred_url):
            c = dict(c0)
            # ensure source_url is present
            c.setdefault("source_url", preferred_url)
            # reject junk early
            if c.get("is_junk") is True:
                continue

            # eligibility gates (reuse existing)
            ok, _r = _fix17_candidate_allowed_with_reason(c, spec, canonical_key=canonical_key)
            if not ok:
                continue
            try:
                if callable(fn_v2) and not bool(fn_v2(spec, c)):
                    continue
            except Exception:
                pass

            # score = context overlap (if available) + numeric proximity (if available)
            ctx = c.get("context_snippet") or c.get("context") or ""
            toks = _fix41afc56_tokens(ctx)
            overlap = 0.0
            if anchor_toks:
                overlap = (len(anchor_toks & toks) / max(1, len(anchor_toks)))

            vn = c.get("value_norm")
            try:
                vn = float(vn) if vn is not None else None
            except Exception:
                vn = None

            prox = 0.0
            if prev_vn is not None and vn is not None and prev_vn != 0:
                prox = -abs(vn - prev_vn) / abs(prev_vn)

            # small boost when unit evidence matches schema family
            uf = str(c.get("unit_family") or "").lower()
            uf_s = str((spec or {}).get("unit_family") or "").lower()
            uf_boost = 0.1 if (uf and uf_s and uf == uf_s) else 0.0

            score = (2.0 * overlap) + prox + uf_boost
            if best_score is None or score > best_score:
                best = c
                best_score = score

        if isinstance(best, dict):
            try:
                best["_fix41afc56_same_source_rescue"] = True
                best["_fix41afc56_preferred_url"] = preferred_url
                best["_fix41afc56_score"] = best_score
            except Exception:
                pass
            return best
        return None

    # =====================================================================
    # PATCH FIX41AFC56 END
    # =====================================================================


    # Debug sink (additive mutation; safe if ignored)
    dbg = prev_response.setdefault("_evolution_rebuild_debug", {})
    rej = dbg.setdefault("anchor_rejects_fix17", [])
    used = dbg.setdefault("anchor_used_fix17", [])

    rebuilt = {}

    for canonical_key, a in (metric_anchors or {}).items():
        if not isinstance(a, dict):
            continue
        ah = a.get("anchor_hash") or a.get("anchor") or ""
        if not ah:
            rej.append({"canonical_key": canonical_key, "reason": "anchor_missing"})
            continue

        spec = (metric_schema.get(canonical_key) if isinstance(metric_schema, dict) else None) or {}
        spec = dict(spec)
        spec.setdefault("name", a.get("name") or canonical_key)
        spec.setdefault("canonical_key", canonical_key)

        c = cand_index.get(ah)
        if not isinstance(c, dict):
            # =====================================================================
            # PATCH FIX41AFC41A START — anchor compatibility resolver (deterministic)
            # If the exact anchor_hash is not present in the deterministic index, try
            # a conservative compatibility match within the current candidate pool.
            # This is additive and does NOT alter hashing, snapshots, or fastpath.
            # =====================================================================
            try:
                _compat = globals().get("_fix41afc41_anchor_compat_resolve")
                c_compat = _compat(spec, baseline_sources_cache, ah, canonical_key=canonical_key) if callable(_compat) else None
                if isinstance(c_compat, dict):
                    # Record that we used compat matching (retain schema anchor hash)
                    try:
                        dbg.setdefault("anchor_compat_used_fix41afc41", []).append({
                            "canonical_key": canonical_key,
                            "anchor_hash_schema": ah,
                            "anchor_hash_candidate": c_compat.get("anchor_hash"),
                            "reason": c_compat.get("_anchor_compat_reason_fix41afc41") or "compat_match"
                        })
                    except Exception:
                        pass
                    c = c_compat
                else:
                    # PATCH FIX41AFC49A START — anchor signature resolve before rejecting missing anchor
                    try:
                        _sig = globals().get("_fix41afc49_anchor_signature_resolve")
                        c_sig = _sig(spec, a, baseline_sources_cache, canonical_key=canonical_key, dbg=dbg) if callable(_sig) else None
                        if isinstance(c_sig, dict):
                            c = c_sig
                        else:
                            rej.append({"canonical_key": canonical_key, "anchor_hash": ah, "reason": "anchor_not_found_in_index"})
                            continue
                    except Exception:
                        rej.append({"canonical_key": canonical_key, "anchor_hash": ah, "reason": "anchor_not_found_in_index"})
                        continue
                    # PATCH FIX41AFC49A END — anchor signature resolve before rejecting missing anchor
            except Exception:
                rej.append({"canonical_key": canonical_key, "anchor_hash": ah, "reason": "anchor_not_found_in_index"})
                continue
            # =====================================================================
            # PATCH FIX41AFC41A END
            # =====================================================================



        # =====================================================================
        # PATCH FIX41AFC51B START — ensure anchor-resolved candidate is fully hydrated
        # (This is critical for downstream unit_cmp/value_norm/value_range parity)
        try:
            c = _fix41afc51_enrich_candidate_payload(c, anchor_hash=str(ah or ""), anchor_dict=a, canonical_key=canonical_key)
        except Exception:
            pass
        # PATCH FIX41AFC51B END

        # =====================================================================
        # PATCH FIX41AFC56B START — enforce preferred-source for anchored candidates
        # =====================================================================
        try:
            _pref_u = _fix41afc56_preferred_url_for_metric(prev_response, canonical_key, a)
            _cand_u = _fix41afc56_norm_url(c.get("source_url") or c.get("url") or "")
            if _pref_u and _cand_u and _cand_u != _pref_u:
                _resc = _fix41afc56_best_candidate_same_source(spec, a, prev_response, baseline_sources_cache, canonical_key=canonical_key)
                if isinstance(_resc, dict):
                    try:
                        dbg.setdefault("anchor_preferred_source_rescue_fix41afc56", []).append({
                            "canonical_key": canonical_key,
                            "preferred_url": _pref_u,
                            "replaced_url": _cand_u,
                            "rescued_anchor_hash": _resc.get("anchor_hash"),
                            "rescued_value_norm": _resc.get("value_norm"),
                        })
                    except Exception:
                        pass
                    c = _resc
                else:
                    # No rescue candidate in preferred source — reject cross-source hijack deterministically
                    try:
                        rej.append({"canonical_key": canonical_key, "anchor_hash": ah, "reason": "preferred_source_no_match_fix41afc56", "preferred_url": _pref_u, "selected_url": _cand_u})
                    except Exception:
                        pass
                    continue
        except Exception:
            pass
        # =====================================================================
        # PATCH FIX41AFC56B END
        # =====================================================================
        ok, reason = _fix17_candidate_allowed_with_reason(c, spec, canonical_key=canonical_key)
        if not ok:
            # PATCH FIX41AFC49B START — anchor signature retry on ineligible candidate
            try:
                if str(reason or "").strip() in ("unit_mismatch", "unit_required_missing", "bare_year", "junk_hard_block"):
                    _sig = globals().get("_fix41afc49_anchor_signature_resolve")
                    c_sig = _sig(spec, a, baseline_sources_cache, canonical_key=canonical_key, dbg=dbg) if callable(_sig) else None
                    if isinstance(c_sig, dict):
                        ok2, reason2 = _fix17_candidate_allowed_with_reason(c_sig, spec, canonical_key=canonical_key)
                        if ok2:
                            c = c_sig
                        else:
                            rej.append({"canonical_key": canonical_key, "anchor_hash": ah, "reason": reason})
                            continue
                    else:
                        rej.append({"canonical_key": canonical_key, "anchor_hash": ah, "reason": reason})
                        continue
                else:
                    rej.append({"canonical_key": canonical_key, "anchor_hash": ah, "reason": reason})
                    continue
            except Exception:
                rej.append({"canonical_key": canonical_key, "anchor_hash": ah, "reason": reason})
                continue
            # PATCH FIX41AFC49B END — anchor signature retry on ineligible candidate

        # PATCH FIX41AFC28B START
        # Additional hard eligibility stops (instrumentation-only + safety):
        # - Block junk/phone/contact candidates even if earlier gates allow them.
        # - Enforce unit_tag when schema demands magnitude/unit_sales with unit_tag.
        try:
            _reasons28 = _fix41afc28_candidate_block_reasons(spec, c)
            if _reasons28:
                try:
                    rej.append({
                        "canonical_key": canonical_key,
                        "anchor_hash": ah,
                        "reason": ";".join(_reasons28),
                        "patch": "FIX41AFC28B"
                    })
                except Exception:
                    pass
                continue
        except Exception:
            pass
        # PATCH FIX41AFC28B END

        rebuilt[canonical_key] = {
            "canonical_key": canonical_key,
            "name": spec.get("name") or canonical_key,
            "value": c.get("value"),
            "unit": c.get("unit") or "",
            "value_norm": c.get("value_norm"),
            "source_url": c.get("source_url") or "",
            "anchor_hash": c.get("anchor_hash") or ah,
            "evidence": [{
                "source_url": c.get("source_url") or "",
                "raw": c.get("raw") or "",
                "context_snippet": (c.get("context_snippet") or c.get("context") or c.get("context_window") or "")[:400],
                "anchor_hash": c.get("anchor_hash") or ah,
                "method": "anchor_hash_rebuild_fix17",
            }],
            "anchor_used": True,
        }    # PATCH FIX41AFC39_UNIT_OUT_APPLY_ANCHOR_REBUILD START
    try:
        _fix41afc39_backfill_candidate_unit_fields(c)
        if isinstance(rebuilt.get(canonical_key), dict):
            if not str(rebuilt[canonical_key].get("unit") or "").strip():
                rebuilt[canonical_key]["unit"] = _fix41afc39_best_unit_out(c, spec)
                rebuilt[canonical_key]["unit_backfilled_fix41afc39"] = True
            if c.get("unit_family") and not rebuilt[canonical_key].get("unit_family"):
                rebuilt[canonical_key]["unit_family"] = c.get("unit_family")
            if c.get("base_unit") and not rebuilt[canonical_key].get("base_unit"):
                rebuilt[canonical_key]["base_unit"] = c.get("base_unit")
            ev0 = (rebuilt[canonical_key].get("evidence") or [])
            if isinstance(ev0, list) and ev0 and isinstance(ev0[0], dict):
                if not ev0[0].get("unit") and rebuilt[canonical_key].get("unit"):
                    ev0[0]["unit"] = rebuilt[canonical_key].get("unit")
                if not ev0[0].get("unit_family") and rebuilt[canonical_key].get("unit_family"):
                    ev0[0]["unit_family"] = rebuilt[canonical_key].get("unit_family")
    except Exception:
        pass
    # PATCH FIX41AFC39_UNIT_OUT_APPLY_ANCHOR_REBUILD END

    used.append({"canonical_key": canonical_key, "anchor_hash": ah, "source_url": c.get("source_url") or ""})

    return rebuilt


def rebuild_metrics_from_snapshots_schema_only_fix17(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    FIX17 schema-only rebuild:
      - Uses fix16 keyword de-yearing
      - Applies strict fix17 unit requirement inference (even if schema dim missing)
      - Applies fix15 junk exclusion + bare-year exclusion
      - Deterministic tie-breaks
    """
    import re

    if not isinstance(prev_response, dict):
        return {}

    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )
    if not isinstance(metric_schema, dict) or not metric_schema:
        return {}

    # =====================================================================
    # PATCH FIX41AFC21D (ADDITIVE): Anchor-first selection parity in schema-only FIX17
    # Goal:
    #   - If an anchor exists for a canonical metric, prefer the anchored candidate
    #     before any keyword/context scoring (parity with analysis intent).
    # Notes:
    #   - Additive only; does not refactor existing scoring.
    #   - Safe if metric_anchors/candidate_index are missing.
    # =====================================================================
    metric_anchors_fix41afc21d = (
        prev_response.get("metric_anchors")
        or (prev_response.get("primary_response") or {}).get("metric_anchors")
        or (prev_response.get("results") or {}).get("metric_anchors")
        or {}
    )
    fn_idx_fix41afc21d = globals().get("_es_build_candidate_index_deterministic")
    cand_index_fix41afc21d = fn_idx_fix41afc21d(baseline_sources_cache) if callable(fn_idx_fix41afc21d) else {}
    dbg_fix41afc21d = prev_response.setdefault("_evolution_rebuild_debug", {})
    dbg_fix41afc21d.setdefault("schema_only_anchor_overrides_fix41afc21d", [])
    # =====================================================================
    # END PATCH FIX41AFC21D
    # =====================================================================

    # Flatten snapshot candidates (no re-fetch)
    if isinstance(baseline_sources_cache, dict) and isinstance(baseline_sources_cache.get("snapshots"), list):
        sources = baseline_sources_cache.get("snapshots", [])
    elif isinstance(baseline_sources_cache, list):
        sources = baseline_sources_cache
    else:
        sources = []

    candidates = []
    for s in sources:
        if not isinstance(s, dict):
            continue
        url = s.get("source_url") or s.get("url") or ""
        xs = s.get("extracted_numbers")
        if isinstance(xs, list) and xs:
            for c in xs:
                if not isinstance(c, dict):
                    continue
                c2 = dict(c)
                c2.setdefault("source_url", url)
                candidates.append(c2)

    def _norm(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def _cand_sort_key(c: dict):
        try:
            return (
                str(c.get("anchor_hash") or ""),
                str(c.get("source_url") or ""),
                int(c.get("start_idx") or 0),
                str(c.get("raw") or ""),
                str(c.get("unit") or ""),
                float(c.get("value_norm") or 0.0),
            )
        except Exception:
            return ("", "", 0, "", "", 0.0)

    candidates.sort(key=_cand_sort_key)

    # Debug sink
    dbg = prev_response.setdefault("_evolution_rebuild_debug", {})
    dbg.setdefault("schema_only_zero_hit_metrics_fix17", [])

    rebuilt = {}

    for canonical_key, sch in metric_schema.items():
        if not isinstance(sch, dict):
            continue

        spec = dict(sch)
        spec.setdefault("canonical_key", canonical_key)
        spec.setdefault("name", sch.get("name") or canonical_key)
        # =====================================================================
        # PATCH FIX41AFC23C (ADDITIVE): if an anchor exists for this metric, require anchor-matched candidates only
        # =====================================================================
        _fix41afc23_anchor_required_hash = None
        try:
            _a_req = metric_anchors_fix41afc21d.get(canonical_key) if isinstance(metric_anchors_fix41afc21d, dict) else None
            if isinstance(_a_req, dict):
                _fix41afc23_anchor_required_hash = _a_req.get("anchor_hash") or _a_req.get("anchor") or _a_req.get("candidate_id")
            if _fix41afc23_anchor_required_hash:
                _fix41afc23_anchor_required_hash = str(_fix41afc23_anchor_required_hash)
        except Exception:
            _fix41afc23_anchor_required_hash = None
        # =====================================================================
        # END PATCH FIX41AFC23C
        # =====================================================================


        # =====================================================================
        # PATCH FIX41AFC21D (ADDITIVE): Anchor-first override for this metric (schema-only path)
        # =====================================================================
        try:
            _a = metric_anchors_fix41afc21d.get(canonical_key) if isinstance(metric_anchors_fix41afc21d, dict) else None
            _ah = None
            if isinstance(_a, dict):
                _ah = _a.get("anchor_hash") or _a.get("anchor") or _a.get("candidate_id")
            if _ah:
                _ah_s = str(_ah)
                _cand = None
                try:
                    _cand = cand_index_fix41afc21d.get(_ah_s) or cand_index_fix41afc21d.get(_ah)
                except Exception:
                    _cand = None

                # If no index hit, fall back to a deterministic scan (keeps behavior additive)
                if _cand is None and isinstance(candidates, list):
                    for _c0 in candidates:
                        if not isinstance(_c0, dict):
                            continue
                        if str(_c0.get("anchor_hash") or "") == _ah_s:
                            _cand = _c0
                            break

                if isinstance(_cand, dict):
                    _ok, _reason = _fix17_candidate_allowed_with_reason(_cand, spec, canonical_key=canonical_key)
                    fn_v2 = globals().get("_metric_candidate_is_eligible_v2")
                    _ok2 = True
                    try:
                        if callable(fn_v2):
                            _ok2 = bool(fn_v2(spec, _cand))
                    except Exception:
                        _ok2 = True

                    # PATCH FIX41AFC28D START
                    # Even for anchor override, enforce hard-block reasons (junk/phone/unit-tag) to prevent anchor index pollution.
                    try:
                        _reasons28 = _fix41afc28_candidate_block_reasons(spec, _cand)
                        if _reasons28:
                            try:
                                dbg_fix41afc21d["schema_only_anchor_overrides_fix41afc21d"].append({
                                    "canonical_key": canonical_key,
                                    "anchor_hash": _ah_s,
                                    "source_url": _cand.get("source_url") or "",
                                    "blocked_reasons": list(_reasons28),
                                    "patch": "FIX41AFC28D"
                                })
                            except Exception:
                                pass
                            _ok = False
                    except Exception:
                        pass
                    # PATCH FIX41AFC28D END

                    if _ok and _ok2:
                        rebuilt[canonical_key] = {
                            "canonical_key": canonical_key,
                            "name": spec.get("name") or canonical_key,
                            "value": _cand.get("value"),
                            "unit": _cand.get("unit") or "",
                            "value_norm": _cand.get("value_norm"),
                            "source_url": _cand.get("source_url") or "",
                            "anchor_hash": _cand.get("anchor_hash") or _ah_s,
                            "evidence": [{
                                "source_url": _cand.get("source_url") or "",
                                "raw": _cand.get("raw") or "",
                                "context_snippet": (_cand.get("context_snippet") or _cand.get("context") or _cand.get("context_window") or "")[:400],
                                "anchor_hash": _cand.get("anchor_hash") or _ah_s,
                                "method": "schema_only_anchor_match_override_fix41afc21d",
                            }],
                            "anchor_used": True,
                            "selection_reason": "anchor_match_override_fix41afc21d",
                        }


                        # PATCH FIX41AFC39_UNIT_OUT_APPLY_SCHEMA_ANCHOR_OVERRIDE START
                        try:
                            _fix41afc39_backfill_candidate_unit_fields(_cand)
                            if isinstance(rebuilt.get(canonical_key), dict):
                                if not str(rebuilt[canonical_key].get("unit") or "").strip():
                                    rebuilt[canonical_key]["unit"] = _fix41afc39_best_unit_out(_cand, spec)
                                    rebuilt[canonical_key]["unit_backfilled_fix41afc39"] = True
                                if _cand.get("unit_family") and not rebuilt[canonical_key].get("unit_family"):
                                    rebuilt[canonical_key]["unit_family"] = _cand.get("unit_family")
                                if _cand.get("base_unit") and not rebuilt[canonical_key].get("base_unit"):
                                    rebuilt[canonical_key]["base_unit"] = _cand.get("base_unit")
                                ev0 = (rebuilt[canonical_key].get("evidence") or [])
                                if isinstance(ev0, list) and ev0 and isinstance(ev0[0], dict):
                                    if not ev0[0].get("unit") and rebuilt[canonical_key].get("unit"):
                                        ev0[0]["unit"] = rebuilt[canonical_key].get("unit")
                                    if not ev0[0].get("unit_family") and rebuilt[canonical_key].get("unit_family"):
                                        ev0[0]["unit_family"] = rebuilt[canonical_key].get("unit_family")
                        except Exception:
                            pass
                        # PATCH FIX41AFC39_UNIT_OUT_APPLY_SCHEMA_ANCHOR_OVERRIDE END

                        # PATCH FIX41AFC38_EMIT_TOPK START
                        try:
                            if canonical_key in rebuilt and isinstance(rebuilt[canonical_key], dict):
                                rebuilt[canonical_key]["top_candidates_considered_fix41afc38"] = list(top_candidates_considered_fix41afc38) if isinstance(top_candidates_considered_fix41afc38, list) else []
                                rebuilt[canonical_key]["winner_reason_fix41afc38"] = "selected_best_score"
                        except Exception:
                            pass
                        # PATCH FIX41AFC38_EMIT_TOPK END
                        try:
                            dbg_fix41afc21d["schema_only_anchor_overrides_fix41afc21d"].append({
                                "canonical_key": canonical_key,
                                "anchor_hash": _ah_s,
                                "source_url": _cand.get("source_url") or "",
                            })
                        except Exception:
                            pass
                        continue
        except Exception:
            pass
        # =====================================================================
        # END PATCH FIX41AFC21D
        # =====================================================================

        # Use fix16 year-like & keyword pruning if available
        metric_is_year_like = _fix17_metric_is_year_like(spec, canonical_key=canonical_key)

        keywords = sch.get("keywords") or sch.get("keyword_hints") or []
        if isinstance(keywords, str):
            keywords = [keywords]

        fn_prune = globals().get("_fix16_prune_year_keywords")
        if callable(fn_prune):
            keywords2 = fn_prune(list(keywords), metric_is_year_like)
        else:
            keywords2 = list(keywords)

        kw_norm = [_norm(k) for k in (keywords2 or []) if k]

        # PATCH FIX41AFC38_TOPK_INIT START

        top_candidates_considered_fix41afc38 = []

        # PATCH FIX41AFC38_TOPK_INIT END


        best = None
        best_tie = None
        best_hits = 0
        for c in candidates:

            # PATCH FIX41AFC37 START
            # Pre-selection eligibility gate: skip junk/bare-year/unitless candidates before scoring.
            try:
                _ok37, _r37 = _fix41afc37_preselect_eligible(spec, c, canonical_key=canonical_key)
                if not _ok37:
                    try:
                        _dbg = prev_response.setdefault("_evolution_rebuild_debug", {}) if isinstance(prev_response, dict) else {}
                        _m = _dbg.setdefault("preselect_skips_fix41afc37", {})
                        _lst = _m.setdefault(canonical_key, [])
                        if isinstance(_lst, list) and len(_lst) < 5:
                            _lst.append({
                                "source_url": c.get("source_url") or "",
                                "raw": (c.get("raw") or "")[:80],
                                "reasons": list(_r37)[:3],
                            })
                    except Exception:
                        pass
                    continue
            except Exception:
                pass
            # PATCH FIX41AFC37 END
            # PATCH FIX41AFC38_SCALE_TOPK START
            try:
                if isinstance(c, dict) and not (c.get("unit_family") or "").strip():
                    _uf = _fix41afc38_guess_unit_family(c)
                    if _uf:
                        c["unit_family"] = _uf
                        c["_unit_family_backfilled_fix41afc38"] = True
                _fix41afc38_magnitude_context_override(c)
                _pv = _fix41afc38_prev_value_norm(prev_response, canonical_key)
                _fix41afc38_scale_sanity_adjust(c, spec, _pv)
            except Exception:
                pass
            try:
                if isinstance(top_candidates_considered_fix41afc38, list) and len(top_candidates_considered_fix41afc38) < 5:
                    top_candidates_considered_fix41afc38.append({
                        "value_norm": c.get("value_norm"),
                        "raw": (c.get("raw") or "")[:80],
                        "unit_tag": c.get("unit_tag") or c.get("unit") or "",
                        "unit_family": c.get("unit_family") or "",
                        "source_url": c.get("source_url") or "",
                        "_rescaled": bool(c.get("_fix41afc38_rescaled")),
                        "_rescale_factor": c.get("_fix41afc38_rescale_factor"),
                    })
            except Exception:
                pass
            # PATCH FIX41AFC38_SCALE_TOPK END

            ok, _reason = _fix17_candidate_allowed_with_reason(c, spec, canonical_key=canonical_key)
            if not ok:
                continue

            # PATCH FIX41AFC28C START
            # Hard-block junk/contact/phone candidates + enforce required unit_tag for magnitude/unit_sales when schema has unit_tag.
            # Also record lightweight reject reasons for diagnostics (top-N).
            try:
                _dbg28 = prev_response.setdefault("_evolution_rebuild_debug", {})
                if isinstance(_dbg28, dict):
                    _dbg28.setdefault("eligibility_fail_reasons_fix41afc28", {})
                _reasons28 = _fix41afc28_candidate_block_reasons(spec, c)
                if _reasons28:
                    try:
                        _dmap = _dbg28.get("eligibility_fail_reasons_fix41afc28") if isinstance(_dbg28, dict) else None
                        if isinstance(_dmap, dict):
                            _lst = _dmap.setdefault(canonical_key, [])
                            if isinstance(_lst, list) and len(_lst) < 5:
                                _lst.append({
                                    "source_url": c.get("source_url") or "",
                                    "raw": (c.get("raw") or "")[:80],
                                    "reasons": list(_reasons28),
                                })
                    except Exception:
                        pass
                    continue
            except Exception:
                pass
            # PATCH FIX41AFC28C END
            # =====================================================================
            # PATCH FIX41AFC23C (ADDITIVE): anchor-required filtering when anchor exists
            # =====================================================================
            try:
                if _fix41afc23_anchor_required_hash:
                    if str(c.get("anchor_hash") or "") != _fix41afc23_anchor_required_hash:
                        continue
            except Exception:
                pass
            # =====================================================================
            # END PATCH FIX41AFC23C
            # =====================================================================

            # =====================================================================
            # PATCH FIX41AFC23B2 (ADDITIVE): hard unit-required gate for unit_sales/magnitude (blocks unitless years/phone tails)
            # =====================================================================
            try:
                _dim = str(spec.get("dimension") or "").lower()
                _uf = str(spec.get("unit_family") or "").lower()
                _u = str(c.get("unit") or c.get("unit_tag") or "").strip()
                if _dim == "unit_sales" or _uf == "magnitude":
                    if not _u:
                        continue
                    if _u == "%" or "$" in _u or "usd" in _u.lower() or "sgd" in _u.lower() or "eur" in _u.lower():
                        continue
            except Exception:
                pass
            # =====================================================================
            # END PATCH FIX41AFC23B2
            # =====================================================================


            ctx = _norm(c.get("context_snippet") or c.get("context") or c.get("context_window") or "")
            raw = _norm(c.get("raw") or "")

            hits = 0
            for k in kw_norm:
                if k and (k in ctx or k in raw):
                    hits += 1

            tie = (-hits,) + _cand_sort_key(c)
            if best is None or tie < best_tie:
                best = c
                best_tie = tie
                best_hits = hits

        if not isinstance(best, dict):
            continue

        # If schema has keywords, require at least one hit.
        if kw_norm and best_hits <= 0:
            dbg["schema_only_zero_hit_metrics_fix17"].append({"canonical_key": canonical_key, "reason": "no_keyword_hits"})
            continue

        rebuilt[canonical_key] = {
            "canonical_key": canonical_key,
            "name": spec.get("name") or canonical_key,
            "value": best.get("value"),
            "unit": best.get("unit") or "",
            "value_norm": best.get("value_norm"),
            "source_url": best.get("source_url") or "",
            "anchor_hash": best.get("anchor_hash") or "",
            "evidence": [{
                "source_url": best.get("source_url") or "",
                "raw": best.get("raw") or "",
                "context_snippet": (best.get("context_snippet") or best.get("context") or best.get("context_window") or "")[:400],
                "anchor_hash": best.get("anchor_hash") or "",
                "method": "schema_only_rebuild_fix17",
            }],
            "anchor_used": False,
        }


        # PATCH FIX41AFC39_UNIT_OUT_APPLY_SCHEMA_BEST START
        try:
            _fix41afc39_backfill_candidate_unit_fields(best)
            if isinstance(rebuilt.get(canonical_key), dict):
                if not str(rebuilt[canonical_key].get("unit") or "").strip():
                    rebuilt[canonical_key]["unit"] = _fix41afc39_best_unit_out(best, spec)
                    rebuilt[canonical_key]["unit_backfilled_fix41afc39"] = True
                if best.get("unit_family") and not rebuilt[canonical_key].get("unit_family"):
                    rebuilt[canonical_key]["unit_family"] = best.get("unit_family")
                if best.get("base_unit") and not rebuilt[canonical_key].get("base_unit"):
                    rebuilt[canonical_key]["base_unit"] = best.get("base_unit")
                ev0 = (rebuilt[canonical_key].get("evidence") or [])
                if isinstance(ev0, list) and ev0 and isinstance(ev0[0], dict):
                    if not ev0[0].get("unit") and rebuilt[canonical_key].get("unit"):
                        ev0[0]["unit"] = rebuilt[canonical_key].get("unit")
                    if not ev0[0].get("unit_family") and rebuilt[canonical_key].get("unit_family"):
                        ev0[0]["unit_family"] = rebuilt[canonical_key].get("unit_family")
        except Exception:
            pass
        # PATCH FIX41AFC39_UNIT_OUT_APPLY_SCHEMA_BEST END

    return rebuilt


# =====================================================================
# PATCH FIX17 (ADDITIVE): wire FIX17 rebuilds into the existing dispatch
# - Keep names identical so evolution uses these as the LAST definitions
# =====================================================================

def rebuild_metrics_from_snapshots_with_anchors(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:  # noqa: F811
    return rebuild_metrics_from_snapshots_with_anchors_fix17(prev_response, baseline_sources_cache, web_context=web_context)


def rebuild_metrics_from_snapshots_schema_only(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:  # noqa: F811
    return rebuild_metrics_from_snapshots_schema_only_fix17(prev_response, baseline_sources_cache, web_context=web_context)

# =====================================================================
# END PATCH FIX17
# =====================================================================



# =====================================================================
# PATCH FIX18 (ADDITIVE): Anchor-authoritative rebuild (no schema fallback)
# Goal:
#   - If analysis emitted an anchor for a canonical metric, evolution MUST NOT
#     fall back to schema-only selection when the anchor cannot be used.
#   - This closes the final loophole where unitless year tokens win schema-only
#     scoring after an anchor rejection.
# Implementation:
#   - Provide a FIX18 schema-only rebuild that:
#       (1) rebuilds anchored metrics via rebuild_metrics_from_snapshots_with_anchors_fix17
#       (2) rebuilds ONLY unanchored metrics via rebuild_metrics_from_snapshots_schema_only_fix17
#       (3) merges results deterministically (anchored first)
#   - Re-wire the public rebuild_metrics_from_snapshots_schema_only to FIX18.
# Notes:
#   - Fully deterministic; no refetch; no LLM.
#   - Additive only: leaves FIX17 implementations intact.
# =====================================================================

# PATCH FIX41AFC24A START
def _canonical_value_normalize_v1(schema: dict, metric: dict) -> dict:
    """
    Post-selection normalization parity helper.

    Goal: align Evolution's emitted metric 'value/value_norm' with Analysis' presentation canonicalization
    (e.g., 17.8 million units -> 18 million units for large-count unit_sales).

    Additive-only: does not change candidate eligibility/selection/hashing. Rebuild-only caller.
    """
    try:
        sch = schema if isinstance(schema, dict) else {}
        m = metric if isinstance(metric, dict) else {}
        dim = (sch.get("dimension") or m.get("dimension") or "").strip().lower()
        uf = (sch.get("unit_family") or m.get("unit_family") or "").strip().lower()
        ut = (m.get("unit_tag") or m.get("unit") or "").strip().lower()
        vn = m.get("value_norm", None)
        if vn is None:
            return m
        try:
            vn_f = float(vn)
        except Exception:
            return m

        # Detect magnitude-style million/billion tags (kept intentionally conservative)
        is_magnitude = (uf == "magnitude") or ("million" in ut) or (ut in {"m", "mn", "million", "millions", "b", "bn", "billion", "billions"})
        # Percent
        is_percent = (dim == "percent") or (uf == "percent") or ("%" in ut) or ("percent" in ut)
        # Currency (very light detection; schema-driven if present)
        is_currency = (dim == "currency") or (uf == "currency") or any(tok in ut for tok in ["usd", "sgd", "eur", "gbp", "$", "€", "£"])

        vn_norm = vn_f
        # --- Canonicalization rules (mirrors typical Analysis rounding behavior) ---
        if is_percent:
            # 1 decimal is usually stable for dashboards
            vn_norm = round(vn_f, 1)
        elif is_currency:
            # Keep 2 decimals for small values, 1 decimal for medium, integer for very large
            if abs(vn_f) >= 100:
                vn_norm = round(vn_f)
            elif abs(vn_f) >= 10:
                vn_norm = round(vn_f, 1)
            else:
                vn_norm = round(vn_f, 2)
        elif dim == "unit_sales" and is_magnitude:
            # Key parity rule observed in your outputs: large million-unit counts are integer-rounded.
            if abs(vn_f) >= 10:
                vn_norm = round(vn_f)
            elif abs(vn_f) >= 1:
                vn_norm = round(vn_f, 1)
            else:
                vn_norm = round(vn_f, 2)
        elif is_magnitude:
            # Generic magnitude rounding
            if abs(vn_f) >= 100:
                vn_norm = round(vn_f)
            elif abs(vn_f) >= 10:
                vn_norm = round(vn_f, 1)
            else:
                vn_norm = round(vn_f, 2)

        # Apply if changed (keep existing keys; add debug fields)
        if vn_norm != vn_f:
            m = dict(m)
            m["value_norm"] = float(vn_norm)
            # Also update 'value' when it is numeric-like
            try:
                # Preserve ints as ints for display parity
                if float(vn_norm).is_integer():
                    m["value"] = int(round(float(vn_norm)))
                else:
                    m["value"] = float(vn_norm)
            except Exception:
                pass
            m["post_norm_applied"] = True
            m["post_norm_rule"] = "canonical_value_normalize_v1"
        return m
    except Exception:
        return metric
# PATCH FIX41AFC24A END


# =====================================================================
# PATCH FIX41AFC25 (ADDITIVE): Parity lock / assertion guard (instrumentation-only)
#
# Goal:
# - Make analysis/evolution parity non-accidental by checking (debug-only) that
#   anchored metrics in Evolution still resolve to the SAME anchor_hash and a
#   canonically-normalized value consistent with the anchored candidate in the
#   current snapshot pool.
#
# Safety:
# - Purely additive. No behavior changes to selection, eligibility, hashing, or fastpath.
# - Emits ONLY debug fields; never blocks execution.
# =====================================================================
def _parity_assert_fix41afc25(prev_response: dict, rebuilt: dict, baseline_sources_cache, schema_map: dict = None) -> list:
    try:
        if not isinstance(prev_response, dict) or not isinstance(rebuilt, dict):
            return []
        schema_map = schema_map if isinstance(schema_map, dict) else {}

        # Get anchors (best-effort; supports multiple container shapes)
        anchors = {}
        try:
            fn = globals().get("_get_metric_anchors_any")
            if callable(fn):
                anchors = fn(prev_response) or {}
        except Exception:
            anchors = {}
        if not anchors:
            try:
                anchors = prev_response.get("metric_anchors") or (prev_response.get("primary_response") or {}).get("metric_anchors") or {}
            except Exception:
                anchors = {}
        if not isinstance(anchors, dict) or not anchors:
            return []

        # Build anchor_hash -> candidate index from the actual current pool
        ah_to_cand = {}
        try:
            if isinstance(baseline_sources_cache, list):
                for _row in (baseline_sources_cache or []):
                    if not isinstance(_row, dict):
                        continue
                    nums = _row.get("extracted_numbers")
                    if not isinstance(nums, list) or not nums:
                        continue
                    for _c in nums:
                        if not isinstance(_c, dict):
                            continue
                        _ah = _c.get("anchor_hash") or ""
                        if isinstance(_ah, str) and _ah and _ah not in ah_to_cand:
                            ah_to_cand[_ah] = _c
        except Exception:
            pass

        def _num(x):
            try:
                if x is None:
                    return None
                if isinstance(x, (int, float)):
                    return float(x)
                s = str(x).strip()
                if not s:
                    return None
                return float(s)
            except Exception:
                return None

        violations = []
        for ck, a in anchors.items():
            if not isinstance(ck, str) or not ck:
                continue
            if not isinstance(a, dict):
                continue
            ah = a.get("anchor_hash") or ""
            if not isinstance(ah, str) or not ah:
                continue

            mv = rebuilt.get(ck)
            if not isinstance(mv, dict):
                continue

            # Check anchor hash match in emitted metric
            cur_ah = mv.get("anchor_hash") or mv.get("cur_anchor_hash") or mv.get("selected_anchor_hash") or ""
            if isinstance(cur_ah, str) and cur_ah and cur_ah != ah:
                violations.append({
                    "canonical_key": ck,
                    "reason": "anchor_hash_mismatch",
                    "expected_anchor_hash": ah,
                    "current_anchor_hash": cur_ah,
                })
                continue  # no need to value-compare if anchor mismatched

            # Find expected candidate from pool for this anchor hash
            cand = ah_to_cand.get(ah)
            if not isinstance(cand, dict):
                violations.append({
                    "canonical_key": ck,
                    "reason": "anchor_candidate_missing_in_pool",
                    "expected_anchor_hash": ah,
                })
                continue

            # Build expected metric-shaped dict and apply same post-norm helper (if available)
            schema = schema_map.get(ck) if isinstance(schema_map.get(ck), dict) else {}
            expected_metric = {
                "value": cand.get("value"),
                "value_norm": cand.get("value_norm") if cand.get("value_norm") is not None else cand.get("value"),
                "unit": cand.get("unit") or cand.get("unit_raw") or "",
                "dimension": (schema.get("dimension") if isinstance(schema, dict) else "") or mv.get("dimension") or "",
                "anchor_hash": ah,
            }
            try:
                fn_norm = globals().get("_canonical_value_normalize_v1")
                if callable(fn_norm):
                    expected_metric = fn_norm(schema, expected_metric) or expected_metric
            except Exception:
                pass

            exp = _num(expected_metric.get("value_norm"))
            cur = _num(mv.get("value_norm") if mv.get("value_norm") is not None else mv.get("cur_value_norm"))
            if exp is None or cur is None:
                continue

            # Tolerances (debug-only): conservative defaults
            dim = (schema.get("dimension") if isinstance(schema, dict) else "") or (mv.get("dimension") or "")
            abs_eps = 1e-9
            rel_eps = 0.0005
            if str(dim).lower() in ("percent", "percentage", "market_share"):
                abs_eps = 0.05  # 0.05 percentage-point
                rel_eps = 0.01

            ok = abs(cur - exp) <= max(abs_eps, rel_eps * max(1.0, abs(exp)))
            if not ok:
                violations.append({
                    "canonical_key": ck,
                    "reason": "value_norm_mismatch_post_norm",
                    "expected_anchor_hash": ah,
                    "expected_value_norm": exp,
                    "current_value_norm": cur,
                    "dimension": dim,
                })

        return violations
    except Exception:
        return []
# =====================================================================
# END PATCH FIX41AFC25
# =====================================================================


def rebuild_metrics_from_snapshots_schema_only_fix18(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    FIX18 rebuild:
      - Anchored metrics: ONLY from anchor-aware rebuild (fix17), or skipped if rejected.
      - Unanchored metrics: schema-only rebuild (fix17) as before.
      - Never allows schema-only fallback to fill an anchored canonical_key.
    """

# ============================================================
    # PATCH FIX41AFC36D START — defensive backfill for baseline_sources_cache before rebuild
    # ============================================================
    try:
        _fix41afc36_backfill_in_sources_cache(baseline_sources_cache)
        # PATCH FIX41AFC41C START — enforce unit-token family (magnitude tokens)
        try:
            _fix41afc41_force_unit_token_family_in_sources_cache(baseline_sources_cache)
        except Exception:
            pass
        # PATCH FIX41AFC41C END
        if isinstance(prev_response, dict):
            if isinstance(prev_response.get("baseline_sources_cache"), (dict, list)):
                _fix41afc36_backfill_in_sources_cache(prev_response.get("baseline_sources_cache"))
            if isinstance(prev_response.get("results"), dict) and isinstance(prev_response["results"].get("source_results"), list):
                for _sr in prev_response["results"]["source_results"]:
                    if isinstance(_sr, dict) and isinstance(_sr.get("extracted_numbers"), list):
                        _fix41afc36_backfill_candidates_list(_sr.get("extracted_numbers"))
    except Exception:
        pass
    # ============================================================
    # PATCH FIX41AFC36D END
# ============================================================


    if not isinstance(prev_response, dict):
        return {}

    # Anchored part (authoritative when present)
    fn_anchor = globals().get("rebuild_metrics_from_snapshots_with_anchors_fix17")
    anchored = fn_anchor(prev_response, baseline_sources_cache, web_context=web_context) if callable(fn_anchor) else {}

    # Identify anchored keys (only those with a concrete anchor_hash)
    metric_anchors_any = globals().get("_get_metric_anchors_any")
    metric_anchors = metric_anchors_any(prev_response) if callable(metric_anchors_any) else (
        prev_response.get("metric_anchors") or {}
    )
    anchored_keys = set()
    try:
        for k, a in (metric_anchors or {}).items():
            if isinstance(a, dict) and (a.get("anchor_hash") or a.get("anchor")):
                anchored_keys.add(k)
    except Exception:
        anchored_keys = set()

    # Build a shallow prev_response copy where schema-only sees ONLY unanchored metrics
    pr2 = dict(prev_response)
    ms = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )
    if isinstance(ms, dict) and anchored_keys:
        ms2 = {k: v for k, v in ms.items() if k not in anchored_keys}
        pr2["metric_schema_frozen"] = ms2

    # Unanchored part
    fn_schema = globals().get("rebuild_metrics_from_snapshots_schema_only_fix17")
    unanchored = fn_schema(pr2, baseline_sources_cache, web_context=web_context) if callable(fn_schema) else {}

    # Deterministic merge: anchored first, then unanchored
    rebuilt = {}
    if isinstance(anchored, dict):
        rebuilt.update(anchored)
    if isinstance(unanchored, dict):
        for k, v in unanchored.items():
            if k not in rebuilt:
                rebuilt[k] = v
    # PATCH FIX41AFC24B START
    # Post-selection normalization parity: normalize emitted metric values to match Analysis presentation rules.
    try:
        schema_map = {}
        if isinstance(prev_response, dict):
            schema_map = prev_response.get("metric_schema_frozen") or {}
        if isinstance(rebuilt, dict) and rebuilt:
            for _ck, _mv in list(rebuilt.items()):
                _schema = {}
                if isinstance(schema_map, dict) and _ck in schema_map and isinstance(schema_map.get(_ck), dict):
                    _schema = schema_map.get(_ck) or {}
                # Fallback to prev canonical schema when frozen map missing
                if not _schema and isinstance(prev_response, dict):
                    pmc = prev_response.get("primary_metrics_canonical") or {}
                    if isinstance(pmc, dict) and _ck in pmc and isinstance(pmc.get(_ck), dict):
                        _schema = pmc.get(_ck) or {}
                if isinstance(_mv, dict):
                    rebuilt[_ck] = _canonical_value_normalize_v1(_schema, _mv)
            # Attach lightweight debug marker
            if isinstance(prev_response, dict):
                dbg = prev_response.setdefault("_evolution_rebuild_debug", {})
                if isinstance(dbg, dict):
                    dbg["post_norm_fix41afc24b_applied"] = True
    except Exception:
        pass
# PATCH FIX41AFC24B END
# PATCH FIX41AFC25_EMIT START
    # Parity lock (instrumentation-only): assert anchored metrics resolve consistently.
    try:
        _viol = _parity_assert_fix41afc25(prev_response, rebuilt, baseline_sources_cache, schema_map=schema_map)
        if _viol:
            # Emit into rebuild debug (non-behavioral)
            try:
                prev_response.setdefault("_evolution_rebuild_debug", {})
                if isinstance(prev_response.get("_evolution_rebuild_debug"), dict):
                    prev_response["_evolution_rebuild_debug"]["parity_fix41afc25_violations"] = list(_viol or [])
                    prev_response["_evolution_rebuild_debug"]["parity_fix41afc25_violation"] = True
                    prev_response["_evolution_rebuild_debug"]["parity_fix41afc25_violation_count"] = int(len(_viol or []))
            except Exception:
                pass
    except Exception:
        pass
# PATCH FIX41AFC25_EMIT END
# PATCH FIX41AFC26 START
    # Schema-authoritative reconstruction for rebuilt metrics:
    # Force dimension/unit/unit_tag/unit_family from frozen schema (analysis parity).
    try:
        if isinstance(schema_map, dict) and isinstance(rebuilt, dict):
            for _ck, _m in list(rebuilt.items()):
                if not isinstance(_m, dict):
                    continue
                _schema = schema_map.get(_ck) if isinstance(schema_map, dict) else None
                if not isinstance(_schema, dict):
                    continue
                # Only override these schema-authoritative fields (do not alter value selection).
                for _fld in ("dimension", "unit", "unit_tag", "unit_family"):
                    _sv = _schema.get(_fld)
                    if _sv is not None and _sv != "":
                        _m[_fld] = _sv
                _m["_schema_authoritative_fix41afc26"] = True
    except Exception:
        pass
# PATCH FIX41AFC26 END
    # PATCH FIX41AFC27 START
    # Anchor-scoped value_range parity:
    # When an anchor exists/was used, restrict value_range construction to the
    # anchor neighborhood to avoid misleading ultra-wide ranges from unrelated
    # snapshot candidates (presentation-only; does NOT change selection).
    try:
        _vr_scoped_keys_fix41afc27 = []
        _metric_anchors_fix41afc27 = None
        try:
            _metric_anchors_fix41afc27 = (prev_response or {}).get("metric_anchors") or {}
        except Exception:
            _metric_anchors_fix41afc27 = {}

        def _fix41afc27_is_truthy_anchor(v):
            return bool(v) and str(v).lower() not in ("none", "null", "nan", "false", "0")

        def _fix41afc27_fmt_range(_mn, _mx, _unit):
            if _mn is None or _mx is None:
                return ""
            def _g(x):
                try:
                    if isinstance(x, (int, float)) and abs(x) >= 1000:
                        return f"{x:.0f}"
                    if isinstance(x, (int, float)) and abs(x) >= 100:
                        return f"{x:.1f}".rstrip("0").rstrip(".")
                    if isinstance(x, (int, float)) and abs(x) >= 10:
                        return f"{x:.2f}".rstrip("0").rstrip(".")
                    return f"{x:.4g}"
                except Exception:
                    return str(x)
            if _mn == _mx:
                return f"{_g(_mn)} {_unit}".strip()
            return f"{_g(_mn)}–{_g(_mx)} {_unit}".strip()

        def _fix41afc27_scope_value_range(metric_obj: dict, canonical_key: str):
            if not isinstance(metric_obj, dict):
                return
            _a = (_metric_anchors_fix41afc27 or {}).get(canonical_key) or {}
            _anchor_hash = _a.get("anchor_hash") if isinstance(_a, dict) else None
            _anchor_used = bool(metric_obj.get("anchor_used"))
            if not (_fix41afc27_is_truthy_anchor(_anchor_hash) or _anchor_used):
                return

            ev = metric_obj.get("evidence") or []
            if not isinstance(ev, list) or not ev:
                return

            sel_vn = metric_obj.get("value_norm")
            try:
                sel_vn = float(sel_vn) if sel_vn is not None else None
            except Exception:
                sel_vn = None

            ev_anchor = []
            if _fix41afc27_is_truthy_anchor(_anchor_hash):
                for e in ev:
                    if isinstance(e, dict) and e.get("anchor_hash") == _anchor_hash:
                        ev_anchor.append(e)

            scoped = ev_anchor

            if not scoped and sel_vn is not None and sel_vn != 0:
                tol = 0.30  # +/-30% neighborhood (presentation only)
                for e in ev:
                    if not isinstance(e, dict):
                        continue
                    vn = e.get("value_norm")
                    try:
                        vn = float(vn) if vn is not None else None
                    except Exception:
                        vn = None
                    if vn is None:
                        continue
                    if abs(vn - sel_vn) / abs(sel_vn) <= tol:
                        scoped.append(e)

            if not scoped:
                return

            vns = []
            examples = []
            for e in scoped:
                if not isinstance(e, dict):
                    continue
                vn = e.get("value_norm")
                try:
                    vn = float(vn) if vn is not None else None
                except Exception:
                    vn = None
                if vn is None:
                    continue
                vns.append(vn)
                if len(examples) < 5:
                    examples.append({
                        "raw": e.get("raw", ""),
                        "source_url": e.get("url") or e.get("source_url") or metric_obj.get("source_url", ""),
                        "context_snippet": (e.get("context_snippet", "") or "")[:220]
                    })

            if not vns:
                return

            _mn, _mx = min(vns), max(vns)
            metric_obj["value_range"] = {
                "min": _mn,
                "max": _mx,
                "n": len(vns),
                "examples": examples,
                "method": "anchor_scoped_fix41afc27"
            }
            _unit_disp = metric_obj.get("unit") or metric_obj.get("unit_tag") or ""
            metric_obj["value_range_display"] = _fix41afc27_fmt_range(_mn, _mx, _unit_disp)
            metric_obj["_value_range_scope_fix41afc27"] = "anchor_scoped"
            _vr_scoped_keys_fix41afc27.append(canonical_key)

        if isinstance(rebuilt, dict):
            for _ck, _m in list(rebuilt.items()):
                _fix41afc27_scope_value_range(_m, _ck)

        try:
            prev_response.setdefault("_evolution_rebuild_debug", {})
            prev_response["_evolution_rebuild_debug"]["value_range_scoped_fix41afc27_count"] = len(_vr_scoped_keys_fix41afc27)
            prev_response["_evolution_rebuild_debug"]["value_range_scoped_fix41afc27_keys"] = _vr_scoped_keys_fix41afc27[:50]
        except Exception:
            pass
    except Exception:
        pass
    # PATCH FIX41AFC27 END

    # PATCH FIX41AFC29 START
    # Anchor-Cohort Lock (presentation-only): when anchor exists/used, scope value_range/source_span
    try:
        import re as _re

        def _fix41afc29_is_truthy(v):
            return v is not None and str(v).strip().lower() not in ("", "none", "null", "nan")

        def _fix41afc29_get_schema(schema_map, ck):
            if isinstance(schema_map, dict) and ck in schema_map and isinstance(schema_map.get(ck), dict):
                return schema_map.get(ck)
            return {}

        def _fix41afc29_target_year(schema: dict, metric_obj: dict):
            # Prefer schema keywords / canonical_key hints; fallback to name.
            year = None
            try:
                kws = schema.get("keywords") if isinstance(schema, dict) else None
                if isinstance(kws, list):
                    for k in kws:
                        m = _re.search(r"\b(19\d{2}|20\d{2})\b", str(k))
                        if m:
                            year = int(m.group(1)); break
                if year is None:
                    ck = (schema.get("canonical_key") if isinstance(schema, dict) else "") or (metric_obj.get("canonical_key") if isinstance(metric_obj, dict) else "")
                    m = _re.search(r"\b(19\d{2}|20\d{2})\b", str(ck))
                    if m:
                        year = int(m.group(1))
                if year is None:
                    nm = (schema.get("name") if isinstance(schema, dict) else "") or (metric_obj.get("name") if isinstance(metric_obj, dict) else "")
                    m = _re.search(r"\b(19\d{2}|20\d{2})\b", str(nm))
                    if m:
                        year = int(m.group(1))
            except Exception:
                year = None
            return year

        def _fix41afc29_is_projected(schema: dict, metric_obj: dict):
            s = " ".join([
                str((schema or {}).get("name") or ""),
                str((metric_obj or {}).get("name") or ""),
                str((metric_obj or {}).get("original_name") or "")
            ]).lower()
            return any(t in s for t in ["project", "forecast", "outlook", "by ", "expected", "estimate", "projec", "cagr"])

        def _fix41afc29_extract_any_years(text: str):
            yrs = []
            try:
                for m in _re.finditer(r"\b(19\d{2}|20\d{2})\b", text or ""):
                    yrs.append(int(m.group(1)))
            except Exception:
                pass
            return yrs

        def _fix41afc29_pick_anchor_hash(metric_obj: dict):
            # Prefer explicit anchor_hash. Else infer from the evidence item closest to selected value_norm.
            ah = metric_obj.get("anchor_hash") if isinstance(metric_obj, dict) else None
            if _fix41afc29_is_truthy(ah):
                return ah
            ev = metric_obj.get("evidence") if isinstance(metric_obj, dict) else None
            if not isinstance(ev, list) or not ev:
                return None
            sel = metric_obj.get("value_norm")
            try:
                sel = float(sel) if sel is not None else None
            except Exception:
                sel = None
            best = None
            best_d = None
            for e in ev:
                if not isinstance(e, dict):
                    continue
                eah = e.get("anchor_hash")
                if not _fix41afc29_is_truthy(eah):
                    continue
                vn = e.get("value_norm")
                try:
                    vn = float(vn) if vn is not None else None
                except Exception:
                    vn = None
                if sel is None or vn is None:
                    continue
                d = abs(vn - sel)
                if best is None or d < best_d:
                    best = eah
                    best_d = d
            return best

        def _fix41afc29_in_year_cohort(e: dict, target_year: int, projected: bool):
            txt = (e.get("context_snippet") or "") + " " + (e.get("raw") or "")
            txt_l = txt.lower()
            yrs = _fix41afc29_extract_any_years(txt)
            if target_year is None:
                return True
            # If explicit other years are present, reject unless the target year appears.
            if yrs and (target_year not in yrs):
                return False
            # For non-projected metrics, reject future-horizon phrases like "by 2040" when target_year is earlier.
            if not projected:
                m = _re.search(r"\bby\s+(19\d{2}|20\d{2})\b", txt_l)
                if m:
                    y = int(m.group(1))
                    if target_year is not None and y > target_year:
                        return False
            return True

        def _fix41afc29_scope_value_range(metric_obj: dict, ck: str, schema_map: dict):
            if not isinstance(metric_obj, dict):
                return False

            schema = _fix41afc29_get_schema(schema_map, ck)
            target_year = _fix41afc29_target_year(schema, metric_obj)
            projected = _fix41afc29_is_projected(schema, metric_obj)

            anchor_used = bool(metric_obj.get("anchor_used"))
            anchor_hash = _fix41afc29_pick_anchor_hash(metric_obj)

            # Only lock when anchored/anchor_used (avoid changing unanchored behavior)
            if not (anchor_used or _fix41afc29_is_truthy(anchor_hash)):
                return False

            ev = metric_obj.get("evidence") or []
            if not isinstance(ev, list) or not ev:
                return False

            # Step 1: strict anchor_hash cohort
            cohort = []
            if _fix41afc29_is_truthy(anchor_hash):
                cohort = [e for e in ev if isinstance(e, dict) and str(e.get("anchor_hash") or "") == str(anchor_hash)]

            # Step 2: year cohort (semantic)
            if not cohort and target_year is not None:
                cohort = [e for e in ev if isinstance(e, dict) and _fix41afc29_in_year_cohort(e, target_year, projected)]

            # Step 3: neighborhood cohort (±30% around selected value_norm)
            if not cohort:
                sel = metric_obj.get("value_norm")
                try:
                    sel = float(sel) if sel is not None else None
                except Exception:
                    sel = None
                if sel is not None and sel != 0:
                    for e in ev:
                        if not isinstance(e, dict):
                            continue
                        vn = e.get("value_norm")
                        try:
                            vn = float(vn) if vn is not None else None
                        except Exception:
                            vn = None
                        if vn is None:
                            continue
                        if abs(vn - sel) / abs(sel) <= 0.30:
                            cohort.append(e)

            if not cohort:
                return False

            # Recompute value_range using cohort only (presentation-only)
            vals = []
            for e in cohort:
                vn = e.get("value_norm")
                try:
                    vn = float(vn) if vn is not None else None
                except Exception:
                    vn = None
                if vn is None:
                    continue
                vals.append(vn)

            if not vals:
                return False

            vmin, vmax = min(vals), max(vals)
            unit_tag = metric_obj.get("unit_tag") or metric_obj.get("unit") or ""
            # display formatting consistent with existing style: 4 sig figs for small, 1 dp for larger
            def _fmt(x):
                try:
                    x = float(x)
                except Exception:
                    return str(x)
                ax = abs(x)
                if ax >= 100:
                    return str(int(round(x)))
                if ax >= 10:
                    return f"{x:.1f}".rstrip("0").rstrip(".")
                if ax >= 1:
                    return f"{x:.2f}".rstrip("0").rstrip(".")
                return f"{x:.4f}".rstrip("0").rstrip(".")

            metric_obj["value_range"] = {
                "min": vmin,
                "max": vmax,
                "n": len(vals),
                "examples": [
                    {"raw": (e.get("raw") or ""), "source_url": (e.get("url") or e.get("source_url") or ""), "context_snippet": (e.get("context_snippet") or "")}
                    for e in cohort[:4]
                    if isinstance(e, dict)
                ],
                "method": "anchor_cohort_lock_fix41afc29",
            }
            metric_obj["value_range_display"] = f"{_fmt(vmin)}–{_fmt(vmax)} {unit_tag}".strip()

            # Also tighten source_span if present (min/mid/max)
            try:
                metric_obj["source_span"] = {
                    "min": vmin,
                    "mid": (vmin + vmax) / 2.0,
                    "max": vmax,
                    "unit": unit_tag,
                }
            except Exception:
                pass

            metric_obj["_value_range_scope_fix41afc29"] = "anchor_cohort_lock"
            return True

        # Apply across rebuilt
        _schema_map_fix41afc29 = None
        try:
            _schema_map_fix41afc29 = (
                prev_response.get("metric_schema_frozen")
                or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
                or prev_response.get("primary_metrics_canonical")
                or {}
            )
        except Exception:
            _schema_map_fix41afc29 = {}

        _cohort_locked_keys_fix41afc29 = []
        if isinstance(rebuilt, dict):
            for _ck, _m in list(rebuilt.items()):
                try:
                    if _fix41afc29_scope_value_range(_m, _ck, _schema_map_fix41afc29):
                        _cohort_locked_keys_fix41afc29.append(_ck)
                except Exception:
                    pass

        try:
            prev_response.setdefault("_evolution_rebuild_debug", {})
            prev_response["_evolution_rebuild_debug"]["value_range_cohort_locked_fix41afc29_count"] = len(_cohort_locked_keys_fix41afc29)
            prev_response["_evolution_rebuild_debug"]["value_range_cohort_locked_fix41afc29_keys"] = _cohort_locked_keys_fix41afc29[:50]
        except Exception:
            pass
    except Exception:
        pass
    # PATCH FIX41AFC29 END


    # PATCH FIX41AFC30 START
    # Schema-unit rescaling for evidence/value_range aggregation (presentation-only).
    # Some evidence items may carry value_norm in a different magnitude scale (e.g., billions)
    # while the metric schema/unit_tag expects millions. This patch rescales evidence-derived
    # ranges into the schema's display unit scale to eliminate misleading ranges.
    try:
        def _fix41afc30_unit_mult_from_text(u: str):
            try:
                u = (u or "").lower()
            except Exception:
                u = ""
            if not u:
                return None
            # magnitude tokens / words
            if "trillion" in u or u.strip() in ("t",):
                return 1e12
            if "billion" in u or u.strip() in ("b",):
                return 1e9
            if "million" in u or u.strip() in ("m",):
                return 1e6
            if "thousand" in u or u.strip() in ("k",):
                return 1e3
            return None

        def _fix41afc30_get_mult_schema(metric_obj: dict):
            if not isinstance(metric_obj, dict):
                return None
            # Prefer explicit multiplier fields if present
            m = metric_obj.get("multiplier_to_base")
            try:
                m = float(m) if m is not None else None
            except Exception:
                m = None
            if m and m > 0:
                return m
            # Derive from unit_tag/unit strings
            ut = metric_obj.get("unit_tag") or ""
            uu = metric_obj.get("unit") or ""
            return _fix41afc30_unit_mult_from_text(ut) or _fix41afc30_unit_mult_from_text(uu)

        def _fix41afc30_get_mult_cand(e: dict):
            if not isinstance(e, dict):
                return None
            m = e.get("multiplier_to_base")
            try:
                m = float(m) if m is not None else None
            except Exception:
                m = None
            if m and m > 0:
                return m
            ut = e.get("unit_tag") or e.get("unit") or ""
            bu = e.get("base_unit") or ""
            return _fix41afc30_unit_mult_from_text(str(ut)) or _fix41afc30_unit_mult_from_text(str(bu))

        def _fix41afc30_rescale_vn_to_schema(metric_obj: dict, e: dict):
            # Returns rescaled value_norm into schema's unit scale (if derivable); else original.
            if not isinstance(e, dict):
                return None
            vn = e.get("value_norm")
            try:
                vn = float(vn) if vn is not None else None
            except Exception:
                vn = None
            if vn is None:
                return None

            # Only apply for magnitude-like families (unit_sales/magnitude). Avoid percent/currency.
            fam = (metric_obj.get("unit_family") or "").lower() if isinstance(metric_obj, dict) else ""
            dim = (metric_obj.get("dimension") or "").lower() if isinstance(metric_obj, dict) else ""
            if ("percent" in fam) or ("percent" in dim):
                return vn
            if ("currency" in fam) or ("currency" in dim):
                return vn

            sm = _fix41afc30_get_mult_schema(metric_obj)
            em = _fix41afc30_get_mult_cand(e)
            if not sm or not em:
                return vn

            # base_value = vn * em ; schema_value = base_value / sm
            try:
                base_val = vn * float(em)
                return base_val / float(sm)
            except Exception:
                return vn

        def _fix41afc30_apply_rescaled_ranges(metric_obj: dict, canonical_key: str):
            if not isinstance(metric_obj, dict):
                return False
            vr = metric_obj.get("value_range") or {}
            if not isinstance(vr, dict):
                return False
            # Only rescale when range was computed by cohort/anchor scoping logic
            method = (vr.get("method") or "")
            if ("fix41afc29" not in method) and ("fix41afc27" not in method):
                return False

            ev = metric_obj.get("evidence") or []
            if not isinstance(ev, list) or not ev:
                return False

            # Use the same cohort criteria already applied: if cohort examples exist, try to match by raw strings.
            # Otherwise rescale all evidence and rely on existing method scoping having reduced noise.
            vals = []
            for e in ev:
                if not isinstance(e, dict):
                    continue
                rvn = _fix41afc30_rescale_vn_to_schema(metric_obj, e)
                if rvn is None:
                    continue
                vals.append(rvn)

            if not vals:
                return False

            vmin = min(vals)
            vmax = max(vals)

            # Update range + display using schema unit_tag/unit
            try:
                metric_obj["value_range"] = dict(vr)
                metric_obj["value_range"]["min"] = vmin
                metric_obj["value_range"]["max"] = vmax
                metric_obj["value_range"]["method"] = str(method) + "|rescaled_fix41afc30"
            except Exception:
                metric_obj["value_range"] = {"min": vmin, "max": vmax, "n": len(vals), "method": "rescaled_fix41afc30"}

            _unit_disp = metric_obj.get("unit_tag") or metric_obj.get("unit") or ""
            try:
                # reuse formatter if present
                _fmt_fn = locals().get("_fix41afc27_fmt_range")
                if callable(_fmt_fn):
                    metric_obj["value_range_display"] = _fmt_fn(vmin, vmax, _unit_disp)
                else:
                    metric_obj["value_range_display"] = f"{vmin:.4g}–{vmax:.4g} {_unit_disp}".strip()
            except Exception:
                metric_obj["value_range_display"] = f"{vmin}–{vmax} {_unit_disp}".strip()

            metric_obj["_value_range_rescaled_fix41afc30"] = True
            return True

        _fix41afc30_rescaled_keys = []
        if isinstance(rebuilt, dict):
            for _ck, _m in list(rebuilt.items()):
                try:
                    if _fix41afc30_apply_rescaled_ranges(_m, _ck):
                        _fix41afc30_rescaled_keys.append(_ck)
                except Exception:
                    pass

        try:
            prev_response["_evolution_rebuild_debug"]["value_range_rescaled_fix41afc30_count"] = len(_fix41afc30_rescaled_keys)
            prev_response["_evolution_rebuild_debug"]["value_range_rescaled_fix41afc30_keys"] = _fix41afc30_rescaled_keys[:50]
        except Exception:
            pass
    except Exception:
        pass
    # PATCH FIX41AFC30 END

    # PATCH FIX41AFC32 START
    # Mandatory schema-unit rescaling + cohort-scoped value_range for ALL paths (presentation-only).
    # Purpose: eliminate residual range volatility when some rebuild paths leave value_range.method as
    # "snapshot_candidates" or otherwise bypass FIX41AFC29/30 hooks. This patch does not change selection
    # (current_value), only recomputes value_range/value_range_display using:
    #   1) Anchor-hash cohort (preferred) when anchored
    #   2) Target-year cohort if year can be inferred
    #   3) ±30% neighborhood around selected value_norm as last resort
    # Then rescales evidence values into the schema unit scale before computing min/max.

    try:
        def _fix41afc32_guess_target_year(metric_obj: dict, schema_obj: dict, canonical_key: str):
            # Heuristic: prefer explicit 4-digit year in canonical_key/schema/name.
            import re
            for s in [canonical_key, (schema_obj or {}).get("name"), (metric_obj or {}).get("name")]:
                try:
                    s = str(s or "")
                except Exception:
                    s = ""
                m = re.search(r"\b(19|20)\d{2}\b", s)
                if m:
                    try:
                        y = int(m.group(0))
                        if 1900 <= y <= 2100:
                            return y
                    except Exception:
                        pass
            return None

        def _fix41afc32_has_unit_evidence(e: dict) -> bool:
            try:
                return bool((e or {}).get("unit_tag") or (e or {}).get("unit") or (e or {}).get("base_unit") or (e or {}).get("unit_family"))
            except Exception:
                return False

        def _fix41afc32_ctx_has_year(e: dict, year: int) -> bool:
            if not year:
                return False
            y = str(int(year))
            for k in ("context", "raw_context", "context_window", "context_text"):
                try:
                    ctx = (e or {}).get(k) or ""
                    if y in str(ctx):
                        return True
                except Exception:
                    continue
            return False

        def _fix41afc32_rescale_vn(metric_obj: dict, e: dict):
            # Prefer FIX41AFC30 rescaler if available in scope, else identity.
            try:
                fn = locals().get("_fix41afc30_rescale_vn_to_schema") or globals().get("_fix41afc30_rescale_vn_to_schema")
                if callable(fn):
                    return fn(metric_obj, e)
            except Exception:
                pass
            try:
                vn = (e or {}).get("value_norm")
                return float(vn) if vn is not None else None
            except Exception:
                return None

        def _fix41afc32_compute_range_from_evidence(metric_obj: dict, schema_obj: dict, canonical_key: str):
            if not isinstance(metric_obj, dict):
                return False
            ev = metric_obj.get("evidence") or []
            if not isinstance(ev, list) or not ev:
                return False

            selected_vn = metric_obj.get("value_norm")
            try:
                selected_vn = float(selected_vn) if selected_vn is not None else None
            except Exception:
                selected_vn = None

            anchored = bool(metric_obj.get("anchor_hash") or metric_obj.get("anchor_used"))
            anchor_hash = metric_obj.get("anchor_hash") or ""
            year = _fix41afc32_guess_target_year(metric_obj, schema_obj, canonical_key)

            # Step 1: cohort filter
            cohort = []
            if anchored and anchor_hash:
                cohort = [e for e in ev if isinstance(e, dict) and (e.get("anchor_hash") == anchor_hash)]
            if not cohort and year:
                cohort = [e for e in ev if isinstance(e, dict) and _fix41afc32_ctx_has_year(e, year)]
            if not cohort and selected_vn is not None:
                # ±30% neighborhood fallback
                lo = selected_vn * 0.70
                hi = selected_vn * 1.30
                cohort = []
                for e in ev:
                    if not isinstance(e, dict):
                        continue
                    vn = _fix41afc32_rescale_vn(metric_obj, e)
                    try:
                        vn = float(vn) if vn is not None else None
                    except Exception:
                        vn = None
                    if vn is None:
                        continue
                    if lo <= vn <= hi:
                        cohort.append(e)

            if not cohort:
                cohort = ev  # last resort: avoid losing range entirely

            # Step 2: rescale all cohort values into schema scale before min/max
            vals = []
            for e in cohort:
                if not isinstance(e, dict):
                    continue
                vn = _fix41afc32_rescale_vn(metric_obj, e)
                try:
                    vn = float(vn) if vn is not None else None
                except Exception:
                    vn = None
                if vn is None:
                    continue
                # Extra safety: ignore bare years when unitless to avoid range pollution
                try:
                    iv = int(vn)
                    if 1900 <= iv <= 2100 and not _fix41afc32_has_unit_evidence(e):
                        continue
                except Exception:
                    pass
                vals.append(vn)

            if not vals:
                return False

            mn = min(vals)
            mx = max(vals)
            vr = metric_obj.get("value_range") if isinstance(metric_obj.get("value_range"), dict) else {}
            vr = dict(vr or {})
            vr["min"] = mn
            vr["max"] = mx
            # Mark method without destroying existing provenance
            m0 = vr.get("method") or "snapshot_candidates"
            if "fix41afc32" not in str(m0):
                vr["method"] = f"{m0}|fix41afc32_mandatory_rescale_cohort"
            metric_obj["value_range"] = vr

            # Display (keep simple and deterministic)
            ut = (schema_obj or {}).get("unit_tag") or metric_obj.get("unit_tag") or metric_obj.get("unit") or ""
            try:
                metric_obj["value_range_display"] = f"{mn:.4g}–{mx:.4g} {ut}".strip()
            except Exception:
                metric_obj["value_range_display"] = ""
            metric_obj["_value_range_scope_fix41afc32"] = "anchor_cohort_or_neighborhood"
            metric_obj["_value_range_rescaled_fix41afc32"] = True
            return True

        # Apply to all rebuilt metrics that have a value_range or evidence list
        _fix41afc32_keys = []
        for _ck, _mobj in (rebuilt or {}).items():
            try:
                _schema_obj = None
                if isinstance(schema_map, dict):
                    _schema_obj = schema_map.get(_ck)
                # If schema_map missing, fall back to prev_response schema stores
                if not _schema_obj and isinstance(prev_response, dict):
                    _schema_obj = (prev_response.get("metric_schema_frozen") or {}).get(_ck) or (prev_response.get("primary_metrics_canonical") or {}).get(_ck)
                if _fix41afc32_compute_range_from_evidence(_mobj, _schema_obj or {}, _ck):
                    _fix41afc32_keys.append(_ck)
            except Exception:
                continue

        try:
            prev_response.setdefault("_evolution_rebuild_debug", {})
            prev_response["_evolution_rebuild_debug"]["value_range_mandatory_fix41afc32_count"] = len(_fix41afc32_keys)
            prev_response["_evolution_rebuild_debug"]["value_range_mandatory_fix41afc32_keys"] = _fix41afc32_keys[:50]
        except Exception:
            pass
    except Exception:
        pass
    # PATCH FIX41AFC32 END


    # PATCH FIX41AFC32_SCORECARD START
    # Extraction quality scorecard (instrumentation-only) to track convergence across patches.
    # Counts are derived from baseline_sources_cache/source_results extracted_numbers pools.
    try:
        def _fix41afc32_scorecard_from_pool(baseline_sources_cache):
            score = {
                "candidate_total": 0,
                "candidate_junk": 0,
                "candidate_year_only": 0,
                "candidate_unitless": 0,
                "candidate_has_unit": 0,
                "candidate_has_percent": 0,
                "candidate_has_currency": 0,
            }

            def _is_year_only(e: dict) -> bool:
                try:
                    vn = e.get("value_norm")
                    if vn is None:
                        return False
                    iv = int(float(vn))
                    if not (1900 <= iv <= 2100):
                        return False
                    # year-only if no unit evidence
                    return not bool(e.get("unit_tag") or e.get("unit") or e.get("base_unit") or e.get("unit_family"))
                except Exception:
                    return False

            def _has_percent(e: dict) -> bool:
                try:
                    if "%" in str(e.get("raw_disp") or ""):
                        return True
                    uf = (e.get("unit_family") or "").lower()
                    ut = (e.get("unit_tag") or e.get("unit") or "").lower()
                    return ("percent" in uf) or ("%" in ut) or ("percent" in ut)
                except Exception:
                    return False

            def _has_currency(e: dict) -> bool:
                try:
                    uf = (e.get("unit_family") or "").lower()
                    if "currency" in uf:
                        return True
                    ctx = (e.get("context") or e.get("raw_context") or "")
                    ctx = str(ctx)
                    # light heuristic: common currency symbols/codes
                    return any(tok in ctx for tok in ["$", "USD", "EUR", "SGD", "GBP", "JPY", "CNY", "RMB", "AUD", "CAD"])
                except Exception:
                    return False

            def _is_junk(e: dict) -> bool:
                try:
                    if e.get("is_junk") is True:
                        return True
                    jr = (e.get("junk_reason") or e.get("reject_reason") or "")
                    return bool(jr) and ("junk" in str(jr).lower() or "phone" in str(jr).lower() or "cookie" in str(jr).lower())
                except Exception:
                    return False

            def _has_unit(e: dict) -> bool:
                try:
                    return bool(e.get("unit_tag") or e.get("unit") or e.get("base_unit") or e.get("unit_family"))
                except Exception:
                    return False

            # baseline_sources_cache may be a list of source dicts or dict keyed by url
            src_items = []
            try:
                if isinstance(baseline_sources_cache, list):
                    src_items = baseline_sources_cache
                elif isinstance(baseline_sources_cache, dict):
                    src_items = list(baseline_sources_cache.values())
            except Exception:
                src_items = []

            for s in src_items:
                try:
                    nums = (s or {}).get("extracted_numbers") or (s or {}).get("numbers") or []
                    if not isinstance(nums, list):
                        continue
                    for e in nums:
                        if not isinstance(e, dict):
                            continue
                        score["candidate_total"] += 1
                        if _is_junk(e):
                            score["candidate_junk"] += 1
                        if _is_year_only(e):
                            score["candidate_year_only"] += 1
                        if _has_unit(e):
                            score["candidate_has_unit"] += 1
                        else:
                            score["candidate_unitless"] += 1
                        if _has_percent(e):
                            score["candidate_has_percent"] += 1
                        if _has_currency(e):
                            score["candidate_has_currency"] += 1
                except Exception:
                    continue
            return score

        def _fix41afc32_scorecard_from_rebuilt(rebuilt: dict):
            score = {
                "rebuilt_metric_count": 0,
                "anchor_used_count": 0,
                "schema_eligible_count": 0,
                "blocked_count": 0,
                "top_reject_reasons": {},
            }
            if not isinstance(rebuilt, dict):
                return score
            score["rebuilt_metric_count"] = len(rebuilt)
            for ck, m in rebuilt.items():
                if not isinstance(m, dict):
                    continue
                if m.get("anchor_used") is True:
                    score["anchor_used_count"] += 1
                if m.get("eligibility_pass_reason_fix41afc28"):
                    score["schema_eligible_count"] += 1
                if m.get("cur_value_blocked_reason") or m.get("value_blocked_reason"):
                    score["blocked_count"] += 1
            return score

        _pool_score = _fix41afc32_scorecard_from_pool(baseline_sources_cache)
        _rebuilt_score = _fix41afc32_scorecard_from_rebuilt(rebuilt)

        # Merge and add light rates
        _scorecard = dict(_pool_score or {})
        _scorecard.update(_rebuilt_score or {})
        try:
            tot = float(_scorecard.get("candidate_total") or 0.0)
            if tot > 0:
                _scorecard["junk_rate"] = (_scorecard.get("candidate_junk", 0) / tot)
                _scorecard["year_only_rate"] = (_scorecard.get("candidate_year_only", 0) / tot)
                _scorecard["unitless_rate"] = (_scorecard.get("candidate_unitless", 0) / tot)
        except Exception:
            pass

        try:
            prev_response.setdefault("_evolution_rebuild_debug", {})
            prev_response["_evolution_rebuild_debug"]["scorecard_fix41afc32"] = _scorecard
        except Exception:
            pass
    except Exception:
        pass
    # PATCH FIX41AFC32_SCORECARD END



    # PATCH FIX41AFC34 START
    # Schema-authoritative propagation (unit_family/unit/unit_tag) for rebuilt metric objects.
    # This is presentation/metadata parity only; does NOT affect selection, hashing, or fastpath.
    try:
        _schema_map_fix41afc34 = None
        try:
            _schema_map_fix41afc34 = schema_map if isinstance(locals().get("schema_map"), dict) else None
        except Exception:
            _schema_map_fix41afc34 = None
        if _schema_map_fix41afc34 is None and isinstance(prev_response, dict):
            _schema_map_fix41afc34 = prev_response.get("metric_schema_frozen") or {}
        _applied_fix41afc34 = []
        if isinstance(rebuilt, dict) and isinstance(_schema_map_fix41afc34, dict):
            for _ck_fix41afc34, _mv_fix41afc34 in rebuilt.items():
                if not isinstance(_mv_fix41afc34, dict):
                    continue
                _sc_fix41afc34 = _schema_map_fix41afc34.get(_ck_fix41afc34) or {}
                if not isinstance(_sc_fix41afc34, dict) or not _sc_fix41afc34:
                    continue
                _changed_fix41afc34 = False
                for _fld_fix41afc34 in ("unit_family", "unit", "unit_tag"):
                    _sv_fix41afc34 = _sc_fix41afc34.get(_fld_fix41afc34)
                    if _sv_fix41afc34 is None or _sv_fix41afc34 == "":
                        continue
                    if _mv_fix41afc34.get(_fld_fix41afc34) is None or _mv_fix41afc34.get(_fld_fix41afc34) == "":
                        _mv_fix41afc34[_fld_fix41afc34] = _sv_fix41afc34
                        _changed_fix41afc34 = True
                if _changed_fix41afc34:
                    _mv_fix41afc34["_schema_authority_fix41afc34"] = True
                    _applied_fix41afc34.append(_ck_fix41afc34)
        try:
            if isinstance(prev_response, dict):
                prev_response.setdefault("_evolution_rebuild_debug", {})
                prev_response["_evolution_rebuild_debug"]["schema_authority_applied_fix41afc34_count"] = len(_applied_fix41afc34)
                prev_response["_evolution_rebuild_debug"]["schema_authority_applied_fix41afc34_keys"] = _applied_fix41afc34[:50]
        except Exception:
            pass
    except Exception:
        pass
    # PATCH FIX41AFC34 END
    # =====================================================================
    # PATCH FIX41AFC41D START — DIAG state v2 derived from source_results
    # Instrumentation-only: derive attempted/fetched/failed from actual artifacts
    # so debugging signals cannot drift from real fetch state.
    # =====================================================================
    try:
        _dbg = prev_response.setdefault('_evolution_rebuild_debug', {}) if isinstance(prev_response, dict) else {}
        _diag_fn = globals().get('_fix41afc41_diag_state_v2')
        if callable(_diag_fn):
            _dbg['diag_state_v2_fix41afc41'] = _diag_fn(prev_response)
    except Exception:
        pass
    # =====================================================================
    # PATCH FIX41AFC41D END

    # PATCH FIX41AFC43 START
    # Schema-authoritative normalization reconciliation:
    #   - Fix magnitude value_range double-normalization (e.g., 17.8 -> 0.0178) when unit_tag implies millions.
    #   - Allow dimension='unknown' + unit_family='magnitude' to remain eligible (handled upstream); here we only fix presentation range.
    try:
        _dbg = prev_response.setdefault("_evolution_rebuild_debug", {})
        _vr_fixed_keys = []
        for _ck, _m in (rebuilt or {}).items():
            try:
                if not isinstance(_m, dict):
                    continue
                _md = _m.get("metric_definition") or {}
                _schema = {}
                try:
                    _schema = (prev_response.get("metric_schema_frozen") or {}).get(_ck, {}) or {}
                except Exception:
                    _schema = {}
                # Prefer schema for unit_family/unit_tag
                _uf = (_m.get("unit_family") or _schema.get("unit_family") or _md.get("unit_family") or "").strip().lower()
                _ut = (_m.get("unit_tag") or _schema.get("unit_tag") or _md.get("unit_tag") or _m.get("unit") or "").strip()
                _vr = _m.get("value_range") if isinstance(_m.get("value_range"), dict) else None
                _vn = _m.get("value_norm")
                if not _vr or _vn is None:
                    continue

                # Detect "millions" style metrics that were scaled down again:
                # If value_norm is O(1..1000) but range endpoints are O(0.001..1) and ratio ~1000,
                # it's likely an accidental /1000 applied somewhere.
                if _uf == "magnitude" and _ut:
                    _ut_l = _ut.lower()
                    _is_million = ("million" in _ut_l) or (_ut.strip().upper() == "M") or (" m" == _ut_l)  # defensive
                    if _is_million:
                        _vmin = _vr.get("min")
                        _vmax = _vr.get("max")
                        if isinstance(_vmin, (int, float)) and isinstance(_vmax, (int, float)) and isinstance(_vn, (int, float)):
                            # if already correct, do nothing
                            if _vmax > 1.0 and _vmin > 1.0:
                                pass
                            else:
                                # Use the min ratio (more stable): 17.8 / 0.0178 = 1000
                                _ratio = None
                                try:
                                    if _vmin not in (0, None):
                                        _ratio = float(_vn) / float(_vmin)
                                except Exception:
                                    _ratio = None
                                if _ratio is not None and 850.0 <= _ratio <= 1150.0:
                                    _vr["min"] = float(_vmin) * 1000.0
                                    _vr["max"] = float(_vmax) * 1000.0
                                    _vr["method"] = (str(_vr.get("method") or "").strip() + "|fix41afc43_rescale_million").strip("|")
                                    # Rebuild display string conservatively
                                    try:
                                        _m["value_range_display"] = f"{_vr['min']:.4g}–{_vr['max']:.4g} {_ut}".strip()
                                    except Exception:
                                        pass
                                    _m["_value_range_rescaled_fix41afc43"] = True
                                    _vr_fixed_keys.append(_ck)
            except Exception:
                continue
        _dbg["value_range_rescaled_fix41afc43_count"] = len(_vr_fixed_keys)
        _dbg["value_range_rescaled_fix41afc43_keys"] = _vr_fixed_keys[:50]
    except Exception:
        pass
    # PATCH FIX41AFC43 END


    # =====================================================================
    # ================================================================
    # PATCH FIX41AFC44 START — schema-authoritative value_range rebuild
    # Purpose:
    #   Some magnitude metrics (esp. "million units") were still emitting value_range
    #   as if values were base-units (e.g., 0.0178–0.09 million units) which causes
    #   downstream eligibility/range checks to blank "Current". This patch rebuilds
    #   value_range directly in schema units from evidence/candidate norms.
    try:
        _vr_fix44_cnt = 0
        _vr_fix44_keys = []
        _schema_map_fix44 = schema_map if isinstance(schema_map, dict) else (prev_response.get("metric_schema_frozen") if isinstance(prev_response, dict) else None) or {}
        for _ck_fix44, _m_fix44 in (rebuilt or {}).items():
            if not isinstance(_m_fix44, dict):
                continue
            _schema_fix44 = _schema_map_fix44.get(_ck_fix44) if isinstance(_schema_map_fix44, dict) else None
            if not isinstance(_schema_fix44, dict):
                continue
            # Only act on magnitude-family metrics where schema implies "million" scaling
            _uf_fix44 = (_schema_fix44.get("unit_family") or _m_fix44.get("unit_family") or "").strip().lower()
            if _uf_fix44 != "magnitude":
                continue
            _ut_fix44 = (_schema_fix44.get("unit_tag") or _schema_fix44.get("unit") or _m_fix44.get("unit_tag") or _m_fix44.get("unit") or "")
            _ut_l_fix44 = str(_ut_fix44).lower()
            _implies_million_fix44 = ("million" in _ut_l_fix44) or (_ut_l_fix44.strip() in ("m", "mn"))
            if not _implies_million_fix44:
                continue

            _sel_vn_fix44 = _m_fix44.get("value_norm")
            if _sel_vn_fix44 is None:
                continue

            # Collect evidence norms
            _ev_list_fix44 = _m_fix44.get("evidence") or []
            _ev_vns_fix44 = []
            for _ev_fix44 in _ev_list_fix44:
                if not isinstance(_ev_fix44, dict):
                    continue
                _vn = _ev_fix44.get("value_norm")
                if _vn is None:
                    continue
                try:
                    _vn_f = float(_vn)
                except Exception:
                    continue
                if _vn_f != _vn_f:
                    continue
                _ev_vns_fix44.append(_vn_f)

            # If evidence list is empty, try any embedded candidate list (best-effort)
            if not _ev_vns_fix44:
                _cand_list_fix44 = _m_fix44.get("candidates") or _m_fix44.get("candidates_considered") or []
                for _c_fix44 in _cand_list_fix44:
                    if not isinstance(_c_fix44, dict):
                        continue
                    _vn = _c_fix44.get("value_norm")
                    if _vn is None:
                        continue
                    try:
                        _vn_f = float(_vn)
                    except Exception:
                        continue
                    if _vn_f != _vn_f:
                        continue
                    _ev_vns_fix44.append(_vn_f)

            if len(_ev_vns_fix44) < 2:
                continue

            # Detect double-normalization signature: range is 1000x smaller than selected
            _vr_fix44 = _m_fix44.get("value_range") or {}
            _vr_min_fix44 = _vr_fix44.get("min")
            _vr_max_fix44 = _vr_fix44.get("max")
            try:
                _vr_min_f = float(_vr_min_fix44) if _vr_min_fix44 is not None else None
                _vr_max_f = float(_vr_max_fix44) if _vr_max_fix44 is not None else None
            except Exception:
                _vr_min_f = _vr_max_f = None

            # Normalize evidence norms into schema units:
            # - If evidence norms look like base units (e.g., 17800000) convert to millions
            # - Else assume already in schema units (e.g., 17.8)
            _ev_max_fix44 = max(_ev_vns_fix44)
            _ev_min_fix44 = min(_ev_vns_fix44)
            _ev_units_millions_fix44 = _ev_vns_fix44
            try:
                _sel_vn_f = float(_sel_vn_fix44)
            except Exception:
                _sel_vn_f = None
            if _ev_max_fix44 > 1000 and (_sel_vn_f is not None and _sel_vn_f < 1000):
                _ev_units_millions_fix44 = [v / 1_000_000.0 for v in _ev_vns_fix44]
                _ev_min_fix44 = min(_ev_units_millions_fix44)
                _ev_max_fix44 = max(_ev_units_millions_fix44)

            _need_rebuild_fix44 = False
            try:
                if _sel_vn_f is not None and _vr_min_f is not None and _vr_max_f is not None:
                    if (_sel_vn_f >= 1.0) and (_vr_max_f <= 1.0) and (_vr_max_f > 0):
                        _need_rebuild_fix44 = True
                    if (_vr_max_f < (_ev_max_fix44 * 0.2)):
                        _need_rebuild_fix44 = True
                else:
                    _need_rebuild_fix44 = True
            except Exception:
                _need_rebuild_fix44 = True

            if not _need_rebuild_fix44:
                continue

            _new_min_fix44 = float(_ev_min_fix44)
            _new_max_fix44 = float(_ev_max_fix44)
            if _new_min_fix44 > _new_max_fix44:
                _new_min_fix44, _new_max_fix44 = _new_max_fix44, _new_min_fix44

            _m_fix44.setdefault("value_range", {})
            if isinstance(_m_fix44.get("value_range"), dict):
                _m_fix44["value_range"]["min"] = _new_min_fix44
                _m_fix44["value_range"]["max"] = _new_max_fix44
                _m_fix44["value_range"]["method"] = str(_m_fix44["value_range"].get("method") or "") + "|fix41afc44_schema_unit_range"
            _unit_out_fix44 = (_m_fix44.get("unit_tag") or _schema_fix44.get("unit_tag") or _schema_fix44.get("unit") or _m_fix44.get("unit") or "").strip()
            if _unit_out_fix44 and ("million" not in _unit_out_fix44.lower()) and (_unit_out_fix44.lower() in ("m", "mn")):
                _unit_out_fix44 = "million"
            _m_fix44["value_range_display"] = f"{_new_min_fix44:g}–{_new_max_fix44:g} {_unit_out_fix44}".strip()

            _m_fix44["_value_range_rebuilt_fix41afc44"] = True
            _vr_fix44_cnt += 1
            if len(_vr_fix44_keys) < 50:
                _vr_fix44_keys.append(_ck_fix44)
        if isinstance(prev_response, dict):
            prev_response.setdefault("_evolution_rebuild_debug", {})
            prev_response["_evolution_rebuild_debug"]["value_range_rebuilt_fix41afc44_count"] = _vr_fix44_cnt
            prev_response["_evolution_rebuild_debug"]["value_range_rebuilt_fix41afc44_keys"] = _vr_fix44_keys
    except Exception as _e_fix44:
        try:
            if isinstance(prev_response, dict):
                prev_response.setdefault("_evolution_rebuild_debug", {})
                prev_response["_evolution_rebuild_debug"]["value_range_rebuilt_fix41afc44_error"] = str(_e_fix44)
        except Exception:
            pass
    # PATCH FIX41AFC44 END
    # ================================================================

    return rebuilt


# Re-wire schema-only entrypoint to FIX18 (keep names identical for evolution dispatch)
def rebuild_metrics_from_snapshots_schema_only(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:  # noqa: F811
    return rebuild_metrics_from_snapshots_schema_only_fix18(prev_response, baseline_sources_cache, web_context=web_context)

# =====================================================================
# END PATCH FIX18
# =====================================================================


# ==============================================================================
# PATCH FIX24 (ADDITIVE): Sheets-first replay for unchanged sources+data, and
# scrape+hash gate for evolution to prevent any rebuild/picking when unchanged.
#
# Goals:
#   1) Evolution ALWAYS performs a current scrape/fetch pass to compute a "current"
#      snapshot hash (v2 preferred).
#   2) If current hash == prior analysis hash (v2 preferred), evolution stops and
#      replays the prior FULL analysis payload rehydrated from Google Sheets,
#      publishing metrics to the dashboard WITHOUT any metric rebuild/selection.
#   3) If hashes differ, evolution proceeds via the existing deterministic path
#      (same rebuild/anchors logic as used elsewhere in this codebase).
#
# This patch is purely additive:
#   - Preserves original run_source_anchored_evolution as run_source_anchored_evolution_BASE
#   - Adds helper functions prefixed _fix24_*
#   - Overrides run_source_anchored_evolution by re-defining it below
# ==============================================================================

try:
    run_source_anchored_evolution_BASE = run_source_anchored_evolution  # type: ignore
except Exception:
    run_source_anchored_evolution_BASE = None  # type: ignore


def _fix24_get_prev_full_payload(previous_data: dict) -> dict:
    """
    Load the FULL prior analysis payload from Google Sheets if possible.
    Falls back to previous_data if already full.
    """
    try:
        if not isinstance(previous_data, dict):
            return {}
        # If it already looks like a full payload (contains canonical metrics), return as-is
        if isinstance(previous_data.get("primary_metrics_canonical"), dict) and previous_data["primary_metrics_canonical"]:
            return previous_data

        # Preferred: explicit snapshot_store_ref / full_store_ref
        ref = previous_data.get("full_store_ref") or previous_data.get("snapshot_store_ref") or ""
        # Fallback: sheet id
        if (not ref) and isinstance(previous_data.get("_sheet_id"), str) and previous_data.get("_sheet_id"):
            # Assume HistoryFull
            ref = f"gsheet:HistoryFull:{previous_data.get('_sheet_id')}"

        if isinstance(ref, str) and ref.startswith("gsheet:"):
            parts = ref.split(":")
            ws_title = parts[1] if len(parts) > 1 and parts[1] else "HistoryFull"
            aid = parts[2] if len(parts) > 2 else ""
            if aid:
                fn = globals().get("load_full_history_payload_from_sheet")
                if callable(fn):
                    full = fn(aid, worksheet_title=ws_title)
                    if isinstance(full, dict) and full:
                        return full
    except Exception:
        pass

    return previous_data if isinstance(previous_data, dict) else {}


def _fix24_extract_source_urls(prev_full: dict) -> list:
    """
    Determine the URL list to fetch for current-hash computation.
    Uses analysis 'sources' if available, else URLs from baseline_sources_cache.
    """
    urls = []
    try:
        if isinstance(prev_full, dict):
            s = prev_full.get("sources")
            if isinstance(s, list) and s:
                urls = [str(u) for u in s if isinstance(u, str) and u.strip()]
            if not urls:
                # Try results.source_results urls
                r = prev_full.get("results") if isinstance(prev_full.get("results"), dict) else {}
                sr = r.get("source_results") if isinstance(r, dict) else None
                if isinstance(sr, list):
                    for item in sr:
                        if isinstance(item, dict):
                            u = item.get("url") or item.get("source_url")
                            if u:
                                urls.append(str(u))
            if not urls:
                # Try baseline_sources_cache urls
                r = prev_full.get("results") if isinstance(prev_full.get("results"), dict) else {}
                bsc = None
                if isinstance(r, dict):
                    bsc = r.get("baseline_sources_cache")
                if not isinstance(bsc, list):
                    bsc = prev_full.get("baseline_sources_cache")
                if isinstance(bsc, list):
                    for item in bsc:
                        if isinstance(item, dict):
                            u = item.get("source_url") or item.get("url")
                            if u:
                                urls.append(str(u))
    except Exception:
        pass

    # Stable de-dupe order
    seen = set()
    out = []
    for u in urls:
        uu = (u or "").strip()
        if not uu or uu in seen:
            continue
        seen.add(uu)
        out.append(uu)
    return out[:25]


def _fix24_build_scraped_meta(urls: list, max_chars_per_source: int = 180000) -> dict:
    """
    Fetch each URL (deterministically) and return scraped_meta in the same shape
    attach_source_snapshots_to_analysis expects: {url: {"status":..., "text":..., "extracted_numbers":[...]}}
    """
    scraped_meta = {}
    fetch_fn = globals().get("fetch_url_content_with_status") or globals().get("fetch_url_content")
    extract_fn = globals().get("extract_numbers_with_context")

    for u in urls or []:
        url = str(u or "").strip()
        if not url:
            continue
        try:
            if callable(fetch_fn) and fetch_fn.__name__.endswith("_with_status"):
                text, status = fetch_fn(url)
            elif callable(fetch_fn):
                text = fetch_fn(url)
                status = "success_direct" if (text and str(text).strip()) else "empty"
            else:
                text, status = (None, "no_fetch_fn")

            txt = "" if text is None else str(text)
            if max_chars_per_source and len(txt) > int(max_chars_per_source):
                txt = txt[: int(max_chars_per_source)]

            nums = []
            if callable(extract_fn) and txt.strip():
                try:
                    nums = extract_fn(txt, source_url=url)
                    if nums is None:
                        nums = []
                except Exception:
                    nums = []

            scraped_meta[url] = {
                "status": status,
                "text": txt,
                "extracted_numbers": nums if isinstance(nums, list) else [],
            }
        except Exception as e:
            scraped_meta[url] = {"status": f"exception:{type(e).__name__}", "text": "", "extracted_numbers": []}

    return scraped_meta


def _fix24_baseline_sources_cache_from_scraped_meta(scraped_meta: dict) -> list:
    """
    Use attach_source_snapshots_to_analysis (existing deterministic normalizer) to produce
    baseline_sources_cache from scraped_meta, ensuring value_norm/unit_tag fields are present.
    """
    try:
        fn = globals().get("attach_source_snapshots_to_analysis")
        if not callable(fn):
            return []
        dummy = {"results": {}}
        web_context = {"scraped_meta": scraped_meta or {}}
        fn(dummy, web_context)
        r = dummy.get("results") if isinstance(dummy.get("results"), dict) else {}
        bsc = r.get("baseline_sources_cache") if isinstance(r, dict) else None
        return bsc if isinstance(bsc, list) else []
    except Exception:
        return []


def _fix24_get_prev_hashes(prev_full: dict) -> dict:
    """
    Extract prior snapshot hashes (v2 preferred) from a full analysis payload.
    """
    out = {"v2": "", "v1": ""}
    try:
        if not isinstance(prev_full, dict):
            return out
        out["v2"] = str(prev_full.get("source_snapshot_hash_v2") or "")
        out["v1"] = str(prev_full.get("source_snapshot_hash") or "")
        r = prev_full.get("results") if isinstance(prev_full.get("results"), dict) else {}
        if isinstance(r, dict):
            out["v2"] = out["v2"] or str(r.get("source_snapshot_hash_v2") or "")
            out["v1"] = out["v1"] or str(r.get("source_snapshot_hash") or "")
    except Exception:
        pass
    return out


def _fix24_compute_current_hashes(baseline_sources_cache: list) -> dict:
    """
    Compute current snapshot hashes (v2 preferred).
    """
    out = {"v2": "", "v1": ""}
    try:
        fn1 = globals().get("compute_source_snapshot_hash")
        fn2 = globals().get("compute_source_snapshot_hash_v2")
        if callable(fn2):
            out["v2"] = str(fn2(baseline_sources_cache) or "")
        if callable(fn1):
            out["v1"] = str(fn1(baseline_sources_cache) or "")
    except Exception:
        pass
    return out


def _fix24_make_replay_output(prev_full: dict, hashes: dict) -> dict:
    """
    Build a minimal evolution payload for the dashboard that reflects the prior analysis
    payload verbatim (no rebuild). This avoids the diff panel showing years by ensuring
    the "evolution column" is sourced from stored canonical metrics.
    """
    pmc = prev_full.get("primary_metrics_canonical") if isinstance(prev_full, dict) else {}
    pmc = pmc if isinstance(pmc, dict) else {}

    # Build a deterministic "no-change" metric_changes list WITHOUT re-selecting metrics.
    metric_changes = []
    try:
        for ckey in sorted(pmc.keys()):
            m = pmc.get(ckey) if isinstance(pmc.get(ckey), dict) else {}
            name = str(m.get("name") or m.get("metric_name") or ckey)
            v = m.get("value_norm", m.get("value"))
            unit = m.get("base_unit") or m.get("unit_tag") or m.get("unit") or ""
            metric_changes.append({
                "canonical_key": ckey,
                "name": name,
                "previous_value": v,
                "current_value": v,
                "previous_unit": unit,
                "current_unit": unit,
                "change_type": "unchanged",
                "confidence": 1.0,
            })
    except Exception:
        metric_changes = []

    return {
        "status": "ok",
        "mode": "replay_unchanged_fix24",
        "message": "Sources + data unchanged (hash match). Replaying prior analysis snapshot from Sheets.",
        "sources_checked": int(len(prev_full.get("sources") or [])) if isinstance(prev_full, dict) else 0,
        "sources_fetched": 0,
        "sources_failed": 0,
        "sources_skipped": 0,
        "source_results": [],
        "metric_changes": metric_changes,
        "change_stats": {
            "metrics_increased": 0,
            "metrics_decreased": 0,
            "metrics_unchanged": len(metric_changes),
            "metrics_total": len(metric_changes),
        },
        "debug": {
            "fix24": True,
            "prev_source_snapshot_hash_v2": hashes.get("prev_v2",""),
            "cur_source_snapshot_hash_v2": hashes.get("cur_v2",""),
            "prev_source_snapshot_hash": hashes.get("prev_v1",""),
            "cur_source_snapshot_hash": hashes.get("cur_v1",""),
            "hash_equal_v2": bool(hashes.get("prev_v2") and hashes.get("cur_v2") and hashes.get("prev_v2")==hashes.get("cur_v2")),
            "hash_equal_v1": bool(hashes.get("prev_v1") and hashes.get("cur_v1") and hashes.get("prev_v1")==hashes.get("cur_v1")),
        },
        # Provide the replay payload so the dashboard can render canonical metrics directly if desired
        "replay_analysis_payload": prev_full,
    }


# PATCH FIX41G: removed misplaced top-level web_context normalization block (was causing NameError)


def run_source_anchored_evolution(previous_data: dict, web_context: dict = None) -> dict:
    """
    PATCH FIX24 (ADDITIVE): Evolution flow is:
      1) Rehydrate prior full analysis payload from Sheets (HistoryFull)
      2) Scrape/fetch current sources to build scraped_meta + baseline_sources_cache_current
      3) Compute current snapshot hash (v2 preferred)
      4) If hash matches prior analysis: STOP and replay from Sheets (no rebuild/selection)
      5) If changed: proceed with the existing deterministic evolution path, but ensure
         it routes through the same snapshot/anchor deterministic plumbing used elsewhere.

    Note: This does NOT refactor existing evolution code; it wraps it.
    """
    # Step 1: Rehydrate prior payload
    prev_full = _fix24_get_prev_full_payload(previous_data or {})
    prev_hashes = _fix24_get_prev_hashes(prev_full)

    # Step 2: Build current scraped_meta by fetching the same URLs used previously
    urls = _fix24_extract_source_urls(prev_full)
    # =====================================================================
    # PATCH FIX41AFC3 (ADDITIVE): Recover Evolution injected URLs into web_context['extra_urls']
    #
    # Purpose:
    # - Streamlit may provide injected URLs only via diagnostic fields
    #   (diag_extra_urls_ui / diag_extra_urls_ui_raw).
    # - Downstream evolution admission & fetch logic keys off web_context['extra_urls'].
    #
    # Behavior:
    # - If web_context['extra_urls'] is empty/missing, recover from (in order):
    #     1) web_context['diag_extra_urls_ui']     (list)
    #     2) web_context['diag_extra_urls_ui_raw'] (str; newline/comma separated)
    # - Normalize/canonicalize via _inj_diag_norm_url_list (tracking params stripped).
    #
    # Safety:
    # - Purely additive wiring; no effect when no injection is present.
    # - Never raises; falls back silently.
    # =====================================================================
    try:
        if isinstance(web_context, dict):
            _wc_extra0 = web_context.get('extra_urls')
            _needs = (not isinstance(_wc_extra0, (list, tuple)) or not _wc_extra0)
            if _needs:
                _recovered = []
                _v_list = web_context.get('diag_extra_urls_ui')
                if isinstance(_v_list, (list, tuple)) and _v_list:
                    _recovered = list(_v_list)
                if not _recovered:
                    _raw = web_context.get('diag_extra_urls_ui_raw')
                    if isinstance(_raw, str) and _raw.strip():
                        _parts = []
                        for _line in _raw.splitlines():
                            _line = (_line or '').strip()
                            if not _line:
                                continue
                            for _p in _line.split(','):
                                _p = (_p or '').strip()
                                if _p:
                                    _parts.append(_p)
                        if _parts:
                            _recovered = _parts
                if _recovered:
                    _recovered_norm = _inj_diag_norm_url_list(_recovered)
                    if _recovered_norm:
                        web_context['extra_urls'] = list(_recovered_norm)
                        web_context.setdefault('debug', {})
                        if isinstance(web_context.get('debug'), dict):
                            web_context['debug'].setdefault('fix41afc3', {})
                            if isinstance(web_context['debug'].get('fix41afc3'), dict):
                                web_context['debug']['fix41afc3'].update({
                                    'extra_urls_recovered': True,
                                    'extra_urls_recovered_count': int(len(_recovered_norm)),
                                })
    except Exception:
        pass
    # =====================================================================


    # =====================================================================
    # PATCH EVO_ROUTE_INJECTED_URLS_THROUGH_FWC_V1 (ADDITIVE)
    #
    # Goal:
    # - When Evolution UI provides injected URLs, route them through the SAME
    #   admission/normalization/dedupe logic used by analysis (fetch_web_context),
    #   but in IDENTITY-ONLY mode (no scraping).
    #
    # Why:
    # - Previously, injected URLs could appear in ui_norm/intake_norm but never
    #   reach the admission gate, yielding empty admission_decisions.
    #
    # Behavior:
    # - Only active when web_context['extra_urls'] is non-empty.
    # - Does NOT change fastpath logic directly; it only defines the "current URL
    #   universe" inputs (urls) used for hashing/scrape_meta building.
    #
    # Safety:
    # - identity_only=True prevents any network scrape inside fetch_web_context.
    # - Purely additive; if anything fails, it falls back to existing urls list.
    # =====================================================================
    try:
        _evo_extra_urls_raw = (web_context or {}).get("extra_urls") or []
        _evo_extra_urls_norm = _inj_diag_norm_url_list(_evo_extra_urls_raw)
        if _evo_extra_urls_norm:
            _baseline_urls_for_fwc = _fix24_extract_source_urls(prev_full) or []
            _baseline_urls_for_fwc_norm = _inj_diag_norm_url_list(_baseline_urls_for_fwc)
            _q_for_fwc = str((prev_full or {}).get("question") or (previous_data or {}).get("question") or "").strip()
            _fwc = fetch_web_context(
                _q_for_fwc or "evolution_identity_only",
                num_sources=int(min(12, max(1, len(_baseline_urls_for_fwc_norm) + len(_evo_extra_urls_norm)))),
                fallback_mode=True,
                fallback_urls=_baseline_urls_for_fwc_norm,
                existing_snapshots=(prev_full or {}).get("baseline_sources_cache") or (prev_full or {}).get("baseline_sources_cache_v2") or None,
                extra_urls=_evo_extra_urls_norm,
                diag_run_id=str((web_context or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                diag_extra_urls_ui_raw=(web_context or {}).get("diag_extra_urls_ui_raw"),
                identity_only=True,
            ) or {}
            _fwc_admitted = _fwc.get("web_sources") or _fwc.get("sources") or []
            if isinstance(_fwc_admitted, list) and _fwc_admitted:
                urls = list(_fwc_admitted)
            # Attach the admission decisions to web_context for unified inj_trace reporting
            if isinstance(web_context, dict):
                if isinstance(_fwc.get("diag_injected_urls"), dict):
                    web_context.setdefault("diag_injected_urls", {})
                    if isinstance(web_context.get("diag_injected_urls"), dict):
                        # do not clobber if already present
                        for _k, _v in _fwc.get("diag_injected_urls").items():
                            web_context["diag_injected_urls"].setdefault(_k, _v)
                web_context.setdefault("debug", {})
                if isinstance(web_context.get("debug"), dict):
                    web_context["debug"].setdefault("evo_fwc_identity_only", {})
                    if isinstance(web_context["debug"].get("evo_fwc_identity_only"), dict):
                        web_context["debug"]["evo_fwc_identity_only"].update({
                            "called": True,
                            "baseline_urls_count": int(len(_baseline_urls_for_fwc_norm)),
                            "extra_urls_count": int(len(_evo_extra_urls_norm)),
                            "admitted_count": int(len(urls or [])),
                            "admitted_set_hash": _inj_diag_set_hash(_inj_diag_norm_url_list(urls or [])),
                        })
    except Exception:
        pass
    # =====================================================================
    # =====================================================================


    # =====================================================================
        # =====================================================================
    # PATCH FIX41AFC9 (ADDITIVE): Merge injected URLs into `urls` universe BEFORE scrape_meta build
    #
    # Problem observed (inj_trace_v1):
    # - Injected URL shows up in ui_norm/intake_norm, but can still vanish from admitted_norm/hash_inputs_norm.
    # - Root cause: the identity-only fetch_web_context() step may replace `urls` with an admitted list
    #   that excludes injected URLs, and later injected logic may read from a different variable/path.
    #
    # Goal:
    # - If injected URLs are present in web_context['extra_urls'], ensure they are ALWAYS merged into the
    #   local `urls` list used by _fix24_build_scraped_meta(), so the injected URLs are at least
    #   attempted (scraped_meta populated) and can become part of current hash identity when successful.
    #
    # Safety:
    # - Only active when injection is present.
    # - Purely additive: does not change hashing algorithm or fastpath rules; it only ensures the URL
    #   universe includes the injected URLs when the user provided them.
    # - Never raises.
    # =====================================================================
    try:
        _fx9_wc = web_context if isinstance(web_context, dict) else {}
        _fx9_inj = _inj_diag_norm_url_list((_fx9_wc or {}).get("extra_urls") or [])
        if _fx9_inj and isinstance(urls, list):
            _fx9_seen = set(_inj_diag_norm_url_list([(_d.get("url") if isinstance(_d, dict) else _d) for _d in urls]))
            _fx9_added = []
            for _u in _fx9_inj:
                if _u in _fx9_seen:
                    continue
                _fx9_seen.add(_u)
                urls.append(_u)
                _fx9_added.append(_u)

            if isinstance(_fx9_wc, dict):
                _fx9_wc.setdefault("debug", {})
                if isinstance(_fx9_wc.get("debug"), dict):
                    _fx9_wc["debug"].setdefault("fix41afc9", {})
                    if isinstance(_fx9_wc["debug"].get("fix41afc9"), dict):
                        _fx9_wc["debug"]["fix41afc9"].update({
                            "merged_into_urls_universe": True,
                            "injected_urls_count": int(len(_fx9_inj)),
                            "added_to_urls_count": int(len(_fx9_added)),
                            "added_to_urls": list(_fx9_added),
                            "urls_count_after_merge": int(len(urls)),
                        })
    except Exception:
        pass
    # =====================================================================

# PATCH FIX41AFC6 (ADDITIVE): When injected URL delta exists, actually FETCH it
    #
    # Observation (from inj_trace_v1 in evolution JSON):
    # - Injected URL appears in ui_norm/intake_norm, but attempted/persisted remain empty,
    #   and the injected URL never reaches hash_inputs because evolution never performs a
    #   fetch cycle for the injected delta (it only replays cached snapshots).
    #
    # Goal:
    # - ONLY when injected URLs introduce a true delta vs the baseline source universe,
    #   run fetch_web_context() in normal mode (identity_only=False) so the injected URL
    #   is actually attempted/persisted and can become a first-class current source
    #   (and thus can affect downstream identity/hash inputs).
    #
    # Safety:
    # - No effect on no-injection runs.
    # - No effect when injection is empty or introduces no delta.
    # - Uses existing_snapshots to avoid re-fetching baseline sources.
    # - Never raises; falls back to existing behavior.
    # =====================================================================
    try:
        _fix41afc6_wc = web_context if isinstance(web_context, dict) else {}
        _fix41afc6_inj = _inj_diag_norm_url_list((_fix41afc6_wc or {}).get("extra_urls") or [])
        _fix41afc6_base = _inj_diag_norm_url_list(_fix24_extract_source_urls(prev_full) or [])
        _fix41afc6_delta = sorted(list(set(_fix41afc6_inj) - set(_fix41afc6_base))) if _fix41afc6_inj else []
        if _fix41afc6_delta:
            _fix41afc6_q = str((prev_full or {}).get("question") or (previous_data or {}).get("question") or "").strip()
            _fix41afc6_prev_snap = (prev_full or {}).get("baseline_sources_cache") or (prev_full or {}).get("baseline_sources_cache_v2") or None

            _fix41afc6_fwc = fetch_web_context(
                _fix41afc6_q or "evolution_injection_fetch",
                num_sources=int(min(12, max(1, len(_fix41afc6_base) + len(_fix41afc6_inj)))),
                fallback_mode=True,
                fallback_urls=_fix41afc6_base,
                existing_snapshots=_fix41afc6_prev_snap,
                extra_urls=_fix41afc6_inj,
                diag_run_id=str((_fix41afc6_wc or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                diag_extra_urls_ui_raw=(_fix41afc6_wc or {}).get("diag_extra_urls_ui_raw"),
                # PATCH FIX41AFC8 (ADDITIVE): force scrape injected extras even if not admitted
                force_scrape_extra_urls=True,
                force_admit_extra_urls=True,
                identity_only=False,
            ) or {}

            # Prefer the admitted list from fetch_web_context (it includes injected URLs that pass admission)
            _fix41afc6_admitted = _fix41afc6_fwc.get("web_sources") or _fix41afc6_fwc.get("sources") or []
            if isinstance(_fix41afc6_admitted, list) and _fix41afc6_admitted:
                urls = list(_fix41afc6_admitted)

            # Bubble up a small marker so inj_trace_v1 can report whether evolution actually called FWC
            if isinstance(web_context, dict):
                web_context["evolution_calls_fetch_web_context"] = True
                web_context.setdefault("debug", {})
                if isinstance(web_context.get("debug"), dict):
                    web_context["debug"].setdefault("fix41afc6", {})
                    if isinstance(web_context["debug"].get("fix41afc6"), dict):
                        web_context["debug"]["fix41afc6"].update({
                            "called_fetch_web_context": True,
                            "injected_delta_count": int(len(_fix41afc6_delta)),
                            "injected_delta": list(_fix41afc6_delta),
                            "admitted_count": int(len(_fix41afc6_admitted or [])) if isinstance(_fix41afc6_admitted, list) else 0,
                        })

                    # =====================================================================
                    # PATCH FIX41AFC8 (ADDITIVE): Emit forced-fetch diagnostics for injected delta
                    # =====================================================================
                    try:
                        web_context.setdefault("debug", {})
                        if isinstance(web_context.get("debug"), dict):
                            web_context["debug"].setdefault("fix41afc8", {})
                            if isinstance(web_context["debug"].get("fix41afc8"), dict):
                                # delta URLs are what we intend to force-attempt
                                _fx8_delta_urls = list(_fix41afc6_delta or [])
                                # attempted/persist outcomes can be inferred from fetch_web_context scraped_meta
                                _fx8_results = {}
                                try:
                                    _fx8_sm = _fix41afc6_fwc.get("scraped_meta") or {}
                                    if isinstance(_fx8_sm, dict):
                                        for _u in _fx8_delta_urls:
                                            meta = _fx8_sm.get(_u) or {}
                                            if isinstance(meta, dict) and meta:
                                                _fx8_results[_u] = meta.get("status") or meta.get("fetch_status") or meta.get("reason") or "attempted"
                                            else:
                                                _fx8_results[_u] = "not_in_scraped_meta"
                                except Exception:
                                    pass
                                web_context["debug"]["fix41afc8"].update({
                                    "forced_fetch_reason": "injected_delta_present_force_fetch_even_if_not_admitted",
                                    "forced_fetch_urls": _fx8_delta_urls,
                                    "forced_fetch_count": int(len(_fx8_delta_urls)),
                                    "forced_fetch_results": _fx8_results,
                                })
                    except Exception:
                        pass

    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH FIX41AFC7 (ADDITIVE): Early recovery + latching of injected URLs into web_context["extra_urls"]
    #
    # Problem observed in evolution JSON:
    # - ui_norm/intake_norm contains the injected URL (from Streamlit textarea),
    #   but web_context["extra_urls"] can still be empty at evolution core, which
    #   causes downstream "fetch injected delta" and "forced admit" patches to see
    #   an empty injected set and skip.
    #
    # Goal:
    # - If web_context["extra_urls"] is empty, recover injected URLs from the same
    #   Streamlit diagnostic fields used at intake:
    #     1) web_context["diag_extra_urls_ui"] (list)
    #     2) web_context["diag_extra_urls_ui_raw"] (string, newline/comma separated)
    # - Normalize/canonicalize deterministically via _inj_diag_norm_url_list().
    # - Latch the recovered list back into web_context["extra_urls"] so ALL later
    #   injected URL logic (fetch + forced admit + hash identity) sees the same set.
    #
    # Safety:
    # - Purely additive. No effect when extra_urls already present.
    # - Never raises; falls back silently.
    # =====================================================================
    try:
        if isinstance(web_context, dict):
            _fix41afc7_norm = []
            _existing = web_context.get("extra_urls")
            _need = (not isinstance(_existing, (list, tuple)) or not list(_existing))
            if _need:
                _raw = []
                _v_list = web_context.get("diag_extra_urls_ui")
                if isinstance(_v_list, (list, tuple)) and _v_list:
                    _raw = list(_v_list)
                if not _raw:
                    _v_raw = web_context.get("diag_extra_urls_ui_raw")
                    if isinstance(_v_raw, str) and _v_raw.strip():
                        _parts = []
                        for _line in _v_raw.splitlines():
                            _line = (_line or "").strip()
                            if not _line:
                                continue
                            for _p in _line.split(","):
                                _p = (_p or "").strip()
                                if _p:
                                    _parts.append(_p)
                        if _parts:
                            _raw = _parts

                _fix41afc7_norm = _inj_diag_norm_url_list(_raw)
                if _fix41afc7_norm:
                    web_context["extra_urls"] = list(_fix41afc7_norm)

            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].setdefault("fix41afc7", {})
                if isinstance(web_context["debug"].get("fix41afc7"), dict):
                    web_context["debug"]["fix41afc7"].update({
                        "recovery_needed": bool(_need),
                        "recovered_extra_urls_count": int(len(_fix41afc7_norm or [])),
                        "recovered_extra_urls": list(_fix41afc7_norm or [])[:20],
                        "extra_urls_present_after_recovery": bool(isinstance(web_context.get("extra_urls"), (list, tuple)) and list(web_context.get("extra_urls") or [])),
                    })
    except Exception:
        pass
    # =====================================================================

# PATCH INJ_DIAG_EVO_CORE (ADDITIVE): allow optional injected URLs (Scenario B)
    # - Only active if caller provides web_context['extra_urls']
    # - Does NOT affect default fastpath behavior.
    # =====================================================================
    _inj_diag_run_id = ""
    _inj_extra_urls = []
    try:
        _inj_diag_run_id = str((web_context or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo")
        _inj_extra_urls = _inj_diag_norm_url_list((web_context or {}).get("extra_urls") or [])
    except Exception:
        _inj_diag_run_id = _inj_diag_make_run_id("evo")
        _inj_extra_urls = []

    try:
        if _inj_extra_urls:
            _u_seen = set([str(u or "").strip() for u in (urls or []) if str(u or "").strip()])
            for _u in _inj_extra_urls:
                if _u not in _u_seen:
                    _u_seen.add(_u)
                    urls.append(_u)
    except Exception:
        pass
    # =====================================================================


    # =====================================================================

    # =====================================================================
    # PATCH FIX41AFC4 (ADDITIVE): Force-admit injected URL deltas into evolution URL universe
    #
    # Problem (observed in evolution JSON):
    # - Injected URL appears in ui_norm/intake_norm but is missing from admitted_norm,
    #   so it never reaches attempted/persisted/hash_inputs.
    # - This typically happens when admission/allowlist logic rejects injected URLs
    #   before the fetch loop, leaving attempted empty.
    #
    # Goal:
    # - ONLY when injection is present AND it introduces a true delta vs the baseline
    #   source universe, ensure the injected URLs are included in `urls` (the universe
    #   FIX24 uses for scrape_meta building).
    #
    # Safety:
    # - Purely additive; no effect when no injection or no delta.
    # - Does not modify fastpath logic/hashing; it only ensures injected URLs are
    #   present in the post-intake universe when delta exists.
    # - Never raises; falls back silently.
    # =====================================================================
    try:
        _fix41afc4_inj_norm = _inj_diag_norm_url_list(_inj_extra_urls or [])
        _fix41afc4_base_norm = _inj_diag_norm_url_list(_fix24_extract_source_urls(prev_full) or [])
        _fix41afc4_delta = sorted(list(set(_fix41afc4_inj_norm) - set(_fix41afc4_base_norm))) if _fix41afc4_inj_norm else []
        _fix41afc4_applied = False

        if _fix41afc4_delta:
            # Determine expected URL container shape (strings vs dicts)
            _urls_list = urls if isinstance(urls, list) else []
            _urls_are_dicts = bool(_urls_list) and isinstance(_urls_list[0], dict)

            # Build a normalized "seen" set from existing urls
            if _urls_are_dicts:
                _seen_norm = set(_inj_diag_norm_url_list([(_d.get("url") if isinstance(_d, dict) else "") for _d in _urls_list]))
            else:
                _seen_norm = set(_inj_diag_norm_url_list(_urls_list))

            for _u in _fix41afc4_delta:
                if _u in _seen_norm:
                    continue
                _seen_norm.add(_u)
                if _urls_are_dicts:
                    _urls_list.append({"url": _u})
                else:
                    _urls_list.append(_u)
                _fix41afc4_applied = True

            urls = _urls_list  # rebind defensively

        if isinstance(web_context, dict):
            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].setdefault("fix41afc4", {})
                if isinstance(web_context["debug"].get("fix41afc4"), dict):
                    web_context["debug"]["fix41afc4"].update({
                        "forced_admit_applied": bool(_fix41afc4_applied),
                        "forced_admit_injected_urls_count": int(len(_fix41afc4_delta)),
                        "forced_admit_injected_urls": list(_fix41afc4_delta),
                        "baseline_urls_count": int(len(_fix41afc4_base_norm)),
                        "urls_after_forced_admit_count": int(len(_inj_diag_norm_url_list([(_d.get("url") if isinstance(_d, dict) else _d) for _d in (urls or [])] if isinstance(urls, list) else []))),
                    })
    except Exception:
        pass
    # =====================================================================


# PATCH EVO_INJ_ADMISSION_TRACE_V1 (ADDITIVE): pinpoint where injected URLs are dropped
    #
    # Why:
    # - When a URL appears in ui_norm/intake_norm but not in admitted_norm, we need
    #   an explicit reason before we change any behavior.
    #
    # What this records (debug only):
    # - Whether evolution is using fetch_web_context (it is not in FIX24 path)
    # - The pre- and post-injection URL universe
    # - Per-injected-URL admission decision + reason codes
    #
    # Safety:
    # - Does NOT alter control flow, fastpath eligibility, scraping, hashing, or selection.
    # =====================================================================
    try:
        _urls_prev_full = _fix24_extract_source_urls(prev_full)
        _urls_prev_full_norm = _inj_diag_norm_url_list(_urls_prev_full or [])
        _urls_after_merge_norm = _inj_diag_norm_url_list(urls or [])

        _admission_decisions = {}
        for _u in (_inj_extra_urls or []):
            if _u in set(_urls_after_merge_norm):
                _admission_decisions[_u] = {
                    "decision": "admitted",
                    "reason_code": "merged_into_urls_for_scrape",
                }
            else:
                _admission_decisions[_u] = {
                    "decision": "rejected",
                    "reason_code": "not_present_in_urls_after_merge",
                }

        if isinstance(web_context, dict):
            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].setdefault("evo_injection_trace", {})
                if isinstance(web_context["debug"].get("evo_injection_trace"), dict):
                    web_context["debug"]["evo_injection_trace"].update({
                        "uses_fetch_web_context": False,
                        "urls_prev_full_count": int(len(_urls_prev_full_norm)),
                        "urls_prev_full_set_hash": _inj_diag_set_hash(_urls_prev_full_norm),
                        "urls_after_merge_count": int(len(_urls_after_merge_norm)),
                        "urls_after_merge_set_hash": _inj_diag_set_hash(_urls_after_merge_norm),
                        "inj_extra_urls_norm": list(_inj_extra_urls or []),
                        "inj_merge_applied": bool(_inj_extra_urls),
                        "inj_admission_decisions": _admission_decisions,
                    })

            # Also attach to diag_injected_urls for unified downstream reporting
            web_context.setdefault("diag_injected_urls", {})
            if isinstance(web_context.get("diag_injected_urls"), dict):
                web_context["diag_injected_urls"].setdefault("admission_decisions", {})
                if isinstance(web_context["diag_injected_urls"].get("admission_decisions"), dict):
                    web_context["diag_injected_urls"]["admission_decisions"].update(_admission_decisions)
                web_context["diag_injected_urls"].setdefault("urls_prev_full_norm", _urls_prev_full_norm)
                web_context["diag_injected_urls"].setdefault("urls_after_merge_norm", _urls_after_merge_norm)
    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH FIX41AFC11 (ADDITIVE): Injection admission override + must-fetch lane (delta-only)
    #
    # Problem:
    # - Injected URLs can appear in UI intake but get dropped pre-admission, resulting in:
    #     attempted=0, persisted_norm=0, hash_inputs_norm unchanged.
    #
    # Goal:
    # - When injected URL DELTA exists (vs current urls baseline), deterministically:
    #     1) Force-admit injected delta into local `urls` universe (so downstream meta sees it)
    #     2) Force-fetch injected delta via fetch_web_context(force_scrape_extra_urls=True),
    #        so we get attempted/persisted entries or an explicit failure reason.
    #
    # Safety:
    # - No effect when no injection / no delta.
    # - Does not weaken normal fastpath logic (already bypassed upstream when delta exists).
    # =====================================================================
    try:
        _fix41afc11_wc = web_context if isinstance(web_context, dict) else {}
        # Robust recovery (order required)
        _fix41afc11_extra = []
        if isinstance(_fix41afc11_wc.get("extra_urls"), (list, tuple)) and _fix41afc11_wc.get("extra_urls"):
            _fix41afc11_extra = list(_fix41afc11_wc.get("extra_urls") or [])
        elif isinstance(_fix41afc11_wc.get("diag_extra_urls_ui"), (list, tuple)) and _fix41afc11_wc.get("diag_extra_urls_ui"):
            _fix41afc11_extra = list(_fix41afc11_wc.get("diag_extra_urls_ui") or [])
        elif isinstance(_fix41afc11_wc.get("diag_extra_urls_ui_raw"), str) and (_fix41afc11_wc.get("diag_extra_urls_ui_raw") or "").strip():
            _raw = str(_fix41afc11_wc.get("diag_extra_urls_ui_raw") or "")
            _parts = []
            for _line in _raw.splitlines():
                _line = (_line or "").strip()
                if not _line:
                    continue
                for _p in _line.split(","):
                    _p = (_p or "").strip()
                    if _p:
                        _parts.append(_p)
            _fix41afc11_extra = _parts

        _fix41afc11_inj_norm = _inj_diag_norm_url_list(_fix41afc11_extra) if _fix41afc11_extra else []
        _fix41afc11_urls_norm = _inj_diag_norm_url_list(urls) if urls else []
        _fix41afc11_inj_set = set(_fix41afc11_inj_norm or [])
        _fix41afc11_base_set = set(_fix41afc11_urls_norm or [])
        _fix41afc11_delta = sorted(list(_fix41afc11_inj_set - _fix41afc11_base_set)) if _fix41afc11_inj_set else []

        if _fix41afc11_delta:
            # (1) Force-admit delta into local urls universe deterministically
            _added = []
            if urls and isinstance(urls[0], dict):
                _seen = set(_fix41afc11_urls_norm or [])
                for _u in _fix41afc11_delta:
                    if _u in _seen:
                        continue
                    urls.append({"url": _u, "source": "injected_force_admit", "is_injected": True})
                    _seen.add(_u)
                    _added.append(_u)
            else:
                _seen = set(_fix41afc11_urls_norm or [])
                for _u in _fix41afc11_delta:
                    if _u in _seen:
                        continue
                    urls.append(_u)
                    _seen.add(_u)
                    _added.append(_u)

            # (2) Must-fetch lane: force scrape extra urls even if not admitted by normal filter
            _q = str((prev_full or {}).get("question") or (previous_data or {}).get("question") or "evolution_injection_force_fetch").strip()
            _prev_snap = (prev_full or {}).get("baseline_sources_cache") or (prev_full or {}).get("baseline_sources_cache_v2") or None
            try:
                _fwc = fetch_web_context(
                    _q or "evolution_injection_force_fetch",
                    num_sources=int(min(12, max(1, len(_fix41afc11_urls_norm or []) + len(_fix41afc11_inj_norm or [])))),
                    fallback_mode=True,
                    fallback_urls=list(_fix41afc11_urls_norm or []),
                    existing_snapshots=_prev_snap,
                    extra_urls=list(_fix41afc11_inj_norm or []),
                    diag_run_id=str((_fix41afc11_wc or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                    diag_extra_urls_ui_raw=(_fix41afc11_wc or {}).get("diag_extra_urls_ui_raw"),
                    identity_only=False,
                    force_scrape_extra_urls=True,
                force_admit_extra_urls=True,
                ) or {}
            except TypeError:
                # Backward-compat: older fetch_web_context without force_scrape_extra_urls
                _fwc = fetch_web_context(
                    _q or "evolution_injection_force_fetch",
                    num_sources=int(min(12, max(1, len(_fix41afc11_urls_norm or []) + len(_fix41afc11_inj_norm or [])))),
                    fallback_mode=True,
                    fallback_urls=list(_fix41afc11_urls_norm or []),
                    existing_snapshots=_prev_snap,
                    extra_urls=list(_fix41afc11_inj_norm or []),
                    diag_run_id=str((_fix41afc11_wc or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                    diag_extra_urls_ui_raw=(_fix41afc11_wc or {}).get("diag_extra_urls_ui_raw"),
                    identity_only=False,
                ) or {}

            # If fetch_web_context returns a concrete web_sources list, prefer it for downstream scraped_meta
            _fwc_sources = _fwc.get("web_sources") or _fwc.get("sources") or None
            if isinstance(_fwc_sources, list) and _fwc_sources:
                urls = list(_fwc_sources)

            # Emit explicit debug fields
            if isinstance(web_context, dict):
                web_context.setdefault("debug", {})
                if isinstance(web_context.get("debug"), dict):
                    web_context["debug"].setdefault("fix41afc11", {})
                    if isinstance(web_context["debug"].get("fix41afc11"), dict):
                        web_context["debug"]["fix41afc11"].update({
                            "inj_force_admit_applied": True,
                            "inj_force_admit_count": int(len(_added)),
                            "inj_force_admit_urls": list(_added),
                            "inj_delta_count": int(len(_fix41afc11_delta)),
                            "inj_delta": list(_fix41afc11_delta),
                            "inj_must_fetch_called": True,
                            "inj_must_fetch_sources_count": int(len(_fwc_sources or [])) if isinstance(_fwc_sources, list) else 0,
                        })
    except Exception:
        pass
    # =====================================================================



    scraped_meta = _fix24_build_scraped_meta(urls)

    # Step 3: Normalize into baseline_sources_cache and hash
    cur_bsc = _fix24_baseline_sources_cache_from_scraped_meta(scraped_meta)

    # =====================================================================
    # PATCH EVO_INJECTED_URLS_AS_CURRENT_SOURCES_V1 (ADDITIVE):
    # Policy + wiring alignment for Evolution UI injected URLs
    #
    # Goal:
    # - Treat Evolution-tab injected URLs as part of the *current source universe*
    #   for hash identity *when they fetch successfully*, consistent with baseline
    #   sources (successful snapshots contribute to identity).
    #
    # Behavior (safe):
    # - If an injected URL was provided (web_context['extra_urls']) and its scrape
    #   status is success, it must appear in cur_bsc so that hash inputs can include it.
    # - If success-but-missing occurs (unexpected), we add a synthetic url-only entry
    #   tagged for hash identity (debug only; no numbers).
    # - If fetch failed, we do NOT force hash mismatch (consistent with policy),
    #   but we record explicit attempted status + reason into diagnostics.
    #
    # Safety:
    # - Does NOT alter fastpath eligibility logic directly; it only ensures that
    #   the identity inputs reflect the actual successfully fetched current sources.
    # - Purely additive; never removes or refactors existing logic.
    # =====================================================================
    try:
        _inj_sm = scraped_meta if isinstance(scraped_meta, dict) else {}
        _inj_attempted_rows = []
        _inj_success_urls = set()
        for _u in (_inj_extra_urls or []):
            _m = _inj_sm.get(_u) if isinstance(_inj_sm.get(_u), dict) else {}
            _st = str(_m.get('status') or _m.get('fetch_status') or '')
            _reason = str(_m.get('status_detail') or _m.get('fail_reason') or '')
            _clen = _m.get('clean_text_len') or _m.get('content_len') or 0
            _inj_attempted_rows.append({
                'url': _u,
                'status': _st or 'unknown',
                'reason': _reason,
                'content_len': int(_clen) if str(_clen).isdigit() else 0,
            })
            if (_st or '').lower() in ('success','ok','fetched'):
                _inj_success_urls.add(_u)

        # Ensure success injected urls are represented in cur_bsc (hash identity)
        if _inj_success_urls and isinstance(cur_bsc, list):
            _bsc_urls = set()
            for _row in cur_bsc:
                if isinstance(_row, dict):
                    _bu = str(_row.get('url') or _row.get('source_url') or '').strip()
                    if _bu:
                        _bsc_urls.add(_bu)
            _missing_success = sorted(list(_inj_success_urls - _bsc_urls))
            for _u in _missing_success:
                cur_bsc.append({
                    'url': _u,
                    'status': 'success',
                    'status_detail': 'synthetic_success_missing_in_bsc',
                    'clean_text': '',
                    'clean_text_len': 0,
                    'extracted_numbers': [],
                    'numbers_found': 0,
                    'fingerprint': 'synthetic_url_only_for_hash',
                    'is_synthetic_for_hash': True,
                })

        # Write attempted status into web_context diagnostics for transparency
        if isinstance(web_context, dict):
            web_context.setdefault('diag_injected_urls', {})
            if isinstance(web_context.get('diag_injected_urls'), dict):
                web_context['diag_injected_urls'].setdefault('attempted', [])
                # Only overwrite if empty to avoid clobbering richer traces
                if not web_context['diag_injected_urls'].get('attempted'):
                    web_context['diag_injected_urls']['attempted'] = _inj_attempted_rows
                web_context['diag_injected_urls']['success_urls'] = sorted(list(_inj_success_urls))
    except Exception:
        pass
    # =====================================================================

    cur_hashes = _fix24_compute_current_hashes(cur_bsc)

    # =====================================================================

    # PATCH INJ_DIAG_EVO_DEBUG (ADDITIVE): record injected URL lifecycle (B1)
    # =====================================================================
    try:
        _hash_inputs = _inj_diag_hash_inputs_from_bsc(cur_bsc)
        if isinstance(web_context, dict):
            web_context.setdefault("diag_injected_urls", {})
            if isinstance(web_context.get("diag_injected_urls"), dict):
                web_context["diag_injected_urls"].update({
                    "run_id": _inj_diag_run_id,
                    "ui_raw": (web_context or {}).get("diag_extra_urls_ui_raw") or "",
                    "ui_norm": _inj_extra_urls,
                    "intake_norm": _inj_extra_urls,
                    "admitted": list(urls or []),
                    "hash_inputs": _hash_inputs,
                    "injected_in_hash_inputs": sorted(list(set(_inj_extra_urls) & set(_hash_inputs))),
                    "set_hashes": {
                        "hash_inputs": _inj_diag_set_hash(_hash_inputs),
                        "admitted": _inj_diag_set_hash(list(urls or [])),
                    }
                })
    except Exception:
        pass
    # =====================================================================



    # Step 4: Compare (v2 preferred)
    equal_v2 = bool(prev_hashes.get("v2") and cur_hashes.get("v2") and prev_hashes["v2"] == cur_hashes["v2"])
    equal_v1 = bool(prev_hashes.get("v1") and cur_hashes.get("v1") and prev_hashes["v1"] == cur_hashes["v1"])
    unchanged = equal_v2 or (not prev_hashes.get("v2") and equal_v1)

    # =====================================================================
    # PATCH FIX40 (ADDITIVE): Force rebuild override (Scenario B)
    # If the UI (or caller) requests force_rebuild, we intentionally bypass
    # the unchanged fastpath even if hashes match, to exercise rebuild logic.
    # =====================================================================
    _force_rebuild = False
    try:
        _force_rebuild = bool((web_context or {}).get("force_rebuild"))
    except Exception:
        _force_rebuild = False
    if _force_rebuild:
        unchanged = False
        _fix41_force_rebuild_honored = True
    else:
        _fix41_force_rebuild_honored = False
    # =====================================================================

    if unchanged:
        hashes = {
            "prev_v2": prev_hashes.get("v2",""),
            "cur_v2": cur_hashes.get("v2",""),
            "prev_v1": prev_hashes.get("v1",""),
            "cur_v1": cur_hashes.get("v1",""),
        }
        out_replay = _fix24_make_replay_output(prev_full, hashes)
        # =====================================================================
        # PATCH FIX41 (ADDITIVE): Attach force-rebuild debug to replay output
        # =====================================================================
        try:
            if isinstance(out_replay, dict):
                out_replay.setdefault("code_version", CODE_VERSION)
                out_replay.setdefault("debug", {}).setdefault("fix41", {})
                out_replay["debug"]["fix41"].update({
                    "force_rebuild_seen": bool(_fix41_force_rebuild_seen),
                    "force_rebuild_honored": bool(locals().get("_fix41_force_rebuild_honored", False)),
                    "path": "replay_unchanged",
                })
        except Exception:
            pass

        # =====================================================================
        # PATCH EVO_INJ_TRACE_REPLAY1 (ADDITIVE): emit inj_trace_v1 even on replay fastpath
        # Why:
        # - The FIX24 replay path returns early (skipping compute_source_anchored_diff),
        #   which previously meant results.debug.inj_trace_v1 might be missing.
        # - We need injected-URL lifecycle visibility even when hashes match (fastpath/replay)
        #   to validate UI wiring and to explain why a mismatch did/did not occur.
        # Safety:
        # - Pure debug emission only; does NOT affect hash logic, scraping, or fastpath decisions.
        # =====================================================================
        try:
            _wc = web_context if isinstance(web_context, dict) else {}
            _diag = _wc.get("diag_injected_urls") if isinstance(_wc.get("diag_injected_urls"), dict) else {}

            # =====================================================================
            # PATCH INJ_TRACE_V1_ENRICH_EVOLUTION_REPLAY_ARTIFACTS (ADDITIVE)
            # Populate attempted/persisted for injected URLs from scraped_meta/cur_bsc
            # even when replay fastpath returns early.
            # Also attach an explicit reason when injected URLs are present but not
            # admitted/hashed due to replay semantics.
            # =====================================================================
            try:
                if isinstance(_diag, dict):
                    # Enrich from scraped_meta (injected only) and from BSC (all)
                    _diag = _inj_trace_v1_enrich_diag_from_scraped_meta(_diag, scraped_meta, (_inj_extra_urls or []))
                    _diag = _inj_trace_v1_enrich_diag_from_bsc(_diag, cur_bsc if isinstance(cur_bsc, list) else [])
                    # Explain replay semantics when UI extras exist
                    _ui_norm = _inj_diag_norm_url_list(_diag.get("ui_norm") or [])
                    if _ui_norm:
                        _diag.setdefault("admission_reason", "fastpath_replay_no_rebuild_no_admission")
                        _diag.setdefault("injection_effective", False)
            except Exception:
                pass
            # =====================================================================

            _hash_inputs_replay = _inj_diag_hash_inputs_from_bsc(cur_bsc)
            _trace_replay = _inj_trace_v1_build(
                diag_injected_urls=_diag,
                hash_inputs=_hash_inputs_replay,
                stage="evolution",
                path="fastpath_replay",
                rebuild_pool=None,
                rebuild_selected=None,
            )
            out_replay.setdefault("results", {})
            if isinstance(out_replay.get("results"), dict):
                out_replay["results"].setdefault("debug", {})
                if isinstance(out_replay["results"].get("debug"), dict):
                    out_replay["results"]["debug"]["inj_trace_v1"] = _trace_replay
        except Exception:
            pass
        # =====================================================================
        # END PATCH EVO_INJ_TRACE_REPLAY1
        # =====================================================================
        return out_replay

    # Step 5: Changed -> run deterministic evolution diff using existing machinery.
    # Provide web_context with scraped_meta so compute_source_anchored_diff can reconstruct snapshots deterministically.
    wc = {"scraped_meta": scraped_meta}
    # =====================================================================
    # PATCH FIX40 (ADDITIVE): Preserve caller flags (e.g., force_rebuild) into web_context
    # so downstream diff/rebuild logic can record provenance if needed.
    # =====================================================================
    try:
        if isinstance(web_context, dict):
            wc.update({k: v for k, v in web_context.items() if k != "scraped_meta"})
    except Exception:
        pass
    # =====================================================================



    if callable(run_source_anchored_evolution_BASE):
        try:
            out = run_source_anchored_evolution_BASE(prev_full, web_context=wc)
            if isinstance(out, dict):
                out.setdefault("debug", {})
                if isinstance(out["debug"], dict):
                    out["debug"]["fix24"] = True
                    out["debug"]["fix24_mode"] = "recompute_changed"
                    # =====================================================================
                    # PATCH FIX40 (ADDITIVE): record Scenario B override
                    # =====================================================================
                    out["debug"]["fix40_force_rebuild"] = bool(_force_rebuild)
                    # =====================================================================
# =====================================================================
# =====================================================================
# =====================================================================
# =====================================================================
                    out["debug"]["prev_source_snapshot_hash_v2"] = prev_hashes.get("v2","")
                    out["debug"]["cur_source_snapshot_hash_v2"] = cur_hashes.get("v2","")
                    out["debug"]["prev_source_snapshot_hash"] = prev_hashes.get("v1","")
                    out["debug"]["cur_source_snapshot_hash"] = cur_hashes.get("v1","")
            return out
        except Exception as e:
            # Fall through to original behavior if anything unexpected
            pass

    # Ultimate fallback: call compute_source_anchored_diff directly if base runner not available
    fn = globals().get("compute_source_anchored_diff")
    if callable(fn):
        try:
            out_changed = fn(prev_full, web_context=wc)
            # =====================================================================
            # PATCH FIX41 (ADDITIVE): Attach force-rebuild debug to changed output
            # =====================================================================
            try:
                if isinstance(out_changed, dict):
                    out_changed.setdefault("code_version", CODE_VERSION)
                    out_changed.setdefault("debug", {}).setdefault("fix41", {})
                    out_changed["debug"]["fix41"].update({
                        "force_rebuild_seen": bool(_fix41_force_rebuild_seen),
                        "force_rebuild_honored": bool(locals().get("_fix41_force_rebuild_honored", False)) or bool(_fix41_force_rebuild_seen),
                        "path": "changed_compute_source_anchored_diff",
                    })
            except Exception:
                pass
            return out_changed
        except Exception:
            pass

    return {
        "status": "failed",
        "message": "FIX24: Evolution recompute failed (no callable base evolution runner).",
        "sources_checked": len(urls),
        "sources_fetched": len(urls),
        "metric_changes": [],
        "debug": {"fix24": True, "fix24_mode": "recompute_failed"},
    }


# ==============================================================================
# FIX32 (ADDITIVE): Unit-required hard gate in evolution diff rendering
#
# Problem:
# - Evolution diff table can show unit-less / year-like integers (e.g. 2024, 2033, 1500)
#   in the "Current" column for metrics whose schema requires currency/percent/rate/ratio.
# - This happens when upstream selection or LLM delta yields a numeric candidate that lacks
#   token-level unit evidence (unit_tag/unit_family/base_unit empty), and the diff layer
#   currently treats it as a valid numeric current value.
#
# Target invariant:
# - For any metric with unit_family in {currency, percent, rate, ratio} (or with a non-empty
#   unit_tag/unit), a unit-less candidate must be treated as ineligible and must not be
#   rendered as a valid "Current" value in the evolution table/output.
#
# Approach (purely additive):
# - Preserve existing diff implementation as diff_metrics_by_name_FIX31_BASE.
# - Override diff_metrics_by_name with a wrapper that post-processes each row:
#     * if schema says unit is required AND current unit evidence is missing,
#       then mark unit_mismatch=True and render current_value="N/A" (and cur_value_norm=None).
# - No refactors; does not change upstream extraction/building logic, only prevents
#   unit-less values from appearing as "valid" in evolution output.
# ==============================================================================
try:
    diff_metrics_by_name_FIX31_BASE = diff_metrics_by_name  # type: ignore
except Exception:
    diff_metrics_by_name_FIX31_BASE = None  # type: ignore

def _fix32_metric_requires_unit(metric_def: dict) -> bool:
    """Return True if schema implies this metric requires unit evidence."""
    try:
        spec = metric_def if isinstance(metric_def, dict) else {}
        uf = str(spec.get("unit_family") or "").strip().lower()
        ut = str(spec.get("unit_tag") or spec.get("unit") or "").strip()
        # Core families we treat as unit-required
        if uf in ("currency", "percent", "rate", "ratio"):
            return True
        # If schema explicitly declares a unit tag/unit, treat as required (except year/time-ish)
        if ut:
            # Avoid requiring "year" units
            blob = (uf + " " + ut + " " + str(spec.get("name") or "")).lower()
            if "year" in blob or "time" in blob:
                return False
            return True
    except Exception:
        pass
    return False

def _fix32_has_token_unit_evidence(metric_row: dict) -> bool:
    """
    Token-level unit evidence heuristic:
    - Prefer structured fields: base_unit/unit/unit_tag/unit_family
    - Fall back to raw token containing '$' or '%' or currency code immediately adjacent.
    Deterministic; does NOT attempt any NLP.
    """
    try:
        m = metric_row if isinstance(metric_row, dict) else {}
        for k in ("base_unit", "unit", "unit_tag", "unit_family"):
            if str(m.get(k) or "").strip():
                return True

        raw = str(m.get("raw") or m.get("value") or "")
        if not raw:
            return False
        r = raw.strip()
        rl = r.lower()

        # direct symbol evidence on the token
        if any(sym in r for sym in ("$", "€", "£", "¥", "%")):
            return True

        # compact currency codes adjacent to the token
        # NOTE: keep conservative (requires code in same token string)
        if any(code in rl for code in ("usd", "sgd", "eur", "gbp", "aud", "cad", "jpy", "cny")):
            return True

    except Exception:
        return False
    return False

def diff_metrics_by_name(prev_response: dict, cur_response: dict):  # noqa: F811
    """
    FIX32 wrapper: calls existing diff, then enforces the unit-required hard gate at render time.
    """
    if not callable(diff_metrics_by_name_FIX31_BASE):
        # Fallback: nothing we can do
        return ([], 0, 0, 0, 0)

    metric_changes, unchanged, increased, decreased, found = diff_metrics_by_name_FIX31_BASE(prev_response, cur_response)

    try:
        if not isinstance(metric_changes, list):
            return metric_changes, unchanged, increased, decreased, found

        for row in metric_changes:
            if not isinstance(row, dict):
                continue

            md = row.get("metric_definition") or {}
            if not _fix32_metric_requires_unit(md):
                continue

            cur_unit_cmp = str(row.get("cur_unit_cmp") or "").strip()
            # If diff already says mismatch, keep it; we only add the missing-unit mismatch
            if cur_unit_cmp:
                continue

            # Determine whether the current-side metric row shows any unit evidence at all
            # We use the current canonical row when present; else fall back to row fields.
            cur_metrics = (cur_response or {}).get("primary_metrics_canonical") or {}
            ck = row.get("canonical_key") or ""
            cm = cur_metrics.get(ck) if isinstance(cur_metrics, dict) else None

            has_ev = _fix32_has_token_unit_evidence(cm or {})
            if not has_ev:
                # Enforce: missing unit where required => mismatch + no current value rendered
                row["unit_mismatch"] = True
                row["cur_unit_cmp"] = ""
                row["current_value"] = "N/A"
                row["cur_value_norm"] = None

                # Make the reason machine-detectable (non-breaking extra field)
                row.setdefault("guardrail_reason", "unit_required_missing_current_unit")

                # Re-classify change as unit_mismatch (keeps deterministic reporting)
                row["change_type"] = "unit_mismatch"
                row["change_pct"] = None

    except Exception:
        pass

    return metric_changes, unchanged, increased, decreased, found

# ==============================================================================
# END FIX32
# ==============================================================================


# =====================================================================
# PATCH FIX41AFC25_VERSION (ADDITIVE): CODE_VERSION bump for auditability
# =====================================================================
try:
    CODE_VERSION = "fix41afc25_evo_parity_lock_assert_guard_v1"
except Exception:
    pass
# =====================================================================
# END PATCH FIX41AFC25_VERSION
# =====================================================================



# PATCH FIX41AFC26_VERSION START
CODE_VERSION = "fix41afc51_evo_anchor_payload_enrich_v1"
# PATCH FIX41AFC26_VERSION END


# PATCH FIX41AFC27_VERSION START
CODE_VERSION = "fix41afc51_evo_anchor_payload_enrich_v1"
# PATCH FIX41AFC27_VERSION END


# PATCH FIX41AFC29_VERSION START
CODE_VERSION = "fix41afc51_evo_anchor_payload_enrich_v1"
# PATCH FIX41AFC29_VERSION END


# PATCH FIX41AFC29_INDENT START
# (surgical) Ensure return rebuilt remains within function scope.
CODE_VERSION = "fix41afc51_evo_anchor_payload_enrich_v1"
# PATCH FIX41AFC29_INDENT END

# PATCH FIX41AFC30_VERSION START
CODE_VERSION = "fix41afc51_evo_anchor_payload_enrich_v1"
# PATCH FIX41AFC30_VERSION END


# =====================================================================
# PATCH FIX41AFC31 (ADDITIVE): Bare-year hard block for dashboard "Current"
# - Implemented as a safe wrapper around diff_metrics_by_name to avoid touching
#   existing locked-down diff logic.
# - Blocks naked calendar years (1900-2100) with missing unit evidence for any
#   non-year metric (including dimension="unknown").
# =====================================================================
try:
    diff_metrics_by_name_FIX41AFC31_PRE = diff_metrics_by_name  # type: ignore
except Exception:
    diff_metrics_by_name_FIX41AFC31_PRE = None

import re

def diff_metrics_by_name(prev_response: dict, cur_response: dict):
    out = None
    if callable(diff_metrics_by_name_FIX41AFC31_PRE):
        out = diff_metrics_by_name_FIX41AFC31_PRE(prev_response, cur_response)
    else:
        fn = globals().get("diff_metrics_by_name_LEGACY")
        out = fn(prev_response, cur_response) if callable(fn) else ([], 0, 0, 0, 0)

    try:
        metric_changes = out[0] if isinstance(out, (list, tuple)) and len(out) > 0 else []
    except Exception:
        metric_changes = []

    def _is_year_metric_fix41afc31(row: dict) -> bool:
        try:
            md = row.get("metric_definition") if isinstance(row, dict) else None
            md = md if isinstance(md, dict) else {}
            dim = str(md.get("dimension") or "").strip().lower()
            unit = str(md.get("unit") or "").strip().lower()
            unit_tag = str(md.get("unit_tag") or "").strip().lower()
            ckey = str(row.get("canonical_key") or "").lower()
            name = str(row.get("name") or "").lower()
            if dim in ("year", "years", "date", "time_year", "calendar_year"):
                return True
            if ("year" in unit) or ("year" in unit_tag):
                return True
            if re.search(r"__years?\\b", ckey):
                return True
            if ("year" in name) and ("cagr" not in name) and ("range" not in name):
                return True
        except Exception:
            pass
        return False

    try:
        for row in metric_changes:
            if not isinstance(row, dict):
                continue
            if _is_year_metric_fix41afc31(row):
                continue

            cur_val = row.get("cur_value_norm", None)
            yr = None
            try:
                if cur_val is not None:
                    fv = float(cur_val)
                    if abs(fv - round(fv)) < 1e-9:
                        iv = int(round(fv))
                        if 1900 <= iv <= 2100:
                            yr = iv
            except Exception:
                yr = None

            if yr is None:
                continue

            cur_unit_cmp = str(row.get("cur_unit_cmp") or "").strip()
            unit_mismatch = bool(row.get("unit_mismatch")) if "unit_mismatch" in row else False
            blocked_reason = str(row.get("cur_value_blocked_reason") or "").strip()

            if (not cur_unit_cmp) and (not blocked_reason) and (not unit_mismatch):
                row["current_value"] = ""
                row["cur_value_norm"] = None
                row["cur_unit_cmp"] = ""
                row["cur_value_blocked_reason"] = "bare_year_hard_block_fix41afc31"
    except Exception:
        pass

    return out

# =====================================================================
# PATCH FIX41AFC31_VERSION (ADDITIVE): Version bump for auditability
# =====================================================================
try:
    CODE_VERSION = "fix41afc31_evo_bare_year_hard_block_v2"
except Exception:
    pass
# =====================================================================



# PATCH FIX41AFC32_VERSION START
try:
    CODE_VERSION = "fix41afc32_evo_value_range_mandatory_rescale_scorecard_v1"
except Exception:
    pass
# PATCH FIX41AFC32_VERSION END




# =====================================================================
# PATCH FIX41AFC33 START (ADDITIVE): unit_family fill + schema unit-required hard block + blocked_reason fill
# =====================================================================
try:
    _fix41afc33__orig_diff_metrics_by_name = diff_metrics_by_name
except Exception:
    _fix41afc33__orig_diff_metrics_by_name = None

def _fix41afc33_infer_unit_family(cand: dict) -> str:
    """Best-effort deterministic unit_family inference from existing fields (ADDITIVE)."""
    try:
        uf = (cand or {}).get("unit_family")
        if isinstance(uf, str) and uf.strip():
            return uf.strip()

        unit = " ".join([
            str((cand or {}).get("unit") or ""),
            str((cand or {}).get("unit_tag") or ""),
            str((cand or {}).get("base_unit") or "")
        ]).strip().lower()

        ctx = " ".join([
            str((cand or {}).get("raw") or ""),
            str((cand or {}).get("raw_disp") or ""),
            str((cand or {}).get("context") or ""),
            str((cand or {}).get("measure_assoc") or "")
        ]).strip().lower()

        # Percent
        if "%" in unit or "percent" in unit or "%" in ctx or "percent" in ctx:
            return "percent"

        # Currency (symbols or ISO-ish codes)
        currency_codes = ["usd","sgd","eur","gbp","jpy","cny","rmb","aud","cad","chf","inr","hkd","nzd","sek","nok","dkk","krw","twd"]
        if any(code in unit.split() for code in currency_codes) or any(sym in ctx for sym in ["$", "€", "£", "¥"]):
            return "currency"

        # Energy-ish
        if any(tok in unit for tok in ["kwh","mwh","gwh","twh"]) or any(tok in ctx for tok in ["kwh","mwh","gwh","twh"]):
            return "energy"

        # Magnitude / scale tags
        # (keep conservative: only label magnitude when explicit scale tokens appear)
        if any(tok in unit for tok in ["million","billion","trillion","thousand"]) or any(tok in ctx for tok in ["million","billion","trillion","thousand"]):
            return "magnitude"
        # short tags K/M/B/T
        short = (str((cand or {}).get("unit_tag") or "") or str((cand or {}).get("base_unit") or "")).strip().lower()
        if short in ("k","m","b","t"):
            return "magnitude"

        return ""
    except Exception:
        return ""

def _fix41afc33_fill_unit_family_in_obj(obj):
    """Walk common snapshot/source structures and fill unit_family on extracted_numbers candidates (ADDITIVE)."""
    try:
        if obj is None:
            return

        if isinstance(obj, dict):
            # Direct extracted_numbers list
            if isinstance(obj.get("extracted_numbers"), list):
                for cand in obj["extracted_numbers"]:
                    if isinstance(cand, dict):
                        uf = _fix41afc33_infer_unit_family(cand)
                        if uf and not str(cand.get("unit_family") or "").strip():
                            cand["unit_family"] = uf
                            cand["_unit_family_inferred_fix41afc33"] = True

            # Recurse into likely containers
            for k, v in list(obj.items()):
                if isinstance(v, (dict, list)):
                    _fix41afc33_fill_unit_family_in_obj(v)

        elif isinstance(obj, list):
            for it in obj:
                if isinstance(it, (dict, list)):
                    _fix41afc33_fill_unit_family_in_obj(it)
    except Exception:
        pass

def _fix41afc33_schema_implies_unit_required(schema: dict) -> bool:
    """Schema-driven unit-required: if schema expects percent/currency/magnitude (or has unit_tag), require explicit unit evidence."""
    try:
        if not isinstance(schema, dict):
            return False
        dim = str(schema.get("dimension") or "").strip().lower()
        uf  = str(schema.get("unit_family") or "").strip().lower()
        ut  = str(schema.get("unit_tag") or "").strip()
        u   = str(schema.get("unit") or "").strip()

        if ut or u:
            return True
        if uf in ("percent","currency","magnitude","energy"):
            return True
        if dim in ("percent","currency","unit_sales","energy","price","rate","ratio"):
            return True
        return False
    except Exception:
        return False

def _fix41afc33_has_any_unit_evidence(row: dict) -> bool:
    try:
        return any([
            str(row.get("cur_unit_cmp") or "").strip(),
            str(row.get("cur_unit_family") or "").strip(),
            str(row.get("cur_unit_tag") or "").strip(),
            str(row.get("cur_base_unit") or "").strip(),
        ])
    except Exception:
        return False

def diff_metrics_by_name(prev_response: dict, cur_response: dict):
    """Wrapper (ADDITIVE) to enforce schema-unit-required + blocked_reason fill; preserves prior behavior."""
    if _fix41afc33__orig_diff_metrics_by_name is None:
        return ([], [], [], [], [])

    out = _fix41afc33__orig_diff_metrics_by_name(prev_response, cur_response)

    try:
        metric_changes = out[0] if isinstance(out, (list, tuple)) and len(out) > 0 else []
        if isinstance(metric_changes, list):
            for row in metric_changes:
                if not isinstance(row, dict):
                    continue

                # Ensure unit_family inference is available for display/debug
                if not str(row.get("cur_unit_family") or "").strip():
                    # Try infer from current row-level unit fields
                    pseudo = {
                        "unit": row.get("cur_unit_cmp") or "",
                        "unit_tag": row.get("cur_unit_tag") or "",
                        "base_unit": row.get("cur_base_unit") or "",
                        "raw_disp": row.get("cur_raw") or "",
                        "context": row.get("cur_context") or "",
                    }
                    uf = _fix41afc33_infer_unit_family(pseudo)
                    if uf:
                        row["cur_unit_family"] = uf
                        row["_cur_unit_family_inferred_fix41afc33"] = True

                schema = row.get("metric_definition") if isinstance(row.get("metric_definition"), dict) else {}
                cur_value_norm = row.get("cur_value_norm")
                cur_blocked_reason = str(row.get("cur_value_blocked_reason") or "").strip()

                # Schema-unit-required hard block (dashboard safety) — applies even when dimension is unknown
                if cur_value_norm is not None and str(row.get("current_value") or "").strip() != "":
                    if _fix41afc33_schema_implies_unit_required(schema) and (not _fix41afc33_has_any_unit_evidence(row)):
                        row["current_value"] = ""
                        row["cur_value_norm"] = None
                        row["cur_unit_cmp"] = ""
                        if not cur_blocked_reason:
                            row["cur_value_blocked_reason"] = "unit_required_missing_fix41afc33"
                        continue

                # Always populate blocked reason if blanked (debuggability)
                if str(row.get("current_value") or "") == "" and (row.get("cur_value_norm") is None):
                    if not cur_blocked_reason:
                        if bool(row.get("unit_mismatch")):
                            row["cur_value_blocked_reason"] = "unit_mismatch_hard_block"
                        else:
                            row["cur_value_blocked_reason"] = "blanked_unspecified_fix41afc33"
    except Exception:
        pass

    return out

# =====================================================================
# PATCH FIX41AFC33 END
# =====================================================================

# =====================================================================
# PATCH FIX41AFC33B START (ADDITIVE): Fill unit_family in artifacts prior to rebuild (non-behavioral enrichment)
# =====================================================================
try:
    _fix41afc33__orig_rebuild_fix18 = rebuild_metrics_from_snapshots_schema_only_fix18
except Exception:
    _fix41afc33__orig_rebuild_fix18 = None

def rebuild_metrics_from_snapshots_schema_only_fix18(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """Wrapper (ADDITIVE): enrich candidates with inferred unit_family before calling FIX18 rebuild."""
    try:
        # Fill in baseline artifacts + cur/prev source_results structures if present
        _fix41afc33_fill_unit_family_in_obj(baseline_sources_cache)
        if isinstance(prev_response, dict):
            _fix41afc33_fill_unit_family_in_obj(prev_response.get("results", {}).get("source_results"))
            _fix41afc33_fill_unit_family_in_obj(prev_response.get("baseline_sources_cache"))
    except Exception:
        pass

    if _fix41afc33__orig_rebuild_fix18 is None:
        return {}
    return _fix41afc33__orig_rebuild_fix18(prev_response, baseline_sources_cache, web_context=web_context)

# =====================================================================
# PATCH FIX41AFC33B END
# =====================================================================

# =====================================================================
# PATCH FIX41AFC33_VERSION (ADDITIVE): Version bump for auditability
# =====================================================================
try:
    CODE_VERSION = "fix41afc33_unit_family_schema_unit_required_reason_v1"
except Exception:
    pass
# =====================================================================

# PATCH FIX41AFC34_VERSION START
CODE_VERSION = "fix41afc51_evo_anchor_payload_enrich_v1"
# PATCH FIX41AFC34_VERSION END


# PATCH FIX41AFC35_VERSION START
CODE_VERSION = "fix41afc51_evo_anchor_payload_enrich_v1"
# PATCH FIX41AFC35_VERSION END


# PATCH FIX41AFC36_VERSION START
CODE_VERSION = "fix41afc51_evo_anchor_payload_enrich_v1"
# PATCH FIX41AFC36_VERSION END


# PATCH FIX41AFC37_VERSION START
CODE_VERSION = "fix41afc51_evo_anchor_payload_enrich_v1"
# PATCH FIX41AFC37_VERSION END

# PATCH FIX41AFC38_VERSION START
CODE_VERSION = "fix41afc51_evo_anchor_payload_enrich_v1"
# PATCH FIX41AFC38_VERSION END

# PATCH FIX41AFC39_VERSION START
try:
    CODE_VERSION = "fix41afc39_evo_unit_out_backfill_ctxsnippet_v1"
except Exception:
    pass
# PATCH FIX41AFC39_VERSION END


# ============================================================
# PATCH FIX41AFC40_VERSION START
# Auditability: bump CODE_VERSION for this patch bundle.
# ============================================================
try:
    CODE_VERSION = "fix41afc40_unit_token_authority_unitcmp_backfill_runtimefp_v1"
except Exception:
    pass
# PATCH FIX41AFC40_VERSION END


# =====================================================================
# PATCH FIX41AFC41_HELPERS START
# Anchor compatibility resolver + unit-token family enforcement + DIAG v2
# Additive-only utilities used by FIX41AFC41.
# =====================================================================

def _fix41afc41_unit_token_family(c: dict) -> str:
    """Infer unit_family from explicit unit tokens. Conservative."""
    try:
        u = str(c.get("unit") or c.get("base_unit") or "").strip()
        ut = str(c.get("unit_tag") or "").strip().lower()
        raw = str(c.get("raw") or "").strip()
        txt = " ".join([u, ut, raw]).lower()
        if "%" in txt or u == "%":
            return "percent"
        # magnitude tokens
        if u in ("K", "M", "B", "T"):
            return "magnitude"
        if any(w in txt for w in ("million", "billion", "trillion", "thousand")):
            return "magnitude"
        # currency (very light)
        if any(tok in txt for tok in ("usd", "sgd", "eur", "gbp", "$", "€", "£")):
            return "currency"
        # energy
        if any(tok in txt for tok in ("kwh", "mwh", "gwh", "twh")):
            return "energy"
        return str(c.get("unit_family") or "").strip()
    except Exception:
        return str((c or {}).get("unit_family") or "").strip()

def _fix41afc41_force_unit_token_family_in_candidate(c: dict) -> None:
    """Force magnitude/currency/percent family from explicit unit tokens (does not guess from surrounding context)."""
    try:
        if not isinstance(c, dict):
            return
        uf = _fix41afc41_unit_token_family(c)
        if uf:
            # If candidate already has a stronger unit_family, keep it; otherwise set.
            cur = str(c.get("unit_family") or "").strip()
            # Explicit magnitude tokens should NEVER be overwritten to percent due to nearby '%'
            if uf == "magnitude":
                c["unit_family"] = "magnitude"
                return
            if not cur:
                c["unit_family"] = uf
    except Exception:
        return

def _fix41afc41_force_unit_token_family_in_sources_cache(cache) -> None:
    """Apply unit-token family enforcement across extracted_numbers in caches."""
    try:
        if cache is None:
            return
        # baseline_sources_cache usually has .get('source_results') OR list-like
        if isinstance(cache, dict):
            srs = cache.get("source_results") or cache.get("results") or cache.get("sources") or []
            if isinstance(srs, list):
                for sr in srs:
                    if isinstance(sr, dict) and isinstance(sr.get("extracted_numbers"), list):
                        for c in sr["extracted_numbers"]:
                            _fix41afc41_force_unit_token_family_in_candidate(c)
            # sometimes direct extracted_numbers
            if isinstance(cache.get("extracted_numbers"), list):
                for c in cache.get("extracted_numbers"):
                    _fix41afc41_force_unit_token_family_in_candidate(c)
        elif isinstance(cache, list):
            for sr in cache:
                if isinstance(sr, dict) and isinstance(sr.get("extracted_numbers"), list):
                    for c in sr["extracted_numbers"]:
                        _fix41afc41_force_unit_token_family_in_candidate(c)
    except Exception:
        return

def _fix41afc41_anchor_compat_resolve(spec: dict, baseline_sources_cache, schema_anchor_hash: str, canonical_key: str = ""):
    """Compat-match an anchor when exact anchor_hash is missing.

    Strategy (deterministic, conservative):
      - prefer same normalized source_url (if spec has it)
      - numeric closeness to anchor value (if available)
      - context similarity with anchor text (if available)
    Returns a candidate dict (copy) marked with:
      _anchor_compat_force_schema_anchor_fix41afc41 = True
      _anchor_compat_reason_fix41afc41
    """
    try:
        if not schema_anchor_hash:
            return None

        # gather candidates
        fn_all = globals().get("_es_iter_all_candidates_deterministic")
        if callable(fn_all):
            cands = list(fn_all(baseline_sources_cache))
        else:
            # fallback: walk source_results.extracted_numbers
            cands = []
            try:
                srs = []
                if isinstance(baseline_sources_cache, dict):
                    srs = baseline_sources_cache.get("source_results") or []
                elif isinstance(baseline_sources_cache, list):
                    srs = baseline_sources_cache
                for sr in (srs or []):
                    if isinstance(sr, dict) and isinstance(sr.get("extracted_numbers"), list):
                        cands.extend([c for c in sr["extracted_numbers"] if isinstance(c, dict)])
            except Exception:
                cands = []

        if not cands:
            return None

        # expected numeric from anchor record if present
        exp_val = None
        try:
            anchors = (spec.get("metric_anchors") or {}) if isinstance(spec, dict) else {}
            # usually not present here; ignore
        except Exception:
            pass
        # try infer from schema/name year patterns not needed; compat is mainly for exact numeric already in pool.

        # Derive expected unit family from schema
        exp_uf = str(spec.get("unit_family") or "").strip().lower()
        exp_dim = str(spec.get("dimension") or "").strip().lower()

        # Normalized target URL if present in spec
        spec_url = (spec.get("source_url") or spec.get("url") or "").strip()
        norm_fn = globals().get("_inj_diag_norm_url_one") or globals().get("_norm_url")
        spec_url_n = norm_fn(spec_url) if callable(norm_fn) else spec_url

        def norm_ctx(s: str) -> str:
            import re
            return re.sub(r"[^a-z0-9%]+", " ", (s or "").lower()).strip()

        # Schema anchor hash is derived from context+raw; we can approximate by comparing context snippets.
        best = None
        best_score = -1.0

        for c in cands:
            try:
                if not isinstance(c, dict):
                    continue
                # enforce token-based unit family before comparing
                _fix41afc41_force_unit_token_family_in_candidate(c)
                uf = str(c.get("unit_family") or "").strip().lower()
                if exp_uf and uf and (uf != exp_uf):
                    # allow magnitude vs unit_sales synonyms: treat magnitude as ok for unit_sales
                    if not (exp_dim == "unit_sales" and uf == "magnitude"):
                        continue

                # URL proximity
                cu = (c.get("source_url") or "").strip()
                cu_n = norm_fn(cu) if callable(norm_fn) else cu
                url_bonus = 1.0 if (spec_url_n and cu_n and spec_url_n == cu_n) else 0.0

                # numeric score: prefer close to schema-ish magnitude by scale if dimension unit_sales
                vn = c.get("value_norm")
                num_score = 0.0
                if isinstance(vn, (int, float)):
                    # prefer non-year values
                    if 1900 <= int(vn) <= 2100 and str(c.get("unit") or c.get("unit_tag") or "").strip() == "":
                        continue
                    # small preference for reasonable magnitudes
                    num_score = 1.0

                # context similarity: compare normalized snippet with spec name tokens
                ctx = norm_ctx(c.get("context_snippet") or c.get("context") or "")
                name = norm_ctx(spec.get("name") or canonical_key or "")
                sim = 0.0
                if ctx and name:
                    ctx_set = set(ctx.split())
                    name_set = set(name.split())
                    inter = len(ctx_set & name_set)
                    uni = max(1, len(ctx_set | name_set))
                    sim = inter / uni

                score = (2.0 * url_bonus) + (1.0 * num_score) + (3.0 * sim)
                if score > best_score:
                    best_score = score
                    best = c
            except Exception:
                continue

        if not isinstance(best, dict):
            return None

        out = dict(best)
        out["_anchor_compat_force_schema_anchor_fix41afc41"] = True
        out["_anchor_compat_reason_fix41afc41"] = "url_bonus+ctx_similarity" if best_score >= 2.0 else "ctx_similarity"
        return out
    except Exception:
        return None

def _fix41afc41_diag_state_v2(prev_response: dict) -> dict:
    """Derive attempted/fetched/failed counts from actual source_results state (instrumentation-only)."""
    try:
        res = (prev_response or {}).get("results") or {}
        srs = res.get("source_results") or []
        out = {"attempted": 0, "fetched": 0, "failed": 0, "persisted": 0}
        if not isinstance(srs, list):
            return out
        out["attempted"] = len([x for x in srs if isinstance(x, dict)])
        for sr in srs:
            if not isinstance(sr, dict):
                continue
            st = str(sr.get("status") or "").lower()
            if "fetch" in st or st in ("ok", "success", "fetched"):
                out["fetched"] += 1
            if "fail" in st or st in ("error", "failed"):
                out["failed"] += 1
            # persisted if snapshot refs exist
            if sr.get("snapshot_store_ref") or sr.get("source_snapshot_hash"):
                out["persisted"] += 1
        return out
    except Exception:
        return {"attempted": 0, "fetched": 0, "failed": 0, "persisted": 0}

# =====================================================================
# PATCH FIX41AFC41_HELPERS END
# =====================================================================

# PATCH FIX41AFC41_VERSION START
CODE_VERSION = "fix41afc51_evo_anchor_payload_enrich_v1"
# PATCH FIX41AFC41_VERSION END


# ============================================================
# PATCH FIX41AFC42_VERSION START
# ============================================================
try:
    CODE_VERSION = "fix41afc42_unit_family_precedence_lock_v1"
except Exception:
    pass
# ============================================================
# PATCH FIX41AFC42_VERSION END
# ============================================================



# PATCH FIX41AFC43_DIFF_WRAPPER START
def _fix41afc43_postprocess_metric_changes(metric_changes, prev_response=None, cur_response=None):
    """Postprocess diff rows to enforce schema-invalid dim/unit blocks (dashboard-safe) and improve sign-off determinism."""
    try:
        out = []
        for row in (metric_changes or []):
            if not isinstance(row, dict):
                out.append(row)
                continue
            md = row.get("metric_definition") or {}
            dim = (md.get("dimension") or md.get("dim") or "").strip().lower()
            unit = (md.get("unit") or "").strip().lower()
            uf = (md.get("unit_family") or "").strip().lower()

            # Schema-invalid: currency metric but percent unit family/unit token
            if dim == "currency" and (unit in ("%", "percent") or uf == "percent"):
                row = dict(row)
                row["current_value"] = ""
                row["cur_value_norm"] = None
                row["cur_unit_cmp"] = ""
                row["cur_value_blocked_reason"] = "schema_invalid_dim_unit_fix41afc43"
                out.append(row)
                continue

            # Schema-invalid: percent metric but currency unit family/token
            if dim == "percent" and (uf == "currency" or ("usd" in unit or "sgd" in unit or "eur" in unit or "gbp" in unit)):
                row = dict(row)
                row["current_value"] = ""
                row["cur_value_norm"] = None
                row["cur_unit_cmp"] = ""
                row["cur_value_blocked_reason"] = "schema_invalid_dim_unit_fix41afc43"
                out.append(row)
                continue

            out.append(row)
        return out
    except Exception:
        return metric_changes

def diff_metrics_by_name_fix41afc43(prev_response: dict, cur_response: dict):
    metric_changes, unchanged, increased, decreased, found = diff_metrics_by_name(prev_response, cur_response)
    metric_changes = _fix41afc43_postprocess_metric_changes(metric_changes, prev_response, cur_response)
    return metric_changes, unchanged, increased, decreased, found

try:
    # Rewire diff to include FIX41AFC43 postprocessing without refactoring original logic
    diff_metrics_by_name = diff_metrics_by_name_fix41afc43
except Exception:
    pass
# PATCH FIX41AFC43_DIFF_WRAPPER END


# PATCH FIX41AFC43_VERSION START
CODE_VERSION = "fix41afc51_evo_anchor_payload_enrich_v1"
# PATCH FIX41AFC43_VERSION END


# ================================================================
# PATCH FIX41AFC44_VERSION START — version bump (auditability)
try:
    CODE_VERSION = "fix41afc44_evo_schema_unit_value_range_rebuild_v1"
except Exception:
    pass
# PATCH FIX41AFC44_VERSION END
# ================================================================

# PATCH FIX41AFC47_VERSION START
CODE_VERSION = "fix41afc51_evo_anchor_payload_enrich_v1"
# PATCH FIX41AFC47_VERSION END


# =====================================================================
# PATCH FIX41AFC48_VERSION START — version bump (audit)
CODE_VERSION = "fix41afc51_evo_anchor_payload_enrich_v1"
# PATCH FIX41AFC48_VERSION END — version bump (audit)



# =====================================================================
# PATCH FIX41AFC49 START — anchor signature resolve (value+unit+source)
# Motivation:
#   When anchor_hash/candidate_id drift (or compat-matching is ambiguous),
#   fall back to a deterministic signature match using:
#     - source_url (if known)
#     - anchor value_norm/value
#     - anchor unit_tag / expected schema unit_family
# This is additive and does NOT touch fastpath, hashing, or snapshot writes.
# =====================================================================

def _fix41afc49_anchor_signature_resolve(spec: dict, anchor_dict: dict, baseline_sources_cache, *, canonical_key: str = "", dbg: dict = None):
    """Deterministically try to resolve an anchored metric to a current candidate by signature.
    Returns candidate dict or None.
    """
    try:
        if not isinstance(spec, dict) or not isinstance(anchor_dict, dict):
            return None
        # expected
        exp_family = (spec.get("unit_family") or "").strip().lower()
        exp_tag = (spec.get("unit_tag") or spec.get("unit") or "").strip()
        # anchor-provided hints
        a_url = (anchor_dict.get("source_url") or anchor_dict.get("url") or "").strip()
        a_v = anchor_dict.get("value_norm")
        if a_v is None:
            a_v = anchor_dict.get("value")
        try:
            a_v = float(a_v) if a_v is not None and str(a_v).strip() != "" else None
        except Exception:
            a_v = None
        a_utag = (anchor_dict.get("unit_tag") or anchor_dict.get("unit") or "").strip()

        if a_v is None:
            return None

        # Gather candidates (deterministic iteration order)
        cands = []
        if isinstance(baseline_sources_cache, list):
            for it in baseline_sources_cache:
                if not isinstance(it, dict):
                    continue
                url = (it.get("url") or it.get("source_url") or "").strip()
                if a_url and url and url != a_url:
                    continue
                nums = it.get("extracted_numbers") or it.get("numbers") or []
                if isinstance(nums, list):
                    for c in nums:
                        if isinstance(c, dict):
                            cands.append(c)

        if not cands:
            return None

        def _cfam(c):
            return (c.get("unit_family") or "").strip().lower()

        def _utag(c):
            return (c.get("unit_tag") or c.get("unit") or "").strip()

        def _vnorm(c):
            v = c.get("value_norm")
            if v is None:
                v = c.get("value")
            try:
                return float(v) if v is not None and str(v).strip() != "" else None
            except Exception:
                return None

        # Score candidates:
        #  - same unit_family is strong
        #  - close value_norm is strong
        #  - unit_tag overlap (contains expected or anchor tag) is medium
        best = None
        best_score = None
        for c in cands:
            cv = _vnorm(c)
            if cv is None:
                continue
            fam = _cfam(c)

            # hard family gate if spec is explicit (percent/currency/etc.)
            if exp_family in ("percent", "currency", "magnitude", "energy", "count") and fam and fam != exp_family:
                continue

            # distance score (cap to avoid huge floats)
            dist = abs(cv - a_v)
            dist_score = 1.0 / (1.0 + min(dist, 1e9))

            ut = _utag(c)
            ut_low = ut.lower()
            tag_score = 0.0
            if exp_tag and exp_tag.lower() in ut_low:
                tag_score += 0.6
            if a_utag and a_utag.lower() in ut_low:
                tag_score += 0.4

            fam_score = 0.0
            if exp_family and fam == exp_family:
                fam_score = 1.0
            elif exp_family and not fam:
                fam_score = 0.1
            elif not exp_family:
                fam_score = 0.2

            score = fam_score * 2.0 + tag_score + dist_score
            if best is None or (best_score is not None and score > best_score):
                best = c
                best_score = score

        if isinstance(best, dict):
            if isinstance(dbg, dict):
                dbg.setdefault("anchor_signature_resolve_fix41afc49", []).append({
                    "canonical_key": canonical_key or (spec.get("canonical_key") or ""),
                    "source_url": a_url,
                    "anchor_value": a_v,
                    "anchor_unit_tag": a_utag,
                    "expected_unit_family": exp_family,
                    "expected_unit_tag": exp_tag,
                    "resolved_value_norm": best.get("value_norm"),
                    "resolved_unit_tag": best.get("unit_tag"),
                    "resolved_unit_family": best.get("unit_family"),
                })
            return best
        return None
    except Exception:
        return None

# =====================================================================
# PATCH FIX41AFC49 END — anchor signature resolve (value+unit+source)
# =====================================================================


# =====================================================================
# PATCH FIX41AFC52 START (ADDITIVE): Post-rebuild evidence rescue to prevent blank/incorrect "current" values
#   - If selected metric is ineligible vs schema (unit_family mismatch / missing required unit evidence),
#     attempt to rescue using the metric's own evidence pool (schema-authoritative).
#   - This is a deterministic, schema-driven post-processing step; does NOT touch fastpath/hashing.
# =====================================================================

try:
    _fix41afc52__orig_rebuild_fix18 = rebuild_metrics_from_snapshots_schema_only_fix18
except Exception:
    _fix41afc52__orig_rebuild_fix18 = None

def _fix41afc52_schema_unit_family(schema: dict) -> str:
    try:
        if not isinstance(schema, dict):
            return ""
        uf = str(schema.get("unit_family") or "").strip().lower()
        if uf:
            return uf
        dim = str(schema.get("dimension") or "").strip().lower()
        if dim in ("percent", "percentage", "share", "ratio"):
            return "percent"
        if dim in ("currency", "money", "revenue", "market_size", "market_value", "price"):
            return "currency"
        if dim in ("unit_sales", "count", "volume"):
            return "magnitude"
        if dim in ("energy",):
            return "energy"
        return ""
    except Exception:
        return ""

def _fix41afc52_infer_unit_family(obj: dict) -> str:
    # Prefer existing FIX41AFC33 inference if present.
    try:
        fn = globals().get("_fix41afc33_infer_unit_family")
        if callable(fn):
            return str(fn(obj) or "").strip().lower()
    except Exception:
        pass
    try:
        return str((obj or {}).get("unit_family") or "").strip().lower()
    except Exception:
        return ""

def _fix41afc52_has_required_unit_evidence(schema: dict, cand: dict) -> bool:
    try:
        if not isinstance(schema, dict) or not isinstance(cand, dict):
            return False
        # Percent: require explicit % token or percent unit_family
        uf = _fix41afc52_schema_unit_family(schema)
        raw = str(cand.get("raw") or cand.get("raw_disp") or "").strip()
        unit = str(cand.get("unit") or "").strip()
        ctx = str(cand.get("context") or cand.get("context_snippet") or "").lower()

        if uf == "percent":
            if "%" in raw or "%" in unit:
                return True
            if _fix41afc52_infer_unit_family(cand) == "percent":
                return True
            return False

        # Currency: require explicit currency token (USD/$/US$ etc) OR currency unit_family locked
        if uf == "currency":
            if _fix41afc52_infer_unit_family(cand) == "currency":
                # still ensure some currency token exists somewhere if possible
                if any(tok in (raw + " " + ctx) for tok in ["usd", "us$", "$", "eur", "gbp", "sgd", "aud", "cad", "jpy", "cny", "rmb"]):
                    return True
                # If unit_family explicitly currency but tokens stripped, allow schema-authoritative fallback
                return True
            return any(tok in (raw + " " + ctx) for tok in ["usd", "us$", "$", "eur", "gbp", "sgd", "aud", "cad", "jpy", "cny", "rmb"])

        # Magnitude/energy: require any unit hint OR inferred family
        if uf in ("magnitude", "energy"):
            if str(cand.get("unit") or "").strip():
                return True
            if _fix41afc52_infer_unit_family(cand) in (uf,):
                return True
            return False

        # Unknown schema: do not require
        return True
    except Exception:
        return True

def _fix41afc52_candidate_is_eligible(schema: dict, cand: dict) -> bool:
    try:
        if not isinstance(schema, dict) or not isinstance(cand, dict):
            return False
        uf_schema = _fix41afc52_schema_unit_family(schema)
        uf_cand = _fix41afc52_infer_unit_family(cand)
        if uf_schema:
            if uf_cand and uf_cand != uf_schema:
                return False
            # When schema implies unit required, enforce required evidence
            fn_req = globals().get("_fix41afc33_schema_implies_unit_required")
            if callable(fn_req) and fn_req(schema):
                if not _fix41afc52_has_required_unit_evidence(schema, cand):
                    return False
        return True
    except Exception:
        return False

def _fix41afc52_pick_best_candidate(schema: dict, candidates: list) -> dict:
    """Deterministic selection: highest context_score, then highest absolute value_norm (tie-break), then first."""
    best = None
    best_key = None
    for c in candidates or []:
        if not isinstance(c, dict):
            continue
        cs = c.get("context_score")
        try:
            cs = float(cs) if cs is not None else 0.0
        except Exception:
            cs = 0.0
        vn = c.get("value_norm")
        if vn is None:
            vn = c.get("value")
        try:
            vn = float(vn) if vn is not None else 0.0
        except Exception:
            vn = 0.0
        key = (cs, abs(vn))
        if best is None or key > best_key:
            best = c
            best_key = key
    return best or {}

def _fix41afc52_apply_candidate_to_metric(schema: dict, metric: dict, cand: dict):
    """Mutate metric in-place with rescued candidate, schema-authoritative unit display."""
    try:
        if not isinstance(metric, dict) or not isinstance(cand, dict):
            return
        vn = cand.get("value_norm")
        if vn is None:
            vn = cand.get("value")
        try:
            vn_f = float(vn) if vn is not None else None
        except Exception:
            vn_f = None

        # Use schema unit_tag as the display unit_cmp where possible (parity & dashboard safety)
        unit_tag = str(schema.get("unit_tag") or "").strip()
        unit_cmp = unit_tag if unit_tag else str(metric.get("unit") or metric.get("unit_tag") or cand.get("unit") or "").strip()
        metric["value_norm"] = vn_f
        metric["value"] = vn_f if vn_f is not None else metric.get("value")
        metric["unit"] = unit_cmp
        metric["unit_tag"] = unit_cmp
        metric["unit_family"] = _fix41afc52_schema_unit_family(schema) or _fix41afc52_infer_unit_family(cand) or str(metric.get("unit_family") or "")
        metric["source_url"] = cand.get("source_url") or cand.get("url") or metric.get("source_url")
        metric["context_snippet"] = cand.get("context_snippet") or cand.get("context") or metric.get("context_snippet")
        metric["candidate_id"] = cand.get("candidate_id") or metric.get("candidate_id")
        metric["anchor_hash"] = cand.get("anchor_hash") or metric.get("anchor_hash")
    except Exception:
        pass

def rebuild_metrics_from_snapshots_schema_only_fix18(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """Wrapper (ADDITIVE): post-rebuild rescue using evidence pool when selection is blank/invalid vs schema."""
    if _fix41afc52__orig_rebuild_fix18 is None:
        return {}

    rebuilt = _fix41afc52__orig_rebuild_fix18(prev_response, baseline_sources_cache, web_context=web_context)

    try:
        schema_map = {}
        if isinstance(prev_response, dict):
            schema_map = prev_response.get("metric_schema_frozen") or prev_response.get("primary_metrics_canonical") or {}
        metrics = {}
        if isinstance(rebuilt, dict):
            metrics = rebuilt.get("primary_metrics_canonical") or rebuilt.get("primary_metrics") or {}

        if isinstance(metrics, dict) and isinstance(schema_map, dict):
            for ck, metric in metrics.items():
                if not isinstance(metric, dict):
                    continue
                schema = schema_map.get(ck) if isinstance(schema_map.get(ck), dict) else {}
                # Determine if selected metric is eligible
                uf_schema = _fix41afc52_schema_unit_family(schema)
                uf_metric = str(metric.get("unit_family") or "").strip().lower()
                if not uf_metric:
                    uf_metric = _fix41afc52_infer_unit_family(metric)

                bad = False
                if uf_schema and uf_metric and uf_metric != uf_schema:
                    bad = True
                # Missing unit evidence when schema requires it
                fn_req = globals().get("_fix41afc33_schema_implies_unit_required")
                if callable(fn_req) and fn_req(schema):
                    if not _fix41afc52_has_required_unit_evidence(schema, metric):
                        bad = True
                # Blank / None values
                if metric.get("value_norm") is None and (metric.get("value") in (None, "", [])):
                    bad = True

                if not bad:
                    continue

                # Rescue from evidence pool
                evidence = metric.get("evidence") if isinstance(metric.get("evidence"), list) else []
                # Sometimes evidence is under value_range.examples
                if not evidence and isinstance(metric.get("value_range"), dict):
                    ex = metric.get("value_range", {}).get("examples")
                    if isinstance(ex, list):
                        evidence = ex

                eligible = [c for c in evidence if _fix41afc52_candidate_is_eligible(schema, c)]
                if not eligible:
                    # Record why we couldn't rescue (instrumentation only)
                    metric["_fix41afc52_rescue_used"] = False
                    metric["_fix41afc52_rescue_reason"] = "no_eligible_evidence"
                    continue

                chosen = _fix41afc52_pick_best_candidate(schema, eligible)
                if chosen:
                    metric["_fix41afc52_rescue_used"] = True
                    metric["_fix41afc52_rescue_reason"] = "selected_from_evidence_pool"
                    metric["_fix41afc52_rescue_prev_value_norm"] = metric.get("value_norm")
                    metric["_fix41afc52_rescue_prev_unit"] = metric.get("unit") or metric.get("unit_tag")
                    _fix41afc52_apply_candidate_to_metric(schema, metric, chosen)

        # Attach debug marker for auditability
        if isinstance(rebuilt, dict):
            dbg = rebuilt.get("debug")
            if not isinstance(dbg, dict):
                dbg = {}
                rebuilt["debug"] = dbg
            dbg["fix41afc52_post_rebuild_rescue"] = {
                "applied": True,
                "note": "Post-rebuild schema-driven rescue from evidence pool for blank/ineligible selections."
            }
    except Exception:
        pass

    return rebuilt

# PATCH FIX41AFC52 END
# =====================================================================

# PATCH FIX41AFC52V START — bump CODE_VERSION
CODE_VERSION = "fix41afc52_evo_post_rebuild_evidence_rescue_v1"
# PATCH FIX41AFC52V END — bump CODE_VERSION

# =====================================================================
# PATCH FIX41AFC56V START — bump CODE_VERSION
CODE_VERSION = "fix41afc56_evo_preferred_source_rescue_v1"
# PATCH FIX41AFC56V END — bump CODE_VERSION
# =====================================================================


# =====================================================================
# PATCH FIX41AFC62 START
# Preferred-source + schema-scale-token rescue (post-rebuild) to prevent
# cross-source roam when anchor_hash/candidate_id drifts, and to prevent
# unitless candidates from being "upcast" to schema unit_tag.
#
# Additive-only: does NOT refactor existing rebuild logic. We provide a
# wrapper rebuild fn that Evolution will prefer via globals().get(
# "rebuild_metrics_from_snapshots_schema_only").
# =====================================================================

def _fix41afc62_norm_url(u: str) -> str:
    try:
        fn = globals().get("_fix41afc60_norm_url") or globals().get("_fix41afc56_norm_url")
        if callable(fn):
            return fn(u)
    except Exception:
        pass
    try:
        return (u or "").strip()
    except Exception:
        return str(u)

def _fix41afc62_preferred_url(prev_response: dict, canonical_key: str, schema: dict = None) -> str:
    try:
        # Prefer explicit anchor source_url when present
        ma = (prev_response or {}).get("metric_anchors") or (prev_response or {}).get("primary_response", {}).get("metric_anchors") or {}
        a = (ma or {}).get(canonical_key) or {}
        u = (a.get("source_url") or a.get("url") or "").strip()
        if u:
            return u
    except Exception:
        pass
    try:
        # Fallback to helper if it exists
        fn = globals().get("_fix41afc56_preferred_url_for_metric")
        if callable(fn):
            u = fn(prev_response, canonical_key)
            if u:
                return str(u).strip()
    except Exception:
        pass
    try:
        # Last: schema/primary metric source_url (if present)
        s = schema or {}
        u = (s.get("source_url") or s.get("url") or "").strip()
        if u:
            return u
    except Exception:
        pass
    return ""

def _fix41afc62_schema_scale_token(schema: dict) -> str:
    """Return scale token hint like 'million'/'billion' if schema unit_tag implies it."""
    try:
        fn = globals().get("_fix41afc59_schema_unit_tag_requires_token")
        if callable(fn):
            return fn((schema or {}).get("unit_tag") or (schema or {}).get("unit") or "")
    except Exception:
        pass
    return ""

def _fix41afc62_candidate_matches_scale(schema: dict, cand: dict) -> bool:
    """If schema implies a scale token, require it to appear on candidate (unit/unit_tag/raw/context)."""
    try:
        tok = _fix41afc62_schema_scale_token(schema)
        if not tok:
            return True
        fn = globals().get("_fix41afc59_candidate_matches_schema_unit_tag")
        if callable(fn):
            return bool(fn((schema or {}).get("unit_tag") or "", cand or {}))
        # fallback: very light check
        hay = " ".join([
            str((cand or {}).get("unit") or ""),
            str((cand or {}).get("unit_tag") or ""),
            str((cand or {}).get("raw") or ""),
            str((cand or {}).get("context_snippet") or ""),
        ]).lower()
        return tok.lower() in hay
    except Exception:
        return True

def _fix41afc62_evidence_find_selected(metric: dict) -> dict:
    """Try to locate the evidence dict corresponding to the current selected anchor_hash."""
    try:
        if not isinstance(metric, dict):
            return {}
        ah = str(metric.get("anchor_hash") or metric.get("cur_anchor_hash") or "").strip()
        ev = metric.get("evidence") or []
        if ah and isinstance(ev, list):
            for e in ev:
                if isinstance(e, dict) and str(e.get("anchor_hash") or "") == ah:
                    return e
    except Exception:
        pass
    return {}

def _fix41afc62_should_rescue(schema: dict, metric: dict, preferred_url: str) -> bool:
    """Rescue when selected is cross-source OR selected is unitless/scale-missing for magnitude schema."""
    try:
        if not isinstance(metric, dict):
            return False
        sel_e = _fix41afc62_evidence_find_selected(metric)
        sel_url = str((sel_e.get("source_url") or sel_e.get("url") or metric.get("source_url") or "")).strip()
        if preferred_url and _fix41afc62_norm_url(sel_url) and _fix41afc62_norm_url(preferred_url):
            if _fix41afc62_norm_url(sel_url) != _fix41afc62_norm_url(preferred_url):
                return True
        # Scale-token guard: if schema implies scale, selected evidence must carry it
        if not _fix41afc62_candidate_matches_scale(schema, sel_e or metric):
            return True
        # Unitless guard for magnitude/energy: require explicit unit evidence on selected evidence
        uf = str((schema or {}).get("unit_family") or "").strip().lower()
        if uf in ("magnitude", "energy"):
            u = str((sel_e.get("unit_tag") or sel_e.get("unit") or sel_e.get("base_unit") or "")).strip()
            if not u:
                # only rescue if we have a preferred_url (avoid changing unanchored behavior)
                return bool(preferred_url)
        return False
    except Exception:
        return False

def _fix41afc62_best_from_preferred(schema: dict, preferred_url: str, evidence: list) -> dict:
    """Pick best eligible candidate from preferred_url evidence, deterministic."""
    try:
        if not preferred_url or not isinstance(evidence, list):
            return {}
        pref_n = _fix41afc62_norm_url(preferred_url)
        pool = []
        fn_el = globals().get("_fix41afc52_candidate_is_eligible")
        for e in evidence:
            if not isinstance(e, dict):
                continue
            eurl = str(e.get("source_url") or e.get("url") or "").strip()
            if pref_n and _fix41afc62_norm_url(eurl) != pref_n:
                continue
            # Respect eligibility
            try:
                if callable(fn_el) and not fn_el(schema, e):
                    continue
            except Exception:
                pass
            # Respect scale-token requirement (prevents unitless "170" from being treated as "million units")
            if not _fix41afc62_candidate_matches_scale(schema, e):
                continue
            pool.append(e)
        if not pool:
            return {}
        fn_pick = globals().get("_fix41afc52_pick_best_candidate")
        if callable(fn_pick):
            return dict(fn_pick(schema, pool) or {})
        # fallback: first
        return dict(pool[0])
    except Exception:
        return {}


# =====================================================================
# PATCH FIX41AFC63 START
# Percent-proxy carry-forward when:
#   - schema unit_family is percent
#   - preferred-source rescue finds no eligible candidate (often because
#     metric is proxy/evidence_missing and rebuilt metric has empty evidence)
#
# This restores analysis/evolution parity for proxy percent metrics and
# prevents cross-source hijack from injected pages.
# Additive-only: invoked only inside FIX41AFC62 wrapper flow.
# =====================================================================

def _fix41afc63_is_percent_schema(schema: dict) -> bool:
    try:
        uf = str((schema or {}).get("unit_family") or "").strip().lower()
        ut = str((schema or {}).get("unit_tag") or (schema or {}).get("unit") or "").strip().lower()
        return (uf == "percent") or (ut == "%") or ("percent" in ut)
    except Exception:
        return False

def _fix41afc63_prev_metric(prev_response: dict, canonical_key: str) -> dict:
    try:
        pm = (prev_response or {}).get("primary_metrics_canonical") or {}
        if isinstance(pm, dict) and isinstance(pm.get(canonical_key), dict):
            return pm.get(canonical_key) or {}
    except Exception:
        pass
    try:
        # fallback: some payloads nest in primary_response
        pm = (prev_response or {}).get("primary_response", {}).get("primary_metrics_canonical") or {}
        if isinstance(pm, dict) and isinstance(pm.get(canonical_key), dict):
            return pm.get(canonical_key) or {}
    except Exception:
        pass
    return {}

def _fix41afc63_apply_prev_as_current(schema: dict, m: dict, prevm: dict) -> bool:
    """Carry forward previous metric value into current rebuilt metric m."""
    try:
        if not isinstance(m, dict) or not isinstance(prevm, dict):
            return False
        pv = prevm.get("value")
        pvn = prevm.get("value_norm")
        pu = prevm.get("unit") or prevm.get("unit_tag") or "%"
        # apply
        m["value"] = pv
        m["value_norm"] = pvn if pvn is not None else pv
        m["unit"] = pu
        m["unit_tag"] = prevm.get("unit_tag") or "%"
        # preserve preferred source for dashboard traceability
        if prevm.get("source_url"):
            m["source_url"] = prevm.get("source_url")
        # mark proxy carry-forward
        m["is_proxy"] = True
        m["proxy_type"] = m.get("proxy_type") or prevm.get("proxy_type") or "evidence_missing"
        m["proxy_reason"] = "percent_proxy_carry_forward_no_eligible_candidate_fix41afc63"
        return True
    except Exception:
        return False

# PATCH FIX41AFC63 END
# =====================================================================

def rebuild_metrics_from_snapshots_schema_only(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    FIX41AFC62 wrapper:
      - Calls existing schema-only rebuild (FIX16) if available
      - Post-processes rebuilt metrics to prevent cross-source hijack when anchors drift:
          * preferred-source rescue using evidence
          * schema scale-token guard to avoid unitless coercion
      - Leaves fastpath/hashing untouched (wrapper only affects rebuild path).
    """
    fn_base = (
        globals().get("rebuild_metrics_from_snapshots_schema_only_fix16")
        or globals().get("rebuild_metrics_from_snapshots_schema_only")
        or globals().get("rebuild_metrics_from_snapshots")
    )
    if not callable(fn_base):
        return {}
    rebuilt = fn_base(prev_response, baseline_sources_cache, web_context=web_context)
    try:
        if not isinstance(rebuilt, dict):
            return rebuilt
        # Build schema map for lookups
        schema_map = (prev_response or {}).get("metric_schema_frozen") or (prev_response or {}).get("metric_schema") or {}
        prim_map = (prev_response or {}).get("primary_metrics_canonical") or {}
        dbg = (prev_response or {}).setdefault("_evolution_rebuild_debug", {})
        dbg.setdefault("fix41afc62_rescues", [])
        for ck, m in list(rebuilt.items()):
            if not isinstance(m, dict):
                continue
            schema = {}
            if isinstance(schema_map, dict) and ck in schema_map and isinstance(schema_map.get(ck), dict):
                schema = schema_map.get(ck) or {}
            elif isinstance(prim_map, dict) and ck in prim_map and isinstance(prim_map.get(ck), dict):
                schema = prim_map.get(ck) or {}
            pref = _fix41afc62_preferred_url(prev_response, ck, schema=schema)
            if not pref:
                continue
            if not _fix41afc62_should_rescue(schema, m, pref):
                continue
            ev = m.get("evidence") or []
            best = _fix41afc62_best_from_preferred(schema, pref, ev)
            if not best:
                # FIX41AFC63: for percent metrics that are proxy/evidence_missing,
                # carry forward prev value instead of blanking or roaming.
                carried = False
                if _fix41afc63_is_percent_schema(schema):
                    prevm = _fix41afc63_prev_metric(prev_response, ck)
                    if isinstance(prevm, dict) and (prevm.get("value_norm") is not None or prevm.get("value") is not None):
                        carried = _fix41afc63_apply_prev_as_current(schema, m, prevm)
                        if carried:
                            try:
                                m["anchor_used"] = True
                                m["anchor_used_reason"] = "percent_proxy_carry_forward_fix41afc63"
                                m["preferred_source_url_fix41afc63"] = pref
                            except Exception:
                                pass
                # If we had a preferred source but no eligible candidate there, leave metric unchanged
                # (unless carried forward) and add debug for visibility.
                dbg["fix41afc62_rescues"].append({
                    "canonical_key": ck,
                    "preferred_url": pref,
                    "rescued": False,
                    "carried_forward": bool(carried),
                    "reason": "no_eligible_candidate_in_preferred_source" if not carried else "percent_proxy_carry_forward_fix41afc63",
                })
                continue
            # Apply rescued candidate to metric
            try:
                fn_apply = globals().get("_fix41afc52_apply_candidate_to_metric")
                if callable(fn_apply):
                    fn_apply(schema, m, best)
                else:
                    # minimal in-place apply fallback
                    m["value_norm"] = best.get("value_norm", best.get("value"))
                    m["value"] = best.get("value", m.get("value"))
                    m["unit"] = best.get("unit", m.get("unit"))
                    m["unit_tag"] = best.get("unit_tag", m.get("unit_tag"))
                    m["anchor_hash"] = best.get("anchor_hash", m.get("anchor_hash"))
                    m["source_url"] = best.get("source_url", m.get("source_url"))
            except Exception:
                pass
            try:
                m["anchor_used"] = True
                m["anchor_used_reason"] = "preferred_source_scale_token_rescue_fix41afc62"
                m["preferred_source_url_fix41afc62"] = pref
            except Exception:
                pass
            dbg["fix41afc62_rescues"].append({
                "canonical_key": ck,
                "preferred_url": pref,
                "rescued": True,
                "selected_anchor_hash": str(best.get("anchor_hash") or ""),
                "selected_raw": str(best.get("raw") or ""),
            })
    except Exception:
        return rebuilt
    return rebuilt

# PATCH FIX41AFC62 END
# =====================================================================

# PATCH FIX41AFC62_VERSION START
try:
    CODE_VERSION = "fix41afc62_preferred_source_scale_token_rescue_v1"
except Exception:
    pass
# PATCH FIX41AFC62_VERSION END

# PATCH FIX41AFC63_VERSION START
try:
    CODE_VERSION = "fix41afc63_percent_proxy_carry_forward_v1"
except Exception:
    pass
# PATCH FIX41AFC63_VERSION END

# =========================================================
# AI FINANCIAL RESEARCH ASSISTANT ‚Äì HYBRID VERIFICATION v5.3
# WITH WEB SEARCH INTEGRATION (SerpAPI + ScrapingDog)
# Complete standalone version - ready to deploy
# =========================================================

import os
import json
import requests
import pandas as pd
import plotly.express as px
import streamlit as st
from sentence_transformers import SentenceTransformer, util
from transformers import pipeline
from collections import Counter
import google.generativeai as genai
import numpy as np
from bs4 import BeautifulSoup
import re

# ----------------------------
# STEP 1: CONFIGURATION
# ----------------------------
# Try Streamlit secrets first, fallback to environment variables
try:
    PERPLEXITY_KEY = st.secrets["PERPLEXITY_API_KEY"]
    GEMINI_KEY = st.secrets["GEMINI_API_KEY"]
    SERPAPI_KEY = st.secrets.get("SERPAPI_KEY", "")
    SCRAPINGDOG_KEY = st.secrets.get("SCRAPINGDOG_KEY", "")
except:
    PERPLEXITY_KEY = os.getenv("PERPLEXITY_API_KEY")
    GEMINI_KEY = os.getenv("GEMINI_API_KEY")
    SERPAPI_KEY = os.getenv("SERPAPI_KEY", "")
    SCRAPINGDOG_KEY = os.getenv("SCRAPINGDOG_KEY", "")

if not PERPLEXITY_KEY or not GEMINI_KEY:
    st.error("Missing API keys. Please set PERPLEXITY_API_KEY and GEMINI_API_KEY.")
    st.stop()

PERPLEXITY_URL = "https://api.perplexity.ai/chat/completions"

# Configure Gemini properly
genai.configure(api_key=GEMINI_KEY)
gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')

RESPONSE_TEMPLATE = """
You are a research assistant. Return ONLY valid JSON formatted as:
{
  "summary": "Brief summary of findings.",
  "key_insights": ["Insight 1", "Insight 2"],
  "metrics": {"GDP Growth (%)": number, "Inflation (%)": number, "Unemployment (%)": number},
  "visual_data": {"labels": ["Q1","Q2"], "values": [2.3,2.5]},
  "table": [{"Country": "US", "GDP": 25.5, "Inflation": 3.4}],
  "sources": ["https://imf.org", "https://reuters.com"],
  "confidence_score": 85,
  "data_freshness": "As of [date]"
}
"""

SYSTEM_PROMPT = (
    "You are an AI research analyst that only answers topics related to business, finance, economics, or markets.\n"
    "Output strictly in the JSON structure below:\n"
    f"{RESPONSE_TEMPLATE}"
)

# Cache model loading
@st.cache_resource
def load_models():
    """Load ML models once and cache them"""
    classifier = pipeline(
        "zero-shot-classification", 
        model="facebook/bart-large-mnli", 
        device=-1
    )
    embed = SentenceTransformer("all-MiniLM-L6-v2")
    return classifier, embed

# Domain classifier and embedder
ALLOWED_TOPICS = ["finance", "economics", "markets", "business", "macroeconomics"]
domain_classifier, embedder = load_models()

# ----------------------------
# STEP 2: WEB SEARCH FUNCTIONS
# ----------------------------
def search_serpapi(query: str, num_results: int = 5):
    """Search using SerpAPI (Google Search)"""
    if not SERPAPI_KEY:
        st.info("üí° SerpAPI key not configured. Add SERPAPI_KEY to secrets for enhanced web search.")
        return []
    
    url = "https://serpapi.com/search"
    params = {
        "engine": "google",
        "q": f"{query} finance economics markets",
        "api_key": SERPAPI_KEY,
        "num": num_results,
        "tbm": "nws",  # News results
        "tbs": "qdr:m"  # Past month
    }
    
    try:
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        results = []
        
        # Extract news results
        for item in data.get("news_results", [])[:num_results]:
            results.append({
                "title": item.get("title", ""),
                "link": item.get("link", ""),
                "snippet": item.get("snippet", ""),
                "date": item.get("date", ""),
                "source": item.get("source", {}).get("name", "")
            })
        
        # Fallback to organic results if no news
        if not results:
            for item in data.get("organic_results", [])[:num_results]:
                results.append({
                    "title": item.get("title", ""),
                    "link": item.get("link", ""),
                    "snippet": item.get("snippet", ""),
                    "date": "",
                    "source": item.get("source", "")
                })
        
        if results:
            st.success(f"‚úÖ Found {len(results)} sources via SerpAPI")
        return results
        
    except requests.exceptions.RequestException as e:
        st.warning(f"‚ö†Ô∏è SerpAPI search error: {e}")
        return []
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Error processing SerpAPI results: {e}")
        return []


def scrape_url_scrapingdog(url: str):
    """Scrape content from URL using ScrapingDog"""
    if not SCRAPINGDOG_KEY:
        return None
    
    api_url = "https://api.scrapingdog.com/scrape"
    params = {
        "api_key": SCRAPINGDOG_KEY,
        "url": url,
        "dynamic": "false"
    }
    
    try:
        response = requests.get(api_url, params=params, timeout=15)
        response.raise_for_status()
        
        # Parse HTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Remove unwanted elements
        for element in soup(["script", "style", "nav", "footer", "header", "aside"]):
            element.decompose()
        
        # Extract text
        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = '\n'.join(chunk for chunk in chunks if chunk)
        
        return text[:3000]  # Limit to 3000 chars
        
    except Exception as e:
        st.warning(f"‚ö†Ô∏è ScrapingDog error for {url[:50]}: {e}")
        return None


def fetch_web_context(query: str, num_sources: int = 3):
    """Fetch web context by searching and optionally scraping"""
    # Step 1: Search for sources
    search_results = search_serpapi(query, num_results=5)
    
    if not search_results:
        return {
            "search_results": [],
            "scraped_content": {},
            "summary": "",
            "sources": []
        }
    
    # Step 2: Scrape top results (if ScrapingDog is configured)
    scraped_content = {}
    if SCRAPINGDOG_KEY:
        st.info(f"üîç Scraping top {min(num_sources, len(search_results))} sources...")
        for i, result in enumerate(search_results[:num_sources]):
            url = result["link"]
            content = scrape_url_scrapingdog(url)
            if content:
                scraped_content[url] = content
                st.success(f"‚úì Scraped {i+1}/{num_sources}: {result['source']}")
    
    # Step 3: Create summary
    context_parts = []
    for r in search_results:
        date_str = f" ({r['date']})" if r['date'] else ""
        context_parts.append(
            f"**{r['title']}**{date_str}\n"
            f"Source: {r['source']}\n"
            f"{r['snippet']}\n"
            f"URL: {r['link']}"
        )
    
    summary = "\n\n---\n\n".join(context_parts)
    sources = [r["link"] for r in search_results]
    
    return {
        "search_results": search_results,
        "scraped_content": scraped_content,
        "summary": summary,
        "sources": sources
    }

# ----------------------------
# STEP 3: ENHANCED QUERY FUNCTIONS
# ----------------------------
def query_perplexity_with_context(query: str, web_context: dict, temperature=0.7):
    """Query Perplexity with web-scraped context"""
    
    # Build enhanced prompt with web context
    if web_context["summary"]:
        context_section = f"""
LATEST WEB RESEARCH (Current as of today):
{web_context['summary']}

"""
        # Add scraped content if available
        if web_context['scraped_content']:
            context_section += "\nDETAILED CONTENT FROM TOP SOURCES:\n"
            for url, content in list(web_context['scraped_content'].items())[:2]:
                context_section += f"\nFrom {url}:\n{content[:800]}...\n"
        
        enhanced_query = f"{context_section}\n{SYSTEM_PROMPT}\n\nUser Question: {query}"
    else:
        enhanced_query = f"{SYSTEM_PROMPT}\n\nUser Question: {query}"
    
    headers = {
        "Authorization": f"Bearer {PERPLEXITY_KEY}", 
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": "sonar",
        "temperature": temperature,
        "max_tokens": 2000,
        "messages": [
            {"role": "user", "content": enhanced_query}
        ]
    }
    
    try:
        resp = requests.post(
            PERPLEXITY_URL, 
            headers=headers, 
            json=payload, 
            timeout=45
        )
        
        if resp.status_code != 200:
            error_detail = resp.text
            st.error(f"Perplexity API Error {resp.status_code}: {error_detail}")
            raise Exception(f"Perplexity returned {resp.status_code}: {error_detail}")
        
        response_data = resp.json()
        
        if "choices" not in response_data:
            raise Exception(f"No 'choices' in response")
        
        content = response_data["choices"][0]["message"]["content"]
        
        if not content or not content.strip():
            raise Exception("Perplexity returned empty response")
        
        # Validate JSON and inject sources
        try:
            parsed = json.loads(content)
            # Add our web sources
            if web_context["sources"]:
                existing_sources = parsed.get("sources", [])
                all_sources = existing_sources + web_context["sources"]
                parsed["sources"] = list(set(all_sources))[:10]  # Unique, max 10
                parsed["data_freshness"] = "Current (web-scraped + real-time search)"
            content = json.dumps(parsed)
        except json.JSONDecodeError:
            # If not valid JSON, wrap it
            st.warning("Reformatting Perplexity response...")
            content = json.dumps({
                "summary": content[:500],
                "key_insights": [content[:200]],
                "metrics": {},
                "visual_data": {},
                "table": [],
                "sources": web_context["sources"],
                "confidence_score": 50,
                "data_freshness": "Current (web-scraped)"
            })
        
        return content
        
    except Exception as e:
        st.error(f"Perplexity query error: {e}")
        raise


def query_gemini(query: str):
    """Query Gemini API with proper error handling"""
    prompt = f"{SYSTEM_PROMPT}\n\nUser query: {query}"
    
    try:
        response = gemini_model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.3,
                max_output_tokens=1000,
            )
        )
        content = response.text
        
        if not content or not content.strip():
            raise Exception("Gemini returned empty response")
        
        try:
            json.loads(content)
        except json.JSONDecodeError:
            st.warning("Gemini returned non-JSON response, reformatting...")
            content = json.dumps({
                "summary": content[:500],
                "key_insights": [content[:200]],
                "metrics": {},
                "visual_data": {},
                "table": [],
                "sources": [],
                "confidence_score": 50
            })
        
        return content
    except Exception as e:
        st.warning(f"Gemini API error: {e}")
        return json.dumps({
            "summary": "Gemini validation unavailable due to API error.",
            "key_insights": ["Cross-validation could not be performed"],
            "metrics": {},
            "visual_data": {},
            "table": [],
            "sources": [],
            "confidence_score": 0
        })

# ----------------------------
# STEP 4: SELF-CONSISTENCY WITH WEB SEARCH
# ----------------------------
def generate_self_consistent_responses_with_web(query, web_context, n=3):
    """Generate multiple responses with web context"""
    st.info(f"Generating {n} independent analyst responses with web context...")
    responses, scores = [], []
    success_count = 0
    
    for i in range(n):
        try:
            r = query_perplexity_with_context(query, web_context, temperature=0.8)
            responses.append(r)
            scores.append(parse_confidence(r))
            success_count += 1
        except Exception as e:
            st.warning(f"Attempt {i+1}/{n} failed: {e}")
            continue
    
    if success_count == 0:
        st.error("All Perplexity API calls failed.")
        return [], []
    
    st.success(f"Successfully generated {success_count}/{n} responses")
    return responses, scores


def majority_vote(responses):
    """Select most common response via voting"""
    if not responses:
        return ""
    cleaned = [r.strip() for r in responses if r]
    if not cleaned:
        return ""
    return Counter(cleaned).most_common(1)[0][0]


def parse_confidence(text):
    """Extract confidence score from JSON response"""
    try:
        js = json.loads(text)
        conf = js.get("confidence_score", 0)
        return float(conf)
    except (json.JSONDecodeError, ValueError, TypeError):
        return 0.0

# ----------------------------
# STEP 5: VALIDATION FUNCTIONS
# ----------------------------
def semantic_similarity_score(a, b):
    """Calculate semantic similarity between two texts"""
    try:
        v1, v2 = embedder.encode([a, b])
        sim = util.cos_sim(v1, v2)
        if hasattr(sim, 'item'):
            score = sim.item()
        elif hasattr(sim, 'shape'):
            score = float(sim[0][0])
        else:
            score = float(sim)
        return round(score * 100, 2)
    except Exception as e:
        st.warning(f"Embedding error: {e}")
        return 0.0


def numeric_alignment_score(j1, j2):
    """Compare numeric metrics between two JSON responses"""
    m1 = j1.get("metrics", {})
    m2 = j2.get("metrics", {})
    
    if not m1 or not m2:
        return None
    
    total_diff, count = 0, 0
    
    for key in m1:
        if key in m2:
            try:
                v1, v2 = float(m1[key]), float(m2[key])
                
                if v1 == 0 and v2 == 0:
                    diff = 0
                elif max(abs(v1), abs(v2)) == 0:
                    diff = 0
                else:
                    diff = abs(v1 - v2) / max(abs(v1), abs(v2))
                
                total_diff += diff
                count += 1
            except (ValueError, TypeError):
                continue
    
    if count == 0:
        return None
    
    alignment = 1 - (total_diff / count)
    return round(alignment * 100, 2)

# ----------------------------
# STEP 6: DASHBOARD RENDERER
# ----------------------------
def render_dashboard(response, final_conf, sem_conf, num_conf, web_context=None):
    """Render the financial dashboard with results"""
    
    if not response or not response.strip():
        st.error("Received empty response from model")
        return
    
    try:
        data = json.loads(response)
    except json.JSONDecodeError as e:
        st.error(f"Invalid JSON returned by model: {e}")
        st.write("**Raw response received:**")
        st.code(response[:1000], language="text")
        return

    # Confidence and freshness display
    col1, col2 = st.columns(2)
    col1.metric("Overall Confidence (%)", f"{final_conf:.1f}")
    freshness = data.get("data_freshness", "Unknown")
    col2.metric("Data Freshness", freshness)
    
    # Main summary
    st.header("üìä Financial Summary")
    st.write(data.get("summary", "No summary available."))

    # Key insights
    st.subheader("Key Insights")
    insights = data.get("key_insights", [])
    if insights:
        for insight in insights:
            st.markdown(f"- {insight}")
    else:
        st.info("No key insights provided.")

    # Metrics display
    st.subheader("Metrics")
    mets = data.get("metrics", {})
    if mets:
        cols = st.columns(len(mets))
        for i, (k, v) in enumerate(mets.items()):
            cols[i].metric(k, v)
    else:
        st.info("No metrics available.")

    # Visualization
    st.subheader("Trend Visualization")
    vis = data.get("visual_data", {})
    if "labels" in vis and "values" in vis:
        try:
            df = pd.DataFrame({
                "Period": vis["labels"], 
                "Value": vis["values"]
            })
            fig = px.line(
                df, 
                x="Period", 
                y="Value", 
                title="Quarterly Trends", 
                markers=True
            )
            st.plotly_chart(fig, use_container_width=True)
        except Exception as e:
            st.warning(f"Could not render visualization: {e}")
    else:
        st.info("No visual data available.")

    # Data table
    st.subheader("Data Table")
    tab = data.get("table", [])
    if tab:
        try:
            st.dataframe(pd.DataFrame(tab), use_container_width=True)
        except Exception as e:
            st.warning(f"Could not render table: {e}")
    else:
        st.info("No tabular data available.")

    # Sources - PROMINENT DISPLAY
    st.subheader("üìö Sources & References")
    sources = data.get("sources", [])
    if sources:
        st.success(f"‚úÖ Information from {len(sources)} sources:")
        for i, s in enumerate(sources, 1):
            st.markdown(f"{i}. [{s}]({s})")
    else:
        st.info("No sources cited.")
    
    # Show web search stats if available
    if web_context and web_context.get("search_results"):
        with st.expander("üîç Web Search Details"):
            st.write(f"**Sources Found:** {len(web_context['search_results'])}")
            st.write(f"**Pages Scraped:** {len(web_context.get('scraped_content', {}))}")
            for result in web_context['search_results']:
                st.markdown(f"- **{result['title']}** ({result.get('source', 'Unknown')})")

    # Validation metrics
    st.subheader("Validation Metrics")
    col1, col2 = st.columns(2)
    col1.metric("Semantic Similarity", f"{sem_conf:.2f}%")
    if num_conf is not None:
        col2.metric("Numeric Alignment", f"{num_conf:.2f}%")
    else:
        col2.info("No numeric data to compare")

# ----------------------------
# STEP 7: MAIN WORKFLOW
# ----------------------------
def main():
    st.set_page_config(
        page_title="Yureeka Market Research Assistant", 
        layout="wide"
    )
    st.title("üíπ Yureeka AI Market Analyst")
    st.caption("Self-Consistency + Cross-Model Verification + Live Web Search")
    
    # Show web search status
    col1, col2 = st.columns([3, 1])
    with col1:
        st.markdown("""
        This assistant combines:
        - **Self-consistency reasoning** via multiple Perplexity analyses
        - **Cross-model validation** using Gemini 2.0 Flash
        - **Live web search** (SerpAPI + ScrapingDog)
        - **Semantic and numeric alignment** scoring
        """)
    with col2:
        web_status = "‚úÖ Enabled" if SERPAPI_KEY else "‚ö†Ô∏è Not configured"
        st.metric("Web Search", web_status)

    q = st.text_input("Enter your question about markets, finance, or economics:")
    
    # Web search toggle
    use_web_search = st.checkbox(
        "üåê Enable live web search (recommended for current data)", 
        value=bool(SERPAPI_KEY),
        disabled=not SERPAPI_KEY,
        help="Searches the web for latest information before analysis"
    )

    if st.button("Analyze") and q:
        
        # Fetch web context if enabled
        web_context = {}
        if use_web_search:
            with st.spinner("üîç Searching the web for latest information..."):
                web_context = fetch_web_context(q, num_sources=3)
        
        # Generate responses
        if web_context and web_context.get("search_results"):
            responses, scores = generate_self_consistent_responses_with_web(q, web_context, n=3)
        else:
            st.info("üìö Proceeding with AI model knowledge only...")
            # Fallback to original method without web context
            empty_context = {"search_results": [], "scraped_content": {}, "summary": "", "sources": []}
            responses, scores = generate_self_consistent_responses_with_web(q, empty_context, n=3)
        
        if not responses or not scores:
            st.error("Primary model failed to generate any valid responses.")
            return
        
        if len(responses) != len(scores):
            st.error("Internal error: response/score alignment mismatch")
            return
        
        # Select best response
        voted_response = majority_vote(responses)
        max_score = max(scores)
        best_idx = scores.index(max_score)
        best_response = responses[best_idx]
        
        chosen_primary = best_response if best_response else voted_response

        if not chosen_primary:
            st.error("Could not determine primary response.")
            return

        # Cross-validation with Gemini
        st.info("Cross‚Äëverifying via Gemini 2.0 Flash...")
        secondary_resp = query_gemini(q)

        # Scoring
        sem_conf = semantic_similarity_score(chosen_primary, secondary_resp)
        
        try:
            j1 = json.loads(chosen_primary)
        except json.JSONDecodeError:
            j1 = {}
        
        try:
            j2 = json.loads(secondary_resp)
        except json.JSONDecodeError:
            j2 = {}
        
        num_conf = numeric_alignment_score(j1, j2)
        base_conf = max_score

        # Calculate final confidence
        confidence_components = [base_conf, sem_conf]
        if num_conf is not None:
            confidence_components.append(num_conf)
        
        final_conf = np.mean(confidence_components)

        # Display results
        render_dashboard(chosen_primary, final_conf, sem_conf, num_conf, web_context)
        
        # Debug info
        with st.expander("üîç Debug Information"):
            st.write("**Primary Response (Perplexity):**")
            st.code(chosen_primary, language="json")
            st.write("**Validation Response (Gemini):**")
            st.code(secondary_resp, language="json")
            st.write(f"**All Confidence Scores:** {scores}")
            st.write(f"**Selected Best Score:** {base_conf}")
            if web_context:
                st.write(f"**Web Sources Found:** {len(web_context.get('search_results', []))}")

# ----------------------------
# RUN STREAMLIT
# ----------------------------
if __name__ == "__main__":
    main()

# ===============================================================================
# YUREEKA AI RESEARCH ASSISTANT v7.41
# With Web Search, Evidence-Based Verification, Confidence Scoring
# SerpAPI Output with Evolution Layer Version
# Updated SerpAPI parameters for stable output
# Deterministic Output From LLM
# Deterministic Evolution Core Using Python Diff Engine
# Anchored Evolution Analysis Using JSON As Input Into Model
# Implementation of Source-Based Evolution
# Saving of JSON output Files into Google Sheets
# Canonical Metric Registry + Semantic Hashing of Findings
# Removal of Evolution Decisions from LLM
# Further Enhancements to Minimize Evolution Drift (Metric)
# Saving of Extraction Cache in JSON
# Prioritize High Quality Sources With Source Freshness Tracking
# Timestamps = Timezone Naive
# Improved Stability of Handling of Duplicate Canonicalized IDs
# Deterministic Main and Side Topic Extractor
# Range Aware Canonical Metrics
# Range + Source Attribution
# Proxy Labeler + Geo Tagging
# Improved Main Topic + Side Topic Extractor Using Deterministic-->NLP-->LLM layer
# Guardrails For Main + Side Topic Handling
# Numeric Consistency Scores
# Multi-Side Enumerations
# Dashboard Unit Presentation Fixes (Main + Evolution)
# Domain-Agnostic Question Profiling
# Baseline Caching Contains HTTP Validators + Numeric Data
# URL canonicalization
# Evolution Layer Leverage On New Analysis Pipeline to Minimise Volatility
# Canonicalization of Evolution Layer Metrics To Match Analysis Layer
# Fix URL/path Collapese Issue Causing + Tighten Evolution Extraction (Topic Gating)
# canonical-key-first matching
# Evolution Pipeline to Consume analysis upstream artifacts
# safety-net hard gates (minimal) before matching
# Tighten canonical identity + unit-family constraints
# Fingerprint freshness gating to evolution
# Fix SerpAPI access and fetching
# Keeps your snapshot-friendly scraped_meta (with extracted numbers + fingerprint fields)
# Safe fallback scraper when ScrapingDog is unavailable
# Prevent caching “empty results” from SerpAPI (no poisoned cache)
# Restoration of Range Estimates For Metrics
# Improved Junk Tagging and Rejection
# One Canononical Operator for Analysis + Evolution Layers
# Metric Aware Range Construction Everywhere
# Anchor Matching Correctness
# Unit Measure + Attribute Association e.g. M + units (sold)
# Enriched metric_schema_frozen (analysis side)
# THIS VERSION HAS THE PLUMBING LOCKED-DOWN
# ONLY THE METRIC EXTRACTION LAYER FOR EVOLUTION REQUIRES WORK
# ================================================================================

import io
import os
import re
import json
import requests
import pandas as pd
import plotly.express as px
import streamlit as st
import base64
import hashlib
import numpy as np
import difflib
import gspread
import google.generativeai as genai
from pypdf import PdfReader
from pathlib import Path
from google.oauth2.service_account import Credentials
from dataclasses import dataclass
from typing import Dict, List, Optional, Any, Union, Tuple
from sentence_transformers import SentenceTransformer, util
from transformers import pipeline
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup
from collections import Counter
from pydantic import BaseModel, Field, ValidationError, ConfigDict

# =========================
# VERSION STAMP (ADDITIVE)
# =========================
CODE_VERSION = 'fix41afc19_evo_fix16_anchor_rebuild_override_v1_fix2b_hardwire_v34f'  # PATCH FIX41F (ADD): set CODE_VERSION to filename
# =====================================================================
# PATCH V21_VERSION_BUMP (ADDITIVE): bump CODE_VERSION for audit
# =====================================================================
#CODE_VERSION = 'fix41afc19_evo_fix16_anchor_rebuild_override_v1_fix2b_hardwire_v21'

# =====================================================================
# PATCH V22_VERSION_BUMP (ADDITIVE): bump CODE_VERSION for audit
# =====================================================================
#CODE_VERSION = 'fix41afc19_evo_fix16_anchor_rebuild_override_v1_fix2b_hardwire_v22'
# PATCH FIX41AFC6 (ADD): bump CODE_VERSION to new patch filename
#CODE_VERSION = "fix41afc19_evo_fix16_anchor_rebuild_override_v1_fix2b_hardwire_v34e"

# =====================================================================
# PATCH FIX41T (ADDITIVE): bump CODE_VERSION marker for this patched build
# - Purely a version label for debugging/traceability.
# - Does NOT alter runtime logic.
# =====================================================================
#CODE_VERSION = "fix41t_evo_extra_url_injection_trace_replay"
# =====================================================================
# PATCH FIX41U (ADDITIVE): bump CODE_VERSION marker for this patched build
# =====================================================================
#CODE_VERSION = "fix41u_evo_diag_prewire_replay_visibility"
# =====================================================================
# PATCH FIX41J (ADD): bump CODE_VERSION to this file version (additive override)
# PATCH FIX40 (ADD): prior CODE_VERSION preserved above
# PATCH FIX33E (ADD): previous CODE_VERSION was: CODE_VERSION = "fix33_fixed_indent.py"  # PATCH FIX33D (ADD): set CODE_VERSION to filename
# PATCH FIX33D (ADD): previous CODE_VERSION was: CODE_VERSION = "v7_41_endstate_fix24_sheets_replay_scrape_unified_engine_fix27_strict_schema_gate_v2"
# =====================================================================
# PATCH FINAL (ADDITIVE): end-state single bump label (non-breaking)
# NOTE: We do not overwrite CODE_VERSION to avoid any legacy coupling.
# =====================================================================
# PATCH FIX41AFC18 (ADDITIVE): bump CODE_VERSION to this file version
# =====================================================================
#CODE_VERSION = "fix41afc18_evo_schema_preserve_guard_on_injection_v1"
# =====================================================================
# Consumers can prefer ENDSTATE_FINAL_VERSION when present.
# =====================================================================
ENDSTATE_FINAL_VERSION = "v7_41_endstate_final_1"
INJ_TRACE_PATCH_VERSION = "fix41q_inj_trace_v1_always_emit"
# =====================================================================

# =====================================================================
# PATCH ES2/ES8/ES9 (ADDITIVE): shared determinism helpers for drift=0
# - Deterministic sorting / tie-breaking helpers
# - Deterministic candidate index builder (anchor_hash -> best candidate)
# - Lightweight schema + universe hashing for convergence checks
# - One-button end-state validation harness (callable)
# NOTE: Additive only; existing logic remains intact.
# =====================================================================
import hashlib as _es_hashlib

def _es_hash_text(s: str) -> str:
    try:
        return _es_hashlib.sha256((s or "").encode("utf-8")).hexdigest()
    except Exception:
        return ""

def _es_stable_sort_key(v):
    """
    Deterministic sort key that never relies on Python's randomized hash().
    Keeps ordering stable across runs for mixed types.
    """
    try:
        if v is None:
            return (0, "")
        if isinstance(v, (int, float)):
            return (1, f"{v:.17g}")
        if isinstance(v, str):
            return (2, v)
        if isinstance(v, bytes):
            return (3, v.decode("utf-8", "ignore"))
        if isinstance(v, dict):
            items = sorted(((str(k), _es_stable_sort_key(vv)) for k, vv in v.items()), key=lambda x: x[0])
            return (4, str(items))
        if isinstance(v, (list, tuple, set)):
            lst = list(v)
            try:
                lst.sort(key=_es_stable_sort_key)
            except Exception:
                lst = sorted(lst, key=lambda x: str(x))
            return (5, str([_es_stable_sort_key(x) for x in lst]))
        return (9, str(v))
    except Exception:
        return (9, str(v))

def _es_sorted_pairs_from_sources_cache(baseline_sources_cache):
    pairs = []
    for sr in (baseline_sources_cache or []):
        if not isinstance(sr, dict):
            continue
        u = (sr.get("source_url") or sr.get("url") or "").strip()
        fp = (sr.get("source_fingerprint") or sr.get("fingerprint") or sr.get("content_fingerprint") or "").strip()
        if u and fp:
            pairs.append((u, fp))
    pairs.sort(key=lambda t: (t[0], t[1]))
    return pairs

def _es_compute_canonical_universe_hash(primary_metrics_canonical: dict, metric_schema_frozen: dict) -> str:
    try:
        keys = set()
        if isinstance(primary_metrics_canonical, dict):
            keys.update([str(k) for k in primary_metrics_canonical.keys()])
        if isinstance(metric_schema_frozen, dict):
            keys.update([str(k) for k in metric_schema_frozen.keys()])
        return _es_hash_text("|".join(sorted(keys)))
    except Exception:
        return ""

def _es_compute_schema_hash(metric_schema_frozen: dict) -> str:
    """
    Deterministic hash of schema fields that affect numeric comparisons.
    Keeps it lightweight: tolerances + units + scale hints only.
    """
    try:
        if not isinstance(metric_schema_frozen, dict):
            return ""
        rows = []
        for k in sorted(metric_schema_frozen.keys()):
            s = metric_schema_frozen.get(k) or {}
            if not isinstance(s, dict):
                continue
            abs_eps = s.get("abs_eps", s.get("ABS_EPS"))
            rel_eps = s.get("rel_eps", s.get("REL_EPS"))
            unit = s.get("unit") or s.get("units") or ""
            scale = s.get("scale") or s.get("magnitude") or ""
            rows.append(f"{k}::abs={abs_eps}::rel={rel_eps}::unit={unit}::scale={scale}")
        return _es_hash_text("|".join(rows))
    except Exception:
        return ""

def _es_build_candidate_index_deterministic(baseline_sources_cache):
    """
    Deterministically build anchor_hash -> candidate map.
    If multiple candidates share the same anchor_hash, choose the best by a stable
    tie-breaker that prefers:
      - higher anchor_confidence
      - longer context_snippet (more evidence)
      - stable context_hash / numeric value / unit
      - stable source_url
    """
    try:
        buckets = {}
        for sr in (baseline_sources_cache or []):
            if not isinstance(sr, dict):
                continue
            su = sr.get("source_url") or sr.get("url") or ""
            for cand in (sr.get("extracted_numbers") or []):
                if not isinstance(cand, dict):
                    continue
                ah = cand.get("anchor_hash") or cand.get("anchor") or ""
                if not ah:
                    continue
                c2 = dict(cand)
                if "source_url" not in c2:
                    c2["source_url"] = su
                buckets.setdefault(ah, []).append(c2)

        out = {}
        for ah in sorted(buckets.keys()):
            cands = buckets.get(ah) or []
            def _cand_key(c):
                try:
                    conf = c.get("anchor_confidence")
                    conf_key = -(float(conf) if conf is not None else 0.0)
                except Exception:
                    conf_key = 0.0
                ctx = (c.get("context_snippet") or c.get("context") or "")
                ctx_len = -len(str(ctx))
                ctx_hash = c.get("context_hash") or ""
                val = c.get("value")
                unit = c.get("unit") or ""
            # PATCH FIX27 (ADDITIVE): Eligibility gate BEFORE scoring.
            # Reject bare-year tokens for non-year metrics when there is no token unit evidence.
            if expected_kind != "year":
                raw_token = (c.get("raw") or "").strip()
                if _fix27_is_bare_year_token(raw_token, c.get("value_norm")) and not _fix27_has_any_unit_evidence(c):
                    continue
            # Typed metrics require explicit token-level unit evidence
            if expected_kind == "currency" and not _fix27_has_currency_evidence(c):
                continue
            if expected_kind == "percent" and not _fix27_has_percent_evidence(c):
                continue
            if expected_kind == "unit" and not _fix27_has_any_unit_evidence(c):
                continue
                su = c.get("source_url") or ""
                return (conf_key, ctx_len, str(ctx_hash), _es_stable_sort_key(val), str(unit), str(su))
            cands_sorted = sorted(cands, key=_cand_key)
            out[ah] = cands_sorted[0] if cands_sorted else None
        return out
    except Exception:
        return {}

def end_state_validation_harness(baseline_analysis: dict, evolution_output: dict, min_stability: float = 99.9) -> dict:
    """
    PATCH ES9 (ADDITIVE): one-button end-state validation (warn-only helper)
    Use this to assert drift=0 on identical inputs.

    Returns a dict with pass/fail booleans and diagnostic fields.
    This does NOT mutate inputs.
    """
    report = {
        "passed": False,
        "checks": {},
        "notes": [],
    }
    try:
        base_prev = baseline_analysis or {}
        evo = evolution_output or {}

        # Snapshot hash
        base_snap = base_prev.get("source_snapshot_hash") or base_prev.get("results", {}).get("source_snapshot_hash")
        evo_snap = evo.get("source_snapshot_hash")

        # Universe + schema hashes
        base_uni = base_prev.get("canonical_universe_hash") or base_prev.get("results", {}).get("canonical_universe_hash")
        base_sch = base_prev.get("schema_hash") or base_prev.get("results", {}).get("schema_hash")
        evo_uni = evo.get("canonical_universe_hash")
        evo_sch = evo.get("schema_hash")

        report["checks"]["snapshot_hash_match"] = bool(base_snap and evo_snap and base_snap == evo_snap)
        report["checks"]["canonical_universe_hash_match"] = bool(base_uni and evo_uni and base_uni == evo_uni)
        report["checks"]["schema_hash_match"] = bool(base_sch and evo_sch and base_sch == evo_sch)

        # Stability threshold (warn-only semantics: "passed" includes match + stability)
        try:
            st = float(evo.get("stability_score") or 0.0)
        except Exception:
            st = 0.0
        report["checks"]["stability_meets_threshold"] = bool(st + 1e-9 >= float(min_stability))

        # Drift suspicion flag (if your pipeline sets it)
        report["checks"]["drift_suspected_flag_false"] = (evo.get("drift_suspected") is False)

        # Final pass condition
        report["passed"] = (
            report["checks"]["snapshot_hash_match"]
            and report["checks"]["canonical_universe_hash_match"]
            and report["checks"]["schema_hash_match"]
            and report["checks"]["stability_meets_threshold"]
        )

        if not report["passed"]:
            report["notes"].append("If hashes match but stability is low, inspect candidate tie-breaks and ordering.")
    except Exception:
        report["notes"].append("Validation harness encountered an exception (non-fatal).")
    return report
# =====================================================================

            # =========================


# =========================================================
# GOOGLE SHEETS HISTORY STORAGE
# =========================================================

SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive"
]
MAX_HISTORY_ITEMS = 50

@st.cache_resource
def get_google_sheet():
    """Connect to Google Sheet (cached connection)"""
    try:
        creds = Credentials.from_service_account_info(
            dict(st.secrets["gcp_service_account"]),
            scopes=SCOPES
        )
        client = gspread.authorize(creds)

        # ===================== PATCH GS1 (ADDITIVE): prefer explicit History worksheet =====================
        # Why:
        # - Your spreadsheet contains multiple tabs (e.g., "New Analysis", "History", "HistoryFull", "Snapshots")
        # - sheet1 is often NOT "History", so get_history() reads the wrong tab and sees "no analyses"
        # Behavior:
        # - Default worksheet_title = "History" (override via secrets: google_sheets.history_worksheet)
        # - Fallback to sheet1 only if the worksheet doesn't exist
        spreadsheet_name = (
            st.secrets.get("google_sheets", {}).get("spreadsheet_name", "Yureeka_JSON")
        )
        ss = client.open(spreadsheet_name)

        worksheet_title = st.secrets.get("google_sheets", {}).get("history_worksheet", "History")
        try:
            sheet = ss.worksheet(worksheet_title)
        except Exception:
            sheet = ss.sheet1
        # =================== END PATCH GS1 (ADDITIVE) ===================

        # Ensure headers exist - handle response object
        try:
            headers = sheet.row_values(1)
            if not headers or len(headers) == 0 or headers[0] != "id":
                # update() returns a response object in newer gspread - ignore it
                _ = sheet.update('A1:E1', [["id", "timestamp", "question", "confidence", "data"]])
        except gspread.exceptions.APIError:
            _ = sheet.update('A1:E1', [["id", "timestamp", "question", "confidence", "data"]])
        except Exception:
            pass  # Headers probably already exist

        return sheet

    except gspread.exceptions.SpreadsheetNotFound:
        st.error("❌ Spreadsheet not found. Create 'Yureeka_JSON' (or your configured name) and share with service account.")
        return None
    except Exception as e:
        error_str = str(e)
        # Ignore Response [200] - it's actually success
        if "Response [200]" in error_str:
            # This means the connection worked, try to return the sheet anyway
            try:
                creds = Credentials.from_service_account_info(
                    dict(st.secrets["gcp_service_account"]),
                    scopes=SCOPES
                )
                client = gspread.authorize(creds)

                # ===================== PATCH GS1b (ADDITIVE): same worksheet selection in fallback =====================
                spreadsheet_name = st.secrets.get("google_sheets", {}).get("spreadsheet_name", "Yureeka_JSON")
                ss = client.open(spreadsheet_name)
                worksheet_title = st.secrets.get("google_sheets", {}).get("history_worksheet", "History")
                try:
                    return ss.worksheet(worksheet_title)
                except Exception:
                    return ss.sheet1
                # =================== END PATCH GS1b (ADDITIVE) ===================
            except:
                pass
        st.error(f"❌ Failed to connect to Google Sheets: {e}")
        return None

def generate_analysis_id() -> str:
    """Generate unique ID for analysis"""
    return f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hashlib.md5(str(datetime.now().timestamp()).encode()).hexdigest()[:6]}"


# =====================================================================
# PATCH AI_A (ADDITIVE): emit metric_anchors in analysis payload (analysis-time)
# Why:
# - Evolution/diff are now anchor-driven; analysis must persist a deterministic
#   canonical_key -> anchor_hash mapping for drift=0 convergence.
# - Some UI/Sheets wrappers omit anchors unless explicitly emitted.
# Determinism:
# - Only uses existing evidence/candidates already present in the analysis payload.
# - No re-fetching; no heuristic matching.
# =====================================================================
def _emit_metric_anchors_in_analysis_payload(analysis_obj: dict) -> dict:
    try:
        if not isinstance(analysis_obj, dict):
            return analysis_obj

        # If already present and non-empty, keep as-is
        existing = analysis_obj.get("metric_anchors")
        if isinstance(existing, dict) and existing:
            return analysis_obj

        # Identify the primary response container (some payloads store it nested)
        pr = analysis_obj.get("primary_response") if isinstance(analysis_obj.get("primary_response"), dict) else None
        pr = pr or analysis_obj

        # Canonical metrics (preferred)
        pmc = pr.get("primary_metrics_canonical") if isinstance(pr, dict) else None
        if not isinstance(pmc, dict) or not pmc:
            pmc = analysis_obj.get("primary_metrics_canonical") if isinstance(analysis_obj.get("primary_metrics_canonical"), dict) else {}

        # Candidate lookup table from baseline snapshots (if present)
        bsc = None
        try:
            r = analysis_obj.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache"), list):
                bsc = r.get("baseline_sources_cache")
        except Exception:
            bsc = None
        if bsc is None and isinstance(analysis_obj.get("baseline_sources_cache"), list):
            bsc = analysis_obj.get("baseline_sources_cache")

        def _safe_str(x):
            try:
                return str(x).strip()
            except Exception:
                return ""

        # Build (anchor_hash -> best candidate) index deterministically
        anchor_to_candidate = {}
        cand_to_candidate = {}
        try:
            if isinstance(bsc, list):
                for sr in bsc:
                    if not isinstance(sr, dict):
                        continue
                    surl = sr.get("source_url") or sr.get("url")
                    for n in (sr.get("extracted_numbers") or []):
                        if not isinstance(n, dict):
                            continue
                        ah = _safe_str(n.get("anchor_hash"))
                        cid = _safe_str(n.get("candidate_id"))
                        if ah and ah not in anchor_to_candidate:
                            anchor_to_candidate[ah] = dict(n, source_url=n.get("source_url") or surl)
                        if cid and cid not in cand_to_candidate:
                            cand_to_candidate[cid] = dict(n, source_url=n.get("source_url") or surl)
        except Exception:
            pass

        metric_anchors = {}

        # Deterministic iteration for stable JSON output
        for ckey in sorted([str(k) for k in (pmc or {}).keys()]):
            m = pmc.get(ckey)
            if not isinstance(m, dict):
                continue

            # Pick anchor identifiers from evidence first (most authoritative)
            ev = m.get("evidence") or []
            if not isinstance(ev, list):
                ev = []

            best = None
            for e in ev:
                if not isinstance(e, dict):
                    continue
                ah = _safe_str(e.get("anchor_hash") or e.get("anchor"))
                cid = _safe_str(e.get("candidate_id"))
                if ah or cid:
                    best = e
                    break

            # Fallback: sometimes metric row carries anchor_hash directly
            if best is None:
                best = {
                    "anchor_hash": m.get("anchor_hash") or m.get("anchor"),
                    "candidate_id": m.get("candidate_id"),
                    "source_url": m.get("source_url") or m.get("url"),
                    "context_snippet": m.get("context_snippet") or m.get("context"),
                    "anchor_confidence": m.get("anchor_confidence"),
                }

            ah = _safe_str(best.get("anchor_hash") or best.get("anchor"))
            cid = _safe_str(best.get("candidate_id"))
            surl = best.get("source_url") or best.get("url")
            ctx = best.get("context_snippet") or best.get("context")
            aconf = best.get("anchor_confidence")

            # Enrich from candidate index if needed
            if (not surl) or (not ctx):
                cand = None
                if ah and ah in anchor_to_candidate:
                    cand = anchor_to_candidate.get(ah)
                elif cid and cid in cand_to_candidate:
                    cand = cand_to_candidate.get(cid)
                if isinstance(cand, dict):
                    surl = surl or (cand.get("source_url") or cand.get("url"))
                    ctx = ctx or (cand.get("context_snippet") or cand.get("context"))

            # Only emit if we actually have an anchor id
            if not (ah or cid):
                continue

            try:
                if isinstance(ctx, str):
                    ctx = ctx.strip()[:220]
                else:
                    ctx = None
            except Exception:
                ctx = None

            try:
                aconf = float(aconf) if aconf is not None else None
            except Exception:
                aconf = None

            metric_anchors[ckey] = {
                "canonical_key": ckey,
                "anchor_hash": ah or None,
                "candidate_id": cid or None,
                "source_url": surl or None,
                "context_snippet": ctx,
                "anchor_confidence": aconf,
            }

            # -----------------------------------------------------------------
            # PATCH AI_B (ADDITIVE): also backfill anchor fields onto the metric row
            # -----------------------------------------------------------------
            try:
                if ah and not _safe_str(m.get("anchor_hash")):
                    m["anchor_hash"] = ah
                if cid and not _safe_str(m.get("candidate_id")):
                    m["candidate_id"] = cid
                if surl and not (m.get("source_url") or m.get("url")):
                    m["source_url"] = surl
                if ctx and not (m.get("context_snippet") or m.get("context")):
                    m["context_snippet"] = ctx
                if aconf is not None and m.get("anchor_confidence") is None:
                    m["anchor_confidence"] = aconf
            except Exception:
                pass
            # -----------------------------------------------------------------

        if metric_anchors:
            # -----------------------------------------------------------------
            # PATCH AI_C (ADDITIVE): persist in all common locations
            # -----------------------------------------------------------------
            try:
                analysis_obj["metric_anchors"] = metric_anchors
            except Exception:
                pass
            try:
                if isinstance(pr, dict):
                    pr.setdefault("metric_anchors", metric_anchors)
            except Exception:
                pass
            try:
                analysis_obj.setdefault("results", {})
                if isinstance(analysis_obj["results"], dict):
                    analysis_obj["results"].setdefault("metric_anchors", metric_anchors)
            except Exception:
                pass
            # -----------------------------------------------------------------

        return analysis_obj
    except Exception:
        return analysis_obj
# =====================================================================

def add_to_history(analysis: dict) -> bool:
    """
    Save analysis to Google Sheet (or session fallback).

    ADDITIVE end-state wiring:
      - If a baseline source cache exists, build & store:
          * evidence_records (structured, cached)
          * metric_anchors (baseline metrics anchored to evidence)
      - Prevent Google Sheets 50,000-char single-cell limit errors by shrinking only
        the JSON payload written into the single "analysis json" cell when necessary.

    Backward compatible:
      - Only adds keys; does not remove existing fields.
      - Never blocks saving if enrichment fails.
      - If Sheets unavailable, falls back to session_state.
    """

    # =====================================================================
    # PATCH AI_A_CALL (ADDITIVE): ensure metric_anchors emitted before persistence
    # =====================================================================
    try:
        analysis = _emit_metric_anchors_in_analysis_payload(analysis)
    except Exception:
        pass
    # =====================================================================

    import json
    import re
    import streamlit as st
    from datetime import datetime

    SHEETS_CELL_LIMIT = 50000

    # -----------------------
    # PATCH A1 (ADDITIVE): robustly locate baseline_sources_cache
    # - Added primary_response.baseline_sources_cache as extra fallback
    # -----------------------
    baseline_cache = (
        analysis.get("baseline_sources_cache")
        or (analysis.get("primary_response", {}) or {}).get("baseline_sources_cache")
        or (analysis.get("results", {}) or {}).get("baseline_sources_cache")
        or (analysis.get("results", {}) or {}).get("source_results")
    )

    # -----------------------
    # PATCH A2 (ADDITIVE): build evidence_records deterministically
    # -----------------------
    def _build_evidence_records_from_baseline_cache(baseline_cache_obj):
        records = []
        if not isinstance(baseline_cache_obj, list):
            return records

        # helper: safe sha1 fallback if needed
        def _sha1(s: str) -> str:
            try:
                import hashlib
                return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()
            except Exception:
                return ""

        for sr in baseline_cache_obj:
            if not isinstance(sr, dict):
                continue
            url = sr.get("url") or ""
            fp = sr.get("fingerprint")
            fetched_at = sr.get("fetched_at")

            nums = sr.get("extracted_numbers") or []
            clean_nums = []

            if isinstance(nums, list):
                for n in nums:
                    if not isinstance(n, dict):
                        continue

                    # optional canonicalization hook
                    try:
                        fn = globals().get("canonicalize_numeric_candidate")
                        if callable(fn):
                            n = fn(dict(n))
                    except Exception:
                        n = dict(n)

                    raw = (n.get("raw") or "").strip()
                    ctx = (n.get("context_snippet") or n.get("context") or "").strip()
                    anchor_hash = n.get("anchor_hash") or _sha1(f"{url}|{raw}|{ctx[:240]}")

                    clean_nums.append({
                        "value": n.get("value"),
                        "unit": n.get("unit"),
                        "unit_tag": n.get("unit_tag"),
                        "unit_family": n.get("unit_family"),
                        "base_unit": n.get("base_unit"),
                        "multiplier_to_base": n.get("multiplier_to_base"),
                        "value_norm": n.get("value_norm"),

                        "raw": raw,
                        "context_snippet": ctx[:240],
                        "anchor_hash": anchor_hash,
                "candidate_id": hashlib.sha1(str(anchor_hash or "").encode("utf-8")).hexdigest()[:16] if anchor_hash else None,

            # =====================================================================
            # PATCH AI2 (ADDITIVE): anchor integrity fields
            # - candidate_id is a stable short id derived from anchor_hash
            # - anchor_basis documents what the anchor_hash was built from
            # =====================================================================
            "candidate_id": (str(anchor_hash)[:16] if anchor_hash else None),
            "anchor_basis": "url|raw|context",
            # =====================================================================
                        "source_url": n.get("source_url") or url,

                        "start_idx": n.get("start_idx"),
                        "end_idx": n.get("end_idx"),

                        "is_junk": bool(n.get("is_junk")) if isinstance(n.get("is_junk"), bool) else False,
                        "junk_reason": n.get("junk_reason") or "",

                        "measure_kind": n.get("measure_kind"),
                        "measure_assoc": n.get("measure_assoc"),
                    })

            # stable ordering (prefer your helper if present)
            try:
                if "sort_snapshot_numbers" in globals() and callable(globals()["sort_snapshot_numbers"]):
                    clean_nums = sort_snapshot_numbers(clean_nums)
                else:
                    clean_nums = sorted(
                        clean_nums,
                        key=lambda x: (str(x.get("anchor_hash") or ""), str(x.get("raw") or ""))
                    )
            except Exception:
                pass

            records.append({
                "url": url,
                "fetched_at": fetched_at,
                "fingerprint": fp,
                "numbers": clean_nums,
            })

        # stable ordering (prefer helper if present)
        try:
            if "sort_evidence_records" in globals() and callable(globals()["sort_evidence_records"]):
                records = sort_evidence_records(records)
            else:
                records = sorted(records, key=lambda r: str(r.get("url") or ""))
        except Exception:
            pass

        return records

    # -----------------------
    # PATCH A3 (ADDITIVE): build metric_anchors deterministically (schema-first if present)
    # -----------------------
    def _build_metric_anchors(primary_metrics_canonical, evidence_records):
        """
        Build a deterministic metric_anchors mapping for drift=0.

        PATCH AI1 (ADDITIVE): Anchor integrity
        - Prefer the anchor_hash/candidate_id already chosen during analysis (metric["evidence"]).
        - Fall back to scanning evidence_records for a candidate with the same anchor_hash/candidate_id.
        - As a last resort, pick a best candidate deterministically by (abs(value_norm-target), context length).
        - NEVER invent anchors; if no usable candidate, omit the anchor for that metric.
        """
        anchors = {}
        if not isinstance(primary_metrics_canonical, dict) or not primary_metrics_canonical:
            return anchors
        if not isinstance(evidence_records, list):
            evidence_records = []

        import hashlib

        def _sha1(s: str) -> str:
            try:
                return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()
            except Exception:
                return ""

        def _ensure_anchor_fields(c: dict, source_url: str = "") -> dict:
            c = c if isinstance(c, dict) else {}
            # context snippet normalization
            ctx = c.get("context_snippet") or c.get("context") or ""
            if isinstance(ctx, str):
                ctx = ctx.strip()[:240]
            else:
                ctx = ""
            raw = c.get("raw")
            if raw is None:
                # stable raw representation
                v = c.get("value_norm") if c.get("value_norm") is not None else c.get("value")
                u = c.get("base_unit") or c.get("unit") or ""
                raw = f"{v}{u}"
            raw = str(raw)[:120]

            ah = c.get("anchor_hash") or c.get("anchor") or ""
            if not ah:
                ah = _sha1(f"{source_url}|{raw}|{ctx}")
                if ah:
                    c["anchor_hash"] = ah

            if not c.get("candidate_id") and ah:
                c["candidate_id"] = str(ah)[:16]

            # keep normalized ctx/source_url for downstream
            if source_url and not c.get("source_url"):
                c["source_url"] = source_url
            if ctx and not c.get("context_snippet"):
                c["context_snippet"] = ctx

            return c

        # Pre-index evidence_records by (anchor_hash, candidate_id)
        anchor_index = {}
        candidate_index = {}
        value_index = {}  # ckey -> list of candidates (for fallback)

        for rec in evidence_records:
            if not isinstance(rec, dict):
                continue
            url = rec.get("source_url") or rec.get("url") or ""
            for c in (rec.get("candidates") or rec.get("extracted_numbers") or []):
                if not isinstance(c, dict):
                    continue
                c = _ensure_anchor_fields(c, url)
                ah = c.get("anchor_hash")
                cid = c.get("candidate_id")
                if ah and ah not in anchor_index:
                    anchor_index[ah] = c
                if cid and cid not in candidate_index:
                    candidate_index[cid] = c

        # Determine anchors per metric
        for ckey, m in primary_metrics_canonical.items():
            if not isinstance(m, dict):
                continue

            # --- Preferred: use the analysis-chosen evidence (integrity) ---
            chosen = None
            ev = m.get("evidence") or []
            if isinstance(ev, list) and ev:
                # pick first usable evidence deterministically
                for e in ev:
                    if not isinstance(e, dict):
                        continue
                    url = e.get("source_url") or e.get("url") or ""
                    e2 = _ensure_anchor_fields(dict(e), url)
                    ah = e2.get("anchor_hash")
                    cid = e2.get("candidate_id")
                    if ah or cid:
                        chosen = e2
                        break

            # --- Fallback 1: resolve by anchor_hash/candidate_id in evidence_records ---
            if isinstance(chosen, dict):
                ah = chosen.get("anchor_hash")
                cid = chosen.get("candidate_id")
                if ah and ah in anchor_index:
                    chosen = dict(anchor_index[ah])
                elif cid and cid in candidate_index:
                    chosen = dict(candidate_index[cid])

            # --- Fallback 2: deterministic best-by-value in the same source_url (if any) ---
            if not isinstance(chosen, dict) or not (chosen.get("anchor_hash") or chosen.get("candidate_id")):
                # gather candidates from evidence_records that match the metric's preferred source_url (if known)
                preferred_url = ""
                try:
                    if isinstance(ev, list) and ev:
                        preferred_url = str((ev[0] or {}).get("source_url") or (ev[0] or {}).get("url") or "")
                except Exception:
                    preferred_url = ""

                target = m.get("value_norm")
                try:
                    target = float(target) if target is not None else None
                except Exception:
                    target = None

                pool = []
                for rec in evidence_records:
                    if not isinstance(rec, dict):
                        continue
                    url = str(rec.get("source_url") or rec.get("url") or "")
                    if preferred_url and url != preferred_url:
                        continue
                    for c in (rec.get("candidates") or rec.get("extracted_numbers") or []):
                        if not isinstance(c, dict):
                            continue
                        cc = _ensure_anchor_fields(dict(c), url)
                        pool.append(cc)

                if pool:
                    def _score(cc):
                        ctx = cc.get("context_snippet") or ""
                        try:
                            v = cc.get("value_norm")
                            v = float(v) if v is not None else None
                        except Exception:
                            v = None
                        dv = abs(v - target) if (v is not None and target is not None) else 1e30
                        return (dv, -len(str(ctx)), str(cc.get("anchor_hash") or ""), str(url))
                    pool.sort(key=_score)
                    chosen = pool[0]

            if not isinstance(chosen, dict):
                continue

            # emit anchor record (stable shape)
            anchors[ckey] = {
                "canonical_key": ckey,
                "anchor_hash": chosen.get("anchor_hash"),
            # =====================================================================
# PATCH AI3 (ADDITIVE): anchor integrity fingerprint (analysis-time)
# Why:
# - Provides a stable, inspectable signature tying the anchor to a specific
#   candidate (url + anchor_hash + value_norm + base_unit).
# - Helps detect silent anchor drift across analysis/evolution.
# =====================================================================
"anchor_integrity": {
    "candidate_id": chosen.get("candidate_id"),
    "value_norm": chosen.get("value_norm"),
    "base_unit": chosen.get("base_unit") or chosen.get("unit"),
    "fingerprint": chosen.get("fingerprint"),
    "integrity_hash": _es_hash_text(
        f"{ckey}|{chosen.get('anchor_hash')}|{chosen.get('source_url') or chosen.get('url') or ''}|{chosen.get('value_norm')}|{chosen.get('base_unit') or chosen.get('unit') or ''}"
    ) if callable(globals().get("_es_hash_text")) else None,
},
# =====================================================================
                "candidate_id": chosen.get("candidate_id"),
                "source_url": chosen.get("source_url") or chosen.get("url"),
                "context_snippet": chosen.get("context_snippet") or chosen.get("context"),
                "anchor_confidence": chosen.get("anchor_confidence") or chosen.get("confidence"),
            }

        # deterministic ordering (stable JSON)
        try:
            anchors = dict(sorted(anchors.items(), key=lambda kv: str(kv[0])))
        except Exception:
            pass

        return anchors
        def _tokenize(s: str):
            return [t for t in re.findall(r"[a-z0-9]+", (s or "").lower()) if len(t) > 2]

        # PATCH A3.1 (ADDITIVE): tiny float helper for deterministic closeness scoring
        def _to_float(x):
            try:
                return float(x)
            except Exception:
                return None

        # PATCH A3.9 (ADDITIVE): currency evidence helper
        # - Needed because many currency metrics appear as magnitude-tagged numbers (e.g., "40.7M")
        #   with currency implied in nearby context ("USD", "revenue", "$", etc.)
        def _has_currency_evidence(raw: str, ctx: str) -> bool:
            r = (raw or "")
            c = (ctx or "").lower()
            if any(s in r for s in ["$", "S$", "€", "£"]):
                return True
            if any(code in c for code in [" usd", "sgd", " eur", " gbp", " aud", " cad", " jpy", " cny", " rmb"]):
                return True
            strong_kw = [
                "revenue", "turnover", "valuation", "valued at", "market value", "market size",
                "sales value", "net profit", "operating profit", "gross profit",
                "ebitda", "earnings", "income", "capex", "opex"
            ]
            if any(k in c for k in strong_kw):
                return True
            return False
        # =========================

        # flatten candidates
        all_nums = []
        for rec in evidence_records:
            if not isinstance(rec, dict):
                continue
            for n in (rec.get("numbers") or []):
                if isinstance(n, dict):
                    all_nums.append(n)

        # PATCH A3.2 (ADDITIVE): normalize_unit_tag + unit_family hooks (if present)
        _norm_tag_fn = globals().get("normalize_unit_tag")
        _unit_family_fn = globals().get("unit_family")

        for ckey, m in primary_metrics_canonical.items():
            if not isinstance(m, dict):
                continue

            schema = (metric_schema_frozen or {}).get(ckey) if isinstance(metric_schema_frozen, dict) else None
            expected_family = (schema.get("unit_family") or "").lower().strip() if isinstance(schema, dict) else ""
            expected_unit = (schema.get("unit") or "").strip() if isinstance(schema, dict) else ""
            expected_dim = (schema.get("dimension") or "").lower().strip() if isinstance(schema, dict) else ""

            # tokens: schema keywords + metric name tokens
            toks = []
            if isinstance(schema, dict):
                for k in (schema.get("keywords") or []):
                    toks.extend(_tokenize(str(k)))
            toks.extend(_tokenize(m.get("name") or m.get("original_name") or ""))
            toks = list(dict.fromkeys(toks))[:40]

            best = None
            best_key = None

            # PATCH A3.3 (ADDITIVE): metric value reference for closeness bonus
            m_val = _to_float(m.get("value_norm") if m.get("value_norm") is not None else m.get("value"))

            # PATCH A3.4 (ADDITIVE): normalized expected tag (schema unit may be "M", "%", etc.)
            exp_tag = expected_unit
            try:
                if callable(_norm_tag_fn):
                    exp_tag = _norm_tag_fn(expected_unit)
            except Exception:
                pass

            # PATCH A3.10 (ADDITIVE): metric unit_tag (if available) to gate closeness bonus
            m_tag = (m.get("unit_tag") or "").strip()

            for cand in all_nums:
                if cand.get("is_junk") is True:
                    continue

                ctx = cand.get("context_snippet") or ""
                c_ut = (cand.get("unit_tag") or "").strip()
                c_fam = (cand.get("unit_family") or "").lower().strip()

                # =========================
                # PATCH A3.5 (ADDITIVE): derive candidate family if missing
                # - prevents leakage when unit_family wasn't populated upstream
                # =========================
                if not c_fam:
                    try:
                        if callable(_unit_family_fn):
                            c_fam = str(_unit_family_fn(c_ut or "") or "").lower().strip()
                    except Exception:
                        pass
                # =========================

                # =========================
                # PATCH A3.7 (ADDITIVE): prefer unit_tag matching (normalized) over raw unit matching
                # PATCH A3.11 (ADDITIVE): extend normalization fallback to raw/context
                # - helps older snapshots where unit_tag/unit may be empty but raw/context carries scale ("million", "%")
                # =========================
                cand_tag = c_ut
                try:
                    if callable(_norm_tag_fn):
                        cand_tag = _norm_tag_fn(c_ut or cand.get("unit") or cand.get("raw") or ctx)
                except Exception:
                    pass
                # =========================

                # =========================
                # PATCH A3.6 (FIX): schema-first family gate with currency exception
                # - Currency metrics often appear as magnitude candidates ("40.7M") + currency evidence in context.
                # - We allow cand_fam == "magnitude" for expected_family == "currency" ONLY when currency evidence exists.
                # =========================
                if expected_family in ("percent", "currency", "magnitude", "energy"):
                    if expected_family == "currency":
                        if c_fam not in ("currency", "magnitude"):
                            continue
                        if c_fam == "magnitude" and not _has_currency_evidence(cand.get("raw", ""), ctx):
                            continue
                    else:
                        if (c_fam or "") != expected_family:
                            continue
                # =========================

                # dimension/meaning gate using measure_kind when present (soft but helpful)
                mk = cand.get("measure_kind")
                if expected_dim == "percent" and mk and mk not in ("share_pct", "growth_pct", "percent_other"):
                    continue
                if expected_dim == "currency" and mk and mk == "count_units":
                    continue

                c_tokens = set(_tokenize(ctx))
                overlap = sum(1 for t in toks if t in c_tokens) if toks else 0
                score = overlap / max(1, len(toks))

                bonus = 0.0

                # =========================
                # PATCH A3.7 (ADDITIVE): tag-based unit bonus (stronger)
                # =========================
                if exp_tag and cand_tag and cand_tag == exp_tag:
                    bonus += 0.07
                # keep a small legacy bonus if exact unit string matches too
                if expected_unit and (str(cand.get("unit") or "").strip() == expected_unit):
                    bonus += 0.03
                # =========================

                # =========================
                # PATCH A3.8 (ADDITIVE): deterministic value closeness bonus (guarded)
                # - Only apply when units are comparable (tag match or both use value_norm).
                # - Prevents misleading closeness when one side is normalized and the other isn't.
                # =========================
                c_val = _to_float(cand.get("value_norm") if cand.get("value_norm") is not None else cand.get("value"))
                comparable = False
                if m_tag and cand_tag and m_tag == cand_tag:
                    comparable = True
                elif (m.get("value_norm") is not None) and (cand.get("value_norm") is not None):
                    comparable = True

                if comparable and m_val is not None and c_val is not None:
                    denom = max(1e-9, abs(m_val))
                    rel_err = abs(c_val - m_val) / denom
                    if rel_err <= 0.02:
                        bonus += 0.06
                    elif rel_err <= 0.10:
                        bonus += 0.03
                # =========================

                score = float(score + bonus)

                # stable tie-breaker
                key = (
                    score,
                    str(cand.get("source_url") or ""),
                    str(cand.get("anchor_hash") or ""),
                    str(cand.get("raw") or ""),
                )

                if best_key is None or key > best_key:
                    best_key = key
                    best = cand

            if best and best_key and best_key[0] >= 0.10:
                anchors[ckey] = {
                    # =========================
                    # PATCH MA1 (ADDITIVE): legacy compat fields
                    # =========================
                    "metric_id": ckey,
                    "metric_name": (m.get("name") or m.get("original_name") or ckey),
                    # =========================

                    "canonical_key": ckey,
                    "anchor_hash": best.get("anchor_hash"),
                    "source_url": best.get("source_url"),
                    "raw": best.get("raw"),
                    "unit": best.get("unit"),
                    "unit_tag": best.get("unit_tag"),
                    "unit_family": best.get("unit_family"),
                    "base_unit": best.get("base_unit"),
                    "value": best.get("value"),
                    "value_norm": (best.get("value") if best.get("value") is not None else best.get("value_norm")),
                    "measure_kind": best.get("measure_kind"),
                    "measure_assoc": best.get("measure_assoc"),
                    "context_snippet": (best.get("context_snippet") or "")[:220],
                    "anchor_confidence": float(min(100.0, best_key[0] * 100.0)),

                    # =========================
                    # PATCH A3.12 (ADDITIVE): optional fingerprint passthrough (if present)
                    # - Useful later for evolution/debugging; harmless if missing.
                    # =========================
                    "fingerprint": best.get("fingerprint"),
                    # =========================
                }
            else:
                anchors[ckey] = {
                    # =========================
                    # PATCH MA1 (ADDITIVE): legacy compat fields
                    # =========================
                    "metric_id": ckey,
                    "metric_name": (m.get("name") or m.get("original_name") or ckey),
                    # =========================

                    "canonical_key": ckey,
                    "anchor_hash": None,
                    "source_url": None,
                    "raw": None,
                    "anchor_confidence": 0.0,

                    # PATCH A3.12 (ADDITIVE): keep key present for stable shape
                    "fingerprint": None,
                }

        # stable ordering (prefer helper if present)
        try:
            if "sort_metric_anchors" in globals() and callable(globals()["sort_metric_anchors"]):
                ordered = sort_metric_anchors(list(anchors.values()))
                anchors = {
                    a.get("canonical_key"): a
                    for a in ordered
                    if isinstance(a, dict) and a.get("canonical_key")
                }
        except Exception:
            pass

        # =====================================================================
        # PATCH AI_ANCHHASH1 (ADDITIVE): propagate anchor_hash into metric rows
        # Why:
        # - Drift=0 requires prev metrics to carry anchor_hash so diff can compare
        #   prev_anchor_hash vs cur_anchor_hash deterministically.
        # - We ONLY copy existing anchor_hash from anchors map; no fabrication.
        # =====================================================================
        try:
            if isinstance(primary_metrics_canonical, dict) and isinstance(anchors, dict):
                for _ck, _a in anchors.items():
                    if not isinstance(_a, dict):
                        continue
                    _ah = _a.get("anchor_hash") or _a.get("anchor")
                    if not _ah:
                        continue
                    _mrow = primary_metrics_canonical.get(_ck)
                    if isinstance(_mrow, dict) and not _mrow.get("anchor_hash"):
                        _mrow["anchor_hash"] = _ah
        except Exception:
            pass
        # =====================================================================
        return anchors

    # -----------------------
    # PATCH A4 (ADDITIVE): enrich analysis (never block saving)
    # -----------------------
    try:
        if isinstance(baseline_cache, list) and baseline_cache:
            evidence_records = _build_evidence_records_from_baseline_cache(baseline_cache)

            # =========================
            # PATCH A4.1 (ADDITIVE): evidence layer versioning (pipeline attribution)
            # - Use CODE_VERSION if available; else keep numeric fallback
            # =========================
            try:
                cv = globals().get("CODE_VERSION")
                analysis.setdefault("evidence_layer_version", cv or 1)
            except Exception:
                analysis.setdefault("evidence_layer_version", 1)
            analysis.setdefault("evidence_layer_schema_version", 1)
            # =========================

            # stash on analysis (additive)
            analysis["evidence_records"] = evidence_records

            # build anchors using canonical metrics + frozen schema if present
            primary_resp = analysis.get("primary_response") or {}
            if isinstance(primary_resp, dict):
                pmc = primary_resp.get("primary_metrics_canonical") or analysis.get("primary_metrics_canonical") or {}
                schema = primary_resp.get("metric_schema_frozen") or analysis.get("metric_schema_frozen") or {}
            else:
                pmc = analysis.get("primary_metrics_canonical") or {}
                schema = analysis.get("metric_schema_frozen") or {}

            metric_anchors = _build_metric_anchors(pmc, schema, evidence_records)
            # =====================================================================
            # PATCH ANCH_EMIT1 (ADDITIVE): emit metric_anchors into analysis payload
            # Why:
            # - Evolution (and diff) expects anchors to be discoverable without guessing.
            # - Some storage paths wrap/summarize analysis objects; we persist anchors
            #   at multiple stable locations to survive those wrappers.
            # Determinism:
            # - Anchors are derived only from existing evidence_records / schema / pmc.
            # - No re-fetching; no heuristic matching.
            # =====================================================================
            try:
                if isinstance(metric_anchors, dict) and metric_anchors:
                    # Top-level (preferred)
                    if not isinstance(analysis.get("metric_anchors"), dict):
                        analysis["metric_anchors"] = metric_anchors

                    # Under primary_response (common for older shapes)
                    pr = analysis.get("primary_response")
                    if isinstance(pr, dict) and not isinstance(pr.get("metric_anchors"), dict):
                        pr["metric_anchors"] = metric_anchors

                    # Under results (some evolution lookups)
                    res = analysis.get("results")
                    if isinstance(res, dict) and not isinstance(res.get("metric_anchors"), dict):
                        res["metric_anchors"] = metric_anchors

                    # Lightweight debug hint for wrappers
                    try:
                        dbg = analysis.get("debug")
                        if not isinstance(dbg, dict):
                            dbg = {}
                            analysis["debug"] = dbg
                        dbg.setdefault("metric_anchor_count", int(len(metric_anchors)))
                    except Exception:
                        pass
            except Exception:
                pass
            # =====================================================================

            analysis["metric_anchors"] = metric_anchors
            # =====================================================================
            # =====================================================================
            # PATCH AI4 (ADDITIVE): anchor integrity audit (analysis-time)
            # Why:
            # - Detect duplicate anchor_hash values across canonical keys.
            # - Detect missing anchor_hash on anchors and on baseline canonical metrics.
            # - Provide non-breaking debug/audit fields for drift investigations.
            # Notes:
            # - Purely additive; does not mutate anchors beyond attaching audit metadata.
            # =====================================================================
            try:
                if isinstance(analysis, dict):
                    _ma = analysis.get('metric_anchors')
                    _pmc = analysis.get('primary_metrics_canonical')
                    _dup = {}  # anchor_hash -> [canonical_key,...]
                    _missing_anchor = []
                    _missing_metric_anchor = []
                    if isinstance(_ma, dict):
                        for _ck, _a in _ma.items():
                            if not isinstance(_a, dict):
                                _missing_anchor.append(str(_ck))
                                continue
                            _ah = _a.get('anchor_hash') or _a.get('anchor')
                            if not _ah:
                                _missing_anchor.append(str(_ck))
                                continue
                            _ah = str(_ah)
                            _dup.setdefault(_ah, []).append(str(_ck))
                    # anchors missing on baseline metrics (best-effort diagnostic)
                    if isinstance(_pmc, dict):
                        for _ck, _m in _pmc.items():
                            if not isinstance(_m, dict):
                                continue
                            if not (_m.get('anchor_hash') or _m.get('anchor') or _m.get('candidate_id')):
                                _missing_metric_anchor.append(str(_ck))
                    _dup_only = {k: v for k, v in _dup.items() if isinstance(v, list) and len(v) > 1}
                    analysis['anchor_integrity_audit'] = {
                        'anchor_count': int(len(_ma)) if isinstance(_ma, dict) else 0,
                        'duplicate_anchor_hash_count': int(len(_dup_only)),
                        'duplicate_anchor_hash_examples': dict(list(_dup_only.items())[:10]) if _dup_only else {},
                        'missing_anchor_hash_count': int(len(_missing_anchor)),
                        'missing_anchor_hash_examples': _missing_anchor[:20],
                        'metrics_missing_any_anchor_id_count': int(len(_missing_metric_anchor)),
                        'metrics_missing_any_anchor_id_examples': _missing_metric_anchor[:20],
                    }
            except Exception:
                pass
            # =====================================================================

    # -----------------------
    # Existing Google Sheet save behavior (guarded)
    # -----------------------
    except Exception:
        pass

    # =====================================================================
    # PATCH D (ADDITIVE): propagate metric_anchors onto metric rows + evidence
    # Why:
    # - Drift=0 depends on analysis and evolution sharing the SAME anchor IDs.
    # - Some downstream code paths expect anchor_hash on the metric row itself
    #   and/or inside evidence entries (not only in analysis["metric_anchors"]).
    # - This patch copies existing anchor metadata only (no fabrication, no refetch).
    # =====================================================================
    try:
        import re
        import hashlib

        def _norm_ctx(s: str) -> str:
            try:
                return re.sub(r"\s+", " ", (s or "").strip())
            except Exception:
                return (s or "").strip()

        def _compute_anchor_hash_fallback(url: str, ctx: str) -> str:
            try:
                u = (url or "").strip()
                c = _norm_ctx(ctx or "")
                if not u or not c:
                    return ""
                return hashlib.sha1((u + "||" + c).encode("utf-8")).hexdigest()[:16]
            except Exception:
                return ""

        def _compute_anchor_hash(url: str, ctx: str) -> str:
            try:
                fn = globals().get("compute_anchor_hash")
                if callable(fn):
                    return str(fn(url, ctx) or "")
            except Exception:
                pass
            return _compute_anchor_hash_fallback(url, ctx)

        # Locate canonical metrics dict (prefer primary_response)
        _pmc = None
        _pr0 = analysis.get("primary_response") if isinstance(analysis, dict) else None
        if isinstance(_pr0, dict) and isinstance(_pr0.get("primary_metrics_canonical"), dict):
            _pmc = _pr0.get("primary_metrics_canonical")
        if _pmc is None and isinstance(analysis, dict) and isinstance(analysis.get("primary_metrics_canonical"), dict):
            _pmc = analysis.get("primary_metrics_canonical")

        if isinstance(metric_anchors, dict) and isinstance(_pmc, dict):
            for _ckey, _a in metric_anchors.items():
                if not isinstance(_ckey, str) or not _ckey:
                    continue
                if not isinstance(_a, dict) or not _a:
                    continue

                _m = _pmc.get(_ckey)
                if not isinstance(_m, dict):
                    continue

                _ah = str(_a.get("anchor_hash") or _a.get("anchor") or "").strip()
                _src = str(_a.get("source_url") or _a.get("url") or "").strip()
                _ctx = _a.get("context_snippet") or _a.get("context") or ""
                _ctx = _ctx.strip() if isinstance(_ctx, str) else ""

                # Copy onto metric row (only if missing)
                if _ah and not _m.get("anchor_hash"):
                    _m["anchor_hash"] = _ah
                if _src and not (_m.get("source_url") or _m.get("url")):
                    _m["source_url"] = _src
                if _ctx and not (_m.get("context_snippet") or _m.get("context")):
                    _m["context_snippet"] = _ctx[:220]

                # Pass through extra metadata if present (additive)
                if _a.get("anchor_confidence") is not None and _m.get("anchor_confidence") is None:
                    _m["anchor_confidence"] = _a.get("anchor_confidence")
                if _a.get("candidate_id") and not _m.get("candidate_id"):
                    _m["candidate_id"] = _a.get("candidate_id")
                if _a.get("fingerprint") and not _m.get("fingerprint"):
                    _m["fingerprint"] = _a.get("fingerprint")

                # Ensure evidence entries carry anchor_hash (deterministic; no new evidence)
                _ev = _m.get("evidence")
                if isinstance(_ev, list) and _ev:
                    for _e in _ev:
                        if not isinstance(_e, dict):
                            continue
                        if _e.get("anchor_hash"):
                            continue
                        _e_url = str(_e.get("url") or _e.get("source_url") or _src or "").strip()
                        _e_ctx = _e.get("context_snippet") or _e.get("context") or _ctx or ""
                        _e_ctx = _e_ctx.strip() if isinstance(_e_ctx, str) else ""
                        _eh = _compute_anchor_hash(_e_url, _e_ctx)
                        if _eh:
                            _e["anchor_hash"] = _eh
    except Exception:
        pass
    # =====================================================================


    def _try_make_sheet_json(obj: dict) -> str:
        try:
            fn = globals().get("make_sheet_safe_json")
            if callable(fn):
                return fn(obj)
        except Exception:
            pass
        return json.dumps(obj, ensure_ascii=False, default=str)

    def _shrink_for_sheets(original: dict) -> dict:
        base_copy = dict(original)
        s = _try_make_sheet_json(base_copy)
        if isinstance(s, str) and len(s) <= SHEETS_CELL_LIMIT:
            return base_copy

        reduced = dict(base_copy)
        removed = []

        for k in [
            "evidence_records",
            "baseline_sources_cache",
            "metric_anchors",
            "source_results",
            "web_context",
            "scraped_meta",
            "raw_sources",
            "raw_text",
            "debug",
        ]:
            if k in reduced:
                reduced.pop(k, None)
                removed.append(k)

        reduced.setdefault("_sheet_write", {})
        if isinstance(reduced["_sheet_write"], dict):
            reduced["_sheet_write"]["truncated"] = True
            reduced["_sheet_write"]["removed_keys"] = removed[:50]

        s2 = _try_make_sheet_json(reduced)
        if isinstance(s2, str) and len(s2) <= SHEETS_CELL_LIMIT:
            return reduced

        return {
            "question": original.get("question"),
            "timestamp": original.get("timestamp"),
            "final_confidence": original.get("final_confidence"),
            "question_profile": original.get("question_profile"),
            "primary_response": original.get("primary_response") or {},
            "_sheet_write": {
                "truncated": True,
                "mode": "minimal_fallback",
                "note": "Full analysis too large for single Google Sheets cell (50k limit).",
            },
        }

    # Try Sheets
    try:
        sheet = get_google_sheet()
    except Exception:
        sheet = None

    if not sheet:
        if "analysis_history" not in st.session_state:
            st.session_state.analysis_history = []
        st.session_state.analysis_history.append(analysis)
        try:
            st.session_state["last_analysis"] = analysis
        except Exception:
            pass
        return False

    try:
        analysis_id = generate_analysis_id()


        # =====================================================================
        # PATCH ES1F (ADDITIVE): persist full snapshots + pointer for Sheets rows
        # - If full baseline_sources_cache exists (list-shaped), store it outside
        #   Sheets keyed by source_snapshot_hash, and attach pointer fields into
        #   analysis/results for deterministic evolution rehydration.
        # - Pure enrichment only (no refetch, no heuristics).
        # =====================================================================
        try:
            _bsc = None
            if isinstance(analysis, dict):
                _bsc = analysis.get("results", {}).get("baseline_sources_cache") or analysis.get("baseline_sources_cache")


            # =================================================================
            # PATCH SS6B (ADDITIVE): if snapshots were already summarized away,
            # rebuild minimal snapshot shape from evidence_records (deterministic).
            # This enables snapshot persistence even when baseline_sources_cache
            # is a summary dict in the main analysis object.
            # =================================================================
            try:
                if (not isinstance(_bsc, list)) and isinstance(analysis, dict):
                    _er = None
                    # prefer nested results evidence_records first
                    if isinstance(analysis.get("results"), dict):
                        _er = analysis["results"].get("evidence_records")
                    if _er is None:
                        _er = analysis.get("evidence_records")
                    _rebuilt = build_baseline_sources_cache_from_evidence_records(_er)
                    if isinstance(_rebuilt, list) and _rebuilt:
                        _bsc = _rebuilt
            except Exception:
                pass
            # =================================================================

            if isinstance(_bsc, list) and _bsc:
                _ssh = compute_source_snapshot_hash(_bsc)

                # =========================
                # PATCH A2 (ADD): also compute snapshot hash v2 for stronger identity
                # =========================
                _ssh_v2 = None
                try:
                    _ssh_v2 = compute_source_snapshot_hash_v2(_bsc)
                except Exception:
                    _ssh_v2 = None
                if _ssh:
                    # =============================================================
                    # PATCH SS4 (ADDITIVE): store snapshots to Snapshots worksheet when possible
                    # - Persists full baseline_sources_cache in a dedicated worksheet tab.
                    # - Falls back to local snapshot_store file if Sheets snapshot store unavailable.
                    # - Pointer ref stored as 'gsheet:Snapshots:<hash>' when successful.
                    # =============================================================
                    _gs_ref = ""
                    try:
                        _gs_ref = store_full_snapshots_to_sheet(_bsc, _ssh, worksheet_title="Snapshots")
                        # =========================
                        # PATCH A3 (ADD): mirror-write snapshots under v2 hash as well
                        # =========================
                        if _ssh_v2 and isinstance(_ssh_v2, str) and _ssh_v2 != _ssh:
                            try:
                                store_full_snapshots_to_sheet(_bsc, _ssh_v2, worksheet_title="Snapshots")
                            except Exception:
                                pass
                    except Exception:
                        _gs_ref = ""

                    _ref = store_full_snapshots_local(_bsc, _ssh)

                    analysis["source_snapshot_hash"] = analysis.get("source_snapshot_hash") or _ssh
                    analysis.setdefault("results", {})
                    if isinstance(analysis["results"], dict):
                        analysis["results"]["source_snapshot_hash"] = analysis["results"].get("source_snapshot_hash") or _ssh
                        # PATCH A4 (ADD): store v2 hash in results for downstream consumers
                        try:
                            if _ssh_v2:
                                analysis["results"]["source_snapshot_hash_v2"] = analysis["results"].get("source_snapshot_hash_v2") or _ssh_v2
                                # =========================
                                # PATCH FIX37 (ADD): stable snapshot hash alias for fastpath alignment
                                # - Prefer v2 (stable) when present; fall back to legacy v1.
                                # =========================
                                try:
                                    _ssh_stable = _ssh_v2 or _ssh
                                    if _ssh_stable:
                                        analysis["source_snapshot_hash_stable"] = analysis.get("source_snapshot_hash_stable") or _ssh_stable
                                        analysis["results"]["source_snapshot_hash_stable"] = analysis["results"].get("source_snapshot_hash_stable") or _ssh_stable
                                except Exception:
                                    pass
                        except Exception:
                            pass

                    if _ref:
                        analysis["snapshot_store_ref"] = analysis.get("snapshot_store_ref") or _ref
                        if isinstance(analysis["results"], dict):
                            analysis["results"]["snapshot_store_ref"] = analysis["results"].get("snapshot_store_ref") or _ref
                            # PATCH A5 (ADD): v2 snapshot ref for convenience
                            try:
                                if _ssh_v2:
                                    analysis["results"]["snapshot_store_ref_v2"] = analysis["results"].get("snapshot_store_ref_v2") or f"gsheet:Snapshots:{_ssh_v2}"
                            except Exception:
                                pass
                    # =============================================================
                    # PATCH SS4B (ADDITIVE): prefer Sheets snapshot ref when available
                    # =============================================================
                    try:
                        if _gs_ref:
                            analysis["snapshot_store_ref"] = _gs_ref
                            if isinstance(analysis.get("results"), dict):
                                analysis["results"]["snapshot_store_ref"] = _gs_ref
                    except Exception:
                        pass

        except Exception:
            pass
        # =====================================================================

        payload_for_sheets = _shrink_for_sheets(analysis)
        payload_json = _try_make_sheet_json(payload_for_sheets)

        # =====================================================================
        # PATCH A5 (BUGFIX, REQUIRED): never write invalid JSON to Sheets
        # - Previous hard truncation produced non-JSON (prefix + random suffix),
        #   causing history loaders (json.loads) to skip the row entirely.
        # - This wrapper guarantees valid JSON even when we must truncate.
        # =====================================================================
        if isinstance(payload_json, str) and len(payload_json) > SHEETS_CELL_LIMIT:
            try:
                payload_json = json.dumps(
                    {
                        "_sheet_write": {
                            "truncated": True,
                            "mode": "hard_truncation_wrapper",
                            "note": "Payload exceeded Google Sheets single-cell limit; stored preview only.",
                        },
                        # keep a preview for debugging/UI; still parseable JSON
                        "preview": payload_json[: max(0, SHEETS_CELL_LIMIT - 600)],
                        "analysis_id": analysis_id,
                        "timestamp": analysis.get("timestamp", datetime.now().isoformat()),
                        "question": (analysis.get("question", "") or "")[:200],
                    },
                    ensure_ascii=False,
                    default=str,
                )
            except Exception:
                # ultra-safe fallback: still valid JSON
                payload_json = '{"_sheet_write":{"truncated":true,"mode":"hard_truncation_wrapper","note":"json.dumps failed"}}'
        # =====================================================================
        # =====================================================================
        # PATCH HF_PERSIST1 (ADDITIVE): Persist full payload to HistoryFull when History cell is wrapped/truncated
        # Why:
        # - Evolution rebuild requires schema/anchors which may be lost in a sheets-safe wrapper
        # - HistoryFull stores the full JSON keyed by analysis_id for later rehydration
        # Behavior:
        # - If payload_json indicates truncation/wrapper OR is very large, write full payload to HistoryFull
        # - Attach a pointer full_store_ref to both analysis and the wrapper object (when possible)
        # =====================================================================
        try:
            is_truncated = False
            try:
                if isinstance(payload_json, str) and ('"_sheet_write"' in payload_json or '"_sheets_safe"' in payload_json):
                    # quick signal; parse if possible
                    try:
                        _pj = json.loads(payload_json)
                        sw = _pj.get("_sheet_write") if isinstance(_pj, dict) else None
                        if isinstance(sw, dict) and sw.get("truncated") is True:
                            is_truncated = True
                        if _pj.get("_sheets_safe") is True:
                            is_truncated = True
                    except Exception:
                        # if we can't parse and it's huge, treat as truncated risk
                        if len(payload_json) > 45000:
                            is_truncated = True
                elif isinstance(payload_json, str) and len(payload_json) > 45000:
                    is_truncated = True
            except Exception:
                pass

            if is_truncated:
                full_payload_json = ""
                try:
                    full_payload_json = json.dumps(analysis, ensure_ascii=False, default=str)
                except Exception:
                    full_payload_json = ""

                if full_payload_json:
                    ok_full = write_full_history_payload_to_sheet(analysis_id, full_payload_json, worksheet_title="HistoryFull")
                    if ok_full:
                        ref = f"gsheet:HistoryFull:{analysis_id}"
                        try:
                            analysis["full_store_ref"] = ref
                        except Exception:
                            pass
                        # If payload_json is a wrapper dict, embed ref too
                        try:
                            _pj2 = json.loads(payload_json)
                            if isinstance(_pj2, dict):
                                _pj2["full_store_ref"] = ref
                                sw2 = _pj2.get("_sheet_write")
                                if isinstance(sw2, dict):
                                    sw2["full_store_ref"] = ref
                                    _pj2["_sheet_write"] = sw2
                                payload_json = json.dumps(_pj2, ensure_ascii=False, default=str)
                        except Exception:
                            pass
        except Exception:
            pass
        # =====================================================================


        row = [
            analysis_id,
            analysis.get("timestamp", datetime.now().isoformat()),
            (analysis.get("question", "") or "")[:100],
            str(analysis.get("final_confidence", "")),
            payload_json,
        ]
        sheet.append_row(row, value_input_option="RAW")

        try:
            st.session_state["last_analysis"] = analysis
        except Exception:
            pass

        return True

    except Exception as e:
        st.warning(f"⚠️ Failed to save to Google Sheets: {e}")
        if "analysis_history" not in st.session_state:
            st.session_state.analysis_history = []
        st.session_state.analysis_history.append(analysis)
        try:
            st.session_state["last_analysis"] = analysis
        except Exception:
            pass
        return False


def normalize_unit_tag(unit_str: str) -> str:
    """
    Canonical unit tags used for drift=0 comparisons.
    """
    u = (unit_str or "").strip()
    if not u:
        return ""
    ul = u.lower().replace(" ", "")

    # energy units
    if ul == "twh":
        return "TWh"
    if ul == "gwh":
        return "GWh"
    if ul == "mwh":
        return "MWh"
    if ul == "kwh":
        return "kWh"
    if ul == "wh":
        return "Wh"

    # magnitudes
    if ul in ("t", "trillion", "tn"):
        return "T"
    if ul in ("b", "bn", "billion"):
        return "B"
    if ul in ("m", "mn", "mio", "million"):
        return "M"
    if ul in ("k", "thousand", "000"):
        return "K"

    # percent
    if ul in ("%", "pct", "percent"):
        return "%"

    return u


def unit_family(unit_tag: str) -> str:
    """
    Unit family classifier for gating.
    """
    ut = (unit_tag or "").strip()

    if ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
        return "energy"
    if ut == "%":
        return "percent"
    if ut in ("T", "B", "M", "K"):
        return "magnitude"

    return ""


def canonicalize_numeric_candidate(candidate: dict) -> dict:


    """


    Additive: attach canonical numeric fields to a candidate dict.


    Safe to call multiple times.



    PATCH AI4 (ADDITIVE): anchor integrity


    - Ensures anchor_hash + candidate_id are present when possible (derived if missing).


    - Does not change extraction behavior; only enriches fields.


    """


    import hashlib



    if not isinstance(candidate, dict):


        return {}



    # ---------- numeric value ----------


    v_raw = candidate.get("value_norm")


    v = None


    if v_raw is not None:


        try:


            v = float(v_raw)


        except Exception:


            v = None


    if v is None:


        try:


            v0 = candidate.get("value")


            if v0 is None:


                return candidate


            v = float(v0)


        except Exception:


            return candidate



    # ---------- unit normalization ----------


    try:


        ut = normalize_unit_tag(candidate.get("unit_tag") or candidate.get("unit") or "")


    except Exception:


        ut = str(candidate.get("unit_tag") or candidate.get("unit") or "").strip()



    try:


        fam = normalize_unit_family(ut)


    except Exception:


        fam = ""



    # If candidate already has base_unit/multiplier_to_base, respect them


    base_unit = candidate.get("base_unit")


    mult = candidate.get("multiplier_to_base")



    try:


        mult = float(mult) if mult is not None else None


    except Exception:


        mult = None



    # Minimal deterministic mapping (extend as needed)


    if (not base_unit) or (mult is None):


        base_unit = ""


        mult = 1.0



        # percents


        if ut in ("%", "pct"):


            base_unit, mult = "%", 1.0



        # energy


        elif ut == "MWh":


            base_unit, mult = "Wh", 1e6


        elif ut == "kWh":


            base_unit, mult = "Wh", 1e3


        elif ut == "Wh":


            base_unit, mult = "Wh", 1.0



        # power


        elif ut == "GW":


            base_unit, mult = "W", 1e9


        elif ut == "MW":


            base_unit, mult = "W", 1e6


        elif ut == "kW":


            base_unit, mult = "W", 1e3


        elif ut == "W":


            base_unit, mult = "W", 1.0



        # mass


        elif ut in ("Mt", "million_tonnes", "million_tons"):


            base_unit, mult = "t", 1e6


        elif ut in ("kt", "kilo_tonnes", "kilo_tons"):


            base_unit, mult = "t", 1e3


        elif ut in ("t", "tonne", "tonnes", "ton", "tons"):


            base_unit, mult = "t", 1.0



        # count-ish


        elif ut in ("vehicles", "units", "count"):


            base_unit, mult = ut, 1.0



        else:


            # unknown unit: treat as-is


            base_unit, mult = (ut or str(candidate.get("unit") or "").strip()), 1.0



    # Only set defaults to avoid overriding existing enriched fields


    candidate.setdefault("unit_tag", ut)


    candidate.setdefault("unit_family", fam)


    candidate.setdefault("base_unit", base_unit)


    candidate.setdefault("multiplier_to_base", mult)



    # value_norm: if already present, do not overwrite


    if candidate.get("value_norm") is None:


        try:


            candidate["value_norm"] = float(v) * float(mult)


        except Exception:


            pass



    # ---------- anchor integrity ----------


    def _sha1(s: str) -> str:


        try:


            return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()


        except Exception:


            return ""



    ah = candidate.get("anchor_hash") or candidate.get("anchor")


    if not ah:


        # attempt deterministic derive if fields exist


        src = candidate.get("source_url") or candidate.get("url") or ""


        ctx = candidate.get("context_snippet") or candidate.get("context") or ""


        if isinstance(ctx, str):


            ctx = ctx.strip()[:240]


        else:


            ctx = ""


        raw = candidate.get("raw")


        if raw is None:


            raw = f"{candidate.get('value')}{candidate.get('unit') or ''}"


        ah = _sha1(f"{src}|{str(raw)[:120]}|{ctx}") if (src or ctx) else ""


        if ah:


            candidate["anchor_hash"] = ah



    if not candidate.get("candidate_id") and ah:


        candidate["candidate_id"] = str(ah)[:16]



    return candidate

def rebuild_metrics_from_snapshots(
    prev_response: dict,
    baseline_sources_cache: list,
    web_context: dict = None
) -> dict:
    """
    Deterministic rebuild using cached snapshots only.
    If sources unchanged, rebuilt metrics converge with analysis.

    Behavior:
      1) Primary: anchor_hash match via prev_response.metric_anchors
      2) Fallback: schema-first deterministic selection when anchor missing
         using metric_schema_frozen + context match + deterministic tie-break.

    NOTE: Dead/unreachable legacy code previously below an early return has been removed
    (explicitly approved).
    """
    import re
    import hashlib

    # =========================
    # PATCH RMS0 (ADDITIVE): typing imports for Dict/Any/List used below
    # - Prevents NameError if typing symbols are not imported globally.
    # =========================
    from typing import Dict, Any, List
    # =========================

    prev_response = prev_response if isinstance(prev_response, dict) else {}

    # =========================
    # PATCH RMS0.1 (ADDITIVE): accept anchors stored under alternate keys
    # - Backward compatible: does not change existing behavior if metric_anchors exists.
    # =========================
    prev_anchors = (
        prev_response.get("metric_anchors")
        or prev_response.get("anchors")
        or {}
    )
    # =========================

    if not isinstance(prev_anchors, dict):
        prev_anchors = {}

    rebuilt: Dict[str, Any] = {}

    # ---------- schema + canonical lookup ----------
    metric_schema = prev_response.get("metric_schema_frozen") or {}
    if not isinstance(metric_schema, dict):
        metric_schema = {}

    # =========================
    # PATCH RB2 (ADDITIVE): ensure baseline_sources_cache is a full list (rehydrate from snapshot store if needed)
    # - Handles cases where history rows store only a summarized baseline_sources_cache, but full snapshots exist
    #   in the Snapshots sheet (referenced by snapshot_store_ref / source_snapshot_hash).
    # =========================
    try:
        if (not isinstance(baseline_sources_cache, list)) or (isinstance(baseline_sources_cache, dict) and baseline_sources_cache.get("_summary") is True):
            # Prefer already-rehydrated cache on prev_response["results"]["baseline_sources_cache"]
            _maybe = (prev_response.get("results", {}) or {}).get("baseline_sources_cache")
            if isinstance(_maybe, list) and _maybe:
                baseline_sources_cache = _maybe
            else:
                store_ref = prev_response.get("snapshot_store_ref") or (prev_response.get("results", {}) or {}).get("snapshot_store_ref")
                source_hash = prev_response.get("source_snapshot_hash") or (prev_response.get("results", {}) or {}).get("source_snapshot_hash")
                if (not store_ref) and source_hash:
                    store_ref = f"gsheet:Snapshots:{source_hash}"
                if isinstance(store_ref, str) and store_ref.startswith("gsheet:Snapshots:"):
                    _hash = store_ref.split(":")[-1]
                    _full = load_full_snapshots_from_sheet(_hash)
                    if isinstance(_full, list) and _full:
                        baseline_sources_cache = _full
    except Exception:
        pass

    prev_can = prev_response.get("primary_metrics_canonical") or {}
    if not isinstance(prev_can, dict):
        prev_can = {}

    # =========================
    # PATCH RMS0.2 (ADDITIVE): compute full metric key universe
    # - Important: some metrics may not have anchors yet; we still must rebuild them
    #   (otherwise evolution "misses" metrics and diffs become unstable).
    # =========================
    metric_key_universe = set()
    try:
        metric_key_universe.update(list(prev_can.keys()))
        metric_key_universe.update(list(prev_anchors.keys()))
    except Exception:
        metric_key_universe = set(prev_can.keys()) if isinstance(prev_can, dict) else set()
    # =========================

    # ---------- deterministic candidate id (tie-breaker) ----------
    def _candidate_id(c: dict) -> str:
        try:
            url = str(c.get("source_url") or c.get("url") or "")
            ah = str(c.get("anchor_hash") or "")
            vn = c.get("value_norm")
            bu = str(c.get("base_unit") or c.get("unit") or c.get("unit_tag") or "")
            mk = str(c.get("measure_kind") or "")
            vn_s = ""
            if vn is not None:
                try:
                    vn_s = f"{float(vn):.12g}"
                except Exception:
                    vn_s = str(vn)
            s = f"{url}|{ah}|{vn_s}|{bu}|{mk}"
            return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()
        except Exception:
            return ""

    # =====================================================================
    # PATCH RMS_E0 (ADDITIVE): small evidence extraction helper
    # - Ensures we consistently carry anchor/evidence fields onto rebuilt metrics.
    # - Purely additive; never affects selection logic.
    # =====================================================================
    def _extract_evidence_fields(c: dict) -> dict:
        if not isinstance(c, dict):
            return {}
        ctx = (c.get("context_snippet") or c.get("context") or "").strip()
        return {
            "raw": c.get("raw"),
            "candidate_id": c.get("candidate_id") or _candidate_id(c),
            "context_snippet": ctx[:240] if isinstance(ctx, str) else None,
            "measure_kind": c.get("measure_kind"),
            "measure_assoc": c.get("measure_assoc"),
            "start_idx": c.get("start_idx"),
            "end_idx": c.get("end_idx"),
            # optional passthroughs if upstream provides them
            "fingerprint": c.get("fingerprint"),
        }
    # =====================================================================

    # =====================================================================
    # PATCH RMS_E1 (ADDITIVE): anchor metadata getter
    # - Pull anchor_confidence (and any other safe fields) from prev_anchors entry.
    # - Helps diff/UI show confidence without recomputing.
    # =====================================================================
    def _anchor_meta(anchor_obj) -> dict:
        if isinstance(anchor_obj, dict):
            out = {}
            if anchor_obj.get("anchor_confidence") is not None:
                try:
                    out["anchor_confidence"] = float(anchor_obj.get("anchor_confidence"))
                except Exception:
                    pass
            # optional passthroughs if present
            if anchor_obj.get("source_url"):
                out["anchor_source_url"] = anchor_obj.get("source_url")
            if anchor_obj.get("raw"):
                out["anchor_raw"] = anchor_obj.get("raw")
            if anchor_obj.get("candidate_id"):
                out["anchor_candidate_id"] = anchor_obj.get("candidate_id")
            return out
        return {}
    # =====================================================================

    # ---------- collect candidates + anchor map ----------
    anchor_to_candidate: Dict[str, Dict[str, Any]] = {}
    all_candidates: List[Dict[str, Any]] = []

    for src in baseline_sources_cache or []:
        if not isinstance(src, dict):
            continue
        src_url = src.get("url") or src.get("source_url") or ""

        # =================================================================
        # PATCH RMS_E2 (ADDITIVE): capture source fingerprint on candidates
        # - Helps later debugging and “same source” proofs.
        # =================================================================
        src_fp = src.get("fingerprint")
        # =================================================================

        for c in (src.get("extracted_numbers") or []):
            if not isinstance(c, dict):
                continue

            # canonicalize if available (safe if repeated)
            try:
                c = canonicalize_numeric_candidate(dict(c))
            except Exception:
                c = dict(c)

            # ensure stable url carried through
            if not c.get("source_url"):
                c["source_url"] = src_url

            # =============================================================
            # PATCH RMS_E2 (ADDITIVE): attach fingerprint if missing
            # =============================================================
            if src_fp and not c.get("fingerprint"):
                c["fingerprint"] = src_fp
            # =============================================================

            ah = c.get("anchor_hash")
            if ah:
                if ah not in anchor_to_candidate:
                    anchor_to_candidate[ah] = c
                else:
                    old = anchor_to_candidate[ah]
                    if old.get("is_junk") and not c.get("is_junk"):
                        anchor_to_candidate[ah] = c

            all_candidates.append(c)

    # ---------- schema-first helpers ----------
    def _schema_for_key(metric_key: str) -> dict:
        d = metric_schema.get(metric_key)
        return d if isinstance(d, dict) else {}

    def _expected_from_schema(metric_key: str):
        d = _schema_for_key(metric_key)

        unit_family_s = str(d.get("unit_family") or "").strip().lower()
        dim_s = str(d.get("dimension") or "").strip().lower()
        unit_s = str(d.get("unit") or "").strip()
        name_l = str(d.get("name") or "").lower()

        expected_family = ""
        if unit_family_s in ("percent", "currency", "energy"):
            expected_family = unit_family_s
        else:
            ut = normalize_unit_tag(unit_s)
            if ut == "%":
                expected_family = "percent"
            elif ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
                expected_family = "energy"
            elif dim_s == "currency":
                expected_family = "currency"

        currencyish = (unit_family_s == "currency" or dim_s == "currency")

        expected_kind = None
        if expected_family == "percent":
            if any(k in name_l for k in ["growth", "cagr", "increase", "decrease", "yoy", "qoq", "mom", "rate"]):
                expected_kind = "growth_pct"
            else:
                expected_kind = "share_pct"
        if currencyish or expected_family == "currency":
            expected_kind = "money"
        if expected_kind is None and any(k in name_l for k in [
            "units", "unit sales", "vehicle sales", "vehicles sold", "sold",
            "deliveries", "shipments", "registrations", "volume"
        ]):
            expected_kind = "count_units"

        kw = d.get("keywords")
        schema_keywords = [str(x).strip() for x in kw] if isinstance(kw, list) else []
        schema_keywords = [x for x in schema_keywords if x]

        return expected_family, currencyish, expected_kind, schema_keywords, unit_s

    def _ctx_match_score(tokens: List[str], ctx: str) -> float:
        fn = globals().get("calculate_context_match")
        if callable(fn):
            try:
                return float(fn(tokens, ctx))
            except Exception:
                pass

        c = (ctx or "").lower()
        toks = [t.lower() for t in (tokens or []) if t and len(t) >= 2]
        if not toks:
            return 0.0
        hit = sum(1 for t in toks if t in c)
        return hit / max(1, len(toks))

    def _currency_evidence(raw: str, ctx: str) -> bool:
        r = (raw or "")
        c = (ctx or "").lower()
        if any(s in r for s in ["$", "S$", "€", "£"]):
            return True
        if any(code in c for code in [" usd", "sgd", " eur", " gbp", " aud", " cad", " jpy", " cny", " rmb"]):
            return True
        if any(k in c for k in ["revenue", "turnover", "valuation", "market size", "market value", "profit", "earnings", "ebitda"]):
            return True
        return False

    def _is_yearish_value(v) -> bool:
        try:
            iv = int(float(v))
            return 1900 <= iv <= 2099
        except Exception:
            return False

    # =========================
    # PATCH RMS_BASE (ADDITIVE): helper to overlay rebuilt fields onto prior canonical metric
    # - Keeps metric identity fields (name/canonical_key/dimension/etc.) stable for diffing.
    # - Only overwrites value-ish/source-ish fields with rebuilt candidate data.
    # =========================
    def _overlay_base(metric_key: str, patch: dict) -> dict:
        base = {}
        try:
            if isinstance(prev_can.get(metric_key), dict):
                base = dict(prev_can.get(metric_key) or {})
        except Exception:
            base = {}
        out = dict(base)
        try:
            if isinstance(patch, dict):
                out.update(patch)
        except Exception:
            pass
        return out
    # =========================

    # ---------- 1) primary rebuild by anchor ----------
    rebuilt_by_anchor = set()

    for metric_key, anchor in prev_anchors.items():
        ah = None
        if isinstance(anchor, dict):
            ah = anchor.get("anchor_hash") or anchor.get("anchor")
        elif isinstance(anchor, str):
            ah = anchor

        if ah and ah in anchor_to_candidate:
            c = anchor_to_candidate[ah]

            # =========================
            # PATCH RMS1 (ADDITIVE): overlay rebuilt candidate onto base canonical metric
            # - Keeps canonical identity fields intact for downstream diffs/UI.
            # =========================
            rebuilt[metric_key] = _overlay_base(metric_key, {
                "value": c.get("value"),
                "unit": c.get("unit"),
                "value_norm": c.get("value_norm"),
                "base_unit": c.get("base_unit"),
                "unit_tag": c.get("unit_tag"),
                "unit_family": c.get("unit_family"),
                "anchor_hash": ah,
                "source_url": c.get("source_url"),
                "context_snippet": (c.get("context_snippet") or c.get("context") or "")[:240],
                "measure_kind": c.get("measure_kind"),
                "measure_assoc": c.get("measure_assoc"),
                "rebuild_method": "anchor",

                # =============================================================
                # PATCH RMS_E3 (ADDITIVE): attach evidence + anchor metadata
                # - candidate_id used as stable ID for UI/debugging
                # - anchor_confidence helps diff/UI set match_confidence
                # =============================================================
                **_extract_evidence_fields(c),
                **_anchor_meta(anchor),
                # =============================================================
            })
            # =========================

            rebuilt_by_anchor.add(metric_key)

    # ---------- 2) fallback rebuild when anchor missing ----------
    # NOTE: existing loop only iterated prev_anchors.keys(); we keep it as-is,
    # and then add an extra additive loop to cover metrics without anchors. (PATCH RMS2)
    for metric_key in prev_anchors.keys():
        if metric_key in rebuilt_by_anchor:
            continue

        expected_family, currencyish, expected_kind, schema_keywords, schema_unit = _expected_from_schema(metric_key)

        # conservative fallback if schema is thin
        if not expected_family and metric_key in prev_can and isinstance(prev_can.get(metric_key), dict):
            pm = prev_can.get(metric_key) or {}
            ut = normalize_unit_tag(pm.get("unit") or schema_unit or "")
            if ut == "%":
                expected_family = "percent"
            elif ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
                expected_family = "energy"

        # tokens for context scoring
        tokens = []
        if schema_keywords:
            tokens = schema_keywords
        else:
            # fallback to build_metric_keywords(schema_name)
            schema_name = ""
            try:
                schema_name = str(_schema_for_key(metric_key).get("name") or "")
            except Exception:
                schema_name = ""
            fn_bmk = globals().get("build_metric_keywords")
            if callable(fn_bmk):
                try:
                    tokens = fn_bmk(schema_name or metric_key) or []
                except Exception:
                    tokens = []
            else:
                tokens = []

        best = None
        best_key = None
        best_score = -1.0

        for c in all_candidates:
            if not isinstance(c, dict):
                continue

            # fallback skips junk (anchor path already handled above)
            if c.get("is_junk") is True:
                continue

            ctx = (c.get("context") or c.get("context_snippet") or "").strip()
            if not ctx:
                continue

            # stop timeline years contaminating non-year metrics
            if expected_family not in ("percent", "energy") and not (currencyish or expected_family == "currency"):
                if (c.get("unit_tag") in ("", None)) and _is_yearish_value(c.get("value")):
                    continue
            # =====================================================================
            # PATCH FIX41AFC5 (ADDITIVE): hard-reject year-only + unitless candidates (evolution rebuild parity)
            # Why:
            #   - Prevent "2024"/"2025" from being selected as metric values (especially count/magnitude_other)
            #   - Applies regardless of expected_family, but only when the candidate is unitless/non-percent.
            # Determinism:
            #   - Pure filtering; stable ordering; no refetch.
            # =====================================================================
            try:
                _vnorm = c.get("value_norm", None)
                if _vnorm is None:
                    _vnorm = c.get("value", None)
                _is_year = _is_yearish_value(_vnorm)
                _cand_ut0 = (c.get("unit_tag") or normalize_unit_tag(c.get("unit") or "") or "").strip()
                _cand_fam0 = (c.get("unit_family") or unit_family(_cand_ut0) or "").strip().lower()
                _mk0 = str(c.get("measure_kind") or "").strip().lower()
                _is_pct0 = bool(c.get("is_percent") or c.get("has_percent") or (_cand_ut0 == "%") or (_cand_fam0 == "percent"))
                _has_curr0 = bool(str(c.get("currency_symbol") or c.get("currency") or "").strip())
                _has_unit_ev0 = bool(_cand_ut0 or _cand_fam0 or _is_pct0 or _has_curr0)
                # year-only guard (unitless, non-percent, non-currency)
                if _is_year and (not _has_unit_ev0) and (not _is_pct0) and (not _has_curr0) and (_mk0 in ("magnitude_other", "count_units", "count", "number", "")):
                    try:
                        _fix41afc5_dbg["rejected_year_only"] = int(_fix41afc5_dbg.get("rejected_year_only", 0) or 0) + 1
                    except Exception:
                        pass
                    continue
                # magnitude_other guard (unitless, non-percent, non-currency)
                if (_mk0 == "magnitude_other") and (not _has_unit_ev0) and (not _is_pct0) and (not _has_curr0):
                    try:
                        _fix41afc5_dbg["rejected_magnitude_other_unitless"] = int(_fix41afc5_dbg.get("rejected_magnitude_other_unitless", 0) or 0) + 1
                    except Exception:
                        pass
                    continue
            except Exception:
                pass

            cand_ut = c.get("unit_tag") or normalize_unit_tag(c.get("unit") or "")
            cand_fam = (c.get("unit_family") or unit_family(cand_ut) or "").strip().lower()
            mk = c.get("measure_kind")

            # unit-family gating
            if expected_family == "percent":
                if cand_fam != "percent" and cand_ut != "%":
                    continue
            elif expected_family == "energy":
                if cand_fam != "energy":
                    continue
            elif currencyish or expected_family == "currency":
                if cand_fam not in ("currency", "magnitude"):
                    continue
                if not _currency_evidence(c.get("raw", ""), ctx):
                    continue
                if mk == "count_units":
                    continue

            # measure-kind gating (only if candidate provides it)
            if expected_kind and mk and mk != expected_kind:
                continue

            # normalize value for ranking
            try:
                c2 = canonicalize_numeric_candidate(dict(c))
            except Exception:
                c2 = c

            val_norm = c2.get("value_norm")
            if val_norm is None:
                try:
                    val_norm = float(c2.get("value"))
                except Exception:
                    continue

            ctx_score = _ctx_match_score(tokens, ctx)
            if ctx_score <= 0.0:
                continue

            url = str(c2.get("source_url") or c2.get("url") or "")
            cid = c2.get("candidate_id") or _candidate_id({**c2, "value_norm": val_norm})

            # deterministic tie-break (max)
            key = (
                float(ctx_score),
                float(val_norm),
                url,
                str(cid),
            )

            if best_key is None or key > best_key:
                best_key = key
                best_score = float(ctx_score)
                best = {**c2, "value_norm": val_norm, "candidate_id": cid}

        if best:
            # =========================
            # PATCH RMS1 (ADDITIVE): overlay onto base canonical metric
            # =========================
            rebuilt[metric_key] = _overlay_base(metric_key, {
                "value": best.get("value"),
                "unit": best.get("unit") or best.get("unit_tag"),
                "value_norm": best.get("value_norm"),
                "base_unit": best.get("base_unit"),
                "unit_tag": best.get("unit_tag"),
                "unit_family": best.get("unit_family"),
                "anchor_hash": best.get("anchor_hash"),
                "source_url": best.get("source_url") or best.get("url"),
                "context_snippet": (best.get("context_snippet") or best.get("context") or "")[:240],
                "measure_kind": best.get("measure_kind"),
                "measure_assoc": best.get("measure_assoc"),
                "rebuild_method": "schema_fallback",
                "fallback_ctx_score": round(best_score, 6),
                "candidate_id": best.get("candidate_id"),

                # =============================================================
                # PATCH RMS_E4 (ADDITIVE): attach standardized evidence fields
                # - Ensures candidate_id/raw/context are always present when possible.
                # - Adds anchor_confidence derived from fallback_ctx_score.
                # =============================================================
                **_extract_evidence_fields(best),
                "anchor_confidence": float(min(100.0, max(0.0, best_score) * 100.0)) if best_score is not None else 0.0,
                # =============================================================
            })
            # =========================

    # =========================
    # PATCH RMS2 (ADDITIVE): ensure metrics without anchors are also rebuilt
    # - Your existing fallback loop only iterates prev_anchors.keys().
    # - This loop covers the remaining canonical metrics (prev_can keys) that are missing
    #   from prev_anchors, using the SAME schema-first logic (copied, not refactored).
    # - Additive: does not alter prior behavior for anchored metrics.
    # =========================
    for metric_key in (metric_key_universe or set()):
        if metric_key in rebuilt:
            continue

        expected_family, currencyish, expected_kind, schema_keywords, schema_unit = _expected_from_schema(metric_key)

        if not expected_family and metric_key in prev_can and isinstance(prev_can.get(metric_key), dict):
            pm = prev_can.get(metric_key) or {}
            ut = normalize_unit_tag(pm.get("unit") or schema_unit or "")
            if ut == "%":
                expected_family = "percent"
            elif ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
                expected_family = "energy"

        tokens = []
        if schema_keywords:
            tokens = schema_keywords
        else:
            schema_name = ""
            try:
                schema_name = str(_schema_for_key(metric_key).get("name") or "")
            except Exception:
                schema_name = ""
            fn_bmk = globals().get("build_metric_keywords")
            if callable(fn_bmk):
                try:
                    tokens = fn_bmk(schema_name or metric_key) or []
                except Exception:
                    tokens = []
            else:
                tokens = []

        best = None
        best_key = None
        best_score = -1.0

        for c in all_candidates:
            if not isinstance(c, dict):
                continue
            if c.get("is_junk") is True:
                continue

            ctx = (c.get("context") or c.get("context_snippet") or "").strip()
            if not ctx:
                continue

            if expected_family not in ("percent", "energy") and not (currencyish or expected_family == "currency"):
                if (c.get("unit_tag") in ("", None)) and _is_yearish_value(c.get("value")):
                    continue

            cand_ut = c.get("unit_tag") or normalize_unit_tag(c.get("unit") or "")
            cand_fam = (c.get("unit_family") or unit_family(cand_ut) or "").strip().lower()
            mk = c.get("measure_kind")

            if expected_family == "percent":
                if cand_fam != "percent" and cand_ut != "%":
                    continue
            elif expected_family == "energy":
                if cand_fam != "energy":
                    continue
            elif currencyish or expected_family == "currency":
                if cand_fam not in ("currency", "magnitude"):
                    continue
                if not _currency_evidence(c.get("raw", ""), ctx):
                    continue
                if mk == "count_units":
                    continue

            if expected_kind and mk and mk != expected_kind:
                continue

            try:
                c2 = canonicalize_numeric_candidate(dict(c))
            except Exception:
                c2 = c

            val_norm = c2.get("value_norm")
            if val_norm is None:
                try:
                    val_norm = float(c2.get("value"))
                except Exception:
                    continue

            ctx_score = _ctx_match_score(tokens, ctx)
            if ctx_score <= 0.0:
                continue

            url = str(c2.get("source_url") or c2.get("url") or "")
            cid = c2.get("candidate_id") or _candidate_id({**c2, "value_norm": val_norm})

            key = (
                float(ctx_score),
                float(val_norm),
                url,
                str(cid),
            )

            if best_key is None or key > best_key:
                best_key = key
                best_score = float(ctx_score)
                best = {**c2, "value_norm": val_norm, "candidate_id": cid}

        if best:
            rebuilt[metric_key] = _overlay_base(metric_key, {
                "value": best.get("value"),
                "unit": best.get("unit") or best.get("unit_tag"),
                "value_norm": best.get("value_norm"),
                "base_unit": best.get("base_unit"),
                "unit_tag": best.get("unit_tag"),
                "unit_family": best.get("unit_family"),
                "anchor_hash": best.get("anchor_hash"),
                "source_url": best.get("source_url") or best.get("url"),
                "context_snippet": (best.get("context_snippet") or best.get("context") or "")[:240],
                "measure_kind": best.get("measure_kind"),
                "measure_assoc": best.get("measure_assoc"),
                "rebuild_method": "schema_fallback_no_anchor",
                "fallback_ctx_score": round(best_score, 6),
                "candidate_id": best.get("candidate_id"),

                # =============================================================
                # PATCH RMS_E5 (ADDITIVE): attach standardized evidence fields
                # =============================================================
                **_extract_evidence_fields(best),
                "anchor_confidence": float(min(100.0, max(0.0, best_score) * 100.0)) if best_score is not None else 0.0,
                # =============================================================
            })
        else:
            # stable placeholder (do not fabricate)
            if isinstance(prev_can.get(metric_key), dict):
                rebuilt[metric_key] = _overlay_base(metric_key, {
                    "rebuild_method": "not_found_in_snapshots",

                    # =============================================================
                    # PATCH RMS_E6 (ADDITIVE): keep evidence fields present for stable shape
                    # =============================================================
                    "anchor_hash": None,
                    "source_url": None,
                    "context_snippet": None,
                    "raw": None,
                    "candidate_id": None,
                    "anchor_confidence": 0.0,
                    # =============================================================
                })
    # =========================

    # =====================================================================
    # PATCH RMS_FALLBACK1 (ADDITIVE): never return empty rebuild when we have a baseline universe
    # Why:
    #   - Source-anchored evolution is snapshot-gated; if snapshots exist but rebuild fails
    #     (missing anchors/schema mismatch/edge cases), returning {} causes evolution to hard-fail.
    #   - For determinism + drift-0 testing, we prefer a safe fallback that preserves the
    #     canonical metric universe from the previous analysis while emitting an explicit flag.
    #
    # Behavior:
    #   - If 'rebuilt' is empty/non-dict, fall back to prev_response['primary_metrics_canonical'].
    #   - Marks each metric with '_rebuild_fallback_used': True (additive field).
    #   - DOES NOT fabricate new values; it reuses previous canonical values only.
    # =====================================================================
    try:
        if not isinstance(rebuilt, dict) or not rebuilt:
            prev_universe = {}
            if isinstance(prev_response, dict):
                prev_universe = prev_response.get("primary_metrics_canonical") or {}
            if isinstance(prev_universe, dict) and prev_universe:
                rebuilt = {}
                for ck in sorted(prev_universe.keys()):
                    m = prev_universe.get(ck)
                    if isinstance(m, dict):
                        mm = dict(m)
                        mm["_rebuild_fallback_used"] = True
                        # Ensure ES7 fields exist (pure enrichment)
                        mm.setdefault("canonical_key", ck)
                        mm.setdefault("anchor_used", False)
                        mm.setdefault("anchor_confidence", 0.0)
                        rebuilt[ck] = mm
                # Add top-level marker (additive)
                try:
                    rebuilt["_rebuild_status"] = "fallback_prev_primary_metrics_canonical"
                except Exception:
                    pass
    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH FIX41AFC5 (ADDITIVE): attach eligibility-hardening debug counters
    # =====================================================================
    try:
        if isinstance(rebuilt, dict):
            rebuilt.setdefault("_fix41afc5_debug", {})
            if isinstance(rebuilt.get("_fix41afc5_debug"), dict):
                rebuilt["_fix41afc5_debug"].update(dict(_fix41afc5_dbg))
    except Exception:
        pass
    # =====================================================================

    return rebuilt




# =====================================================================
# PATCH RMS_MIN1 (ADDITIVE): Minimal schema-driven rebuild from snapshots
# ---------------------------------------------------------------------
# Goal:
#   - Provide a deterministic, evolution-safe metric rebuild that uses ONLY:
#       (a) baseline_sources_cache snapshots (and their extracted_numbers)
#       (b) frozen metric schema (metric_schema_frozen)
#   - No re-fetch, no LLM inference, no heuristic "best guess" beyond schema fields.
#
# Contract:
#   - Returns a dict shaped like primary_metrics_canonical:
#       { canonical_key: { ...metric fields... } }
#   - Deterministic tie-break ordering.
# =====================================================================


# =====================================================================
# PATCH F (deterministic): Explicit candidate exclusion in rebuild stage
#   - Enforce that ANY candidate flagged as junk is excluded from:
#       * candidate indexing
#       * candidate scoring
#       * final metric assignment
#   - Additionally, suppress "year-like" unitless tokens (e.g., 2024/2025) for
#     non-year metrics (currency/percent/rate/ratio/growth/etc.) to prevent
#     year fixation during evolution.
#   - Purely deterministic: no LLM, no refetch, no heuristics outside schema cues.
# =====================================================================

def _candidate_disallowed_for_metric(_cand: dict, _spec: dict = None) -> bool:
    """Return True if a snapshot candidate must not be used to assign a metric value."""
    if not isinstance(_cand, dict):
        return True

    # 1) Hard exclusion: explicit junk flags / reasons from extraction phase
    if _cand.get("is_junk") is True:
        return True
    jr = str(_cand.get("junk_reason") or "").strip().lower()
    if jr:
        # If a junk_reason exists, treat it as non-selectable deterministically.
        return True

    # 2) Deterministic anti-year-fixation: unitless year-like tokens are disallowed
    #    for most numeric metrics (unless schema clearly indicates a "year" metric).
    try:
        v = _cand.get("value_norm", _cand.get("value"))
        unitish = str(_cand.get("base_unit") or _cand.get("unit_tag") or _cand.get("unit") or "").strip()
        if unitish == "" and isinstance(v, (int, float)):
            if abs(float(v) - round(float(v))) < 1e-9:
                vi = int(round(float(v)))
                if 1900 <= vi <= 2100:
                    if isinstance(_spec, dict):
                        nm = str(_spec.get("name") or "").lower()
                        cid = str(_spec.get("canonical_id") or _spec.get("canonical_key") or "").lower()
                        kws = _spec.get("keywords") or []
                        kws_s = " ".join([str(k).lower() for k in kws]) if isinstance(kws, list) else str(kws).lower()

                        # Allow explicit year metrics
                        if ("year" in nm) or ("year" in cid) or ("founded" in nm) or ("since" in nm) or ("year" in kws_s):
                            return False

                        uf = str(_spec.get("unit_family") or "").lower().strip()
                        ut = str(_spec.get("unit_tag") or _spec.get("unit") or "").lower().strip()

                        # For common non-year metric families, exclude year-like tokens.
                        if uf in ("currency", "percent", "rate", "ratio", "growth", "share"):
                            return True
                        if "%" in ut:
                            return True
                        if any(w in nm for w in ("cagr", "revenue", "growth", "market", "sales", "profit", "margin", "volume")):
                            return True

                    # Default: unitless year-like token is not a valid metric value.
                    return True
    except Exception:
        pass

    return False

def rebuild_metrics_from_snapshots_schema_only(
    prev_response: dict,
    baseline_sources_cache: list,
    web_context: dict = None
) -> dict:
    """Schema-driven deterministic rebuild from cached snapshots only.

    This is intentionally minimal:
      - It does NOT attempt free-form metric discovery.
      - It ONLY populates metrics declared in the frozen schema.
      - Candidate selection is driven by schema fields (keywords + unit family/tag).
      - Deterministic sorting ensures stable output ordering.

    Returns:
      Dict[str, Dict] shaped like primary_metrics_canonical.
    """
    import re

# =====================================================================
    # =====================================================================
    # PATCH FIX33 (ADDITIVE): enforce unit-required eligibility in schema-only rebuild
    # Why:
    #   - When anchors are not used (anchor_used:false), schema-only rebuild can still
    #     select unit-less year tokens (e.g., 2024/2025) for currency/percent metrics.
    #   - This patch hard-rejects candidates with no token-level unit evidence when
    #     the schema (or canonical_key suffix) implies a unit is required.
    #   - Also optionally emits compact debug metadata for top candidates/rejections.
    # Determinism:
    #   - Pure filtering + stable ordering; no refetch; no randomness.
    # =====================================================================

    def _fix33_schema_unit_required(spec_unit_family: str, spec_unit_tag: str, canonical_key: str) -> bool:
        uf = str(spec_unit_family or "").strip().lower()
        ut = str(spec_unit_tag or "").strip().lower()
        ck = str(canonical_key or "").strip().lower()
        if uf in {"currency", "percent", "rate", "ratio"}:
            return True
        if ut in {"%", "percent"}:
            return True
        # Canonical-key suffix conventions (backstop)
        if ck.endswith("__currency") or ck.endswith("__percent") or ck.endswith("__rate") or ck.endswith("__ratio"):
            return True
        return False

    def _fix33_candidate_has_unit_evidence(c: dict) -> bool:
        if not isinstance(c, dict):
            return False
        # Any explicit unit/currency/% evidence is enough to qualify as "has unit".
        if str(c.get("unit_tag") or "").strip():
            return True
        if str(c.get("unit_family") or "").strip():
            return True
        if str(c.get("base_unit") or "").strip():
            return True
        if str(c.get("unit") or "").strip():
            return True
        if str(c.get("currency_symbol") or c.get("currency") or "").strip():
            return True
        if bool(c.get("is_percent") or c.get("has_percent")):
            return True
        mk = str(c.get("measure_kind") or "").strip().lower()
        if mk in {"money", "percent", "percentage", "rate", "ratio"}:
            return True
        toks = c.get("unit_tokens") or c.get("unit_evidence_tokens") or []
        if isinstance(toks, (list, tuple)) and len(toks) > 0:
            return True
        return False

    _fix33_dbg = False
    try:
        _fix33_dbg = bool((web_context or {}).get("debug_evolution") or ((prev_response or {}).get("debug") or {}).get("debug_evolution"))
    except Exception:
        _fix33_dbg = False


    # -------------------------
    # Resolve frozen schema (supports multiple storage locations)
    # -------------------------
    schema = None
    try:
        if isinstance(prev_response, dict):
            if isinstance(prev_response.get("metric_schema_frozen"), dict):
                schema = prev_response.get("metric_schema_frozen")
            elif isinstance(prev_response.get("primary_response"), dict) and isinstance(prev_response["primary_response"].get("metric_schema_frozen"), dict):
                schema = prev_response["primary_response"].get("metric_schema_frozen")
            elif isinstance(prev_response.get("results"), dict) and isinstance(prev_response["results"].get("metric_schema_frozen"), dict):
                schema = prev_response["results"].get("metric_schema_frozen")
    except Exception:
        schema = None

    if not isinstance(schema, dict) or not schema:
        return {}

    # -------------------------
    # Collect candidates from snapshots (no re-fetch)
    # -------------------------
    candidates = []
    if isinstance(baseline_sources_cache, list):
        for src in baseline_sources_cache:
            if not isinstance(src, dict):
                continue
            nums = src.get("extracted_numbers")
            if not isinstance(nums, list) or not nums:
                continue
            for n in nums:
                if not isinstance(n, dict):
                    continue
                # Filter junk deterministically (strict rebuild exclusion)
                if _candidate_disallowed_for_metric(n, None):
                    continue
                # Normalize a few fields to ensure stable downstream access
                c = dict(n)
                if not c.get("source_url"):
                    c["source_url"] = src.get("url", "") or src.get("source_url", "") or ""
                candidates.append(c)

    # Deterministic candidate ordering (no set/dict iteration surprises)
    def _cand_sort_key(c: dict):
        return (
            str(c.get("source_url") or ""),
            str(c.get("anchor_hash") or ""),
            int(c.get("start_idx") or 0),
            str(c.get("raw") or ""),
            str(c.get("unit_tag") or ""),
            str(c.get("unit_family") or ""),
            float(c.get("value_norm") or c.get("value") or 0.0),
        )

    candidates.sort(key=_cand_sort_key)

    if not candidates:
        return {}

    # -------------------------
    # Deterministic schema-driven selection
    # -------------------------
    def _norm_text(s: str) -> str:
        return re.sub(r"\s+", " ", (s or "").lower()).strip()

    out = {}

    for canonical_key in sorted(schema.keys()):
        spec = schema.get(canonical_key) or {}
        if not isinstance(spec, dict):
            continue

        spec_keywords = spec.get("keywords") or []
        if not isinstance(spec_keywords, list):
            spec_keywords = []
        spec_keywords_norm = [str(k).lower().strip() for k in spec_keywords if str(k).strip()]

        spec_unit_tag = str(spec.get("unit_tag") or spec.get("unit") or "").strip()
        spec_unit_family = str(spec.get("unit_family") or "").strip()

        # Score candidates by schema keyword hits, then filter by unit constraints if present.
        best = None
        best_key = None

        # ============================================================
        # PATCH FIX33 (ADDITIVE): per-metric debug collectors
        # ============================================================
        _fix33_top = []
        _fix33_rej = {}

        for c in candidates:
            # PATCH F: strict candidate exclusion at scoring time
            if _candidate_disallowed_for_metric(c, spec):
                continue
            # =====================================================================
            # PATCH AI2 (ADDITIVE): guard against year-only candidates on currency-like metrics
            # Why:
            # - Some sources contain many years (e.g., 2023, 2024) that can outscore true values.
            # - For currency-ish metrics, suppress candidates that look like bare years unless context clearly indicates money.
            # Determinism:
            # - Pure filter; does not invent candidates or refetch content.
            # =====================================================================
            try:
                def _ai2_is_year_only(c: dict):
                    """Return True if candidate is a likely standalone year (1900-2100) with no unit."""
                    try:
                        c = c if isinstance(c, dict) else {}
                        # Prefer canonical numeric
                        v = c.get("value_norm")
                        if v is None:
                            v = c.get("value")
                        try:
                            iv = int(float(v))
                        except Exception:
                            return False
                        if iv < 1900 or iv > 2100:
                            return False
                        # Must be truly 4-digit (avoid 2023.5 etc)
                        try:
                            if abs(float(v) - float(iv)) > 1e-9:
                                return False
                        except Exception:
                            pass

                        # If the candidate itself signals time/year, do not treat as "junk year".
                        u = str(c.get("base_unit") or c.get("unit") or "").strip().lower()
                        ut = str(c.get("unit_tag") or "").strip().lower()
                        uf = str(c.get("unit_family") or "").strip().lower()
                        if "year" in u or "year" in ut or "year" in uf or "time" in uf:
                            return False

                        raw = str(c.get("raw") or "").strip()
                        sval = str(iv)

                        # -------------------------------------------------------------
                        # PATCH E (ADDITIVE): strict handling when raw contains context
                        # - Some extractors store a wider raw window (e.g. includes '$721m ... in 2023')
                        # - Currency symbols elsewhere in raw should NOT make a year candidate non-year.
                        # - Only treat as non-year if the currency symbol is directly attached to the year.
                        # -------------------------------------------------------------
                        try:
                            if re.search(r"(\$|usd|eur|gbp|aud|cad|sgd)\s*"+re.escape(sval)+r"\b", raw.lower()):
                                return False
                        except Exception:
                            pass

                        # If raw is basically just the year token (allow brackets/punctuation), it's year-only.
                        try:
                            raw2 = re.sub(r"[\s,.;:()\[\]{}<>]", "", raw)
                            if raw2 == sval:
                                return True
                        except Exception:
                            pass

                        # If raw contains multiple numbers, it's likely context; still treat as year-only
                        # when this candidate has no unit.
                        try:
                            nums = re.findall(r"\d{2,}", raw)
                            if len(nums) >= 2:
                                return True
                        except Exception:
                            pass

                        # If raw contains month names, likely a date; treat as year-only (we suppress dates too).
                        try:
                            if re.search(r"\b(jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)\b", raw.lower()):
                                return True
                        except Exception:
                            pass

                        return True
                    except Exception:
                        return False
                def _ai2_schema_currencyish(sd: dict) -> bool:
                    try:
                        if not isinstance(sd, dict):
                            return False
                        u = str(sd.get('unit') or sd.get('base_unit') or '').lower()
                        if any(x in u for x in ('usd','sgd','eur','gbp','jpy','$','€','£')):
                            return True
                        # heuristic keywords on definition (safe, schema-driven-ish)
                        nm = str(sd.get('name') or '').lower()
                        if any(x in nm for x in ('revenue','sales','cost','price','capex','opex','investment','spend','spending','expenditure','value')):
                            return True
                        return False
                    except Exception:
                        return False

                _sd = locals().get('schema_def')
                if _ai2_schema_currencyish(_sd) and _ai2_is_year_only(c):
                    continue

                # =====================================================================
                # PATCH YEAR3 (ADDITIVE): suppress year-only candidates for percent/CAGR-like metrics too
                # Why: year tokens (e.g., 2025) can outrank true percent values when unit evidence is weak.
                # Safe: only suppress when candidate has no explicit unit and looks like a bare year.
                # =====================================================================
                try:
                    if _ai2_is_year_only(c):
                        _sd_name = str((_sd or {}).get('name') or '').lower()
                        _sd_ckey = str((_sd or {}).get('canonical_key') or ckey or '').lower()
                        _sd_unit_tag = str((_sd or {}).get('unit_tag') or '').lower()
                        _sd_unit_family = str((_sd or {}).get('unit_family') or '').lower()
                        if ('cagr' in _sd_name) or ('cagr' in _sd_ckey) or (_sd_unit_tag in ('percent','pct')) or (_sd_unit_family in ('percent','ratio','rate')):
                            continue
                except Exception:
                    pass
                # =====================================================================
            except Exception:
                pass
            # =====================================================================
            ctx = _norm_text(c.get("context_snippet") or "")
            if not ctx:
                continue

            # Keyword hits: schema-driven (no external heuristics)
            hits = 0
            if spec_keywords_norm:
                for kw in spec_keywords_norm:
                    if kw and kw in ctx:
                        hits += 1

            if spec_keywords_norm and hits == 0:
                continue

            # Unit constraints (only if schema declares them)
            if spec_unit_family:
                if str(c.get("unit_family") or "").strip() != spec_unit_family:
                    # allow a unit_tag-only match when family is missing in candidate
                    if not (spec_unit_tag and str(c.get("unit_tag") or "").strip() == spec_unit_tag):
                        continue

            if spec_unit_tag:
                # if a tag is specified, prefer exact tag matches
                if str(c.get("unit_tag") or "").strip() != spec_unit_tag:
                    # allow family match when tag differs
                    if not (spec_unit_family and str(c.get("unit_family") or "").strip() == spec_unit_family):
                        continue

            # =====================================================================
            # PATCH FIX41AFC5 (ADDITIVE): reject year-only candidates early (schema-only rebuild parity)
            # =====================================================================
            try:
                _vnorm = c.get("value_norm", None)
                if _vnorm is None:
                    _vnorm = c.get("value", None)
                _is_year = _is_yearish_value(_vnorm)
                _mk0 = str(c.get("measure_kind") or "").strip().lower()
                _cand_ut0 = str(c.get("unit_tag") or "").strip()
                _cand_fam0 = str(c.get("unit_family") or "").strip().lower()
                _is_pct0 = bool(c.get("is_percent") or c.get("has_percent") or (_cand_ut0 == "%") or (_cand_fam0 == "percent"))
                _has_curr0 = bool(str(c.get("currency_symbol") or c.get("currency") or "").strip())
                _has_unit_ev0 = bool(_cand_ut0 or _cand_fam0 or _is_pct0 or _has_curr0 or str(c.get("base_unit") or c.get("unit") or "").strip())
                if _is_year and (not _has_unit_ev0) and (not _is_pct0) and (not _has_curr0) and (_mk0 in ("magnitude_other", "count_units", "count", "number", "")):
                    try:
                        _fix41afc5_dbg2["rejected_year_only"] = int(_fix41afc5_dbg2.get("rejected_year_only", 0) or 0) + 1
                    except Exception:
                        pass
                    continue
            except Exception:
                pass

            # =====================================================================
            # PATCH FIX33 (ADDITIVE): hard-reject unit-less candidates when unit is required
            # =====================================================================
            try:
                _req = _fix33_schema_unit_required(spec_unit_family, spec_unit_tag, canonical_key)
                _has_unit_ev = _fix33_candidate_has_unit_evidence(c)
                if _req and not _has_unit_ev:
                    # Track rejection (debug)
                    if _fix33_dbg:
                        try:
                            _fix33_rej["missing_unit_required"] = int(_fix33_rej.get("missing_unit_required", 0) or 0) + 1
                        except Exception:
                            pass
                    continue

                # Track top candidates (debug)
                if _fix33_dbg:
                    try:
                        _fix33_top.append({
                            "raw": c.get("raw"),
                            "value_norm": c.get("value_norm"),
                            "unit_tag": c.get("unit_tag"),
                            "unit_family": c.get("unit_family"),
                            "base_unit": c.get("base_unit") or c.get("unit"),
                            "measure_kind": c.get("measure_kind"),
                            "hits": hits,
                            "has_unit_ev": bool(_has_unit_ev),
                            "source_url": c.get("source_url"),
                            "anchor_hash": c.get("anchor_hash"),
                        })
                    except Exception:
                        pass
            except Exception:
                pass

            # Deterministic tie-break:
            #   (-hits, then stable candidate identity tuple)
            tie = (-hits,) + _cand_sort_key(c)
            if best is None or tie < best_key:
                best = c
                best_key = tie

        if not isinstance(best, dict):
            continue

        # Emit a minimal canonical metric row (schema-driven, deterministic)
        metric = {
            "name": spec.get("name") or spec.get("canonical_id") or canonical_key,
            "value": best.get("value"),
            "unit": best.get("unit") or spec.get("unit") or "",
            "unit_tag": best.get("unit_tag") or spec.get("unit_tag") or "",
            "unit_family": best.get("unit_family") or spec.get("unit_family") or "",
            "base_unit": best.get("base_unit") or best.get("unit_tag") or spec.get("unit_tag") or "",
            "multiplier_to_base": best.get("multiplier_to_base") if best.get("multiplier_to_base") is not None else 1.0,
            "value_norm": best.get("value_norm") if best.get("value_norm") is not None else best.get("value"),
            "canonical_id": spec.get("canonical_id") or spec.get("canonical_key") or canonical_key,
            "canonical_key": canonical_key,
            "dimension": spec.get("dimension") or "",
            "original_name": spec.get("name") or "",
            "geo_scope": "unknown",
            "geo_name": "",
            "is_proxy": False,
            "proxy_type": "",
            "provenance": {
                "method": "schema_keyword_match",
                "best_candidate": {
                    "raw": best.get("raw"),
                    "source_url": best.get("source_url"),
                    "context_snippet": best.get("context_snippet"),
                    "anchor_hash": best.get("anchor_hash"),
                    "start_idx": best.get("start_idx"),
                    "end_idx": best.get("end_idx"),
                },
            },
        }

# ============================================================
        # ============================================================
        # PATCH FIX33 (ADDITIVE): selection debug (top candidates + rejection counts)
        # ============================================================
        try:
            if _fix33_dbg and isinstance(metric, dict):
                try:
                    _fix33_top_sorted = sorted(
                        _fix33_top,
                        key=lambda d: (-(int(d.get("hits") or 0)), str(d.get("value_norm") or ""), str(d.get("raw") or "")),
                    )
                except Exception:
                    _fix33_top_sorted = _fix33_top
                metric.setdefault("provenance", {})
                metric["provenance"]["fix33_top_candidates"] = list(_fix33_top_sorted[:10])
                metric["provenance"]["fix33_rejected_reason_counts"] = dict(_fix33_rej or {})
        except Exception:
            pass

        out[canonical_key] = metric

    return out



# ===================== PATCH RMS_AWARE1 (ADDITIVE) =====================
def rebuild_metrics_from_snapshots_with_anchors(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    Anchor-aware deterministic rebuild (analysis-aligned):
      - Uses ONLY snapshots/cache + frozen schema + prior metric_anchors (if present)
      - No re-fetch
      - No heuristic matching outside anchor_hash + schema dimension checks
      - Deterministic ordering and selection

    Strategy:
      1) Load metric_anchors (canonical_key -> {anchor_hash, ...}) from prev_response (any common nesting).
      2) Flatten snapshot candidates (extracted_numbers) from baseline_sources_cache.
      3) For each canonical_key with an anchor_hash:
           pick candidate with matching anchor_hash (and compatible unit family if inferable).
      4) Build primary_metrics_canonical-like dict.

    Returns: dict {canonical_key: metric_obj}
    """
    import re

    if not isinstance(prev_response, dict):
        return {}

    # 1) Pull anchors from any common location
    metric_anchors = (
        prev_response.get("metric_anchors")
        or (prev_response.get("primary_response") or {}).get("metric_anchors")
        or (prev_response.get("results") or {}).get("metric_anchors")
    )
    if not isinstance(metric_anchors, dict) or not metric_anchors:
        return {}

    # 2) Pull frozen schema (for name/dimension hints; optional but preferred)
    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )

    # Flatten candidates from baseline_sources_cache (list of source dicts with extracted_numbers)
    if isinstance(baseline_sources_cache, dict) and isinstance(baseline_sources_cache.get("snapshots"), list):
        sources = baseline_sources_cache.get("snapshots", [])
    elif isinstance(baseline_sources_cache, list):
        sources = baseline_sources_cache
    else:
        sources = []

    candidates = []
    for s in sources:
        if not isinstance(s, dict):
            continue
        url = s.get("source_url") or s.get("url") or ""
        xs = s.get("extracted_numbers")
        if isinstance(xs, list) and xs:
            for c in xs:
                if not isinstance(c, dict):
                    continue
                if _candidate_disallowed_for_metric(c, None):
                    continue
                c2 = dict(c)
                c2.setdefault("source_url", url)
                candidates.append(c2)

    # Deterministic sort key (stable across runs)
    def _cand_sort_key(c: dict):
        try:
            return (
                str(c.get("anchor_hash") or ""),
                str(c.get("source_url") or ""),
                int(c.get("start_idx") or 0),
                str(c.get("raw") or ""),
                str(c.get("unit") or ""),
                float(c.get("value_norm") or 0.0),
            )
        except Exception:
            return ("", "", 0, "", "", 0.0)

    candidates.sort(key=_cand_sort_key)

    # Unit family inference (lightweight; used only as a compatibility guard)
    def _unit_family(unit: str) -> str:
        u = (unit or "").strip().lower()
        if u in ("%", "percent", "percentage"):
            return "percent"
        if any(x in u for x in ("usd", "$", "eur", "gbp", "jpy", "cny", "aud", "sgd")):
            return "currency"
        if any(x in u for x in ("unit", "units", "vehicle", "vehicles", "kwh", "mwh", "gwh", "twh", "ton", "tons")):
            return "quantity"
        return ""

    rebuilt = {}

    # 3) Anchor_hash match first (no schema-free guessing)
    for canonical_key, a in metric_anchors.items():
        if not isinstance(a, dict):
            continue
        ah = a.get("anchor_hash") or a.get("anchor") or ""
        if not ah:
            continue

        sch = metric_schema.get(canonical_key) if isinstance(metric_schema, dict) else None
        name = (sch or {}).get("name") or a.get("name") or canonical_key
        expected_dim = ((sch or {}).get("dimension") or (sch or {}).get("unit_family") or "").strip().lower()

        best = None

        # =====================================================================
        # PATCH AI_CAND3 (ADDITIVE): pick best candidate among same anchor_hash
        # =====================================================================
        same = [c for c in candidates if isinstance(c, dict) and (c.get("anchor_hash") or "") == ah and c.get("is_junk") is not True]
        best = _pick_best_candidate(
            same,
            expected_dim=expected_dim,
            expected_unit_family=str((schema.get(ckey) or {}).get("unit_family") or ""),
            expected_base_unit=str((schema.get(ckey) or {}).get("base_unit") or ""),
        )
        if not best:
            continue
        # =====================================================================

        rebuilt[canonical_key] = {
            "canonical_key": canonical_key,
            "name": name,
            "value": best.get("value"),
            "unit": best.get("unit") or "",
            "value_norm": best.get("value_norm"),
            "source_url": best.get("source_url") or "",
            "anchor_hash": best.get("anchor_hash") or "",
            "evidence": [{
                "source_url": best.get("source_url") or "",
                "raw": best.get("raw") or "",
                "context_snippet": (best.get("context") or best.get("context_window") or "")[:400],
                "anchor_hash": best.get("anchor_hash") or "",
                "method": "anchor_hash_rebuild",
            }],
        }

    return rebuilt
# =================== END PATCH RMS_AWARE1 (ADDITIVE) ===================



def get_history(limit: int = MAX_HISTORY_ITEMS) -> List[Dict]:
    """Load analysis history from Google Sheet"""
    sheet = get_google_sheet()
    if not sheet:
        # Fallback to session state
        return st.session_state.get('analysis_history', [])

    try:
        # ============================================================
        # PATCH GH_KEY1 (ADDITIVE): Use the actual worksheet title as cache key
        # Why:
        # - Your sheet names are: 'Sheet1', 'Snapshots', 'HistoryFull'
        # - There is no worksheet called 'History'
        # - Using cache_key='History' can cache empty reads under the wrong key.
        # ============================================================
        _ws_title = getattr(sheet, "title", "") or "Sheet1"
        _cache_key = f"History::{_ws_title}"
        # ============================================================

        # Get all rows (skip header)
        values = []
        try:
            values = sheets_get_all_values_cached(sheet, cache_key=_cache_key)
        except Exception:
            values = []

        # ============================================================
        # PATCH GH_FALLBACK1 (ADDITIVE): One direct-read retry if cached read is empty
        # Why:
        # - If a prior transient read/429 produced an empty cached value,
        #   evolution may temporarily see no history even though rows exist.
        # ============================================================
        if not values or len(values) < 2:
            try:
                direct = sheet.get_all_values()
                if direct and len(direct) >= 2:
                    values = direct
            except Exception:
                pass
        # ============================================================

        all_rows = values[1:] if values and len(values) >= 2 else []

        # ============================================================
        # PATCH GH_RL1 (ADDITIVE): Rate-limit fallback for History reads
        # ============================================================
        try:
            if (not all_rows) and globals().get("_SHEETS_LAST_READ_ERROR"):
                if ("RESOURCE_EXHAUSTED" in str(_SHEETS_LAST_READ_ERROR)
                    or "Quota exceeded" in str(_SHEETS_LAST_READ_ERROR)
                    or "429" in str(_SHEETS_LAST_READ_ERROR)):
                    return st.session_state.get('analysis_history', [])
        except Exception:
            pass
        # ============================================================

        # Parse and return most recent
        history = []
        for row in all_rows[-limit:]:
            if len(row) >= 5:
                raw_cell = row[4]
                try:
                    data = json.loads(raw_cell)
                    data['_sheet_id'] = row[0]  # Keep track of sheet row ID

                    # (your existing GH2 / ES1G / GH1 / GH3 logic unchanged)
                    # ...
                    history.append(data)

                except json.JSONDecodeError:
                    # (your existing GH1 rescue logic unchanged)
                    continue

        # (your existing GH3 sort unchanged)
        return history

    except Exception as e:
        st.warning(f"⚠️ Failed to load from Google Sheets: {e}")
        return st.session_state.get('analysis_history', [])


def get_analysis_by_id(analysis_id: str) -> Optional[Dict]:
    """Get a specific analysis by ID"""
    sheet = get_google_sheet()
    if not sheet:
        return None

    try:
        # Find row with matching ID
        cell = sheet.find(analysis_id)
        if cell:
            row = sheet.row_values(cell.row)
            if len(row) >= 5:
                return json.loads(row[4])
    except Exception as e:
        st.warning(f"⚠️ Failed to find analysis: {e}")

    return None

def delete_from_history(analysis_id: str) -> bool:
    """Delete an analysis from history"""
    sheet = get_google_sheet()
    if not sheet:
        return False

    try:
        cell = sheet.find(analysis_id)
        if cell:
            sheet.delete_rows(cell.row)
            return True
    except Exception as e:
        st.warning(f"⚠️ Failed to delete: {e}")

    return False

def clear_history() -> bool:
    """Clear all history (keep headers)"""
    sheet = get_google_sheet()
    if not sheet:
        st.session_state.analysis_history = []
        return True

    try:
        # Get row count
        all_rows = sheets_get_all_values_cached(sheet, cache_key="History")
        if len(all_rows) > 1:
            # Delete all rows except header
            sheet.delete_rows(2, len(all_rows))
        return True
    except Exception as e:
        st.warning(f"⚠️ Failed to clear history: {e}")
        return False

def format_history_label(analysis: Dict) -> str:
    """Format a history item for dropdown display"""
    timestamp = analysis.get('timestamp', '')
    question = analysis.get('question', 'Unknown query')[:40]
    confidence = analysis.get('final_confidence', '')

    try:
        dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
        now = datetime.now()
        delta = now - dt.replace(tzinfo=None)

        if delta.total_seconds() < 3600:
            time_str = f"{int(delta.total_seconds() / 60)}m ago"
        elif delta.total_seconds() < 86400:
            time_str = f"{int(delta.total_seconds() / 3600)}h ago"
        elif delta.days == 1:
            time_str = "Yesterday"
        elif delta.days < 7:
            time_str = f"{delta.days}d ago"
        else:
            time_str = dt.strftime("%b %d")
    except:
        time_str = timestamp[:10] if timestamp else "Unknown"

    conf_str = f" ({confidence:.0f}%)" if isinstance(confidence, (int, float)) else ""
    return f"{time_str}: {question}...{conf_str}"

def get_history_options() -> List[Tuple[str, int]]:
    """Get formatted history options for dropdown"""
    history = get_history()
    options = []
    for i, analysis in enumerate(reversed(history)):  # Most recent first
        label = format_history_label(analysis)
        actual_index = len(history) - 1 - i
        options.append((label, actual_index))
    return options

# =========================================================
# 1. CONFIGURATION & API KEY VALIDATION
# =========================================================

def load_api_keys():
    """Load and validate API keys from secrets or environment"""

    # =====================================================================
    # PATCH FIX41AFC5 (ADDITIVE): debug counters for schema-only rebuild eligibility hardening
    # =====================================================================
    _fix41afc5_dbg2 = {"rejected_year_only": 0, "rejected_unitless": 0, "rejected_magnitude_other_unitless": 0}
    # =====================================================================

    # =====================================================================
    # PATCH FIX41AFC5 (ADDITIVE): debug counters for rebuild eligibility hardening
    # =====================================================================
    _fix41afc5_dbg = {"rejected_year_only": 0, "rejected_unitless": 0, "rejected_magnitude_other_unitless": 0}
    # =====================================================================
    try:
        PERPLEXITY_KEY = st.secrets.get("PERPLEXITY_API_KEY") or os.getenv("PERPLEXITY_API_KEY", "")
        GEMINI_KEY = st.secrets.get("GEMINI_API_KEY") or os.getenv("GEMINI_API_KEY", "")
        SERPAPI_KEY = st.secrets.get("SERPAPI_KEY") or os.getenv("SERPAPI_KEY", "")
        SCRAPINGDOG_KEY = st.secrets.get("SCRAPINGDOG_KEY") or os.getenv("SCRAPINGDOG_KEY", "")
    except Exception:
        PERPLEXITY_KEY = os.getenv("PERPLEXITY_API_KEY", "")
        GEMINI_KEY = os.getenv("GEMINI_API_KEY", "")
        SERPAPI_KEY = os.getenv("SERPAPI_KEY", "")
        SCRAPINGDOG_KEY = os.getenv("SCRAPINGDOG_KEY", "")

    # Validate critical keys
    if not PERPLEXITY_KEY or len(PERPLEXITY_KEY) < 10:
        st.error("❌ PERPLEXITY_API_KEY is missing or invalid")
        st.stop()

    if not GEMINI_KEY or len(GEMINI_KEY) < 10:
        st.error("❌ GEMINI_API_KEY is missing or invalid")
        st.stop()

    return PERPLEXITY_KEY, GEMINI_KEY, SERPAPI_KEY, SCRAPINGDOG_KEY

PERPLEXITY_KEY, GEMINI_KEY, SERPAPI_KEY, SCRAPINGDOG_KEY = load_api_keys()
PERPLEXITY_URL = "https://api.perplexity.ai/chat/completions"

# Configure Gemini
genai.configure(api_key=GEMINI_KEY)
gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')

# =========================================================
# 2. PYDANTIC MODELS
# =========================================================

class MetricDetail(BaseModel):
    """Individual metric with name, value, and unit"""
    name: str = Field(..., description="Metric name")
    value: Union[float, int, str] = Field(..., description="Metric value")
    unit: str = Field(default="", description="Unit of measurement")
    model_config = ConfigDict(extra='ignore')

class TopEntityDetail(BaseModel):
    """Entity in top_entities list"""
    name: str = Field(..., description="Entity name")
    share: Optional[str] = Field(None, description="Market share")
    growth: Optional[str] = Field(None, description="Growth rate")
    model_config = ConfigDict(extra='ignore')

class TrendForecastDetail(BaseModel):
    """Trend forecast item"""
    trend: str = Field(..., description="Trend description")
    direction: Optional[str] = Field(None, description="Direction indicator")
    timeline: Optional[str] = Field(None, description="Timeline")
    model_config = ConfigDict(extra='ignore')

class VisualizationData(BaseModel):
    chart_labels: List[str] = Field(default_factory=list)
    chart_values: List[Union[float, int]] = Field(default_factory=list)
    chart_title: Optional[str] = Field("Trend Analysis")
    chart_type: Optional[str] = Field("line")
    x_axis_label: Optional[str] = None
    y_axis_label: Optional[str] = None
    model_config = ConfigDict(extra='ignore')

class ComparisonBar(BaseModel):
    """Comparison bar chart data"""
    title: str = Field("Comparison", description="Chart title")
    categories: List[str] = Field(default_factory=list)
    values: List[Union[float, int]] = Field(default_factory=list)
    model_config = ConfigDict(extra='ignore')

class BenchmarkTable(BaseModel):
    """Benchmark table row"""
    category: str
    value_1: Union[float, int, str] = Field(default=0, description="Numeric value or string")
    value_2: Union[float, int, str] = Field(default=0, description="Numeric value or string")
    model_config = ConfigDict(extra='ignore')

class Action(BaseModel):
    """Investment/action recommendation"""
    recommendation: str = Field("Neutral", description="Buy/Hold/Sell/Neutral")
    confidence: str = Field("Medium", description="High/Medium/Low")
    rationale: str = Field("", description="Reasoning")
    model_config = ConfigDict(extra='ignore')

class LLMResponse(BaseModel):
    """Complete LLM response schema"""
    executive_summary: str = Field(..., description="High-level summary")
    primary_metrics: Dict[str, MetricDetail] = Field(default_factory=dict)
    key_findings: List[str] = Field(default_factory=list)
    top_entities: List[TopEntityDetail] = Field(default_factory=list)
    trends_forecast: List[TrendForecastDetail] = Field(default_factory=list)
    visualization_data: Optional[VisualizationData] = None
    comparison_bars: Optional[ComparisonBar] = None
    benchmark_table: Optional[List[BenchmarkTable]] = None
    sources: List[str] = Field(default_factory=list)
    confidence: Union[float, int] = Field(default=75)
    freshness: Optional[str] = Field(None)
    action: Optional[Action] = None
    model_config = ConfigDict(extra='ignore')

# =========================================================
# 3. PROMPTS
# =========================================================

RESPONSE_TEMPLATE = """
{
  "executive_summary": "3-4 sentence high-level answer",
  "primary_metrics": {
    "metric_1": {"name": "Key Metric 1", "value": 25.5, "unit": "%"},
    "metric_2": {"name": "Key Metric 2", "value": 623, "unit": "$B"}
  },
  "key_findings": [
    "Finding 1 with quantified impact",
    "Finding 2 explaining drivers"
  ],
  "top_entities": [
    {"name": "Entity 1", "share": "25%", "growth": "15%"}
  ],
  "trends_forecast": [
    {"trend": "Trend description", "direction": "↑", "timeline": "2025-2027"}
  ],
  "visualization_data": {
    "chart_labels": ["2023", "2024", "2025"],
    "chart_values": [100, 120, 145],
    "chart_title": "Market Growth",
    "chart_type": "line"
  },
  "comparison_bars": {
    "title": "Market Share",
    "categories": ["A", "B", "C"],
    "values": [45, 30, 25]
  },
  "benchmark_table": [
    {"category": "Company A", "value_1": 25.5, "value_2": 623}
  ],
  "sources": ["source1.com"],
  "confidence": 87,
  "freshness": "Dec 2024"
}
"""



SYSTEM_PROMPT = f"""You are a professional market research analyst.

CRITICAL RULES:
1. Return ONLY valid JSON. NO markdown, NO code blocks, NO extra text.
2. NO citation references like [1][2] inside strings.
3. Use double quotes for all keys and string values.
4. NO trailing commas in arrays or objects.
5. Escape internal quotes with backslash.
6. If the prompt includes "Query Structure", you MUST follow it:
   - Treat "MAIN QUESTION" as the primary topic and address it FIRST.
   - Treat "SIDE QUESTIONS" as secondary topics and address them AFTER the main topic.
   - Do NOT let a side question replace the main question just because it is more specific.
   - In executive_summary, clearly separate: "Main:" then "Side:" when side questions exist.


NUMERIC FIELD RULES (IMPORTANT):
- In benchmark_table: value_1 and value_2 MUST be numbers (never "N/A", "null", or text)
- If data unavailable, use 0 for benchmark_table values
- In primary_metrics: values can be numbers or strings with units (e.g., "25.5" or "25.5 billion")
- In top_entities: share and growth can be strings (e.g., "25%")

REQUIRED FIELDS (provide substantive data):

**executive_summary** - MUST be 4-6 complete sentences covering:
  • Sentence 1: Direct answer with specific quantitative data (market size, revenue, units, etc.)
  • Sentence 2: Major players or regional breakdown with percentages/numbers
  • Sentence 3: Key growth drivers or market dynamics
  • Sentence 4: Future outlook with projected CAGR, timeline, or target values
  • Sentence 5 (optional): Challenge, risk, or competitive dynamic

  BAD (too short): "The EV market is growing rapidly due to government policies."

  GOOD: "The global electric vehicle market reached 14.2 million units sold in 2023, representing 18% of total auto sales. China dominates with 60% market share, followed by Europe (25%) and North America (10%). Growth is driven by battery cost reductions (down 89% since 2010), expanding charging infrastructure, and stricter emission regulations in over 20 countries. The market is projected to grow at 21% CAGR through 2030, reaching 40 million units annually. However, supply chain constraints for lithium and cobalt remain key challenges."

- primary_metrics (3+ metrics with numbers)
- key_findings (3+ findings with quantitative details)
- top_entities (3+ companies/countries with market share %)
- trends_forecast (2+ trends with timelines)
- visualization_data (MUST have chart_labels and chart_values)
- benchmark_table (if included, value_1 and value_2 must be NUMBERS, not "N/A")

Even if web data is sparse, use your knowledge to provide complete, detailed analysis.

Output ONLY this JSON structure:
{RESPONSE_TEMPLATE}
"""

EVOLUTION_PROMPT_TEMPLATE = """You are a market research analyst performing an UPDATE ANALYSIS.

You have been given a PREVIOUS ANALYSIS from {time_ago}. Your task is to:
1. Search for CURRENT data on the same metrics and entities
2. Identify what has CHANGED vs what has STAYED THE SAME
3. Provide updated values where data has changed
4. Flag any metrics/entities that are no longer relevant or have new entries

PREVIOUS ANALYSIS:
==================
Question: {previous_question}
Timestamp: {previous_timestamp}

Previous Executive Summary:
{previous_summary}

Previous Key Metrics:
{previous_metrics}

Previous Top Entities:
{previous_entities}

Previous Key Findings:
{previous_findings}
==================

CRITICAL RULES:
1. Return ONLY valid JSON. NO markdown, NO code blocks.
2. For EACH metric, indicate if it INCREASED, DECREASED, or stayed UNCHANGED
3. Keep the SAME metric names as previous analysis for easy comparison
4. If a metric is no longer available, mark it as "discontinued"
5. If there's a NEW important metric, add it with status "new"


REQUIRED OUTPUT FORMAT:
{{
  "executive_summary": "Updated 4-6 sentence summary noting key changes since last analysis",
  "analysis_delta": {{
    "time_since_previous": "{time_ago}",
    "overall_trend": "improving/declining/stable",
    "major_changes": ["Change 1", "Change 2"],
    "data_freshness": "Q4 2024"
  }},
  "primary_metrics": {{
    "metric_key": {{
      "name": "Same metric name as before",
      "previous_value": 100,
      "current_value": 110,
      "unit": "$B",
      "change_pct": 10.0,
      "direction": "increased/decreased/unchanged",
      "status": "updated/discontinued/new"
    }}
  }},
  "key_findings": [
    "[UNCHANGED] Finding that remains true",
    "[UPDATED] Finding with new data",
    "[NEW] Completely new finding",
    "[REMOVED] Finding no longer relevant - reason"
  ],
  "top_entities": [
    {{
      "name": "Company A",
      "previous_share": "25%",
      "current_share": "27%",
      "previous_rank": 1,
      "current_rank": 1,
      "change": "increased",
      "status": "updated"
    }}
  ],
  "trends_forecast": [
    {{"trend": "Trend description", "direction": "↑", "timeline": "2025-2027", "confidence": "high/medium/low"}}
  ],
  "visualization_data": {{
    "chart_labels": ["Previous", "Current"],
    "chart_values": [100, 110],
    "chart_title": "Market Size Evolution"
  }},
  "sources": ["source1.com", "source2.com"],
  "confidence": 85,
  "freshness": "Dec 2024",
  "drift_summary": {{
    "metrics_changed": 2,
    "metrics_unchanged": 3,
    "entities_reshuffled": 1,
    "findings_updated": 4,
    "overall_stability_pct": 75
  }}
}}

NOW, search for CURRENT information to UPDATE the previous analysis.
Focus on finding CHANGES to the metrics and entities listed above.

User Question: {query}
"""

# =========================================================
# 4. MODEL LOADING
# =========================================================

@st.cache_resource(show_spinner="🔧 Loading AI models...")
def load_models():
    """Load and cache sentence transformer and classifier"""
    try:
        classifier = pipeline(
            "zero-shot-classification",
            model="facebook/bart-large-mnli",
            device=-1
        )
        embedder = SentenceTransformer("all-MiniLM-L6-v2")
        return classifier, embedder
    except Exception as e:
        st.error(f"❌ Model loading failed: {e}")
        st.stop()

domain_classifier, embedder = load_models()

# =========================================================
# 5. JSON REPAIR FUNCTIONS
# =========================================================

def repair_llm_response(data: dict) -> dict:
    """
    Repair common LLM JSON structure issues:

    - Convert primary_metrics from list -> dict (stable keys)
    - Normalize MetricDetail fields so currency+unit do NOT get lost:
        "29.8 S$B" / "S$29.8B" / "S$29.8 billion" -> value=29.8, unit="S$B"
        "$204.7B" -> value=204.7, unit="$B"
        "9.8%" -> value=9.8, unit="%"
    - Ensure top_entities and trends_forecast are lists
    - Fix visualization_data legacy keys (labels/values)
    - Fix benchmark_table numeric values
    - Remove 'action' block entirely (no longer used)
    - Add minimal required fields if missing

    NOTE: This function is intentionally conservative: it normalizes obvious formatting
    without trying to "invent" missing values.
    """
    if not isinstance(data, dict):
        return {}

    def _to_list(x):
        if x is None:
            return []
        if isinstance(x, list):
            return x
        if isinstance(x, dict):
            return list(x.values())
        return []

    def _coerce_number(s: str):
        try:
            return float(str(s).replace(",", "").strip())
        except Exception:
            return None

    def _normalize_metric_item(item: dict) -> dict:
        """
        Normalize a single metric dict in-place-ish and return it.

        Goal: preserve currency + magnitude in `unit`, keep `value` numeric when possible.
        """
        if not isinstance(item, dict):
            return {"name": "N/A", "value": "N/A", "unit": ""}

        name = item.get("name")
        if not isinstance(name, str) or not name.strip():
            name = "N/A"
        item["name"] = name

        raw_val = item.get("value")
        raw_unit = item.get("unit")

        unit = (raw_unit or "")
        if not isinstance(unit, str):
            unit = str(unit)

        # If already numeric and unit looks okay, keep as-is
        if isinstance(raw_val, (int, float)) and isinstance(unit, str):
            item["unit"] = unit.strip()
            return item

        # Try to parse string value forms like:
        # "S$29.8B", "29.8 S$B", "$ 204.7 billion", "9.8%", "12 percent"
        if isinstance(raw_val, str):
            txt = raw_val.strip()

            # Also allow unit to carry the number sometimes (rare but happens)
            # e.g. value="29.8", unit="S$B" is already fine.
            # But if unit is empty and txt contains unit, we extract.
            # Percent detection
            if re.search(r'(%|\bpercent\b)', txt, flags=re.I):
                num = _coerce_number(re.sub(r'[^0-9\.\-\,]+', '', txt))
                if num is not None:
                    item["value"] = num
                    item["unit"] = "%"
                    return item

            # Currency detection
            currency = ""
            # Normalize currency tokens in either value or unit
            combo = f"{txt} {unit}".strip()

            if re.search(r'\bSGD\b', combo, flags=re.I) or "S$" in combo.upper():
                currency = "S$"
            elif re.search(r'\bUSD\b', combo, flags=re.I) or "$" in combo:
                currency = "$"

            # Magnitude detection
            # Accept: T/B/M/K, or words
            mag = ""
            if re.search(r'\btrillion\b', combo, flags=re.I):
                mag = "T"
            elif re.search(r'\bbillion\b', combo, flags=re.I):
                mag = "B"
            elif re.search(r'\bmillion\b', combo, flags=re.I):
                mag = "M"
            elif re.search(r'\bthousand\b', combo, flags=re.I):
                mag = "K"
            else:
                m = re.search(r'([TBMK])\b', combo.replace(" ", ""), flags=re.I)
                if m:
                    mag = m.group(1).upper()

            # Extract numeric
            num = _coerce_number(re.sub(r'[^0-9\.\-\,]+', '', txt))
            if num is not None:
                # If unit was present and meaningful (and already includes %), keep it
                if unit.strip() == "%":
                    item["value"] = num
                    item["unit"] = "%"
                    return item

                # Build unit as currency+magnitude when any found
                # If neither found, keep existing unit (may be e.g. "years", "points")
                if currency or mag:
                    item["value"] = num
                    item["unit"] = f"{currency}{mag}".strip()
                    return item

                # No currency/mag detected: keep unit if provided; else blank
                item["value"] = num
                item["unit"] = unit.strip()
                return item

            # If we can’t parse into a number, at least preserve the original text
            item["value"] = txt
            item["unit"] = unit.strip()
            return item

        # Non-string, non-numeric (None, dict, list, etc.)
        if raw_val is None or raw_val == "":
            item["value"] = "N/A"
        else:
            item["value"] = str(raw_val)

        item["unit"] = unit.strip()
        return item

    # -------------------------
    # primary_metrics normalization
    # -------------------------
    metrics = data.get("primary_metrics")

    # list -> dict
    if isinstance(metrics, list):
        new_metrics = {}
        for i, item in enumerate(metrics):
            if not isinstance(item, dict):
                continue
            item = _normalize_metric_item(item)

            raw_name = item.get("name", f"metric_{i+1}")
            key = re.sub(r'[^a-z0-9_]', '', str(raw_name).lower().replace(" ", "_")).strip("_")
            if not key:
                key = f"metric_{i+1}"

            original_key = key
            j = 1
            while key in new_metrics:
                key = f"{original_key}_{j}"
                j += 1

            new_metrics[key] = item

        data["primary_metrics"] = new_metrics

    elif isinstance(metrics, dict):
        # Normalize each metric dict entry
        cleaned = {}
        for k, v in metrics.items():
            if isinstance(v, dict):
                cleaned[str(k)] = _normalize_metric_item(v)
            else:
                # If someone stored a scalar, wrap it
                cleaned[str(k)] = _normalize_metric_item({"name": str(k), "value": v, "unit": ""})
        data["primary_metrics"] = cleaned

    else:
        data["primary_metrics"] = {}

    # -------------------------
    # list-like fields
    # -------------------------
    data["top_entities"] = _to_list(data.get("top_entities"))
    data["trends_forecast"] = _to_list(data.get("trends_forecast"))
    data["key_findings"] = _to_list(data.get("key_findings"))

    # Ensure strings in key_findings
    data["key_findings"] = [str(x) for x in data["key_findings"] if x is not None and str(x).strip()]

    # -------------------------
    # visualization_data legacy keys
    # -------------------------
    if isinstance(data.get("visualization_data"), dict):
        viz = data["visualization_data"]
        if "labels" in viz and "chart_labels" not in viz:
            viz["chart_labels"] = viz.pop("labels")
        if "values" in viz and "chart_values" not in viz:
            viz["chart_values"] = viz.pop("values")

        # Coerce chart_labels/values types gently
        if "chart_labels" in viz and not isinstance(viz["chart_labels"], list):
            viz["chart_labels"] = [str(viz["chart_labels"])]
        if "chart_values" in viz and not isinstance(viz["chart_values"], list):
            viz["chart_values"] = [viz["chart_values"]]

    # -------------------------
    # benchmark_table numeric cleaning
    # -------------------------
    if isinstance(data.get("benchmark_table"), list):
        cleaned_table = []
        for row in data["benchmark_table"]:
            if not isinstance(row, dict):
                continue

            if "category" not in row:
                row["category"] = "Unknown"

            for key in ["value_1", "value_2"]:
                if key not in row:
                    row[key] = 0
                    continue

                val = row.get(key)
                if isinstance(val, str):
                    val_upper = val.upper().strip()
                    if val_upper in ["N/A", "NA", "NULL", "NONE", "", "-", "—"]:
                        row[key] = 0
                    else:
                        try:
                            cleaned = re.sub(r'[^\d.-]', '', val)
                            row[key] = float(cleaned) if '.' in cleaned else int(cleaned) if cleaned else 0
                        except Exception:
                            row[key] = 0
                elif isinstance(val, (int, float)):
                    pass
                else:
                    row[key] = 0

            cleaned_table.append(row)

        data["benchmark_table"] = cleaned_table

    # -------------------------
    # Remove action block entirely
    # -------------------------
    data.pop("action", None)

    # -------------------------
    # Minimal required top-level fields
    # -------------------------
    if not isinstance(data.get("executive_summary"), str) or not data.get("executive_summary", "").strip():
        data["executive_summary"] = "No executive summary provided."

    if not isinstance(data.get("sources"), list):
        data["sources"] = []

    if "confidence" not in data:
        data["confidence"] = 60

    if not isinstance(data.get("freshness"), str) or not data.get("freshness", "").strip():
        data["freshness"] = "Current"

    return data


def validate_numeric_fields(data: dict, context: str = "LLM Response") -> None:
    """
    Guardrail logger (and gentle coercer) for numeric lists used in charts/tables.

    We keep this lightweight: warn when strings appear where numbers are expected,
    and attempt to coerce when safe.
    """
    if not isinstance(data, dict):
        return

    # Check benchmark_table
    if "benchmark_table" in data and isinstance(data["benchmark_table"], list):
        for i, row in enumerate(data["benchmark_table"]):
            if isinstance(row, dict):
                for key in ["value_1", "value_2"]:
                    val = row.get(key)
                    if isinstance(val, str):
                        st.warning(
                            f"⚠️ {context}: benchmark_table[{i}].{key} is string: '{val}' (coercing to 0 if invalid)"
                        )
                        try:
                            cleaned = re.sub(r"[^\d\.\-]", "", val)
                            row[key] = float(cleaned) if cleaned else 0
                        except Exception:
                            row[key] = 0

    # Check visualization_data chart_values
    viz = data.get("visualization_data")
    if isinstance(viz, dict):
        vals = viz.get("chart_values")
        if isinstance(vals, list):
            new_vals = []
            for j, v in enumerate(vals):
                if isinstance(v, (int, float)):
                    new_vals.append(v)
                elif isinstance(v, str):
                    try:
                        cleaned = re.sub(r"[^\d\.\-]", "", v)
                        new_vals.append(float(cleaned) if cleaned else 0.0)
                        st.warning(f"⚠️ {context}: visualization_data.chart_values[{j}] is string: '{v}' (coerced)")
                    except Exception:
                        new_vals.append(0.0)
                else:
                    new_vals.append(0.0)
            viz["chart_values"] = new_vals


def preclean_json(raw: str) -> str:
    """
    Remove markdown fences and common citation markers before JSON parsing.
    Conservative: tries not to destroy legitimate JSON content.
    """
    if not raw or not isinstance(raw, str):
        return ""

    text = raw.strip()

    # Remove leading/trailing code fences (```json ... ```)
    text = re.sub(r'^\s*```(?:json)?\s*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\s*```\s*$', '', text)

    text = text.strip()

    # Remove common citation formats the model may append
    # [web:1], [1], (1) etc. (but avoid killing array syntax by being specific)
    text = re.sub(r'\[web:\d+\]', '', text, flags=re.IGNORECASE)
    text = re.sub(r'(?<!")\[\d+\](?!")', '', text)   # not inside quotes
    text = re.sub(r'(?<!")\(\d+\)(?!")', '', text)   # not inside quotes

    return text.strip()


def parse_json_safely(json_str: str, context: str = "LLM") -> dict:
    """
    Parse JSON with aggressive error recovery:
    1) Pre-clean markdown/citations
    2) Extract the *first* JSON object
    3) Repair common issues (unquoted keys, trailing commas, True/False/Null)
    4) Try parsing; if it fails, attempt a small set of pragmatic fixes
    """
    if json_str is None:
        return {}
    if not isinstance(json_str, str):
        json_str = str(json_str)

    if not json_str.strip():
        return {}

    cleaned = preclean_json(json_str)

    # Extract first JSON object (most LLM outputs are one object)
    match = re.search(r'\{.*\}', cleaned, flags=re.DOTALL)
    if not match:
        st.warning(f"⚠️ No JSON object found in {context} response")
        return {}

    json_content = match.group(0)

    # Structural repairs
    try:
        # Fix unquoted keys: {key: -> {"key":
        json_content = re.sub(
            r'([\{\,]\s*)([a-zA-Z_][a-zA-Z0-9_\-]*)(\s*):',
            r'\1"\2"\3:',
            json_content
        )

        # Remove trailing commas
        json_content = re.sub(r',\s*([\]\}])', r'\1', json_content)

        # Fix boolean/null capitalization
        json_content = re.sub(r':\s*True\b', ': true', json_content)
        json_content = re.sub(r':\s*False\b', ': false', json_content)
        json_content = re.sub(r':\s*Null\b', ': null', json_content)

    except Exception as e:
        st.warning(f"⚠️ {context}: Regex repair failed: {e}")

    # Attempt parse with a few passes
    attempts = 0
    last_err = None

    while attempts < 6:
        try:
            return json.loads(json_content)
        except json.JSONDecodeError as e:
            last_err = e
            msg = (e.msg or "").lower()

            # Pass 1: replace smart quotes
            if attempts == 0:
                json_content = (
                    json_content.replace("“", '"')
                                .replace("”", '"')
                                .replace("’", "'")
                )

            # Pass 2: single-quote keys/strings -> double quotes (limited)
            elif attempts == 1:
                # Only do this if it looks like single quotes dominate
                if json_content.count("'") > json_content.count('"'):
                    json_content = re.sub(r"\'", '"', json_content)

            # Pass 3: try removing control characters
            elif attempts == 2:
                json_content = re.sub(r"[\x00-\x1F\x7F]", "", json_content)

            # Pass 4: if unterminated string, try escaping a quote near the error
            elif "unterminated string" in msg or "unterminated" in msg:
                pos = e.pos
                # Try escaping a quote a bit before pos
                for i in range(pos - 1, max(0, pos - 200), -1):
                    if i < len(json_content) and json_content[i] == '"':
                        if i == 0 or json_content[i - 1] != "\\":
                            json_content = json_content[:i] + '\\"' + json_content[i + 1:]
                            break

            # Pass 5+: give up
            attempts += 1
            continue

    st.error(f"❌ Failed to parse JSON from {context}: {str(last_err)[:180] if last_err else 'unknown error'}")
    return {}




def parse_query_structure_safe(json_str: str, user_question: str) -> Dict:
    """
    Parse LLM-derived query structure with guaranteed deterministic fallback.
    Never raises, never returns empty dict.
    """
    parsed = parse_json_safely(json_str, context="LLM Query Structure")

    if isinstance(parsed, dict) and parsed:
        # Minimal schema validation
        if "main" in parsed or "category" in parsed:
            return parsed

    # 🔒 Deterministic fallback (NO LLM)
    return {
        "category": "unknown",
        "category_confidence": 0.0,
        "main": user_question,
        "side": []
    }


def extract_json_object(text: str) -> Optional[Dict]:
    """
    Best-effort extraction of the first JSON object from a string.
    Returns dict or None.
    """
    if not text or not isinstance(text, str):
        return None

    # Common cleanup
    cleaned = text.strip()
    cleaned = cleaned.replace("```json", "").replace("```", "").strip()

    # Fast path
    try:
        obj = json.loads(cleaned)
        if isinstance(obj, dict):
            return obj
    except Exception:
        pass

    # Regex: first {...} block (non-greedy)
    try:
        m = re.search(r"\{.*\}", cleaned, flags=re.DOTALL)
        if not m:
            return None
        candidate = m.group(0)
        obj = json.loads(candidate)
        if isinstance(obj, dict):
            return obj
    except Exception:
        return None

    return None


# =========================================================
# 6. WEB SEARCH FUNCTIONS
#   SERPAPI STABILITY CONFIGURATION
# =========================================================

# Fixed parameters to prevent geo/personalization variance

SERPAPI_STABILITY_CONFIG = {
    "gl": "us",                    # Fixed country
    "hl": "en",                    # Fixed language
    "google_domain": "google.com", # Fixed domain
    "nfpr": "1",                   # No auto-query correction
    "safe": "active",              # Consistent safe search
    "device": "desktop",           # Fixed device type
    "no_cache": "false",           # Allow Google caching (more stable)
}

# Preferred domains for consistent sourcing (sorted by priority)
PREFERRED_SOURCE_DOMAINS = [
    "statista.com", "reuters.com", "bloomberg.com", "imf.org", "wsj.com", "bcg.com", "opec.org",
    "worldbank.org", "mckinsey.com", "deloitte.com", "spglobal.com", "ft.com", "pwc.com", "semiconductors.org",
    "ft.com", "economist.com", "wsj.com", "forbes.com", "cnbc.com", "kpmg.com", "eia.org"
]

# Search results cache
_search_cache: Dict[str, Tuple[List[Dict], datetime]] = {}
SEARCH_CACHE_TTL_HOURS = 24

def get_search_cache_key(query: str) -> str:
    """Generate stable cache key for search query"""
    normalized = re.sub(r'\s+', ' ', query.lower().strip())
    normalized = re.sub(r'\b(today|current|latest|now|recent)\b', '', normalized)
    return hashlib.md5(normalized.encode()).hexdigest()[:16]

def get_cached_search_results(query: str) -> Optional[List[Dict]]:
    """
    Get cached search results if still valid.

    IMPORTANT:
    - Never treat cached empty results as valid.
      Returning [] here "poisons" the pipeline for hours and makes SerpAPI look broken.
    """
    try:
        cache_key = get_search_cache_key(query)
        if cache_key in _search_cache:
            cached_results, cached_time = _search_cache[cache_key]
            if datetime.now() - cached_time < timedelta(hours=SEARCH_CACHE_TTL_HOURS):
                # ✅ Do not reuse empty cache entries
                if isinstance(cached_results, list) and len(cached_results) == 0:
                    return None
                return cached_results
            # expired
            del _search_cache[cache_key]
    except Exception:
        return None
    return None


def cache_search_results(query: str, results: List[Dict]):
    """
    Cache search results.

    IMPORTANT:
    - Do NOT cache empty lists
    - Do NOT cache lists that contain no usable URLs
      (prevents "poisoned cache" that makes SerpAPI appear broken)
    """
    try:
        if not isinstance(query, str) or not query.strip():
            return
        if not isinstance(results, list) or not results:
            return

        # Require at least one usable url/link
        has_url = False
        for r in results:
            if isinstance(r, dict):
                u = (r.get("link") or r.get("url") or "").strip()
                if u:
                    has_url = True
                    break
            elif isinstance(r, str) and r.strip():
                has_url = True
                break

        if not has_url:
            return

        cache_key = get_search_cache_key(query)
        _search_cache[cache_key] = (results, datetime.now())
    except Exception:
        return


# =========================================================
# LLM RESPONSE CACHE - Prevents variance on identical inputs
# =========================================================
_llm_cache: Dict[str, Tuple[str, datetime]] = {}
LLM_CACHE_TTL_HOURS = 24  # Cache LLM responses for 24 hours

def get_llm_cache_key(query: str, web_context: Dict) -> str:
    """Generate cache key from query + source URLs"""
    # Include source URLs so cache invalidates if sources change
    source_urls = sorted(web_context.get("sources", [])[:5])
    cache_input = f"{query.lower().strip()}|{'|'.join(source_urls)}"
    return hashlib.md5(cache_input.encode()).hexdigest()[:20]

def get_cached_llm_response(query: str, web_context: Dict) -> Optional[str]:
    """Get cached LLM response if still valid"""
    cache_key = get_llm_cache_key(query, web_context)
    if cache_key in _llm_cache:
        cached_response, cached_time = _llm_cache[cache_key]
        if datetime.now() - cached_time < timedelta(hours=LLM_CACHE_TTL_HOURS):
            return cached_response
        del _llm_cache[cache_key]
    return None

def cache_llm_response(query: str, web_context: Dict, response: str):
    """Cache LLM response"""
    cache_key = get_llm_cache_key(query, web_context)
    _llm_cache[cache_key] = (response, datetime.now())


def sort_results_deterministically(results: List[Dict]) -> List[Dict]:
    """Sort results for consistent ordering"""
    def sort_key(r):
        link = r.get("link", "").lower()
        # Priority: preferred domains first, then alphabetical
        priority = 999
        for i, domain in enumerate(PREFERRED_SOURCE_DOMAINS):
            if domain in link:
                priority = i
                break
        return (priority, link)
    return sorted(results, key=sort_key)


def classify_source_reliability(source: str) -> str:
    """Classify source as High/Medium/Low quality"""
    source = source.lower() if isinstance(source, str) else ""

    high = ["gov", "imf", "worldbank", "central bank", "fed", "ecb", "reuters", "spglobal", "economist", "mckinsey", "bcg", "cognitive market research",
            "financial times", "wsj", "oecd", "bloomberg", "tradingeconomics", "deloitte", "hsbc", "imarc", "booz allen", "bakerinstitute.org", "wef",
           "kpmg", "semiconductors.org", "eu", "iea", "world bank", "opec", "jpmorgan", "citibank", "goldmansachs", "j.p. morgan", "oecd",
           "world bank", "sec", "federalreserve", "bls", "bea"]
    medium = ["wikipedia", "forbes", "cnbc", "yahoo", "ceic", "statista", "trendforce", "digitimes", "idc", "gartner", "marketwatch", "fortune", "investopedia"]
    low = ["blog", "medium.com", "wordpress", "ad", "promo"]

    for h in high:
        if h in source:
            return "✅ High"
    for m in medium:
        if m in source:
            return "⚠️ Medium"
    for l in low:
        if l in source:
            return "❌ Low"

    return "⚠️ Medium"

def source_quality_score(sources: List[str]) -> float:
    """Calculate average source quality (0-100)"""
    if not sources:
        return 50.0  # Lower default when no sources

    weights = {"✅ High": 100, "⚠️ Medium": 60, "❌ Low": 30}
    scores = [weights.get(classify_source_reliability(s), 60) for s in sources]
    return sum(scores) / len(scores) if scores else 50.0

@st.cache_data(ttl=3600, show_spinner=False)
def search_serpapi(query: str, num_results: int = 10) -> List[Dict]:
    """Search Google via SerpAPI with stability controls"""
    if not SERPAPI_KEY:
        return []

    # Check cache first (this is the ONLY cache we use - removed @st.cache_data to avoid conflicts)
    cached = get_cached_search_results(query)
    if cached:
        st.info("📦 Using cached search results")
        return cached

    # Aggressive query normalization for consistent searches
    query_normalized = query.lower().strip()

    # Remove temporal words that cause variance
    query_normalized = re.sub(r'\b(latest|current|today|now|recent|new|upcoming|this year|this month)\b', '', query_normalized)

    # Normalize whitespace
    query_normalized = re.sub(r'\s+', ' ', query_normalized).strip()

    # Add year for consistency
    if not re.search(r'\b20\d{2}\b', query_normalized):
        query_normalized = f"{query_normalized} 2024"

    # Build search terms
    query_lower = query_normalized
    industry_kw = ["industry", "market", "sector", "size", "growth", "players"]

    if any(kw in query_lower for kw in industry_kw):
        search_terms = f"{query_normalized} market size growth statistics"
        tbm, tbs = "", ""  # Organic results (more stable than news)
    else:
        search_terms = f"{query_normalized} finance economics data"
        tbm, tbs = "", ""  # Use organic for stability

    params = {
        "engine": "google",
        "q": search_terms,
        "api_key": SERPAPI_KEY,
        "num": num_results,
        "tbm": tbm,
        "tbs": tbs,
        **SERPAPI_STABILITY_CONFIG  # Add fixed location params
    }

    try:
        resp = requests.get("https://serpapi.com/search", params=params, timeout=10)
        resp.raise_for_status()
        data = resp.json()

        results = []

        # Prefer organic results (more stable than news)
        for item in data.get("organic_results", [])[:num_results]:
            results.append({
                "title": item.get("title", ""),
                "link": item.get("link", ""),
                "snippet": item.get("snippet", ""),
                "date": item.get("date", ""),
                "source": item.get("source", "")
            })

        # Fall back to news only if no organic results
        if not results:
            for item in data.get("news_results", [])[:num_results]:
                src = item.get("source", {})
                source_name = src.get("name", "") if isinstance(src, dict) else str(src)
                results.append({
                    "title": item.get("title", ""),
                    "link": item.get("link", ""),
                    "snippet": item.get("snippet", ""),
                    "date": item.get("date", ""),
                    "source": source_name
                })

        # Sort deterministically
        results = sort_results_deterministically(results)
        results = results[:num_results]

        # Cache results
        if results:
            cache_search_results(query, results)

        return results

    except Exception as e:
        st.warning(f"⚠️ SerpAPI error: {e}")
        return []



# =====================================================================
# PATCH INJ_DIAG_HELPERS (ADDITIVE): Injected-URL diagnostics helpers
# - Pure helpers (no control-flow changes)
# - Used to trace injected extra URLs across: UI -> intake -> scrape -> snapshots -> hashing -> rebuild
# =====================================================================
def _inj_diag_make_run_id(prefix: str = "run") -> str:
    """Short correlation id for a single analysis/evolution run."""
    try:
        import os, time, hashlib
        seed = f"{prefix}|{time.time()}|{os.getpid()}|{os.urandom(8).hex()}"
        return hashlib.sha256(seed.encode("utf-8")).hexdigest()[:12]
    except Exception:
        try:
            import random
            return f"{prefix}_{random.randint(100000,999999)}"
        except Exception:
            return f"{prefix}_unknown"


# =====================================================================
# PATCH INJ_URL_CANON_V1 (ADDITIVE): Canonicalize injected URLs
# - Strips common tracking/query parameters from injected URLs ONLY
# - Keeps scheme/host/path; preserves non-tracking query params (sorted)
# - Adds deterministic canonical form for stable admission/dedupe/hashing
# =====================================================================
def _canonicalize_injected_url(url: str) -> str:
    """Canonicalize injected URLs by stripping known tracking params.

    This is intentionally conservative and applied only to user-injected URLs
    (extra URLs), not to SERP-derived URLs.
    """
    try:
        from urllib.parse import urlsplit, urlunsplit, parse_qsl, urlencode
        u = str(url or "").strip()
        if not u:
            return ""
        if not (u.startswith("http://") or u.startswith("https://")):
            return u

        parts = urlsplit(u)
        # Normalize scheme/host case
        scheme = (parts.scheme or "").lower()
        netloc = (parts.netloc or "").lower()
        path = parts.path or ""
        fragment = ""  # drop fragments for stability

        # Tracking params to drop (exact match)
        drop_exact = {
            "guccounter", "guce_referrer", "guce_referrer_sig",
            "gclid", "fbclid", "msclkid", "mc_cid", "mc_eid",
            "ref", "ref_src",
        }
        # Drop prefixes (utm_*, etc.)
        drop_prefixes = ("utm_",)

        qs = []
        for k, v in parse_qsl(parts.query or "", keep_blank_values=True):
            kk = (k or "").strip()
            if not kk:
                continue
            k_lower = kk.lower()
            if k_lower in drop_exact:
                continue
            if any(k_lower.startswith(p) for p in drop_prefixes):
                continue
            qs.append((kk, v))

        # Sort query params for determinism
        qs_sorted = sorted(qs, key=lambda kv: (kv[0].lower(), str(kv[1])))

        query = urlencode(qs_sorted, doseq=True) if qs_sorted else ""
        return urlunsplit((scheme, netloc, path, query, fragment))
    except Exception:
        try:
            return str(url or "").strip()
        except Exception:
            return ""

def _inj_diag_norm_url_list(extra_urls: Any) -> list:
    """Normalize/dedupe injected URL list (http/https only) with canonicalization.

    NOTE: This is used for injected/extra URL diagnostics and admission wiring only.
    It canonicalizes by stripping known tracking params for stability.
    """
    out = []
    try:
        if extra_urls is None:
            return []
        items = extra_urls
        if isinstance(items, str):
            items = [u.strip() for u in items.splitlines()]
        if not isinstance(items, (list, tuple, set)):
            items = [str(items)]
        seen = set()
        for u in items:
            uu = str(u or "").strip()
            if not uu:
                continue
            if not (uu.startswith("http://") or uu.startswith("https://")):
                continue
            cu = _canonicalize_injected_url(uu) or uu
            if cu in seen:
                continue
            seen.add(cu)
            out.append(cu)
    except Exception:
        return []
    return out


def _inj_diag_set_hash(urls: list) -> str:
    """Stable sha256 of sorted URL list (for compact logging)."""
    try:
        import hashlib
        lst = [str(u or "").strip() for u in (urls or []) if str(u or "").strip()]
        lst = sorted(set(lst))
        payload = "|".join(lst)
        return hashlib.sha256(payload.encode("utf-8")).hexdigest()
    except Exception:
        return ""

def _inj_diag_hash_inputs_from_bsc(baseline_sources_cache: Any) -> list:
    """Extract deterministic URL inputs used by snapshot hashing (v1/v2 both include URL)."""
    urls = []
    try:
        if not isinstance(baseline_sources_cache, list):
            return []
        for sr in baseline_sources_cache:
            if not isinstance(sr, dict):
                continue
            u = (sr.get("source_url") or sr.get("url") or "").strip()
            if u:
                urls.append(u)
    except Exception:
        return []
    return sorted(set(urls))

# =====================================================================
# PATCH INJ_HASH_V1 (ADDITIVE): optional inclusion of injected URLs in snapshot hash inputs
# Default behavior is OFF to avoid disrupting locked fastpath.
#
# When enabled, injected URLs that were persisted (per diag_injected_urls.persisted*)
# but are missing from baseline_sources_cache will be added as *synthetic* source
# records (url-only) so that:
#   - source_snapshot_hash (v1/v2) reflects injected sources deterministically
#   - evolution rebuild sees the same snapshot pool and hash identity via persistence
#
# Safety:
#   - Does NOT modify fastpath logic.
#   - Does NOT change metric selection (synthetic records have no extracted_numbers).
#   - Only activates when INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH is True.
# =====================================================================
INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH = False  # ✅ default OFF (locked fastpath safe)
CODE_VERSION_INJ_HASH_V1 = "fix41r_inj_hash_optional_include"  # additive version marker

def _inj_hash_should_include() -> bool:
    """Single switch for inclusion; additive-only. Supports env override."""
    try:
        import os
        v = os.getenv("YUREEKA_INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH", "").strip().lower()
        if v in ("1", "true", "yes", "y", "on"):
            return True
        if v in ("0", "false", "no", "n", "off"):
            return False
    except Exception:
        pass
    return bool(globals().get("INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH", False))

# =====================================================================
# PATCH INJ_HASH_POLICY_ALIGN_V1 (Additive, policy-aligned)
# Goal:
#   - Align injected URL "new data" identity semantics with baseline sources:
#       If an injected URL is PERSISTED as a successful snapshot, it should
#       participate in snapshot hash inputs by default (unless explicitly disabled).
#   - Preserve existing safety switch INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH
#     and its env override for emergency forcing.
#
# Controls:
#   - Default behavior (policy-aligned): ON when persisted injected URLs exist.
#   - Explicit disable: env YUREEKA_EXCLUDE_INJECTED_URLS_FROM_SNAPSHOT_HASH=1
#   - Explicit force include: env YUREEKA_INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH=1
#
# Notes:
#   - Fastpath logic is NOT modified.
#   - This only affects hash identity input construction; metric selection remains unchanged.
# =====================================================================
INJECTED_URL_HASH_POLICY_ALIGN_WITH_BASELINE = True  # ✅ default ON (policy-aligned)

def _inj_hash_policy_explicit_disable() -> bool:
    try:
        import os
        v = os.getenv("YUREEKA_EXCLUDE_INJECTED_URLS_FROM_SNAPSHOT_HASH", "").strip().lower()
        return v in ("1", "true", "yes", "y", "on")
    except Exception:
        return False

def _inj_hash_policy_should_include(persisted_injected_urls) -> bool:
    """Policy-aligned include decision for injected URLs in hash identity.

    - If explicitly disabled via env, returns False.
    - If explicitly forced via existing switch/env, returns True.
    - Otherwise, when policy-align is enabled and persisted injected URLs exist, returns True.
    - Else, falls back to legacy _inj_hash_should_include().
    """
    try:
        if _inj_hash_policy_explicit_disable():
            return False
        # Respect existing forcing mechanism first
        if _inj_hash_should_include():
            return True
        if bool(globals().get("INJECTED_URL_HASH_POLICY_ALIGN_WITH_BASELINE", True)) and (persisted_injected_urls or []):
            return True
    except Exception:
        pass
    return _inj_hash_should_include()

def _inj_hash_add_synthetic_sources(
    baseline_sources_cache: Any,
    injected_persisted_urls: list,
    now_iso: str = ""
) -> tuple:
    """
    Return (bsc_augmented, added_urls, reasons_by_url) without mutating the original list.
    Synthetic records are url-only, deterministic, and safe for selection logic.
    """
    reasons = {}
    added = []
    try:
        bsc = list(baseline_sources_cache) if isinstance(baseline_sources_cache, list) else []
        inj = _inj_diag_norm_url_list(injected_persisted_urls or [])
        if not inj:
            return (bsc, added, reasons)

        existing = set(_inj_diag_hash_inputs_from_bsc(bsc))
        for u in inj:
            if u in existing:
                reasons[u] = "present_in_bsc"
                continue
            # Add synthetic source record (no numbers) so hash includes the URL deterministically
            added.append(u)
            reasons[u] = "added_synthetic_for_hash"
            bsc.append({
                "url": u,
                "source_url": u,
                "status": "fetched",
                "status_detail": "synthetic_injected_for_hash",
                "numbers_found": 0,
                "fetched_at": now_iso or "",
                "fingerprint": "",
                "extracted_numbers": [],
                "__inj_synthetic": True,
            })

        # Keep deterministic ordering identical to existing conventions
        bsc = sorted(bsc, key=lambda x: str((x or {}).get("url") or ""))
        return (bsc, added, reasons)
    except Exception:
        try:
            return (list(baseline_sources_cache) if isinstance(baseline_sources_cache, list) else [], [], {})
        except Exception:
            return ([], [], {})

# =====================================================================
# PATCH INJ_TRACE_V1_HELPERS (ADDITIVE): canonical injected-URL lifecycle trace builder
# Objective:
# - Emit ONE canonical diagnostic payload in a fixed location for every run:
#     results.debug.inj_trace_v1  (analysis outputs)
#     results.debug.inj_trace_v1  (evolution outputs; mirrored from output.debug)
# - Purely additive; does NOT alter fastpath logic or selection control flow.
# =====================================================================
def _inj_trace_v1_build(
    diag_injected_urls: dict,
    hash_inputs: list,
    stage: str = "analysis",
    path: str = "",
    rebuild_pool: list = None,
    rebuild_selected: list = None,
    hash_exclusion_reasons: dict = None,
) -> dict:
    try:
        d = diag_injected_urls if isinstance(diag_injected_urls, dict) else {}
        ui_raw = d.get("ui_raw") if isinstance(d.get("ui_raw"), (str, list)) else (d.get("extra_urls_ui_raw") or "")
        ui_norm = _inj_diag_norm_url_list(d.get("ui_norm") or d.get("extra_urls_ui_norm") or d.get("extra_urls_normalized") or [])
        intake_norm = _inj_diag_norm_url_list(d.get("intake_norm") or d.get("extra_urls_intake_norm") or d.get("extra_urls") or [])
        admitted_norm = _inj_diag_norm_url_list(d.get("admitted") or d.get("extra_urls_admitted") or [])
        persisted_norm = _inj_diag_norm_url_list(d.get("persisted") or d.get("persisted_norm") or [])

        attempted = d.get("attempted") if isinstance(d.get("attempted"), list) else []
        # Keep attempted minimal and stable
        attempted_min = []
        for a in attempted:
            if not isinstance(a, dict):
                continue
            attempted_min.append({
                "url": str(a.get("url") or ""),
                "status": str(a.get("status") or a.get("fetch_status") or ""),
                "reason": str(a.get("reason") or a.get("fail_reason") or ""),
                "content_len": a.get("content_len"),
            })

        hash_inputs_norm = _inj_diag_norm_url_list(hash_inputs or [])
        rebuild_pool_norm = _inj_diag_norm_url_list(rebuild_pool or [])
        rebuild_selected_norm = _inj_diag_norm_url_list(rebuild_selected or [])

        # Deterministic deltas (set-based; small lists)
        def _delta(a, b):
            try:
                return sorted(list(set(a or []) - set(b or [])))[:100]
            except Exception:
                return []

        deltas = {
            "ui_minus_intake": _delta(ui_norm, intake_norm),
            "intake_minus_admitted": _delta(intake_norm, admitted_norm),
            "admitted_minus_attempted": _delta(admitted_norm, [x.get("url") for x in attempted_min if isinstance(x, dict)]),
            "attempted_minus_persisted": _delta([x.get("url") for x in attempted_min if isinstance(x, dict)], persisted_norm),
            "persisted_minus_hash_inputs": _delta(persisted_norm, hash_inputs_norm),
            "hash_inputs_minus_rebuild_pool": _delta(hash_inputs_norm, rebuild_pool_norm) if rebuild_pool is not None else [],
            "rebuild_pool_minus_selected": _delta(rebuild_pool_norm, rebuild_selected_norm) if rebuild_selected is not None else [],
        }


        # === PATCH EVO_INJ_ADMISSION_REASON_CODES_V1 START ===
        # Purpose: make evolution/analysis admission & selection drops explain themselves with stable reason codes.
        # Purely additive: diagnostics only (does not alter fastpath, hashing, scrape, or rebuild behavior).
        admission_rejection_reasons = {}
        attempted_rejection_reasons = {}
        try:
            # Prefer explicit per-URL decisions if present (from other EVO admission tracing patches)
            _decisions = d.get("inj_admission_decisions") or d.get("admission_decisions") or {}
            if isinstance(_decisions, dict):
                for _u, _v in _decisions.items():
                    if not _u:
                        continue
                    if isinstance(_v, dict):
                        _decision = str(_v.get("decision") or "")
                        _reason = str(_v.get("reason_code") or _v.get("reason") or "")
                    else:
                        _decision = str(_v or "")
                        _reason = ""
                    if _decision.lower().startswith("reject"):
                        admission_rejection_reasons[str(_u)] = _reason or "rejected_by_merge"
        except Exception:
            pass

        # Heuristic reason coding for intake→admitted drops
        for _u in (deltas.get("intake_minus_admitted") or []):
            if not _u:
                continue
            if _u in admission_rejection_reasons:
                continue
            _rsn = ""
            try:
                if not str(_u).startswith(("http://", "https://")):
                    _rsn = "invalid_scheme"
                elif str(stage) == "evolution" and str(path).startswith(("fastpath", "fastpath_replay")):
                    # In fastpath/replay, extra URLs may be visible but not admitted into the scrape/hash universe by policy.
                    _rsn = "fastpath_replay_no_admission"
                else:
                    _rsn = "unknown_rejected_pre_admission"
            except Exception:
                _rsn = "unknown_rejected_pre_admission"
            admission_rejection_reasons[str(_u)] = _rsn

        # Heuristic reason coding for admitted→attempted drops
        _attempted_urls = [x.get("url") for x in attempted_min if isinstance(x, dict) and x.get("url")]
        for _u in (deltas.get("admitted_minus_attempted") or []):
            if not _u:
                continue
            if str(stage) == "evolution" and str(path).startswith(("fastpath", "fastpath_replay")):
                attempted_rejection_reasons[str(_u)] = "fastpath_replay_no_fetch"
            else:
                attempted_rejection_reasons[str(_u)] = "not_fetched_or_filtered_before_fetch"

        # Policy/context snapshot (small + stable)
        try:
            import os as _os
            policy = {
                "exclude_injected_from_hash_env": str(_os.getenv("YUREEKA_EXCLUDE_INJECTED_URLS_FROM_SNAPSHOT_HASH") or ""),
                "force_include_injected_in_hash_env": str(_os.getenv("YUREEKA_INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH") or ""),
                "evolution_calls_fetch_web_context": d.get("evolution_calls_fetch_web_context"),
                "evolution_fastpath_allows_injection": False,
            }
        except Exception:
            policy = {}
        # === PATCH EVO_INJ_ADMISSION_REASON_CODES_V1 END ===
        return {
            "run_id": str(d.get("run_id") or ""),
            "stage": str(stage or ""),
            "path": str(path or ""),
            "ui_raw": ui_raw,
            "ui_norm": ui_norm,
            "intake_norm": intake_norm,
            "admitted_norm": admitted_norm,
            "attempted": attempted_min,
            "persisted_norm": persisted_norm,
            "hash_inputs_norm": hash_inputs_norm,
            "rebuild_pool_norm": rebuild_pool_norm if rebuild_pool is not None else None,
            "rebuild_selected_norm": rebuild_selected_norm if rebuild_selected is not None else None,
            "counts": {
                "ui_norm": int(len(ui_norm)),
                "intake_norm": int(len(intake_norm)),
                "admitted_norm": int(len(admitted_norm)),
                "attempted": int(len(attempted_min)),
                "persisted_norm": int(len(persisted_norm)),
                "hash_inputs_norm": int(len(hash_inputs_norm)),
                "rebuild_pool_norm": int(len(rebuild_pool_norm)) if rebuild_pool is not None else None,
                "rebuild_selected_norm": int(len(rebuild_selected_norm)) if rebuild_selected is not None else None,
            },
            "set_hashes": {
                "ui_norm": _inj_diag_set_hash(ui_norm),
                "intake_norm": _inj_diag_set_hash(intake_norm),
                "admitted_norm": _inj_diag_set_hash(admitted_norm),
                "persisted_norm": _inj_diag_set_hash(persisted_norm),
                "hash_inputs_norm": _inj_diag_set_hash(hash_inputs_norm),
                "rebuild_pool_norm": _inj_diag_set_hash(rebuild_pool_norm) if rebuild_pool is not None else "",
                "rebuild_selected_norm": _inj_diag_set_hash(rebuild_selected_norm) if rebuild_selected is not None else "",
            },
            "deltas": deltas,
            "rejection_reasons": {
                "intake_minus_admitted": admission_rejection_reasons,
                "admitted_minus_attempted": attempted_rejection_reasons,
            },
            "policy": policy,
        }
    except Exception:
        return {"stage": str(stage or ""), "path": str(path or ""), "error": "inj_trace_build_failed"}
# =====================================================================
# PATCH INJ_TRACE_V1_ENRICH_FROM_ARTIFACTS (ADDITIVE)
# Purpose:
# - Populate inj_trace_v1 attempted/persisted fields from *real* artifacts when
#   the upstream diag_injected_urls payload is partial (common in baseline/no-injection
#   or fastpath replay scenarios).
# - Pure diagnostics only: does NOT alter control flow, hashing, scraping, or selection.
#
# Artifacts supported:
#   - baseline_sources_cache (BSC): list of per-url snapshot dicts
#   - scraped_meta: dict keyed by url with status/status_detail/clean_text_len
# =====================================================================

def _inj_trace_v1_enrich_diag_from_bsc(diag: dict, baseline_sources_cache: list) -> dict:
    """Add attempted/persisted evidence into diag_injected_urls from baseline_sources_cache."""
    try:
        d = diag if isinstance(diag, dict) else {}
        bsc = baseline_sources_cache if isinstance(baseline_sources_cache, list) else []
        # If attempted already present, do not overwrite (avoid clobbering richer traces).
        if not isinstance(d.get("attempted"), list) or not d.get("attempted"):
            attempted = []
            for row in bsc:
                if not isinstance(row, dict):
                    continue
                u = str(row.get("url") or row.get("source_url") or "").strip()
                if not u:
                    continue
                st = str(row.get("status") or row.get("fetch_status") or "").strip() or "unknown"
                rs = str(row.get("status_detail") or row.get("fail_reason") or "").strip()
                clen = row.get("clean_text_len") or row.get("content_len") or 0
                try:
                    clen_i = int(clen)
                except Exception:
                    clen_i = 0
                attempted.append({"url": u, "status": st, "reason": rs, "content_len": clen_i})
            if attempted:
                d["attempted"] = attempted

        # Persisted: if missing, derive from successful snapshot rows in BSC.
        if not isinstance(d.get("persisted"), (list, str)) or not d.get("persisted"):
            persisted = []
            for row in bsc:
                if not isinstance(row, dict):
                    continue
                u = str(row.get("url") or row.get("source_url") or "").strip()
                if not u:
                    continue
                st = str(row.get("status") or row.get("fetch_status") or "").lower().strip()
                if st in ("success", "ok", "fetched"):
                    persisted.append(u)
            if persisted:
                d["persisted"] = _inj_diag_norm_url_list(persisted)

        return d
    except Exception:
        return diag if isinstance(diag, dict) else {}

def _inj_trace_v1_enrich_diag_from_scraped_meta(diag: dict, scraped_meta: dict, extra_urls: list) -> dict:
    """Add attempted/persisted evidence into diag_injected_urls from scraped_meta (evolution-side)."""
    try:
        d = diag if isinstance(diag, dict) else {}
        sm = scraped_meta if isinstance(scraped_meta, dict) else {}
        xs = _inj_diag_norm_url_list(extra_urls or [])
        if not xs:
            return d

        # attempted rows for injected urls
        if not isinstance(d.get("attempted"), list) or not d.get("attempted"):
            attempted = []
            for u in xs:
                m = sm.get(u) if isinstance(sm.get(u), dict) else {}
                st = str(m.get("status") or m.get("fetch_status") or "").strip() or "unknown"
                rs = str(m.get("status_detail") or m.get("fail_reason") or "").strip()
                clen = m.get("clean_text_len") or m.get("content_len") or 0
                try:
                    clen_i = int(clen)
                except Exception:
                    clen_i = 0
                attempted.append({"url": u, "status": st, "reason": rs, "content_len": clen_i})
            d["attempted"] = attempted

        # persisted success urls (only for injected)
        if not isinstance(d.get("persisted"), (list, str)) or not d.get("persisted"):
            persisted = []
            for a in (d.get("attempted") or []):
                if not isinstance(a, dict):
                    continue
                st = str(a.get("status") or "").lower().strip()
                if st in ("success", "ok", "fetched"):
                    u = str(a.get("url") or "").strip()
                    if u:
                        persisted.append(u)
            d["persisted"] = _inj_diag_norm_url_list(persisted)

        return d
    except Exception:
        return diag if isinstance(diag, dict) else {}
# =====================================================================

# =====================================================================

# =====================================================================

def scrape_url(url: str) -> Optional[str]:
    """
    Scrape webpage content.

    Priority:
      1) ScrapingDog (if SCRAPINGDOG_KEY is present)
      2) Safe fallback: direct requests + BeautifulSoup visible-text extraction

    Returns:
      - Clean visible text (<= 3000 chars) or None
    """
    import re

    url_s = (url or "").strip()
    if not url_s:
        return None

    def _clean_html_to_text(html: str) -> str:
        try:
            from bs4 import BeautifulSoup  # type: ignore
            soup = BeautifulSoup(html or "", "html.parser")
            for tag in soup(["script", "style", "nav", "footer", "header", "aside", "noscript", "form"]):
                try:
                    tag.decompose()
                except Exception:
                    pass
            txt = soup.get_text(separator="\n")
        except Exception:
            # fallback: strip tags
            txt = re.sub(r"(?is)<(script|style|noscript).*?>.*?</\1>", " ", html or "")
            txt = re.sub(r"(?is)<[^>]+>", " ", txt)
        # normalize whitespace
        lines = [ln.strip() for ln in (txt or "").splitlines() if ln.strip()]
        out = "\n".join(lines)
        out = re.sub(r"\n{3,}", "\n\n", out)
        return out.strip()

    def _direct_fetch(u: str) -> Optional[str]:
        try:
            headers = {
                "User-Agent": (
                    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/120.0.0.0 Safari/537.36"
                ),
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9",
                "Cache-Control": "no-cache",
                "Pragma": "no-cache",
            }
            resp = requests.get(u, headers=headers, timeout=12, allow_redirects=True)
            if resp.status_code >= 400:
                return None

            ctype = (resp.headers.get("Content-Type") or "").lower()
            if "application/pdf" in ctype:
                return None

            cleaned = _clean_html_to_text(resp.text or "")
            cleaned = cleaned.strip()
            if not cleaned:
                return None
            return cleaned[:3000]
        except Exception:
            return None

    # 1) ScrapingDog path (if configured)
    if globals().get("SCRAPINGDOG_KEY"):
        try:
            params = {"api_key": SCRAPINGDOG_KEY, "url": url_s, "dynamic": "false"}
            resp = requests.get("https://api.scrapingdog.com/scrape", params=params, timeout=15)
            if resp.status_code < 400:
                cleaned = _clean_html_to_text(resp.text or "").strip()
                if cleaned:
                    return cleaned[:3000]
        except Exception:
            pass  # fall through to direct fetch

    # 2) Safe fallback
    return _direct_fetch(url_s)


def fetch_web_context(
    query: str,
    num_sources: int = 3,
    *,
    fallback_mode: bool = False,
    fallback_urls: list = None,
    existing_snapshots: Any = None,   # <-- ADDITIVE
    # ============================================================
    # PATCH FWC_EXTRA_URLS1 (ADDITIVE)
    # ============================================================
    extra_urls: Any = None,
    # ============================================================
    # PATCH INJ_DIAG_FWC_ARGS (ADDITIVE): correlation + UI raw
    # ============================================================
    diag_run_id: str = "",
    diag_extra_urls_ui_raw: Any = None,
    # ============================================================
    # PATCH FWC_IDENTITY_ONLY1 (ADDITIVE): admission-only mode (no scraping)
    # ============================================================
    identity_only: bool = False,
    # ============================================================
    # PATCH FIX41AFC8 (ADDITIVE): force scrape extra_urls even if not admitted
    # - Default False: no behavior change.
    # - When True: normalized extra URLs will be appended to admitted list for scraping.
    # ============================================================
    force_scrape_extra_urls: bool = False,
    # ============================================================
    # PATCH FIX41AFC13 (ADDITIVE): force admit extra_urls into admitted list (pre-admission override)
    # - Default False: no behavior change.
    # - When True: normalized extra URLs will be appended to admitted list (not just scrape list),
    #   enabling deterministic admission of injected URLs when delta exists.
    # ============================================================
    force_admit_extra_urls: bool = False,
) -> dict:

    """
    Web context collector used by BOTH analysis + evolution.

    Enhancements:
    - Dashboard telemetry (sources found / HQ / admitted / scraped / success)
    - Keeps snapshot-friendly scraped_meta (fingerprint + extracted_numbers + numbers_found)
    - Uses scrape_url() which now has ScrapingDog + safe fallback scraper
    - Restores legacy contract: web_context["sources"] AND ["web_sources"]
    """
    import re
    from datetime import datetime, timezone

    def _now_iso() -> str:
        return datetime.now(timezone.utc).isoformat()

    def _is_probably_url(s: str) -> bool:
        if not s or not isinstance(s, str):
            return False
        t = s.strip()
        if " " in t:
            return False
        if re.match(r"^https?://", t, flags=re.I):
            return True
        if re.match(r"^[a-z0-9.-]+\.[a-z]{2,}(/.*)?$", t, flags=re.I):
            return True
        return False

    def _normalize_url(s: str) -> str:
        t = (s or "").strip()
        if not t:
            return ""
        if re.match(r"^https?://", t, flags=re.I):
            return t
        if re.match(r"^[a-z0-9.-]+\.[a-z]{2,}(/.*)?$", t, flags=re.I):
            return "https://" + t
        return ""


    out = {
        "query": query,
        "sources": [],        # ✅ legacy key many downstream blocks expect
        "web_sources": [],    # ✅ newer key used by evolution/snapshots
        "search_results": [],
        "scraped_meta": {},
        "scraped_content": {},
        "errors": [],
        "status": "ok",
        "status_detail": "",
        "fetched_at": _now_iso(),
        "debug_counts": {},   # ✅ telemetry for dashboard + JSON debugging
    }

    # ---- ADDITIVE: snapshot reuse lookup (Change #3) ----
    snap_lookup = {}
    if isinstance(existing_snapshots, dict):
        snap_lookup = existing_snapshots
    elif isinstance(existing_snapshots, list):
        for s in existing_snapshots:
            if isinstance(s, dict) and s.get("url"):
                snap_lookup[str(s.get("url")).strip()] = s

    extractor_fp = get_extractor_fingerprint()
    # ----------------------------------------------------


    q = (query or "").strip()
    if not q:
        out["status"] = "no_query"
        out["status_detail"] = "empty_query"
        return out

    # -----------------------------
    # 1) Search (SerpAPI) OR fallback_urls
    # -----------------------------
    search_results = []
    urls_raw = []

    if not fallback_mode:
        try:
            sr = search_serpapi(q, num_results=10) or []
            if isinstance(sr, list):
                search_results = sr
        except Exception as e:
            out["errors"].append(f"search_failed:{type(e).__name__}")
            search_results = []

        out["search_results"] = search_results

        # Extract urls from results
        for r in (search_results or []):
            if isinstance(r, dict):
                u = (r.get("link") or r.get("url") or "").strip()
                if _is_probably_url(u):
                    urls_raw.append(u)
            elif isinstance(r, str):
                if _is_probably_url(r):
                    urls_raw.append(r.strip())

    else:
        # Evolution fallback: use provided URLs
        if isinstance(fallback_urls, list):
            for u in fallback_urls:
                if isinstance(u, str) and _is_probably_url(u.strip()):
                    urls_raw.append(u.strip())

    # -----------------------------
    # 2) Compute "HQ" counts (like old version)
    # -----------------------------
    total_found = len(search_results) if not fallback_mode else len(urls_raw)
    hq_count = 0

    try:
        fn_rel = globals().get("classify_source_reliability")
        if callable(fn_rel) and not fallback_mode:
            for r in (search_results or []):
                if not isinstance(r, dict):
                    continue
                u = (r.get("link") or "").strip()
                if not u:
                    continue
                label = fn_rel(u) or ""
                if "✅" in str(label):
                    hq_count += 1
    except Exception:
        hq_count = 0

    # -----------------------------
    # 3) Sanitize + normalize + dedupe
    # -----------------------------
    normed = []
    seen = set()
    for u in (urls_raw or []):
        nu = _normalize_url(u)
        if not nu:
            continue
        if nu in seen:
            continue
        seen.add(nu)
        normed.append(nu)

    # admitted for scraping (top N)
    try:
        n = int(num_sources or 3)
    except Exception:
        n = 3
    n = max(1, min(12, n))
    admitted = normed[:n] if not fallback_mode else normed  # fallback_mode typically wants all

    # =====================================================================
    # PATCH FIX41AFC8 (ADDITIVE): Force-scrape normalized extra URLs even if admission filters drop them
    #
    # Why:
    # - In evolution injection scenarios, extra URLs may be deliberately outside the normal
    #   admitted universe (domain allowlists, heuristics, etc.), but the user's intent is
    #   to attempt a fetch so the run can either persist a snapshot or fail with a concrete reason.
    #
    # Behavior:
    # - When force_scrape_extra_urls=True and normalized extras exist, append them into the
    #   admitted list (deduped, stable order) so downstream scraping attempts occur.
    #
    # Safety:
    # - Default is False (no change for normal runs).
    # - Never raises.
    # =====================================================================
    try:
        if bool(force_scrape_extra_urls):
            _fx8_extras = []
            if "_extras" in locals() and isinstance(_extras, list):
                _fx8_extras = [u for u in _extras if isinstance(u, str) and u.strip()]
            if _fx8_extras and isinstance(admitted, list):
                _seen = set([u for u in admitted if isinstance(u, str)])
                for _u in _fx8_extras:
                    if _u not in _seen:
                        admitted.append(_u)
                        _seen.add(_u)
                # breadcrumb for diagnostics
                try:
                    out.setdefault("debug_counts", {})
                    if isinstance(out.get("debug_counts"), dict):
                        out["debug_counts"].setdefault("fix41afc8", {})
                        if isinstance(out["debug_counts"].get("fix41afc8"), dict):
                            out["debug_counts"]["fix41afc8"].update({
                                "force_scrape_extra_urls": True,
                                "force_scrape_extra_urls_count": int(len(_fx8_extras)),
                            })
                except Exception:
                    pass
    except Exception:
        pass


    # ============================================================
    # PATCH FWC_EXTRA_URLS2 (ADDITIVE)
    # ============================================================
    try:
        _extras_in = extra_urls or []
        _extras = []
        _canon_map = {}
        if isinstance(_extras_in, str):
            _extras_in = [u.strip() for u in _extras_in.splitlines()]
        if isinstance(_extras_in, (list, tuple)):
            for u in _extras_in:
                u = str(u or "").strip()
                if not u:
                    continue
                if not (u.startswith("http://") or u.startswith("https://")):
                    continue
                _canon = _canonicalize_injected_url(u) or u
                _extras.append(_canon)
                try:
                    _canon_map[u] = _canon
                except Exception:
                    pass
        _seen = set()
        merged = []
        for u in _extras + (admitted or []):
            if u in _seen:
                continue
            _seen.add(u)
            merged.append(u)
        admitted = merged
        out.setdefault("debug", {})
        if isinstance(out.get("debug"), dict):
            out["debug"].setdefault("fwc_extra_urls", {})
            out["debug"]["fwc_extra_urls"]["extra_urls_count"] = int(len(_extras))
            out["debug"]["fwc_extra_urls"]["admitted_count_after_merge"] = int(len(admitted or []))
            out["debug"]["fwc_extra_urls"]["extra_urls"] = _extras[:20]
    except Exception:
        pass


    # =====================================================================
    # PATCH INJ_DIAG_FWC_STAGE (ADDITIVE): injected-URL stage checkpoints (A1-A3)
    # Records: UI->intake->admitted, and later enriches with scrape outcomes.
    # =====================================================================
    try:
        _diag_run = str(diag_run_id or "") or _inj_diag_make_run_id("analysis")
        out["diag_run_id"] = out.get("diag_run_id") or _diag_run

        _ui_raw = diag_extra_urls_ui_raw if diag_extra_urls_ui_raw is not None else extra_urls
        _ui_norm = _inj_diag_norm_url_list(_ui_raw)
        _intake_norm = list(_extras or []) if "_extras" in locals() and isinstance(_extras, list) else _inj_diag_norm_url_list(extra_urls)

        out["diag_injected_urls"] = {
            "run_id": _diag_run,
            "ui_raw": _ui_raw if isinstance(_ui_raw, (str, list, tuple)) else str(_ui_raw or ""),
            "ui_norm": _ui_norm,
            "intake_norm": _intake_norm,
            "admitted": list(admitted or []),
            "attempted": [],
            "persisted": [],
            "hash_inputs": [],
            "rebuild_pool": [],
            "rebuild_selected": [],
            "set_hashes": {
                "ui_norm": _inj_diag_set_hash(_ui_norm),
                "intake_norm": _inj_diag_set_hash(_intake_norm),
                "admitted": _inj_diag_set_hash(list(admitted or [])),
            },
            "canon_map": dict(_canon_map) if "_canon_map" in locals() else {},
            "deltas": {
                "ui_minus_intake": sorted(list(set(_ui_norm) - set(_intake_norm))),
                "intake_minus_admitted": sorted(list(set(_intake_norm) - set(list(admitted or [])))),
            },
        }
    except Exception:
        pass
    # =====================================================================


    # =====================================================================
    # PATCH FIX41AFC13 (ADDITIVE): Pre-admission override for extra_urls (injection lane)
    #
    # Goal:
    # - When force_admit_extra_urls is True, ensure normalized extra_urls are INCLUDED in the
    #   admitted list itself (not only the scrape list). This prevents injected URLs from dying
    #   at "intake_minus_admitted" and allows deterministic fetch/persist behavior.
    #
    # Safety:
    # - Default flag False => no behavior change.
    # - Never raises.
    # =====================================================================
    try:
        if force_admit_extra_urls:
            _fix41afc13_extra = _inj_diag_norm_url_list(extra_urls) if extra_urls else []
            if _fix41afc13_extra:
                _fix41afc13_before = list(admitted or [])
                _fix41afc13_set = set(_inj_diag_norm_url_list(_fix41afc13_before))
                _fix41afc13_added = []
                for _u in _fix41afc13_extra:
                    if _u and _u not in _fix41afc13_set:
                        _fix41afc13_before.append(_u)
                        _fix41afc13_set.add(_u)
                        _fix41afc13_added.append(_u)
                if _fix41afc13_added:
                    admitted = _fix41afc13_before
                    out.setdefault("debug_counts", {})
                    if isinstance(out.get("debug_counts"), dict):
                        out["debug_counts"].update({
                            "forced_admit_extra_urls_count": int(len(_fix41afc13_added)),
                        })
                    out.setdefault("debug", {})
                    if isinstance(out.get("debug"), dict):
                        out["debug"].setdefault("fix41afc13", {})
                        if isinstance(out["debug"].get("fix41afc13"), dict):
                            out["debug"]["fix41afc13"].update({
                                "forced_admit_applied": True,
                                "forced_admit_added": list(_fix41afc13_added),
                                "forced_admit_total_extra": int(len(_fix41afc13_extra)),
                            })
    except Exception:
        pass
    # =====================================================================

    out["sources"] = admitted
    out["web_sources"] = admitted

    # Telemetry before scrape
    out["debug_counts"].update({
        "total_found": int(total_found),
        "high_quality": int(hq_count),
        "admitted_for_scraping": int(len(admitted)),
        "fallback_mode": bool(fallback_mode),
    })

    # Dashboard info (restored)
    try:
        if not fallback_mode:
            st.info(
                f"🔍 Sources Found: **{out['debug_counts']['total_found']} total** | "
                f"**{out['debug_counts']['high_quality']} high-quality** | "
                f"Scraping **{out['debug_counts']['admitted_for_scraping']}**"
            )
        else:
            st.info(
                f"🧩 Fallback Sources: **{out['debug_counts']['admitted_for_scraping']}** (no SerpAPI search)"
            )
    except Exception:
        pass

    if not admitted:
        out["status"] = "no_sources"
        out["status_detail"] = "empty_sources_after_filter"
        return out

    # ============================================================
    # PATCH FWC_IDENTITY_ONLY2 (ADDITIVE): identity-only early return
    # ============================================================
    try:
        if bool(identity_only):
            out["status"] = out.get("status") or "ok"
            out["status_detail"] = out.get("status_detail") or "identity_only"
            return out
    except Exception:
        pass




    # -----------------------------
    # 4) Scrape + extract numbers (snapshot-friendly scraped_meta)
    # -----------------------------
    fn_fp = globals().get("fingerprint_text")
    fn_extract = globals().get("extract_numbers_with_context") or globals().get("extract_numeric_candidates") or globals().get("extract_numbers_from_text")

    scraped_attempted = 0
    scraped_ok_text = 0
    scraped_ok_numbers = 0
    scraped_failed = 0

    # optional progress bar
    progress = None
    try:
        progress = st.progress(0)
    except Exception:
        progress = None

    for i, url in enumerate(admitted):
        scraped_attempted += 1

        meta = {
            "url": url,
            "fetched_at": _now_iso(),
            "status": "failed",
            "status_detail": "",
            "content_type": "",
            "content_len": 0,
            "clean_text_len": 0,
            "fingerprint": None,
            "numbers_found": 0,
            "extracted_numbers": [],
            "content": "",
            "clean_text": "",
        }

        try:
            text = scrape_url(url)  # ✅ ScrapingDog + fallback inside scrape_url
            if not text or not str(text).strip():
                meta["status"] = "failed"
                meta["status_detail"] = "failed:no_text"
                scraped_failed += 1
                out["scraped_meta"][url] = meta
            else:
                cleaned = str(text).strip()
                meta["status"] = "fetched"
                meta["status_detail"] = "success"
                meta["content"] = cleaned
                meta["clean_text"] = cleaned
                meta["content_len"] = len(cleaned)
                meta["clean_text_len"] = len(cleaned)

                # fingerprint
                try:
                    if callable(fn_fp):
                        meta["fingerprint"] = fn_fp(cleaned)
                    else:
                        meta["fingerprint"] = fingerprint_text(cleaned) if callable(globals().get("fingerprint_text")) else None
                except Exception:
                    meta["fingerprint"] = None

                # ---- ADDITIVE: reuse extracted_numbers when unchanged (Change #3) ----
                meta["extractor_fingerprint"] = extractor_fp
                prev = snap_lookup.get(url) if isinstance(snap_lookup, dict) else None
                if isinstance(prev, dict):
                    if prev.get("fingerprint") == meta.get("fingerprint") and prev.get("extractor_fingerprint") == extractor_fp:
                        prev_nums = prev.get("extracted_numbers")
                        if isinstance(prev_nums, list) and prev_nums:
                            meta["extracted_numbers"] = prev_nums
                            meta["numbers_found"] = len(prev_nums)
                            meta["reused_snapshot"] = True

                            out["scraped_meta"][url] = meta
                            out["scraped_content"][url] = cleaned

                            scraped_ok_text += 1
                            if meta["numbers_found"] > 0:
                                scraped_ok_numbers += 1

                            if progress:
                                try:
                                    progress.progress((i + 1) / max(1, len(admitted)))
                                except Exception:
                                    pass

                            continue
                meta["reused_snapshot"] = False
                # ---------------------------------------------------------------

                # numeric extraction (analysis-aligned if fn exists)
                nums = []
                try:
                    if callable(fn_extract):
                        nums = fn_extract(cleaned, url=url) if "url" in fn_extract.__code__.co_varnames else fn_extract(cleaned)
                except Exception:
                    nums = []

                if isinstance(nums, list):
                    meta["extracted_numbers"] = nums
                    meta["numbers_found"] = len(nums)

                    # ---- ADDITIVE: stable IDs + ordering (Change #2 / Part 1) ----
                    urlv = meta.get("url") or url
                    fpv = meta.get("fingerprint") or ""

                    for n in (meta["extracted_numbers"] or []):
                        if isinstance(n, dict):
                            if "extracted_number_id" not in n:
                                n["extracted_number_id"] = make_extracted_number_id(urlv, fpv, n)
                            if not n.get("source_url"):
                                n["source_url"] = urlv

                    meta["extracted_numbers"] = sort_snapshot_numbers(meta["extracted_numbers"])
                    meta["numbers_found"] = len(meta["extracted_numbers"])
                    # --------------------------------------------------------------

                out["scraped_meta"][url] = meta
                out["scraped_content"][url] = cleaned

                scraped_ok_text += 1
                if meta["numbers_found"] > 0:
                    scraped_ok_numbers += 1

        except Exception as e:
            meta["status"] = "failed"
            meta["status_detail"] = f"failed:exception:{type(e).__name__}"
            scraped_failed += 1
            out["scraped_meta"][url] = meta
            out["errors"].append(meta["status_detail"])

        if progress:
            try:
                progress.progress((i + 1) / max(1, len(admitted)))
            except Exception:
                pass


    # =====================================================================
    # PATCH INJ_DIAG_FWC_POSTSCRAPE (ADDITIVE): finalize scrape outcomes (A3)
    # =====================================================================
    try:
        d = out.get("diag_injected_urls")
        if isinstance(d, dict):
            _inj = set(d.get("intake_norm") or [])
            sm = out.get("scraped_meta") or {}
            attempted = []
            persisted = []
            if isinstance(sm, dict):
                for u in sorted(_inj):
                    meta = sm.get(u) or {}
                    status = (meta.get("status") or "")
                    status_detail = (meta.get("status_detail") or "")
                    content = meta.get("clean_text") or meta.get("content") or ""
                    attempted.append({
                        "url": u,
                        "attempted": bool(u in (admitted or [])),
                        "fetch_status": "success" if (str(status_detail).startswith("success") or status == "fetched") else ("failed" if meta else "skipped"),
                        "fail_reason": (str(status_detail) or str(status) or "")[:80],
                        "content_len": int(len(content) if isinstance(content, str) else 0),
                        "numbers_found": int(meta.get("numbers_found") or 0),
                    })
                    if str(status_detail).startswith("success") or status == "fetched":
                        persisted.append(u)
            d["attempted"] = attempted
            d["persisted"] = persisted
            d.setdefault("set_hashes", {})
            if isinstance(d["set_hashes"], dict):
                d["set_hashes"]["persisted"] = _inj_diag_set_hash(persisted)
    except Exception:
        pass
    # =====================================================================

    out["debug_counts"].update({
        "scraped_attempted": int(scraped_attempted),
        "scraped_ok_text": int(scraped_ok_text),
        "scraped_ok_numbers": int(scraped_ok_numbers),
        "scraped_failed": int(scraped_failed),
    })

    # Dashboard scrape summary
    try:
        st.info(
            f"🧽 Scrape Results: **{out['debug_counts']['scraped_ok_text']} ok-text** | "
            f"**{out['debug_counts']['scraped_ok_numbers']} ok-numbers** | "
            f"**{out['debug_counts']['scraped_failed']} failed**"
        )
    except Exception:
        pass

    # status summarization

    # =====================================================================
    # PATCH FWC_EXTRA_URLS_TRACE2 (ADDITIVE): trace how injected URLs were handled
    # Why:
    # - When scenario B "extra URLs" are provided, it can be unclear whether they:
    #     (a) were normalized/deduped
    #     (b) were admitted into the scrape list
    #     (c) were successfully scraped
    #     (d) actually entered the snapshot-hash pool used by analysis/evolution
    # - This patch records a deterministic, non-invasive trace in web_context only.
    # =====================================================================
    try:
        if not isinstance(out.get("debug_counts"), dict):
            out["debug_counts"] = {}
        _dbg_counts = out["debug_counts"]

        _extra_trace = {
            "extra_urls_requested": list(extra_urls or []) if isinstance(extra_urls, list) else [],
            "extra_urls_normalized": list(_extras or []) if "_extras" in locals() and isinstance(_extras, list) else [],
            "extra_urls_admitted": [],
            "extra_urls_scraped": [],
            "extra_urls_in_hash_pool": [],
            "notes": [],
        }

        # Which extras actually made it into the final admitted URL list?
        try:
            _admitted_urls = []
            if "admitted" in locals() and isinstance(admitted, list):
                _admitted_urls = [u for u in admitted if isinstance(u, str) and u.strip()]
            _extra_set = set(_extra_trace["extra_urls_normalized"])
            _extra_trace["extra_urls_admitted"] = [u for u in _admitted_urls if u in _extra_set]
        except Exception:
            pass

        # How did each extra URL scrape?
        try:
            sm = out.get("scraped_meta") or {}
            if isinstance(sm, dict):
                for u in _extra_trace["extra_urls_normalized"]:
                    meta = sm.get(u) or {}
                    if isinstance(meta, dict) and meta:
                        content = meta.get("clean_text") or meta.get("content") or ""
                        fp = meta.get("fingerprint")
                        _extra_trace["extra_urls_scraped"].append({
                            "url": u,
                            "status": meta.get("status"),
                            "status_detail": meta.get("status_detail"),
                            "fingerprint": (fp[:16] if isinstance(fp, str) else fp),
                            "numbers_found": meta.get("numbers_found"),
                            "content_len": (len(content) if isinstance(content, str) else 0),
                            "content_type": meta.get("content_type") or "",
                        })
        except Exception:
            pass

        # Approximate "hash pool" membership (non-invasive):
        # we mark extras whose scrape produced a non-empty fingerprint + some text.
        try:
            sm = out.get("scraped_meta") or {}
            if isinstance(sm, dict):
                for row in (_extra_trace.get("extra_urls_scraped") or []):
                    u = row.get("url")
                    meta = sm.get(u) or {}
                    content = meta.get("clean_text") or meta.get("content") or ""
                    fp = meta.get("fingerprint")
                    if isinstance(fp, str) and fp and isinstance(content, str) and len(content) >= 200:
                        _extra_trace["extra_urls_in_hash_pool"].append(u)
        except Exception:
            pass

        out["extra_urls_debug"] = _extra_trace
        _dbg_counts["extra_urls_trace"] = {
            "requested": len(_extra_trace.get("extra_urls_requested") or []),
            "normalized": len(_extra_trace.get("extra_urls_normalized") or []),
            "admitted": len(_extra_trace.get("extra_urls_admitted") or []),
            "scraped": len(_extra_trace.get("extra_urls_scraped") or []),
            "in_hash_pool": len(_extra_trace.get("extra_urls_in_hash_pool") or []),
        }
    except Exception:
        pass
    # =====================================================================
    if scraped_ok_text == 0:
        out["status"] = "failed"
        out["status_detail"] = "no_usable_text"
    elif scraped_ok_numbers == 0:
        out["status"] = "partial"
        out["status_detail"] = "text_ok_numbers_empty"
    else:
        out["status"] = "success"
        out["status_detail"] = "ok"

    return out



def fingerprint_text(text: str) -> str:
    """Stable short fingerprint for fetched content (for debugging + determinism checks)."""
    if not text:
        return ""
    normalized = re.sub(r"\s+", " ", text.strip().lower())
    return hashlib.md5(normalized.encode("utf-8")).hexdigest()[:12]

def unit_clean_first_letter(unit: str) -> str:
    """Normalize units to first letter (T/B/M/K/%), ignoring $ and spaces."""
    if not unit:
        return ""
    u = unit.replace("$", "").replace(" ", "").strip().upper()
    return u[0] if u else ""

# =========================================================
# 7. LLM QUERY FUNCTIONS
# =========================================================

def query_perplexity(query: str, web_context: Dict, query_structure: Optional[Dict[str, Any]] = None) -> Optional[str]:
    """
    Query Perplexity and return a validated JSON string (LLMResponse-compatible).
    Removes 'action' and excludes None fields from output JSON.
    """
    if not PERPLEXITY_KEY:
        st.error("❌ PERPLEXITY_KEY not set.")
        return None

    query_structure = query_structure or {}
    structure_txt = ""
    ordering_contract = ""

    try:
        structure_txt, ordering_contract = build_query_structure_prompt(query_structure)
    except Exception:
        structure_txt = ""
        ordering_contract = ""

    # Web context: show top sources + snippets
    sources = (web_context.get("sources", []) if isinstance(web_context, dict) else []) or []
    search_results = (web_context.get("search_results", []) if isinstance(web_context, dict) else []) or []
    search_count = int(web_context.get("search_count", len(search_results)) if isinstance(web_context, dict) else 0)

    context_section = "WEB CONTEXT:\n"
    for url in sources[:6]:
        content = (web_context.get("scraped_content", {}) or {}).get(url) if isinstance(web_context, dict) else None
        if content:
            context_section += f"\n{url}:\n{str(content)[:800]}...\n"
        else:
            context_section += f"\n{url}\n"

    enhanced_query = (
        f"{context_section}\n"
        f"{SYSTEM_PROMPT}\n\n"
        f"User Question: {query}\n\n"
        f"{structure_txt}\n\n"
        f"{ordering_contract}\n"
        f"Web search returned {search_count} results.\n"
        f"Return ONLY valid JSON matching the template and include all required fields."
    )

    headers = {
        "Authorization": f"Bearer {PERPLEXITY_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "sonar",
        "temperature": 0.0,
        "max_tokens": 2400,
        "top_p": 1.0,
        "messages": [{"role": "user", "content": enhanced_query}]
    }

    try:
        resp = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=45)
        resp.raise_for_status()
        data = resp.json()

        if "choices" not in data:
            raise Exception("No 'choices' in Perplexity response")

        content = data["choices"][0]["message"]["content"]
        if not content or not content.strip():
            raise Exception("Empty Perplexity response")

        parsed = parse_json_safely(content, "Perplexity")
        if not parsed:
            return create_fallback_response(query, search_count, web_context)

        repaired = repair_llm_response(parsed)

        # Ensure action is removed even if present
        repaired.pop("action", None)

        validate_numeric_fields(repaired, "Perplexity")

        try:
            llm_obj = LLMResponse.model_validate(repaired)

            # Ensure action not present (belt + suspenders)
            if hasattr(llm_obj, "action"):
                llm_obj.action = None

            # Merge web sources
            if isinstance(web_context, dict) and web_context.get("sources"):
                existing = llm_obj.sources or []
                merged = list(dict.fromkeys(existing + web_context["sources"]))
                llm_obj.sources = merged[:10]
                llm_obj.freshness = "Current (web-enhanced)"

            result = llm_obj.model_dump_json(exclude_none=True)
            cache_llm_response(query, web_context, result)
            return result

        except ValidationError as e:
            st.warning(f"⚠️ Pydantic validation failed: {e}")
            return create_fallback_response(query, search_count, web_context)

    except Exception as e:
        st.error(f"❌ Perplexity API error: {e}")
        return create_fallback_response(query, search_count, web_context)


def query_perplexity_raw(prompt: str, max_tokens: int = 400, timeout: int = 30) -> str:
    """
    Raw Perplexity call that returns text only.
    IMPORTANT: Does NOT attempt to validate as LLMResponse.
    """
    headers = {
        "Authorization": f"Bearer {PERPLEXITY_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "sonar",
        "temperature": 0.0,
        "top_p": 1.0,
        "max_tokens": max_tokens,
        "messages": [{"role": "user", "content": prompt}],
    }

    resp = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=timeout)
    resp.raise_for_status()
    data = resp.json()
    return (data.get("choices", [{}])[0].get("message", {}) or {}).get("content", "") or ""

def create_fallback_response(query: str, search_count: int, web_context: Dict) -> str:
    """Create fallback response matching schema, excluding None fields and removing action."""
    fallback = LLMResponse(
        executive_summary=f"Analysis of '{query}' completed with {search_count} web sources. Schema validation used fallback structure.",
        primary_metrics={
            "sources": MetricDetail(name="Web Sources", value=search_count, unit="sources"),
            "quality": MetricDetail(name="Data Quality", value=70, unit="%")
        },
        key_findings=[
            f"Web search found {search_count} relevant sources.",
            "Primary model output required fallback due to format issues.",
            "Manual review of raw data recommended for accuracy."
        ],
        top_entities=[
            TopEntityDetail(name="Source 1", share="N/A", growth="N/A")
        ],
        trends_forecast=[
            TrendForecastDetail(trend="Schema validation used fallback", direction="⚠️", timeline="Now")
        ],
        visualization_data=VisualizationData(
            chart_labels=["Attempt"],
            chart_values=[search_count],
            chart_title="Search Results"
        ),
        sources=web_context.get("sources", []),
        confidence=60,
        freshness="Current (fallback)",
        action=None
    )

    return fallback.model_dump_json(exclude_none=True)


# =========================================================
# 7B. ANCHORED EVOLUTION QUERY
# =========================================================

def _ensure_metric_labels(metric_changes: list) -> list:
    """
    Backward/forward compatible label normalization:
    - guarantees a non-empty display label
    - adds aliases so different UIs render correctly: metric_name, metric, label
    """
    import re

    def _prettify(s: str) -> str:
        s = str(s or "").strip()
        if not s:
            return ""
        s = s.replace("__", " ").replace("_", " ")
        s = re.sub(r"\s+", " ", s).strip()
        return s[:120]

    out = []
    for row in (metric_changes or []):
        if not isinstance(row, dict):
            continue

        name = row.get("name")
        if isinstance(name, str):
            name = name.strip()
        else:
            name = ""

        # try to derive a label if name missing (canonical_key or metric_definition.name)
        if not name:
            md = row.get("metric_definition") if isinstance(row.get("metric_definition"), dict) else {}
            name = (md.get("name") or "").strip() if isinstance(md.get("name"), str) else ""
        if not name:
            ckey = row.get("canonical_key")
            name = _prettify(ckey) if ckey else "Unknown Metric"

        # write canonical label + aliases
        row["name"] = name
        row.setdefault("metric_name", name)
        row.setdefault("metric", name)
        row.setdefault("label", name)

        out.append(row)

    return out


def format_previous_metrics(metrics: Dict) -> str:
    """Format previous metrics for prompt"""
    if not metrics:
        return "No previous metrics available"

    lines = []
    for key, m in metrics.items():
        if isinstance(m, dict):
            lines.append(f"- {m.get('name', key)}: {m.get('value', 'N/A')} {m.get('unit', '')}")
    return "\n".join(lines) if lines else "No metrics"

def format_previous_entities(entities: List) -> str:
    """Format previous entities for prompt"""
    if not entities:
        return "No previous entities available"

    lines = []
    for i, e in enumerate(entities, 1):
        if isinstance(e, dict):
            lines.append(f"{i}. {e.get('name', 'Unknown')}: {e.get('share', 'N/A')} share, {e.get('growth', 'N/A')} growth")
    return "\n".join(lines) if lines else "No entities"

def format_previous_findings(findings: List) -> str:
    """Format previous findings for prompt"""
    if not findings:
        return "No previous findings available"

    lines = [f"- {f}" for f in findings if f]
    return "\n".join(lines) if lines else "No findings"

def calculate_time_ago(timestamp_str: str) -> str:
    """Calculate human-readable time difference"""
    try:
        prev_time = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
        delta = datetime.now() - prev_time.replace(tzinfo=None)

        hours = delta.total_seconds() / 3600
        if hours < 24:
            return f"{hours:.1f} hours ago"
        elif hours < 168:  # 7 days
            return f"{hours/24:.1f} days ago"
        elif hours < 720:  # 30 days
            return f"{hours/168:.1f} weeks ago"
        else:
            return f"{hours/720:.1f} months ago"
    except:
        return "unknown time ago"

def query_perplexity_anchored(query: str, previous_data: Dict, web_context: Dict, temperature: float = 0.1) -> str:
    """
    Query Perplexity with previous analysis as anchor.
    This produces an evolution-aware response that tracks changes.
    """

    prev_response = previous_data.get("primary_response", {})
    prev_timestamp = previous_data.get("timestamp", "")
    prev_question = previous_data.get("question", query)

    time_ago = calculate_time_ago(prev_timestamp)

    # Build the anchored prompt
    anchored_prompt = EVOLUTION_PROMPT_TEMPLATE.format(
        time_ago=time_ago,
        previous_question=prev_question,
        previous_timestamp=prev_timestamp,
        previous_summary=prev_response.get("executive_summary", "No previous summary"),
        previous_metrics=format_previous_metrics(prev_response.get("primary_metrics", {})),
        previous_entities=format_previous_entities(prev_response.get("top_entities", [])),
        previous_findings=format_previous_findings(prev_response.get("key_findings", [])),
        query=query
    )

    # Add web context if available
    if web_context.get("summary"):
        anchored_prompt = f"CURRENT WEB RESEARCH:\n{web_context['summary']}\n\n{anchored_prompt}"

    # API request
    headers = {
        "Authorization": f"Bearer {PERPLEXITY_KEY}",
        "Content-Type": "application/json"
    }


    payload = {
        "model": "sonar",
        "temperature": 0.0,      # DETERMINISTIC
        "max_tokens": 2500,
        "top_p": 1.0,            # DETERMINISTIC
        "messages": [{"role": "user", "content": anchored_prompt}]
    }

    try:
        resp = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=60)
        resp.raise_for_status()
        data = resp.json()

        if "choices" not in data:
            raise Exception("No choices in response")

        content = data["choices"][0]["message"]["content"]
        if not content:
            raise Exception("Empty response")

        # Parse JSON
        parsed = parse_json_safely(content, "Perplexity-Anchored")
        if not parsed:
            return create_anchored_fallback(query, previous_data, web_context)

        # Add sources from web context
        if web_context.get("sources"):
            existing = parsed.get("sources", [])
            parsed["sources"] = list(dict.fromkeys(existing + web_context["sources"]))[:10]

        return json.dumps(parsed)

    except Exception as e:
        st.error(f"❌ Anchored query error: {e}")
        return create_anchored_fallback(query, previous_data, web_context)

def create_anchored_fallback(query: str, previous_data: Dict, web_context: Dict) -> str:
    """Create fallback for anchored evolution query"""
    prev_response = previous_data.get("primary_response", {})

    fallback = {
        "executive_summary": f"Evolution analysis for '{query}' - model returned invalid format. Showing previous data.",
        "analysis_delta": {
            "time_since_previous": calculate_time_ago(previous_data.get("timestamp", "")),
            "overall_trend": "unknown",
            "major_changes": ["Unable to determine changes - API error"],
            "data_freshness": "Unknown"
        },
        "primary_metrics": prev_response.get("primary_metrics", {}),
        "key_findings": ["[UNCHANGED] " + f for f in prev_response.get("key_findings", [])[:3]],
        "top_entities": prev_response.get("top_entities", []),
        "trends_forecast": prev_response.get("trends_forecast", []),
        "sources": web_context.get("sources", []),
        "confidence": 50,
        "freshness": "Fallback",
        "drift_summary": {
            "metrics_changed": 0,
            "metrics_unchanged": len(prev_response.get("primary_metrics", {})),
            "entities_reshuffled": 0,
            "findings_updated": 0,
            "overall_stability_pct": 100
        }
    }
    return json.dumps(fallback)

# =========================================================
# 8. VALIDATION & SCORING
# =========================================================


def parse_number_with_unit(val_str: str) -> float:
    """
    Parse a numeric string into a comparable base scale.
    Returns a float in "millions" for currency/volume-like values.
    Percentages are returned as their numeric value (e.g., "9.8%" -> 9.8).

    Handles:
      - $58.3B, 58.3B, S$29.8B, 29.8 S$B, USD 21.18 B
      - 58.3 billion, 58.3 bn, 58.3 million, 58.3 mn, 570 thousand
      - 570,000 (interpreted as an absolute count -> converted to millions)
      - 9.8% (kept as 9.8)
    """
    if val_str is None:
        return 0.0

    s = str(val_str).strip()
    if not s:
        return 0.0

    s_low = s.lower()

    # If it's a percentage, return the raw percent number (not millions)
    if "%" in s_low:
        m = re.search(r'(-?\d+(?:\.\d+)?)', s_low)
        if not m:
            return 0.0
        try:
            return float(m.group(1))
        except Exception:
            return 0.0

    # Normalize: remove commas and common currency tokens/symbols
    # (keep letters because we need bn/mn/b/m/k detection)
    s_low = s_low.replace(",", " ")
    for token in ["s$", "usd", "sgd", "us$", "$", "€", "£", "aud", "cad"]:
        s_low = s_low.replace(token, " ")

    # Collapse whitespace
    s_low = re.sub(r"\s+", " ", s_low).strip()

    # Extract the first number
    m = re.search(r'(-?\d+(?:\.\d+)?)', s_low)
    if not m:
        return 0.0

    try:
        num = float(m.group(1))
    except Exception:
        return 0.0

    # Look at the remaining text after the number for unit words/suffix
    tail = s_low[m.end():].strip()

    # Decide multiplier (base = millions)
    # billions -> *1000, millions -> *1, thousands -> *0.001
    multiplier = 1.0

    # Word-based units
    if re.search(r'\b(trillion|tn)\b', tail):
        multiplier = 1_000_000.0  # trillion -> million
    elif re.search(r'\b(billion|bn)\b', tail):
        multiplier = 1000.0
    elif re.search(r'\b(million|mn)\b', tail):
        multiplier = 1.0
    elif re.search(r'\b(thousand|k)\b', tail):
        multiplier = 0.001
    else:
        # Suffix-style units (possibly with spaces), e.g. "29.8 b", "21.18 b", "58.3m"
        # We only look at the very first letter-ish token in tail.
        t0 = tail[:4].strip()  # enough to catch "b", "m", "k"
        if t0.startswith("b"):
            multiplier = 1000.0
        elif t0.startswith("m"):
            multiplier = 1.0
        elif t0.startswith("k"):
            multiplier = 0.001
        else:
            # No unit detected. If it's a big integer like 570000 (jobs, people),
            # interpret as an absolute count and convert to millions.
            # (570000 -> 0.57 million)
            if abs(num) >= 10000 and float(num).is_integer():
                multiplier = 1.0 / 1_000_000.0
            else:
                multiplier = 1.0

    return num * multiplier


def numeric_consistency_with_sources(primary_data: dict, web_context: dict) -> float:
    """Compare primary numbers vs source numbers"""
    primary_metrics = primary_data.get("primary_metrics", {})
    primary_numbers = []

    for metric in primary_metrics.values():
        if isinstance(metric, dict):
            val = metric.get("value")
            num = parse_number_with_unit(str(val))
            if num > 0:
                primary_numbers.append(num)

    if not primary_numbers:
        return 50.0  # Neutral when no metrics to compare

    # Extract source numbers with same parsing
    source_numbers = []
    search_results = web_context.get("search_results", [])

    for result in search_results:
        snippet = str(result.get("snippet", ""))
        # Match patterns like "$58.3B", "123M", "456 billion"
        patterns = [
            r'\$?(\d+(?:\.\d+)?)\s*([BbMmKk])',  # $58.3B
            r'(\d+(?:\.\d+)?)\s*(billion|million|thousand)',  # 58.3 billion
        ]

        for pattern in patterns:
            matches = re.findall(pattern, snippet, re.IGNORECASE)
            for num, unit in matches:
                source_numbers.append(parse_number_with_unit(f"{num}{unit[0].upper()}"))

    if not source_numbers:
        return 50.0  # Neutral when no source numbers found

    # Check agreement (within 25% tolerance)
    agreements = 0
    for p_num in primary_numbers:
        for s_num in source_numbers:
            if abs(p_num - s_num) / max(p_num, s_num, 1) < 0.25:
                agreements += 1
                break

    # Scale: 0 agreements = 30%, all agreements = 95%
    agreement_ratio = agreements / len(primary_numbers)
    agreement_pct = 30.0 + (agreement_ratio * 65.0)
    return min(agreement_pct, 95.0)

def numeric_consistency_with_sources_v2(primary_data: dict, web_context: dict) -> float:
    """
    Stable numeric consistency (0-100):
    - Evidence text: search_results snippets + web_context summary + scraped_content
    - Unit-aware parsing via parse_number_with_unit()
    - Range-aware (supports min/max if metric has a 'range' dict)
    - Downweights proxy metrics (is_proxy=True) so they don't tank the score
    """

    try:
        # Prefer canonical metrics if available (has is_proxy, range, etc.)
        metrics = primary_data.get("primary_metrics_canonical") or primary_data.get("primary_metrics") or {}
        if not isinstance(metrics, dict) or not metrics:
            return 50.0

        # -----------------------------
        # Build evidence text corpus
        # -----------------------------
        texts = []

        # 1) snippets
        sr = (web_context or {}).get("search_results") or []
        if isinstance(sr, list):
            for r in sr:
                if isinstance(r, dict):
                    snip = r.get("snippet", "")
                    if isinstance(snip, str) and snip.strip():
                        texts.append(snip)

        # 2) summary
        summary = (web_context or {}).get("summary") or ""
        if isinstance(summary, str) and summary.strip():
            texts.append(summary)

        # 3) scraped_content
        scraped = (web_context or {}).get("scraped_content") or {}
        if isinstance(scraped, dict):
            for _, content in scraped.items():
                if isinstance(content, str) and content.strip():
                    texts.append(content)

        evidence_text = "\n".join(texts)
        if not evidence_text.strip():
            return 45.0  # no evidence stored

        # -----------------------------
        # Extract numeric candidates from evidence text
        # -----------------------------
        # Keep this broad; parse_number_with_unit will normalize.
        patterns = [
            r'\$?\s?\d{1,3}(?:,\d{3})*(?:\.\d+)?\s*[BbMmKk]\b',                 # 29.8B, 570K, 1.2M
            r'\$?\s?\d+(?:\.\d+)?\s*(?:billion|million|thousand|bn|mn)\b',      # 29.8 billion, 29.8 bn
            r'\b\d{1,3}(?:,\d{3})+(?:\.\d+)?\b',                               # 570,000
            r'\b\d+(?:\.\d+)?\s*%\b',                                          # 9.8%
        ]

        evidence_numbers = []
        lowered = evidence_text.lower()

        for pat in patterns:
            for m in re.findall(pat, lowered, flags=re.IGNORECASE):
                n = parse_number_with_unit(str(m))
                if n and n > 0:
                    evidence_numbers.append(n)

        # If nothing extracted, don’t penalize too hard
        if not evidence_numbers:
            return 50.0

        # -----------------------------
        # Verify each metric against evidence numbers (tolerance match)
        # -----------------------------
        def _metric_candidates(m: dict) -> list:
            """Return list of candidate numeric values for a metric (range-aware)."""
            out = []
            if not isinstance(m, dict):
                return out

            # Range support: check min/max if present
            rng = m.get("range") if isinstance(m.get("range"), dict) else None
            if rng:
                if rng.get("min") is not None:
                    out.append(rng.get("min"))
                if rng.get("max") is not None:
                    out.append(rng.get("max"))

            # Also check main value
            if m.get("value") is not None:
                out.append(m.get("value"))

            return out

        def _parse_metric_num(val, unit_hint: str = "") -> float:
            # build a value+unit string so parse_number_with_unit has a chance
            if val is None:
                return 0.0
            s = str(val)
            if unit_hint and unit_hint.lower() not in s.lower():
                s = f"{s} {unit_hint}"
            return parse_number_with_unit(s)

        def _is_supported(target: float, evidence_nums: list, rel_tol: float = 0.25) -> bool:
            # same tolerance approach as v1 (25%)
            if not target or target <= 0:
                return False
            for e in evidence_nums:
                if e <= 0:
                    continue
                if abs(target - e) / max(target, e, 1) < rel_tol:
                    return True
            return False

        supported_w = 0.0
        total_w = 0.0

        for _, m in metrics.items():
            if not isinstance(m, dict):
                continue

            unit = str(m.get("unit") or "").strip()

            # proxy weighting
            is_proxy = bool(m.get("is_proxy"))
            w = 0.5 if is_proxy else 1.0

            cands = _metric_candidates(m)
            if not cands:
                continue

            # parse candidates into numeric values
            parsed_targets = []
            for c in cands:
                n = _parse_metric_num(c, unit_hint=unit)
                if n and n > 0:
                    parsed_targets.append(n)

            if not parsed_targets:
                continue

            total_w += w

            # supported if ANY candidate matches evidence
            if any(_is_supported(t, evidence_numbers, rel_tol=0.25) for t in parsed_targets):
                supported_w += w

        if total_w <= 0:
            return 50.0

        ratio = supported_w / total_w
        # Map: keep a soft floor so one miss doesn't tank the whole run
        score = 30.0 + (ratio * 65.0)  # same scale as v1 (30..95)
        return min(max(score, 20.0), 95.0)

    except Exception:
        return 45.0



def source_consensus(web_context: dict) -> float:
    """
    Calculate source consensus based on proportion of high-quality sources.
    Returns continuous score 0-100 based on quality distribution.
    """
    reliabilities = web_context.get("source_reliability", [])

    if not reliabilities:
        return 50.0  # Neutral when no sources

    total = len(reliabilities)
    high_count = sum(1 for r in reliabilities if "✅" in str(r))
    medium_count = sum(1 for r in reliabilities if "⚠️" in str(r))
    low_count = sum(1 for r in reliabilities if "❌" in str(r))

    # Weighted score: High=100, Medium=60, Low=30
    weighted_sum = (high_count * 100) + (medium_count * 60) + (low_count * 30)
    consensus_score = weighted_sum / total

    # Bonus for having multiple high-quality sources
    if high_count >= 3:
        consensus_score = min(100, consensus_score + 10)
    elif high_count >= 2:
        consensus_score = min(100, consensus_score + 5)

    return round(consensus_score, 1)

def evidence_based_veracity(primary_data: dict, web_context: dict) -> dict:
    """
    Evidence-driven veracity scoring.
    Returns breakdown of component scores and overall score (0-100).
    """
    breakdown = {}

    # 1. SOURCE QUALITY (35% weight)
    sources = primary_data.get("sources", [])
    src_score = source_quality_score(sources)
    breakdown["source_quality"] = round(src_score, 1)

    # 2. NUMERIC CONSISTENCY (30% weight)
    num_score = numeric_consistency_with_sources_v2(primary_data, web_context)
    breakdown["numeric_consistency"] = round(num_score, 1)

    # 3. CITATION DENSITY (20% weight)
    # FIXED: Higher score when sources support findings, not penalize detail
    sources_count = len(sources)
    findings_count = len(primary_data.get("key_findings", []))
    metrics_count = len(primary_data.get("primary_metrics", {}))

    # Total claims = findings + metrics
    total_claims = findings_count + metrics_count

    if total_claims == 0:
        citations_score = 40.0  # Low score for no claims
    else:
        # Ratio of sources to claims - ideal is ~0.5-1.0 sources per claim
        ratio = sources_count / total_claims
        if ratio >= 1.0:
            citations_score = 90.0  # Well-supported
        elif ratio >= 0.5:
            citations_score = 70.0 + (ratio - 0.5) * 40  # 70-90 range
        elif ratio >= 0.25:
            citations_score = 50.0 + (ratio - 0.25) * 80  # 50-70 range
        else:
            citations_score = ratio * 200  # 0-50 range

    breakdown["citation_density"] = round(min(citations_score, 95.0), 1)

    # 4. SOURCE CONSENSUS (15% weight)
    consensus_score = source_consensus(web_context)
    breakdown["source_consensus"] = round(consensus_score, 1)

    # Calculate weighted total
    total_score = (
        breakdown["source_quality"] * 0.35 +
        breakdown["numeric_consistency"] * 0.30 +
        breakdown["citation_density"] * 0.20 +
        breakdown["source_consensus"] * 0.15
    )

    breakdown["overall"] = round(total_score, 1)

    return breakdown

def calculate_final_confidence(
    base_conf: float,
    evidence_score: float
) -> float:
    """
    Calculate final confidence score.

    Formula balances model confidence with evidence quality:
    - Evidence has higher weight (65%) as it's more objective
    - Model confidence (35%) is adjusted by evidence quality

    This ensures:
    - High model + High evidence → High final (~85-90%)
    - High model + Low evidence → Medium final (~55-65%)
    - Low model + High evidence → Medium-High final (~70-80%)
    - Low model + Low evidence → Low final (~40-50%)
    """

    # Normalize inputs to 0-100 range
    base_conf = max(0, min(100, base_conf))
    evidence_score = max(0, min(100, evidence_score))

    # 1. EVIDENCE COMPONENT (65% weight) - Primary driver
    evidence_component = evidence_score * 0.65

    # 2. MODEL COMPONENT (35% weight) - Adjusted by evidence quality
    # When evidence is weak, model confidence is discounted
    evidence_multiplier = 0.5 + (evidence_score / 200)  # Range: 0.5 to 1.0
    model_component = base_conf * evidence_multiplier * 0.35

    final = evidence_component + model_component

    # Ensure result is in valid range
    return round(max(0, min(100, final)), 1)

# =========================================================
# 8A. DETERMINISTIC DIFF ENGINE
# Pure Python computation - no LLM variance
# =========================================================

@dataclass
class MetricDiff:
    """Single metric change record"""
    name: str
    old_value: Optional[float]
    new_value: Optional[float]
    old_raw: str  # Original string representation
    new_raw: str
    unit: str
    change_pct: Optional[float]
    change_type: str  # 'increased', 'decreased', 'unchanged', 'added', 'removed'

@dataclass
class EntityDiff:
    """Single entity ranking change record"""
    name: str
    old_rank: Optional[int]
    new_rank: Optional[int]
    old_share: Optional[str]
    new_share: Optional[str]
    rank_change: Optional[int]  # Positive = moved up
    change_type: str  # 'moved_up', 'moved_down', 'unchanged', 'added', 'removed'

@dataclass
class FindingDiff:
    """Single finding change record"""
    old_text: Optional[str]
    new_text: Optional[str]
    similarity: float  # 0-100
    change_type: str  # 'retained', 'modified', 'added', 'removed'

@dataclass
class EvolutionDiff:
    """Complete diff between two analyses"""
    old_timestamp: str
    new_timestamp: str
    time_delta_hours: Optional[float]
    metric_diffs: List[MetricDiff]
    entity_diffs: List[EntityDiff]
    finding_diffs: List[FindingDiff]
    stability_score: float  # 0-100
    summary_stats: Dict[str, int]

# =========================================================
# CANONICAL METRIC REGISTRY & SEMANTIC FINDING HASH
# Add this section after the dataclass definitions (around line 1587)
# =========================================================

# ------------------------------------
# CANONICAL METRIC REGISTRY
# Removes LLM control over metric identity
# ------------------------------------

# Metric type definitions with aliases
            # =========================
# PATCH MR1 (ADDITIVE): de-ambiguate "sales" so unit-sales doesn't map to Revenue
# - Remove standalone "sales" from Revenue aliases (too ambiguous)
# - Add money-explicit revenue phrases instead ("sales revenue", "sales value", etc.)
# - Add a couple of volume-style aliases under units_sold ("sales volume", "volume sales")
            # =========================

METRIC_REGISTRY = {
    # Market Size metrics
    "market_size": {
        "canonical_name": "Market Size",
        "aliases": [
            "market size", "market value", "market cap", "total market",
            "global market", "market valuation", "industry size",
            "total addressable market", "tam", "market worth"
        ],
        "unit_type": "currency",
        "category": "size"
    },
    "market_size_current": {
        "canonical_name": "Current Market Size",
        "aliases": [
            "2024 market size", "2025 market size", "current market",
            "present market size", "today market", "current year market",
            "market size 2024", "market size 2025"
        ],
        "unit_type": "currency",
        "category": "size"
    },
    "market_size_projected": {
        "canonical_name": "Projected Market Size",
        "aliases": [
            "projected market", "forecast market", "future market",
            "2026 market", "2027 market", "2028 market", "2029 market", "2030 market",
            "market projection", "expected market size", "estimated market"
        ],
        "unit_type": "currency",
        "category": "size"
    },

    # Growth metrics
    "cagr": {
        "canonical_name": "CAGR",
        "aliases": [
            "cagr", "compound annual growth", "compound growth rate",
            "annual growth rate", "growth rate", "yearly growth"
        ],
        "unit_type": "percentage",
        "category": "growth"
    },
    "yoy_growth": {
        "canonical_name": "YoY Growth",
        "aliases": [
            "yoy growth", "year over year", "year-over-year",
            "annual growth", "yearly growth rate", "growth percentage"
        ],
        "unit_type": "percentage",
        "category": "growth"
    },

    # Revenue metrics
    "revenue": {
        "canonical_name": "Revenue",
        "aliases": [
            "revenue",
            # =========================
            # PATCH MR1 (CHANGED): removed ambiguous standalone alias "sales"
            # =========================
            # "sales",
            # =========================
            "total revenue", "annual revenue",
            "yearly revenue", "gross revenue",

            # =========================
            # PATCH MR1 (ADDITIVE): money-explicit sales phrasing (revenue-like)
            # =========================
            "sales revenue",
            "revenue from sales",
            "sales value",
            "value of sales",
            "sales (value)",
            "turnover",  # common finance synonym
            # =========================
        ],
        "unit_type": "currency",
        "category": "financial"
    },

    # Market share
    "market_share": {
        "canonical_name": "Market Share",
        "aliases": [
            "market share", "share", "market portion", "market percentage",
            "share of market"
        ],
        "unit_type": "percentage",
        "category": "share"
    },

    # Volume metrics
    "units_sold": {
        "canonical_name": "Units Sold",
        "aliases": [
            "units sold", "unit sales", "volume", "units shipped",
            "shipments", "deliveries", "production volume",

            # =========================
            # PATCH MR1 (ADDITIVE): common unit-sales phrasing variants
            # =========================
            "sales volume",
            "volume sales",
            # =========================
        ],
        "unit_type": "count",
        "category": "volume"
    },

    # Pricing
    "average_price": {
        "canonical_name": "Average Price",
        "aliases": [
            "average price", "avg price", "mean price", "asp",
            "average selling price", "unit price"
        ],
        "unit_type": "currency",
        "category": "pricing"
    },

    # -------------------------
    # Country / Macro metrics
    # -------------------------
    "gdp": {
        "canonical_name": "GDP",
        "aliases": ["gdp", "gross domestic product", "economic output"],
        "unit_type": "currency",
        "category": "macro"
    },
    "gdp_per_capita": {
        "canonical_name": "GDP per Capita",
        "aliases": ["gdp per capita", "gdp/capita", "income per person", "per capita gdp"],
        "unit_type": "currency",
        "category": "macro"
    },
    "gdp_growth": {
        "canonical_name": "GDP Growth",
        "aliases": ["gdp growth", "economic growth", "growth rate of gdp", "real gdp growth"],
        "unit_type": "percentage",
        "category": "macro"
    },
    "population": {
        "canonical_name": "Population",
        "aliases": ["population", "population size", "number of people"],
        "unit_type": "count",
        "category": "macro"
    },
    "exports": {
        "canonical_name": "Exports",
        "aliases": ["exports", "export value", "total exports"],
        "unit_type": "currency",
        "category": "trade"
    },
    "imports": {
        "canonical_name": "Imports",
        "aliases": ["imports", "import value", "total imports"],
        "unit_type": "currency",
        "category": "trade"
    },
    "inflation": {
        "canonical_name": "Inflation",
        "aliases": ["inflation", "cpi", "consumer price index", "inflation rate"],
        "unit_type": "percentage",
        "category": "macro"
    },
    "interest_rate": {
        "canonical_name": "Interest Rate",
        "aliases": ["interest rate", "policy rate", "benchmark rate", "central bank rate"],
        "unit_type": "percentage",
        "category": "macro"
    }
}

            # =========================
# END PATCH MR1
            # =========================

# Year extraction pattern
YEAR_PATTERN = re.compile(r'(20\d{2})')

# ------------------------------------
# DETERMINISTIC QUESTION SIGNALS
# Drives metric table templates (no LLM)
# ------------------------------------

QUESTION_CATEGORY_TEMPLATES = {
    "country": [
        "gdp",
        "gdp_per_capita",
        "gdp_growth",
        "population",
        "exports",
        "imports",
        "inflation",
        "interest_rate",
    ],
    "industry": [
        "market_size_current",
        "market_size_projected",
        "cagr",
        "revenue",
        "market_share",
        "units_sold",
        "average_price",
    ],
}

def get_expected_metric_ids_for_category(category: str) -> List[str]:
    """
    Domain-agnostic mapping from a template/category string to expected metric IDs.

    Backward compatible:
      - accepts legacy categories like 'country', 'industry', 'company', 'generic'
      - also accepts template IDs like 'ENTITY_OVERVIEW_MARKET_LIGHT_V1', etc.

    NOTE:
    - This function returns a *default* set for a given template/category.
    - The profiler (classify_question_signals) can override/compose expected_metric_ids dynamically.
    """
    c_raw = (category or "unknown").strip()
    c = c_raw.lower().strip()

    # -------------------------
    # New generalized templates
    # -------------------------
    if c in {"entity_overview_country_light_v1", "entity_overview_country_v1"}:
        return [
            "population",
            "gdp_nominal",
            "gdp_per_capita",
            "gdp_growth",
            "inflation",
            "currency",
            "unemployment",
            "exports",
            "imports",
            "top_industries",
        ]

    if c in {"entity_overview_market_light_v1"}:
        return [
            "market_size_current",
            "market_size_projected",
            "cagr",
            "key_trends",
            "top_players",
        ]

    if c in {"entity_overview_market_heavy_v1"}:
        return [
            "market_size_current",
            "market_size_projected",
            "cagr",
            "key_trends",
            "top_players",
            "key_regions",
            "segments",
            "market_share",
            "revenue",
            "units_sold",
            "average_price",
        ]

    if c in {"entity_overview_company_light_v1", "entity_overview_company_v1"}:
        return [
            "revenue",
            "growth",
            "gross_margin",
            "operating_margin",
            "net_income",
            "market_cap",
            "valuation_multiple",
        ]

    if c in {"entity_overview_product_light_v1", "entity_overview_product_v1"}:
        return [
            "average_price",
            "units_sold",
            "market_share",
            "growth",
            "key_trends",
        ]

    if c in {"entity_overview_topic_v1", "generic_v1"}:
        return []

    # -------------------------
    # Legacy categories (still supported)
    # -------------------------
    if c == "country":
        return get_expected_metric_ids_for_category("ENTITY_OVERVIEW_COUNTRY_LIGHT_V1")

    if c == "industry":
        # legacy industry defaults to light market
        return get_expected_metric_ids_for_category("ENTITY_OVERVIEW_MARKET_LIGHT_V1")

    if c == "company":
        return get_expected_metric_ids_for_category("ENTITY_OVERVIEW_COMPANY_LIGHT_V1")

    if c == "generic":
        return []

    # fallback
    return []


def classify_question_signals(query: str) -> Dict[str, Any]:
    """
    Deterministically classify query and return:
      - category: high-level bucket used for templates (country | industry | company | generic)
      - expected_metric_ids: list[str]
      - signals: list[str] (debuggable reasons)
      - years: list[int]
      - regions: list[str]
      - intents: list[str] (market_size, growth_forecast, competitive_landscape, pricing, regulation, consumer_demand, supply_chain, investment, macro_outlook)
    """
    q_raw = (query or "").strip()
    q = q_raw.lower().strip()
    signals: List[str] = []

    if not q:
        return {
            "category": "generic",
            "expected_metric_ids": [],
            "signals": ["empty_query"],
            "years": [],
            "regions": [],
            "intents": []
        }

    # -------------------------
    # 1) Extract years (deterministic)
    # -------------------------
    years: List[int] = []
    try:
        year_matches = re.findall(r"\b(19|20)\d{2}\b", q_raw)
        # The regex above returns the first group; re-run with a non-capturing group to capture full year strings.
        year_matches_full = re.findall(r"\b(?:19|20)\d{2}\b", q_raw)
        years = sorted({int(y) for y in year_matches_full})
        if years:
            signals.append(f"years:{','.join(map(str, years[:8]))}")
    except Exception:
        years = []

    # -------------------------
    # 2) Extract regions/countries (best-effort deterministic; spaCy if available)
    # -------------------------
    regions: List[str] = []
    try:
        nlp = _try_spacy_nlp()
        if nlp:
            doc = nlp(q_raw)
            gpes = [ent.text.strip() for ent in getattr(doc, "ents", []) if ent.label_ in ("GPE", "LOC")]
            regions = []
            for g in gpes:
                if g and g.lower() not in [x.lower() for x in regions]:
                    regions.append(g)
            if regions:
                signals.append(f"regions_spacy:{','.join(regions[:6])}")
    except Exception:
        pass

    # Fallback: very lightweight region tokens
    if not regions:
        region_tokens = [
            "singapore", "malaysia", "indonesia", "thailand", "vietnam", "philippines",
            "china", "india", "japan", "korea", "australia",
            "usa", "united states", "europe", "uk", "united kingdom",
            "asean", "southeast asia", "sea", "global", "worldwide"
        ]
        hits = [t for t in region_tokens if t in q]
        if hits:
            # Keep original casing loosely (title-case single words)
            regions = [h.title() if " " not in h else h.upper() if h in ("usa", "uk") else h.title() for h in hits[:6]]
            signals.append(f"regions_kw:{','.join(hits[:6])}")

    # -------------------------
    # 3) Intent detection (domain-agnostic)
    # -------------------------
    intent_patterns: Dict[str, List[str]] = {
        "market_size": ["market size", "tam", "total addressable market", "how big", "size of the market", "market value"],
        "growth_forecast": ["cagr", "forecast", "projection", "by 20", "growth rate", "expected to", "outlook", "trend"],
        "competitive_landscape": ["key players", "competitors", "market share", "top companies", "leading players", "who are the players"],
        "pricing": ["pricing", "price", "asp", "average selling price", "cost", "margins"],
        "consumer_demand": ["demand", "users", "penetration", "adoption", "consumer", "customer", "behavior"],
        "supply_chain": ["supply", "capacity", "production", "manufacturing", "inventory", "shipment", "lead time"],
        "regulation": ["regulation", "policy", "law", "compliance", "tax", "tariff", "subsidy"],
        "investment": ["investment", "capex", "funding", "valuation", "roi", "profit", "ebitda"],
        "macro_outlook": ["gdp", "inflation", "interest rate", "policy rate", "exports", "imports", "currency", "exchange rate", "per capita"],
    }

    intents: List[str] = []
    for intent, pats in intent_patterns.items():
        if any(p in q for p in pats):
            intents.append(intent)

    # Small disambiguation: "by 2030" etc. strongly suggests forecast if years exist
    if years and "growth_forecast" not in intents and any(yr >= 2025 for yr in years):
        intents.append("growth_forecast")

    if intents:
        signals.append(f"intents:{','.join(intents[:10])}")

    # -------------------------
    # 4) Category decision (template driver)
    # -------------------------
    # Keep it coarse: country vs industry vs company vs generic
    country_kw = [
        "gdp", "per capita", "population", "exports", "imports",
        "inflation", "cpi", "interest rate", "policy rate", "central bank",
        "currency", "exchange rate"
    ]
    company_kw = ["revenue", "earnings", "profit", "ebitda", "guidance", "quarter", "fy", "10-k", "10q", "balance sheet"]
    industry_kw = [
        "market", "industry", "sector", "tam", "cagr", "market size", "market share",
        "key players", "competitors", "pricing", "forecast", "outlook"
    ]

    country_hits = [k for k in country_kw if k in q]
    company_hits = [k for k in company_kw if k in q]
    industry_hits = [k for k in industry_kw if k in q]

    # If macro intent is present, strongly bias to country
    if "macro_outlook" in intents and (regions or country_hits):
        category = "country"
        signals.append("category_rule:macro_outlook_bias_country")
    elif company_hits and not industry_hits:
        category = "company"
        signals.append(f"category_rule:company_keywords:{','.join(company_hits[:5])}")
    elif industry_hits and not country_hits:
        category = "industry"
        signals.append(f"category_rule:industry_keywords:{','.join(industry_hits[:5])}")
    elif industry_hits and country_hits:
        # tie-break: if market sizing/competitive signals exist -> industry; if macro_outlook -> country
        if "macro_outlook" in intents:
            category = "country"
            signals.append("category_rule:mixed_signals_macro_wins")
        else:
            category = "industry"
            signals.append("category_rule:mixed_signals_default_to_industry")
    else:
        category = "generic"
        signals.append("category_rule:no_template_keywords")

    # -------------------------
    # 5) Expected metric IDs (category + intent)
    # -------------------------
    expected_metric_ids: List[str] = []
    try:
        expected_metric_ids = get_expected_metric_ids_for_category(category) or []
    except Exception:
        expected_metric_ids = []

    # Add a few intent-driven metric IDs (only if your registry supports them)
    intent_metric_suggestions = {
        "market_size": ["market_size", "market_size_2024", "market_size_2025"],
        "growth_forecast": ["cagr", "forecast_period", "market_size_2030"],
        "competitive_landscape": ["market_share", "top_players"],
        "pricing": ["avg_price", "asp"],
        "consumer_demand": ["users", "penetration", "arpu"],
        "supply_chain": ["capacity", "shipments"],
        "investment": ["capex", "profit", "ebitda"],
        "macro_outlook": ["gdp", "inflation", "interest_rate", "exchange_rate"],
    }

    for intent in intents:
        for mid in intent_metric_suggestions.get(intent, []):
            if mid not in expected_metric_ids:
                expected_metric_ids.append(mid)

    return {
        "category": category,
        "expected_metric_ids": expected_metric_ids,
        "signals": signals,
        "years": years,
        "regions": regions,
        "intents": intents
    }


    def _contains_any(needle_list: List[str]) -> bool:
        return any(k in q for k in needle_list)

    # -------------------------
    # Determine intents
    # -------------------------
    intents: List[str] = []
    for intent, kws in intent_triggers.items():
        if _contains_any(kws):
            intents.append(intent)

    if intents:
        signals.append("intents:" + ",".join(sorted(set(intents))))

    # -------------------------
    # Determine entity_kind (best-effort heuristic)
    # -------------------------
    is_marketish = _contains_any(market_entity_kw) or any(i in intents for i in ["size", "growth", "forecast", "share", "segments", "players", "regions"])
    is_companyish = _contains_any(company_entity_kw) and not _contains_any(country_entity_kw)
    is_countryish = _contains_any(country_entity_kw) and not is_companyish
    is_productish = _contains_any(product_entity_kw) and not (is_marketish or is_countryish or is_companyish)

    if is_countryish:
        entity_kind = "country"
        signals.append("entity_kind:country")
    elif is_companyish:
        entity_kind = "company"
        signals.append("entity_kind:company")
    elif is_productish:
        entity_kind = "product"
        signals.append("entity_kind:product")
    elif is_marketish:
        entity_kind = "market"
        signals.append("entity_kind:market")
    else:
        entity_kind = "topic_general"
        signals.append("entity_kind:topic_general")

    # -------------------------
    # Determine scope
    # -------------------------
    is_comparative = _contains_any(comparative_kw)
    is_forecasty = _contains_any(forecast_kw) or bool(YEAR_PATTERN.findall(q_raw))

    # Broad overview should win when user explicitly asks for general explainer
    # BUT: if they also mention measurable intents (size/growth/forecast/etc.), treat as metrics_light.
    is_broad_phrase = _contains_any(broad_phrases)

    if is_comparative:
        scope = "comparative"
        signals.append("scope:comparative")
    elif is_forecasty and any(i in intents for i in ["forecast", "growth", "size"]):
        scope = "forecast_specific"
        signals.append("scope:forecast_specific")
    elif is_broad_phrase and not intents:
        scope = "broad_overview"
        signals.append("scope:broad_overview")
    else:
        # metrics light vs heavy
        heavy_asks = ["segments", "share", "volume", "regions", "players"]
        heavy_requested = any(i in intents for i in heavy_asks)
        if heavy_requested:
            scope = "metrics_heavy"
            signals.append("scope:metrics_heavy")
        else:
            scope = "metrics_light"
            signals.append("scope:metrics_light")

    # -------------------------
    # Map entity_kind -> category (backward compatible)
    # -------------------------
    if entity_kind == "country":
        category = "country"
    elif entity_kind == "company":
        category = "company"
    elif entity_kind in {"market", "product"}:
        category = "industry"
    else:
        category = "generic"

    # -------------------------
    # Choose generalized template + tiers
    # -------------------------
    # Tier meanings:
    #  1 = high extractability (size/growth/forecast)
    #  2 = medium (players/regions/basic segments)
    #  3 = low (granular channels, detailed splits) -> only if explicitly asked
    if category == "country":
        metric_template_id = "ENTITY_OVERVIEW_COUNTRY_LIGHT_V1" if scope != "metrics_heavy" else "ENTITY_OVERVIEW_COUNTRY_LIGHT_V1"
        metric_tiers_enabled = [1]
    elif category == "company":
        metric_template_id = "ENTITY_OVERVIEW_COMPANY_LIGHT_V1"
        metric_tiers_enabled = [1]
    elif category == "industry":
        if scope in {"metrics_heavy", "comparative"}:
            metric_template_id = "ENTITY_OVERVIEW_MARKET_HEAVY_V1"
            metric_tiers_enabled = [1, 2]
        else:
            metric_template_id = "ENTITY_OVERVIEW_MARKET_LIGHT_V1"
            metric_tiers_enabled = [1]
    else:
        metric_template_id = "ENTITY_OVERVIEW_TOPIC_V1"
        metric_tiers_enabled = []

    # -------------------------
    # Build expected_metric_ids dynamically from intents (domain-agnostic)
    # -------------------------
    # Slot -> metric id mapping (kept generic; avoids tourism specialization)
    # If you later add more canonical IDs, expand these mappings.
    market_slot_to_id = {
        "size_current": "market_size_current",
        "size_projected": "market_size_projected",
        "growth_cagr": "cagr",
        "growth_yoy": "growth",
        "share_key": "market_share",
        "volume_current": "units_sold",
        "price_avg": "average_price",
        "players_top": "top_players",
        "regions_key": "key_regions",
        "segments_basic": "segments",
        "trends": "key_trends",
        "revenue": "revenue",
    }

    company_slot_to_id = {
        "revenue": "revenue",
        "growth": "growth",
        "gross_margin": "gross_margin",
        "operating_margin": "operating_margin",
        "net_income": "net_income",
        "market_cap": "market_cap",
        "valuation_multiple": "valuation_multiple",
        "trends": "key_trends",
    }

    country_slot_to_id = {
        "population": "population",
        "gdp_nominal": "gdp_nominal",
        "gdp_per_capita": "gdp_per_capita",
        "gdp_growth": "gdp_growth",
        "inflation": "inflation",
        "currency": "currency",
        "unemployment": "unemployment",
        "exports": "exports",
        "imports": "imports",
        "top_industries": "top_industries",
        "trends": "key_trends",
    }

    # Determine slots from intents
    slots: List[str] = []
    if entity_kind == "country":
        # For countries: macro defaults if broad, otherwise macro intents
        if scope == "broad_overview":
            slots = ["population", "gdp_nominal", "gdp_per_capita", "gdp_growth", "inflation", "currency", "top_industries"]
        else:
            # If user asks for macro (or didn’t specify), still give a tight macro set
            slots = ["population", "gdp_nominal", "gdp_growth", "inflation", "currency"]
            if "macro" in intents:
                slots += ["unemployment", "exports", "imports"]

        mapper = country_slot_to_id

    elif entity_kind == "company":
        slots = ["revenue", "growth", "gross_margin", "operating_margin", "net_income", "market_cap", "valuation_multiple"]
        mapper = company_slot_to_id

    elif entity_kind in {"market", "product"}:
        # Tier 1 core (always when metrics_* scope)
        if scope == "broad_overview":
            slots = ["trends", "players_top"]
        else:
            slots = ["size_current", "growth_cagr"]
            if "forecast" in intents:
                slots.append("size_projected")
            if "trends" in intents:
                slots.append("trends")
            # Tier 2 (only when explicitly asked or heavy scope)
            if scope in {"metrics_heavy", "comparative"}:
                if "players" in intents:
                    slots.append("players_top")
                if "regions" in intents:
                    slots.append("regions_key")
                if "segments" in intents:
                    slots.append("segments_basic")
                if "share" in intents:
                    slots.append("share_key")
                if "volume" in intents:
                    slots.append("volume_current")
                if "price" in intents:
                    slots.append("price_avg")
            else:
                # metrics_light: include players/trends only if asked
                if "players" in intents:
                    slots.append("players_top")
                if "regions" in intents:
                    slots.append("regions_key")

        mapper = market_slot_to_id

    else:
        # topic_general
        slots = []
        mapper = {}

    expected_metric_ids = []
    for s in slots:
        mid = mapper.get(s)
        if mid:
            expected_metric_ids.append(mid)

    # If still empty but template provides defaults, use template defaults
    if not expected_metric_ids:
        expected_metric_ids = get_expected_metric_ids_for_category(metric_template_id)

    # De-dup while preserving order
    seen = set()
    expected_metric_ids = [x for x in expected_metric_ids if not (x in seen or seen.add(x))]

    # -------------------------
    # Preferred source classes (generic)
    # -------------------------
    if category == "country":
        preferred_source_classes = ["official_stats", "government", "reputable_org", "reference"]
    elif category == "company":
        preferred_source_classes = ["official_filings", "investor_relations", "reputable_org", "news"]
    elif category == "industry":
        preferred_source_classes = ["industry_association", "reputable_org", "official_stats", "news", "research_portal"]
    else:
        preferred_source_classes = ["reference", "official_stats", "reputable_org"]

    # Attach year detection signal
    years = sorted(set(YEAR_PATTERN.findall(q_raw))) if YEAR_PATTERN.findall(q_raw) else []
    if years:
        signals.append("years_detected:" + ",".join(years))

    return {
        "category": category,
        "expected_metric_ids": expected_metric_ids,
        "signals": signals,
        "entity_kind": entity_kind,
        "scope": scope,
        "metric_template_id": metric_template_id,
        "metric_tiers_enabled": metric_tiers_enabled,
        "preferred_source_classes": preferred_source_classes,
        "intents": sorted(set(intents)),
    }


def get_canonical_metric_id(metric_name: str) -> Tuple[str, str]:
    """
    Map a metric name to its canonical ID and display name.

    Returns:
        Tuple of (canonical_id, canonical_display_name)

    Example:
        "2024 Market Size" -> ("market_size_2024", "Market Size (2024)")
        "Global Market Value" -> ("market_size", "Market Size")
        "CAGR 2024-2030" -> ("cagr_2024_2030", "CAGR (2024-2030)")
    """
    import re

    if not metric_name:
        return ("unknown", "Unknown Metric")

    name_lower = metric_name.lower().strip()
    name_normalized = re.sub(r"[^\w\s]", " ", name_lower)
    name_normalized = re.sub(r"\s+", " ", name_normalized).strip()

    # Extract years
    years = YEAR_PATTERN.findall(metric_name)
    year_suffix = "_".join(sorted(years)) if years else ""

    # =========================
    # PATCH CM1 (ADDITIVE): intent signals to prevent "sales" -> "revenue" mis-maps
    # =========================
    name_words = set(name_normalized.split())

    # Explicit money intent (strong)
    money_tokens = {
        "revenue", "turnover", "valuation", "valued", "value", "market", "capex", "opex",
        "profit", "earnings", "ebitda", "income",
        "usd", "sgd", "eur", "gbp", "aud", "cad", "jpy", "cny", "rmb"
    }
    # Currency symbols appear in raw text sometimes
    has_currency_symbol = any(sym in metric_name for sym in ["$", "€", "£", "S$"])

    has_money_intent = bool(name_words & money_tokens) or has_currency_symbol

    # Explicit unit/count intent (strong)
    unit_tokens = {
        "unit", "units", "deliveries", "shipments", "registrations", "vehicles",
        "sold", "salesvolume", "volume", "pcs", "pieces"
    }
    # normalize joined token cases like "sales volume"
    joined = name_normalized.replace(" ", "")
    has_unit_intent = bool(name_words & unit_tokens) or any(t in joined for t in ["salesvolume", "unitsold", "vehiclesold"])
    # =========================

    # Find best matching registry entry
    best_match_id = None
    best_match_score = 0.0

    # =========================
    # PATCH CM2 (ADDITIVE): helper to identify revenue-like registry targets
    # =========================
    def _is_revenue_like(metric_id: str, config: dict) -> bool:
        mid = (metric_id or "").lower()
        cname = str((config or {}).get("canonical_name") or "").lower()
        # treat "market value" / "valuation" as currency-like too
        if any(k in cname for k in ["revenue", "market value", "valuation", "market size", "turnover"]):
            return True
        if any(k in mid for k in ["revenue", "market_value", "market_size", "valuation"]):
            return True
        return False
    # =========================

    for metric_id, config in METRIC_REGISTRY.items():
        for alias in config["aliases"]:
            # Remove years from alias for comparison
            alias_no_year = YEAR_PATTERN.sub("", alias).strip().lower()
            alias_no_year = re.sub(r"[^\w\s]", " ", alias_no_year)
            alias_no_year = re.sub(r"\s+", " ", alias_no_year).strip()

            name_no_year = YEAR_PATTERN.sub("", name_normalized).strip()

            # ---- base score from your existing logic ----
            score = 0.0

            # Exact match
            if alias_no_year == name_no_year and alias_no_year:
                score = 1.0

            # Containment match
            elif alias_no_year and (alias_no_year in name_no_year or name_no_year in alias_no_year):
                score = len(alias_no_year) / max(len(name_no_year), 1)

            # Word overlap match
            else:
                alias_words = set(alias_no_year.split())
                name_words_local = set(name_no_year.split())
                if alias_words and name_words_local:
                    overlap = len(alias_words & name_words_local) / len(alias_words | name_words_local)
                    score = max(score, overlap)

            # =========================
            # PATCH CM3 (ADDITIVE): disambiguation penalties/guards
            # - Block "sales" -> revenue when no money intent
            # - Block unit-intent -> revenue-like
            # - Require explicit money intent for revenue-like (soft guard, not hard stop)
            # =========================
            if score > 0.0:
                revenue_like = _is_revenue_like(metric_id, config)

                # If target is revenue-like but name has strong unit intent, penalize heavily
                if revenue_like and has_unit_intent and not has_money_intent:
                    score *= 0.20  # strong downweight

                # If target is revenue-like but name has NO money intent at all, penalize
                if revenue_like and not has_money_intent:
                    score *= 0.55  # moderate downweight

                # If name includes the word "sales" but no money intent, avoid mapping to revenue-like
                if revenue_like and ("sales" in name_no_year.split()) and not has_money_intent:
                    score *= 0.60

                # Conversely: if target is NOT revenue-like but name has money intent, slight penalty
                if (not revenue_like) and has_money_intent and ("sales" in name_no_year.split()) and not has_unit_intent:
                    score *= 0.85
            # =========================

            if score > best_match_score:
                best_match_id = metric_id
                best_match_score = score

            if best_match_score == 1.0:
                break

        if best_match_score == 1.0:
            break

    # Build canonical ID and display name
    if best_match_id and best_match_score > 0.4:
        config = METRIC_REGISTRY[best_match_id]
        canonical_base = best_match_id
        display_name = config["canonical_name"]

        if year_suffix:
            canonical_id = f"{canonical_base}_{year_suffix}"
            if len(years) == 1:
                display_name = f"{display_name} ({years[0]})"
            else:
                display_name = f"{display_name} ({'-'.join(years)})"
        else:
            canonical_id = canonical_base

        return (canonical_id, display_name)

    # Fallback: create ID from normalized name
    fallback_id = re.sub(r"\s+", "_", name_normalized)
    if year_suffix:
        fallback_id = f"{fallback_id}_{year_suffix}" if year_suffix not in fallback_id else fallback_id

    return (fallback_id, metric_name)

# ------------------------------------
# GEO + PROXY TAGGING (DETERMINISTIC)
# ------------------------------------

import re
from typing import Dict, Any, Tuple, List, Optional

REGION_KEYWORDS = {
    "APAC": ["apac", "asia pacific", "asia-pacific"],
    "SOUTHEAST_ASIA": ["southeast asia", "asean", "sea "],  # note space to reduce false matches
    "ASIA": ["asia"],
    "EUROPE": ["europe", "eu", "emea"],
    "NORTH_AMERICA": ["north america"],
    "LATAM": ["latin america", "latam"],
    "MIDDLE_EAST": ["middle east", "mena"],
    "AFRICA": ["africa"],
    "OCEANIA": ["oceania", "australia", "new zealand"],
}

GLOBAL_KEYWORDS = ["global", "worldwide", "world", "international", "across the world"]

# Minimal country map (expand deterministically over time)
COUNTRY_KEYWORDS = {
    "Singapore": ["singapore", "sg"],
    "United States": ["united states", "usa", "u.s.", "us"],
    "United Kingdom": ["united kingdom", "uk", "u.k.", "britain", "england"],
    "China": ["china", "prc"],
    "Japan": ["japan"],
    "India": ["india"],
    "Indonesia": ["indonesia"],
    "Malaysia": ["malaysia"],
    "Thailand": ["thailand"],
    "Vietnam": ["vietnam"],
    "Philippines": ["philippines"],
}

def infer_geo_scope(*texts: str) -> Dict[str, str]:
    """
    Deterministically infer geography from text.
    Returns {"geo_scope": "local|regional|global|unknown", "geo_name": "<name or ''>"}.
    Priority: country > region > global.
    """
    combined = " ".join([t for t in texts if isinstance(t, str) and t.strip()]).lower()
    if not combined:
        return {"geo_scope": "unknown", "geo_name": ""}

    # 1) Country/local (most specific)
    for country, kws in COUNTRY_KEYWORDS.items():
        for kw in kws:
            if kw in combined:
                return {"geo_scope": "local", "geo_name": country}

    # 2) Region
    for region_name, kws in REGION_KEYWORDS.items():
        for kw in kws:
            if kw in combined:
                pretty = region_name.replace("_", " ").title()
                return {"geo_scope": "regional", "geo_name": pretty}

    # 3) Global
    for kw in GLOBAL_KEYWORDS:
        if kw in combined:
            return {"geo_scope": "global", "geo_name": "Global"}

    return {"geo_scope": "unknown", "geo_name": ""}


# ---- Proxy labeling ----
# "Proxy" = adjacent metric that can help approximate the target but isn't the target definition.
# You can expand these sets deterministically.

PROXY_PATTERNS = [
    # (pattern, proxy_type, reason_template)
    (r"\bapparel\b|\bfashion\b|\bclothing\b", "adjacent_category", "Uses apparel/fashion as an adjacent proxy for streetwear."),
    (r"\bfootwear\b|\bsneaker\b|\bshoes\b", "subsegment", "Uses footwear/sneakers as a subsegment proxy for the broader market."),
    (r"\bresale\b|\bsecondary market\b", "channel_proxy", "Uses resale/secondary-market measures as a channel proxy."),
    (r"\be-?commerce\b|\bonline sales\b|\bsocial commerce\b", "channel_proxy", "Uses e-commerce indicators as a channel proxy."),
    (r"\btourism\b|\bvisitor\b|\btravel retail\b", "demand_driver", "Uses tourism indicators as a demand-driver proxy."),
    (r"\bsearch interest\b|\bgoogle trends\b|\bweb traffic\b", "interest_proxy", "Uses interest/attention measures as a proxy."),
]

# These are words that signal "core market size" style metrics (usually non-proxy if they match the user topic).
CORE_MARKET_PATTERNS = [
    r"\bmarket size\b",
    r"\bmarket value\b",
    r"\brevenue\b",
    r"\bsales\b",
    r"\bcagr\b",
    r"\bgrowth\b",
    r"\bprojected\b|\bforecast\b",
]

def infer_proxy_label(
    metric_name: str,
    question_text: str = "",
    category_hint: str = "",
    *extra_context: str
) -> Dict[str, Any]:
    """
    Deterministically label a metric as proxy/non-proxy.

    Returns fields:
      is_proxy: bool
      proxy_type: str
      proxy_reason: str
      proxy_confidence: float (0-1)
      proxy_target: str (best-guess target topic)
    """
    name = (metric_name or "").lower().strip()
    q = (question_text or "").lower().strip()
    ctx = " ".join([c for c in extra_context if isinstance(c, str)]).lower()

    combined = " ".join([name, q, ctx]).strip()

    # Default: not proxy
    out = {
        "is_proxy": False,
        "proxy_type": "",
        "proxy_reason": "",
        "proxy_confidence": 0.0,
        "proxy_target": ""
    }

    if not combined:
        return out

    # Best-effort target topic extraction (very light heuristic)
    # If you already have question signals elsewhere, you can pass them in category_hint/question_text.
    # Here we just keep a short phrase if present.
    proxy_target = ""
    if "streetwear" in q:
        proxy_target = "streetwear"
    elif "semiconductor" in q:
        proxy_target = "semiconductors"
    elif "battery" in q:
        proxy_target = "batteries"
    out["proxy_target"] = proxy_target

    # If metric name itself looks like core market patterns AND includes the target keyword, treat as non-proxy.
    # (prevents incorrectly labeling "Singapore streetwear market size" as proxy)
    core_like = any(re.search(p, name) for p in CORE_MARKET_PATTERNS)
    if core_like:
        # If it explicitly contains the topic keyword, strongly non-proxy
        if proxy_target and proxy_target in name:
            return out
        # If it says "streetwear market" in name, non-proxy even if target not detected
        if "streetwear" in name:
            return out

    # Detect proxies using patterns.
    for pat, ptype, reason in PROXY_PATTERNS:
        if re.search(pat, combined):
            out["is_proxy"] = True
            out["proxy_type"] = ptype
            out["proxy_reason"] = reason
            # Confidence: stronger if pattern appears in metric name; weaker if only in context.
            if re.search(pat, name):
                out["proxy_confidence"] = 0.9
            elif re.search(pat, ctx):
                out["proxy_confidence"] = 0.7
            else:
                out["proxy_confidence"] = 0.6
            return out

    return out


def merge_group_geo(group: List[Dict[str, Any]]) -> Tuple[str, str]:
    """
    Choose the most frequent geo tag within a merged group deterministically.
    Returns (geo_scope, geo_name).
    """
    items = []
    for g in group:
        s = g.get("geo_scope", "unknown")
        n = g.get("geo_name", "")
        if s and s != "unknown":
            items.append((s, n))

    if not items:
        return "unknown", ""

    counts: Dict[str, int] = {}
    for s, n in items:
        k = f"{s}|{n}"
        counts[k] = counts.get(k, 0) + 1

    best_k = max(counts.items(), key=lambda kv: kv[1])[0]  # deterministic tie via insertion order after stable sort
    s, n = best_k.split("|", 1)
    return s, n


def merge_group_proxy(group: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Merge proxy labels for duplicates deterministically.
    If ANY member is proxy -> merged metric is proxy.
    Choose the highest-confidence proxy candidate.
    """
    best = None
    best_conf = -1.0

    for g in group:
        is_proxy = bool(g.get("is_proxy", False))
        conf = float(g.get("proxy_confidence", 0.0) or 0.0)
        if is_proxy and conf > best_conf:
            best_conf = conf
            best = g

    if best is None:
        return {
            "is_proxy": False,
            "proxy_type": "",
            "proxy_reason": "",
            "proxy_confidence": 0.0,
            "proxy_target": "",
        }

    return {
        "is_proxy": True,
        "proxy_type": best.get("proxy_type", ""),
        "proxy_reason": best.get("proxy_reason", ""),
        "proxy_confidence": float(best.get("proxy_confidence", 0.0) or 0.0),
        "proxy_target": best.get("proxy_target", ""),
    }

def canonicalize_metrics(
    metrics: Dict,
    merge_duplicates_to_range: bool = True,
    question_text: str = "",
    category_hint: str = ""
) -> Dict:
    """
    Convert metrics to canonical IDs, but NEVER merge across incompatible dimensions.

    Key fix:
      - Adds deterministic 'dimension' classification and incorporates it into canonical keys.
      - Prevents revenue vs unit-sales from merging just because the year matches.
      - Keeps your geo + proxy tagging behavior.

    Output:
      canonicalized[canonical_key] -> metric dict with:
        - canonical_id (base id)
        - canonical_key (dimension-safe id you should use everywhere downstream)
        - dimension (currency | unit_sales | percent | count | index | unknown)
        - name (dimension-corrected display name)
    """
    import re  # ========================= PATCH C0 (ADDITIVE): missing import =========================

    if not isinstance(metrics, dict):
        return {}

    # =========================
    # PATCH C1 (ADDITIVE): safe helpers for canonical numeric fields
    # - Prefer existing normalize_unit_tag/unit_family/canonicalize_numeric_candidate if present.
    # - Never breaks if those helpers are missing.
    # =========================
    def _safe_normalize_unit_tag(u: str) -> str:
        try:
            fn = globals().get("normalize_unit_tag")
            if callable(fn):
                return fn(u or "")
        except Exception:
            pass
        # minimal fallback (kept conservative)
        uu = (u or "").strip()
        ul = uu.lower().replace(" ", "")
        if ul in ("%", "pct", "percent"):
            return "%"
        if ul in ("twh",):
            return "TWh"
        if ul in ("gwh",):
            return "GWh"
        if ul in ("mwh",):
            return "MWh"
        if ul in ("kwh",):
            return "kWh"
        if ul in ("wh",):
            return "Wh"
        if ul in ("t", "trillion", "tn"):
            return "T"
        if ul in ("b", "bn", "billion"):
            return "B"
        if ul in ("m", "mn", "mio", "million"):
            return "M"
        if ul in ("k", "thousand", "000"):
            return "K"
        return uu

    def _safe_unit_family(unit_tag: str) -> str:
        try:
            fn = globals().get("unit_family")
            if callable(fn):
                return fn(unit_tag or "")
        except Exception:
            pass
        ut = (unit_tag or "").strip()
        if ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
            return "energy"
        if ut == "%":
            return "percent"
        if ut in ("T", "B", "M", "K"):
            return "magnitude"
        # currency not reliably derived here (handled elsewhere)
        return ""
    # =========================

    def infer_metric_dimension(metric_name: str, unit_raw: str) -> str:
        n = (metric_name or "").lower()
        u = (unit_raw or "").strip().lower()

        # Percent
        if "%" in u or "percent" in n or "share" in n or "cagr" in n:
            return "percent"

        # Currency signals
        currency_tokens = ["$", "s$", "usd", "sgd", "eur", "€", "gbp", "£", "jpy", "¥", "cny", "rmb", "aud", "cad"]
        if any(t in u for t in currency_tokens) or any(t in n for t in ["revenue", "market value", "valuation", "value (", "usd", "sgd", "eur"]):
            return "currency"

        # Unit sales / shipments
        unit_tokens = ["unit", "units", "sold", "sales volume", "shipments", "registrations", "deliveries", "vehicles", "pcs", "pieces", "volume"]
        if any(t in n for t in unit_tokens):
            return "unit_sales"

        # Pure counts
        if any(t in n for t in ["count", "number of", "install base", "installed base", "users", "subscribers"]) and "revenue" not in n:
            return "count"

        # Index / score
        if any(t in n for t in ["index", "score", "rating"]):
            return "index"

        return "unknown"

    def display_name_for_dimension(original_display: str, dim: str) -> str:
        if not original_display:
            return original_display

        od = original_display.strip()
        od_low = od.lower()

        if dim == "unit_sales":
            if "revenue" in od_low or "market value" in od_low or "valuation" in od_low:
                return re.sub(r"(?i)revenue|market value|valuation", "Unit Sales", od).strip()
            if od_low.startswith("sales"):
                return "Unit Sales" + od[len("Sales"):]
            if "sales" in od_low:
                return re.sub(r"(?i)sales", "Unit Sales", od).strip()
            return od

        if dim == "currency":
            if "unit sales" in od_low:
                return re.sub(r"(?i)unit sales", "Revenue", od).strip()
            return od

        if dim == "percent":
            if "unit sales" in od_low or "revenue" in od_low:
                return od
            return od

        return od

    candidates = []

    for key, metric in metrics.items():
        if not isinstance(metric, dict):
            continue

        original_name = metric.get("name", key)
        canonical_id, canonical_name = get_canonical_metric_id(original_name)

        # =========================
        # PATCH CM1 (ADDITIVE): registry-guided dimension hint
        # - If the canonical base metric is in METRIC_REGISTRY, use its unit_type
        #   as a strong prior for dimension classification.
        # - This reduces mislabel drift like "Revenue" being assigned as unit_sales
        #   (or vice-versa) purely from noisy LLM labels.
        #
        # NOTE (conflict fix, additive):
        # - Your prior code risked UnboundLocalError due to base_id scoping.
        # - We keep your legacy behavior, but guard it and define base_id upfront.
        # =========================

        registry_unit_type = ""

        # ---- PATCH CM1.A (ADDITIVE): define base_id upfront to prevent UnboundLocalError ----
        base_id = ""
        # -------------------------------------------------------------------------------

        try:
            # =========================
            # PATCH CM1.B (BUGFIX + ADDITIVE): registry base_id extraction
            # - canonical_id may contain underscores inside the base id (e.g., "market_size_2025")
            # - Find the LONGEST registry key that is a prefix of canonical_id.
            # =========================
            try:
                reg = globals().get("METRIC_REGISTRY")
                cid = str(canonical_id or "")
                if isinstance(reg, dict) and cid:
                    # choose the longest matching prefix key
                    for k in reg.keys():
                        ks = str(k)
                        if cid == ks or cid.startswith(ks + "_"):
                            if len(ks) > len(base_id):
                                base_id = ks

                    if base_id and isinstance(reg.get(base_id), dict):
                        registry_unit_type = (reg[base_id].get("unit_type") or "").strip().lower()
            except Exception:
                # keep safe defaults
                pass
            # =========================

            # -------------------------------------------------------------------
            # PATCH CM1.C (ADDITIVE): legacy code preserved, but guarded
            # - This block is redundant with CM1.B, but we keep it as requested.
            # - Guard prevents:
            #   (1) base_id undefined
            #   (2) overwriting registry_unit_type already computed above
            # -------------------------------------------------------------------
            if not registry_unit_type:
                reg = globals().get("METRIC_REGISTRY")
                if base_id and isinstance(reg, dict) and base_id in reg and isinstance(reg[base_id], dict):
                    registry_unit_type = (reg[base_id].get("unit_type") or "").strip().lower()
            # -------------------------------------------------------------------

        except Exception:
            registry_unit_type = ""

        # Map registry unit_type -> canonicalize_metrics dimension vocabulary
        # (keep it small + deterministic)
        if registry_unit_type:
            if registry_unit_type in ("currency",):
                registry_dim_hint = "currency"
            elif registry_unit_type in ("percentage", "percent"):
                registry_dim_hint = "percent"
            elif registry_unit_type in ("count",):
                # keep "unit_sales" vs "count" distinction:
                # registry says count; name-based inference decides "unit_sales" if it sees units/shipments/deliveries
                registry_dim_hint = "count"
            else:
                registry_dim_hint = ""
        else:
            registry_dim_hint = ""
        # =========================

        raw_unit = (metric.get("unit") or "").strip()

        # =========================
        # PATCH C2 (ADDITIVE): compute unit_tag/unit_family without changing existing unit behavior
        # - We keep your existing unit_norm logic for backwards compatibility.
        # - But we ALSO attach unit_tag + unit_family so downstream can gate deterministically.
        # =========================
        unit_tag = metric.get("unit_tag") or _safe_normalize_unit_tag(raw_unit)
        unit_family_tag = metric.get("unit_family") or _safe_unit_family(unit_tag)
        # =========================

        unit_norm = raw_unit.upper()  # keep original behavior (do not change)
        dim = infer_metric_dimension(str(original_name), raw_unit)

        # =========================
        # PATCH CM1 (ADDITIVE): apply registry hint as override / guardrail
        # - If registry says currency/percent, force that dimension.
        # - If registry says count, prevent accidental "currency"/"percent".
        # =========================
        if registry_dim_hint in ("currency", "percent"):
            dim = registry_dim_hint
        elif registry_dim_hint == "count":
            # Allow unit_sales if name clearly indicates it; else keep "count"
            if dim in ("currency", "percent"):
                dim = "count"
        # =========================

        canonical_key = f"{canonical_id}__{dim}"

        parsed_val = parse_to_float(metric.get("value"))
        value_for_sort = parsed_val if parsed_val is not None else str(metric.get("value", ""))

        stable_sort_key = (
            str(original_name).lower().strip(),
            dim,
            unit_norm,
            str(value_for_sort),
            str(key),
        )

        geo = infer_geo_scope(
            str(original_name),
            str(metric.get("context_snippet", "")),
            str(metric.get("source", "")),
            str(metric.get("source_url", "")),
        )

        proxy = infer_proxy_label(
            str(original_name),
            str(question_text),
            str(category_hint),
            str(metric.get("context_snippet", "")),
            str(metric.get("source", "")),
            str(metric.get("source_url", "")),
        )

        # =========================
        # PATCH C3 (ADDITIVE): canonicalize numeric fields on the candidate metric dict
        # - If canonicalize_numeric_candidate exists, it will attach:
        #   unit_tag/unit_family/base_unit/multiplier_to_base/value_norm
        # - If not, we attach minimal fields ourselves (still additive).
        # =========================
        metric_enriched = dict(metric)  # never mutate caller's dict
        try:
            fn_can = globals().get("canonicalize_numeric_candidate")
            if callable(fn_can):
                metric_enriched = fn_can(metric_enriched)
        except Exception:
            pass

        # Ensure minimal canonical fields exist (additive)
        metric_enriched.setdefault("unit_tag", unit_tag)
        metric_enriched.setdefault("unit_family", unit_family_tag)
        # =========================

        candidates.append({
            "canonical_id": canonical_id,
            "canonical_key": canonical_key,
            "canonical_name": display_name_for_dimension(canonical_name, dim),
            "original_name": original_name,

            # NOTE: store enriched metric
            "metric": metric_enriched,

            "unit": unit_norm,
            "parsed_val": parsed_val,
            "dimension": dim,
            "stable_sort_key": stable_sort_key,
            "geo_scope": geo["geo_scope"],
            "geo_name": geo["geo_name"],
            **proxy,
        })

    candidates.sort(key=lambda x: x["stable_sort_key"])

    grouped: Dict[str, List[Dict]] = {}
    for c in candidates:
        grouped.setdefault(c["canonical_key"], []).append(c)

    canonicalized: Dict[str, Dict] = {}

    for ckey, group in grouped.items():
        if len(group) == 1 or not merge_duplicates_to_range:
            g = group[0]
            m = g["metric"]

            # =========================
            # PATCH C4 (ADDITIVE): keep canonical numeric & semantic fields on output row
            # (only adds keys; does not remove/rename existing keys)
            # =========================
            out_row = {
                **m,
                "name": g["canonical_name"],
                "canonical_id": g["canonical_id"],
                "canonical_key": ckey,
                "dimension": g["dimension"],
                "original_name": g["original_name"],
                "geo_scope": g.get("geo_scope", "unknown"),
                "geo_name": g.get("geo_name", ""),
                "is_proxy": bool(g.get("is_proxy", False)),
                "proxy_type": g.get("proxy_type", ""),
                "proxy_reason": g.get("proxy_reason", ""),
                "proxy_confidence": float(g.get("proxy_confidence", 0.0) or 0.0),
                "proxy_target": g.get("proxy_target", ""),
            }
            # Ensure these exist if upstream provided them
            for k in ["anchor_hash", "source_url", "context_snippet", "measure_kind", "measure_assoc",
                      "unit_tag", "unit_family", "base_unit", "multiplier_to_base", "value_norm"]:
                if k in m and k not in out_row:
                    out_row[k] = m.get(k)
            canonicalized[ckey] = out_row
            # =========================
            continue

        # Merge duplicates within SAME dimension-safe canonical_key
        base = group[0]
        base_metric = dict(base["metric"])
        base_metric["name"] = base["canonical_name"]
        base_metric["canonical_id"] = base["canonical_id"]
        base_metric["canonical_key"] = ckey
        base_metric["dimension"] = base["dimension"]

        geo_scope, geo_name = merge_group_geo(group)
        base_metric["geo_scope"] = geo_scope
        base_metric["geo_name"] = geo_name

        merged_proxy = merge_group_proxy(group)
        base_metric.update(merged_proxy)

        vals = [g["parsed_val"] for g in group if g["parsed_val"] is not None]
        raw_vals = [str(g["metric"].get("value", "")) for g in group]
        orig_names = [g["original_name"] for g in group]

        units = [g["unit"] for g in group if g["unit"]]
        unit_base = units[0] if units else (base_metric.get("unit") or "")
        base_metric["unit"] = unit_base

        base_metric["original_names"] = orig_names
        base_metric["raw_values"] = raw_vals

        # =========================
        # PATCH C5 (ADDITIVE): optional canonical range using value_norm if present
        # - Keeps your existing "range" untouched.
        # - Adds "range_norm" when we can compute it.
        # =========================
        vals_norm = []
        for g in group:
            mm = g.get("metric") if isinstance(g, dict) else {}
            if isinstance(mm, dict) and mm.get("value_norm") is not None:
                try:
                    vals_norm.append(float(mm.get("value_norm")))
                except Exception:
                    pass
        # =========================

        # =====================================================================
        # PATCH ANCHOR_VAL1 (ADDITIVE): set metric value from selected evidence candidate
        # Why:
        # - Avoid median/aggregate drift between analysis and evolution.
        # - If we already chose a specific evidence candidate (candidate_id/anchor_hash),
        #   that candidate should determine the metric's reported value/value_norm/unit.
        # Determinism:
        # - Select the evidence row with highest confidence if present, else first.
        # - No re-fetching, no new extraction; uses existing evidence payload only.
        # =====================================================================
        _anchored_value_set = False
        try:
            _ev = base_metric.get("evidence")
            if isinstance(_ev, list) and _ev:
                # pick best evidence deterministically
                def _ev_score(e):
                    try:
                        c = e.get("confidence")
                        return float(c) if c is not None else 0.0
                    except Exception:
                        return 0.0
                _ev_sorted = sorted([e for e in _ev if isinstance(e, dict)], key=_ev_score, reverse=True)
                _best = _ev_sorted[0] if _ev_sorted else None

                if isinstance(_best, dict):
                    # Prefer canonical normalized fields if present
                    _bn = _best.get("value_norm")
                    _bu = _best.get("base_unit") or _best.get("unit")
                    _rawv = _best.get("raw") if _best.get("raw") is not None else _best.get("value")

                    if _bn is not None:
                        try:
                            base_metric["value_norm"] = float(_bn)
                        except Exception:
                            pass

                    # Preserve unit/base_unit
                    if _bu:
                        try:
                            base_metric["base_unit"] = str(_bu)
                        except Exception:
                            pass
                    if _best.get("unit"):
                        base_metric["unit"] = _best.get("unit")

                    # Preserve raw/value display from evidence (preferred)
                    if _rawv is not None:
                        base_metric["raw"] = _rawv
                        base_metric["value"] = _rawv

                    # Helpful debug: show that we anchored value from evidence
                    base_metric.setdefault("debug", {})
                    if isinstance(base_metric.get("debug"), dict):
                        base_metric["debug"]["value_origin"] = "evidence_best_candidate"
                        base_metric["debug"]["evidence_candidate_id"] = _best.get("candidate_id") or _best.get("anchor_hash")
                    _anchored_value_set = True
        except Exception:
            pass
        if vals and not _anchored_value_set:

            vals_sorted = sorted(vals)
            vmin, vmax = vals_sorted[0], vals_sorted[-1]
            vmed = vals_sorted[len(vals_sorted) // 2]
            base_metric["value"] = vmed
            base_metric["range"] = {
                "min": vmin,
                "max": vmax,
                "candidates": vals_sorted,
                "n": len(vals_sorted),
            }
        else:
            base_metric["range"] = {"min": None, "max": None, "candidates": [], "n": 0}

        if len(vals_norm) >= 2:
            vn = sorted(vals_norm)
            base_metric["range_norm"] = {
                "min": vn[0],
                "max": vn[-1],
                "candidates": vn,
                "n": len(vn),
                "unit": base_metric.get("base_unit") or base_metric.get("unit") or "",
            }
        # =========================

        canonicalized[ckey] = base_metric

    return canonicalized



def freeze_metric_schema(canonical_metrics: Dict) -> Dict:
    """
    Lock metric identity + expected schema for future evolution.

    Key fix:
      - Stores canonical_key (dimension-safe)
      - Stores dimension + unit family
      - Keywords include dimension hints to improve later matching
    """
    frozen = {}
    if not isinstance(canonical_metrics, dict):
        return frozen

    # =========================
    # PATCH F1 (ADDITIVE): prefer shared normalize_unit_tag/unit_family helpers if present
    # This improves consistency with extractor + attribution gating.
    # Falls back safely to old heuristics.
    # =========================
    def _normalize_unit_safe(u: str) -> str:
        try:
            fn = globals().get("normalize_unit_tag")
            if callable(fn):
                return fn(u or "")
        except Exception:
            pass
        return (u or "").strip()

    def _unit_family_safe(unit_raw: str, dim_hint: str = "") -> str:
        # 1) dimension-first (strongest signal)
        d = (dim_hint or "").strip().lower()
        if d in ("percent", "pct"):
            return "percent"
        if d in ("currency",):
            return "currency"
        if d in ("energy",):
            return "energy"
        if d in ("unit_sales", "count"):
            # You’ve been treating M/B/T as “magnitude” for counts; keep aligned.
            return "magnitude"
        if d in ("index", "score"):
            return "index"

        # 2) if you already have a unit-family helper in the codebase, use it
        try:
            fn = globals().get("unit_family")
            if callable(fn):
                uf = fn(_normalize_unit_safe(unit_raw))
                if isinstance(uf, str) and uf.strip():
                    return uf.strip().lower()
        except Exception:
            pass

        # 3) fallback to old heuristic (your original logic)
        u = (unit_raw or "").strip().lower()
        if not u:
            return "unknown"
        if "%" in u:
            return "percent"
        if any(t in u for t in ["$", "s$", "usd", "sgd", "eur", "€", "gbp", "£", "jpy", "¥", "cny", "rmb"]):
            return "currency"
        if any(t in u for t in ["b", "bn", "billion", "m", "mn", "million", "k", "thousand", "t", "trillion"]):
            return "magnitude"
        return "other"
    # =========================

    for ckey, m in canonical_metrics.items():
        if not isinstance(m, dict):
            continue

        dim = (m.get("dimension") or "").strip() or "unknown"
        name = m.get("name")
        unit = (m.get("unit") or "").strip()

        # =========================
        # PATCH F2 (ADDITIVE): compute unit_family using dimension-first logic
        # =========================
        uf = _unit_family_safe(unit, dim_hint=dim)
        # =========================

        # Keywords: name + dimension token to prevent cross-dimension matches later
        kws = extract_context_keywords(name or "") or []
        if dim and dim not in kws:
            kws.append(dim)
        if uf and uf not in kws:
            kws.append(uf)

        # =========================
        # PATCH F3 (ADDITIVE): preserve schema unit more safely
        # - Keep your existing behavior in 'unit' (backward compatible),
        #   BUT also add 'unit_tag' which is the canonicalized unit used downstream.
        # - This avoids the "SGD -> S" schema corruption that breaks currency gating.
        # =========================
        unit_tag = _normalize_unit_safe(unit)
        # Keep existing 'unit' output to avoid breaking consumers:
        unit_out = unit_clean_first_letter(unit.upper())
        # =========================

        frozen[ckey] = {
            "canonical_key": ckey,
            "canonical_id": m.get("canonical_id") or ckey.split("__", 1)[0],
            "dimension": dim,
            "name": name,

            # Existing field kept exactly (backward compatible)
            "unit": unit_out,

            # =========================
            # PATCH F3 (ADDITIVE): extra stable fields (non-breaking additions)
            # =========================
            "unit_tag": unit_tag,          # e.g., "%", "M", "B", "TWh"
            "unit_family": uf,             # e.g., "currency", "percent", "magnitude"
            # =========================

            "keywords": kws[:30],
        }

    return frozen


# =========================================================
# RANGE + SOURCE ATTRIBUTION (DETERMINISTIC, NO LLM)
# =========================================================

def stable_json_hash(obj: Any) -> str:
    import hashlib, json
    try:
        s = json.dumps(obj, sort_keys=True, ensure_ascii=False, separators=(",", ":"))
    except Exception:
        s = str(obj)
    return hashlib.sha256(s.encode("utf-8", errors="ignore")).hexdigest()

def make_extracted_number_id(source_url: str, fingerprint: str, n: Dict) -> str:
    payload = {
        "url": source_url or "",
        "fp": fingerprint or "",
        "start": n.get("start_idx"),
        "end": n.get("end_idx"),
        "value": n.get("value"),
        "unit": normalize_unit(n.get("unit") or ""),
        "raw": n.get("raw") or "",
        "ctx": " ".join((n.get("context_snippet") or "").split())[:240],
    }
    return stable_json_hash(payload)

def sort_snapshot_numbers(numbers: List[Dict]) -> List[Dict]:
    """
    Deterministic ordering for extracted_numbers in snapshots.

    Backward compatible + robust:
      - Uses start/end idx when present
      - Avoids hard dependency on normalize_unit() (may not exist)
      - Falls back to normalize_unit_tag() if available
    """

    # =========================
    # PATCH SS1 (ADDITIVE): safe unit normalizer
    # - Prefer normalize_unit() if it exists
    # - Else fall back to normalize_unit_tag() if present
    # - Else just return stripped unit
    # =========================
    _norm_unit_fn = globals().get("normalize_unit")
    _norm_tag_fn = globals().get("normalize_unit_tag")

    def _safe_norm_unit(u: str) -> str:
        u = (u or "").strip()
        try:
            if callable(_norm_unit_fn):
                return str(_norm_unit_fn(u) or "")
        except Exception:
            pass
        try:
            if callable(_norm_tag_fn):
                # normalize_unit_tag expects tags / unit-ish strings; still better than raw
                return str(_norm_tag_fn(u) or "")
        except Exception:
            pass
        return u
    # =========================

    def k(n: Dict[str, Any]):
        n = n or {}
        return (
            n.get("start_idx") if isinstance(n.get("start_idx"), int) else 10**18,
            n.get("end_idx") if isinstance(n.get("end_idx"), int) else 10**18,

            # stable identity ordering
            str(n.get("anchor_hash") or ""),

            # unit + value
            _safe_norm_unit(str(n.get("unit") or "")),
            str(n.get("unit_tag") or ""),
            str(n.get("value_norm") if n.get("value_norm") is not None else n.get("value")),

            # final tie-breakers
            str(n.get("raw") or ""),
            str(n.get("context_snippet") or n.get("context") or "")[:80],
        )

    return sorted((numbers or []), key=k)

def sort_evidence_records(records: List[Dict]) -> List[Dict]:
    """
    Deterministic ordering for evidence_records.

    Backward compatible:
      - Uses url + fingerprint (as you had)
      - Adds fetched_at as tie-breaker if present (non-breaking)
    """

    # =========================
    # PATCH SE1 (ADDITIVE): add fetched_at tie-breaker (optional)
    # =========================
    def k(r: Dict[str, Any]):
        r = r or {}
        return (
            str(r.get("url") or ""),
            str(r.get("fingerprint") or ""),
            str(r.get("fetched_at") or ""),
        )
    # =========================

    return sorted((records or []), key=k)

def sort_metric_anchors(anchors: List[Dict]) -> List[Dict]:
    # =========================
    # PATCH MA2 (ADDITIVE): canonical-first stable sort
    # - Prefer canonical_key (new)
    # - Fall back to metric_id/metric_name (legacy)
    # =========================
    return sorted(
        (anchors or []),
        key=lambda a: (
            str((a or {}).get("canonical_key") or ""),
            str((a or {}).get("metric_id") or ""),
            str((a or {}).get("metric_name") or ""),
            str((a or {}).get("source_url") or ""),
        ),
    )


def normalize_unit(unit: str) -> str:
    """
    Deterministic unit normalizer used across analysis/evolution.

    Goals:
    - Preserve domain units like TWh/GWh/MWh/kWh (do NOT collapse to T/M/etc.)
    - Normalize magnitude suffixes case-insensitively: b/m/t/k -> B/M/T/K
    - Normalize percent consistently to "%"
    - Avoid clever heuristics; only normalize when confidently recognized
    """
    if not unit:
        return ""

    u0 = str(unit).strip()
    if not u0:
        return ""

    ul = u0.strip().lower().replace(" ", "")

    # --- Domain energy units (short-circuit, must be first) ---
    # Normalize casing to canonical display forms
    if "twh" == ul or ul.endswith("twh"):
        return "TWh"
    if "gwh" == ul or ul.endswith("gwh"):
        return "GWh"
    if "mwh" == ul or ul.endswith("mwh"):
        return "MWh"
    if "kwh" == ul or ul.endswith("kwh"):
        return "kWh"
    if ul == "wh":
        return "Wh"

    # --- Percent ---
    if ul in ("%", "percent", "pct"):
        return "%"

    # --- Currency prefixes/symbols (do not try to infer currency codes here) ---
    # Keep currency detection elsewhere; unit here is for magnitude tags.
    # If unit is literally "usd"/"$" etc, strip to empty.
    if ul in ("$", "usd", "sgd", "eur", "gbp", "aud", "cad", "jpy", "cny", "rmb"):
        return ""

    # --- Magnitude tags (case-insensitive) ---
    # IMPORTANT: handle single-letter forms used by extractor ("m", "b", "t", "k")
    if ul in ("trillion", "tn", "t"):
        return "T"
    if ul in ("billion", "bn", "b"):
        return "B"
    if ul in ("million", "mn", "mio", "m"):
        return "M"
    if ul in ("thousand", "k", "000"):
        return "K"

    # Unknown: return original trimmed (preserve domain-specific tokens)
    return u0.strip()




def to_billions(value: float, unit_tag: str) -> Optional[float]:
    """Convert T/B/M tagged values into billions. Leaves % unchanged (returns None for % here)."""
    try:
        v = float(value)
    except Exception:
        return None

    if unit_tag == "T":
        return v * 1000.0
    if unit_tag == "B":
        return v
    if unit_tag == "M":
        return v / 1000.0
    return None


def build_metric_keywords(metric_name: str) -> List[str]:
    """Reuse your existing keyword extractor, but ensure we always have something."""
    kws = extract_context_keywords(metric_name) or []
    # Add simple fallback tokens (deterministic)
    for t in re.findall(r"[a-zA-Z]{4,}", str(metric_name).lower()):
        if t not in kws:
            kws.append(t)
    return kws[:25]


def extract_numbers_from_scraped_sources(
    scraped_content: Dict[str, str],
) -> List[Dict[str, Any]]:
    """
    Deterministically extract numeric candidates from all scraped source texts.
    Returns list of {url, value, unit_tag, raw, context}.
    """
    candidates: List[Dict[str, Any]] = []
    if not isinstance(scraped_content, dict):
        return candidates

    for url, content in scraped_content.items():
        if not content or not isinstance(content, str) or len(content) < 200:
            continue

        # =========================
        # PATCH 1 (ADDITIVE): pass source_url through (improves anchor stability)
        # =========================
        nums = extract_numbers_with_context(content, source_url=url)
        # =========================

        for n in nums:
            # =========================
            # PATCH 1 (ADDITIVE): prefer extractor-provided unit_tag if present; else normalize
            # =========================
            unit_tag = n.get("unit_tag")
            if not unit_tag:
                unit_tag = normalize_unit_tag(n.get("unit", ""))
            # =========================

            row = {
                "url": url,
                "value": n.get("value"),
                "unit_tag": unit_tag,
                "raw": n.get("raw", ""),
                "context": (n.get("context") or ""),
            }

            # =========================
            # PATCH 3 (ADDITIVE): preserve measure association tags if extractor provides them
            # =========================
            if "measure_kind" in n:
                row["measure_kind"] = n.get("measure_kind")
            if "measure_assoc" in n:
                row["measure_assoc"] = n.get("measure_assoc")
            # =========================

            # =========================
            # PATCH 1 (ADDITIVE): preserve extra fields if extractor provides them
            # (backwards compatible: we only add keys, never remove)
            # =========================
            for k in [
                "unit", "is_junk", "junk_reason", "anchor_hash",
                "start_idx", "end_idx", "context_snippet",
                "unit_family", "base_unit", "multiplier_to_base", "value_norm"
            ]:
                if k in n:
                    row[k] = n.get(k)
            # =========================

            # ============================================================
            # PATCH 9 (ADDITIVE): enforce canonical numeric fields uniformly
            # Why:
            #   - Some candidates may not carry unit_family/base_unit/value_norm yet
            #   - We want every candidate (analysis + evolution) to have the same
            #     canonical fields so diff + span logic is stable and drift-free.
            #
            # This is additive and safe to call multiple times.
            # ============================================================
            try:
                fn_can = globals().get("canonicalize_numeric_candidate")
                if callable(fn_can):
                    row = fn_can(row) or row
                else:
                    row = canonicalize_numeric_candidate(row) or row
            except Exception:
                pass

            # --- ADDITIVE: ensure canonical keys exist even if canonicalize failed ---
            row.setdefault("unit_family", unit_family(row.get("unit_tag", "") or ""))
            row.setdefault("base_unit", row.get("unit_tag", "") or "")
            row.setdefault("multiplier_to_base", 1.0)
            if row.get("value") is not None and row.get("value_norm") is None:
                try:
                    row["value_norm"] = float(row.get("value"))
                except Exception:
                    pass
            # ------------------------------------------------------------------------
            # ============================================================

            candidates.append(row)

    return candidates


def attribute_span_to_sources(
    metric_name: str,
    metric_unit: str,
    scraped_content: Dict[str, str],
    rel_tol: float = 0.08,
    # =========================
    # PATCH S1 (ADDITIVE): optional schema inputs (non-breaking)
    # - If provided, we enforce schema-first gating for drift stability.
    # - If not provided, we fall back to existing heuristic behavior.
    # =========================
    canonical_key: str = "",
    metric_schema: Dict[str, Any] = None,
    # =========================
) -> Dict[str, Any]:
    """
    Build a deterministic span (min/mid/max) for a metric, and attribute min/max to sources.
    Uses only scraped content + regex extractions (NO LLM).

    Schema-first behavior (when metric_schema/canonical_key provided):
      - Enforces unit_family and currency/count/percent gating from frozen schema
      - Uses measure_kind tags when available to avoid semantic leakage
      - Keeps deterministic tie-breaking
    """
    import re
    import hashlib

    unit_tag_hint = normalize_unit_tag(metric_unit)
    keywords = build_metric_keywords(metric_name)

    all_candidates = extract_numbers_from_scraped_sources(scraped_content)
    filtered: List[Dict[str, Any]] = []

    metric_l = (metric_name or "").lower()

    # =========================
    # PATCH S2 (ADDITIVE): resolve schema entry (if available)
    # =========================
    schema_entry = None
    if isinstance(metric_schema, dict) and canonical_key and isinstance(metric_schema.get(canonical_key), dict):
        schema_entry = metric_schema.get(canonical_key)
    # =========================

    # =========================
    # PATCH S3 (ADDITIVE): schema-derived expectations with safe fallbacks
    # =========================
    schema_unit_family = ""
    schema_dimension = ""
    schema_unit = ""
    if isinstance(schema_entry, dict):
        schema_unit_family = (schema_entry.get("unit_family") or "").strip().lower()
        schema_dimension = (schema_entry.get("dimension") or "").strip().lower()
        schema_unit = (schema_entry.get("unit") or "").strip()

    expected_family = ""
    if schema_unit_family in ("percent", "currency", "energy"):
        expected_family = schema_unit_family
    if not expected_family:
        ut = normalize_unit_tag(metric_unit)
        if ut == "%":
            expected_family = "percent"
        elif ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
            expected_family = "energy"
        else:
            expected_family = ""

    currencyish = False
    if schema_unit_family == "currency" or schema_dimension == "currency":
        currencyish = True
    if not currencyish:
        mu = (metric_unit or "").lower()
        if any(x in mu for x in ["usd", "sgd", "eur", "gbp", "$", "s$", "€", "£", "aud", "cad", "jpy", "cny", "rmb"]):
            currencyish = True
    if not currencyish and any(x in metric_l for x in ["revenue", "turnover", "valuation", "market value", "market size",
                                                       "profit", "earnings", "ebitda", "capex", "opex"]):
        currencyish = True
    # =========================

    # =========================
    # PATCH S4 (ADDITIVE): expected measure_kind (schema-first with fallback)
    # =========================
    expected_kind = None

    if expected_family == "percent":
        if any(k in metric_l for k in ["growth", "cagr", "increase", "decrease", "yoy", "qoq", "mom", "rate"]):
            expected_kind = "growth_pct"
        else:
            expected_kind = "share_pct"

    if currencyish:
        expected_kind = "money"

    if expected_kind is None and any(k in metric_l for k in [
        "units", "unit sales", "vehicle sales", "vehicles sold", "sold", "sales volume",
        "deliveries", "shipments", "registrations", "volume"
    ]):
        expected_kind = "count_units"
    # =========================

    # =========================
    # PATCH S5 (ADDITIVE): year-ish suppression helpers (unchanged behavior)
    # =========================
    metric_is_yearish = any(k in metric_l for k in ["year", "years", "fy", "fiscal", "calendar", "timeline", "target year"])

    def _looks_like_year_value(v) -> bool:
        try:
            iv = int(float(v))
            return 1900 <= iv <= 2099
        except Exception:
            return False

    def _ctx_has_year_range(ctx: str) -> bool:
        return bool(re.search(r"\b(19|20)\d{2}\s*(?:-|–|—|to)\s*(19|20)\d{2}\b", ctx or "", flags=re.I))
    # =========================

    # =========================
    # PATCH S6 (ADDITIVE): currency evidence check (used only when currencyish)
    # =========================
    def _has_currency_evidence(raw: str, ctx: str) -> bool:
        r = (raw or "")
        c = (ctx or "").lower()

        if any(s in r for s in ["$", "S$", "€", "£"]):
            return True
        if any(code in c for code in [" usd", "sgd", " eur", " gbp", " aud", " cad", " jpy", " cny", " rmb"]):
            return True

        strong_kw = [
            "revenue", "turnover", "valuation", "valued at", "market value", "market size",
            "sales value", "net profit", "operating profit", "gross profit",
            "ebitda", "earnings", "income", "capex", "opex"
        ]
        if any(k in c for k in strong_kw):
            return True
        return False
    # =========================

    # =========================================================================
    # PATCH S11 (ADDITIVE): deterministic candidate_id for tie-breaking
    # - Stable across runs, depends only on stable fields
    # - Used ONLY as final tie-breaker (won't change non-tie outcomes)
    # =========================================================================
    def _candidate_id(x: dict) -> str:
        try:
            url = str(x.get("url") or x.get("source_url") or "")
            ah = str(x.get("anchor_hash") or "")
            vn = x.get("value_norm")
            bu = str(x.get("base_unit") or x.get("unit") or x.get("unit_tag") or "")
            mk = str(x.get("measure_kind") or "")
            # normalize numeric string for stability
            vn_s = ""
            if vn is not None:
                try:
                    vn_s = f"{float(vn):.12g}"
                except Exception:
                    vn_s = str(vn)
            s = f"{url}|{ah}|{vn_s}|{bu}|{mk}"
            return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()
        except Exception:
            return ""
    # =========================================================================

    for c in all_candidates:
        ctx = c.get("context", "")
        if not ctx:
            continue

        if c.get("is_junk") is True:
            continue

        if not metric_is_yearish:
            if (c.get("unit_tag") in ("", None)) and _looks_like_year_value(c.get("value")):
                continue
            if _looks_like_year_value(c.get("value")) and _ctx_has_year_range(ctx):
                continue

        ctx_score = calculate_context_match(keywords, ctx)
        if ctx_score <= 0.0:
            continue

        cand_ut = c.get("unit_tag") or normalize_unit_tag(c.get("unit") or "")
        cand_fam = (c.get("unit_family") or unit_family(cand_ut) or "").strip().lower()

        if expected_family:
            if expected_family == "percent" and cand_fam != "percent":
                continue
            if expected_family == "currency":
                if cand_fam not in ("currency", "magnitude"):
                    continue
                if not _has_currency_evidence(c.get("raw", ""), ctx):
                    continue
            if expected_family == "energy" and cand_fam != "energy":
                continue

        if expected_kind:
            mk = c.get("measure_kind")
            if mk and mk != expected_kind:
                continue

        val_norm = None
        if expected_family == "percent" or unit_tag_hint == "%":
            if cand_ut != "%":
                continue
            val_norm = c.get("value")

        elif expected_family == "energy":
            val_norm = c.get("value_norm")
            if val_norm is None:
                val_norm = c.get("value")

        elif currencyish or expected_family == "currency":
            if c.get("measure_kind") == "count_units":
                continue
            if cand_ut not in ("T", "B", "M"):
                continue
            val_norm = to_billions(c.get("value"), cand_ut)
            if val_norm is None:
                continue

        else:
            try:
                val_norm = float(c.get("value"))
            except Exception:
                continue

        row = {
            **c,
            "unit_tag": cand_ut,
            "unit_family": cand_fam,
            "value_norm": val_norm,
            "ctx_score": float(ctx_score),
        }

        # =========================
        # PATCH S11 (ADDITIVE): attach candidate_id (safe extra field)
        # =========================
        row.setdefault("candidate_id", _candidate_id(row))
        # =========================

        filtered.append(row)

    if not filtered:
        return {
            "span": None,
            "source_attribution": None,
            "evidence": []
        }

    # Deterministic selection: value_norm then ctx_score then url then candidate_id
    # =========================================================================
    # PATCH S12 (ADDITIVE): candidate_id as final tie-breaker
    # =========================================================================
    def min_key(x):
        return (
            float(x["value_norm"]),
            -float(x["ctx_score"]),
            str(x.get("url", "")),
            str(x.get("candidate_id", "")),
        )

    def max_key(x):
        return (
            -float(x["value_norm"]),
            -float(x["ctx_score"]),
            str(x.get("url", "")),
            str(x.get("candidate_id", "")),
        )
    # =========================================================================

    min_item = sorted(filtered, key=min_key)[0]
    max_item = sorted(filtered, key=max_key)[0]

    vmin = float(min_item["value_norm"])
    vmax = float(max_item["value_norm"])
    vmid = (vmin + vmax) / 2.0

    if expected_family == "percent" or unit_tag_hint == "%":
        unit_out = "%"
    elif currencyish or expected_family == "currency":
        unit_out = "billion USD"
    elif expected_family == "energy":
        unit_out = "Wh"
    else:
        unit_out = metric_unit or (schema_unit or "")

    evidence = []
    for it in sorted(filtered, key=lambda x: (-float(x["ctx_score"]), str(x.get("url", "")), str(x.get("candidate_id", ""))))[:12]:
        evidence.append({
            "url": it.get("url"),
            "raw": it.get("raw"),
            "unit_tag": it.get("unit_tag"),
            "unit_family": it.get("unit_family"),
            "measure_kind": it.get("measure_kind"),
            "measure_assoc": it.get("measure_assoc"),
            "value_norm": it.get("value_norm"),
            "candidate_id": it.get("candidate_id"),
            # PATCH EVID_AH1 (ADDITIVE): carry anchor_hash for evolution matching
            "anchor_hash": it.get("anchor_hash"),
            # PATCH EVID_AH2 (ADDITIVE): carry stable span fields when present
            "start_idx": it.get("start_idx"),
            "end_idx": it.get("end_idx"),
            # PATCH EVID_AH3 (ADDITIVE): carry normalized value basis when present
            "value_norm": it.get("value_norm"),
            "base_unit": it.get("base_unit"),
            "multiplier_to_base": it.get("multiplier_to_base"),  # PATCH S11: exposed for transparency
            "context_snippet": (it.get("context") or "")[:220],
            "context_score": round(float(it.get("ctx_score", 0.0)) * 100, 1),
        })

    return {
        "span": {
            "min": round(vmin, 4),
            "mid": round(vmid, 4),
            "max": round(vmax, 4),
            "unit": unit_out
        },
        "source_attribution": {
            "min": {
                "url": min_item.get("url"),
                "raw": min_item.get("raw"),
                "measure_kind": min_item.get("measure_kind"),
                "measure_assoc": min_item.get("measure_assoc"),
                "value_norm": min_item.get("value_norm"),
                "candidate_id": min_item.get("candidate_id"),  # PATCH S11
                "context_snippet": (min_item.get("context") or "")[:220],
                "context_score": round(float(min_item.get("ctx_score", 0.0)) * 100, 1),
            },
            "max": {
                "url": max_item.get("url"),
                "raw": max_item.get("raw"),
                "measure_kind": max_item.get("measure_kind"),
                "measure_assoc": max_item.get("measure_assoc"),
                "value_norm": max_item.get("value_norm"),
                "candidate_id": max_item.get("candidate_id"),  # PATCH S11
                "context_snippet": (max_item.get("context") or "")[:220],
                "context_score": round(float(max_item.get("ctx_score", 0.0)) * 100, 1),
            }
        },
        "evidence": evidence
    }



def add_range_and_source_attribution_to_canonical_metrics(
    canonical_metrics: Dict[str, Any],
    web_context: dict,
    # =========================
    # PATCH R1 (ADDITIVE): optional schema-first inputs
    # If provided, attribution uses frozen schema to avoid semantic/unit leakage.
    # =========================
    metric_schema: Dict[str, Any] = None,
    # =========================
) -> Dict[str, Any]:
    """
    Enrich canonical metrics with deterministic range + source attribution.

    IMPORTANT:
    - canonical_metrics is expected to be keyed by canonical_key (dimension-safe),
      i.e. the output of canonicalize_metrics().
    - Schema-first mode (recommended): pass metric_schema=metric_schema_frozen so
      attribute_span_to_sources() can enforce unit_family / measure_kind gates.
    - Backward compatible: if metric_schema not provided, attribution falls back
      to existing heuristic behavior inside attribute_span_to_sources().
    """
    enriched: Dict[str, Any] = {}
    if not isinstance(canonical_metrics, dict):
        return enriched

    scraped = (web_context or {}).get("scraped_content") or {}
    if not isinstance(scraped, dict):
        scraped = {}

    # =========================
    # PATCH R2 (ADDITIVE): resolve schema dict safely
    # =========================
    schema = metric_schema if isinstance(metric_schema, dict) else {}
    # =========================

    for ckey, m in canonical_metrics.items():
        if not isinstance(m, dict):
            continue

        metric_name = m.get("name") or m.get("original_name") or str(ckey)
        metric_unit = m.get("unit") or ""

        # =========================
        # PATCH R3 (BUGFIX): schema-first wiring (no undefined prev_response/ckey)
        # - canonical_key is the dict key (ckey)
        # - metric_schema is the frozen schema dict (if provided)
        # =========================
        span_pack = attribute_span_to_sources(
            metric_name=metric_name,
            metric_unit=metric_unit,
            scraped_content=scraped,
            canonical_key=str(ckey),
            metric_schema=schema,
        )
        # =========================

        mm = dict(m)

        # Preserve old behavior: only add keys (don’t remove anything)
        if isinstance(span_pack, dict):
            if span_pack.get("span") is not None:
                mm["source_span"] = span_pack.get("span")
            if span_pack.get("source_attribution") is not None:
                mm["source_attribution"] = span_pack.get("source_attribution")
            if span_pack.get("evidence") is not None:
                mm["evidence"] = span_pack.get("evidence")

        enriched[ckey] = mm

    return enriched





# ------------------------------------
# SEMANTIC FINDING HASH
# Removes wording-based churn from findings comparison
# ------------------------------------

# Semantic components to extract from findings
FINDING_PATTERNS = {
    # Growth/decline patterns
    "growth": [
        r'(?:grow(?:ing|th)?|increas(?:e|ing)|expand(?:ing)?|ris(?:e|ing)|up)\s*(?:by|at|of)?\s*(\d+(?:\.\d+)?)\s*%?',
        r'(\d+(?:\.\d+)?)\s*%?\s*(?:growth|increase|expansion|rise)',
    ],
    "decline": [
        r'(?:declin(?:e|ing)|decreas(?:e|ing)|fall(?:ing)?|drop(?:ping)?|down)\s*(?:by|at|of)?\s*(\d+(?:\.\d+)?)\s*%?',
        r'(\d+(?:\.\d+)?)\s*%?\s*(?:decline|decrease|drop|fall)',
    ],

    # Value patterns
    "value": [
        r'\$\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)?',
        r'(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)',
    ],

    # Ranking patterns
    "rank": [
        r'(?:lead(?:ing|er)?|top|first|largest|biggest|#1|number one)',
        r'(?:second|#2|runner.?up)',
        r'(?:third|#3)',
    ],

    # Trend patterns
    "trend_up": [
        r'(?:bullish|optimistic|positive|strong|robust|accelerat)',
    ],
    "trend_down": [
        r'(?:bearish|pessimistic|negative|weak|slow(?:ing)?|decelerat)',
    ],

    # Entity patterns (will be filled dynamically)
    "entities": []
}

# Common stop words to remove
STOP_WORDS = {
    'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
    'should', 'may', 'might', 'must', 'shall', 'can', 'to', 'of', 'in',
    'for', 'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through',
    'during', 'before', 'after', 'above', 'below', 'between', 'under',
    'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where',
    'why', 'how', 'all', 'each', 'few', 'more', 'most', 'other', 'some',
    'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',
    'too', 'very', 'just', 'also', 'now', 'new'
}


def extract_semantic_components(finding: str) -> Dict[str, Any]:
    """
    Extract semantic components from a finding.

    Example:
        "The market is growing at 15% annually" ->
        {
            "direction": "up",
            "percentage": 15.0,
            "subject": "market",
            "timeframe": "annual",
            "entities": [],
            "keywords": ["market", "growing", "annually"]
        }
    """
    if not finding:
        return {}

    finding_lower = finding.lower()
    components = {
        "direction": None,
        "percentage": None,
        "value": None,
        "value_unit": None,
        "subject": None,
        "timeframe": None,
        "entities": [],
        "keywords": []
    }

    # Extract direction
    for pattern in FINDING_PATTERNS["growth"]:
        match = re.search(pattern, finding_lower)
        if match:
            components["direction"] = "up"
            if match.groups():
                try:
                    components["percentage"] = float(match.group(1))
                except:
                    pass
            break

    if not components["direction"]:
        for pattern in FINDING_PATTERNS["decline"]:
            match = re.search(pattern, finding_lower)
            if match:
                components["direction"] = "down"
                if match.groups():
                    try:
                        components["percentage"] = float(match.group(1))
                    except:
                        pass
                break

    # Extract trend sentiment
    if not components["direction"]:
        for pattern in FINDING_PATTERNS["trend_up"]:
            if re.search(pattern, finding_lower):
                components["direction"] = "up"
                break
        for pattern in FINDING_PATTERNS["trend_down"]:
            if re.search(pattern, finding_lower):
                components["direction"] = "down"
                break

    # Extract value
    for pattern in FINDING_PATTERNS["value"]:
        match = re.search(pattern, finding_lower)
        if match:
            try:
                components["value"] = float(match.group(1))
                if len(match.groups()) > 1 and match.group(2):
                    components["value_unit"] = match.group(2)[0].upper()
            except:
                pass
            break

    # Extract timeframe
    timeframe_patterns = {
        "annual": r'\b(?:annual(?:ly)?|year(?:ly)?|per year|yoy|y-o-y)\b',
        "quarterly": r'\b(?:quarter(?:ly)?|q[1-4])\b',
        "monthly": r'\b(?:month(?:ly)?|per month)\b',
        "2024": r'\b2024\b',
        "2025": r'\b2025\b',
        "2026": r'\b2026\b',
        "2030": r'\b2030\b',
    }
    for tf_name, tf_pattern in timeframe_patterns.items():
        if re.search(tf_pattern, finding_lower):
            components["timeframe"] = tf_name
            break

    # Extract subject keywords
    words = re.findall(r'\b[a-z]{3,}\b', finding_lower)
    keywords = [w for w in words if w not in STOP_WORDS]
    components["keywords"] = keywords[:10]  # Limit to top 10

    # Identify likely subject
    subject_candidates = ["market", "industry", "sector", "segment", "revenue", "sales", "demand", "supply"]
    for word in keywords:
        if word in subject_candidates:
            components["subject"] = word
            break

    return components


def compute_semantic_hash(finding: str) -> str:
    """
    Compute a semantic hash for a finding that's invariant to wording changes.

    Two findings with the same meaning should produce the same hash.

    Example:
        "The market is growing at 15% annually" -> "up_15.0_market_annual"
        "Annual growth rate stands at 15%" -> "up_15.0_market_annual"
    """
    components = extract_semantic_components(finding)

    # Build hash components in consistent order
    hash_parts = []

    # Direction
    if components.get("direction"):
        hash_parts.append(components["direction"])

    # Percentage (rounded to avoid float issues)
    if components.get("percentage") is not None:
        hash_parts.append(f"{components['percentage']:.1f}")

    # Value with unit
    if components.get("value") is not None:
        val_str = f"{components['value']:.1f}"
        if components.get("value_unit"):
            val_str += components["value_unit"]
        hash_parts.append(val_str)

    # Subject
    if components.get("subject"):
        hash_parts.append(components["subject"])

    # Timeframe
    if components.get("timeframe"):
        hash_parts.append(components["timeframe"])

    # If we have enough components, use them for hash
    if len(hash_parts) >= 2:
        return "_".join(hash_parts)

    # Fallback: use sorted keywords
    keywords = sorted(components.get("keywords", []))[:5]
    if keywords:
        return "_".join(keywords)

    # Last resort: normalized text hash
    normalized = re.sub(r'\s+', ' ', finding.lower().strip())
    return hashlib.md5(normalized.encode()).hexdigest()[:12]


def compute_semantic_finding_diffs(old_findings: List[str], new_findings: List[str]) -> List[FindingDiff]:
    """
    Compute finding diffs using semantic hashing instead of text similarity.

    This ensures that findings with the same meaning but different wording
    are recognized as the same finding.
    """
    diffs = []
    matched_new_indices = set()

    # Compute hashes for all findings
    old_hashes = [(f, compute_semantic_hash(f), extract_semantic_components(f)) for f in old_findings if f]
    new_hashes = [(f, compute_semantic_hash(f), extract_semantic_components(f)) for f in new_findings if f]

    # Match by semantic hash
    for old_text, old_hash, old_components in old_hashes:
        best_match_idx = None
        best_match_score = 0

        for i, (new_text, new_hash, new_components) in enumerate(new_hashes):
            if i in matched_new_indices:
                continue

            # Exact hash match = same finding
            if old_hash == new_hash:
                best_match_idx = i
                best_match_score = 100
                break

            # Component-based similarity
            score = compute_component_similarity(old_components, new_components)
            if score > best_match_score:
                best_match_score = score
                best_match_idx = i

        if best_match_idx is not None and best_match_score >= 60:
            matched_new_indices.add(best_match_idx)
            new_text = new_hashes[best_match_idx][0]

            if best_match_score >= 90:
                change_type = 'retained'
            else:
                change_type = 'modified'

            diffs.append(FindingDiff(
                old_text=old_text,
                new_text=new_text,
                similarity=best_match_score,
                change_type=change_type
            ))
        else:
            # Finding removed
            diffs.append(FindingDiff(
                old_text=old_text,
                new_text=None,
                similarity=0,
                change_type='removed'
            ))

    # Find added findings
    for i, (new_text, new_hash, new_components) in enumerate(new_hashes):
        if i not in matched_new_indices:
            diffs.append(FindingDiff(
                old_text=None,
                new_text=new_text,
                similarity=0,
                change_type='added'
            ))

    return diffs


def compute_component_similarity(comp1: Dict, comp2: Dict) -> float:
    """
    Compute similarity between two finding component dictionaries.
    Returns a score from 0-100.
    """
    if not comp1 or not comp2:
        return 0

    score = 0
    weights = {
        "direction": 25,
        "percentage": 25,
        "value": 20,
        "subject": 15,
        "timeframe": 10,
        "keywords": 5
    }

    # Direction match
    if comp1.get("direction") and comp2.get("direction"):
        if comp1["direction"] == comp2["direction"]:
            score += weights["direction"]
    elif not comp1.get("direction") and not comp2.get("direction"):
        score += weights["direction"] * 0.5  # Both neutral

    # Percentage match (within 2% tolerance)
    if comp1.get("percentage") is not None and comp2.get("percentage") is not None:
        diff = abs(comp1["percentage"] - comp2["percentage"])
        if diff <= 2:
            score += weights["percentage"]
        elif diff <= 5:
            score += weights["percentage"] * 0.5

    # Value match (within 10% tolerance)
    if comp1.get("value") is not None and comp2.get("value") is not None:
        v1, v2 = comp1["value"], comp2["value"]
        # Normalize by unit
        if comp1.get("value_unit") == comp2.get("value_unit"):
            if v1 > 0 and v2 > 0:
                ratio = min(v1, v2) / max(v1, v2)
                if ratio >= 0.9:
                    score += weights["value"]
                elif ratio >= 0.8:
                    score += weights["value"] * 0.5

    # Subject match
    if comp1.get("subject") and comp2.get("subject"):
        if comp1["subject"] == comp2["subject"]:
            score += weights["subject"]

    # Timeframe match
    if comp1.get("timeframe") and comp2.get("timeframe"):
        if comp1["timeframe"] == comp2["timeframe"]:
            score += weights["timeframe"]

    # Keyword overlap
    kw1 = set(comp1.get("keywords", []))
    kw2 = set(comp2.get("keywords", []))
    if kw1 and kw2:
        overlap = len(kw1 & kw2) / len(kw1 | kw2)
        score += weights["keywords"] * overlap

    return score


# ------------------------------------
# UPDATED METRIC DIFF COMPUTATION
# Using canonical IDs
# ------------------------------------

def compute_metric_diffs_canonical(old_metrics: Dict, new_metrics: Dict) -> List[MetricDiff]:
    """
    Compute metric diffs using canonical IDs for stable matching.
    Range-aware via get_metric_value_span + spans_overlap.
    """
    diffs: List[MetricDiff] = []

    old_canonical = canonicalize_metrics(old_metrics)
    new_canonical = canonicalize_metrics(new_metrics)

    matched_new_ids = set()

    # Match by canonical ID
    for old_id, old_m in old_canonical.items():
        old_name = old_m.get("name", old_id)

        old_span = get_metric_value_span(old_m)
        old_raw = str(old_m.get("value", ""))
        old_unit = old_span.get("unit") or old_m.get("unit", "")
        old_val = old_span.get("mid")

        # -------------------------
        # Direct canonical ID match
        # -------------------------
        if old_id in new_canonical:
            new_m = new_canonical[old_id]
            matched_new_ids.add(old_id)

            new_raw = str(new_m.get("value", ""))
            new_span = get_metric_value_span(new_m)
            new_val = new_span.get("mid")
            new_unit = new_span.get("unit") or new_m.get("unit", old_unit)

            if spans_overlap(old_span, new_span, rel_tol=0.05):
                change_pct = 0.0
                change_type = "unchanged"
            else:
                change_pct = compute_percent_change(old_val, new_val)
                if change_pct is None or abs(change_pct) < 0.5:
                    change_type = "unchanged"
                elif change_pct > 0:
                    change_type = "increased"
                else:
                    change_type = "decreased"

            diffs.append(MetricDiff(
                name=old_name,
                old_value=old_val,
                new_value=new_val,
                old_raw=old_raw,
                new_raw=new_raw,
                unit=new_unit or old_unit,
                change_pct=change_pct,
                change_type=change_type
            ))
            continue  # important: don't fall into base-ID matching

        # -------------------------
        # Base ID match (strip years)
        # -------------------------
        base_id = re.sub(r'_\d{4}(?:_\d{4})*$', '', old_id)
        found = False

        for new_id, new_m in new_canonical.items():
            if new_id in matched_new_ids:
                continue

            new_base_id = re.sub(r'_\d{4}(?:_\d{4})*$', '', new_id)
            if base_id != new_base_id:
                continue

            matched_new_ids.add(new_id)
            found = True

            new_raw = str(new_m.get("value", ""))
            new_span = get_metric_value_span(new_m)
            new_val = new_span.get("mid")
            new_unit = new_span.get("unit") or new_m.get("unit", old_unit)

            if spans_overlap(old_span, new_span, rel_tol=0.05):
                change_pct = 0.0
                change_type = "unchanged"
            else:
                change_pct = compute_percent_change(old_val, new_val)
                if change_pct is None or abs(change_pct) < 0.5:
                    change_type = "unchanged"
                elif change_pct > 0:
                    change_type = "increased"
                else:
                    change_type = "decreased"

            diffs.append(MetricDiff(
                name=old_name,
                old_value=old_val,
                new_value=new_val,
                old_raw=old_raw,
                new_raw=new_raw,
                unit=new_unit or old_unit,
                change_pct=change_pct,
                change_type=change_type
            ))
            break

        if not found:
            diffs.append(MetricDiff(
                name=old_name,
                old_value=old_val,
                new_value=None,
                old_raw=old_raw,
                new_raw="",
                unit=old_unit,
                change_pct=None,
                change_type="removed"
            ))

    # Added metrics
    for new_id, new_m in new_canonical.items():
        if new_id in matched_new_ids:
            continue

        new_name = new_m.get("name", new_id)
        new_raw = str(new_m.get("value", ""))
        new_span = get_metric_value_span(new_m)
        new_val = new_span.get("mid")
        new_unit = new_span.get("unit") or new_m.get("unit", "")

        diffs.append(MetricDiff(
            name=new_name,
            old_value=None,
            new_value=new_val,
            old_raw="",
            new_raw=new_raw,
            unit=new_unit,
            change_pct=None,
            change_type="added"
        ))

    return diffs


# ------------------------------------
# NUMERIC PARSING (DETERMINISTIC)
# ------------------------------------

def parse_to_float(value: Any) -> Optional[float]:
    """
    Deterministically parse any value to float.
    Returns None if unparseable.
    """
    if value is None:
        return None
    if isinstance(value, (int, float)):
        return float(value)
    if not isinstance(value, str):
        return None

    # Clean string
    cleaned = value.strip().upper()
    cleaned = re.sub(r'[,$]', '', cleaned)

    # Handle empty/NA
    if cleaned in ['', 'N/A', 'NA', 'NULL', 'NONE', '-', '—']:
        return None

    # Extract multiplier
    multiplier = 1.0
    if 'TRILLION' in cleaned or cleaned.endswith('T'):
        multiplier = 1_000_000
        cleaned = re.sub(r'T(?:RILLION)?', '', cleaned)
    elif 'BILLION' in cleaned or cleaned.endswith('B'):
        multiplier = 1_000
        cleaned = re.sub(r'B(?:ILLION)?', '', cleaned)
    elif 'MILLION' in cleaned or cleaned.endswith('M'):
        multiplier = 1
        cleaned = re.sub(r'M(?:ILLION)?', '', cleaned)
    elif 'THOUSAND' in cleaned or cleaned.endswith('K'):
        multiplier = 0.001
        cleaned = re.sub(r'K(?:THOUSAND)?', '', cleaned)

    # Handle percentages
    if '%' in cleaned:
        cleaned = cleaned.replace('%', '')
        # Don't apply multiplier to percentages
        multiplier = 1.0

    try:
        return float(cleaned.strip()) * multiplier
    except (ValueError, TypeError):
        return None

def get_metric_value_span(metric: Dict) -> Dict[str, Any]:
    """
    Return a numeric span for a metric to support range-aware canonical metrics.

    Output:
      {
        "min": float|None,
        "max": float|None,
        "mid": float|None,   # representative value (median if range, else parsed value)
        "unit": str,         # normalized (upper/stripped), preserves %/$ units if present
        "is_range": bool
      }
    """
    if not isinstance(metric, dict):
        return {"min": None, "max": None, "mid": None, "unit": "", "is_range": False}

    unit = (metric.get("unit") or "").strip()

    # If metric already has a range object, prefer it
    r = metric.get("range")
    if isinstance(r, dict):
        vmin = r.get("min")
        vmax = r.get("max")
        # ensure numeric
        try:
            vmin = float(vmin) if vmin is not None else None
        except Exception:
            vmin = None
        try:
            vmax = float(vmax) if vmax is not None else None
        except Exception:
            vmax = None

        # Representative = median of candidates if provided, else midpoint of min/max
        candidates = r.get("candidates")
        nums = []
        if isinstance(candidates, list):
            for c in candidates:
                try:
                    nums.append(float(c))
                except Exception:
                    pass
        if nums:
            nums_sorted = sorted(nums)
            mid = nums_sorted[len(nums_sorted) // 2]
        else:
            mid = None
            if vmin is not None and vmax is not None:
                mid = (vmin + vmax) / 2.0
            elif vmin is not None:
                mid = vmin
            elif vmax is not None:
                mid = vmax

        return {
            "min": vmin,
            "max": vmax,
            "mid": mid,
            "unit": unit,
            "is_range": True
        }

    # Non-range metric: parse single numeric value
    val = parse_to_float(metric.get("value"))
    return {
        "min": val,
        "max": val,
        "mid": val,
        "unit": unit,
        "is_range": False
    }


def spans_overlap(a: Dict[str, Any], b: Dict[str, Any], rel_tol: float = 0.05) -> bool:
    """
    Decide whether two spans overlap "enough" to be considered stable.
    rel_tol provides a small widening to avoid false drift from rounding.
    """
    if not a or not b:
        return False
    a_min, a_max = a.get("min"), a.get("max")
    b_min, b_max = b.get("min"), b.get("max")

    if a_min is None or a_max is None or b_min is None or b_max is None:
        return False

    # Widen spans slightly
    a_pad = max(abs(a_max), abs(a_min), 1.0) * rel_tol
    b_pad = max(abs(b_max), abs(b_min), 1.0) * rel_tol

    a_min2, a_max2 = a_min - a_pad, a_max + a_pad
    b_min2, b_max2 = b_min - b_pad, b_max + b_pad

    return not (a_max2 < b_min2 or b_max2 < a_min2)


def compute_percent_change(old_val: Optional[float], new_val: Optional[float]) -> Optional[float]:
    """
    Compute percent change. Returns None if either value is None or old is 0.
    """
    if old_val is None or new_val is None:
        return None
    if old_val == 0:
        return None if new_val == 0 else float('inf')
    return round(((new_val - old_val) / abs(old_val)) * 100, 2)

# ------------------------------------
# NAME MATCHING (DETERMINISTIC)
# ------------------------------------

def normalize_name(name: str) -> str:
    """Normalize name for matching"""
    if not name:
        return ""
    n = name.lower().strip()
    n = re.sub(r'[^\w\s]', '', n)
    n = re.sub(r'\s+', ' ', n)
    return n

def name_similarity(name1: str, name2: str) -> float:
    """Compute similarity ratio between two names (0-1)"""
    n1 = normalize_name(name1)
    n2 = normalize_name(name2)
    if not n1 or not n2:
        return 0.0
    if n1 == n2:
        return 1.0
    # Check containment
    if n1 in n2 or n2 in n1:
        return 0.9
    # Sequence matcher
    return difflib.SequenceMatcher(None, n1, n2).ratio()

def find_best_match(name: str, candidates: List[str], threshold: float = 0.7) -> Optional[str]:
    """Find best matching name from candidates"""
    best_match = None
    best_score = threshold
    for candidate in candidates:
        score = name_similarity(name, candidate)
        if score > best_score:
            best_score = score
            best_match = candidate
    return best_match

# =========================================================
# DETERMINISTIC QUERY STRUCTURE EXTRACTION
# - Classify query into a known category (country / industry / etc.)
# - Extract main question + "side questions" deterministically
# - Optional: spaCy dependency parse (if installed)
# - Optional: embedding similarity (if sentence-transformers/sklearn installed)
# =========================================================

SIDE_CONNECTOR_PATTERNS = [
    r"\bimpact of\b",
    r"\beffect of\b",
    r"\binfluence of\b",
    r"\brole of\b",
    r"\bdriven by\b",
    r"\bcaused by\b",
    r"\bdue to\b",
    r"\bincluding\b",
    r"\bincluding but not limited to\b",
    r"\bwith a focus on\b",
    r"\bespecially\b",
    r"\bnotably\b",
    r"\bplus\b",
    r"\bas well as\b",
    r"\band also\b",
    r"\bvs\b",
    r"\bversus\b",
]

QUESTION_CATEGORIES = {
    "country": {
        "signals": [
            "gdp", "gdp per capita", "population", "inflation", "interest rate",
            "exports", "imports", "trade balance", "currency", "fx", "central bank",
            "unemployment", "fiscal", "budget", "debt", "sovereign", "country"
        ],
        "template_sections": [
            "GDP & GDP per capita", "Growth rates", "Population & demographics",
            "Key industries", "Exports & imports", "Currency & FX trends",
            "Interest rates & inflation", "Risks & outlook"
        ],
    },
    "industry": {
        "signals": [
            "market size", "tam", "cagr", "industry", "sector", "market",
            "key players", "competitive landscape", "drivers", "challenges",
            "regulation", "technology trends", "forecast"
        ],
        "template_sections": [
            "Total Addressable Market (TAM) / Market size", "Growth rates (CAGR/YoY)",
            "Key players", "Key drivers", "Challenges & risks",
            "Technology trends", "Regulatory / environmental factors", "Outlook"
        ],
    },
    "company": {
        "signals": [
            "revenue", "earnings", "profit", "margins", "guidance",
            "business model", "segments", "customers", "competitors",
            "valuation", "multiple", "pe ratio", "cash flow"
        ],
        "template_sections": [
            "Business overview", "Revenue / profitability", "Segments",
            "Competitive position", "Key risks", "Guidance / outlook"
        ],
    },
    "unknown": {
        "signals": [],
        "template_sections": [],
    }
}

def _normalize_q(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip())

def _cleanup_clause(text: str) -> str:
    t = _normalize_q(text)
    t = re.sub(r"^[,;:\-\s]+", "", t)
    t = re.sub(r"[,;:\-\s]+$", "", t)
    return t

def detect_query_category(query: str) -> Dict[str, Any]:
    """
    Deterministically classify query category using keyword signals.
    Returns: {"category": "...", "confidence": 0-1, "matched_signals": [...]}
    """
    q = (query or "").lower()
    best_cat = "unknown"
    best_hits = 0
    best_matched = []

    for cat, cfg in QUESTION_CATEGORIES.items():
        if cat == "unknown":
            continue
        matched = [s for s in cfg["signals"] if s in q]
        hits = len(matched)
        if hits > best_hits:
            best_hits = hits
            best_cat = cat
            best_matched = matched

    # simple confidence: saturate after ~6 hits
    conf = min(best_hits / 6.0, 1.0) if best_hits > 0 else 0.0
    return {"category": best_cat, "confidence": round(conf, 2), "matched_signals": best_matched[:8]}

# =========================================================
# 3A+. LAYERED QUERY STRUCTURE PARSER (Deterministic -> NLP -> Embeddings -> LLM fallback)
# =========================================================

_QUERY_SPLIT_PATTERNS = [
    r"\bas well as\b",
    r"\balong with\b",
    r"\bin addition to\b",
    r"\band\b",
    r"\bplus\b",
    r"\bvs\.?\b",
    r"\bversus\b",
    r",",
    r";",
]

_COUNTRY_OVERVIEW_SIGNALS = [
    "in general", "overview", "tell me about", "general", "profile", "facts about",
    "economy", "population", "gdp", "currency", "exports", "imports",
]

def _normalize_q(q: str) -> str:
    q = (q or "").strip()
    q = re.sub(r"\s+", " ", q)
    return q

def _split_clauses_deterministic(q: str) -> List[str]:
    """
    Deterministically split a question into ordered clauses.

    Supports:
    - comma/connector splits (",", "and", "as well as", "in addition to", etc.)
    - multi-side enumerations like:
        "in addition to: 1. X 2. Y"
        "including: (1) X (2) Y"
        "as well as: • X • Y"
    """
    if not isinstance(q, str):
        return []

    s = q.strip()
    if not s:
        return []

    # Normalize whitespace early (keep original casing if present; upstream may lowercase already)
    s = re.sub(r"\s+", " ", s).strip()

    # --- Step A: If there's an enumeration intro, split head vs tail ---
    # Examples: "in addition to:", "including:", "plus:", "as well as:"
    enum_intro = re.search(
        r"\b(in addition to|in addition|including|in addition to the following|as well as|plus)\b\s*:?\s*",
        s,
        flags=re.IGNORECASE,
    )

    head = s
    tail = ""

    if enum_intro:
        # Split at the FIRST occurrence of the enum phrase
        idx = enum_intro.start()
        # head is everything before the phrase if it exists, otherwise keep whole string
        # but we usually want "Tell me about X in general" to remain in head.
        head = s[:idx].strip().rstrip(",")
        tail = s[enum_intro.end():].strip()

        # If head is empty (e.g., query begins with "In addition to:"), treat everything as head
        if not head:
            head = s
            tail = ""

    clauses: List[str] = []

    # --- Step B: Split head using your existing connector patterns ---
    if head:
        parts = [head]
        for pat in _QUERY_SPLIT_PATTERNS:
            next_parts = []
            for p in parts:
                next_parts.extend(re.split(pat, p, flags=re.IGNORECASE))
            parts = next_parts

        for p in parts:
            p = p.strip(" ,;:.").strip()
            if p:
                clauses.append(p)

    # --- Step C: If tail exists, split as enumerated items/bullets ---
    if tail:
        # Split on "1.", "1)", "(1)", "•", "-", "*"
        # Keep it robust: find item starts, then slice.
        item_start = re.compile(r"(?:^|\s)(?:\(?\d+\)?[\.\)]|[•\-\*])\s+", flags=re.IGNORECASE)
        starts = [m.start() for m in item_start.finditer(tail)]

        if starts:
            # Build slices using detected starts
            spans = []
            for i, st0 in enumerate(starts):
                st = st0
                # Move start to the start of token (strip leading whitespace)
                while st < len(tail) and tail[st].isspace():
                    st += 1
                en = starts[i + 1] if i + 1 < len(starts) else len(tail)
                spans.append((st, en))

            for st, en in spans:
                item = tail[st:en].strip(" ,;:.").strip()
                # Remove the leading bullet/number token again (safety)
                item = re.sub(r"^\(?\d+\)?[\.\)]\s+", "", item)
                item = re.sub(r"^[•\-\*]\s+", "", item)
                item = item.strip(" ,;:.").strip()
                if item:
                    clauses.append(item)
        else:
            # If tail doesn't look enumerated, fall back to normal splitter on tail
            parts = [tail]
            for pat in _QUERY_SPLIT_PATTERNS:
                next_parts = []
                for p in parts:
                    next_parts.extend(re.split(pat, p, flags=re.IGNORECASE))
                parts = next_parts

            for p in parts:
                p = p.strip(" ,;:.").strip()
                if p:
                    clauses.append(p)

    # Final cleanup + dedupe while preserving order
    out: List[str] = []
    seen = set()
    for c in clauses:
        c2 = c.strip()
        if not c2:
            continue
        if c2.lower() in seen:
            continue
        seen.add(c2.lower())
        out.append(c2)

    return out



def _dedupe_clauses(clauses: List[str]) -> List[str]:
    seen = set()
    out = []
    for c in clauses:
        c2 = c.strip().lower()
        if not c2 or c2 in seen:
            continue
        seen.add(c2)
        out.append(c.strip())
    return out

def _choose_main_and_side(clauses: List[str]) -> Tuple[str, List[str]]:
    """
    Pick 'main' as the first clause; side = remainder.
    Deterministic, stable across runs.
    """
    clauses = _dedupe_clauses(clauses)
    if not clauses:
        return "", []
    main = clauses[0]
    side = clauses[1:]
    return main, side

def _try_spacy_nlp():
    """
    Optional NLP layer. If spaCy is installed, use it; otherwise return None.
    """
    try:
        import spacy  # type: ignore
        # Avoid heavy model loading; prefer blank model with sentencizer if no model available.
        try:
            nlp = spacy.load("en_core_web_sm")  # common if installed
        except Exception:
            nlp = spacy.blank("en")
            if "sentencizer" not in nlp.pipe_names:
                nlp.add_pipe("sentencizer")
        return nlp
    except Exception:
        return None

def _nlp_refine_clauses(query: str, clauses: List[str]) -> Dict[str, Any]:
    """
    Use dependency/NER cues to:
      - detect country-overview questions
      - improve main-vs-side decision (coordination / 'as well as' patterns)
    Returns partial overrides: {"main":..., "side":[...], "hints":{...}}
    """
    nlp = _try_spacy_nlp()
    if not nlp:
        return {"hints": {"nlp_used": False}}

    doc = nlp(_normalize_q(query))
    # Named entities that look like places
    gpes = [ent.text for ent in getattr(doc, "ents", []) if ent.label_ in ("GPE", "LOC")]
    gpes_norm = [g.strip() for g in gpes if g and len(g.strip()) > 1]

    # Coordination hint: if query has "as well as" or "and", keep deterministic split,
    # but try to pick the more "general" clause as main when overview signals exist.
    overview_hit = any(sig in (query or "").lower() for sig in _COUNTRY_OVERVIEW_SIGNALS)
    hints = {
        "nlp_used": True,
        "gpe_entities": gpes_norm[:5],
        "overview_signal_hit": bool(overview_hit),
    }

    main, side = _choose_main_and_side(clauses)

    # If overview signals + place entity present, bias main to the overview clause
    if overview_hit and gpes_norm:
        # choose clause with strongest overview signal density
        def score_clause(c: str) -> int:
            c = c.lower()
            return sum(1 for sig in _COUNTRY_OVERVIEW_SIGNALS if sig in c)
        scored = sorted([(score_clause(c), c) for c in clauses], key=lambda x: (-x[0], x[1]))
        if scored and scored[0][0] > 0:
            main = scored[0][1]
            side = [c for c in clauses if c != main]

    return {"main": main, "side": side, "hints": hints}

def _embedding_category_vote(query: str) -> Dict[str, Any]:
    """
    Deterministic 'embedding-like' similarity using TF-IDF (no external model downloads).
    Produces a category suggestion + confidence based on similarity to category descriptors.
    """
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore
        from sklearn.metrics.pairwise import cosine_similarity  # type: ignore
    except Exception:
        return {"category": "unknown", "confidence": 0.0, "method": "tfidf_unavailable"}

    q = _normalize_q(query).lower()
    if not q:
        return {"category": "unknown", "confidence": 0.0, "method": "tfidf_empty"}

    # Build deterministic descriptors from your registry
    cat_texts = []
    cat_names = []
    for cat, cfg in (QUESTION_CATEGORIES or {}).items():
        if not isinstance(cfg, dict) or cat == "unknown":
            continue
        signals = " ".join(cfg.get("signals", [])[:50])
        sections = " ".join((cfg.get("template_sections", []) or [])[:50])
        descriptor = f"{cat} {signals} {sections}".strip()
        if descriptor:
            cat_names.append(cat)
            cat_texts.append(descriptor)

    if not cat_texts:
        return {"category": "unknown", "confidence": 0.0, "method": "tfidf_no_registry"}

    vec = TfidfVectorizer(ngram_range=(1, 2), min_df=1, max_features=8000)
    X = vec.fit_transform(cat_texts + [q])
    sims = cosine_similarity(X[-1], X[:-1]).flatten()

    best_idx = int(sims.argmax()) if sims.size else 0
    best_sim = float(sims[best_idx]) if sims.size else 0.0
    best_cat = cat_names[best_idx] if cat_names else "unknown"

    # Map cosine similarity (~0-1) into a conservative confidence
    conf = max(0.0, min(best_sim / 0.35, 1.0))  # 0.35 sim ~= "high"
    return {"category": best_cat, "confidence": round(conf, 2), "method": "tfidf"}

def _llm_fallback_query_structure(query: str, web_context: Optional[Dict] = None) -> Optional[Dict[str, Any]]:
    """
    Last resort: ask LLM to output ONLY a small JSON query-structure object.
    Guardrail: do NOT let the LLM invent extra side questions unless the user explicitly enumerated them.
    This path must NOT validate against LLMResponse.
    """
    try:
        q = str(query or "").strip()
        if not q:
            return None

        # --- Detect explicit enumeration / list structure in the USER query ---
        # If the user wrote a list (1., 2), bullets, etc.), it's reasonable to accept multiple sides.
        enum_patterns = [
            r"(^|\n)\s*\d+\s*[\.\)]\s+",     # 1.  / 2)
            r"(^|\n)\s*[-•*]\s+",           # - item / • item
            r"(^|\n)\s*[a-zA-Z]\s*[\.\)]\s+"  # a) / b. etc.
        ]
        has_explicit_enumeration = any(re.search(p, q, flags=re.MULTILINE) for p in enum_patterns)

        # Deterministic baseline (what the system already extracted)
        # We use this to clamp LLM hallucinations.
        det_clauses = _split_clauses_deterministic(_normalize_q(q))
        det_main, det_side = _choose_main_and_side(det_clauses)
        det_side = _dedupe_clauses([s.strip() for s in (det_side or []) if isinstance(s, str) and s.strip()])

        prompt = (
            "Extract a query structure.\n"
            "Return ONLY valid JSON with keys:\n"
            "  category: one of [country, industry, company, finance, market, unknown]\n"
            "  category_confidence: number 0-1\n"
            "  main: string (the main question/topic)\n"
            "  side: array of strings (side questions)\n"
            "No extra keys, no commentary.\n\n"
            f"Query: {q}"
        )

        raw = query_perplexity_raw(prompt, max_tokens=250, timeout=30)

        # Parse
        if isinstance(raw, dict):
            parsed = raw
        else:
            if raw is None:
                raw = ""
            if not isinstance(raw, str):
                raw = str(raw)
            parsed = parse_json_safely(raw, "LLM Query Structure")

        if not isinstance(parsed, dict) or parsed.get("main") is None:
            return None

        # --- Clean/normalize fields ---
        llm_main = str(parsed.get("main") or "").strip()
        llm_side = parsed.get("side") if isinstance(parsed.get("side"), list) else []
        llm_side = [str(s).strip() for s in llm_side if s is not None and str(s).strip()]

        # Reject "invented" side items that look like generic outline bullets
        # (Only apply this rejection when the user did NOT explicitly enumerate a list.)
        def _looks_like_outline_item(s: str) -> bool:
            s2 = s.lower().strip()
            bad_starts = (
                "overview", "key", "key stats", "statistics", "major statistics",
                "policies", "infrastructure", "recent trends", "post-covid", "covid",
                "challenges", "opportunities", "drivers", "headwinds",
                "background", "introduction"
            )
            return any(s2.startswith(b) for b in bad_starts)

        # --- Guardrail policy ---
        # If user didn't enumerate, do NOT accept LLM expansion of side questions.
        if not has_explicit_enumeration:
            # Keep deterministic sides only. (You can allow 1 LLM side if deterministic found none.)
            final_side = det_side
            if not final_side and llm_side:
                # Allow at most one side item as a fallback, but avoid outline-like additions.
                cand = llm_side[0]
                final_side = [] if _looks_like_outline_item(cand) else [cand]
        else:
            # User enumerated: accept multiple sides, but still de-dupe and keep deterministic items first
            merged = []
            for s in (det_side + llm_side):
                s = str(s).strip()
                if not s:
                    continue
                if s not in merged:
                    merged.append(s)
            final_side = merged

        # If LLM main is empty or fragment-y, keep deterministic main
        bad_prefixes = ("as well as", "as well", "and ", "also ", "plus ", "as for ")
        if not llm_main or any(llm_main.lower().startswith(p) for p in bad_prefixes):
            llm_main = (det_main or "").strip()

        # Return only allowed keys
        out = {
            "category": parsed.get("category", "unknown") or "unknown",
            "category_confidence": parsed.get("category_confidence", 0.0),
            "main": llm_main,
            "side": final_side,
        }
        return out

    except Exception:
        return None


def _split_side_candidates(query: str) -> List[str]:
    """
    Deterministic splitting into clause candidates.
    We keep it conservative to avoid over-splitting.
    """
    q = _normalize_q(query)
    # Pull quoted strings as strong side-topic candidates
    quoted = re.findall(r"['\"]([^'\"]{2,80})['\"]", q)
    q_wo_quotes = re.sub(r"['\"][^'\"]{2,80}['\"]", " ", q)

    # Split on major separators
    parts = re.split(r"[;]|(?:\s+-\s+)|(?:\s+—\s+)", q_wo_quotes)
    parts = [p for p in (_cleanup_clause(x) for x in parts) if p]

    # Further split on side connectors
    connector_re = "(" + "|".join(SIDE_CONNECTOR_PATTERNS) + ")"
    expanded = []
    for p in parts:
        # break into chunks but keep connector words in-place by splitting into sentences first
        sub = re.split(r"\.\s+|\?\s+|\!\s+", p)
        for s in sub:
            s = _cleanup_clause(s)
            if not s:
                continue
            # if contains connector, split into [before, after...] using first connector
            m = re.search(connector_re, s.lower())
            if m:
                idx = m.start()
                before = _cleanup_clause(s[:idx])
                after = _cleanup_clause(s[idx:])
                if before:
                    expanded.append(before)
                if after:
                    expanded.append(after)
            else:
                expanded.append(s)

    # Add quoted items as standalone candidates (often side topics)
    for qitem in quoted:
        cleaned = _cleanup_clause(qitem)
        if cleaned:
            expanded.append(cleaned)

    # De-dupe while preserving order (deterministic)
    seen = set()
    out = []
    for x in expanded:
        k = x.lower()
        if k not in seen:
            seen.add(k)
            out.append(x)
    return out

def _extract_spacy_side_topics(query: str) -> List[str]:
    """
    Optional: use spaCy dependency parse if available.
    Extracts objects of 'impact/effect/role/influence' patterns.
    """
    try:
        import spacy  # type: ignore
        try:
            nlp = spacy.load("en_core_web_sm")  # type: ignore
        except Exception:
            return []
    except Exception:
        return []

    doc = nlp(query)
    side = []

    triggers = {"impact", "effect", "influence", "role"}
    for token in doc:
        if token.lemma_.lower() in triggers:
            # Look for "of X" attached to trigger
            for child in token.children:
                if child.dep_ == "prep" and child.text.lower() == "of":
                    pobj = next((c for c in child.children if c.dep_ in ("pobj", "dobj", "obj")), None)
                    if pobj is not None:
                        # take subtree as phrase
                        phrase = " ".join(t.text for t in pobj.subtree)
                        phrase = _cleanup_clause(phrase)
                        if phrase and phrase.lower() not in (s.lower() for s in side):
                            side.append(phrase)
    return side[:5]

def _embedding_similarity(a: str, b: str) -> Optional[float]:
    """
    Optional: compute cosine similarity using:
      - sentence-transformers (preferred) OR
      - sklearn TF-IDF fallback
    Returns None if unavailable.
    """
    a = _normalize_q(a)
    b = _normalize_q(b)
    if not a or not b:
        return None

    # 1) sentence-transformers (if installed)
    try:
        from sentence_transformers import SentenceTransformer  # type: ignore
        import numpy as np  # type: ignore
        model = SentenceTransformer("all-MiniLM-L6-v2")
        emb = model.encode([a, b], normalize_embeddings=True)
        sim = float(np.dot(emb[0], emb[1]))
        return max(min(sim, 1.0), -1.0)
    except Exception:
        pass

    # 2) sklearn TF-IDF cosine similarity (deterministic)
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore
        from sklearn.metrics.pairwise import cosine_similarity  # type: ignore
        vec = TfidfVectorizer(stop_words="english")
        X = vec.fit_transform([a, b])
        sim = float(cosine_similarity(X[0], X[1])[0, 0])
        return max(min(sim, 1.0), -1.0)
    except Exception:
        return None

def extract_query_structure(query: str) -> Dict[str, Any]:
    """
    Layered query structure extraction:
      1) Deterministic clause split -> main/side
      2) Deterministic category from keyword signals (detect_query_category)
      3) Optional NLP refinement (spaCy if available)
      4) Deterministic similarity vote (TF-IDF)
      5) LLM fallback ONLY if confidence remains low
    """
    q = _normalize_q(query)
    clauses = _split_clauses_deterministic(q)
    main, side = _choose_main_and_side(clauses)

    # --- Layer 1: deterministic keyword category ---
    det_cat = detect_query_category(q)
    category = det_cat.get("category", "unknown")
    cat_conf = float(det_cat.get("confidence", 0.0))

    debug = {
        "deterministic": {
            "clauses": clauses,
            "main": main,
            "side": side,
            "category": category,
            "confidence": cat_conf,
            "matched_signals": det_cat.get("matched_signals", []),
        }
    }

    # --- Layer 2: NLP refinement (optional) ---
    nlp_out = _nlp_refine_clauses(q, clauses)
    if isinstance(nlp_out, dict):
        hints = nlp_out.get("hints", {})
        debug["nlp"] = hints or {"nlp_used": False}

        # Override main/side if NLP produced them (guard against fragment-y mains)
        nlp_main = (nlp_out.get("main") or "").strip()
        if nlp_main:
            bad_prefixes = ("as well as", "as well", "and ", "also ", "plus ", "as for ")
            if not any(nlp_main.lower().startswith(p) for p in bad_prefixes):
                main = nlp_main

        if isinstance(nlp_out.get("side"), list):
            side = nlp_out["side"]

        # If NLP detects a place + overview cue, bias to "country"
        gpes = (hints or {}).get("gpe_entities", []) if isinstance(hints, dict) else []
        overview_hit = (hints or {}).get("overview_signal_hit", False) if isinstance(hints, dict) else False
        if overview_hit and gpes and cat_conf < 0.45:
            category = "country"
            cat_conf = max(cat_conf, 0.55)

    # --- Layer 3: embedding-style category vote ---
    emb_vote = _embedding_category_vote(q)
    debug["similarity_vote"] = emb_vote

    emb_cat = emb_vote.get("category", "unknown")
    emb_conf = float(emb_vote.get("confidence", 0.0))

    if cat_conf < 0.40 and emb_cat != "unknown" and emb_conf >= 0.45:
        category = emb_cat
        cat_conf = max(cat_conf, min(0.75, emb_conf))

    # --- Layer 4: LLM fallback if still ambiguous ---
    if cat_conf < 0.30:
        llm = _llm_fallback_query_structure(q)
        debug["llm_fallback_used"] = bool(llm)

        if isinstance(llm, dict):
            category = llm.get("category", category) or category
            try:
                cat_conf = float(llm.get("category_confidence", cat_conf))
            except Exception:
                pass

            llm_main = (llm.get("main") or "").strip()
            llm_side = llm.get("side") if isinstance(llm.get("side"), list) else []

            det_main = (main or "").strip()
            det_side = side or []

            def _overview_score(s: str) -> int:
                if not s:
                    return 0
                s2 = s.lower()
                signals = [
                    "in general", "overview", "background", "basic facts",
                    "at a glance", "tell me about", "describe", "introduction"
                ]
                return sum(1 for sig in signals if sig in s2)

            def _is_bad_main(s: str) -> bool:
                if not s or len(s) < 8:
                    return True
                return s.lower().startswith(
                    ("as well as", "as well", "and ", "also ", "plus ", "as for ")
                )

            merged_side = []
            for s in det_side + llm_side:
                s = str(s).strip()
                if s and s not in merged_side:
                    merged_side.append(s)

            det_score = _overview_score(det_main)
            llm_score = _overview_score(llm_main)

            if llm_main and not _is_bad_main(llm_main):
                if not det_main or llm_score > det_score:
                    main = llm_main

            side = merged_side

    side = _dedupe_clauses([s.strip() for s in (side or []) if s.strip()])

    return {
        "category": category or "unknown",
        "category_confidence": round(max(0.0, min(cat_conf, 1.0)), 2),
        "main": (main or "").strip(),
        "side": side,
        "debug": debug,
    }


def format_query_structure_for_prompt(qs: Optional[Dict[str, Any]]) -> str:
    if not qs or not isinstance(qs, dict):
        return ""

    parts = []
    parts.append("STRUCTURED QUESTION (DETERMINISTIC):")
    parts.append(f"- Category: {qs.get('category','unknown')} (conf {qs.get('category_confidence','')})")
    parts.append(f"- Main (answer this FIRST): {qs.get('main','')}")
    side = qs.get("side") or []

    if side:
        parts.append("- Side questions (answer AFTER main, in this exact order):")
        for i, s in enumerate(side[:10], 1):
            parts.append(f"  {i}. {s}")

    tmpl = qs.get("template_sections") or []
    if tmpl:
        parts.append("- Recommended response sections (use as headings if helpful):")
        for t in tmpl[:10]:
            parts.append(f"  - {t}")

    # Hard behavioral instruction to the LLM (kept short and explicit)
    parts.append(
        "RESPONSE RULES:\n"
        "1) Start by answering the MAIN request with general context.\n"
        "2) Then answer EACH side question explicitly (label them).\n"
        "3) Metrics/findings can include both main + side, but do not ignore the main.\n"
        "4) If you provide tourism/industry metrics, ALSO provide basic country/overview facts when main is an overview."
    )

    return "\n".join(parts).strip()


# ------------------------------------
# METRIC DIFF COMPUTATION
# ------------------------------------

def compute_metric_diffs(old_metrics: Dict, new_metrics: Dict) -> List[MetricDiff]:
    """
    Compute deterministic diffs between metric dictionaries.
    Returns list of MetricDiff objects.
    """
    diffs = []
    matched_new_keys = set()

    # Build lookup for new metrics by normalized name
    new_by_name = {}
    for key, m in new_metrics.items():
        if isinstance(m, dict):
            name = m.get('name', key)
            new_by_name[normalize_name(name)] = (key, m)

    # Process old metrics
    for old_key, old_m in old_metrics.items():
        if not isinstance(old_m, dict):
            continue

        old_name = old_m.get('name', old_key)
        old_raw = str(old_m.get('value', ''))
        old_unit = old_m.get('unit', '')
        old_val = parse_to_float(old_m.get('value'))

        # Find matching new metric
        norm_name = normalize_name(old_name)
        match = new_by_name.get(norm_name)

        if not match:
            # Try fuzzy matching
            best = find_best_match(old_name, [m.get('name', k) for k, m in new_metrics.items() if isinstance(m, dict)])
            if best:
                for k, m in new_metrics.items():
                    if isinstance(m, dict) and m.get('name', k) == best:
                        match = (k, m)
                        break

        if match:
            new_key, new_m = match
            matched_new_keys.add(new_key)

            new_raw = str(new_m.get('value', ''))
            new_val = parse_to_float(new_m.get('value'))
            new_unit = new_m.get('unit', old_unit)

            change_pct = compute_percent_change(old_val, new_val)

            # Determine change type
            if change_pct is None:
                change_type = 'unchanged'
            elif abs(change_pct) < 0.5:  # Less than 0.5% change = unchanged
                change_type = 'unchanged'
            elif change_pct > 0:
                change_type = 'increased'
            else:
                change_type = 'decreased'

            diffs.append(MetricDiff(
                name=old_name,
                old_value=old_val,
                new_value=new_val,
                old_raw=old_raw,
                new_raw=new_raw,
                unit=new_unit or old_unit,
                change_pct=change_pct,
                change_type=change_type
            ))
        else:
            # Metric was removed
            diffs.append(MetricDiff(
                name=old_name,
                old_value=old_val,
                new_value=None,
                old_raw=old_raw,
                new_raw='',
                unit=old_unit,
                change_pct=None,
                change_type='removed'
            ))

    # Find added metrics
    for new_key, new_m in new_metrics.items():
        if new_key in matched_new_keys:
            continue
        if not isinstance(new_m, dict):
            continue

        new_name = new_m.get('name', new_key)
        new_raw = str(new_m.get('value', ''))
        new_val = parse_to_float(new_m.get('value'))
        new_unit = new_m.get('unit', '')

        diffs.append(MetricDiff(
            name=new_name,
            old_value=None,
            new_value=new_val,
            old_raw='',
            new_raw=new_raw,
            unit=new_unit,
            change_pct=None,
            change_type='added'
        ))

    return diffs

# ------------------------------------
# ENTITY DIFF COMPUTATION
# ------------------------------------

def compute_entity_diffs(old_entities: List, new_entities: List) -> List[EntityDiff]:
    """
    Compute deterministic diffs between entity rankings.
    """
    diffs = []

    # Build lookups with ranks
    old_lookup = {}
    for i, e in enumerate(old_entities):
        if isinstance(e, dict):
            name = normalize_name(e.get('name', ''))
            old_lookup[name] = {
                'rank': i + 1,
                'share': e.get('share'),
                'original_name': e.get('name', '')
            }

    new_lookup = {}
    for i, e in enumerate(new_entities):
        if isinstance(e, dict):
            name = normalize_name(e.get('name', ''))
            new_lookup[name] = {
                'rank': i + 1,
                'share': e.get('share'),
                'original_name': e.get('name', '')
            }

    # All unique names
    all_names = set(old_lookup.keys()) | set(new_lookup.keys())

    for norm_name in all_names:
        old_data = old_lookup.get(norm_name)
        new_data = new_lookup.get(norm_name)

        if old_data and new_data:
            # Entity exists in both
            rank_change = old_data['rank'] - new_data['rank']  # Positive = moved up

            if rank_change > 0:
                change_type = 'moved_up'
            elif rank_change < 0:
                change_type = 'moved_down'
            else:
                change_type = 'unchanged'

            diffs.append(EntityDiff(
                name=new_data['original_name'],
                old_rank=old_data['rank'],
                new_rank=new_data['rank'],
                old_share=old_data['share'],
                new_share=new_data['share'],
                rank_change=rank_change,
                change_type=change_type
            ))
        elif old_data:
            # Entity removed
            diffs.append(EntityDiff(
                name=old_data['original_name'],
                old_rank=old_data['rank'],
                new_rank=None,
                old_share=old_data['share'],
                new_share=None,
                rank_change=None,
                change_type='removed'
            ))
        else:
            # Entity added
            diffs.append(EntityDiff(
                name=new_data['original_name'],
                old_rank=None,
                new_rank=new_data['rank'],
                old_share=None,
                new_share=new_data['share'],
                rank_change=None,
                change_type='added'
            ))

    # Sort by new rank (added entities at end)
    diffs.sort(key=lambda x: x.new_rank if x.new_rank else 999)
    return diffs

# ------------------------------------
# FINDING DIFF COMPUTATION
# ------------------------------------

def compute_finding_diffs(old_findings: List[str], new_findings: List[str]) -> List[FindingDiff]:
    """
    Compute deterministic diffs between findings using text similarity.
    """
    diffs = []
    matched_new_indices = set()

    # Match old findings to new
    for old_f in old_findings:
        if not old_f:
            continue

        best_match_idx = None
        best_similarity = 0.5  # Minimum threshold

        for i, new_f in enumerate(new_findings):
            if i in matched_new_indices or not new_f:
                continue

            sim = name_similarity(old_f, new_f)  # Reuse name similarity for text
            if sim > best_similarity:
                best_similarity = sim
                best_match_idx = i

        if best_match_idx is not None:
            matched_new_indices.add(best_match_idx)
            similarity_pct = round(best_similarity * 100, 1)

            if similarity_pct >= 90:
                change_type = 'retained'
            else:
                change_type = 'modified'

            diffs.append(FindingDiff(
                old_text=old_f,
                new_text=new_findings[best_match_idx],
                similarity=similarity_pct,
                change_type=change_type
            ))
        else:
            # Finding removed
            diffs.append(FindingDiff(
                old_text=old_f,
                new_text=None,
                similarity=0,
                change_type='removed'
            ))

    # Find added findings
    for i, new_f in enumerate(new_findings):
        if i in matched_new_indices or not new_f:
            continue

        diffs.append(FindingDiff(
            old_text=None,
            new_text=new_f,
            similarity=0,
            change_type='added'
        ))

    return diffs

# =========================================================
# 8C. DETERMINISTIC SOURCE EXTRACTION
# Extract metrics/entities directly from web snippets - NO LLM
# =========================================================

def extract_metrics_from_sources(web_context: Dict) -> Dict:
    """
    Extract numeric metrics directly from web search snippets.
    100% deterministic - no LLM involved.
    """
    extracted = {}
    search_results = web_context.get("search_results", [])

    # Patterns to match common metric formats
    patterns = [
        # Market size patterns
        (r'\$\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)\b', 'market_size'),
        (r'market\s+size[:\s]+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'market_size'),
        (r'valued\s+at\s+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'market_size'),
        (r'worth\s+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'market_size'),

        # Growth rate patterns
        (r'CAGR[:\s]+of?\s*(\d+(?:\.\d+)?)\s*%', 'cagr'),
        (r'(\d+(?:\.\d+)?)\s*%\s*CAGR', 'cagr'),
        (r'grow(?:th|ing)?\s+(?:at\s+)?(\d+(?:\.\d+)?)\s*%', 'growth_rate'),

        # Revenue patterns
        (r'revenue[:\s]+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'revenue'),

        # Year-specific values
        (r'(?:in\s+)?20\d{2}[:\s]+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'year_value'),
    ]

    all_matches = []

    for result in search_results:
        snippet = result.get("snippet", "")
        title = result.get("title", "")
        source = result.get("source", "")
        text = f"{title} {snippet}".lower()

        for pattern, metric_type in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    value_str, unit = match[0], match[1] if len(match) > 1 else ''
                else:
                    value_str, unit = match, ''

                try:
                    value = float(value_str)

                    # Normalize unit
                    unit_lower = unit.lower() if unit else ''
                    if unit_lower in ['t', 'trillion']:
                        unit_normalized = 'T'
                        value_in_billions = value * 1000
                    elif unit_lower in ['b', 'billion']:
                        unit_normalized = 'B'
                        value_in_billions = value
                    elif unit_lower in ['m', 'million']:
                        unit_normalized = 'M'
                        value_in_billions = value / 1000
                    elif unit_lower == '%':
                        unit_normalized = '%'
                        value_in_billions = value  # Keep as-is for percentages
                    else:
                        unit_normalized = ''
                        value_in_billions = value

                    all_matches.append({
                        'type': metric_type,
                        'value': value,
                        'unit': unit_normalized,
                        'value_normalized': value_in_billions,
                        'source': source,
                        'raw': f"{value_str} {unit}".strip()
                    })
                except (ValueError, TypeError):
                    continue

    # Deduplicate and select best matches by type
    metrics_by_type = {}
    for match in all_matches:
        mtype = match['type']
        if mtype not in metrics_by_type:
            metrics_by_type[mtype] = []
        metrics_by_type[mtype].append(match)

    # For each type, take the most common value (mode) or median
    metric_counter = 0
    for mtype, matches in metrics_by_type.items():
        if not matches:
            continue

        # Group by similar values (within 10%)
        value_groups = []
        for m in matches:
            added = False
            for group in value_groups:
                if group and abs(m['value_normalized'] - group[0]['value_normalized']) / max(group[0]['value_normalized'], 0.001) < 0.1:
                    group.append(m)
                    added = True
                    break
            if not added:
                value_groups.append([m])

        # Take the largest group (most consensus)
        if value_groups:
            best_group = max(value_groups, key=len)
            representative = best_group[0]

            metric_counter += 1
            metric_key = f"extracted_{mtype}_{metric_counter}"

            # Map type to readable name
            type_names = {
                'market_size': 'Market Size',
                'cagr': 'CAGR',
                'growth_rate': 'Growth Rate',
                'revenue': 'Revenue',
                'year_value': 'Market Value'
            }

            extracted[metric_key] = {
                'name': type_names.get(mtype, mtype.replace('_', ' ').title()),
                'value': representative['value'],
                'unit': f"${representative['unit']}" if representative['unit'] in ['T', 'B', 'M'] else representative['unit'],
                'source_count': len(best_group),
                'sources': list(set(m['source'] for m in best_group))[:3]
            }

    return extracted


def extract_entities_from_sources(web_context: Dict) -> List[Dict]:
    """
    Extract company/entity names from web search snippets.
    100% deterministic - no LLM involved.
    """
    search_results = web_context.get("search_results", [])

    # Common market leaders that appear in financial contexts
    known_entities = [
        # Tech
        'apple', 'microsoft', 'google', 'alphabet', 'amazon', 'meta', 'facebook',
        'nvidia', 'tesla', 'intel', 'amd', 'qualcomm', 'broadcom', 'cisco',
        'ibm', 'oracle', 'salesforce', 'adobe', 'netflix', 'uber', 'airbnb',
        # Finance
        'jpmorgan', 'goldman sachs', 'morgan stanley', 'bank of america',
        'wells fargo', 'citigroup', 'blackrock', 'vanguard', 'fidelity',
        # Auto
        'toyota', 'volkswagen', 'ford', 'gm', 'general motors', 'honda',
        'bmw', 'mercedes', 'byd', 'nio', 'rivian', 'lucid',
        # Pharma
        'pfizer', 'johnson & johnson', 'roche', 'novartis', 'merck',
        'abbvie', 'eli lilly', 'astrazeneca', 'moderna', 'gilead',
        # Energy
        'exxon', 'chevron', 'shell', 'bp', 'totalenergies', 'conocophillips',
        # Consumer
        'walmart', 'costco', 'home depot', 'nike', 'starbucks', 'mcdonalds',
        'coca-cola', 'pepsi', 'procter & gamble', 'unilever',
        # Regions (for market share by region)
        'north america', 'europe', 'asia pacific', 'asia-pacific', 'apac',
        'china', 'united states', 'japan', 'germany', 'india', 'uk',
        'latin america', 'middle east', 'africa'
    ]

    entity_mentions = {}

    for result in search_results:
        snippet = result.get("snippet", "").lower()
        title = result.get("title", "").lower()
        text = f"{title} {snippet}"

        for entity in known_entities:
            if entity in text:
                # Try to extract market share if mentioned
                share_pattern = rf'{re.escape(entity)}[^.]*?(\d+(?:\.\d+)?)\s*%'
                share_match = re.search(share_pattern, text, re.IGNORECASE)

                share = None
                if share_match:
                    share = f"{share_match.group(1)}%"

                if entity not in entity_mentions:
                    entity_mentions[entity] = {'count': 0, 'shares': []}

                entity_mentions[entity]['count'] += 1
                if share:
                    entity_mentions[entity]['shares'].append(share)

    # Sort by mention count and build list
    sorted_entities = sorted(entity_mentions.items(), key=lambda x: x[1]['count'], reverse=True)

    entities = []
    for entity_name, data in sorted_entities[:10]:  # Top 10
        # Use most common share if available
        share = None
        if data['shares']:
            # Take the most common share value
            share_counts = Counter(data['shares'])
            share = share_counts.most_common(1)[0][0]

        entities.append({
            'name': entity_name.title(),
            'share': share,
            'growth': None,  # Can't reliably extract growth from snippets
            'mention_count': data['count']
        })

    return entities

# ------------------------------------
# STABILITY SCORE COMPUTATION
# ------------------------------------

def compute_stability_score(
    metric_diffs: List[MetricDiff],
    entity_diffs: List[EntityDiff],
    finding_diffs: List[FindingDiff]
) -> float:
    """
    Compute overall stability score (0-100).
    Higher = more stable (less change).
    """
    scores = []

    # Metric stability (40% weight)
    if metric_diffs:
        stable_metrics = sum(1 for m in metric_diffs if m.change_type == 'unchanged')
        small_change = sum(1 for m in metric_diffs if m.change_pct and abs(m.change_pct) < 10)
        metric_score = ((stable_metrics + small_change * 0.5) / len(metric_diffs)) * 100
        scores.append(('metrics', metric_score, 0.4))

    # Entity stability (35% weight)
    if entity_diffs:
        stable_entities = sum(1 for e in entity_diffs if e.change_type == 'unchanged')
        entity_score = (stable_entities / len(entity_diffs)) * 100
        scores.append(('entities', entity_score, 0.35))

    # Finding stability (25% weight)
    if finding_diffs:
        retained = sum(1 for f in finding_diffs if f.change_type in ['retained', 'modified'])
        finding_score = (retained / len(finding_diffs)) * 100
        scores.append(('findings', finding_score, 0.25))

    if not scores:
        return 100.0

    # Weighted average
    total_weight = sum(s[2] for s in scores)
    weighted_sum = sum(s[1] * s[2] for s in scores)
    return round(weighted_sum / total_weight, 1)

# ------------------------------------
# MAIN DIFF COMPUTATION
# ------------------------------------

def compute_evolution_diff(old_analysis: Dict, new_analysis: Dict) -> EvolutionDiff:
    """
    Main entry point: compute complete deterministic diff between two analyses.
    """
    old_response = old_analysis.get('primary_response', {})
    new_response = new_analysis.get('primary_response', {})

    # Timestamps
    old_ts = old_analysis.get('timestamp', '')
    new_ts = new_analysis.get('timestamp', '')

    # Calculate time delta
    time_delta = None
    try:
        old_dt = datetime.fromisoformat(old_ts.replace('Z', '+00:00'))
        new_dt = datetime.fromisoformat(new_ts.replace('Z', '+00:00'))
        time_delta = round((new_dt.replace(tzinfo=None) - old_dt.replace(tzinfo=None)).total_seconds() / 3600, 1)
    except:
        pass

    # Compute diffs using CANONICAL metric registry for stable matching
    metric_diffs = compute_metric_diffs_canonical(
        old_response.get('primary_metrics', {}),
        new_response.get('primary_metrics', {})
    )

    entity_diffs = compute_entity_diffs(
        old_response.get('top_entities', []),
        new_response.get('top_entities', [])
    )

    # Use SEMANTIC finding comparison (stable across wording changes)
    finding_diffs = compute_semantic_finding_diffs(
        old_response.get('key_findings', []),
        new_response.get('key_findings', [])
    )

    # Compute stability
    stability = compute_stability_score(metric_diffs, entity_diffs, finding_diffs)

    # Summary stats
    summary_stats = {
        'metrics_increased': sum(1 for m in metric_diffs if m.change_type == 'increased'),
        'metrics_decreased': sum(1 for m in metric_diffs if m.change_type == 'decreased'),
        'metrics_unchanged': sum(1 for m in metric_diffs if m.change_type == 'unchanged'),
        'metrics_added': sum(1 for m in metric_diffs if m.change_type == 'added'),
        'metrics_removed': sum(1 for m in metric_diffs if m.change_type == 'removed'),
        'entities_moved_up': sum(1 for e in entity_diffs if e.change_type == 'moved_up'),
        'entities_moved_down': sum(1 for e in entity_diffs if e.change_type == 'moved_down'),
        'entities_unchanged': sum(1 for e in entity_diffs if e.change_type == 'unchanged'),
        'entities_added': sum(1 for e in entity_diffs if e.change_type == 'added'),
        'entities_removed': sum(1 for e in entity_diffs if e.change_type == 'removed'),
        'findings_retained': sum(1 for f in finding_diffs if f.change_type == 'retained'),
        'findings_modified': sum(1 for f in finding_diffs if f.change_type == 'modified'),
        'findings_added': sum(1 for f in finding_diffs if f.change_type == 'added'),
        'findings_removed': sum(1 for f in finding_diffs if f.change_type == 'removed'),
    }

    return EvolutionDiff(
        old_timestamp=old_ts,
        new_timestamp=new_ts,
        time_delta_hours=time_delta,
        metric_diffs=metric_diffs,
        entity_diffs=entity_diffs,
        finding_diffs=finding_diffs,
        stability_score=stability,
        summary_stats=summary_stats
    )

# ------------------------------------
# LLM EXPLANATION (ONLY INTERPRETS DIFFS)
# ------------------------------------

def generate_diff_explanation_prompt(diff: EvolutionDiff, query: str) -> str:
    """
    Generate prompt for LLM to EXPLAIN computed diffs (not discover them).
    """
    # Build metric changes text
    metric_changes = []
    for m in diff.metric_diffs:
        if m.change_type == 'increased':
            metric_changes.append(f"- {m.name}: {m.old_raw} → {m.new_raw} ({m.change_pct:+.1f}%) INCREASED")
        elif m.change_type == 'decreased':
            metric_changes.append(f"- {m.name}: {m.old_raw} → {m.new_raw} ({m.change_pct:+.1f}%) DECREASED")
        elif m.change_type == 'added':
            metric_changes.append(f"- {m.name}: NEW metric added with value {m.new_raw}")
        elif m.change_type == 'removed':
            metric_changes.append(f"- {m.name}: REMOVED (was {m.old_raw})")

    # Build entity changes text
    entity_changes = []
    for e in diff.entity_diffs:
        if e.change_type == 'moved_up':
            entity_changes.append(f"- {e.name}: Rank {e.old_rank} → {e.new_rank} (moved UP {e.rank_change} positions)")
        elif e.change_type == 'moved_down':
            entity_changes.append(f"- {e.name}: Rank {e.old_rank} → {e.new_rank} (moved DOWN {abs(e.rank_change)} positions)")
        elif e.change_type == 'added':
            entity_changes.append(f"- {e.name}: NEW entrant at rank {e.new_rank}")
        elif e.change_type == 'removed':
            entity_changes.append(f"- {e.name}: DROPPED OUT (was rank {e.old_rank})")

    # Build findings changes text
    finding_changes = []
    for f in diff.finding_diffs:
        if f.change_type == 'added':
            finding_changes.append(f"- NEW: {f.new_text}")
        elif f.change_type == 'removed':
            finding_changes.append(f"- REMOVED: {f.old_text}")
        elif f.change_type == 'modified':
            finding_changes.append(f"- MODIFIED: '{f.old_text[:50]}...' → '{f.new_text[:50]}...'")

    prompt = f"""You are a market analyst explaining changes between two analysis snapshots.

    QUERY: {query}
    TIME ELAPSED: {diff.time_delta_hours:.1f} hours
    STABILITY SCORE: {diff.stability_score:.0f}%

    COMPUTED METRIC CHANGES:
    {chr(10).join(metric_changes) if metric_changes else "No significant metric changes"}

    COMPUTED ENTITY RANKING CHANGES:
    {chr(10).join(entity_changes) if entity_changes else "No ranking changes"}

    COMPUTED FINDING CHANGES:
    {chr(10).join(finding_changes) if finding_changes else "No finding changes"}

    SUMMARY STATS:
    - Metrics: {diff.summary_stats['metrics_increased']} increased, {diff.summary_stats['metrics_decreased']} decreased, {diff.summary_stats['metrics_unchanged']} unchanged
    - Entities: {diff.summary_stats['entities_moved_up']} moved up, {diff.summary_stats['entities_moved_down']} moved down
    - Findings: {diff.summary_stats['findings_added']} new, {diff.summary_stats['findings_removed']} removed

    YOUR TASK: Provide a 3-5 sentence executive interpretation of these changes.
    - What is the overall trend (improving/declining/stable)?
    - What are the most significant changes and why might they have occurred?
    - What should stakeholders pay attention to?

    Return ONLY a JSON object:
    {{
        "trend": "improving/declining/stable",
        "headline": "One sentence summary of key change",
        "interpretation": "3-5 sentence detailed interpretation",
        "watch_items": ["Item 1 to monitor", "Item 2 to monitor"]
    }}
    """
    return prompt

def get_llm_explanation(diff: EvolutionDiff, query: str) -> Dict:
    """
    Ask LLM to explain the computed diffs (not discover them).
    """
    prompt = generate_diff_explanation_prompt(diff, query)

    headers = {
        "Authorization": f"Bearer {PERPLEXITY_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "sonar",
        "temperature": 0.0,  # Deterministic
        "max_tokens": 500,
        "top_p": 1.0,
        "messages": [{"role": "user", "content": prompt}]
    }

    try:
        resp = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        content = data["choices"][0]["message"]["content"]

        parsed = parse_json_safely(content, "Explanation")
        if parsed:
            return parsed
    except Exception as e:
        st.warning(f"LLM explanation failed: {e}")

    # Fallback
    return {
        "trend": "stable" if diff.stability_score >= 70 else "changing",
        "headline": f"Analysis shows {diff.stability_score:.0f}% stability over {diff.time_delta_hours:.0f} hours",
        "interpretation": "Unable to generate detailed interpretation.",
        "watch_items": []
    }


# =========================================================
# 8B. EVOLUTION DASHBOARD RENDERING
# =========================================================

def render_evolution_results(diff: EvolutionDiff, explanation: Dict, query: str):
    """Render deterministic evolution results"""

    st.header("📈 Evolution Analysis")
    st.markdown(f"**Query:** {query}")

    # Overview metrics
    col1, col2, col3, col4 = st.columns(4)

    if diff.time_delta_hours:
        if diff.time_delta_hours < 24:
            time_str = f"{diff.time_delta_hours:.1f}h"
        else:
            time_str = f"{diff.time_delta_hours/24:.1f}d"
        col1.metric("Time Elapsed", time_str)
    else:
        col1.metric("Time Elapsed", "Unknown")

    col2.metric("Stability", f"{diff.stability_score:.0f}%")

    trend = explanation.get('trend', 'stable')
    trend_icon = {'improving': '📈', 'declining': '📉', 'stable': '➡️'}.get(trend, '➡️')
    col3.metric("Trend", f"{trend_icon} {trend.title()}")

    # Stability indicator
    if diff.stability_score >= 80:
        col4.success("🟢 Highly Stable")
    elif diff.stability_score >= 60:
        col4.warning("🟡 Moderate Changes")
    else:
        col4.error("🔴 Significant Drift")

    # Headline
    st.info(f"**{explanation.get('headline', 'Analysis complete')}**")

    st.markdown("---")

    # Interpretation

    # =====================================================================
    # PATCH FIX39 (ADDITIVE): enforce unit-required gate at render time
    # =====================================================================
    try:
        # best effort: use schema carried on diff (if any) else global latest schema
        schema = getattr(diff, "metric_schema_frozen", None)
        if not isinstance(schema, dict):
            schema = {}
        _fix39_sanitize_evolutiondiff_object(diff, schema)
    except Exception:
        pass

    st.subheader("📋 Interpretation")
    st.markdown(explanation.get('interpretation', 'No interpretation available'))

    # Watch items
    watch_items = explanation.get('watch_items', [])
    if watch_items:
        st.markdown("**🔔 Watch Items:**")
        for item in watch_items:
            st.markdown(f"- {item}")

    st.markdown("---")

    # Metric Changes Table
    st.subheader("💰 Metric Changes")
    if diff.metric_diffs:
        metric_rows = []

        def _fmt_currency_first(raw: str, unit: str) -> str:
            """
            Formats evolution metrics as:
            - S$29.8B
            - $120M
            - 29.8%
            """
            raw = (raw or "").strip()
            unit = (unit or "").strip()

            if not raw or raw == "-":
                return "-"

            # If already currency-first, trust it
            if raw.startswith("S$") or raw.startswith("$"):
                return raw

            # Percent case
            if unit == "%":
                return f"{raw}%"

            # Detect currency from unit
            currency = ""
            scale = unit.replace(" ", "")

            if scale.upper().startswith("SGD"):
                currency = "S$"
                scale = scale[3:]
            elif scale.upper().startswith("USD"):
                currency = "$"
                scale = scale[3:]
            elif scale.startswith("S$"):
                currency = "S$"
                scale = scale[2:]
            elif scale.startswith("$"):
                currency = "$"
                scale = scale[1:]

            # Human-readable units
            if unit.lower().endswith("billion"):
                return f"{currency}{raw} billion".strip()
            if unit.lower().endswith("million"):
                return f"{currency}{raw} million".strip()

            # Compact units (B/M/K)
            if scale.upper() in {"B", "M", "K"}:
                return f"{currency}{raw}{scale}".strip()

            # Fallback
            return f"{currency}{raw} {unit}".strip()

        for m in diff.metric_diffs:
            icon = {
                'increased': '📈', 'decreased': '📉', 'unchanged': '➡️',
                'added': '🆕', 'removed': '❌'
            }.get(m.change_type, '•')

            change_str = f"{m.change_pct:+.1f}%" if m.change_pct is not None else "-"

            prev_raw = m.old_raw or "-"
            curr_raw = m.new_raw or "-"

            metric_rows.append({
                "": icon,
                "Metric": m.name,
                "Previous": _fmt_currency_first(prev_raw, getattr(m, "unit", "") or ""),
                "Current":  _fmt_currency_first(curr_raw, getattr(m, "unit", "") or ""),
                "Change": change_str,
                "Status": m.change_type.replace('_', ' ').title()
            })

        st.dataframe(pd.DataFrame(metric_rows), hide_index=True, use_container_width=True)
    else:
        st.info("No metrics to compare")

    st.markdown("---")

    # Entity Changes Table
    st.subheader("🏢 Entity Ranking Changes")
    if diff.entity_diffs:
        entity_rows = []
        for e in diff.entity_diffs:
            icon = {
                'moved_up': '⬆️', 'moved_down': '⬇️', 'unchanged': '➡️',
                'added': '🆕', 'removed': '❌'
            }.get(e.change_type, '•')

            rank_str = f"{e.rank_change:+d}" if e.rank_change else "-"

            entity_rows.append({
                "": icon,
                "Entity": e.name,
                "Old Rank": e.old_rank or "-",
                "New Rank": e.new_rank or "-",
                "Rank Δ": rank_str,
                "Old Share": e.old_share or "-",
                "New Share": e.new_share or "-"
            })
        st.dataframe(pd.DataFrame(entity_rows), hide_index=True, use_container_width=True)
    else:
        st.info("No entities to compare")

    st.markdown("---")

    # Finding Changes
    st.subheader("🔍 Finding Changes")
    if diff.finding_diffs:
        added = [f for f in diff.finding_diffs if f.change_type == 'added']
        removed = [f for f in diff.finding_diffs if f.change_type == 'removed']
        modified = [f for f in diff.finding_diffs if f.change_type == 'modified']

        if added:
            st.markdown("**🆕 New Findings:**")
            for f in added:
                st.success(f"• {f.new_text}")

        if removed:
            st.markdown("**❌ Removed Findings:**")
            for f in removed:
                st.error(f"• ~~{f.old_text}~~")

        if modified:
            st.markdown("**✏️ Modified Findings:**")
            for f in modified:
                st.warning(f"• {f.new_text} *(similarity: {f.similarity:.0f}%)*")
    else:
        st.info("No findings to compare")

    st.markdown("---")

    # Summary Stats
    st.subheader("📊 Change Summary")
    stats = diff.summary_stats

    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("**Metrics:**")
        st.write(f"📈 {stats['metrics_increased']} increased")
        st.write(f"📉 {stats['metrics_decreased']} decreased")
        st.write(f"➡️ {stats['metrics_unchanged']} unchanged")

    with col2:
        st.markdown("**Entities:**")
        st.write(f"⬆️ {stats['entities_moved_up']} moved up")
        st.write(f"⬇️ {stats['entities_moved_down']} moved down")
        st.write(f"🆕 {stats['entities_added']} new")

    with col3:
        st.markdown("**Findings:**")
        st.write(f"✅ {stats['findings_retained']} retained")
        st.write(f"✏️ {stats['findings_modified']} modified")
        st.write(f"🆕 {stats['findings_added']} new")


# =========================================================
# 8D. SOURCE-ANCHORED EVOLUTION
# Re-fetch the SAME sources from previous analysis for true stability
# Enhanced fetch_url_content function to use scrapingdog as fallback
# =========================================================

def _extract_pdf_text_from_bytes(pdf_bytes: bytes, max_pages: int = 6, max_chars: int = 7000) -> Optional[str]:
    """
    Extract readable text from PDF bytes deterministically.
    Limits pages/chars for speed and consistent output.
    """
    try:
        reader = PdfReader(io.BytesIO(pdf_bytes))
        texts = []
        for i, page in enumerate(reader.pages[:max_pages]):
            t = page.extract_text() or ""
            t = t.replace("\x00", " ").strip()
            if t:
                texts.append(t)
        joined = "\n".join(texts).strip()
        if len(joined) < 200:
            return None
        return joined[:max_chars]
    except Exception:
        return None



def fetch_url_content(url: str) -> Optional[str]:
    """Fetch content from a specific URL with ScrapingDog fallback"""

    def extract_text(html: str) -> Optional[str]:
        """Extract clean text from HTML"""
        soup = BeautifulSoup(html, 'html.parser')
        for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
            tag.decompose()
        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        clean_text = ' '.join(line for line in lines if line)
        return clean_text[:5000] if len(clean_text) > 200 else None

    # Try 1: Direct request
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        }
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()

        if 'captcha' not in resp.text.lower():
            content = extract_text(resp.text)
            if content:
                return content
    except:
        pass

    # Try 2: ScrapingDog API
    if SCRAPINGDOG_KEY:
        try:
            api_url = "https://api.scrapingdog.com/scrape"
            params = {"api_key": SCRAPINGDOG_KEY, "url": url, "dynamic": "false"}
            resp = requests.get(api_url, params=params, timeout=30)
            if resp.status_code == 200:
                content = extract_text(resp.text)
                if content:
                    return content
        except:
            pass

    return None



def fetch_url_content_with_status(url: str, timeout: int = 25):
    """
    Fetch URL content and return (text, status_detail).

    status_detail:
      - "success"
      - "success_pdf"
      - "http_<code>"
      - "exception:<TypeName>"
      - "empty"
      - "success_scrapingdog"

    Hardened:
      - Uses browser-like headers for direct fetch
      - Falls back to ScrapingDog when blocked/empty and SCRAPINGDOG_KEY is available
      - Avoids returning binary garbage as "text"
    """
    import re
    import requests

    def _normalize_url(s: str) -> str:
        t = (s or "").strip()
        if not t:
            return ""
        if re.match(r"^https?://", t, flags=re.I):
            return t
        if re.match(r"^[a-z0-9.-]+\.[a-z]{2,}(/.*)?$", t, flags=re.I):
            return "https://" + t
        return ""

    url = _normalize_url(url)
    if not url:
        return None, "empty"

    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
    }

    # ---------- 1) Direct fetch ----------
    try:
        resp = requests.get(url, timeout=timeout, headers=headers, allow_redirects=True)

        ct = (resp.headers.get("content-type", "") or "").lower()

        if resp.status_code >= 400:
            # If blocked, try ScrapingDog fallback (optional)
            if resp.status_code in (401, 403, 429) and globals().get("SCRAPINGDOG_KEY"):
                txt = _fetch_via_scrapingdog(url, timeout=timeout)
                if txt and txt.strip():
                    return txt, "success_scrapingdog"
            return None, f"http_{resp.status_code}"

        # PDF handling
        if "application/pdf" in ct or url.lower().endswith(".pdf"):
            try:
                import io
                import pdfplumber  # type: ignore
                with pdfplumber.open(io.BytesIO(resp.content)) as pdf:
                    out = []
                    for page in pdf.pages[:20]:
                        t = page.extract_text() or ""
                        if t.strip():
                            out.append(t)
                text = "\n".join(out).strip()
                if not text:
                    return None, "empty"
                return text, "success_pdf"
            except Exception as e:
                return None, f"exception:{type(e).__name__}"

        # Text/HTML
        text = resp.text or ""
        # If empty or suspiciously short, attempt ScrapingDog (optional)
        if (not text.strip() or len(text.strip()) < 300) and globals().get("SCRAPINGDOG_KEY"):
            txt = _fetch_via_scrapingdog(url, timeout=timeout)
            if txt and txt.strip():
                return txt, "success_scrapingdog"

        if not text.strip():
            return None, "empty"

        return text, "success"

    except Exception as e:
        # ScrapingDog as last resort for network-y issues
        try:
            if globals().get("SCRAPINGDOG_KEY"):
                txt = _fetch_via_scrapingdog(url, timeout=timeout)
                if txt and txt.strip():
                    return txt, "success_scrapingdog"
        except Exception:
            pass
        return None, f"exception:{type(e).__name__}"


def _fetch_via_scrapingdog(url: str, timeout: int = 25) -> str:
    """
    Internal helper used by fetch_url_content_with_status.
    Returns raw HTML text from ScrapingDog (or "" on failure).
    """
    import requests

    key = globals().get("SCRAPINGDOG_KEY")
    if not key:
        return ""

    params = {"api_key": key, "url": url, "dynamic": "false"}
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        resp = requests.get("https://api.scrapingdog.com/scrape", params=params, headers=headers, timeout=timeout)
        if resp.status_code >= 400:
            return ""
        return resp.text or ""
    except Exception:
        return ""

def get_extractor_fingerprint() -> str:
    """
    Bump this string whenever you change extraction or normalization behavior.
    Used to decide whether cached extracted_numbers are still valid.
    """
    return "extract_v2_normunits_2026-01-02"



def extract_numbers_from_text(text: str) -> List[Dict]:
    """
    Backward-compatible wrapper.

    v7_34 tightening:
    - Delegate to extract_numbers_with_context() so junk suppression is applied consistently.
    """
    try:
        return extract_numbers_with_context(text or "", source_url="", max_results=600) or []
    except Exception:
        return []


def _parse_iso_dt(ts: Optional[str]) -> Optional[datetime]:
    if not ts:
        return None
    try:
        ts2 = ts.replace("Z", "+00:00")
        dt = datetime.fromisoformat(ts2)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception:
        return None

def now_utc() -> datetime:
    """Timezone-aware UTC now (prevents naive/aware datetime bugs)."""
    return datetime.now(timezone.utc)


def _normalize_number_to_parse_base(value: float, unit: str) -> float:
    u = (unit or "").strip().upper()
    if u == "T":
        return value * 1_000_000
    if u == "B":
        return value * 1_000
    if u == "M":
        return value * 1
    if u == "K":
        return value * 0.001
    if u == "%":
        return value
    return value

def run_source_anchored_evolution(previous_data: dict, web_context: dict = None) -> dict:
    """
    Backward-compatible entrypoint used by the Streamlit Evolution UI.

    Enhancements:
      - Accept optional web_context so evolution can reuse same-run analysis upstream artifacts.
      - ALWAYS returns a dict with required keys (even on crash).
    """
    fn = globals().get("compute_source_anchored_diff")

    def _fail(msg: str) -> dict:
        return {
            "status": "failed",
            "message": msg,
            "sources_checked": 0,
            "sources_fetched": 0,
            "numbers_extracted_total": 0,
            "stability_score": 0.0,
            "summary": {
                "total_metrics": 0,
                "metrics_found": 0,
                "metrics_increased": 0,
                "metrics_decreased": 0,
                "metrics_unchanged": 0,
            },
            "metric_changes": [],
            "source_results": [],
            "interpretation": "Evolution failed.",
        }

    if not callable(fn):
        return _fail("compute_source_anchored_diff() is not defined, so source-anchored evolution cannot run.")

    try:
        # Support both old signature (previous_data) and new signature (previous_data, web_context)
        try:
            out = fn(previous_data, web_context=web_context)
        except TypeError:
            out = fn(previous_data)
    except Exception as e:
        return _fail(f"compute_source_anchored_diff crashed: {e}")

    if not isinstance(out, dict):
        return _fail("compute_source_anchored_diff returned a non-dict payload.")

    # Renderer-required defaults
    out.setdefault("status", "success")
    out.setdefault("message", "")
    out.setdefault("sources_checked", 0)
    out.setdefault("sources_fetched", 0)
    out.setdefault("numbers_extracted_total", 0)
    out.setdefault("stability_score", 0.0)
    out.setdefault("summary", {})
    out["summary"].setdefault("total_metrics", len(out.get("metric_changes") or []))
    out["summary"].setdefault("metrics_found", 0)
    out["summary"].setdefault("metrics_increased", 0)
    out["summary"].setdefault("metrics_decreased", 0)
    out["summary"].setdefault("metrics_unchanged", 0)
    out.setdefault("metric_changes", [])
    out.setdefault("source_results", [])
    out.setdefault("interpretation", "")
    # =====================================================================
    # PATCH FIX39 (ADDITIVE): sanitize evolution output before publish/render
    # =====================================================================
    try:
        _fix39_sanitize_metric_change_rows(out)
    except Exception:
        pass

    return out
# =========================================================
# ROBUST EVOLUTION HELPERS (DETERMINISTIC)
# =========================================================

NON_DATA_CONTEXT_HINTS = [
    "table of contents", "cookie", "privacy", "terms", "copyright",
    "subscribe", "newsletter", "login", "sign in", "nav", "footer"
]


def _truncate_json_safely_for_sheets(json_str: str, max_chars: int = 45000) -> str:
    """
# =====================================================================
# PATCH FIX41G (ADDITIVE): Normalize web_context and capture force_rebuild
# Ensures the UI flag reaches the fastpath gate and is recorded in output.
# =====================================================================
if web_context is None or not isinstance(web_context, dict):
    web_context = {}
_fix41_force_rebuild_seen = False
try:
    _fix41_force_rebuild_seen = bool(web_context.get("force_rebuild"))
except Exception:
    _fix41_force_rebuild_seen = False
# =====================================================================

    PATCH TS1 (ADDITIVE): JSON-safe truncation wrapper
    - Ensures json.loads always succeeds for any returned value.
    - Stores a preview when oversized.
    """
    import json

    s = "" if json_str is None else str(json_str)
    if len(s) <= max_chars:
        return s

    preview_len = max(0, int(max_chars) - 700)
    wrapper = {
        "_sheets_safe": True,
        "_sheet_write": {
            "truncated": True,
            "mode": "json_wrapper",
            "note": "Payload exceeded cell limit; stored preview only.",
        },
        "preview": s[:preview_len],
    }
    try:
        return json.dumps(wrapper, ensure_ascii=False, default=str)
    except Exception:
        return '{"_sheets_safe":true,"_sheet_write":{"truncated":true,"mode":"json_wrapper","note":"json.dumps failed"}}'


def _truncate_for_sheets(s: str, max_chars: int = 45000) -> str:
    """Hard cap to stay under Google Sheets 50k/cell limit."""
    if s is None:
        return ""
    s = str(s)
    if len(s) <= max_chars:
        return s
    head = s[: int(max_chars * 0.75)]
    tail = s[- int(max_chars * 0.20):]
    return head + "\n...\n[TRUNCATED FOR GOOGLE SHEETS]\n...\n" + tail



def _summarize_heavy_fields_for_sheets(obj: dict) -> dict:
    """
    Summarize fields that commonly exceed the per-cell limit while keeping debug utility.
    Only used for Sheets serialization; does NOT modify your in-memory analysis dict.
    """
    if not isinstance(obj, dict):
        return {"_type": str(type(obj)), "value": str(obj)[:500]}

    out = dict(obj)

    # Common bloat fields
    if "scraped_meta" in out:
        sm = out.get("scraped_meta")
        if isinstance(sm, dict):
            compact = {}
            for url, meta in list(sm.items())[:12]:
                if isinstance(meta, dict):
                    compact[url] = {
                        "status": meta.get("status"),
                        "status_detail": meta.get("status_detail"),
                        "numbers_found": meta.get("numbers_found"),
                        "fingerprint": meta.get("fingerprint"),
                        "clean_text_len": meta.get("clean_text_len"),
                    }
            out["scraped_meta"] = {"_summary": True, "count": len(sm), "sample": compact}
        else:
            out["scraped_meta"] = {"_summary": True, "type": str(type(sm))}

    for big_key in ("source_results", "baseline_sources_cache", "baseline_sources_cache_compact"):
        if big_key in out:
            sr = out.get(big_key)
            if isinstance(sr, list):
                sample = []
                for item in sr[:2]:
                    if isinstance(item, dict):
                        item2 = dict(item)
                        if isinstance(item2.get("extracted_numbers"), list):
                            item2["extracted_numbers"] = {"_summary": True, "count": len(item2["extracted_numbers"])}
                        sample.append(item2)
                out[big_key] = {"_summary": True, "count": len(sr), "sample": sample}
            else:
                out[big_key] = {"_summary": True, "type": str(type(sr))}

    # If you store full scraped_content anywhere, summarize it too
    if "scraped_content" in out:
        sc = out.get("scraped_content")
        if isinstance(sc, dict):
            out["scraped_content"] = {"_summary": True, "count": len(sc), "keys_sample": list(sc.keys())[:10]}
        else:
            out["scraped_content"] = {"_summary": True, "type": str(type(sc))}

    # =====================================================================
    # PATCH SS2 (ADDITIVE, REQUIRED): summarize nested heavy fields under out["results"]
    # Why:
    # - Your biggest payload is typically results.baseline_sources_cache (full snapshots)
    # - The previous summarizer only handled top-level keys, so Sheets payload still exceeded limits
    # - This keeps the saved JSON smaller AND keeps json.loads(get_history) working reliably
    # =====================================================================
    try:
        r = out.get("results")
        if isinstance(r, dict):
            r2 = dict(r)

            for big_key in ("baseline_sources_cache", "source_results"):
                if big_key in r2:
                    sr = r2.get(big_key)
                    if isinstance(sr, list):
                        sample = []
                        for item in sr[:2]:
                            if isinstance(item, dict):
                                item2 = dict(item)
                                if isinstance(item2.get("extracted_numbers"), list):
                                    item2["extracted_numbers"] = {
                                        "_summary": True,
                                        "count": len(item2["extracted_numbers"])
                                    }
                                sample.append(item2)
                        r2[big_key] = {"_summary": True, "count": len(sr), "sample": sample}
                    else:
                        r2[big_key] = {"_summary": True, "type": str(type(sr))}

            out["results"] = r2
    except Exception:
        pass
    # =====================================================================

    return out



def make_sheet_safe_json(obj: dict, max_chars: int = 45000) -> str:
    """
    Serialize sheet-safe JSON under the cell limit.

    NOTE / CONFLICT:
      - The prior implementation used _truncate_for_sheets() on the JSON string, which can produce
        invalid JSON (cut mid-string). Invalid JSON rows are skipped by get_history() (json.loads fails),
        so evolution can't pick them up.
      - This patch preserves summarization but replaces raw string truncation with a JSON wrapper
        that is ALWAYS valid JSON.

    Output behavior:
      - If JSON fits: returns full compact JSON string.
      - If too large: returns a valid JSON wrapper with a preview + metadata.
    """
    import json

    # Keep existing behavior: summarize heavy fields
    compact = _summarize_heavy_fields_for_sheets(obj if isinstance(obj, dict) else {"value": obj})
    if isinstance(compact, dict):
        compact["_sheets_safe"] = True

    # Try to serialize
    try:
        s = json.dumps(compact, ensure_ascii=False, default=str)
    except Exception:
        # ultra-safe fallback (still return valid JSON)
        try:
            s = json.dumps({"_sheets_safe": True, "_sheet_write": {"error": "json.dumps failed"}}, ensure_ascii=False)
        except Exception:
            return '{"_sheets_safe":true,"_sheet_write":{"error":"json.dumps failed"}}'

    # If it fits, return as-is
    if isinstance(s, str) and len(s) <= int(max_chars or 45000):
        return s

    # =========================
    # PATCH SS1 (BUGFIX, REQUIRED): valid JSON wrapper when oversized
    # - Never return mid-string truncations that break json.loads in get_history().
    # =========================
    try:
        preview_len = max(0, int(max_chars or 45000) - 700)  # leave room for wrapper fields
        wrapper = {
            "_sheets_safe": True,
            "_sheet_write": {
                "truncated": True,
                "mode": "sheets_safe_wrapper",
                "note": "Payload exceeded cell limit; stored preview only. Full snapshots must be stored separately if needed.",
            },
            # Keep a preview for UI/debugging
            "preview": s[:preview_len],
        }

        # Optional: carry minimal identity fields for convenience (additive)
        if isinstance(obj, dict):
            wrapper["question"] = (obj.get("question") or "")[:200]
            wrapper["timestamp"] = obj.get("timestamp")
            wrapper["code_version"] = obj.get("code_version") or (obj.get("primary_response") or {}).get("code_version")

            # =========================
            # PATCH SS1B (ADDITIVE, REQUIRED FOR SNAPSHOT REHYDRATION):
            # Carry snapshot pointers even when the payload is wrapped.
            # Without these fields, evolution cannot rehydrate full snapshots
            # from the Snapshots worksheet (or local fallback) and will fail
            # the snapshot gate with "No valid snapshots".
            # =========================
            try:
                _ssh = obj.get("source_snapshot_hash") or (obj.get("results") or {}).get("source_snapshot_hash")
                _ref = obj.get("snapshot_store_ref") or (obj.get("results") or {}).get("snapshot_store_ref")
                if _ssh:
                    wrapper["source_snapshot_hash"] = _ssh
                if _ref:
                    wrapper["snapshot_store_ref"] = _ref
            except Exception:
                pass

        return json.dumps(wrapper, ensure_ascii=False, default=str)
    except Exception:
        return '{"_sheets_safe":true,"_sheet_write":{"truncated":true,"mode":"sheets_safe_wrapper","note":"wrapper failed"}}'


# =====================================================================
# PATCH ES1D (ADDITIVE): external snapshot store (local file-based)
# Purpose:
#   - Store full baseline_sources_cache outside Google Sheets when rows
#     are too large (Sheets wrapper / preview mode).
#   - Allow deterministic rehydration for evolution (no refetch).
# =====================================================================
def _snapshot_store_dir() -> str:
    import os
    d = os.path.join(os.getcwd(), "snapshot_store")
    try:
        os.makedirs(d, exist_ok=True)
    except Exception:
        pass
    return d

def store_full_snapshots_local(baseline_sources_cache: list, source_snapshot_hash: str) -> str:
    """
    Store full snapshots deterministically by hash. Returns a store ref string (path).
    Additive-only helper.
    """
    import os, json
    if not source_snapshot_hash:
        return ""
    if not isinstance(baseline_sources_cache, list) or not baseline_sources_cache:
        return ""

    path = os.path.join(_snapshot_store_dir(), f"{source_snapshot_hash}.json")
    try:
        # write-once semantics (deterministic)
        if os.path.exists(path) and os.path.getsize(path) > 0:
            return path
    except Exception:
        pass

    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(baseline_sources_cache, f, ensure_ascii=False, default=str)
        return path
    except Exception:
        return ""

def load_full_snapshots_local(snapshot_store_ref: str) -> list:
    """
    Load full snapshots from a store ref string (path). Returns [] if not available.
    """
    import json, os
    try:
        if not snapshot_store_ref or not isinstance(snapshot_store_ref, str):
            return []
        if not os.path.exists(snapshot_store_ref):
            return []
        with open(snapshot_store_ref, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data if isinstance(data, list) else []
    except Exception:
        return []

# =====================================================================
# PATCH ES1E (ADDITIVE): deterministic source_snapshot_hash helper
# =====================================================================
def compute_source_snapshot_hash(baseline_sources_cache: list) -> str:
    import hashlib
    pairs = []
    for sr in (baseline_sources_cache or []):
        if not isinstance(sr, dict):
            continue
        u = (sr.get("source_url") or sr.get("url") or "").strip()
        fp = (sr.get("fingerprint") or sr.get("content_fingerprint") or "").strip()
        if u:
            pairs.append((u, fp))
    pairs.sort()
    sig = "|".join([f"{u}#{fp}" for (u, fp) in pairs])
    return hashlib.sha256(sig.encode("utf-8")).hexdigest() if sig else ""
# =====================================================================
# =====================================================================
# PATCH SS6 (ADDITIVE): build full baseline_sources_cache from evidence_records
# Why:
# - Sheets-safe summarization may replace baseline_sources_cache/extracted_numbers
#   with summary dicts. However, evidence_records often remains available and is
#   already deterministic, snapshot-derived data.
# - This helper reconstructs the minimal snapshot shape needed for
#   source-anchored evolution WITHOUT re-fetching or heuristic matching.
# =====================================================================

            # =========================
# PATCH A (ADD): Snapshot hash v2 (stable, content-weighted)
# - Keeps v1 compute_source_snapshot_hash() for backward compatibility.
# - v2 includes url + status + fingerprint + (anchor_hash,value_norm,unit_tag) tuples (bounded) for stronger identity.
            # =========================
def compute_source_snapshot_hash_v2(baseline_sources_cache: list, max_items_per_source: int = 120) -> str:
    import hashlib
    import json

    try:
        sources = baseline_sources_cache if isinstance(baseline_sources_cache, list) else []
        parts = []
        for s in sources:
            if not isinstance(s, dict):
                continue
            url = str(s.get("url") or "")
            status = str(s.get("status") or "")
            status_detail = str(s.get("status_detail") or "")
            fingerprint = str(s.get("fingerprint") or "")

            nums = s.get("extracted_numbers") or s.get("numbers") or []
            # Sometimes stored in summarized form
            if isinstance(nums, dict) and nums.get("_summary") and isinstance(nums.get("count"), int):
                # no details available; just use summary
                num_tuples = [("summary_count", int(nums.get("count")))]
            else:
                num_list = nums if isinstance(nums, list) else []
                num_tuples = []
                for n in num_list[: int(max_items_per_source or 120)]:
                    if not isinstance(n, dict):
                        continue
                    ah = str(n.get("anchor_hash") or "")
                    vn = n.get("value_norm")
                    ut = str(n.get("unit_tag") or n.get("unit") or "")
                    # Use JSON for float stability + None handling
                    num_tuples.append((ah, vn, ut))
                # Deterministic order
                num_tuples = sorted(num_tuples, key=lambda t: (t[0], str(t[1]), t[2]))

            parts.append({
                "url": url,
                "status": status,
                "status_detail": status_detail,
                "fingerprint": fingerprint,
                "nums": num_tuples,
            })

        # Deterministic ordering of sources
        parts = sorted(parts, key=lambda d: (d.get("url",""), d.get("fingerprint",""), d.get("status","")))

        payload = json.dumps(parts, ensure_ascii=False, default=str, separators=(",", ":"))
        return hashlib.sha256(payload.encode("utf-8")).hexdigest()
    except Exception:
        # Ultra-safe fallback (still deterministic-ish)
        try:
            return hashlib.sha256(str(baseline_sources_cache).encode("utf-8")).hexdigest()
        except Exception:
            return "0"*64

def build_baseline_sources_cache_from_evidence_records(evidence_records):

    """

    Rebuild a minimal baseline_sources_cache from evidence_records deterministically.


    PATCH AI3 (ADDITIVE): anchor integrity

    - Ensures each candidate has anchor_hash + candidate_id (derived if missing)

    - Preserves analysis-aligned numeric normalization fields when present

    - Deterministic ordering by (source_url, fingerprint)

    """

    import hashlib


    if not isinstance(evidence_records, list) or not evidence_records:

        return []


    def _sha1(s: str) -> str:

        try:

            return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()

        except Exception:

            return ""


    def _ensure_anchor_fields(c: dict, source_url: str = "") -> dict:

        c = c if isinstance(c, dict) else {}

        ctx = c.get("context_snippet") or c.get("context") or ""

        if isinstance(ctx, str):

            ctx = ctx.strip()[:240]

        else:

            ctx = ""

        raw = c.get("raw")

        if raw is None:

            v = c.get("value_norm") if c.get("value_norm") is not None else c.get("value")

            u = c.get("base_unit") or c.get("unit") or ""

            raw = f"{v}{u}"

        raw = str(raw)[:120]


        ah = c.get("anchor_hash") or c.get("anchor") or ""

        if not ah:

            ah = _sha1(f"{source_url}|{raw}|{ctx}")

            if ah:

                c["anchor_hash"] = ah

        if not c.get("candidate_id") and ah:

            c["candidate_id"] = str(ah)[:16]

        if source_url and not c.get("source_url"):

            c["source_url"] = source_url

        if ctx and not c.get("context_snippet"):

            c["context_snippet"] = ctx

        return c


    by_url = {}

    for rec in evidence_records:

        if not isinstance(rec, dict):

            continue

        url = rec.get("source_url") or rec.get("url") or ""

        fp = rec.get("fingerprint") or ""

        # candidates may be stored under candidates or extracted_numbers depending on producer

        cand_list = rec.get("candidates")

        if not isinstance(cand_list, list):

            cand_list = rec.get("extracted_numbers")

        if not isinstance(cand_list, list):

            cand_list = []


        out_cands = []

        for c in cand_list:

            if not isinstance(c, dict):

                continue

            cc = _ensure_anchor_fields(dict(c), url)

            out_cands.append(cc)


        if not out_cands:

            continue


        key = (str(url), str(fp))

        by_url.setdefault(key, {"source_url": url, "fingerprint": fp, "extracted_numbers": []})

        by_url[key]["extracted_numbers"].extend(out_cands)


    rebuilt = list(by_url.values())


    # deterministic sort & per-source deterministic candidate order

    try:

        rebuilt.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))

        for s in rebuilt:

            if isinstance(s, dict) and isinstance(s.get("extracted_numbers"), list):

                s["extracted_numbers"] = sorted(

                    s["extracted_numbers"],

                    key=lambda c: (

                        str(c.get("anchor_hash") or ""),

                        str(c.get("candidate_id") or ""),

                        str(c.get("raw") or ""),

                        str(c.get("unit") or ""),

                    )

                )

    except Exception:

        pass


    # =====================================================================
    # PATCH FIX41AFC5 (ADDITIVE): attach eligibility-hardening debug counters
    # =====================================================================
    try:
        if isinstance(rebuilt, dict):
            rebuilt.setdefault("_fix41afc5_debug", {})
            if isinstance(rebuilt.get("_fix41afc5_debug"), dict):
                rebuilt["_fix41afc5_debug"].update(dict(_fix41afc5_dbg2))
    except Exception:
        pass
    # =====================================================================

    return rebuilt
def _sheets_now_ts():
    import time
    return time.time()

def _sheets_cache_get(key: str):
    try:
        item = _SHEETS_READ_CACHE.get(key)
        if not item:
            return None
        ts, val = item
        if (_sheets_now_ts() - ts) > _SHEETS_READ_CACHE_TTL_SEC:
            return None
        return val
    except Exception:
        return None

def _sheets_cache_set(key: str, val):
    try:
        _SHEETS_READ_CACHE[key] = (_sheets_now_ts(), val)
    except Exception:
        pass

def _is_sheets_rate_limit_error(err: Exception) -> bool:
    s = ""
    try:
        s = str(err) or ""
    except Exception:
        s = ""
    # Common markers seen via gspread/googleapiclient:
    markers = ["RESOURCE_EXHAUSTED", "Quota exceeded", "RATE_LIMIT_EXCEEDED", "429"]
    return any(m in s for m in markers)

def sheets_get_all_values_cached(ws, cache_key: str):
    """
    Cached wrapper for ws.get_all_values() with rate-limit fallback.
    cache_key should be stable for the worksheet (e.g., 'Snapshots', 'HistoryFull', 'History').
    """
    global _SHEETS_LAST_READ_ERROR
    key = f"get_all_values:{cache_key}"
    cached = _sheets_cache_get(key)
    if cached is not None:
        return cached
    try:
        # === PATCH SHEETS_CACHE1 (CONFLICT FIX, MINIMAL): call the underlying worksheet read ===
        # Previous draft accidentally recursed into itself and referenced an undefined variable.
        # This is a direct execution conflict fix (no behavior change intended beyond correctness).
        values = ws.get_all_values() if ws else []
        _sheets_cache_set(key, values)
        return values
    except Exception as e:
        _SHEETS_LAST_READ_ERROR = str(e)
        # Rate-limit fallback: return last cached value if we have one, else empty list
        if _is_sheets_rate_limit_error(e):
            stale = _SHEETS_READ_CACHE.get(key)
            if stale and isinstance(stale, tuple) and len(stale) == 2:
                return stale[1]
            return []
        raise

# =====================================================================
# PATCH SS2 (ADDITIVE): Google Sheets snapshot store (separate worksheet)
# Purpose:
#   - Persist full baseline_sources_cache inside the same Spreadsheet
#     but in a dedicated worksheet (tab), chunked across rows.
#   - Enables deterministic rehydration for evolution without refetch.
# Notes:
#   - Write-once semantics by source_snapshot_hash.
#   - Chunking and reassembly are deterministic (part_index ordering).
# =====================================================================
def get_google_spreadsheet():
    """Connect to Google Spreadsheet (cached connection if available)."""
    try:
        # If get_google_sheet() exists and already opened the spreadsheet as sheet.sheet1,
        # we re-open to obtain the Spreadsheet handle (additive; avoids refactoring).
        import streamlit as st
        from google.oauth2.service_account import Credentials
        import gspread

        SCOPES = [
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive",
        ]
        creds = Credentials.from_service_account_info(
            dict(st.secrets["gcp_service_account"]),
            scopes=SCOPES
        )
        client = gspread.authorize(creds)
        spreadsheet_name = st.secrets.get("google_sheets", {}).get("spreadsheet_name", "Yureeka_JSON")
        return client.open(spreadsheet_name)
    except Exception:
        return None

def _ensure_snapshot_worksheet(spreadsheet, title: str = "Snapshots"):
    """Ensure a worksheet tab exists for snapshot storage."""
    try:
        if not spreadsheet:
            return None
        try:
            ws = spreadsheet.worksheet(title)
            return ws
        except Exception:
            # Create with a reasonable default size; Sheets can expand.
            ws = spreadsheet.add_worksheet(title=title, rows=2000, cols=8)
            try:
                ws.append_row(
                    ["source_snapshot_hash", "part_index", "total_parts", "payload_part", "created_at", "code_version", "fingerprints_sig", "sha256"],
                    value_input_option="RAW",
                )
            except Exception:
                pass
            return ws
    except Exception:
        return None

def store_full_snapshots_to_sheet(baseline_sources_cache: list, source_snapshot_hash: str, worksheet_title: str = "Snapshots", chunk_chars: int = 45000) -> str:
    """
    Store full snapshots to a dedicated worksheet tab in chunked rows.
    Returns a ref string like: 'gsheet:Snapshots:<hash>'
    """
    import json, hashlib
    if not source_snapshot_hash:
        return ""
    if not isinstance(baseline_sources_cache, list) or not baseline_sources_cache:
        return ""

    try:
        ss = get_google_spreadsheet()
        ws = _ensure_snapshot_worksheet(ss, worksheet_title) if ss else None
        if not ws:
            return ""

        # Write-once: if hash already present, do not write again.
        try:
            # Find any existing rows for this hash (skip header)
            existing = ws.findall(source_snapshot_hash)
            if existing:
                return f"gsheet:{worksheet_title}:{source_snapshot_hash}"
        except Exception:
            # best effort; continue to attempt write
            pass

        payload = json.dumps(baseline_sources_cache, ensure_ascii=False, default=str)
        sha = hashlib.sha256(payload.encode("utf-8")).hexdigest()

        # deterministic chunking
        chunk_size = max(1000, int(chunk_chars or 45000))
        parts = [payload[i:i+chunk_size] for i in range(0, len(payload), chunk_size)]
        total = len(parts)

        # Optional fingerprints signature (stable)
        pairs = []
        for sr in baseline_sources_cache:
            if isinstance(sr, dict):
                u = (sr.get("source_url") or sr.get("url") or "").strip()
                fp = (sr.get("fingerprint") or sr.get("content_fingerprint") or "").strip()
                if u:
                    pairs.append((u, fp))
        pairs.sort()
        fingerprints_sig = "|".join([f"{u}#{fp}" for (u, fp) in pairs]) if pairs else ""

        from datetime import datetime
        created_at = datetime.utcnow().isoformat() + "Z"

        # Append rows in order (deterministic)
        code_version = ""
        try:
            # best effort: use global if exists
            code_version = globals().get("CODE_VERSION") or ""
        except Exception:
            code_version = ""

        # Use append_rows if available, else append_row in loop
        rows = []
        for idx, part in enumerate(parts):
            rows.append([source_snapshot_hash, idx, total, part, created_at, code_version, fingerprints_sig, sha])

        try:
            ws.append_rows(rows, value_input_option="RAW")
        except Exception:
            for r in rows:
                try:
                    ws.append_row(r, value_input_option="RAW")
                except Exception:
                    # partial failure: still return empty to avoid false pointer
                    return ""

        return f"gsheet:{worksheet_title}:{source_snapshot_hash}"
    except Exception:
        return ""

def load_full_snapshots_from_sheet(source_snapshot_hash: str, worksheet_title: str = "Snapshots") -> list:
    """Load and reassemble full snapshots list from a dedicated worksheet."""
    import json, hashlib
    if not source_snapshot_hash:
        return []
    try:
        ss = get_google_spreadsheet()
        ws = ss.worksheet(worksheet_title) if ss else None
        if not ws:
            return []

        # =====================================================================
        # PATCH SNAPLOAD1 (ADDITIVE): cache-safe snapshot read fallback
        # Why:
        # - If a prior read hit quota / partial failure and we cached [], evolution
        #   will permanently think "no snapshots exist" until cache clears.
        # Behavior:
        # - Try cached read first (fast)
        # - If empty/too small, do ONE direct read to bypass stale empty cache
        # =====================================================================
        values = []
        try:
            values = sheets_get_all_values_cached(ws, cache_key=worksheet_title)
        except Exception:
            values = []

        if not values or len(values) < 2:
            # Direct retry (best-effort)
            try:
                direct = ws.get_all_values()
                if direct and len(direct) >= 2:
                    values = direct
            except Exception:
                pass
        # =====================================================================
        # END PATCH SNAPLOAD1 (ADDITIVE)
        # =====================================================================

        if not values or len(values) < 2:
            return []

        header = values[0] or []
        # Expect at least: source_snapshot_hash, part_index, total_parts, payload_part
        try:
            col_h = header.index("source_snapshot_hash")
            col_i = header.index("part_index")
            col_t = header.index("total_parts")
            col_p = header.index("payload_part")
            col_sha = header.index("sha256") if "sha256" in header else None
        except Exception:
            # If headers are missing/misaligned, bail safely
            return []

        # Filter rows for this hash
        rows = []
        for r in values[1:]:
            try:
                if len(r) > col_h and r[col_h] == source_snapshot_hash:
                    rows.append(r)
            except Exception:
                continue

        if not rows:
            return []

        # Deterministic sort by part_index
        def _safe_int(x):
            try:
                return int(x)
            except Exception:
                return 0
        rows.sort(key=lambda r: _safe_int(r[col_i] if len(r) > col_i else 0))

        # Reassemble
        payload_parts = []
        for r in rows:
            if len(r) > col_p:
                payload_parts.append(r[col_p] or "")
        payload = "".join(payload_parts)

        # Optional integrity check
        try:
            if col_sha is not None and len(rows[0]) > col_sha:
                expected = rows[0][col_sha] or ""
                if expected:
                    actual = hashlib.sha256(payload.encode("utf-8")).hexdigest()
                    if actual != expected:
                        return []
        except Exception:
            pass

        try:
            data = json.loads(payload)
            return data if isinstance(data, list) else []
        except Exception:
            return []
    except Exception:
        return []

# =====================================================================
# PATCH HF4 (ADDITIVE): HistoryFull payload rehydration support
# Why:
# - Evolution may receive a sheets-safe wrapper that omits primary_response,
#   metric_schema_frozen, metric_anchors, etc.
# - When wrapper includes full_store_ref ("gsheet:HistoryFull:<analysis_id>"),
#   we can deterministically load the full analysis payload (no re-fetch).
# Notes:
# - Additive only. Safe no-op if sheet/tab not present.
# =====================================================================

# ===================== PATCH HF_WRITE1 (ADDITIVE) =====================

def write_full_history_payload_to_sheet(analysis_id: str, payload: str, worksheet_title: str = "HistoryFull", chunk_size: int = 20000) -> bool:
    """Write a full analysis payload (string) into HistoryFull as chunked rows keyed by analysis_id.

    Additive helper to support oversized History cells:
      - Ensures HistoryFull headers exist
      - Splits payload into chunks
      - Stores sha256 for integrity
    """
    import hashlib
    if not analysis_id or not payload:
        return False
    try:
        ss = get_google_spreadsheet()
        if not ss:
            return False
        try:
            ws = ss.worksheet(worksheet_title)
        except Exception:
            # Create sheet if missing (best-effort)
            try:
                ws = ss.add_worksheet(title=worksheet_title, rows=2000, cols=10)
            except Exception:
                ws = ss.worksheet(worksheet_title)

        # Ensure headers exist
        try:
            headers = ws.row_values(1)
            if (not headers) or (len(headers) < 5) or headers[0] != "analysis_id":
                _ = ws.update('A1:E1', [["analysis_id", "part_index", "total_parts", "payload_part", "sha256"]])
        except Exception:
            try:
                _ = ws.update('A1:E1', [["analysis_id", "part_index", "total_parts", "payload_part", "sha256"]])
            except Exception:
                pass

        # Remove existing rows for this analysis_id (optional; keep additive and safe: do not delete to avoid refactor)
        # We will append new parts; loader will read the latest by order if duplicates exist.

        sha = hashlib.sha256(payload.encode("utf-8", errors="ignore")).hexdigest()
        parts = [payload[i:i+chunk_size] for i in range(0, len(payload), chunk_size)]
        total = len(parts) if parts else 0
        if total == 0:
            return False

        rows = []
        for i, part in enumerate(parts):
            rows.append([analysis_id, str(i), str(total), part, sha])

        # Append in one batch if possible
        try:
            ws.append_rows(rows, value_input_option="RAW")
        except Exception:
            # Fallback to per-row append
            for r in rows:
                try:
                    ws.append_row(r, value_input_option="RAW")
                except Exception:
                    return False
        return True
    except Exception:
        return False

# =================== END PATCH HF_WRITE1 (ADDITIVE) ===================

def load_full_history_payload_from_sheet(analysis_id: str, worksheet_title: str = "HistoryFull") -> dict:
    """
    Load the full analysis JSON payload from the HistoryFull worksheet.

    PATCH HF_LOAD_V2 (ADDITIVE):
    - Supports payloads split across multiple rows (chunked writes).
    - Supports current HistoryFull headers:
        analysis_id, part_index, total_parts, payload_json, created_at, code_version, sha256
    - Backward compatible with older variants:
        id, part_index, total_parts, payload_part / data, sha256
    - Deterministic stitching (sort by part_index) + safe JSON parse.

    PATCH HF_LOAD_V3 (ADDITIVE):
    - Verifies chunk completeness when total_parts is available (0..total_parts-1).
    - Optionally verifies sha256 when present (stitched string).
    - Does not change failure mode: still returns {} on any failure.
    """
    try:
        ss = get_google_spreadsheet()
        if not ss:
            return {}

        try:
            ws = ss.worksheet(worksheet_title)
        except Exception:
            return {}

        # Read all rows (prefer cached getter if present)
        try:
            fn = globals().get("sheets_get_all_values_cached")
            rows = fn(ws, cache_key=worksheet_title) if callable(fn) else (ws.get_all_values() or [])
        except Exception:
            try:
                rows = ws.get_all_values() or []
            except Exception:
                return {}

        if not rows or len(rows) < 2:
            return {}

        header = rows[0] or []
        body = rows[1:] or []

        def _col(name: str):
            try:
                return header.index(name)
            except Exception:
                return None

        # -----------------------------------------------------------------
        # PATCH HF_LOAD_V2_COLS (ADDITIVE): map to real sheet headers + legacy
        # -----------------------------------------------------------------
        c_id = _col("analysis_id")
        if c_id is None:
            c_id = _col("id")
        if c_id is None:
            c_id = 0  # last-ditch fallback

        c_part = _col("part_index")
        c_total = _col("total_parts")

        # IMPORTANT: your sheet uses payload_json
        c_payload = _col("payload_json")
        if c_payload is None:
            c_payload = _col("payload_part")
        if c_payload is None:
            c_payload = _col("data")
        if c_payload is None:
            c_payload = len(header) - 1  # last-ditch fallback

        c_sha = _col("sha256")
        # -----------------------------------------------------------------

        target_id = str(analysis_id).strip()
        if not target_id:
            return {}

        # (pidx:int|None, total:int|None, chunk:str, sha:str)
        parts_with_sha = []

        for r in body:
            try:
                if not r:
                    continue

                rid = r[c_id] if c_id < len(r) else ""
                rid = str(rid).strip()
                if rid != target_id:
                    continue

                chunk = r[c_payload] if c_payload < len(r) else ""
                chunk = chunk or ""
                if not isinstance(chunk, str):
                    chunk = str(chunk)

                # part_index (optional)
                pidx = None
                if c_part is not None and c_part < len(r):
                    try:
                        pidx = int(str(r[c_part]).strip())
                    except Exception:
                        pidx = None

                # total_parts (optional)
                tparts = None
                if c_total is not None and c_total < len(r):
                    try:
                        tparts = int(str(r[c_total]).strip())
                    except Exception:
                        tparts = None

                sha = ""
                if c_sha is not None and c_sha < len(r):
                    sha = str(r[c_sha] or "").strip()

                # keep even tiny chunks; concatenation is deterministic
                if chunk.strip() == "":
                    continue

                parts_with_sha.append((pidx, tparts, chunk, sha))
            except Exception:
                continue

        if not parts_with_sha:
            return {}

        # -----------------------------------------------------------------
        # PATCH HF_LOAD_V2_BUCKET (ADDITIVE): if duplicates exist, pick best sha bucket
        # Score = (unique part_index count, total payload length)
        # -----------------------------------------------------------------
        parts = []          # list[(pidx, chunk)]
        chosen_sha = ""     # sha bucket selected (if any)
        chosen_total = None # total_parts inferred for chosen bucket (if any)
        try:
            if any(s for _, _, _, s in parts_with_sha):
                buckets = {}
                for pidx, tparts, chunk, sha in parts_with_sha:
                    key = sha or "__no_sha__"
                    buckets.setdefault(key, []).append((pidx, tparts, chunk))

                def _score(items):
                    idxs = [i for i, _, _ in items if i is not None]
                    uniq = len(set(idxs)) if idxs else 0
                    total_len = sum(len(c or "") for _, _, c in items)
                    return (uniq, total_len)

                best_key = sorted(buckets.keys(), key=lambda k: _score(buckets[k]), reverse=True)[0]
                chosen_sha = "" if best_key == "__no_sha__" else best_key
                best_items = buckets[best_key]

                # Infer total_parts for this bucket (mode / max)
                try:
                    totals = [tp for _, tp, _ in best_items if isinstance(tp, int) and tp > 0]
                    chosen_total = max(totals) if totals else None
                except Exception:
                    chosen_total = None

                parts = [(pidx, chunk) for (pidx, _tparts, chunk) in best_items]
            else:
                parts = [(pidx, chunk) for pidx, _tparts, chunk, _sha in parts_with_sha]
                # Infer total_parts (mode / max) even without sha
                try:
                    totals = [tp for _, tp, _, _ in parts_with_sha if isinstance(tp, int) and tp > 0]
                    chosen_total = max(totals) if totals else None
                except Exception:
                    chosen_total = None
        except Exception:
            parts = [(pidx, chunk) for pidx, _tparts, chunk, _sha in parts_with_sha]
            try:
                totals = [tp for _, tp, _, _ in parts_with_sha if isinstance(tp, int) and tp > 0]
                chosen_total = max(totals) if totals else None
            except Exception:
                chosen_total = None
        # -----------------------------------------------------------------

        # Sort parts deterministically by part_index; None last
        def _sort_key(t):
            pidx, _ = t
            return (pidx is None, pidx if pidx is not None else 0)

        parts.sort(key=_sort_key)

        # -----------------------------------------------------------------
        # PATCH HF_LOAD_V3_COMPLETE (ADDITIVE): completeness check when total_parts known
        # - If total_parts is known and we have part_index values, require 0..total-1.
        # - If incomplete, return {} (do not attempt parse on partial payload).
        # -----------------------------------------------------------------
        try:
            if isinstance(chosen_total, int) and chosen_total > 0:
                idxs = [p for (p, _c) in parts if isinstance(p, int)]
                if idxs:
                    uniq = sorted(set(idxs))
                    expected = list(range(0, chosen_total))
                    if uniq != expected:
                        return {}
        except Exception:
            pass
        # -----------------------------------------------------------------

        # Stitch chunks
        full_json_str = "".join([chunk for _, chunk in parts]).strip()
        if not full_json_str:
            return {}

        # -----------------------------------------------------------------
        # PATCH HF_LOAD_V3_SHA (ADDITIVE): verify sha256 when present
        # - If chosen_sha exists, compare against sha256(stitched_bytes).
        # - If mismatch, return {} (treat as corrupted / wrong bucket).
        # -----------------------------------------------------------------
        try:
            if isinstance(chosen_sha, str) and chosen_sha:
                import hashlib
                digest = hashlib.sha256(full_json_str.encode("utf-8", errors="ignore")).hexdigest()
                if str(digest).lower() != str(chosen_sha).lower():
                    return {}
        except Exception:
            pass
        # -----------------------------------------------------------------

        import json
        try:
            obj = json.loads(full_json_str)
            if isinstance(obj, dict):
                # -----------------------------------------------------------------
                # PATCH HF_LOAD_V3_META (ADDITIVE): optional debug stamp (harmless)
                # Only attaches when parse succeeded.
                # -----------------------------------------------------------------
                try:
                    obj["_rehydration_debug"] = {
                        "worksheet": str(worksheet_title or ""),
                        "analysis_id": str(target_id),
                        "parts_used": int(len(parts)),
                        "total_parts_expected": int(chosen_total) if isinstance(chosen_total, int) else None,
                        "sha_verified": bool(chosen_sha),
                    }
                except Exception:
                    pass
                # -----------------------------------------------------------------
                return obj
            return {}
        except Exception:
            # PATCH HF_LOAD_V2_SALVAGE (ADDITIVE): common salvage for leading/trailing junk
            try:
                # Try to isolate first "{" and last "}" if accidental prefix/suffix exists
                a = full_json_str.find("{")
                b = full_json_str.rfind("}")
                if a != -1 and b != -1 and b > a:
                    obj2 = json.loads(full_json_str[a:b+1])
                    if isinstance(obj2, dict):
                        # (keep same meta stamp behavior)
                        try:
                            obj2["_rehydration_debug"] = {
                                "worksheet": str(worksheet_title or ""),
                                "analysis_id": str(target_id),
                                "parts_used": int(len(parts)),
                                "total_parts_expected": int(chosen_total) if isinstance(chosen_total, int) else None,
                                "sha_verified": bool(chosen_sha),
                                "salvaged": True,
                            }
                        except Exception:
                            pass
                        return obj2
            except Exception:
                pass
            return {}

    except Exception:
        return {}

def fingerprint_text(text: str) -> str:
    """Stable short fingerprint for fetched content (deterministic)."""
    if not text:
        return ""
    normalized = re.sub(r"\s+", " ", text.strip().lower())
    return hashlib.md5(normalized.encode("utf-8")).hexdigest()[:12]

def attach_source_snapshots_to_analysis(analysis: dict, web_context: dict) -> dict:
    """
    Attach stable source snapshots (from web_context.scraped_meta) into analysis.

    Enhancements (v7_34 patch):
    - Ensures scraped_meta.extracted_numbers is always list-like
    - Adds RANGE capture per canonical metric using admitted snapshots:
        primary_metrics_canonical[ckey]["value_range"] = {min,max,n,examples}
      This restores earlier "range vs point estimate" behavior in a compatible way.
    """
    import re
    from datetime import datetime, timezone

    def _now_iso() -> str:
        return datetime.now(timezone.utc).isoformat()

    def _fingerprint(text: str) -> str:
        try:
            fn = globals().get("fingerprint_text")
            if callable(fn):
                return fn(text)
        except Exception:
            pass
        try:
            import hashlib
            t = re.sub(r"\s+", " ", (text or "").strip().lower())
            return hashlib.md5(t.encode("utf-8", errors="ignore")).hexdigest()[:12]
        except Exception:
            return ""

    # =========================================================================
    # PATCH N1 (ADDITIVE): stable anchor_hash fallback helper for snapshots
    # - Does NOT change existing behavior if anchor_hash already present.
    # =========================================================================
    def _sha1(s: str) -> str:
        try:
            import hashlib
            return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()
        except Exception:
            return ""
    # =========================================================================

    # =========================================================================
    # PATCH N2 (ADDITIVE): optional canonicalizer hook for snapshot numbers
    # - Ensures unit_tag/unit_family/base_unit/value_norm are present when possible.
    # - No behavior change if helper missing.
    # =========================================================================
    _canon_fn = globals().get("canonicalize_numeric_candidate")
    def _maybe_canonicalize(n: dict) -> dict:
        try:
            if callable(_canon_fn):
                return _canon_fn(dict(n))
        except Exception:
            pass
        return dict(n)
    # =========================================================================

    def _parse_num(value, unit_hint=""):
        try:
            fn = globals().get("parse_human_number")
            if callable(fn):
                return fn(str(value), unit_hint)
        except Exception:
            pass
        # fallback
        try:
            s = str(value).strip().replace(",", "")
            if not s:
                return None
            return float(re.findall(r"-?\d+(?:\.\d+)?", s)[0])
        except Exception:
            return None

    def _unit_family_from_metric(mdef: dict) -> str:
        # prefer metric schema
        uf = (mdef or {}).get("unit_family") or ""
        uf = str(uf).lower().strip()
        if uf in ("percent", "pct"):
            return "PCT"
        if uf in ("currency",):
            return "CUR"
        if uf in ("magnitude", "unit_sales", "other"):
            return "MAG"
        return "OTHER"

    def _cand_unit_family(cunit: str, craw: str) -> str:
        u = (cunit or "").strip()
        r = (craw or "")
        uu = u.upper()
        ru = r.upper()

        # Percent
        if uu == "%" or "%" in ru:
            return "PCT"

        # Energy
        if any(x in (u or "").lower() for x in ["twh", "gwh", "mwh", "kwh"]) or any(x in (r or "").lower() for x in ["twh", "gwh", "mwh", "kwh"]):
            return "ENERGY"

        # Currency (symbol/code presence)
        #if any(x in ru for x in ["$", "USD", "SGD", "EUR", "GBP", "S$"]) or uu in ("USD", "SGD", "EUR", "GBP"):
        #    return "CUR"

        if re.search(r"(\$|S\$|€|£)\s*\d", r) or any(x in ru for x in ["USD", "SGD", "EUR", "GBP"]) or uu in ("USD","SGD","EUR","GBP"):
            return "CUR"


        # Magnitude (case-insensitive)
        if uu in ("K", "M", "B", "T") or (u or "").lower() in ("k", "m", "b", "t"):
            return "MAG"

        return "OTHER"

    def _tokenize(s: str):
        return [t for t in re.findall(r"[a-z0-9]+", (s or "").lower()) if len(t) > 2]

    def _safe_norm_unit_tag(x: str) -> str:
        try:
            fn = globals().get("normalize_unit_tag")
            if callable(fn):
                return fn(x or "")
        except Exception:
            pass
        return (x or "").strip()


    # -----------------------------
    # Build baseline_sources_cache from scraped_meta (snapshot-friendly)
    # -----------------------------
    baseline_sources_cache = []
    scraped_meta = (web_context or {}).get("scraped_meta") or {}
    if isinstance(scraped_meta, dict):
        for url, meta in scraped_meta.items():
            if not isinstance(meta, dict):
                continue
            nums = meta.get("extracted_numbers") or []
            if nums is None or not isinstance(nums, list):
                nums = []

            content = meta.get("content") or meta.get("clean_text") or (web_context.get("scraped_content", {}) or {}).get(url, "") or ""

            baseline_sources_cache.append({
                "url": url,
                "status": "fetched" if str(meta.get("status_detail", "")).startswith("success") or meta.get("status") == "fetched" else "failed",
                "status_detail": meta.get("status_detail") or meta.get("status") or "",
                "numbers_found": int(meta.get("numbers_found") or (len(nums) if isinstance(nums, list) else 0)),
                "fetched_at": meta.get("fetched_at") or _now_iso(),
                "fingerprint": meta.get("fingerprint") or _fingerprint(content),

                # =====================================================================
                # PATCH N1 (+ N2) (ADDITIVE): preserve full candidate record in snapshots
                # - This is critical for:
                #   * range gating (metric-aware)
                #   * schema-first attribution
                #   * evolution rebuild (anchor_hash + value_norm + unit_family)
                # - Backward compatible: only adds keys; existing keys unchanged.
                # =====================================================================
                "extracted_numbers": [
                    (lambda nn: {
                        "value": nn.get("value"),
                        "unit": nn.get("unit"),
                        "raw": nn.get("raw"),
                        "context_snippet": (nn.get("context_snippet") or nn.get("context") or "")[:240],

                        # keep existing anchor_hash if present; else stable fallback
                        "anchor_hash": (
                            nn.get("anchor_hash")
                            or _sha1(
                                f"{url}|{str(nn.get('raw') or '')}|{(nn.get('context_snippet') or nn.get('context') or '')[:240]}"
                            )
                        ),

                        "source_url": nn.get("source_url") or url,

                        # ---- Additive: junk tagging & deterministic offsets ----
                        "is_junk": nn.get("is_junk"),
                        "junk_reason": nn.get("junk_reason"),
                        "start_idx": nn.get("start_idx"),
                        "end_idx": nn.get("end_idx"),

                        # ---- Additive: normalized unit fields (if already present or canonicalized) ----
                        "unit_tag": nn.get("unit_tag"),
                        "unit_family": nn.get("unit_family"),
                        "base_unit": nn.get("base_unit"),
                        "multiplier_to_base": nn.get("multiplier_to_base"),
                        "value_norm": nn.get("value_norm"),

                        # ---- Additive: semantic association tags (if present) ----
                        "measure_kind": nn.get("measure_kind"),
                        "measure_assoc": nn.get("measure_assoc"),
                    })(_maybe_canonicalize(n))
                    for n in nums
                    if isinstance(n, dict)
                ]
                # =====================================================================
            })

    if baseline_sources_cache:

        # ---- ADDITIVE: stable ordering of snapshots (Change #2) ----
        for s in (baseline_sources_cache or []):
            if isinstance(s, dict) and isinstance(s.get("extracted_numbers"), list):

                # =========================================================================
                # PATCH N3 (ADDITIVE): guard sort_snapshot_numbers if not defined
                # =========================================================================
                try:
                    if "sort_snapshot_numbers" in globals() and callable(globals()["sort_snapshot_numbers"]):
                        s["extracted_numbers"] = sort_snapshot_numbers(s["extracted_numbers"])
                    else:
                        # safe fallback: anchor_hash then raw
                        s["extracted_numbers"] = sorted(
                            s["extracted_numbers"],
                            key=lambda x: (str((x or {}).get("anchor_hash") or ""), str((x or {}).get("raw") or ""))
                        )
                except Exception:
                    pass
                # =========================================================================

                s["numbers_found"] = len(s["extracted_numbers"])

        baseline_sources_cache = sorted(
            baseline_sources_cache,
            key=lambda x: str((x or {}).get("url") or "")
        )
        # -----------------------------------------------------------

        # =====================================================================
        # PATCH INJ_HASH_V1_APPLY (ADDITIVE): optionally include injected URLs in snapshot hash identity
        # - Adds *synthetic* url-only source records for injected URLs that were
        #   persisted (per diag) but are missing from baseline_sources_cache.
        # - Default OFF; only activates when INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH is enabled.
        # - Does NOT alter fastpath logic or metric selection (synthetic has no numbers).
        # =====================================================================
        _inj_hash_added = []
        _inj_hash_reasons = {}
        try:
            _diag_local = {}
            if isinstance(web_context, dict):
                _diag_local = web_context.get("diag_injected_urls") or web_context.get("extra_urls_debug") or {}
            _persisted_for_hash = []
            if isinstance(_diag_local, dict):
                _persisted_for_hash = _inj_diag_norm_url_list(
                    _diag_local.get("persisted_norm") or _diag_local.get("persisted") or []
                )
            _incl_inj_hash = _inj_hash_policy_should_include(_persisted_for_hash)
            if _incl_inj_hash and _persisted_for_hash:
                _bsc_aug, _inj_hash_added, _inj_hash_reasons = _inj_hash_add_synthetic_sources(
                    baseline_sources_cache,
                    _persisted_for_hash,
                    now_iso=_now_iso(),
                )
                baseline_sources_cache = _bsc_aug
        except Exception:
            _inj_hash_added = []
            _inj_hash_reasons = {}
        # =====================================================================

        analysis["baseline_sources_cache"] = baseline_sources_cache
        analysis.setdefault("results", {})
        if isinstance(analysis["results"], dict):

            # =====================================================================
            # PATCH INJ_DIAG_ATTACH_SNAPSHOTS (ADDITIVE): propagate injected-URL trace into analysis
            # - Captures persisted snapshot URLs + exact hash input URL set (A4-A5)
            # - Does NOT alter any gating/selection logic.
            # =====================================================================
            try:
                _diag = {}
                if isinstance(web_context, dict):
                    _diag = web_context.get("diag_injected_urls") or web_context.get("extra_urls_debug") or {}

                _inj_urls = []
                if isinstance(_diag, dict):
                    _inj_urls = _inj_diag_norm_url_list(
                        _diag.get("intake_norm")
                        or _diag.get("extra_urls_normalized")
                        or _diag.get("extra_urls")
                        or []
                    )

                _snap_urls = _inj_diag_hash_inputs_from_bsc(baseline_sources_cache)
                _hash_inputs = _snap_urls

                _h_v1 = ""
                _h_v2 = ""
                try:
                    _h_v1 = compute_source_snapshot_hash(baseline_sources_cache)
                except Exception:
                    _h_v1 = ""
                try:
                    _h_v2 = compute_source_snapshot_hash_v2(baseline_sources_cache)
                except Exception:
                    _h_v2 = ""

                analysis.setdefault("results", {})
                if isinstance(analysis.get("results"), dict):
                    analysis["results"].setdefault("debug", {})
                    if isinstance(analysis["results"].get("debug"), dict):
                        analysis["results"]["debug"].setdefault("inj_diag", {})
                        analysis["results"]["debug"]["inj_diag"].update({
                            "run_id": str((web_context or {}).get("diag_run_id") or _diag.get("run_id") or ""),
                            "injected_urls": _inj_urls[:50],
                            "snapshot_pool_urls_count": int(len(_snap_urls)),
                            "snapshot_pool_urls_hash": _inj_diag_set_hash(_snap_urls),
                            "hash_input_urls_count": int(len(_hash_inputs)),
                            "hash_input_urls_hash": _inj_diag_set_hash(_hash_inputs),
                            "injected_in_snapshot_pool": sorted(list(set(_inj_urls) & set(_snap_urls)))[:50],
                            "injected_in_hash_inputs": sorted(list(set(_inj_urls) & set(_hash_inputs)))[:50],
                            "computed_hash_v1": _h_v1,
                            "computed_hash_v2": _h_v2,
                        })


                        # =====================================================================
                        # PATCH INJ_TRACE_V1_EMIT_ANALYSIS (ADDITIVE): always emit canonical trace
                        # Location: analysis.results.debug.inj_trace_v1
                        # =====================================================================
                        try:

                            # =====================================================================
                            # PATCH INJ_TRACE_V1_ENRICH_ANALYSIS_ARTIFACTS (ADDITIVE)
                            # Ensure inj_trace_v1 shows attempted/persisted evidence even when
                            # upstream diag_injected_urls is partial (e.g., baseline/no-injection).
                            # Diagnostics only.
                            # =====================================================================
                            try:
                                if isinstance(_diag, dict):
                                    _diag = _inj_trace_v1_enrich_diag_from_bsc(_diag, baseline_sources_cache)
                            except Exception:
                                pass
                            # =====================================================================

                            _trace = _inj_trace_v1_build(
                                diag_injected_urls=_diag if isinstance(_diag, dict) else {},
                                hash_inputs=_hash_inputs,
                                stage="analysis",
                                path="analysis",
                                rebuild_pool=None,
                                rebuild_selected=None,
                                hash_exclusion_reasons=(_inj_hash_reasons if isinstance(locals().get('_inj_hash_reasons'), dict) else {}),
                            )
                            analysis["results"]["debug"].setdefault("inj_trace_v1", {})
                            # Do not overwrite if already present; only fill/merge
                            if isinstance(analysis["results"]["debug"].get("inj_trace_v1"), dict):
                                analysis["results"]["debug"]["inj_trace_v1"].update(_trace)
                        except Exception:
                            pass
                        # =====================================================================

            except Exception:
                pass
            # =====================================================================

        analysis["results"]["baseline_sources_cache"] = baseline_sources_cache


    # -----------------------------
    # RANGE capture for canonical metrics
    # -----------------------------
    pmc = analysis.get("primary_response", {}).get("primary_metrics_canonical") if isinstance(analysis.get("primary_response"), dict) else analysis.get("primary_metrics_canonical")
    schema = analysis.get("primary_response", {}).get("metric_schema_frozen") if isinstance(analysis.get("primary_response"), dict) else analysis.get("metric_schema_frozen")

    # Support both placements (your JSON seems to store these at top-level primary_response)
    if pmc is None and isinstance(analysis.get("primary_response"), dict):
        pmc = analysis["primary_response"].get("primary_metrics_canonical")
    if schema is None and isinstance(analysis.get("primary_response"), dict):
        schema = analysis["primary_response"].get("metric_schema_frozen")

    if isinstance(pmc, dict) and isinstance(schema, dict) and baseline_sources_cache:
        # flatten candidates
        all_cands = []
        for sr in baseline_sources_cache:
            for n in (sr.get("extracted_numbers") or []):
                if isinstance(n, dict):
                    all_cands.append(n)

        for ckey, m in pmc.items():
            if not isinstance(m, dict):
                continue
            mdef = schema.get(ckey) or {}
            uf = _unit_family_from_metric(mdef)
            keywords = mdef.get("keywords") or []

            kw_tokens = []
            for k in (keywords or []):
                kw_tokens.extend(_tokenize(str(k)))

            kw_tokens.extend(_tokenize(m.get("name") or m.get("original_name") or ""))
            kw_tokens = list(dict.fromkeys([t for t in kw_tokens if len(t) > 2]))[:40]

            vals = []
            examples = []

            for cand in all_cands:
                craw = str(cand.get("raw") or "")
                cunit = str(cand.get("unit") or "")
                ctx = str(cand.get("context_snippet") or cand.get("context") or "")

                # family gate
                cf = _cand_unit_family(cunit, craw)
                if uf == "PCT" and cf != "PCT":
                    continue
                if uf == "CUR" and cf != "CUR":
                    continue
                # MAG: allow MAG/OTHER but avoid CUR/PCT
                if uf == "MAG" and cf in ("CUR", "PCT"):
                    continue

                # NEW (additive): metric-aware magnitude gate
                if uf == "MAG":

                    cand_tag = _safe_norm_unit_tag(cunit or craw)
                    exp_tag = _safe_norm_unit_tag((mdef.get("unit") or "") or (m.get("unit") or ""))


                    if exp_tag in ("K", "M", "B", "T"):
                        if cand_tag != exp_tag:
                            continue
                    else:
                        if cand_tag not in ("K", "M", "B", "T"):
                            continue

                # token overlap gate
                c_tokens = set(_tokenize(ctx))
                if kw_tokens:
                    overlap = sum(1 for t in kw_tokens if t in c_tokens)
                    if overlap < max(1, min(3, len(kw_tokens) // 8)):
                        continue

                v = _parse_num(cand.get("value"), cunit) or _parse_num(craw, cunit)
                if v is None:
                    continue

                vals.append(float(v))
                if len(examples) < 5:
                    examples.append({
                        "raw": craw[:32],
                        "source_url": cand.get("source_url"),
                        "context_snippet": ctx[:180]
                    })

            if len(vals) >= 2:
                vmin = min(vals)
                vmax = max(vals)
                if abs(vmax - vmin) > max(1e-9, abs(vmin) * 0.02):
                    m["value_range"] = {
                        "min": vmin,
                        "max": vmax,
                        "n": len(vals),
                        "examples": examples,
                        "method": "snapshot_candidates"
                    }
                    try:
                        unit_disp = m.get("unit") or ""
                        m["value_range_display"] = f"{vmin:g}–{vmax:g} {unit_disp}".strip()
                    except Exception:
                        pass
    # =====================================================================
    # PATCH FIX2B_RANGE2 (ADDITIVE): override legacy snapshot_candidates range with schema-unit evidence range
    # =====================================================================
    try:
        _res = analysis.get("results") if isinstance(analysis, dict) else None
        if isinstance(_res, dict):
            for _ck, _m in _res.items():
                if not isinstance(_m, dict):
                    continue
                _ev = _m.get("evidence")
                if not isinstance(_ev, list) or len(_ev) < 2:
                    continue
                _vals = []
                for _e in _ev:
                    if not isinstance(_e, dict):
                        continue
                    _v = _e.get("value_norm")
                    if _v is None:
                        _v = _e.get("value")
                    if isinstance(_v, (int, float)):
                        try:
                            _vals.append(float(_v))
                        except Exception:
                            pass
                if len(_vals) < 2:
                    continue
                _vmin = min(_vals); _vmax = max(_vals)
                if abs(_vmax - _vmin) <= max(1e-9, abs(_vmin) * 0.02):
                    continue
                _m["value_range"] = {"min": _vmin, "max": _vmax, "n": len(_vals), "method": "ph2b_schema_unit_range_v1|fix2b_range2"}
                try:
                    _unit_disp = _m.get("unit") or _m.get("unit_tag") or ""
                    _m["value_range_display"] = f"{_vmin:g}–{_vmax:g} {_unit_disp}".strip()
                except Exception:
                    pass
    except Exception:
        pass
    # =====================================================================


    # =====================================================================
    # PATCH FIX2B_RANGE3 (ADDITIVE): Schema-unit value_range rebuild for primary_metrics_canonical
    #
    # Why:
    # - We observed value_range values scaled as if converted to billions (e.g., 17.8M -> 0.0178)
    #   while still displaying "million units", causing downstream eligibility drift.
    # - FIX2B_RANGE2 only patches analysis["results"] (often empty); the dashboard-facing
    #   canonicals live under analysis["primary_response"]["primary_metrics_canonical"].
    #
    # What:
    # - Rebuild value_range from baseline_sources_cache extracted_numbers, treating candidate.value_norm
    #   as schema units (NO double scaling), constrained to the metric's chosen source_url.
    # - Pure post-processing: NO IO, NO refetch, NO hashing changes.
    # =====================================================================
    try:
        import re
        _pr = analysis.get("primary_response") if isinstance(analysis, dict) else None
        _pmc = _pr.get("primary_metrics_canonical") if isinstance(_pr, dict) else None
        _schema = (
            (analysis.get("metric_schema_frozen") if isinstance(analysis, dict) else None)
            or (_pr.get("metric_schema_frozen") if isinstance(_pr, dict) else None)
            or {}
        )
        _bsc = analysis.get("baseline_sources_cache") if isinstance(analysis, dict) else None
        if isinstance(_pmc, dict) and isinstance(_schema, dict) and isinstance(_bsc, list) and _bsc:
            # Flatten candidate universe from snapshots
            _flat = []
            for _src in _bsc:
                if not isinstance(_src, dict):
                    continue
                _nums = _src.get("extracted_numbers")
                if isinstance(_nums, list):
                    _flat.extend([n for n in _nums if isinstance(n, dict)])
            # Helper: scale evidence presence for common magnitudes
            def _ph2b_scale_token_ok(_spec_unit_tag: str, _cand: dict) -> bool:
                try:
                    sut = str(_spec_unit_tag or "").lower()
                    if not sut:
                        return True
                    # Only enforce for explicit scaled magnitudes
                    if ("million" not in sut) and ("billion" not in sut) and ("thousand" not in sut) and ("trillion" not in sut):
                        return True
                    raw = str(_cand.get("raw") or "").lower()
                    ut = str(_cand.get("unit_tag") or _cand.get("unit") or "").lower()
                    ctx = str(_cand.get("context_snippet") or "").lower()
                    blob = " ".join([raw, ut, ctx])
                    if "million" in sut:
                        return ("million" in blob) or re.search(r"\bm\b", blob) is not None or " mn" in blob
                    if "billion" in sut:
                        return ("billion" in blob) or re.search(r"\bb\b", blob) is not None or " bn" in blob
                    if "thousand" in sut:
                        return ("thousand" in blob) or re.search(r"\bk\b", blob) is not None
                    if "trillion" in sut:
                        return ("trillion" in blob) or re.search(r"\bt\b", blob) is not None
                except Exception:
                    pass
                return True

            for _ck, _m in list(_pmc.items()):
                if not isinstance(_m, dict):
                    continue
                _spec = _schema.get(_ck) if isinstance(_schema, dict) else None
                if not isinstance(_spec, dict):
                    continue
                _src_url = _m.get("source_url") or _spec.get("preferred_url") or ""
                if not _src_url:
                    continue
                _src_url_n = _ph2b_norm_url(_src_url)
                # Collect eligible vals from same source_url
                _vals = []
                _examples = []
                for _c in _flat:
                    try:
                        if _ph2b_norm_url(_c.get("source_url") or "") != _src_url_n:
                            continue
                        if _c.get("is_junk") is True:
                            continue
                        # Reuse FIX16 allowlist if present
                        try:
                            if callable(globals().get("_fix16_candidate_allowed")):
                                if not globals().get("_fix16_candidate_allowed")(_c, _spec, canonical_key=_ck):
                                    continue
                        except Exception:
                            pass
                        # Enforce scale token for scaled magnitudes
                        if not _ph2b_scale_token_ok(_spec.get("unit_tag") or _spec.get("unit") or "", _c):
                            continue
                        _v = _c.get("value_norm")
                        if _v is None:
                            _v = _c.get("value")
                        if isinstance(_v, (int, float)):
                            _vals.append(float(_v))
                            if len(_examples) < 4:
                                _examples.append({
                                    "raw": _c.get("raw"),
                                    "source_url": _c.get("source_url"),
                                    "context_snippet": (str(_c.get("context_snippet") or "")[:180])
                                })
                    except Exception:
                        continue
                if len(_vals) < 2:
                    continue
                _vmin = min(_vals); _vmax = max(_vals)
                if abs(_vmax - _vmin) <= max(1e-9, abs(_vmin) * 0.02):
                    continue
                _m["value_range"] = {
                    "min": _vmin,
                    "max": _vmax,
                    "n": len(_vals),
                    "examples": _examples,
                    "method": "ph2b_schema_unit_range_v2|fix2b_range3"
                }
                try:
                    _unit_disp = _m.get("unit") or _m.get("unit_tag") or _spec.get("unit_tag") or ""
                    _m["value_range_display"] = f"{_vmin:g}–{_vmax:g} {_unit_disp}".strip()
                except Exception:
                    pass
    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH FIX2B_RANGE4 (ADDITIVE): late override of snapshot_candidates range builder (schema-unit semantics)
    #
    # Location:
    # - This runs AFTER the legacy "snapshot_candidates" range builder blocks above.
    #
    # Goal:
    # - Ensure value_range is computed in *schema units* (treat candidate.value_norm as schema units),
    #   avoiding any double-scaling/divide behavior.
    # - Constrain range computation to the metric's chosen current source_url when present,
    #   preventing cross-source range pollution.
    #
    # Notes:
    # - Downstream-only post-processing. Does NOT affect selection, only range/min/max display.
    # - Safe even when evidence is missing: it simply no-ops.
    # =====================================================================
    try:
        _pr = analysis.get("primary_response") if isinstance(analysis, dict) else None
        _pmc = _pr.get("primary_metrics_canonical") if isinstance(_pr, dict) else None
        _schema = (
            (analysis.get("metric_schema_frozen") if isinstance(analysis, dict) else None)
            or (_pr.get("metric_schema_frozen") if isinstance(_pr, dict) else None)
            or {}
        )
        _bsc = analysis.get("baseline_sources_cache") if isinstance(analysis, dict) else None

        if isinstance(_pmc, dict) and isinstance(_schema, dict) and isinstance(_bsc, list) and _bsc:
            # Flatten candidates once
            _flat = []
            for _src in _bsc:
                if not isinstance(_src, dict):
                    continue
                _nums = _src.get("extracted_numbers")
                if isinstance(_nums, list):
                    _flat.extend([n for n in _nums if isinstance(n, dict)])

            def _ph2b_unit_tag_norm(x: str) -> str:
                return _safe_norm_unit_tag(str(x or ""))

            for _ckey, _m in _pmc.items():
                if not isinstance(_m, dict):
                    continue

                _mdef = _schema.get(_ckey) if isinstance(_schema, dict) else None
                _mdef = _mdef if isinstance(_mdef, dict) else {}
                _exp_unit = str(_mdef.get("unit") or _m.get("unit") or _m.get("unit_tag") or "")
                _exp_tag = _ph2b_unit_tag_norm(_exp_unit)

                # If the metric already has a source_url, treat that as the range scope
                _scope_url = _m.get("cur_source_url") or _m.get("source_url") or ""
                _scope_url_n = _norm_url(_scope_url) if _scope_url else ""

                _vals = []
                for _cand in _flat:
                    if not isinstance(_cand, dict):
                        continue
                    # source scope (if known)
                    if _scope_url_n:
                        _c_url = _cand.get("source_url") or ""
                        if _norm_url(_c_url) != _scope_url_n:
                            continue
                    # unit_tag scope (only enforce when schema declares a scaled magnitude tag)
                    if _exp_tag:
                        _cand_tag = _ph2b_unit_tag_norm(_cand.get("unit_tag") or _cand.get("unit") or _cand.get("raw") or "")
                        if _exp_tag in ("K", "M", "B", "T") and _cand_tag != _exp_tag:
                            continue
                    _vn = _cand.get("value_norm")
                    if _vn is None:
                        continue
                    if isinstance(_vn, (int, float)):
                        try:
                            _vals.append(float(_vn))
                        except Exception:
                            pass

                if len(_vals) >= 2:
                    _vmin = min(_vals); _vmax = max(_vals)
                    if abs(_vmax - _vmin) > max(1e-9, abs(_vmin) * 0.02):
                        _m["value_range"] = {
                            "min": _vmin,
                            "max": _vmax,
                            "n": len(_vals),
                            "method": "ph2b_schema_unit_range_v2|fix2b_range4",
                            "scope_url": _scope_url_n or "",
                            "scope_unit_tag": _exp_tag or ""
                        }
                        try:
                            _unit_disp = _m.get("unit") or _m.get("unit_tag") or _exp_unit or ""
                            _m["value_range_display"] = f"{_vmin:g}–{_vmax:g} {_unit_disp}".strip()
                        except Exception:
                            pass
    except Exception:
        pass
    # =====================================================================



    # =====================================================================
    # PATCH V1 (ADDITIVE): analysis & schema version stamping
    # - Pure metadata, NO logic impact
    # - Allows downstream drift attribution:
    #     * pipeline changes vs source changes
    # =====================================================================
    analysis.setdefault("analysis_pipeline_version", "v7_41_endstate_wip_1")
    analysis.setdefault("metric_identity_version", "canon_v2_dim_safe")
    analysis.setdefault("schema_freeze_version", 1)
    # =====================================================================

    # =========================
    # VERSION STAMP (ADDITIVE)
    # =========================
    analysis.setdefault("code_version", CODE_VERSION)
    # =========================


    return analysis



def normalize_unit(unit: str) -> str:
    """Normalize unit to one of: T/B/M/%/'' (deterministic)."""
    if not unit:
        return ""
    u = unit.strip().upper().replace("USD", "").replace("$", "").replace(" ", "")
    if u in ["TRILLION", "T"]:
        return "T"
    if u in ["BILLION", "B"]:
        return "B"
    if u in ["MILLION", "M"]:
        return "M"
    if u in ["PERCENT", "%"]:
        return "%"
    if u in ["K", "THOUSAND"]:
        return "K"
    return u

def normalize_currency_prefix(raw: str) -> bool:
    """True if looks like a currency number ($/USD)."""
    if not raw:
        return False
    s = raw.strip().upper()
    return s.startswith("$") or " USD" in s or s.startswith("USD")

def is_likely_junk_context(ctx: str) -> bool:
    """
    Returns True if a context snippet strongly indicates the number is coming from
    HTML/JS/CSS/asset junk (srcset resize params, scripts, svg path data, etc.)
    rather than real narrative/tabular data.
    """
    import re

    c = (ctx or "").strip()
    if not c:
        return True

    cl = c.lower()

    # Too much binary / garbled text (common when PDF bytes leak through)
    non_print = sum(1 for ch in c if ord(ch) < 9 or (13 < ord(ch) < 32))
    if non_print > 0:
        return True

    # Lots of replacement chars / unusual glyphs → decode garbage
    bad_glyphs = c.count("\ufffd")
    if bad_glyphs >= 1:
        return True

    # Very long uninterrupted “code-ish” context
    if len(c) > 260 and ("{" in c and "}" in c) and ("function" in cl or "var " in cl or "const " in cl):
        return True

    # Hard “asset / markup / script” indicators
    hard_hints = [
        "srcset=", "resize=", "quality=", "offsc", "offscreencanvas", "createelement(\"canvas\")",
        "willreadfrequently", "function(", "webpack", "window.", "document.", "var ", "const ",
        "<script", "</script", "<style", "</style", "text/javascript", "application/javascript",
        "og:image", "twitter:image", "meta property=", "content=\"width=device-width",
        "/wp-content/", ".jpg", ".jpeg", ".png", ".svg", ".webp", ".css", ".js", ".woff", ".woff2",
        "data:image", "base64,", "viewbox", "path d=", "d=\"m", "aria-label=", "class=\""
    ]
    if any(h in cl for h in hard_hints):
        return True

    # SVG path command patterns like "h4.16v-2.56"
    if re.search(r"(?:^|[^a-z0-9])[a-z]\d+(?:\.\d+)?[a-z]-?\d", cl):
        return True

    # Image resize query param like "...jpg?resize=770%2C513..."
    if re.search(r"resize=\d+%2c\d+", cl):
        return True

    # Phone / tracking / footer junk often has lots of separators and few letters
    letters = sum(1 for ch in c if ch.isalpha())
    if len(c) >= 120 and letters / max(1, len(c)) < 0.08:
        return True

    return False


def parse_human_number(value_str: str, unit: str) -> Optional[float]:
    """
    Parse number + unit into a comparable float scale.
    - For T/B/M: returns value in billions (B) to compare apples-to-apples.
    - For %: returns numeric percent.
    """
    if value_str is None:
        return None

    s = str(value_str).strip()
    if not s:
        return None

    # remove currency symbols/commas/space
    s = s.replace("$", "").replace(",", "").strip()

    # handle parentheses for negatives e.g. (12.3)
    if s.startswith("(") and s.endswith(")"):
        s = "-" + s[1:-1].strip()

    try:
        v = float(s)
    except Exception:
        return None

    u = normalize_unit(unit)

    # Normalize magnitudes into BILLIONS for currency-like units
    if u == "T":
        return v * 1000.0
    if u == "B":
        return v
    if u == "M":
        return v / 1000.0
    if u == "K":
        return v / 1_000_000.0

    # Percent: keep as percent number
    if u == "%":
        return v

    # Unknown unit: leave as-is (still useful for ratio filtering)
    return v

def build_prev_numbers(prev_metrics: Dict) -> Dict[str, Dict]:
    """
    Build previous metric lookup keyed by metric_name string.
    Stores:
      - parsed numeric value (for matching)
      - normalized unit (for gating)
      - raw display string INCLUDING currency/magnitude (for dashboards + evolution JSON)
      - raw_value/raw_unit for debugging
    """
    def _format_raw_display(value: Any, unit: str) -> str:
        v = "" if value is None else str(value).strip()
        u = (unit or "").strip()

        if not v:
            return ""

        # Currency prefix handling (SGD/USD keywords OR symbol prefixes)
        currency = ""
        u_nospace = u.replace(" ", "")

        if u_nospace.upper().startswith("SGD"):
            currency = "S$"
            u_tail = u_nospace[3:]
        elif u_nospace.upper().startswith("USD"):
            currency = "$"
            u_tail = u_nospace[3:]
        elif u_nospace.startswith("S$"):
            currency = "S$"
            u_tail = u_nospace[2:]
        elif u_nospace.startswith("$"):
            currency = "$"
            u_tail = u_nospace[1:]
        else:
            u_tail = u_nospace

        # Percent special case
        if u_tail == "%":
            return f"{v}%"

        # Word scales
        if "billion" in u.lower():
            return f"{currency}{v} billion".strip()
        if "million" in u.lower():
            return f"{currency}{v} million".strip()

        # Compact suffix (B/M/K/T)
        if u_tail.upper() in {"T", "B", "M", "K"}:
            return f"{currency}{v}{u_tail.upper()}".strip()

        # Fallback
        return f"{currency}{v} {u}".strip()

    prev_numbers: Dict[str, Dict] = {}
    for key, metric in (prev_metrics or {}).items():
        if not isinstance(metric, dict):
            continue

        metric_name = metric.get("name", key)
        raw_value = metric.get("value", "")
        raw_unit = metric.get("unit", "")

        val = parse_human_number(str(raw_value), raw_unit)
        if val is None:
            continue

        prev_numbers[metric_name] = {
            "value": val,
            "unit": normalize_unit(raw_unit),
            "raw": _format_raw_display(raw_value, raw_unit),   # ✅ now includes currency + unit
            "raw_value": raw_value,
            "raw_unit": raw_unit,
            "keywords": extract_context_keywords(metric_name),
        }

    return prev_numbers

def _extract_baseline_cache(previous_data: dict) -> list:
    """
    Pull prior source snapshots from any known places v7.x stores them.
    Returns a list of source_result-like dicts, or [].
    """
    pd = previous_data or {}
    pr = (pd.get("primary_response") or {}) if isinstance(pd.get("primary_response"), dict) else {}

    for obj in [
        pd.get("baseline_sources_cache"),
        (pd.get("results") or {}).get("baseline_sources_cache") if isinstance(pd.get("results"), dict) else None,
        (pd.get("results") or {}).get("source_results") if isinstance(pd.get("results"), dict) else None,
        pd.get("source_results"),
        pr.get("baseline_sources_cache"),
        (pr.get("results") or {}).get("source_results") if isinstance(pr.get("results"), dict) else None,
    ]:
        if isinstance(obj, list) and obj:
            return obj

    return []


def _extract_query_from_previous(previous_data: dict) -> str:
    """
    Try to recover the original user query/topic from the saved analysis object.
    v7.27 commonly uses 'question'.
    """
    pd = previous_data or {}
    if isinstance(pd.get("question"), str) and pd["question"].strip():
        return pd["question"].strip()

    pr = pd.get("primary_response") or {}
    if isinstance(pr, dict):
        if isinstance(pr.get("question"), str) and pr["question"].strip():
            return pr["question"].strip()
        if isinstance(pr.get("query"), str) and pr["query"].strip():
            return pr["query"].strip()

    meta = pd.get("meta") or {}
    if isinstance(meta, dict) and isinstance(meta.get("question"), str) and meta["question"].strip():
        return meta["question"].strip()

    return ""

def _build_source_snapshots_from_web_context(web_context: dict) -> list:
    """
    Convert fetch_web_context() output (scraped_meta) into evolution snapshots.

    Preferred inputs:
      - web_context["scraped_meta"][url]["extracted_numbers"] (analysis-aligned)

    Safety-net hard gates (small set):
      1) homepage-like URLs downweighted + tagged
      2) nav/chrome/junk context downweighted
      3) year-only suppression (e.g., raw == "2024" and no unit/context)
      4) light topic gate (requires minimal overlap with query tokens)
    """
    import hashlib
    from datetime import datetime
    from urllib.parse import urlparse
    import re

    def _sha1(s: str) -> str:
        return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()

    def _now() -> str:
        try:
            return datetime.utcnow().isoformat() + "+00:00"
        except Exception:
            return datetime.now().isoformat()

    def _is_homepage_url(u: str) -> bool:
        try:
            p = urlparse((u or "").strip())
            path = (p.path or "").strip()
            if path in ("", "/"):
                return True
            low = path.lower().rstrip("/")
            if low in ("/index", "/index.html", "/index.htm", "/home", "/default", "/default.aspx"):
                return True
            return False
        except Exception:
            return False



    def _tokenize(s: str) -> list:
        toks = re.findall(r"[a-z0-9]+", (s or "").lower())
        stop = {"the","and","or","of","in","to","for","by","from","with","on","at","as","a","an","is","are","this","that"}
        return [t for t in toks if len(t) >= 4 and t not in stop]

    def _looks_like_year_only(n: dict) -> bool:
        try:
            raw = str(n.get("raw") or "").strip()
            # =====================================================================
            # PATCH YEAR2 (ADDITIVE): handle years stored as numeric/float strings
            # - If raw is empty or looks like '2024.0', normalize to '2024' for checks.
            # =====================================================================
            try:
                if (not raw) or (raw and raw.replace('.', '', 1).isdigit() and '.' in raw):
                    vraw = n.get('value_norm') if n.get('value_norm') is not None else n.get('value')
                    iv = int(float(str(vraw).strip())) if vraw is not None else None
                    if iv is not None and 1900 <= iv <= 2105:
                        raw = str(iv)
            except Exception:
                pass
            # =====================================================================
            unit = str(n.get("unit") or "").strip()
            ctx = str(n.get("context") or n.get("context_snippet") or "").strip()
            # exactly 4 digits year and nothing else
            if re.fullmatch(r"(19|20)\d{2}", raw) and not unit:
                # if context is empty or super short, treat as junk
                if len(ctx) < 12:
                    return True
            return False
        except Exception:
            return False

    def _is_chrome_ctx(ctx: str) -> bool:
        if not ctx:
            return False
        low = ctx.lower()
        for h in globals().get("NON_DATA_CONTEXT_HINTS", []) or []:
            if h in low:
                return True
        return False

    if not isinstance(web_context, dict):
        return []

    scraped_meta = web_context.get("scraped_meta") or {}
    if not isinstance(scraped_meta, dict) or not scraped_meta:
        return []

    query = (web_context.get("query") or "")
    q_toks = set(_tokenize(query))

    out = []

    for url, meta in scraped_meta.items():
        if not isinstance(meta, dict):
            continue

        url_s = str(url or meta.get("url") or "").strip()
        if not url_s:
            continue

        extracted = meta.get("extracted_numbers") or []
        if not isinstance(extracted, list):
            extracted = []

        fp = meta.get("fingerprint") or meta.get("extract_hash") or meta.get("content_fingerprint")
        if fp and not isinstance(fp, str):
            fp = str(fp)
        if not fp and isinstance(meta.get("clean_text"), str):
            fp = _sha1(meta["clean_text"][:200000])

        status_detail = meta.get("status_detail") or meta.get("status") or ""
        fetched_ok = str(status_detail).startswith("success") or meta.get("status") == "fetched"

        is_homepage = _is_homepage_url(url_s)

        cleaned_numbers = []
        for n in extracted:
            if not isinstance(n, dict):
                continue

            # ---- Hard gate: year-only suppression ----
            if _looks_like_year_only(n):
                continue

            value = n.get("value")
            raw = n.get("raw")
            unit = n.get("unit")
            ctx = n.get("context") or n.get("context_snippet") or ""

            # normalize context
            ctx_s = ctx if isinstance(ctx, str) else ""
            ctx_s = ctx_s.strip()

            # ---- Hard gate: chrome/nav rejection (soft) ----
            chrome_ctx = _is_chrome_ctx(ctx_s)

            # ---- Light topic gate (soft): require some overlap with query tokens ----
            # This is intentionally mild: it *downweights* rather than drops everything.
            ctx_toks = set(_tokenize(ctx_s))
            tok_overlap = len(q_toks.intersection(ctx_toks)) if q_toks and ctx_toks else 0

            # quality scoring (small + interpretable)
            quality = 1.0
            reasons = []

            if is_homepage:
                quality *= 0.25
                reasons.append("homepage_like")

            if chrome_ctx:
                quality *= 0.40
                reasons.append("chrome_context")

            if q_toks and tok_overlap == 0:
                quality *= 0.55
                reasons.append("topic_miss")

            # cap/trim context snippet for JSON size
            ctx_snip = ctx_s[:240]

            cleaned_numbers.append({
                "value": value,
                "unit": unit,
                "raw": raw,
                "source_url": n.get("source_url") or url_s,
                "context_snippet": ctx_snip,
                "anchor_hash": n.get("anchor_hash") or _sha1(f"{url_s}|{ctx_snip}|{raw}|{unit}"),
                # Debug fields for tuning:
                "quality_score": round(float(quality), 3),
                "quality_reasons": reasons,
                "topic_overlap": tok_overlap,
            })

        out.append({
            "url": url_s,
            "status": "fetched_extracted" if cleaned_numbers else ("fetched" if fetched_ok else "failed"),
            "status_detail": status_detail,
            "numbers_found": len(cleaned_numbers),
            "fingerprint": fp or "",
            "fetched_at": meta.get("fetched_at") or _now(),
            "is_homepage_like": bool(is_homepage),
            "extracted_numbers": cleaned_numbers,
        })

    return out



def _build_source_snapshots_from_baseline_cache(baseline_cache: list) -> list:
    """
    Normalize prior cached source_results (from previous run) into a consistent schema.

    Tightening:
      - Detect domain-only/homepage URLs and label them (same as web_context snapshots)
      - Keep backward compatible fields; only add new fields.
    """
    from urllib.parse import urlparse

    def _is_homepage_url(u: str) -> bool:
        try:
            p = urlparse((u or "").strip())
            path = (p.path or "").strip()
            if path in ("", "/"):
                return True
            low = path.lower().rstrip("/")
            if low in ("/index", "/index.html", "/index.htm", "/home", "/default", "/default.aspx"):
                return True
            return False
        except Exception:
            return False

    out = []
    if not isinstance(baseline_cache, list):
        return out

    for sr in baseline_cache:
        if not isinstance(sr, dict):
            continue

        url = sr.get("url") or sr.get("source_url")
        if not url:
            continue
        url_s = str(url).strip()
        if not url_s:
            continue

        extracted = sr.get("extracted_numbers") or []
        if not isinstance(extracted, list):
            extracted = []

        cleaned = []
        for n in extracted:
            if not isinstance(n, dict):
                continue
            cleaned.append({
                "value": n.get("value"),
                "unit": n.get("unit"),
                "raw": n.get("raw"),
                "source_url": n.get("source_url") or url_s,
                "context": (n.get("context") or n.get("context_snippet") or "")[:220]
                if isinstance((n.get("context") or n.get("context_snippet")), str) else "",
            })

        fp = sr.get("fingerprint")
        if fp and not isinstance(fp, str):
            fp = str(fp)

        # --- homepage labeling (tightening #3) ---
        is_homepage = bool(sr.get("is_homepage")) or _is_homepage_url(url_s)
        quality_score = sr.get("quality_score")
        if quality_score is None:
            quality_score = 0.15 if is_homepage else 1.0

        skip_reason = sr.get("skip_reason") or ("homepage_url_low_signal" if is_homepage else "")

        host = sr.get("host") or ""
        path = sr.get("path") or ""
        if not host and not path:
            try:
                p = urlparse(url_s)
                host = p.netloc or ""
                path = p.path or ""
            except Exception:
                pass

        out.append({
            "url": url_s,
            "status": sr.get("status") or "",
            "status_detail": sr.get("status_detail") or "",
            "numbers_found": int(sr.get("numbers_found") or len(cleaned)),
            "fingerprint": fp,
            "fetched_at": sr.get("fetched_at"),
            "extracted_numbers": cleaned,

            # NEW debug fields (safe additions)
            "is_homepage": bool(is_homepage),
            "quality_score": float(quality_score),
            "skip_reason": skip_reason,
            "host": host,
            "path": path,
        })

    return out


def _merge_snapshots_prefer_cached_when_unchanged(current_snaps: list, cached_snaps: list) -> list:
    """
    Policy merge:
      - If current fingerprint matches cached fingerprint for same URL:
        reuse cached snapshot (even if live fetch worked)  ✅ point A
      - Else prefer current (fresh).
      - Add cached snapshots not present in current.
      - Also: if current numbers_found is 0 but cached has >0, reuse cached.
    """
    if not isinstance(current_snaps, list):
        current_snaps = []
    if not isinstance(cached_snaps, list):
        cached_snaps = []

    cached_by_url = {}
    for s in cached_snaps:
        if isinstance(s, dict) and s.get("url"):
            cached_by_url[str(s["url"])] = s

    merged = []
    seen = set()

    for cs in current_snaps:
        if not isinstance(cs, dict) or not cs.get("url"):
            continue
        url = str(cs["url"])
        seen.add(url)

        cached = cached_by_url.get(url)
        if not cached:
            merged.append(cs)
            continue

        cur_fp = cs.get("fingerprint")
        old_fp = cached.get("fingerprint")

        cur_nf = int(cs.get("numbers_found") or 0)
        old_nf = int(cached.get("numbers_found") or 0)

        # If current extraction is empty but cached had numbers, reuse cached.
        if cur_nf == 0 and old_nf > 0:
            merged.append(cached)
            continue

        # Fingerprint unchanged -> reuse cached even if live fetch worked.
        if cur_fp and old_fp and str(cur_fp) == str(old_fp):
            merged.append(cached)
        else:
            merged.append(cs)

    for url, cached in cached_by_url.items():
        if url not in seen:
            merged.append(cached)

    return merged


def _safe_parse_current_analysis(query: str, web_context: dict) -> dict:
    """
    Run the same analysis pipeline used in v7.27 to produce primary_response, but safely.
    Returns dict with at least {primary_response:{primary_metrics:{}}} or {} on failure.
    """
    import json
    qp = globals().get("query_perplexity")
    if not callable(qp):
        return {}

    try:
        raw = qp(query, web_context)
        if not raw or not isinstance(raw, str):
            return {}
        obj = json.loads(raw)
        if not isinstance(obj, dict):
            return {}
        return {"primary_response": obj}
    except Exception:
        return {}


def diff_metrics_by_name_BASE(prev_response: dict, cur_response: dict):
    """
    Canonical-first diff with:
      - HARD STOP when prev canonical_key is missing in current (no name fallback)
      - Row-level metric_definition sourced from PREVIOUS (original new analysis) schema:
          prev_response['metric_schema_frozen'][canonical_key] (preferred)
          else prev_response['primary_metrics_canonical'][canonical_key]
      - Backward compatible: still returns 'name' (non-empty) and existing fields.

    Returns:
            metric_changes, unchanged, increased, decreased, found = fn_diff(prev_response, cur_resp_for_diff)
    """
    import re

    # Defaults (used unless schema provides overrides)
    ABS_EPS = 1e-9
    REL_EPS = 0.0005

    def norm_name(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def parse_num(v, unit=""):
        fn = globals().get("parse_human_number")
        if callable(fn):
            try:
                return fn(str(v), unit)
            except Exception:
                return None
        try:
            return float(str(v).replace(",", "").strip())
        except Exception:
            return None

    # =========================================================================
    # PATCH D1 (ADDITIVE): canonical numeric extractor
    # - Prefer value_norm/base_unit when present (analysis/evolution alignment)
    # - Fall back to existing parse_num(value, unit) when canonical fields missing
    # =========================================================================
    def get_canonical_value_and_unit(m: dict):
        """
        Returns: (val: float|None, unit: str)
        Priority:
          1) value_norm (float-like) + base_unit (if present)
          2) parse_num(value, unit)
        """
        m = m if isinstance(m, dict) else {}

        # 1) canonical path
        if m.get("value_norm") is not None:
            try:
                v = float(m.get("value_norm"))
                u = str(m.get("base_unit") or m.get("unit") or "").strip()
                return v, u
            except Exception:
                pass

        # 2) legacy parse path
        # =================================================================
        # PATCH V27_DISABLE_NUMERIC_INFERENCE_FOR_CURRENT (ADDITIVE)
        # When canonical-for-render is active, do NOT infer/parse numbers from
        # free-form raw strings for CURRENT-side metrics. We only trust
        # schema-canonical value_norm emitted by the rebuild layer.
        #
        # Activation: cur_response['_disable_numeric_inference_v27'] == True
        # Safety: render/diff-layer only. Does not touch fastpath/hashing/etc.
        # =================================================================
        try:
            if isinstance(cur_response, dict) and cur_response.get("_disable_numeric_inference_v27"):
                u = str(m.get("unit") or "").strip()
                return None, u
        except Exception:
            pass
        u = str(m.get("unit") or "").strip()
        v = parse_num(m.get("value"), u)
        return v, u
    # =========================================================================

    # =========================================================================
    # PATCH D0 (ADDITIVE): anchor helpers (drift=0 stability)
    # NOTE (IMPORTANT):
    # - Anchor_hash equality should NOT force "unchanged" if numeric values differ.
    #   It means "we matched the same evidence anchor" (identity/matching), not
    #   that the metric's value necessarily didn't change.
    # - This patch keeps anchor_same, but uses it only for match_confidence +
    #   diagnostics, not as a classification override.
    # =========================================================================
    def _get_anchor_hash_from_metric(m: dict):
        try:
            if isinstance(m, dict):
                ah = m.get("anchor_hash") or m.get("anchor") or m.get("anchorHash")
                return str(ah) if ah else None
        except Exception:
            pass
        return None

    def _get_prev_anchor_hash(prev_resp: dict, ckey: str, pm: dict):
        # 1) direct on metric row
        ah = _get_anchor_hash_from_metric(pm)
        if ah:
            return ah

        # 2) prev_response.metric_anchors[ckey].anchor_hash
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            pass

        return None

    def _get_cur_anchor_hash(cur_resp: dict, ckey: str, cm: dict):
        # 1) direct on metric row (evolution rebuild puts anchor_hash here)
        ah = _get_anchor_hash_from_metric(cm)
        if ah:
            return ah

        # 2) cur_response.metric_anchors[ckey].anchor_hash (if present)
        try:
            ma = (cur_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            pass

        return None
    # =========================================================================

    # =========================================================================
    # PATCH MA2 (ADDITIVE): wire metric_anchors into row fields
    # - Populate context_snippet/source_url from prev_response.metric_anchors[ckey] when available
    # - Output enrichment only
    # =========================================================================
    def _get_prev_anchor_obj(prev_resp: dict, ckey: str):
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                return a if isinstance(a, dict) else {}
        except Exception:
            pass
        return {}

    def _anchor_meta(prev_resp: dict, cur_resp: dict, ckey: str, pm: dict, cm: dict):
        """
        Returns: (source_url, context_snippet, anchor_confidence)
        Priority:
          1) prev_response.metric_anchors[ckey] (baseline anchoring is authoritative)
          2) current metric row fields (if present)
          3) prev metric row fields (if present)
        """
        a = _get_prev_anchor_obj(prev_resp, ckey)

        src = a.get("source_url") or a.get("url")
        ctx = a.get("context_snippet") or a.get("context")
        conf = a.get("anchor_confidence")

        if not src:
            try:
                src = (cm or {}).get("source_url") or (cm or {}).get("url")
            except Exception:
                src = None
        if not ctx:
            try:
                ctx = (cm or {}).get("context_snippet") or (cm or {}).get("context")
            except Exception:
                ctx = None

        if not src:
            try:
                src = (pm or {}).get("source_url") or (pm or {}).get("url")
            except Exception:
                src = None
        if not ctx:
            try:
                ctx = (pm or {}).get("context_snippet") or (pm or {}).get("context")
            except Exception:
                ctx = None

        try:
            if isinstance(ctx, str):
                ctx = ctx.strip()[:220] or None
            else:
                ctx = None
        except Exception:
            ctx = None

        try:
            conf = float(conf) if conf is not None else None
        except Exception:
            conf = None

        return src, ctx, conf
    # =========================================================================

    def prettify_ckey(ckey: str) -> str:
        ckey = str(ckey or "").strip()
        if not ckey:
            return "Unknown Metric"
        parts = ckey.split("__", 1)
        left = parts[0].replace("_", " ").strip()
        right = parts[1].replace("_", " ").strip() if len(parts) > 1 else ""
        left = " ".join(w.capitalize() for w in left.split())
        return f"{left} ({right})" if right else left

    def get_metric_definition(prev_resp: dict, ckey: str) -> dict:
        """
        Pull authoritative definition from the ORIGINAL analysis run (prev_response).
        """
        prev_resp = prev_resp if isinstance(prev_resp, dict) else {}

        # =====================================================================
        # PATCH V33_SCHEMA_PATH_TOLERANT (ADDITIVE)
        # Support both legacy top-level and analysis-shape nested schema locations.
        # =====================================================================
        schema = prev_resp.get("metric_schema_frozen")
        if not isinstance(schema, dict) or not schema:
            schema = _get_nested(prev_resp, "primary_response", "metric_schema_frozen")
        if isinstance(schema, dict):
            d = schema.get(ckey)
            if isinstance(d, dict) and d:
                out = dict(d)
                out.setdefault("canonical_key", ckey)
                return out

        prev_can = prev_resp.get("primary_metrics_canonical")
        if not isinstance(prev_can, dict) or not prev_can:
            prev_can = _get_nested(prev_resp, "primary_response", "primary_metrics_canonical")
        if isinstance(prev_can, dict):
            d = prev_can.get(ckey)
            if isinstance(d, dict) and d:
                out = {
                    "canonical_key": ckey,
                    "canonical_id": d.get("canonical_id"),
                    "dimension": d.get("dimension"),
                    "name": d.get("name") or d.get("original_name"),
                    "unit": d.get("unit"),
                    "geo_scope": d.get("geo_scope"),
                    "geo_name": d.get("geo_name"),
                    "keywords": d.get("keywords"),
                }
                return {k: v for k, v in out.items() if v not in (None, "", [], {})}

        return {"canonical_key": ckey, "name": prettify_ckey(ckey)}

    def get_display_name(prev_resp: dict, prev_can_obj: dict, cur_can_obj: dict, ckey: str) -> str:
        schema = prev_resp.get("metric_schema_frozen")
        if isinstance(schema, dict):
            sm = schema.get(ckey)
            if isinstance(sm, dict):
                v = sm.get("name")
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(prev_can_obj, dict):
            for k in ("name", "original_name"):
                v = prev_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(cur_can_obj, dict):
            for k in ("name", "original_name"):
                v = cur_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        return prettify_ckey(ckey)

    # =========================================================================
    # PATCH D3 (ADDITIVE): schema-driven tolerances (optional)
    # - If schema provides abs_eps/rel_eps use them, else default.
    # =========================================================================
    def get_eps_for_metric(prev_resp: dict, ckey: str):
        ae = ABS_EPS
        re_ = REL_EPS
        try:
            schema = (prev_resp or {}).get("metric_schema_frozen")
            if isinstance(schema, dict):
                d = schema.get(ckey)
                if isinstance(d, dict):
                    if d.get("abs_eps") is not None:
                        try:
                            ae = float(d.get("abs_eps"))
                        except Exception:
                            pass
                    if d.get("rel_eps") is not None:
                        try:
                            re_ = float(d.get("rel_eps"))
                        except Exception:
                            pass
        except Exception:
            pass
        return ae, re_
    # =========================================================================

    prev_response = prev_response if isinstance(prev_response, dict) else {}
    cur_response = cur_response if isinstance(cur_response, dict) else {}

    prev_can = prev_response.get("primary_metrics_canonical")
    cur_can = cur_response.get("primary_metrics_canonical")

    # =====================================================================
    # PATCH V33_DIFF_CANONICAL_PATHS (ADDITIVE)
    # Goal:
    # - Evolution sometimes passes the "analysis-style" response shape where
    #   canonical metrics + frozen schema live under primary_response.*
    # - Our canonical-first diff (Path A) was reading ONLY top-level keys, so
    #   cur_can frequently became {} (or missing), producing blank/N/A current.
    #
    # Fix:
    # - Resolve canonical dict + schema using a tolerant, auditable path order:
    #   1) explicit injected payloads (e.g., _canonical_for_render, canonical_for_render)
    #   2) top-level primary_metrics_canonical / metric_schema_frozen (legacy)
    #   3) nested primary_response.primary_metrics_canonical / metric_schema_frozen (analysis shape)
    #
    # Safety:
    # - Purely affects diff-panel hydration (render-only), not fastpath/hashing/etc.
    # Diagnostics:
    # - debug.diff_panel_canonical_paths_v33 (selected paths + key counts)
    # =====================================================================
    _diff_panel_paths_v33 = {
        "prev_can_path": None,
        "cur_can_path": None,
        "prev_schema_path": None,
        "cur_schema_path": None,
        "prev_can_n": 0,
        "cur_can_n": 0,
    }

    def _get_nested(d: dict, *keys):
        d = d if isinstance(d, dict) else {}
        for k in keys:
            if not isinstance(d, dict):
                return None
            d = d.get(k)
        return d

    # Prefer explicitly injected render-canonical dicts if present
    _cur_injected = cur_response.get("_canonical_for_render") or cur_response.get("canonical_for_render")
    if isinstance(_cur_injected, dict) and _cur_injected:
        cur_can = _cur_injected
        _diff_panel_paths_v33["cur_can_path"] = "_canonical_for_render|canonical_for_render"

    _prev_injected = prev_response.get("_canonical_for_render") or prev_response.get("canonical_for_render")
    if isinstance(_prev_injected, dict) and _prev_injected:
        prev_can = _prev_injected
        _diff_panel_paths_v33["prev_can_path"] = "_canonical_for_render|canonical_for_render"

    # Fall back to nested analysis-shape payloads
    if not isinstance(cur_can, dict) or not cur_can:
        _nested_cur = _get_nested(cur_response, "primary_response", "primary_metrics_canonical")
        if isinstance(_nested_cur, dict) and _nested_cur:
            cur_can = _nested_cur
            _diff_panel_paths_v33["cur_can_path"] = "primary_response.primary_metrics_canonical"

    if not isinstance(prev_can, dict) or not prev_can:
        _nested_prev = _get_nested(prev_response, "primary_response", "primary_metrics_canonical")
        if isinstance(_nested_prev, dict) and _nested_prev:
            prev_can = _nested_prev
            _diff_panel_paths_v33["prev_can_path"] = "primary_response.primary_metrics_canonical"

    # Schema paths (used for eps overrides + metric_definition)
    _prev_schema = prev_response.get("metric_schema_frozen")
    if isinstance(_prev_schema, dict) and _prev_schema:
        _diff_panel_paths_v33["prev_schema_path"] = "metric_schema_frozen"
    else:
        _prev_schema = _get_nested(prev_response, "primary_response", "metric_schema_frozen")
        if isinstance(_prev_schema, dict) and _prev_schema:
            _diff_panel_paths_v33["prev_schema_path"] = "primary_response.metric_schema_frozen"

    _cur_schema = cur_response.get("metric_schema_frozen")
    if isinstance(_cur_schema, dict) and _cur_schema:
        _diff_panel_paths_v33["cur_schema_path"] = "metric_schema_frozen"
    else:
        _cur_schema = _get_nested(cur_response, "primary_response", "metric_schema_frozen")
        if isinstance(_cur_schema, dict) and _cur_schema:
            _diff_panel_paths_v33["cur_schema_path"] = "primary_response.metric_schema_frozen"

    _diff_panel_paths_v33["prev_can_n"] = len(prev_can) if isinstance(prev_can, dict) else 0
    _diff_panel_paths_v33["cur_can_n"] = len(cur_can) if isinstance(cur_can, dict) else 0

    try:
        if isinstance(cur_response, dict):
            cur_response["_diff_panel_canonical_paths_v33"] = dict(_diff_panel_paths_v33)
    except Exception:
        pass


    # =========================
    # Path A: canonical-first
    # =========================
    if isinstance(prev_can, dict) and isinstance(cur_can, dict) and prev_can:
        metric_changes = []
        unchanged = increased = decreased = found = 0

        for ckey, pm in prev_can.items():
            pm = pm if isinstance(pm, dict) else {}
            cm = cur_can.get(ckey)
            cm = cm if isinstance(cm, dict) else {}

            display_name = get_display_name(prev_response, pm, cm, ckey)
            definition = get_metric_definition(prev_response, ckey)

            prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

            # ✅ HARD STOP: canonical key missing in current => not_found (no name fallback)
            if ckey not in cur_can or not isinstance(cur_can.get(ckey), dict):
                # PATCH MA2 (ADDITIVE): fill row fields from metric_anchors where possible
                _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, {})

                metric_changes.append({
                    "name": display_name,
                    "previous_value": prev_raw,
                    "current_value": "N/A",
                    "change_pct": None,
                    "change_type": "not_found",
                    "match_confidence": 0.0,
                    "context_snippet": _ctx,
                    "source_url": _src,
                    "anchor_used": False,  # not applicable when current metric missing
                    "canonical_key": ckey,
                    "metric_definition": definition,
                    "anchor_confidence": _aconf,
                })
                continue

            found += 1

            cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")


            # =====================================================================
            # PATCH FIX2B_EVO_CURFIELDS_V1 (ADDITIVE): enrich evolution "Current" fields with unit + range (analysis semantics)
            #
            # Why:
            # - Evolution dashboard / sheet sanitizer (_fix39_has_unit_evidence) may blank rows when "current_value"
            #   lacks unit evidence even though the canonical metric has unit/value_norm.
            # - Also ensure any attached value_range is rendered in schema units (no scaling here).
            #
            # Notes:
            # - Purely additive: does not change change_pct logic (which uses value_norm via get_canonical_value_and_unit).
            # - Does NOT re-scale value_norm; assumes value_norm is already schema-normalized (FIX16/PH2B semantics).
            # =====================================================================
            _cur_unit_tag = (cm.get("unit") or cm.get("unit_tag") or "").strip()
            _cur_value_txt = "" if cur_raw is None else str(cur_raw).strip()
            _cur_raw_display = _cur_value_txt
            try:
                if _cur_unit_tag and _cur_value_txt and (_cur_unit_tag not in _cur_value_txt):
                    _cur_raw_display = f"{_cur_value_txt} {_cur_unit_tag}".strip()
            except Exception:
                pass

            _cur_value_range = None
            _cur_value_range_display = ""
            try:
                if isinstance(cm.get("value_range"), dict):
                    _cur_value_range = dict(cm.get("value_range"))
                    if _cur_unit_tag and ("min" in _cur_value_range) and ("max" in _cur_value_range):
                        _cur_value_range_display = f"{_cur_value_range['min']:g}–{_cur_value_range['max']:g} {_cur_unit_tag}".strip()
            except Exception:
                pass
            # =====================================================================

            # =========================================================================
            # PATCH D0 (ADDITIVE): anchor identity (do NOT force unchanged)
            # =========================================================================
            prev_ah = _get_prev_anchor_hash(prev_response, ckey, pm)
            cur_ah = _get_cur_anchor_hash(cur_response, ckey, cm)
            anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))
            # =========================================================================

            # PATCH D2 (ADDITIVE): use canonical values for diff when available
            prev_val, prev_unit_cmp = get_canonical_value_and_unit(pm)
            cur_val, cur_unit_cmp = get_canonical_value_and_unit(cm)

            # PATCH D3 (ADDITIVE): metric-specific tolerances (schema overrides)
            abs_eps, rel_eps = get_eps_for_metric(prev_response, ckey)

            change_type = "unknown"
            change_pct = None

            # =========================================================================
            # PATCH D0B (ADDITIVE, REQUIRED): numeric-first classification even if anchors match
            # Why:
            # - anchor_same means "we matched the same evidence anchor"
            # - It MUST NOT short-circuit classification to "unchanged" when values differ.
            # - This fixes the exact bug you observed: prev_value_norm != cur_value_norm
            #   while change_type incorrectly says "unchanged".
            # =========================================================================
            if prev_val is not None and cur_val is not None:
                if abs(prev_val - cur_val) <= max(abs_eps, abs(prev_val) * rel_eps):
                    change_type = "unchanged"
                    change_pct = 0.0
                    unchanged += 1
                elif cur_val > prev_val:
                    change_type = "increased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    increased += 1
                else:
                    change_type = "decreased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    decreased += 1
            # If we cannot compare numerically, fall back:
            # - If anchors match, treat as unchanged ONLY as a last resort (formatting issue)
            elif anchor_same:
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            # =========================================================================

            # PATCH D4 (ADDITIVE): unit mismatch flag (debug only)
            unit_mismatch = False
            try:
                if prev_unit_cmp and cur_unit_cmp and str(prev_unit_cmp) != str(cur_unit_cmp):
                    unit_mismatch = True
            except Exception:
                unit_mismatch = False

            # PATCH MA2 (ADDITIVE): fill row fields from metric_anchors where possible
            _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, cm)

            # PATCH D0C (ADDITIVE): match_confidence reflects anchor identity
            match_conf = 92.0
            try:
                if anchor_same:
                    match_conf = 98.0
            except Exception:
                match_conf = 92.0

            metric_changes.append({
                "name": display_name,
                "previous_value": prev_raw,
                "current_value": cur_raw,
                "current_raw": _cur_raw_display,
                "current_unit_tag": _cur_unit_tag,
                "current_value_norm": cm.get("value_norm"),
                "current_value_range": _cur_value_range,
                "current_value_range_display": _cur_value_range_display,
                "change_pct": change_pct,
                "change_type": change_type,
                "match_confidence": float(match_conf),

                "context_snippet": _ctx,
                "source_url": _src,

                # anchor identity (matching), not classification
                "anchor_used": bool(anchor_same),
                "prev_anchor_hash": prev_ah,
                "cur_anchor_hash": cur_ah,

                "canonical_key": ckey,
                "metric_definition": definition,

                "anchor_confidence": _aconf,

                # expose canonical comparison basis for debugging/convergence
                "prev_value_norm": prev_val,
                "cur_value_norm": cur_val,
                "prev_unit_cmp": prev_unit_cmp,
                "cur_unit_cmp": cur_unit_cmp,
                # PATCH FIX2B_TRACE_V1 (ADDITIVE): expose chosen URL + selector trace (no behavior change)
                "cur_source_url": str((cur_can_obj or {}).get("source_url") or ""),
                "selector_used": str((cur_can_obj or {}).get("selector_used") or ""),
                "evo_selector_trace_v1": (dict((cur_can_obj or {}).get("analysis_selector_trace_v1") or {}) if isinstance((cur_can_obj or {}).get("analysis_selector_trace_v1"), dict) else {}),
                "winner_candidate_debug": (dict((cur_can_obj or {}).get("winner_candidate_debug") or {}) if isinstance((cur_can_obj or {}).get("winner_candidate_debug"), dict) else {}),
                "would_block_reason": str((cur_can_obj or {}).get("would_block_reason") or ""),
"unit_mismatch": bool(unit_mismatch),
                "abs_eps_used": abs_eps,
                "rel_eps_used": rel_eps,
            })

        return metric_changes, unchanged, increased, decreased, found

    # =========================
    # Path B: legacy name fallback
    # =========================
    prev_metrics = prev_response.get("primary_metrics") or {}
    cur_metrics = cur_response.get("primary_metrics") or {}
    if not isinstance(prev_metrics, dict):
        prev_metrics = {}
    if not isinstance(cur_metrics, dict):
        cur_metrics = {}

    prev_index = {}
    for k, m in prev_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            prev_index[norm_name(name)] = (name, m)

    cur_index = {}
    for k, m in cur_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            cur_index[norm_name(name)] = (name, m)

    metric_changes = []
    unchanged = increased = decreased = found = 0

    for nk, (display_name, pm) in prev_index.items():
        prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

        if nk not in cur_index:
            metric_changes.append({
                "name": display_name or "Unknown Metric",
                "previous_value": prev_raw,
                "current_value": "N/A",
                "change_pct": None,
                "change_type": "not_found",
                "match_confidence": 0.0,
                "context_snippet": None,
                "source_url": None,
                "anchor_used": False,
            })
            continue

        found += 1
        _, cm = cur_index[nk]
        cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

        prev_val, _prev_unit_cmp = get_canonical_value_and_unit(pm)
        cur_val, _cur_unit_cmp = get_canonical_value_and_unit(cm)

        prev_ah = _get_anchor_hash_from_metric(pm)
        cur_ah = _get_anchor_hash_from_metric(cm)
        anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))

        change_type = "unknown"
        change_pct = None

        # PATCH D0B mirrors canonical path: numeric-first, anchor fallback only if numeric missing
        if prev_val is not None and cur_val is not None:
            if abs(prev_val - cur_val) <= max(ABS_EPS, abs(prev_val) * REL_EPS):
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            elif cur_val > prev_val:
                change_type = "increased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                increased += 1
            else:
                change_type = "decreased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                decreased += 1
        elif anchor_same:
            change_type = "unchanged"
            change_pct = 0.0
            unchanged += 1

        metric_changes.append({
            "name": display_name or "Unknown Metric",
            "previous_value": prev_raw,
            "current_value": cur_raw,
            "change_pct": change_pct,
            "change_type": change_type,
            "match_confidence": 90.0 if anchor_same else 80.0,
            "context_snippet": None,
            "source_url": None,

            "anchor_used": bool(anchor_same),
            "prev_anchor_hash": prev_ah,
            "cur_anchor_hash": cur_ah,

            "prev_value_norm": prev_val,
            "cur_value_norm": cur_val,
        })

    return metric_changes, unchanged, increased, decreased, found







# =====================================================================
# PATCH WRAP_DIFF_METRICS_BY_NAME (ADDITIVE): preserve original as diff_metrics_by_name_BASE
# and define the patched version below.
# =====================================================================

def diff_metrics_by_name(prev_response: dict, cur_response: dict):
    """
    Canonical-first diff with:
      - HARD STOP when prev canonical_key is missing in current (no name fallback)
      - Row-level metric_definition sourced from PREVIOUS (original new analysis) schema:
          prev_response['metric_schema_frozen'][canonical_key] (preferred)
          else prev_response['primary_metrics_canonical'][canonical_key]
      - Backward compatible: still returns 'name' (non-empty) and existing fields.

    Returns:
      metric_changes, unchanged, increased, decreased, found
    """
    import re

    # Defaults (used unless schema provides overrides)
    ABS_EPS = 1e-9
    REL_EPS = 0.0005

    def norm_name(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def parse_num(v, unit=""):
        fn = globals().get("parse_human_number")
        if callable(fn):
            try:
                return fn(str(v), unit)
            except Exception:
                return None
        try:
            return float(str(v).replace(",", "").strip())
        except Exception:
            return None

    # =========================================================================
    # PATCH D1 (ADDITIVE): canonical numeric extractor
    # - Prefer value_norm/base_unit when present (analysis/evolution alignment)
    # - Fall back to existing parse_num(value, unit) when canonical fields missing
    # =========================================================================
    def get_canonical_value_and_unit(m: dict):
        """
        Returns: (val: float|None, unit: str)
        Priority:
          1) value_norm (float-like) + base_unit (if present)
          2) parse_num(value, unit)
        """
        m = m if isinstance(m, dict) else {}

        # 1) canonical path
        if m.get("value_norm") is not None:
            try:
                v = float(m.get("value_norm"))
                u = str(m.get("base_unit") or m.get("unit") or "").strip()
                return v, u
            except Exception:
                pass

        # 2) legacy parse path
        # =================================================================
        # PATCH V27_DISABLE_NUMERIC_INFERENCE_FOR_CURRENT (ADDITIVE)
        # When canonical-for-render is active, do NOT infer/parse numbers from
        # free-form raw strings for CURRENT-side metrics. We only trust
        # schema-canonical value_norm emitted by the rebuild layer.
        #
        # Activation: cur_response['_disable_numeric_inference_v27'] == True
        # Safety: render/diff-layer only. Does not touch fastpath/hashing/etc.
        # =================================================================
        try:
            if isinstance(cur_response, dict) and cur_response.get("_disable_numeric_inference_v27"):
                u = str(m.get("unit") or "").strip()
                return None, u
        except Exception:
            pass
        u = str(m.get("unit") or "").strip()
        v = parse_num(m.get("value"), u)
        return v, u
    # =========================================================================

    # =========================================================================
    # PATCH D0 (ADDITIVE): anchor helpers (drift=0 stability)
    # NOTE (IMPORTANT):
    # - Anchor_hash equality should NOT force "unchanged" if numeric values differ.
    #   It means "we matched the same evidence anchor" (identity/matching), not
    #   that the metric's value necessarily didn't change.
    # - This patch keeps anchor_same, but uses it only for match_confidence +
    #   diagnostics, not as a classification override.
    # =========================================================================
    def _get_anchor_hash_from_metric(m: dict):
        try:
            if isinstance(m, dict):
                ah = m.get("anchor_hash") or m.get("anchor") or m.get("anchorHash")
                return str(ah) if ah else None
        except Exception:
            pass
        return None

    def _get_prev_anchor_hash(prev_resp: dict, ckey: str, pm: dict):
        # 1) direct on metric row
        ah = _get_anchor_hash_from_metric(pm)
        if ah:
            return ah

        # 2) prev_response.metric_anchors[ckey].anchor_hash
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            pass

        return None

    def _get_cur_anchor_hash(cur_resp: dict, ckey: str, cm: dict):
        # 1) direct on metric row (evolution rebuild puts anchor_hash here)
        ah = _get_anchor_hash_from_metric(cm)
        if ah:
            return ah

        # 2) cur_response.metric_anchors[ckey].anchor_hash (if present)
        try:
            ma = (cur_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            pass

        return None
    # =========================================================================

    # =========================================================================
    # PATCH MA2 (ADDITIVE): wire metric_anchors into row fields
    # - Populate context_snippet/source_url from prev_response.metric_anchors[ckey] when available
    # - Output enrichment only
    # =========================================================================
    def _get_prev_anchor_obj(prev_resp: dict, ckey: str):
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                return a if isinstance(a, dict) else {}
        except Exception:
            pass
        return {}

    def _anchor_meta(prev_resp: dict, cur_resp: dict, ckey: str, pm: dict, cm: dict):
        """
        Returns: (source_url, context_snippet, anchor_confidence)
        Priority:
          1) prev_response.metric_anchors[ckey] (baseline anchoring is authoritative)
          2) current metric row fields (if present)
          3) prev metric row fields (if present)
        """
        a = _get_prev_anchor_obj(prev_resp, ckey)

        src = a.get("source_url") or a.get("url")
        ctx = a.get("context_snippet") or a.get("context")
        conf = a.get("anchor_confidence")

        if not src:
            try:
                src = (cm or {}).get("source_url") or (cm or {}).get("url")
            except Exception:
                src = None
        if not ctx:
            try:
                ctx = (cm or {}).get("context_snippet") or (cm or {}).get("context")
            except Exception:
                ctx = None

        if not src:
            try:
                src = (pm or {}).get("source_url") or (pm or {}).get("url")
            except Exception:
                src = None
        if not ctx:
            try:
                ctx = (pm or {}).get("context_snippet") or (pm or {}).get("context")
            except Exception:
                ctx = None

        try:
            if isinstance(ctx, str):
                ctx = ctx.strip()[:220] or None
            else:
                ctx = None
        except Exception:
            ctx = None

        try:
            conf = float(conf) if conf is not None else None
        except Exception:
            conf = None

        return src, ctx, conf
    # =========================================================================

    def prettify_ckey(ckey: str) -> str:
        ckey = str(ckey or "").strip()
        if not ckey:
            return "Unknown Metric"
        parts = ckey.split("__", 1)
        left = parts[0].replace("_", " ").strip()
        right = parts[1].replace("_", " ").strip() if len(parts) > 1 else ""
        left = " ".join(w.capitalize() for w in left.split())
        return f"{left} ({right})" if right else left

    def get_metric_definition(prev_resp: dict, ckey: str) -> dict:
        """
        Pull authoritative definition from the ORIGINAL analysis run (prev_response).
        """
        prev_resp = prev_resp if isinstance(prev_resp, dict) else {}

        schema = prev_resp.get("metric_schema_frozen")
        if isinstance(schema, dict):
            d = schema.get(ckey)
            if isinstance(d, dict) and d:
                out = dict(d)
                out.setdefault("canonical_key", ckey)
                return out

        prev_can = prev_resp.get("primary_metrics_canonical")
        if isinstance(prev_can, dict):
            d = prev_can.get(ckey)
            if isinstance(d, dict) and d:
                out = {
                    "canonical_key": ckey,
                    "canonical_id": d.get("canonical_id"),
                    "dimension": d.get("dimension"),
                    "name": d.get("name") or d.get("original_name"),
                    "unit": d.get("unit"),
                    "geo_scope": d.get("geo_scope"),
                    "geo_name": d.get("geo_name"),
                    "keywords": d.get("keywords"),
                }
                return {k: v for k, v in out.items() if v not in (None, "", [], {})}

        return {"canonical_key": ckey, "name": prettify_ckey(ckey)}

    def get_display_name(prev_resp: dict, prev_can_obj: dict, cur_can_obj: dict, ckey: str) -> str:
        schema = prev_resp.get("metric_schema_frozen")
        if isinstance(schema, dict):
            sm = schema.get(ckey)
            if isinstance(sm, dict):
                v = sm.get("name")
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(prev_can_obj, dict):
            for k in ("name", "original_name"):
                v = prev_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(cur_can_obj, dict):
            for k in ("name", "original_name"):
                v = cur_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        return prettify_ckey(ckey)

    # =========================================================================
    # PATCH D3 (ADDITIVE): schema-driven tolerances (optional)
    # - If schema provides abs_eps/rel_eps use them, else default.
    # =========================================================================
    def get_eps_for_metric(prev_resp: dict, ckey: str):
        ae = ABS_EPS
        re_ = REL_EPS
        try:
            schema = (prev_resp or {}).get("metric_schema_frozen")
            if isinstance(schema, dict):
                d = schema.get(ckey)
                if isinstance(d, dict):
                    if d.get("abs_eps") is not None:
                        try:
                            ae = float(d.get("abs_eps"))
                        except Exception:
                            pass
                    if d.get("rel_eps") is not None:
                        try:
                            re_ = float(d.get("rel_eps"))
                        except Exception:
                            pass
        except Exception:
            pass
        return ae, re_
    # =========================================================================

    prev_response = prev_response if isinstance(prev_response, dict) else {}
    cur_response = cur_response if isinstance(cur_response, dict) else {}

    # =====================================================================
    # PATCH AI_ANCHMAP1 (ADDITIVE): normalize metric_anchors shape (list -> dict)
    # Why:
    # - Some pipelines persist metric_anchors as a list of records:
    #     [{"canonical_key": ..., "anchor_hash": ..., ...}, ...]
    # - Diff expects a dict mapping canonical_key -> anchor object.
    # Determinism:
    # - Pure reshaping; no new anchors invented.
    # =====================================================================
    def _coerce_metric_anchors_to_dict(resp: dict):
        try:
            if not isinstance(resp, dict):
                return resp
            ma = resp.get("metric_anchors")
            if isinstance(ma, dict) or ma is None:
                return resp
            if isinstance(ma, list):
                out = {}
                for a in ma:
                    if not isinstance(a, dict):
                        continue
                    ck = a.get("canonical_key") or a.get("ckey") or a.get("metric_key")
                    if not ck:
                        continue
                    if ck not in out:
                        out[str(ck)] = a
                resp["metric_anchors"] = out
            return resp
        except Exception:
            return resp

    prev_response = _coerce_metric_anchors_to_dict(prev_response)
    cur_response = _coerce_metric_anchors_to_dict(cur_response)
    # =====================================================================

    prev_can = prev_response.get("primary_metrics_canonical")
    cur_can = cur_response.get("primary_metrics_canonical")

    # =========================
    # Path A: canonical-first
    # =========================
    if isinstance(prev_can, dict) and isinstance(cur_can, dict) and prev_can:
        metric_changes = []
        unchanged = increased = decreased = found = 0

        for ckey, pm in prev_can.items():
            pm = pm if isinstance(pm, dict) else {}
            cm = cur_can.get(ckey)
            cm = cm if isinstance(cm, dict) else {}

            display_name = get_display_name(prev_response, pm, cm, ckey)
            definition = get_metric_definition(prev_response, ckey)

            prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

            # ✅ HARD STOP: canonical key missing in current => not_found (no name fallback)
            if ckey not in cur_can or not isinstance(cur_can.get(ckey), dict):
                # PATCH MA2 (ADDITIVE): fill row fields from metric_anchors where possible
                _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, {})

                metric_changes.append({
                    "name": display_name,
                    "previous_value": prev_raw,
                    "current_value": "N/A",
                    "change_pct": None,
                    "change_type": "not_found",
                    "match_confidence": 0.0,
                    "context_snippet": _ctx,
                    "source_url": _src,
                    "anchor_used": False,  # not applicable when current metric missing
                    "canonical_key": ckey,
                    "metric_definition": definition,
                    "anchor_confidence": _aconf,
                })
                continue

            found += 1

            cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

            # =========================================================================
            # PATCH D0 (ADDITIVE): anchor identity (do NOT force unchanged)
            # =========================================================================
            prev_ah = _get_prev_anchor_hash(prev_response, ckey, pm)
            cur_ah = _get_cur_anchor_hash(cur_response, ckey, cm)
            anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))
            # =========================================================================

            # PATCH D2 (ADDITIVE): use canonical values for diff when available
            prev_val, prev_unit_cmp = get_canonical_value_and_unit(pm)
            cur_val, cur_unit_cmp = get_canonical_value_and_unit(cm)

            # PATCH D3 (ADDITIVE): metric-specific tolerances (schema overrides)
            abs_eps, rel_eps = get_eps_for_metric(prev_response, ckey)

            change_type = "unknown"
            change_pct = None

            # =========================================================================
            # PATCH D0B (ADDITIVE, REQUIRED): numeric-first classification even if anchors match
            # Why:
            # - anchor_same means "we matched the same evidence anchor"
            # - It MUST NOT short-circuit classification to "unchanged" when values differ.
            # - This fixes the exact bug you observed: prev_value_norm != cur_value_norm
            #   while change_type incorrectly says "unchanged".
            # =========================================================================
            if prev_val is not None and cur_val is not None:
                if abs(prev_val - cur_val) <= max(abs_eps, abs(prev_val) * rel_eps):
                    change_type = "unchanged"
                    change_pct = 0.0
                    unchanged += 1
                elif cur_val > prev_val:
                    change_type = "increased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    increased += 1
                else:
                    change_type = "decreased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    decreased += 1
            # If we cannot compare numerically, fall back:
            # - If anchors match, treat as unchanged ONLY as a last resort (formatting issue)
            elif anchor_same:
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            # =========================================================================

            # PATCH D4 (ADDITIVE): unit mismatch flag (debug only)
            unit_mismatch = False
            try:
                if prev_unit_cmp and cur_unit_cmp and str(prev_unit_cmp) != str(cur_unit_cmp):
                    unit_mismatch = True
            except Exception:
                unit_mismatch = False

            # PATCH MA2 (ADDITIVE): fill row fields from metric_anchors where possible
            _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, cm)

            # PATCH D0C (ADDITIVE): match_confidence reflects anchor identity
            match_conf = 92.0
            try:
                if anchor_same:
                    match_conf = 98.0
            except Exception:
                match_conf = 92.0

            metric_changes.append({
                "name": display_name,
                "previous_value": prev_raw,
                "current_value": cur_raw,
                "change_pct": change_pct,
                "change_type": change_type,
                "match_confidence": float(match_conf),

                "context_snippet": _ctx,
                "source_url": _src,

                # anchor identity (matching), not classification
                "anchor_used": bool(anchor_same),
                "prev_anchor_hash": prev_ah,
                "cur_anchor_hash": cur_ah,

                "canonical_key": ckey,
                "metric_definition": definition,

                "anchor_confidence": _aconf,

                # expose canonical comparison basis for debugging/convergence
                "prev_value_norm": prev_val,
                "cur_value_norm": cur_val,
                "prev_unit_cmp": prev_unit_cmp,
                "cur_unit_cmp": cur_unit_cmp,
                "unit_mismatch": bool(unit_mismatch),
                "abs_eps_used": abs_eps,
                "rel_eps_used": rel_eps,
            })

        return metric_changes, unchanged, increased, decreased, found

    # =========================
    # Path B: legacy name fallback
    # =========================
    prev_metrics = prev_response.get("primary_metrics") or {}
    cur_metrics = cur_response.get("primary_metrics") or {}
    if not isinstance(prev_metrics, dict):
        prev_metrics = {}
    if not isinstance(cur_metrics, dict):
        cur_metrics = {}

    prev_index = {}
    for k, m in prev_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            prev_index[norm_name(name)] = (name, m)

    cur_index = {}
    for k, m in cur_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            cur_index[norm_name(name)] = (name, m)

    metric_changes = []
    unchanged = increased = decreased = found = 0

    for nk, (display_name, pm) in prev_index.items():
        prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

        if nk not in cur_index:
            metric_changes.append({
                "name": display_name or "Unknown Metric",
                "previous_value": prev_raw,
                "current_value": "N/A",
                "change_pct": None,
                "change_type": "not_found",
                "match_confidence": 0.0,
                "context_snippet": None,
                "source_url": None,
                "anchor_used": False,
            })
            continue

        found += 1
        _, cm = cur_index[nk]
        cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

        prev_val, _prev_unit_cmp = get_canonical_value_and_unit(pm)
        cur_val, _cur_unit_cmp = get_canonical_value_and_unit(cm)

        prev_ah = _get_anchor_hash_from_metric(pm)
        cur_ah = _get_anchor_hash_from_metric(cm)
        anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))

        change_type = "unknown"
        change_pct = None

        # PATCH D0B mirrors canonical path: numeric-first, anchor fallback only if numeric missing
        if prev_val is not None and cur_val is not None:
            if abs(prev_val - cur_val) <= max(ABS_EPS, abs(prev_val) * REL_EPS):
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            elif cur_val > prev_val:
                change_type = "increased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                increased += 1
            else:
                change_type = "decreased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                decreased += 1
        elif anchor_same:
            change_type = "unchanged"
            change_pct = 0.0
            unchanged += 1

        metric_changes.append({
            "name": display_name or "Unknown Metric",
            "previous_value": prev_raw,
            "current_value": cur_raw,
            "change_pct": change_pct,
            "change_type": change_type,
            "match_confidence": 90.0 if anchor_same else 80.0,
            "context_snippet": None,
            "source_url": None,

            "anchor_used": bool(anchor_same),
            "prev_anchor_hash": prev_ah,
            "cur_anchor_hash": cur_ah,

            "prev_value_norm": prev_val,
            "cur_value_norm": cur_val,
        })

    return metric_changes, unchanged, increased, decreased, found

# =====================================================================
# PATCH DIFF_V2 (ADDITIVE): upgrade diff_metrics_by_name to anchor-first + value_norm-aware
# Why:
# - Enforce drift=0 when the same anchor_hash matches on both sides.
# - Prefer canonical numeric fields (value_norm/base_unit) to avoid unit/scale parsing drift.
# - Support schema-driven tolerances (abs_eps/rel_eps) when present.
# Notes:
# - Preserves prior implementation under diff_metrics_by_name_LEGACY.
# - No behavior change unless this upgraded function is called.
# =====================================================================
try:
    diff_metrics_by_name_LEGACY = diff_metrics_by_name  # type: ignore
except Exception:
    diff_metrics_by_name_LEGACY = None

def diff_metrics_by_name(prev_response: dict, cur_response: dict):
    """
    Canonical-first diff with:
      - HARD STOP when prev canonical_key is missing in current (no name fallback)
      - Row-level metric_definition sourced from PREVIOUS (original new analysis) schema:
          prev_response['metric_schema_frozen'][canonical_key] (preferred)
          else prev_response['primary_metrics_canonical'][canonical_key]
      - Backward compatible: still returns 'name' (non-empty) and existing fields.

    Returns:
      metric_changes, unchanged, increased, decreased, found
    """
    import re

    # Defaults (used unless schema provides overrides)
    ABS_EPS = 1e-9
    REL_EPS = 0.0005

    def norm_name(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def parse_num(v, unit=""):
        fn = globals().get("parse_human_number")
        if callable(fn):
            try:
                return fn(str(v), unit)
            except Exception:
                return None
        try:
            return float(str(v).replace(",", "").strip())
        except Exception:
            return None

    # =========================================================================
    # PATCH D1 (ADDITIVE): canonical numeric extractor
    # - Prefer value_norm/base_unit when present (analysis/evolution alignment)
    # - Fall back to existing parse_num(value, unit) when canonical fields missing
    # =========================================================================
    def get_canonical_value_and_unit(m: dict):
        """
        Returns: (val: float|None, unit: str)
        Priority:
          1) value_norm (float-like) + base_unit (if present)
          2) parse_num(value, unit)
        """
        m = m if isinstance(m, dict) else {}

        # 1) canonical path
        if m.get("value_norm") is not None:
            try:
                v = float(m.get("value_norm"))
                u = str(m.get("base_unit") or m.get("unit") or "").strip()
                return v, u
            except Exception:
                pass

        # 2) legacy parse path
        # =================================================================
        # PATCH V27_DISABLE_NUMERIC_INFERENCE_FOR_CURRENT (ADDITIVE)
        # When canonical-for-render is active, do NOT infer/parse numbers from
        # free-form raw strings for CURRENT-side metrics. We only trust
        # schema-canonical value_norm emitted by the rebuild layer.
        #
        # Activation: cur_response['_disable_numeric_inference_v27'] == True
        # Safety: render/diff-layer only. Does not touch fastpath/hashing/etc.
        # =================================================================
        try:
            if isinstance(cur_response, dict) and cur_response.get("_disable_numeric_inference_v27"):
                u = str(m.get("unit") or "").strip()
                return None, u
        except Exception:
            pass
        u = str(m.get("unit") or "").strip()
        v = parse_num(m.get("value"), u)
        return v, u
    # =========================================================================

    # =========================================================================
    # PATCH D0 (ADDITIVE): anchor helpers (drift=0 stability)
    # NOTE (IMPORTANT):
    # - Anchor_hash equality should NOT force "unchanged" if numeric values differ.
    #   It means "we matched the same evidence anchor" (identity/matching), not
    #   that the metric's value necessarily didn't change.
    # - This patch keeps anchor_same, but uses it only for match_confidence +
    #   diagnostics, not as a classification override.
    # =========================================================================
    def _get_anchor_hash_from_metric(m: dict):
        try:
            if isinstance(m, dict):
                ah = m.get("anchor_hash") or m.get("anchor") or m.get("anchorHash")
                return str(ah) if ah else None
        except Exception:
            pass
        return None

    def _get_prev_anchor_hash(prev_resp: dict, ckey: str, pm: dict):
        # 1) direct on metric row
        ah = _get_anchor_hash_from_metric(pm)
        if ah:
            return ah

        # 2) prev_response.metric_anchors[ckey].anchor_hash
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            pass

        return None

    def _get_cur_anchor_hash(cur_resp: dict, ckey: str, cm: dict):
        # 1) direct on metric row (evolution rebuild puts anchor_hash here)
        ah = _get_anchor_hash_from_metric(cm)
        if ah:
            return ah

        # 2) cur_response.metric_anchors[ckey].anchor_hash (if present)
        try:
            ma = (cur_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            pass

        return None
    # =========================================================================

    # =========================================================================
    # PATCH MA2 (ADDITIVE): wire metric_anchors into row fields
    # - Populate context_snippet/source_url from prev_response.metric_anchors[ckey] when available
    # - Output enrichment only
    # =========================================================================
    def _get_prev_anchor_obj(prev_resp: dict, ckey: str):
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                return a if isinstance(a, dict) else {}
        except Exception:
            pass
        return {}

    def _anchor_meta(prev_resp: dict, cur_resp: dict, ckey: str, pm: dict, cm: dict):
        """
        Returns: (source_url, context_snippet, anchor_confidence)
        Priority:
          1) prev_response.metric_anchors[ckey] (baseline anchoring is authoritative)
          2) current metric row fields (if present)
          3) prev metric row fields (if present)
        """
        a = _get_prev_anchor_obj(prev_resp, ckey)

        src = a.get("source_url") or a.get("url")
        ctx = a.get("context_snippet") or a.get("context")
        conf = a.get("anchor_confidence")

        if not src:
            try:
                src = (cm or {}).get("source_url") or (cm or {}).get("url")
            except Exception:
                src = None
        if not ctx:
            try:
                ctx = (cm or {}).get("context_snippet") or (cm or {}).get("context")
            except Exception:
                ctx = None

        if not src:
            try:
                src = (pm or {}).get("source_url") or (pm or {}).get("url")
            except Exception:
                src = None
        if not ctx:
            try:
                ctx = (pm or {}).get("context_snippet") or (pm or {}).get("context")
            except Exception:
                ctx = None

        try:
            if isinstance(ctx, str):
                ctx = ctx.strip()[:220] or None
            else:
                ctx = None
        except Exception:
            ctx = None

        try:
            conf = float(conf) if conf is not None else None
        except Exception:
            conf = None

        return src, ctx, conf
    # =========================================================================

    def prettify_ckey(ckey: str) -> str:
        ckey = str(ckey or "").strip()
        if not ckey:
            return "Unknown Metric"
        parts = ckey.split("__", 1)
        left = parts[0].replace("_", " ").strip()
        right = parts[1].replace("_", " ").strip() if len(parts) > 1 else ""
        left = " ".join(w.capitalize() for w in left.split())
        return f"{left} ({right})" if right else left

    def get_metric_definition(prev_resp: dict, ckey: str) -> dict:
        """
        Pull authoritative definition from the ORIGINAL analysis run (prev_response).
        """
        prev_resp = prev_resp if isinstance(prev_resp, dict) else {}

        schema = prev_resp.get("metric_schema_frozen")
        if isinstance(schema, dict):
            d = schema.get(ckey)
            if isinstance(d, dict) and d:
                out = dict(d)
                out.setdefault("canonical_key", ckey)
                return out

        prev_can = prev_resp.get("primary_metrics_canonical")
        if isinstance(prev_can, dict):
            d = prev_can.get(ckey)
            if isinstance(d, dict) and d:
                out = {
                    "canonical_key": ckey,
                    "canonical_id": d.get("canonical_id"),
                    "dimension": d.get("dimension"),
                    "name": d.get("name") or d.get("original_name"),
                    "unit": d.get("unit"),
                    "geo_scope": d.get("geo_scope"),
                    "geo_name": d.get("geo_name"),
                    "keywords": d.get("keywords"),
                }
                return {k: v for k, v in out.items() if v not in (None, "", [], {})}

        return {"canonical_key": ckey, "name": prettify_ckey(ckey)}

    def get_display_name(prev_resp: dict, prev_can_obj: dict, cur_can_obj: dict, ckey: str) -> str:
        schema = prev_resp.get("metric_schema_frozen")
        if isinstance(schema, dict):
            sm = schema.get(ckey)
            if isinstance(sm, dict):
                v = sm.get("name")
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(prev_can_obj, dict):
            for k in ("name", "original_name"):
                v = prev_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(cur_can_obj, dict):
            for k in ("name", "original_name"):
                v = cur_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        return prettify_ckey(ckey)

    # =========================================================================
    # PATCH D3 (ADDITIVE): schema-driven tolerances (optional)
    # - If schema provides abs_eps/rel_eps use them, else default.
    # =========================================================================
    def get_eps_for_metric(prev_resp: dict, ckey: str):
        ae = ABS_EPS
        re_ = REL_EPS
        try:
            schema = (prev_resp or {}).get("metric_schema_frozen")
            if isinstance(schema, dict):
                d = schema.get(ckey)
                if isinstance(d, dict):
                    if d.get("abs_eps") is not None:
                        try:
                            ae = float(d.get("abs_eps"))
                        except Exception:
                            pass
                    if d.get("rel_eps") is not None:
                        try:
                            re_ = float(d.get("rel_eps"))
                        except Exception:
                            pass
        except Exception:
            pass
        return ae, re_
    # =========================================================================

    prev_response = prev_response if isinstance(prev_response, dict) else {}
    cur_response = cur_response if isinstance(cur_response, dict) else {}

    # =====================================================================
    # PATCH AI_ANCHMAP1 (ADDITIVE): normalize metric_anchors shape (list -> dict)
    # Why:
    # - Some pipelines persist metric_anchors as a list of records:
    #     [{"canonical_key": ..., "anchor_hash": ..., ...}, ...]
    # - Diff expects a dict mapping canonical_key -> anchor object.
    # Determinism:
    # - Pure reshaping; no new anchors invented.
    # =====================================================================
    def _coerce_metric_anchors_to_dict(resp: dict):
        try:
            if not isinstance(resp, dict):
                return resp
            ma = resp.get("metric_anchors")
            if isinstance(ma, dict) or ma is None:
                return resp
            if isinstance(ma, list):
                out = {}
                for a in ma:
                    if not isinstance(a, dict):
                        continue
                    ck = a.get("canonical_key") or a.get("ckey") or a.get("metric_key")
                    if not ck:
                        continue
                    if ck not in out:
                        out[str(ck)] = a
                resp["metric_anchors"] = out
            return resp
        except Exception:
            return resp

    prev_response = _coerce_metric_anchors_to_dict(prev_response)
    cur_response = _coerce_metric_anchors_to_dict(cur_response)
    # =====================================================================

    prev_can = prev_response.get("primary_metrics_canonical")
    cur_can = cur_response.get("primary_metrics_canonical")

    # =========================
    # Path A: canonical-first
    # =========================
    if isinstance(prev_can, dict) and isinstance(cur_can, dict) and prev_can:
        metric_changes = []
        unchanged = increased = decreased = found = 0

        for ckey, pm in prev_can.items():
            pm = pm if isinstance(pm, dict) else {}
            cm = cur_can.get(ckey)
            cm = cm if isinstance(cm, dict) else {}

            display_name = get_display_name(prev_response, pm, cm, ckey)
            definition = get_metric_definition(prev_response, ckey)

            prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

            # ✅ HARD STOP: canonical key missing in current => not_found (no name fallback)
            if ckey not in cur_can or not isinstance(cur_can.get(ckey), dict):
                # PATCH MA2 (ADDITIVE): fill row fields from metric_anchors where possible
                _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, {})

                metric_changes.append({
                    "name": display_name,
                    "previous_value": prev_raw,
                    "current_value": "N/A",
                    "change_pct": None,
                    "change_type": "not_found",
                    "match_confidence": 0.0,
                    "context_snippet": _ctx,
                    "source_url": _src,
                    "anchor_used": False,  # not applicable when current metric missing
                    "canonical_key": ckey,
                    "metric_definition": definition,
                    "anchor_confidence": _aconf,
                })
                continue

            found += 1

            cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

            # =========================================================================
            # PATCH D0 (ADDITIVE): anchor identity (do NOT force unchanged)
            # =========================================================================
            prev_ah = _get_prev_anchor_hash(prev_response, ckey, pm)
            cur_ah = _get_cur_anchor_hash(cur_response, ckey, cm)
            anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))
            # =========================================================================

            # PATCH D2 (ADDITIVE): use canonical values for diff when available
            prev_val, prev_unit_cmp = get_canonical_value_and_unit(pm)
            cur_val, cur_unit_cmp = get_canonical_value_and_unit(cm)

            # PATCH D3 (ADDITIVE): metric-specific tolerances (schema overrides)
            abs_eps, rel_eps = get_eps_for_metric(prev_response, ckey)

            change_type = "unknown"
            change_pct = None

            # =========================================================================
            # PATCH D0B (ADDITIVE, REQUIRED): numeric-first classification even if anchors match
            # Why:
            # - anchor_same means "we matched the same evidence anchor"
            # - It MUST NOT short-circuit classification to "unchanged" when values differ.
            # - This fixes the exact bug you observed: prev_value_norm != cur_value_norm
            #   while change_type incorrectly says "unchanged".
            # =========================================================================
            if prev_val is not None and cur_val is not None:
                if abs(prev_val - cur_val) <= max(abs_eps, abs(prev_val) * rel_eps):
                    change_type = "unchanged"
                    change_pct = 0.0
                    unchanged += 1
                elif cur_val > prev_val:
                    change_type = "increased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    increased += 1
                else:
                    change_type = "decreased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    decreased += 1
            # If we cannot compare numerically, fall back:
            # - If anchors match, treat as unchanged ONLY as a last resort (formatting issue)
            elif anchor_same:
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            # =========================================================================

            # PATCH D4 (ADDITIVE): unit mismatch flag (debug only)
            unit_mismatch = False
            try:
                if prev_unit_cmp and cur_unit_cmp and str(prev_unit_cmp) != str(cur_unit_cmp):
                    unit_mismatch = True
            except Exception:
                unit_mismatch = False

            # PATCH MA2 (ADDITIVE): fill row fields from metric_anchors where possible
            _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, cm)

            # PATCH D0C (ADDITIVE): match_confidence reflects anchor identity
            match_conf = 92.0
            try:
                if anchor_same:
                    match_conf = 98.0
            except Exception:
                match_conf = 92.0

            metric_changes.append({
                "name": display_name,
                "previous_value": prev_raw,
                "current_value": cur_raw,
                "change_pct": change_pct,
                "change_type": change_type,
                "match_confidence": float(match_conf),

                "context_snippet": _ctx,
                "source_url": _src,

                # anchor identity (matching), not classification
                "anchor_used": bool(anchor_same),
                "prev_anchor_hash": prev_ah,
                "cur_anchor_hash": cur_ah,

                "canonical_key": ckey,
                "metric_definition": definition,

                "anchor_confidence": _aconf,

                # expose canonical comparison basis for debugging/convergence
                "prev_value_norm": prev_val,
                "cur_value_norm": cur_val,
                "prev_unit_cmp": prev_unit_cmp,
                "cur_unit_cmp": cur_unit_cmp,
                "unit_mismatch": bool(unit_mismatch),
                "abs_eps_used": abs_eps,
                "rel_eps_used": rel_eps,
            })

        return metric_changes, unchanged, increased, decreased, found

    # =========================
    # Path B: legacy name fallback
    # =========================
    prev_metrics = prev_response.get("primary_metrics") or {}
    cur_metrics = cur_response.get("primary_metrics") or {}
    if not isinstance(prev_metrics, dict):
        prev_metrics = {}
    if not isinstance(cur_metrics, dict):
        cur_metrics = {}

    prev_index = {}
    for k, m in prev_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            prev_index[norm_name(name)] = (name, m)

    cur_index = {}
    for k, m in cur_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            cur_index[norm_name(name)] = (name, m)

    metric_changes = []
    unchanged = increased = decreased = found = 0

    for nk, (display_name, pm) in prev_index.items():
        prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

        if nk not in cur_index:
            metric_changes.append({
                "name": display_name or "Unknown Metric",
                "previous_value": prev_raw,
                "current_value": "N/A",
                "change_pct": None,
                "change_type": "not_found",
                "match_confidence": 0.0,
                "context_snippet": None,
                "source_url": None,
                "anchor_used": False,
            })
            continue

        found += 1
        _, cm = cur_index[nk]
        cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

        prev_val, _prev_unit_cmp = get_canonical_value_and_unit(pm)
        cur_val, _cur_unit_cmp = get_canonical_value_and_unit(cm)

        prev_ah = _get_anchor_hash_from_metric(pm)
        cur_ah = _get_anchor_hash_from_metric(cm)
        anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))

        change_type = "unknown"
        change_pct = None

        # PATCH D0B mirrors canonical path: numeric-first, anchor fallback only if numeric missing
        if prev_val is not None and cur_val is not None:
            if abs(prev_val - cur_val) <= max(ABS_EPS, abs(prev_val) * REL_EPS):
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            elif cur_val > prev_val:
                change_type = "increased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                increased += 1
            else:
                change_type = "decreased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                decreased += 1
        elif anchor_same:
            change_type = "unchanged"
            change_pct = 0.0
            unchanged += 1

        metric_changes.append({
            "name": display_name or "Unknown Metric",
            "previous_value": prev_raw,
            "current_value": cur_raw,
            "change_pct": change_pct,
            "change_type": change_type,
            "match_confidence": 90.0 if anchor_same else 80.0,
            "context_snippet": None,
            "source_url": None,

            "anchor_used": bool(anchor_same),
            "prev_anchor_hash": prev_ah,
            "cur_anchor_hash": cur_ah,

            "prev_value_norm": prev_val,
            "cur_value_norm": cur_val,
        })

    return metric_changes, unchanged, increased, decreased, found
# =====================================================================

def _fallback_match_from_snapshots(prev_numbers: dict, snapshots: list, anchors_by_name: dict):
    """
    When current analysis is missing, fall back to cached extracted_numbers only.
    If there is no snapshot candidate, return not_found ✅.

    Tightening implemented:
      1) Reject obvious year mismatches:
         - If metric name or prev_raw includes a year (e.g., 2024), require candidate context to contain it.
         - Also reject candidates that are a bare year if metric is not a year metric.
      2) Unit-family gating:
         - percent vs currency vs magnitude vs other (GW/TWh/tons/etc)
      3) Domain/homepage handling:
         - Downweight homepage sources heavily unless anchored (or if no non-homepage pool exists)

    Debugging enhancements:
      - Each metric row includes match_debug with:
        method, pool sizes, required years, unit families, best score, reject counts, top alternatives (small).
    """
    import re

    ABS_EPS = 1e-9
    REL_EPS = 0.0005

    def norm_unit(u: str) -> str:
        fn = globals().get("normalize_unit")
        if callable(fn):
            try:
                return fn(u)
            except Exception:
                pass
        return (u or "").strip()

    def parse_num(v, unit=""):
        fn = globals().get("parse_human_number")
        if callable(fn):
            try:
                return fn(str(v), unit)
            except Exception:
                return None
        try:
            return float(str(v).replace(",", "").strip())
        except Exception:
            return None

    def metric_tokens(name: str):
        toks = re.findall(r"[a-z0-9]+", (name or "").lower())
        stop = {"the","and","or","of","in","to","for","by","from","with","on","at","as"}
        return [t for t in toks if len(t) > 3 and t not in stop][:24]

    def unit_family(unit: str, raw: str = "", ctx: str = "") -> str:
        u = (norm_unit(unit) or "").strip().upper()
        blob = f"{raw or ''} {ctx or ''}".upper()

        # percent
        if u == "%" or "%" in blob:
            return "percent"

        # currency
        if any(x in blob for x in ["USD", "SGD", "EUR", "GBP", "S$", "$", "€", "£"]):
            return "currency"
        if any(x in u for x in ["USD", "SGD", "EUR", "GBP"]) or u.startswith("$") or u.startswith("S$"):
            return "currency"

        # magnitude suffix
        if u in ("K", "M", "B", "T") or any(x in blob for x in [" BILLION", " MILLION", " TRILLION", " BN", " MN"]):
            return "magnitude"

        # otherwise: other units like GW, TWh, tons, units, etc
        return "other"

    def required_years(metric_name: str, prev_raw: str) -> list:
        years = set()
        for s in [metric_name or "", prev_raw or ""]:
            for y in re.findall(r"\b(19\d{2}|20\d{2})\b", str(s)):
                years.add(y)
        return sorted(years)

    def year_ok(req_years: list, ctx: str) -> bool:
        if not req_years:
            return True
        c = (ctx or "").lower()
        return any(y.lower() in c for y in req_years)

    def is_bare_year(raw: str, unit: str) -> bool:
        r = (raw or "").strip()
        if unit and norm_unit(unit) not in ("", None):
            # If there is a unit, don't treat as bare year
            return False
        return bool(re.match(r"^(19\d{2}|20\d{2})$", r))

    def ctx_score(tokens, ctx: str) -> float:
        c = (ctx or "").lower()
        if not tokens:
            return 0.0
        hit = sum(1 for t in tokens if t in c)
        return hit / max(1, len(tokens))

    # Flatten candidates from snapshots ONLY, keep snapshot metadata
    candidates = []
    for sr in (snapshots or []):
        if not isinstance(sr, dict):
            continue
        url = sr.get("url")
        if not url:
            continue
        is_home = bool(sr.get("is_homepage"))
        qs = sr.get("quality_score", 1.0)
        try:
            qs = float(qs)
        except Exception:
            qs = 1.0

        for n in (sr.get("extracted_numbers") or []):
            if not isinstance(n, dict):
                continue
            candidates.append({
                "url": url,
                "value": n.get("value"),
                "unit": norm_unit(n.get("unit") or ""),
                "raw": n.get("raw") or "",
                "context": n.get("context") or "",
                "is_homepage": is_home,
                "quality_score": qs,
            })

    # Pre-split pools for tightening #3
    non_home = [c for c in candidates if not c.get("is_homepage")]
    home = [c for c in candidates if c.get("is_homepage")]

    out_changes = []
    for metric_name, prev in (prev_numbers or {}).items():
        prev_raw = prev.get("raw") or prev.get("value") or "N/A"
        prev_unit = norm_unit(prev.get("unit") or "")
        prev_val = prev.get("value")
        toks = prev.get("keywords") or metric_tokens(metric_name)

        req_years = required_years(metric_name, str(prev_raw))
        prev_fam = unit_family(prev_unit, str(prev_raw), "")

        anchor = anchors_by_name.get(metric_name) or {}
        anchor_url = anchor.get("source_url") if isinstance(anchor, dict) else None

        # Pool policy:
        # - anchored: use anchor_url pool if exists
        # - else: use non-homepage pool when available; only fall back to homepage if necessary
        pool_policy = "non_home_preferred"
        pool = non_home if non_home else candidates
        if anchor_url:
            anchored_pool = [c for c in candidates if c.get("url") == anchor_url]
            if anchored_pool:
                pool = anchored_pool
                pool_policy = "anchored_url"
            else:
                pool_policy = "anchored_url_not_present"

        reject_counts = {"year_mismatch": 0, "unit_mismatch": 0, "bare_year_reject": 0}
        best = None
        best_score = -1e9
        top_alts = []  # store a few near-misses for debugging

        for c in pool:
            ctx = c.get("context", "") or ""
            raw = c.get("raw", "") or ""
            unit = c.get("unit", "") or ""

            # (1) year gating: if required years exist, require them in context
            if not year_ok(req_years, ctx):
                reject_counts["year_mismatch"] += 1
                continue

            # reject bare-year candidates unless the metric itself is a year metric
            # (prevents "2024" being selected as a value for percent/currency/etc)
            if is_bare_year(str(raw), unit) and prev_fam != "other":
                reject_counts["bare_year_reject"] += 1
                continue

            # (2) unit-family gating
            cand_fam = unit_family(unit, raw, ctx)
            if prev_fam != cand_fam:
                reject_counts["unit_mismatch"] += 1
                continue

            score = ctx_score(toks, ctx)

            # bonus for numeric closeness
            cv = parse_num(c.get("value"), unit) or parse_num(raw, unit)
            if prev_val is not None and cv is not None:
                if abs(prev_val - cv) <= max(ABS_EPS, abs(prev_val) * REL_EPS):
                    score += 0.25

            # (3) homepage penalty unless anchored
            if c.get("is_homepage") and not anchor_url:
                score -= 0.35

            # quality_score weighting
            try:
                score *= max(0.1, min(1.0, float(c.get("quality_score", 1.0))))
            except Exception:
                pass

            # keep top alternatives for debugging
            if len(top_alts) < 5:
                top_alts.append({
                    "raw": raw[:60],
                    "unit": unit,
                    "url": c.get("url"),
                    "score": float(score),
                    "is_homepage": bool(c.get("is_homepage")),
                    "ctx": (ctx or "")[:120],
                })

            if score > best_score:
                best_score = score
                best = c

        # sort alt candidates by score desc
        try:
            top_alts.sort(key=lambda x: x.get("score", 0.0), reverse=True)
        except Exception:
            pass

        if not best:
            out_changes.append({
                "name": metric_name,
                "previous_value": prev_raw,
                "current_value": "N/A",
                "change_pct": None,
                "change_type": "not_found",
                "match_confidence": 0.0,
                "context_snippet": None,
                "source_url": None,
                "anchor_used": bool(anchor_url),

                # NEW debug payload
                "match_debug": {
                    "method": "snapshots_only",
                    "pool_policy": pool_policy,
                    "pool_size": int(len(pool)),
                    "req_years": req_years,
                    "prev_unit": prev_unit,
                    "prev_unit_family": prev_fam,
                    "reject_counts": reject_counts,
                    "top_alternatives": top_alts[:3],
                }
            })
            continue

        cur_raw = best.get("raw") or best.get("value")
        cv = parse_num(best.get("value"), best.get("unit")) or parse_num(cur_raw, best.get("unit"))

        change_type = "unknown"
        change_pct = None
        if prev_val is not None and cv is not None:
            if abs(prev_val - cv) <= max(ABS_EPS, abs(prev_val) * REL_EPS):
                change_type = "unchanged"
                change_pct = 0.0
            elif cv > prev_val:
                change_type = "increased"
                change_pct = ((cv - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
            else:
                change_type = "decreased"
                change_pct = ((cv - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0

        conf = max(0.0, min(60.0, best_score * 60.0))

        out_changes.append({
            "name": metric_name,
            "previous_value": prev_raw,
            "current_value": cur_raw,
            "change_pct": change_pct,
            "change_type": change_type,
            "match_confidence": float(conf),
            "context_snippet": (best.get("context") or "")[:200] if isinstance(best.get("context"), str) else None,
            "source_url": best.get("url"),
            "anchor_used": bool(anchor_url),

            # NEW debug payload
            "match_debug": {
                "method": "snapshots_only",
                "pool_policy": pool_policy,
                "pool_size": int(len(pool)),
                "req_years": req_years,
                "prev_unit": prev_unit,
                "prev_unit_family": prev_fam,
                "best_unit": best.get("unit"),
                "best_unit_family": unit_family(best.get("unit") or "", best.get("raw") or "", best.get("context") or ""),
                "best_score": float(best_score),
                "best_is_homepage": bool(best.get("is_homepage")),
                "reject_counts": reject_counts,
                "top_alternatives": top_alts[:3],
            }
        })

    return out_changes


def compute_source_anchored_diff_BASE(previous_data: dict, web_context: dict = None) -> dict:
    """
    Tight source-anchored evolution:
      - Prefer snapshots from analysis (baseline_sources_cache)
      - Optionally reconstruct snapshots from web_context.scraped_meta
      - If no valid snapshots: return not_found (no heuristic junk)

    Always returns a dict.
    """
    import re
    from datetime import datetime, timezone

    def _now():
        return datetime.now(timezone.utc).isoformat()

    def _safe_int(x, default=0):
        try:
            return int(x)
        except Exception:
            return default

    def _fingerprint(text: str):
        try:
            fn = globals().get("fingerprint_text")
            if callable(fn):
                return fn(text or "")
        except Exception:
            pass
        try:
            return fingerprint_text(text or "")
        except Exception:
            return None

    # ============================================================
    # PATCH CSR_UNWRAP1 (ADDITIVE): robust nested retrieval helpers
    # Why:
    # - Some runs store rebuild essentials under primary_response or results.primary_response
    # - Evolution may look only at top-level keys, causing schema=0 / anchors=0
    # ============================================================
    def _get_nested(d, path, default=None):
        try:
            x = d
            for k in path:
                if not isinstance(x, dict):
                    return default
                x = x.get(k)
            return x if x is not None else default
        except Exception:
            return default

    def _first_present(d, paths, default=None):
        for p in paths:
            v = _get_nested(d, p, None)
            if v is not None:
                return v
        return default
    # ============================================================

    # =====================================================================
    # PATCH HF5 (ADDITIVE): rehydrate previous_data from HistoryFull if wrapper
    # Why:
    # - Some UI/Sheets paths provide a summarized wrapper that lacks primary_response,
    #   metric_schema_frozen, metric_anchors, baseline_sources_cache, etc.
    # - If a full_store_ref pointer exists, load the full payload deterministically.
    #
    # NOTE:
    # - Do NOT write to `output` here (output not built yet). We stash flags
    #   and attach them after `output = {...}` is created.
    # =====================================================================
    _prev_rehydrated = False
    _prev_rehydrated_ref = ""

    try:
        if isinstance(previous_data, dict):
            _pr = previous_data.get("primary_response")

            # Determine if we are missing rebuild essentials
            _need = (
                (not isinstance(_pr, dict))
                or (not _pr)
                or (not isinstance(_pr.get("metric_schema_frozen"), dict))
            )

            if _need:
                # Explicit line (requested): simplest location first
                ref = previous_data.get("full_store_ref", "")  # <-- requested line

                # Then fall back to other known wrapper locations (more robust)
                _ref = (
                    ref
                    or (previous_data.get("results") or {}).get("full_store_ref")
                    or (isinstance(_pr, dict) and _pr.get("full_store_ref"))
                    or ""
                )

                # Last-ditch deterministic fallback: if wrapper carries _sheet_id
                if (not _ref) and isinstance(previous_data.get("_sheet_id"), str) and previous_data.get("_sheet_id"):
                    _ref = f"gsheet:HistoryFull:{previous_data.get('_sheet_id')}"

                if isinstance(_ref, str) and _ref.startswith("gsheet:"):
                    parts = _ref.split(":")
                    _ws_title = parts[1] if len(parts) > 1 and parts[1] else "HistoryFull"
                    _aid = parts[2] if len(parts) > 2 else ""
                    full = load_full_history_payload_from_sheet(_aid, worksheet_title=_ws_title) if _aid else {}
                    if isinstance(full, dict) and full:
                        previous_data = full
                        _prev_rehydrated = True
                        _prev_rehydrated_ref = _ref
    except Exception:
        pass
    # =====================================================================

    # ---------- Pull baseline snapshots (VALID only) ----------
    snapshot_origin = "none"
    baseline_sources_cache = []

    try:
        if isinstance(previous_data, dict):
            # 1) results.baseline_sources_cache (preferred)
            r = previous_data.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache"), list):
                baseline_sources_cache = r.get("baseline_sources_cache") or []
                if baseline_sources_cache:
                    snapshot_origin = "analysis_results_cache"

            # 2) top-level baseline_sources_cache
            if not baseline_sources_cache and isinstance(previous_data.get("baseline_sources_cache"), list):
                baseline_sources_cache = previous_data.get("baseline_sources_cache") or []
                if baseline_sources_cache:
                    snapshot_origin = "analysis_top_level_cache"
    except Exception:
        baseline_sources_cache = []

    # =====================================================================
    # PATCH ES1B (ADDITIVE): broaden snapshot discovery (legacy storage shapes)
    # =====================================================================
    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            pr = previous_data.get("primary_response") or {}
            if isinstance(pr, dict):
                # A) primary_response.results.baseline_sources_cache
                r2 = pr.get("results")
                if (not baseline_sources_cache) and isinstance(r2, dict) and isinstance(r2.get("baseline_sources_cache"), list):
                    baseline_sources_cache = r2.get("baseline_sources_cache") or []
                    if baseline_sources_cache:
                        snapshot_origin = "primary_response_results_cache"

                # B) primary_response.baseline_sources_cache
                if (not baseline_sources_cache) and isinstance(pr.get("baseline_sources_cache"), list):
                    baseline_sources_cache = pr.get("baseline_sources_cache") or []
                    if baseline_sources_cache:
                        snapshot_origin = "primary_response_top_level_cache"

                # C) primary_response.results.source_results (reconstruct minimal snapshot shape)
                if (not baseline_sources_cache) and isinstance(r2, dict) and isinstance(r2.get("source_results"), list):
                    rebuilt_sr = []
                    for sr in (r2.get("source_results") or []):
                        if not isinstance(sr, dict):
                            continue
                        u = sr.get("source_url") or sr.get("url")
                        ex = sr.get("extracted_numbers")
                        if u and isinstance(ex, list) and ex:
                            rebuilt_sr.append({
                                "source_url": u,
                                "extracted_numbers": ex,
                                "clean_text": sr.get("clean_text") or sr.get("content") or "",
                                "fingerprint": sr.get("fingerprint"),
                                "fetched_at": sr.get("fetched_at"),
                            })
                    rebuilt_sr.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))
                    if rebuilt_sr:
                        baseline_sources_cache = rebuilt_sr
                        snapshot_origin = "primary_response_source_results_rebuild"

        # D) previous_data.results.source_results fallback
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            r3 = previous_data.get("results")
            if isinstance(r3, dict) and isinstance(r3.get("source_results"), list):
                rebuilt_sr2 = []
                for sr in (r3.get("source_results") or []):
                    if not isinstance(sr, dict):
                        continue
                    u = sr.get("source_url") or sr.get("url")
                    ex = sr.get("extracted_numbers")
                    if u and isinstance(ex, list) and ex:
                        rebuilt_sr2.append({
                            "source_url": u,
                            "extracted_numbers": ex,
                            "clean_text": sr.get("clean_text") or sr.get("content") or "",
                            "fingerprint": sr.get("fingerprint"),
                            "fetched_at": sr.get("fetched_at"),
                        })
                rebuilt_sr2.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))
                if rebuilt_sr2:
                    baseline_sources_cache = rebuilt_sr2
                    snapshot_origin = "analysis_source_results_rebuild"
    except Exception:
        pass

    # =====================================================================
    # PATCH SS6C (ADDITIVE): evidence_records fallback for snapshots (evolution-time)
    # =====================================================================
    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            _er = None
            if isinstance(previous_data.get("results"), dict):
                _er = previous_data["results"].get("evidence_records")
            if _er is None:
                _er = previous_data.get("evidence_records")
            _rebuilt = build_baseline_sources_cache_from_evidence_records(_er)
            if isinstance(_rebuilt, list) and _rebuilt:
                baseline_sources_cache = _rebuilt
                snapshot_origin = "evidence_records_rebuild"
    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH ES1C (ADDITIVE): validate snapshot shape & prepare debug metadata
    # =====================================================================
    _snapshot_debug = None
    try:
        _raw_len = int(len(baseline_sources_cache)) if isinstance(baseline_sources_cache, list) else 0
        _kept = []
        if isinstance(baseline_sources_cache, list):
            for s in baseline_sources_cache:
                if not isinstance(s, dict):
                    continue
                u = s.get("source_url") or s.get("url")
                ex = s.get("extracted_numbers")
                if u and isinstance(ex, list):
                    _kept.append(s)
        _kept.sort(key=lambda d: (str(d.get("source_url") or d.get("url") or ""), str(d.get("fingerprint") or "")))
        baseline_sources_cache = _kept
        _snapshot_debug = {
            "origin": snapshot_origin,
            "raw_count": _raw_len,
            "valid_count": int(len(baseline_sources_cache)),
            "example_urls": [x.get("source_url") or x.get("url") for x in (baseline_sources_cache[:3] if isinstance(baseline_sources_cache, list) else [])],
            "prev_keys": sorted(list(previous_data.keys()))[:40] if isinstance(previous_data, dict) else [],
        }
    except Exception:
        pass
    # =====================================================================

    # 3) reconstruct from web_context.scraped_meta (if provided)
    if (not baseline_sources_cache) and isinstance(web_context, dict):
        try:
            scraped_meta = web_context.get("scraped_meta") or {}
            rebuilt = []
            if isinstance(scraped_meta, dict):
                for url, meta in scraped_meta.items():
                    if not isinstance(meta, dict):
                        continue
                    content = meta.get("clean_text") or meta.get("content") or ""
                    fp = meta.get("fingerprint") or _fingerprint(content)
                    if not fp or len(content or "") < 800:
                        continue
                    nums = meta.get("extracted_numbers") or []
                    if not isinstance(nums, list):
                        nums = []
                    rebuilt.append({
                        "url": url,
                        "status": meta.get("status") or "fetched",
                        "status_detail": meta.get("status_detail") or "",
                        "numbers_found": _safe_int(meta.get("numbers_found"), default=len(nums)),
                        "fetched_at": meta.get("fetched_at") or _now(),
                        "fingerprint": fp,
                        "content_type": meta.get("content_type") or "",
                        "extracted_numbers": [
                            {
                                "value": n.get("value"),
                                "unit": n.get("unit"),
                                "raw": n.get("raw"),
                                "context_snippet": (n.get("context_snippet") or n.get("context") or "")[:200],
                                "anchor_hash": n.get("anchor_hash"),
                                "is_junk": n.get("is_junk"),
                                "junk_reason": n.get("junk_reason"),
                                "unit_tag": n.get("unit_tag"),
                                "unit_family": n.get("unit_family"),
                                "base_unit": n.get("base_unit"),
                                "multiplier_to_base": n.get("multiplier_to_base"),
                                "value_norm": n.get("value_norm"),
                                "start_idx": n.get("start_idx"),
                                "end_idx": n.get("end_idx"),
                                "source_url": n.get("source_url") or url,
                            }
                            for n in nums if isinstance(n, dict)
                        ]
                    })
            if rebuilt:
                baseline_sources_cache = rebuilt
                snapshot_origin = "web_context_scraped_meta"
        except Exception:
            pass

    # Also count invalid snapshots for debug (if present)
    invalid_count = 0
    try:
        if isinstance(previous_data, dict):
            r = previous_data.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache_invalid"), list):
                invalid_count = len(r.get("baseline_sources_cache_invalid") or [])
    except Exception:
        invalid_count = 0

    # ---------- Prepare stable default output ----------
    output = {
        "status": "success",
        "message": "",
        "sources_checked": 0,
        "sources_fetched": 0,
        "numbers_extracted_total": 0,
        "stability_score": 0.0,
        "summary": {
            "total_metrics": 0,
            "metrics_found": 0,
            "metrics_increased": 0,
            "metrics_decreased": 0,
            "metrics_unchanged": 0,
        },
        "metric_changes": [],
        "source_results": [],
        "interpretation": "",
        "snapshot_origin": snapshot_origin,
        "valid_snapshot_count": len(baseline_sources_cache or []),
        "invalid_snapshot_count": int(invalid_count),
        "generated_at": _now(),
    }

    # Attach debug flags (rehydration + snapshot_debug)
    try:
        if _prev_rehydrated:
            output["previous_data_rehydrated"] = True
            output["previous_data_full_store_ref"] = _prev_rehydrated_ref
    except Exception:
        pass
    try:
        if isinstance(_snapshot_debug, dict) and _snapshot_debug:
            output["snapshot_debug"] = _snapshot_debug
    except Exception:
        pass

    # =====================================================================
    # PATCH SS6 (ADDITIVE, REQUIRED): last-chance snapshot rehydration
    # =====================================================================
    try:
        if not baseline_sources_cache and isinstance(previous_data, dict):
            _ref = previous_data.get("snapshot_store_ref") or (previous_data.get("results") or {}).get("snapshot_store_ref")
            _hash = previous_data.get("source_snapshot_hash") or (previous_data.get("results") or {}).get("source_snapshot_hash")

            # ============================================================
            # PATCH FIX41I_SS6_STABLE (ADDITIVE): prefer v2/stable snapshot refs & hashes
            # Why:
            # - Analysis now emits stable/v2 snapshot hashes (source_snapshot_hash_v2 / _stable) and
            #   snapshot_store_ref_v2 pointing at the same Snapshots row key.
            # - Evolution must prefer these fields to keep fastpath alignment intact.
            # ============================================================
            try:
                _ref_v2 = previous_data.get("snapshot_store_ref_v2") or previous_data.get("snapshot_store_ref_stable")
                if (not _ref_v2) and isinstance(previous_data.get("results"), dict):
                    _ref_v2 = (previous_data.get("results") or {}).get("snapshot_store_ref_v2") or (previous_data.get("results") or {}).get("snapshot_store_ref_stable")
                if isinstance(_ref_v2, str) and _ref_v2:
                    _ref = _ref_v2  # prefer stable ref
            except Exception:
                pass

            try:
                _hash_stable = previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2")
                if (not _hash_stable) and isinstance(previous_data.get("results"), dict):
                    _hash_stable = (previous_data.get("results") or {}).get("source_snapshot_hash_stable") or (previous_data.get("results") or {}).get("source_snapshot_hash_v2")
                if isinstance(_hash_stable, str) and _hash_stable:
                    _hash = _hash_stable  # prefer stable hash
            except Exception:
                pass
            # ============================================================

            if isinstance(_ref, str) and _ref.startswith("gsheet:"):
                parts = _ref.split(":")
                _ws_title = parts[1] if len(parts) > 1 and parts[1] else "Snapshots"
                _h = parts[2] if len(parts) > 2 else ""
                baseline_sources_cache = load_full_snapshots_from_sheet(_h, worksheet_title=_ws_title) if _h else []
                if baseline_sources_cache:
                    output["snapshot_origin"] = "sheet_snapshot_store_ref"

            if not baseline_sources_cache and isinstance(_hash, str) and _hash:
                baseline_sources_cache = load_full_snapshots_from_sheet(_hash, worksheet_title="Snapshots")
                if baseline_sources_cache:
                    output["snapshot_origin"] = "sheet_source_snapshot_hash"

            if not baseline_sources_cache and isinstance(_ref, str) and _ref and not _ref.startswith("gsheet:"):
                baseline_sources_cache = load_full_snapshots_local(_ref)
                if baseline_sources_cache:
                    output["snapshot_origin"] = "local_snapshot_store_ref"

            if isinstance(baseline_sources_cache, list):
                output["valid_snapshot_count"] = len(baseline_sources_cache)
    except Exception:
        pass
    # =====================================================================

    # If no valid snapshots, return "not_found"
    if not baseline_sources_cache:
        output["status"] = "failed"
        output["message"] = "No valid snapshots available for source-anchored evolution. (No re-fetch / no heuristic matching performed.)"
        output["interpretation"] = "Snapshot-gated: evolution refused to fabricate matches without valid cached source text."
        return output


    # =====================================================================
    # PATCH FIX41AFC10 (ADDITIVE): Fetch + attach injected URL snapshots when injection delta exists
    #
    # Goal:
    # - We already bypass fastpath when injected URL delta exists.
    # - However, evolution can still be snapshot-gated and never actually fetch the injected URL.
    # - This patch performs a **targeted fetch** for injected URLs that are not already present in
    #   the baseline snapshot universe, then appends them to `baseline_sources_cache` so they can
    #   participate in downstream admission / hashing / metric rebuild deterministically.
    #
    # Non-negotiables:
    # - No change to normal (no-injection) behavior
    # - Additive only; no refactors
    # =====================================================================
    try:
        _fix41afc10_injected_norm = []
        try:
            # Robust extraction order (same intent as FIX41AFC9 lineage)
            _fix41afc10_injected_norm = _inj_diag_norm_url_list(
                (web_context or {}).get("extra_urls")
                or (web_context or {}).get("diag_extra_urls_ui")
                or []
            )
        except Exception:
            _fix41afc10_injected_norm = []

        # If UI raw textarea is present, merge its parsed URLs (newline/comma separated)
        try:
            _fix41afc10_ui_raw = str((web_context or {}).get("diag_extra_urls_ui_raw") or (web_context or {}).get("extra_urls_ui_raw") or "")
            if _fix41afc10_ui_raw.strip():
                _fix41afc10_from_raw = _inj_diag_norm_url_list(_fix41afc10_ui_raw)
                if _fix41afc10_from_raw:
                    _fix41afc10_injected_norm = _inj_diag_norm_url_list((_fix41afc10_injected_norm or []) + _fix41afc10_from_raw)
        except Exception:
            pass

        # Only run when there is any injected URL present at all (do not affect normal runs)
        if _fix41afc10_injected_norm:
            # Build baseline URL set from existing snapshots
            _fix41afc10_base_set = set()
            try:
                for _s in (baseline_sources_cache or []):
                    if isinstance(_s, dict) and _s.get("url"):
                        _fix41afc10_base_set.add(_normalize_url(_s.get("url")) or str(_s.get("url")))
            except Exception:
                pass

            _fix41afc10_missing = [u for u in _fix41afc10_injected_norm if (u and (u not in _fix41afc10_base_set))]
            if _fix41afc10_missing:
                _fix41afc10_attempted = []
                _fix41afc10_persisted = []
                for _u in _fix41afc10_missing:
                    try:
                        _fix41afc10_attempted.append(_u)

                        _txt = None
                        try:
                            _txt = fetch_url_content(_u)
                        except Exception:
                            _txt = None

                        _status = "failed"
                        _detail = "no_text"
                        _nums = []
                        if isinstance(_txt, str) and _txt.strip():
                            _status = "success"
                            _detail = "fetched"
                            try:
                                _nums = extract_numbers_with_context(_txt, source_url=_u) or []
                            except Exception:
                                _nums = []
                        # fingerprint (best-effort; used only for determinism / debugging)
                        _fp = ""
                        try:
                            if isinstance(_txt, str) and _txt:
                                _fp = hashlib.sha256(_txt.encode("utf-8", errors="ignore")).hexdigest()
                        except Exception:
                            _fp = ""

                        _snap = {
                            "url": _u,
                            "status": _status,
                            "status_detail": _detail,
                            "numbers_found": len(_nums or []),
                            "fetched_at": _now(),
                            "fingerprint": _fp,
                            "extracted_numbers": _nums or [],
                        }

                        # Append snapshot (even if failed) so lifecycle is visible + deterministic
                        try:
                            if isinstance(baseline_sources_cache, list):
                                baseline_sources_cache.append(_snap)
                                _fix41afc10_persisted.append(_u)
                        except Exception:
                            pass
                    except Exception:
                        # never break evolution on injected fetch attempts
                        pass

                # Emit explicit debug for closure visibility
                try:
                    if isinstance(output.get("debug"), dict):
                        output.setdefault("debug", {})
                        output["debug"].setdefault("fix35", {})
                        output["debug"]["fix35"]["injected_fetch_attempted_count"] = len(_fix41afc10_attempted)
                        output["debug"]["fix35"]["injected_fetch_attempted"] = _fix41afc10_attempted
                        output["debug"]["fix35"]["injected_fetch_snapshot_appended_count"] = len(_fix41afc10_persisted)
                        output["debug"]["fix35"]["injected_fetch_snapshot_appended"] = _fix41afc10_persisted
                except Exception:
                    pass
    except Exception:
        pass

    # ---------- Use your existing deterministic metric diff helper ----------
    prev_response = (previous_data or {}).get("primary_response", {}) or {}

    # =====================================================================
    # PATCH HF6 (ADDITIVE): tolerate previous_data being the primary_response itself
    # =====================================================================
    try:
        if (not isinstance(prev_response, dict) or not prev_response) and isinstance(previous_data, dict):
            if isinstance(previous_data.get("primary_metrics_canonical"), dict) or isinstance(previous_data.get("metric_schema_frozen"), dict):
                prev_response = previous_data
    except Exception:
        pass
    # =====================================================================

    prev_metrics = prev_response.get("primary_metrics_canonical") or prev_response.get("primary_metrics") or {}

    # ============================================================
    # PATCH CSR_INPUTS1 (ADDITIVE): normalize prev schema/anchors/canon
    # (safe alias for prior `prev_analysis` usage)
    # ============================================================
    prev_analysis = previous_data  # PATCH CSR_INPUTS1_ALIAS (ADDITIVE)
    try:
        prev_schema = _first_present(prev_analysis, [
            ("metric_schema_frozen",),
            ("primary_response", "metric_schema_frozen"),
            ("results", "metric_schema_frozen"),
            ("results", "primary_response", "metric_schema_frozen"),
        ], default=None)

        prev_canon = _first_present(prev_analysis, [
            ("primary_metrics_canonical",),
            ("primary_response", "primary_metrics_canonical"),
            ("results", "primary_metrics_canonical"),
            ("results", "primary_response", "primary_metrics_canonical"),
        ], default=None)

        prev_anchors = _first_present(prev_analysis, [
            ("metric_anchors",),
            ("primary_response", "metric_anchors"),
            ("results", "metric_anchors"),
            ("results", "primary_response", "metric_anchors"),
        ], default=None)

        if (not isinstance(prev_schema, dict) or not prev_schema) and isinstance(prev_canon, dict) and prev_canon:
            try:
                fn = globals().get("freeze_metric_schema")
                if callable(fn):
                    prev_schema = fn(prev_canon)
            except Exception:
                pass
    except Exception:
        pass
    # ============================================================

    # Ensure schema/anchors are available inside prev_response (additive copies)
    try:
        if isinstance(prev_response, dict) and not isinstance(prev_response.get("metric_schema_frozen"), dict):
            if isinstance(previous_data.get("metric_schema_frozen"), dict):
                prev_response["metric_schema_frozen"] = previous_data.get("metric_schema_frozen")
    except Exception:
        pass
    try:
        if isinstance(prev_response, dict) and not isinstance(prev_response.get("metric_anchors"), dict):
            if isinstance(previous_data.get("metric_anchors"), dict):
                prev_response["metric_anchors"] = previous_data.get("metric_anchors")
    except Exception:
        pass

    # Build a minimal current metrics dict from snapshots:
    current_metrics = {}

    # Prefer metric_anchors to rebuild current_metrics (snapshot-gated)
    def _get_metric_anchors(prev: dict) -> dict:
        if not isinstance(prev, dict):
            return {}
        a = prev.get("metric_anchors")
        if isinstance(a, dict) and a:
            return a
        pr = prev.get("primary_response")
        if isinstance(pr, dict):
            a2 = pr.get("metric_anchors")
            if isinstance(a2, dict) and a2:
                return a2
        res = prev.get("results")
        if isinstance(res, dict):
            a3 = res.get("metric_anchors")
            if isinstance(a3, dict) and a3:
                return a3
        return {}

    def _canonicalize_candidate(n: dict) -> dict:
        try:
            fn = globals().get("canonicalize_numeric_candidate")
            if callable(fn):
                return fn(dict(n))
        except Exception:
            pass
        return dict(n)

    def _build_anchor_to_candidate_map(snapshots: list) -> dict:
        m = {}
        for sr in snapshots or []:
            if not isinstance(sr, dict):
                continue
            for n in (sr.get("extracted_numbers") or []):
                if not isinstance(n, dict):
                    continue
                nn = _canonicalize_candidate(n)
                ah = nn.get("anchor_hash")
                if not ah:
                    continue
                if ah not in m:
                    m[ah] = nn
        return m

    try:
        metric_anchors = _get_metric_anchors(previous_data)
        anchor_to_candidate = _build_anchor_to_candidate_map(baseline_sources_cache)

        if isinstance(metric_anchors, dict) and metric_anchors:
            for ckey, a in metric_anchors.items():
                if not isinstance(a, dict):
                    continue
                ah = a.get("anchor_hash") or a.get("anchor")
                if not ah:
                    continue
                cand = anchor_to_candidate.get(ah)
                if not isinstance(cand, dict):
                    continue

                base = prev_metrics.get(ckey) if isinstance(prev_metrics, dict) else None
                out_row = dict(base) if isinstance(base, dict) else {}
                out_row.update({
                    "canonical_key": ckey,
                    "anchor_hash": ah,
                    "anchor_used": True,
                    "anchor_confidence": a.get("anchor_confidence"),
                    "source_url": cand.get("source_url") or a.get("source_url"),
                    "raw": cand.get("raw"),
                    "value": cand.get("value"),
                    "unit": cand.get("unit"),
                    "value_norm": cand.get("value_norm"),
                    "context_snippet": cand.get("context_snippet") or cand.get("context") or "",
                    "candidate_id": cand.get("candidate_id") or a.get("candidate_id"),
                    "fix36_origin": "anchor_mapping",  # PATCH FIX36 (ADD): per-metric provenance
                })
                current_metrics[ckey] = out_row
    except Exception:
        pass

    # ============================================================
    # PATCH FIX36 (ADDITIVE): set current_metrics_origin when anchor mapping populated any metrics
    # ============================================================
    try:
        if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
            if output["debug"]["fix35"].get("current_metrics_origin") in (None, "", "unknown"):
                # If any current metric was filled via anchor mapping, stamp origin
                if isinstance(current_metrics, dict) and any(isinstance(v, dict) and v.get("anchor_used") for v in current_metrics.values()):
                    output["debug"]["fix35"]["current_metrics_origin"] = "anchor_mapping"
    except Exception:
        pass
    # ============================================================

    # Rebuild fallback only if anchors didn't produce metrics
    if not isinstance(current_metrics, dict) or not current_metrics:
        try:
            # =====================================================================
            # PATCH FIX41AFC14 (ADDITIVE): Augment baseline_sources_cache with injected URL delta BEFORE schema-only rebuild
            #
            # Problem:
            # - With fastpath bypassed, evolution may fall back to schema-only rebuild *without* running any
            #   fetch cycle. In that case, injected URLs never enter baseline_sources_cache, so rebuild cannot
            #   see them and injection remains inert (attempted=0, persisted=0).
            #
            # Goal:
            # - If injected URLs are present AND introduce a delta vs the current snapshot universe,
            #   run fetch_web_context() once (normal mode) with force_admit/force_scrape enabled,
            #   then merge any successful injected snapshots into baseline_sources_cache, and only then
            #   call the rebuild function.
            #
            # Safety:
            # - No effect when no injection or no delta.
            # - Does NOT alter hashing logic; it only ensures the snapshot pool reflects successfully fetched injected sources.
            # - Never raises; on any failure, proceeds with the original baseline_sources_cache.
            # =====================================================================
            try:
                _fx14_wc = web_context if isinstance(web_context, dict) else {}
                _fx14_extra_raw = []
                if isinstance(_fx14_wc.get("extra_urls"), (list, tuple)) and _fx14_wc.get("extra_urls"):
                    _fx14_extra_raw = list(_fx14_wc.get("extra_urls") or [])
                elif isinstance(_fx14_wc.get("diag_extra_urls_ui"), (list, tuple)) and _fx14_wc.get("diag_extra_urls_ui"):
                    _fx14_extra_raw = list(_fx14_wc.get("diag_extra_urls_ui") or [])
                elif isinstance(_fx14_wc.get("diag_extra_urls_ui_raw"), str) and (_fx14_wc.get("diag_extra_urls_ui_raw") or "").strip():
                    _raw = str(_fx14_wc.get("diag_extra_urls_ui_raw") or "")
                    _parts = []
                    for _line in _raw.splitlines():
                        _line = (_line or "").strip()
                        if not _line:
                            continue
                        for _p in _line.split(","):
                            _p = (_p or "").strip()
                            if _p:
                                _parts.append(_p)
                    _fx14_extra_raw = _parts

                _fx14_inj = _inj_diag_norm_url_list(_fx14_extra_raw) if _fx14_extra_raw else []
                _fx14_base_urls = []
                if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                    for _r in baseline_sources_cache:
                        if isinstance(_r, dict) and isinstance(_r.get("source_url"), str) and _r.get("source_url"):
                            _fx14_base_urls.append(_r.get("source_url"))
                _fx14_base_set = set(_inj_diag_norm_url_list(_fx14_base_urls)) if _fx14_base_urls else set()
                _fx14_delta = sorted(list(set(_fx14_inj) - _fx14_base_set)) if _fx14_inj else []

                if _fx14_delta:
                    _fx14_q = str((prev_response or {}).get("question") or (previous_data or {}).get("question") or "").strip()
                    _fx14_prev_snap = baseline_sources_cache
                    _fx14_fwc = fetch_web_context(
                        _fx14_q or "evolution_injection_fetch_pre_rebuild",
                        num_sources=int(min(12, max(1, len(_fx14_base_set) + len(_fx14_inj)))),
                        fallback_mode=True,
                        fallback_urls=list(_inj_diag_norm_url_list(_fx14_base_urls)),
                        existing_snapshots=_fx14_prev_snap,
                        extra_urls=list(_fx14_inj),
                        diag_run_id=str((_fx14_wc or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                        diag_extra_urls_ui_raw=(_fx14_wc or {}).get("diag_extra_urls_ui_raw"),
                        identity_only=False,
                        force_scrape_extra_urls=True,
                        force_admit_extra_urls=True,
                    ) or {}

                    try:
                        if isinstance(_fx14_wc, dict):
                            _fx14_wc["evolution_calls_fetch_web_context"] = True
                    except Exception:
                        pass

                    _fx14_sm = _fx14_fwc.get("scraped_meta")
                    _fx14_bsc_new = None
                    try:
                        _fn_bsc = globals().get("_fix24_baseline_sources_cache_from_scraped_meta")
                        if callable(_fn_bsc) and isinstance(_fx14_sm, dict):
                            _fx14_bsc_new = _fn_bsc(_fx14_sm)
                    except Exception:
                        _fx14_bsc_new = None

                    if isinstance(_fx14_bsc_new, list) and _fx14_bsc_new:
                        # Merge: keep original order for existing snapshots, append new unique ones
                        _merged = list(baseline_sources_cache or [])
                        _seen = set(_inj_diag_norm_url_list(_fx14_base_urls))
                        _added = []
                        for _row in _fx14_bsc_new:
                            if not isinstance(_row, dict):
                                continue
                            _u = _row.get("source_url") or ""
                            _nu = (_inj_diag_norm_url(_u) if isinstance(_u, str) else "")
                            if _nu and _nu not in _seen:
                                _merged.append(_row)
                                _seen.add(_nu)
                                _added.append(_nu)
                        if _added:
                            baseline_sources_cache = _merged

                        # Debug
                        try:
                            output.setdefault("debug", {})
                            if isinstance(output.get("debug"), dict):
                                output["debug"].setdefault("fix41afc14", {})
                                if isinstance(output["debug"].get("fix41afc14"), dict):
                                    output["debug"]["fix41afc14"].update({
                                        "inj_delta_count": int(len(_fx14_delta)),
                                        "inj_delta": list(_fx14_delta),
                                        "merged_added_count": int(len(_added)),
                                        "merged_added": list(_added),
                                        "baseline_sources_cache_count_after_merge": int(len(baseline_sources_cache or [])),
                                    })
                        except Exception:
                            pass
            except Exception:
                pass
            # =====================================================================
            fn_rebuild = globals().get("rebuild_metrics_from_snapshots_schema_only") or globals().get("rebuild_metrics_from_snapshots")
            if callable(fn_rebuild):
                current_metrics = fn_rebuild(prev_response, baseline_sources_cache, web_context=web_context)
                # PATCH FIX36 (ADD): provenance for rebuild fallback
                try:
                    if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                        if output["debug"]["fix35"].get("current_metrics_origin") in (None, "", "unknown"):
                            output["debug"]["fix35"]["current_metrics_origin"] = "schema_only_rebuild"
                except Exception:
                    pass
                # =====================================================================
                # PATCH FIX41AFC18 (ADDITIVE): schema-preserve guard on rebuild when injected URLs are present
                # Intent:
                #   - Keep drift-0 for existing metrics even when an injected URL forces rebuild
                #   - Only allow a rebuilt metric to replace the previous metric if it carries
                #     evidence anchors and passes basic schema/unit sanity checks
                #   - Never changes fastpath logic; only post-rebuild metric selection safety
                # =====================================================================
                try:
                    _fix41afc18_prev = prev_response.get("primary_metrics_canonical") if isinstance(prev_response, dict) else None
                    _fix41afc18_schema = prev_response.get("metric_schema_frozen") if isinstance(prev_response, dict) else None

                    # Recover injected-delta set (normalized) from earlier patches, if available
                    _fix41afc18_inj_delta = []
                    try:
                        if isinstance(output.get("debug"), dict):
                            _d15 = output["debug"].get("fix41afc15") or output["debug"].get("fix41afc16") or output["debug"].get("fix41afc14") or {}
                            if isinstance(_d15, dict):
                                _fix41afc18_inj_delta = list(_d15.get("inj_delta") or [])
                    except Exception:
                        _fix41afc18_inj_delta = []
                    _fix41afc18_inj_set = set()
                    try:
                        _norm_fn = globals().get("_inj_diag_norm_url_list")
                        if callable(_norm_fn) and _fix41afc18_inj_delta:
                            _fix41afc18_inj_set = set(_norm_fn(_fix41afc18_inj_delta))
                        else:
                            _fix41afc18_inj_set = set([str(u).strip() for u in (_fix41afc18_inj_delta or []) if str(u).strip()])
                    except Exception:
                        _fix41afc18_inj_set = set([str(u).strip() for u in (_fix41afc18_inj_delta or []) if str(u).strip()])

                    def _fix41afc18_has_evidence(m: dict) -> bool:
                        if not isinstance(m, dict):
                            return False
                        ev = m.get("evidence")
                        if isinstance(ev, list) and len(ev) > 0:
                            return True
                        # fallback: anchor_hash present
                        ah = m.get("anchor_hash") or m.get("anchorHash") or m.get("anchor")
                        return bool(ah and str(ah) not in ("None", "none", ""))

                    def _fix41afc18_unit_ok(prev_m: dict, cur_m: dict) -> bool:
                        try:
                            # Prefer schema unit_family/unit_tag if available
                            ck = str(cur_m.get("canonical_key") or "")
                            schema_row = _fix41afc18_schema.get(ck) if isinstance(_fix41afc18_schema, dict) else None
                            if isinstance(schema_row, dict):
                                exp_fam = str(schema_row.get("unit_family") or "")
                                exp_tag = str(schema_row.get("unit_tag") or schema_row.get("unit") or "")
                                cur_fam = str(cur_m.get("unit_family") or "")
                                cur_tag = str(cur_m.get("unit_tag") or cur_m.get("unit") or "")
                                # If schema expects a family/tag, require match when present
                                if exp_fam and cur_fam and exp_fam != cur_fam:
                                    return False
                                if exp_tag and cur_tag and exp_tag != cur_tag:
                                    return False
                            # Fallback: if both prev and cur have unit_family, require equality
                            pf = str(prev_m.get("unit_family") or "")
                            cf = str(cur_m.get("unit_family") or "")
                            if pf and cf and pf != cf:
                                return False
                            return True
                        except Exception:
                            return True

                    def _fix41afc18_from_injected_source(cur_m: dict) -> bool:
                        try:
                            su = str(cur_m.get("source_url") or cur_m.get("source") or "")
                            if not su:
                                return False
                            # normalize by simple strip only (avoid heavy deps)
                            su = su.strip()
                            # if we have inj set, treat any exact/startswith match as injected
                            for iu in _fix41afc18_inj_set:
                                if not iu:
                                    continue
                                if su == iu or su.startswith(iu) or iu.startswith(su):
                                    return True
                            return False
                        except Exception:
                            return False

                    def _fix41afc18_value(prev_m: dict):
                        for k in ("value_norm", "value"):
                            v = prev_m.get(k) if isinstance(prev_m, dict) else None
                            if isinstance(v, (int, float)):
                                return float(v)
                        return None

                    _fix41afc18_replaced = 0
                    _fix41afc18_preserved = 0
                    _fix41afc18_added = 0
                    _fix41afc18_notes = []

                    # Only apply guard when injected delta is present (no-change case remains locked)
                    if _fix41afc18_inj_set and isinstance(_fix41afc18_prev, dict) and isinstance(current_metrics, dict):
                        for _ck, _prev_m in _fix41afc18_prev.items():
                            if not isinstance(_ck, str) or not _ck:
                                continue
                            _cur_m = current_metrics.get(_ck)
                            if not isinstance(_cur_m, dict):
                                # If rebuild dropped a metric, preserve previous
                                current_metrics[_ck] = _prev_m
                                _fix41afc18_preserved += 1
                                _fix41afc18_notes.append({"canonical_key": _ck, "action": "preserve_prev_metric_missing_in_rebuild"})
                                continue

                            # If rebuild metric lacks evidence, preserve previous (schema-driven drift lock)
                            if not _fix41afc18_has_evidence(_cur_m):
                                current_metrics[_ck] = _prev_m
                                _fix41afc18_preserved += 1
                                _fix41afc18_notes.append({"canonical_key": _ck, "action": "preserve_prev_no_evidence"})
                                continue

                            # If unit sanity fails, preserve previous
                            if not _fix41afc18_unit_ok(_prev_m, _cur_m):
                                current_metrics[_ck] = _prev_m
                                _fix41afc18_preserved += 1
                                _fix41afc18_notes.append({"canonical_key": _ck, "action": "preserve_prev_unit_mismatch"})
                                continue

                            # If the replacement comes only from injected source and is wildly different, preserve prev
                            try:
                                pv = _fix41afc18_value(_prev_m)
                                cv = _fix41afc18_value(_cur_m)
                                if pv is not None and cv is not None and pv != 0:
                                    rel = abs(cv - pv) / max(abs(pv), 1e-9)
                                    if _fix41afc18_from_injected_source(_cur_m) and rel >= 0.50:
                                        current_metrics[_ck] = _prev_m
                                        _fix41afc18_preserved += 1
                                        _fix41afc18_notes.append({"canonical_key": _ck, "action": "preserve_prev_suspicious_injected_delta", "prev": pv, "cur": cv, "rel": rel})
                                        continue
                            except Exception:
                                pass

                            # Otherwise accept rebuilt metric (explicit replace)
                            _fix41afc18_replaced += 1

                        # If rebuild produced new metrics not in schema, keep them but track (non-breaking)
                        for _ck2, _cur_m2 in list(current_metrics.items()):
                            if not isinstance(_ck2, str) or not _ck2:
                                continue
                            if _ck2 not in _fix41afc18_prev:
                                _fix41afc18_added += 1

                    # Emit debug for traceability
                    try:
                        if isinstance(output.get("debug"), dict):
                            output["debug"].setdefault("fix41afc18", {})
                            if isinstance(output["debug"].get("fix41afc18"), dict):
                                output["debug"]["fix41afc18"].update({
                                    "inj_delta_present": bool(_fix41afc18_inj_set),
                                    "inj_delta_count": int(len(_fix41afc18_inj_set)),
                                    "rebuild_metrics_replaced_count": int(_fix41afc18_replaced),
                                    "rebuild_metrics_preserved_count": int(_fix41afc18_preserved),
                                    "rebuild_metrics_added_count": int(_fix41afc18_added),
                                    "notes_sample": _fix41afc18_notes[:10],
                                })
                    except Exception:
                        pass
                except Exception:
                    pass
                # =====================================================================
        except Exception:
            current_metrics = {}

    if not isinstance(current_metrics, dict) or not current_metrics:
        output["status"] = "failed"
        output["message"] = "Valid snapshots exist, but metric rebuild returned empty. No re-fetch / no heuristic matching performed."
        output["source_results"] = baseline_sources_cache[:50]
        output["sources_checked"] = len(baseline_sources_cache)
        output["sources_fetched"] = len(baseline_sources_cache)
        output["interpretation"] = "Snapshot-ready but metric rebuild not implemented or returned empty; add/verify rebuild_metrics_from_snapshots* hooks."
        return output

    # Diff using existing diff helper if present
    metric_changes = []
    try:
        fn_diff = globals().get("diff_metrics_by_name")
        if callable(fn_diff):
            # ============================================================
            # PATCH FIX38 (ADDITIVE): ensure schema is wired into diff layer + emit lookup provenance
            # - Some runs showed schema_unit_family=None in diff rows, preventing unit-required mismatch logic.
            # - We attach metric_schema_frozen to prev_response if missing, and post-process diff rows
            #   to populate schema_unit_family + schema_lookup_source.
            # ============================================================
            _fix38_schema = None
            _fix38_schema_src = ""
            try:
                # Prefer schema already on prev_response
                if isinstance(prev_response, dict) and isinstance(prev_response.get("metric_schema_frozen"), dict) and prev_response.get("metric_schema_frozen"):
                    _fix38_schema = prev_response.get("metric_schema_frozen")
                    _fix38_schema_src = "prev_response.metric_schema_frozen"
                # Else try common containers on previous_data
                elif isinstance(previous_data, dict):
                    if isinstance(previous_data.get("metric_schema_frozen"), dict) and previous_data.get("metric_schema_frozen"):
                        _fix38_schema = previous_data.get("metric_schema_frozen")
                        _fix38_schema_src = "previous_data.metric_schema_frozen"
                    elif isinstance(previous_data.get("primary_response"), dict) and isinstance(previous_data["primary_response"].get("metric_schema_frozen"), dict) and previous_data["primary_response"].get("metric_schema_frozen"):
                        _fix38_schema = previous_data["primary_response"].get("metric_schema_frozen")
                        _fix38_schema_src = "previous_data.primary_response.metric_schema_frozen"
                # Attach if missing
                if isinstance(_fix38_schema, dict) and _fix38_schema and isinstance(prev_response, dict):
                    if not (isinstance(prev_response.get("metric_schema_frozen"), dict) and prev_response.get("metric_schema_frozen")):
                        prev_response["metric_schema_frozen"] = _fix38_schema
            except Exception:
                pass

            # Record schema wiring debug
            try:
                if isinstance(output.get("debug"), dict):
                    output["debug"].setdefault("fix38", {})
                    output["debug"]["fix38"]["schema_attached"] = bool(isinstance(_fix38_schema, dict) and _fix38_schema)
                    output["debug"]["fix38"]["schema_source"] = _fix38_schema_src
            except Exception:
                pass

            cur_resp_for_diff = {"primary_metrics_canonical": current_metrics}
            # ============================================================
            # ============================================================
            # PATCH V34D (ADDITIVE): provide metric_anchors for current side so anchor_hash join can work
            # - compute_source_anchored_diff previously passed only primary_metrics_canonical to diff layer
            # - v34 join requires cur_response.metric_anchors[ckey].anchor_hash (secondary join)
            # - We synthesize metric_anchors from rebuilt canonical metrics when missing.
            # Safety:
            # - Deterministic, inference-free. Uses only anchor_hash already attached to rebuilt canonical metrics.
            # ============================================================
            try:
                if isinstance(cur_resp_for_diff, dict) and "metric_anchors" not in cur_resp_for_diff:
                    _v34d_cur_anchors = {}
                    if isinstance(current_metrics, dict):
                        for _ck_v34d, _m_v34d in current_metrics.items():
                            if not isinstance(_ck_v34d, str) or not _ck_v34d:
                                continue
                            if not isinstance(_m_v34d, dict):
                                continue
                            _ah = _m_v34d.get("anchor_hash") or _m_v34d.get("anchor") or _m_v34d.get("anchor_id") or None
                            if isinstance(_ah, str) and _ah:
                                _v34d_cur_anchors[_ck_v34d] = {"anchor_hash": _ah}
                    if _v34d_cur_anchors:
                        cur_resp_for_diff["metric_anchors"] = _v34d_cur_anchors
            except Exception:
                pass
            # ============================================================
            metric_changes, unchanged, increased, decreased, found = fn_diff(prev_response, cur_resp_for_diff)

            # ============================================================
            # PATCH V34F (ADDITIVE): force-run strict anchor-hash join diff and preserve diagnostics
            # WHY:
            # - Some deployments still route through a legacy diff function (or strip diagnostics),
            #   leaving Current blank/N/A even when a stable anchor match exists.
            # - We deterministically re-run the v34 anchor-join diff against the exact same
            #   (prev_response, cur_resp_for_diff) payload, then adopt its output if it yields
            #   any per-row v34 diagnostics.
            # SAFETY:
            # - Render/diff layer only. No fetch, no inference, no hashing changes.
            # ============================================================
            try:
                _v34f_fn = (
                    globals().get("diff_metrics_by_name_FIX41_V34C_UNWRAP")
                    or globals().get("diff_metrics_by_name_FIX41_V34_ANCHOR_JOIN")
                    or globals().get("diff_metrics_by_name_FIX41_V34_ANCHOR_JOIN".lower())
                )
                if callable(_v34f_fn):
                    _mc_v34f, _u_v34f, _i_v34f, _d_v34f, _f_v34f = _v34f_fn(prev_response, cur_resp_for_diff)
                    # Adopt only if it looks like the v34 pipeline (has diag.diff_join_trace_v1 anywhere)
                    _has_v34_diag = False
                    if isinstance(_mc_v34f, list):
                        for _r_v34f in _mc_v34f:
                            if isinstance(_r_v34f, dict) and isinstance(_r_v34f.get("diag"), dict) and _r_v34f["diag"].get("diff_join_trace_v1"):
                                _has_v34_diag = True
                                break
                    if _has_v34_diag:
                        metric_changes, unchanged, increased, decreased, found = _mc_v34f, _u_v34f, _i_v34f, _d_v34f, _f_v34f
                        # Surface top-level join summary debug (emitted on cur_resp_for_diff by v34)
                        try:
                            _v34f_dbg = None
                            if isinstance(cur_resp_for_diff, dict) and isinstance(cur_resp_for_diff.get("debug"), dict):
                                _v34f_dbg = cur_resp_for_diff["debug"].get("diff_join_anchor_v34")
                            if isinstance(_v34f_dbg, dict) and isinstance(output.get("debug"), dict):
                                output["debug"]["diff_join_anchor_v34"] = _v34f_dbg
                                output["debug"]["diff_join_anchor_v34"]["v34f_forced_rerun"] = True
                        except Exception:
                            pass
            except Exception:
                pass
            # ============================================================
            # ============================================================
            # ============================================================
            # PATCH V34D (ADDITIVE): merge diff join summary debug into evolution output debug
            # - v34 join emits debug.diff_join_anchor_v34 on the cur_response object passed to diff layer
            # - cur_resp_for_diff is local; we surface it to output.debug for auditability.
            # ============================================================
            try:
                _v34d_dbg = None
                if isinstance(cur_resp_for_diff, dict) and isinstance(cur_resp_for_diff.get("debug"), dict):
                    _v34d_dbg = cur_resp_for_diff["debug"].get("diff_join_anchor_v34")
                if isinstance(_v34d_dbg, dict) and isinstance(output.get("debug"), dict):
                    output["debug"]["diff_join_anchor_v34"] = _v34d_dbg
            except Exception:
                pass
            # ============================================================

            # PATCH FIX38 (ADDITIVE): populate schema_unit_family on diff rows (if missing)
            # ============================================================
            try:
                if isinstance(metric_changes, list) and metric_changes and isinstance(_fix38_schema, dict) and _fix38_schema:
                    bad = {}
                    for row in metric_changes:
                        if not isinstance(row, dict):
                            continue
                        ckey = row.get("canonical_key") or row.get("canonical") or row.get("key") or ""
                        if not ckey:
                            continue
                        if row.get("schema_unit_family") in (None, "", "None"):
                            md = _fix38_schema.get(ckey) if isinstance(_fix38_schema.get(ckey), dict) else None
                            uf = ""
                            if isinstance(md, dict):
                                uf = (md.get("unit_family") or md.get("unit") or "").strip()
                            if uf:
                                row["schema_unit_family"] = uf
                                row["fix38_schema_lookup"] = _fix38_schema_src or "attached_schema"
                        # Track any remaining year-like current with missing unit-family for diagnosis
                        try:
                            cv = row.get("cur_value_norm")
                            cu = (row.get("cur_unit_cmp") or "").strip()
                            if isinstance(cv, (int, float)) and 1900 <= float(cv) <= 2100 and not cu:
                                bad[ckey] = {"cur_value_norm": cv, "cur_unit_cmp": cu, "schema_unit_family": row.get("schema_unit_family")}
                        except Exception:
                            pass
                    if bad:
                        output.setdefault("debug", {}).setdefault("fix38", {})["bad_year_currents_sample"] = dict(list(bad.items())[:10])
            except Exception:
                pass
        else:
            metric_changes, unchanged, increased, decreased, found = ([], 0, 0, 0, 0)
    except Exception:
        metric_changes, unchanged, increased, decreased, found = ([], 0, 0, 0, 0)

    # ============================================================
    # PATCH FIX36 (ADDITIVE): attach per-row provenance from current_metrics
    # - Adds row['fix36_origin'] when available so we can see which path produced 'Current'
    # ============================================================
    try:
        if isinstance(metric_changes, list) and isinstance(current_metrics, dict):
            for r in metric_changes:
                if not isinstance(r, dict):
                    continue
                ck = r.get("canonical_key") or r.get("canonical") or ""
                if ck and isinstance(current_metrics.get(ck), dict):
                    if current_metrics[ck].get("fix36_origin"):
                        r["fix36_origin"] = current_metrics[ck].get("fix36_origin")
    except Exception:
        pass
    # ============================================================

    output["metric_changes"] = metric_changes or []
    # =====================================================================
    # PATCH PH2B_S3 (ADDITIVE): Attach selector breadcrumb + current fields per diff row
    # - Makes Phase 2B runbook checks (#3/#7/#9) a fast scan.
    # - Does not alter selection; only exposes chosen current metadata.
    # =====================================================================
    try:
        if isinstance(output.get("metric_changes"), list) and isinstance(current_metrics, dict):
            # local normalizer for safe comparisons in dashboards
            def _ph2b_norm_url(_u: str) -> str:
                try:
                    _u = (_u or "").strip()
                    if not _u:
                        return ""
                    _u = _u.replace("http://", "https://")
                    # drop trailing slash
                    while _u.endswith("/") and len(_u) > 8:
                        _u = _u[:-1]
                    # remove www.
                    _u = _u.replace("://www.", "://")
                    return _u
                except Exception:
                    return (_u or "").strip()

            for _row in output["metric_changes"]:
                if not isinstance(_row, dict):
                    continue
                _ck = _row.get("canonical_key") or _row.get("canonical_id") or ""
                if not _ck:
                    continue
                _cur = current_metrics.get(_ck)
                if not isinstance(_cur, dict):
                    continue

                if "selector_used" not in _row:
                    _row["selector_used"] = _cur.get("selector_used") or ""
                if not _row.get("cur_source_url"):
                    _row["cur_source_url"] = _cur.get("source_url") or ""
                if "cur_source_url_norm" not in _row:
                    _row["cur_source_url_norm"] = _ph2b_norm_url(_row.get("cur_source_url") or "")
                if "cur_value_norm" not in _row and _cur.get("value_norm") is not None:
                    _row["cur_value_norm"] = _cur.get("value_norm")
                if "cur_unit_cmp" not in _row:
                    _row["cur_unit_cmp"] = _cur.get("unit") or _cur.get("unit_tag") or ""
                if "anchor_used" not in _row and isinstance(_cur.get("anchor_used"), bool):
                    _row["anchor_used"] = _cur.get("anchor_used")
    except Exception:
        pass
    # =====================================================================

    output["summary"]["total_metrics"] = len(output["metric_changes"])
    output["summary"]["metrics_found"] = int(found or 0)
    output["summary"]["metrics_increased"] = int(increased or 0)
    output["summary"]["metrics_decreased"] = int(decreased or 0)
    output["summary"]["metrics_unchanged"] = int(unchanged or 0)

    total = max(1, len(output["metric_changes"]))
    output["stability_score"] = (output["summary"]["metrics_unchanged"] / total) * 100.0

    output["source_results"] = baseline_sources_cache[:50]
    output["sources_checked"] = len(baseline_sources_cache)
    output["sources_fetched"] = len(baseline_sources_cache)

    try:
        total_nums = 0
        for sr in baseline_sources_cache or []:
            if isinstance(sr, dict) and isinstance(sr.get("extracted_numbers"), list):
                total_nums += len(sr.get("extracted_numbers") or [])
        output["numbers_extracted_total"] = int(total_nums)
    except Exception:
        pass

    output["message"] = "Source-anchored evolution completed (snapshot-gated, analysis-aligned)."
    output["interpretation"] = "Evolution used cached source snapshots only; no brute-force candidate harvesting."

    return output


# ===================== PATCH RMS_CORE1 (ADDITIVE) =====================




# =====================================================================
# PATCH WRAP_COMPUTE_SOURCE_ANCHORED_DIFF (ADDITIVE): preserve original as compute_source_anchored_diff_BASE
# and define the patched version below.
# =====================================================================

def compute_source_anchored_diff(previous_data: dict, web_context: dict = None) -> dict:
    """
    Tight source-anchored evolution:
      - Prefer snapshots from analysis (baseline_sources_cache)
      - Optionally reconstruct snapshots from web_context.scraped_meta
      - If no valid snapshots: return not_found (no heuristic junk)

    Always returns a dict.
    """
    import re
    from datetime import datetime, timezone

    def _now():
        return datetime.now(timezone.utc).isoformat()

    def _safe_int(x, default=0):
        try:
            return int(x)
        except Exception:
            return default

    def _fingerprint(text: str):
        try:
            fn = globals().get("fingerprint_text")
            if callable(fn):
                return fn(text or "")
        except Exception:
            pass
        try:
            return fingerprint_text(text or "")
        except Exception:
            return None

    # ============================================================
    # PATCH CSR_UNWRAP1 (ADDITIVE): robust nested retrieval helpers
    # Why:
    # - Some runs store rebuild essentials under primary_response or results.primary_response
    # - Evolution may look only at top-level keys, causing schema=0 / anchors=0
    # ============================================================
    def _get_nested(d, path, default=None):
        try:
            x = d
            for k in path:
                if not isinstance(x, dict):
                    return default
                x = x.get(k)
            return x if x is not None else default
        except Exception:
            return default

    def _first_present(d, paths, default=None):
        for p in paths:
            v = _get_nested(d, p, None)
            if v is not None:
                return v
        return default
    # ============================================================

    # =====================================================================
    # PATCH FIX41U (ADDITIVE): Evolution-side injected-URL diag prewire + replay visibility
    # Objective:
    # - Ensure compute_source_anchored_diff can ALWAYS populate web_context.diag_injected_urls
    #   even when the caller only supplies:
    #     * web_context["extra_urls"]
    #     * web_context["diag_extra_urls_ui_raw"]
    #     * web_context["diag_run_id"]
    # - Additionally, if the evolution UI did NOT supply extra_urls, record what the baseline
    #   analysis run had (if any) as "replayed_from_analysis_norm" for diagnostics only.
    # Safety:
    # - Purely additive diagnostics. Does NOT alter fastpath logic or hashing inputs.
    # =====================================================================
    def _fix41u_extract_injected_from_prev(prev: dict) -> dict:
        try:
            if not isinstance(prev, dict):
                return {}
            cand_paths = [
                ["results","debug","inj_trace_v1"],
                ["primary_response","results","debug","inj_trace_v1"],
                ["results","primary_response","results","debug","inj_trace_v1"],
                ["debug","inj_trace_v1"],
            ]
            for p in cand_paths:
                v = _get_nested(prev, p, None)
                if isinstance(v, dict) and v:
                    return v
        except Exception:
            pass
        return {}

    try:
        if web_context is None or not isinstance(web_context, dict):
            web_context = {}

        # Pull what the caller provided (Evolution UI should pass these)
        _fix41u_ui_raw = ""
        try:
            _fix41u_ui_raw = str(web_context.get("diag_extra_urls_ui_raw") or web_context.get("extra_urls_ui_raw") or "")
        except Exception:
            _fix41u_ui_raw = ""

        _fix41u_extra_urls = []
        try:
            _fix41u_extra_urls = _inj_diag_norm_url_list(web_context.get("extra_urls") or [])
        except Exception:
            _fix41u_extra_urls = []

        # If Evolution UI did not supply extras, capture what baseline analysis had (diagnostic only)
        _fix41u_prev_trace = _fix41u_extract_injected_from_prev(previous_data)
        _fix41u_replayed = []
        try:
            if not _fix41u_extra_urls and isinstance(_fix41u_prev_trace, dict):
                _fix41u_replayed = _inj_diag_norm_url_list(_fix41u_prev_trace.get("ui_norm") or [])
        except Exception:
            _fix41u_replayed = []

        # Ensure diag container exists
        web_context.setdefault("diag_injected_urls", {})
        if isinstance(web_context.get("diag_injected_urls"), dict):
            _d = web_context["diag_injected_urls"]
            # run_id continuity
            try:
                _d.setdefault("run_id", str(web_context.get("diag_run_id") or ""))
            except Exception:
                pass
            # UI-provided inputs (preferred truth)
            _d.setdefault("ui_raw", _fix41u_ui_raw)
            _d.setdefault("ui_norm", list(_fix41u_extra_urls))
            _d.setdefault("intake_norm", list(_fix41u_extra_urls))
            # Replay visibility (diagnostic only; NOT used for hashing unless caller also provided extras)
            if _fix41u_replayed:
                _d.setdefault("replayed_from_analysis_norm", list(_fix41u_replayed))
    except Exception:
        pass
    # =====================================================================
    # END PATCH FIX41U
    # =====================================================================


    # =====================================================================
    # PATCH HF5 (ADDITIVE): rehydrate previous_data from HistoryFull if wrapper
    # Why:
    # - Some UI/Sheets paths provide a summarized wrapper that lacks primary_response,
    #   metric_schema_frozen, metric_anchors, baseline_sources_cache, etc.
    # - If a full_store_ref pointer exists, load the full payload deterministically.
    #
    # NOTE:
    # - Do NOT write to `output` here (output not built yet). We stash flags
    #   and attach them after `output = {...}` is created.
    # =====================================================================
    _prev_rehydrated = False
    _prev_rehydrated_ref = ""

    try:
        if isinstance(previous_data, dict):
            _pr = previous_data.get("primary_response")

            # Determine if we are missing rebuild essentials
            _need = (
                (not isinstance(_pr, dict))
                or (not _pr)
                or (not isinstance(_pr.get("metric_schema_frozen"), dict))
            )

            if _need:
                # Explicit line (requested): simplest location first
                ref = previous_data.get("full_store_ref", "")  # <-- requested line

                # Then fall back to other known wrapper locations (more robust)
                _ref = (
                    ref
                    or (previous_data.get("results") or {}).get("full_store_ref")
                    or (isinstance(_pr, dict) and _pr.get("full_store_ref"))
                    or ""
                )

                # Last-ditch deterministic fallback: if wrapper carries _sheet_id
                if (not _ref) and isinstance(previous_data.get("_sheet_id"), str) and previous_data.get("_sheet_id"):
                    _ref = f"gsheet:HistoryFull:{previous_data.get('_sheet_id')}"

                if isinstance(_ref, str) and _ref.startswith("gsheet:"):
                    parts = _ref.split(":")
                    _ws_title = parts[1] if len(parts) > 1 and parts[1] else "HistoryFull"
                    _aid = parts[2] if len(parts) > 2 else ""
                    full = load_full_history_payload_from_sheet(_aid, worksheet_title=_ws_title) if _aid else {}
                    if isinstance(full, dict) and full:
                        previous_data = full
                        _prev_rehydrated = True
                        _prev_rehydrated_ref = _ref
    except Exception:
        pass
    # =====================================================================

    # ---------- Pull baseline snapshots (VALID only) ----------
    snapshot_origin = "none"
    baseline_sources_cache = []

    try:
        if isinstance(previous_data, dict):
            # 1) results.baseline_sources_cache (preferred)
            r = previous_data.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache"), list):
                baseline_sources_cache = r.get("baseline_sources_cache") or []
                if baseline_sources_cache:
                    snapshot_origin = "analysis_results_cache"

            # 2) top-level baseline_sources_cache
            if not baseline_sources_cache and isinstance(previous_data.get("baseline_sources_cache"), list):
                baseline_sources_cache = previous_data.get("baseline_sources_cache") or []
                if baseline_sources_cache:
                    snapshot_origin = "analysis_top_level_cache"
    except Exception:
        baseline_sources_cache = []

    # =====================================================================
    # PATCH ES1B (ADDITIVE): broaden snapshot discovery (legacy storage shapes)
    # =====================================================================
    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            pr = previous_data.get("primary_response") or {}
            if isinstance(pr, dict):
                # A) primary_response.results.baseline_sources_cache
                r2 = pr.get("results")
                if (not baseline_sources_cache) and isinstance(r2, dict) and isinstance(r2.get("baseline_sources_cache"), list):
                    baseline_sources_cache = r2.get("baseline_sources_cache") or []
                    if baseline_sources_cache:
                        snapshot_origin = "primary_response_results_cache"

                # B) primary_response.baseline_sources_cache
                if (not baseline_sources_cache) and isinstance(pr.get("baseline_sources_cache"), list):
                    baseline_sources_cache = pr.get("baseline_sources_cache") or []
                    if baseline_sources_cache:
                        snapshot_origin = "primary_response_top_level_cache"

                # C) primary_response.results.source_results (reconstruct minimal snapshot shape)
                if (not baseline_sources_cache) and isinstance(r2, dict) and isinstance(r2.get("source_results"), list):
                    rebuilt_sr = []
                    for sr in (r2.get("source_results") or []):
                        if not isinstance(sr, dict):
                            continue
                        u = sr.get("source_url") or sr.get("url")
                        ex = sr.get("extracted_numbers")
                        if u and isinstance(ex, list) and ex:
                            rebuilt_sr.append({
                                "source_url": u,
                                "extracted_numbers": ex,
                                "clean_text": sr.get("clean_text") or sr.get("content") or "",
                                "fingerprint": sr.get("fingerprint"),
                                "fetched_at": sr.get("fetched_at"),
                            })
                    rebuilt_sr.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))
                    if rebuilt_sr:
                        baseline_sources_cache = rebuilt_sr
                        snapshot_origin = "primary_response_source_results_rebuild"

        # D) previous_data.results.source_results fallback
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            r3 = previous_data.get("results")
            if isinstance(r3, dict) and isinstance(r3.get("source_results"), list):
                rebuilt_sr2 = []
                for sr in (r3.get("source_results") or []):
                    if not isinstance(sr, dict):
                        continue
                    u = sr.get("source_url") or sr.get("url")
                    ex = sr.get("extracted_numbers")
                    if u and isinstance(ex, list) and ex:
                        rebuilt_sr2.append({
                            "source_url": u,
                            "extracted_numbers": ex,
                            "clean_text": sr.get("clean_text") or sr.get("content") or "",
                            "fingerprint": sr.get("fingerprint"),
                            "fetched_at": sr.get("fetched_at"),
                        })
                rebuilt_sr2.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))
                if rebuilt_sr2:
                    baseline_sources_cache = rebuilt_sr2
                    snapshot_origin = "analysis_source_results_rebuild"
    except Exception:
        pass

    # =====================================================================
    # PATCH SS6C (ADDITIVE): evidence_records fallback for snapshots (evolution-time)
    # =====================================================================
    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            _er = None
            if isinstance(previous_data.get("results"), dict):
                _er = previous_data["results"].get("evidence_records")
            if _er is None:
                _er = previous_data.get("evidence_records")
            _rebuilt = build_baseline_sources_cache_from_evidence_records(_er)
            if isinstance(_rebuilt, list) and _rebuilt:
                baseline_sources_cache = _rebuilt
                snapshot_origin = "evidence_records_rebuild"
    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH ES1C (ADDITIVE): validate snapshot shape & prepare debug metadata
    # =====================================================================
    _snapshot_debug = None
    try:
        _raw_len = int(len(baseline_sources_cache)) if isinstance(baseline_sources_cache, list) else 0
        _kept = []
        if isinstance(baseline_sources_cache, list):
            for s in baseline_sources_cache:
                if not isinstance(s, dict):
                    continue
                u = s.get("source_url") or s.get("url")
                ex = s.get("extracted_numbers")
                if u and isinstance(ex, list):
                    _kept.append(s)
        _kept.sort(key=lambda d: (str(d.get("source_url") or d.get("url") or ""), str(d.get("fingerprint") or "")))
        baseline_sources_cache = _kept
        _snapshot_debug = {
            "origin": snapshot_origin,
            "raw_count": _raw_len,
            "valid_count": int(len(baseline_sources_cache)),
            "example_urls": [x.get("source_url") or x.get("url") for x in (baseline_sources_cache[:3] if isinstance(baseline_sources_cache, list) else [])],
            "prev_keys": sorted(list(previous_data.keys()))[:40] if isinstance(previous_data, dict) else [],
        }
    except Exception:
        pass
    # =====================================================================

    # 3) reconstruct from web_context.scraped_meta (if provided)
    if (not baseline_sources_cache) and isinstance(web_context, dict):
        try:
            scraped_meta = web_context.get("scraped_meta") or {}
            rebuilt = []
            if isinstance(scraped_meta, dict):
                for url, meta in scraped_meta.items():
                    if not isinstance(meta, dict):
                        continue
                    content = meta.get("clean_text") or meta.get("content") or ""
                    fp = meta.get("fingerprint") or _fingerprint(content)
                    if not fp or len(content or "") < 800:
                        continue
                    nums = meta.get("extracted_numbers") or []
                    if not isinstance(nums, list):
                        nums = []
                    rebuilt.append({
                        "url": url,
                        "status": meta.get("status") or "fetched",
                        "status_detail": meta.get("status_detail") or "",
                        "numbers_found": _safe_int(meta.get("numbers_found"), default=len(nums)),
                        "fetched_at": meta.get("fetched_at") or _now(),
                        "fingerprint": fp,
                        "content_type": meta.get("content_type") or "",
                        "extracted_numbers": [
                            {
                                "value": n.get("value"),
                                "unit": n.get("unit"),
                                "raw": n.get("raw"),
                                "context_snippet": (n.get("context_snippet") or n.get("context") or "")[:200],
                                "anchor_hash": n.get("anchor_hash"),
                                "is_junk": n.get("is_junk"),
                                "junk_reason": n.get("junk_reason"),
                                "unit_tag": n.get("unit_tag"),
                                "unit_family": n.get("unit_family"),
                                "base_unit": n.get("base_unit"),
                                "multiplier_to_base": n.get("multiplier_to_base"),
                                "value_norm": n.get("value_norm"),
                                "start_idx": n.get("start_idx"),
                                "end_idx": n.get("end_idx"),
                                "source_url": n.get("source_url") or url,
                            }
                            for n in nums if isinstance(n, dict)
                        ]
                    })
            if rebuilt:
                baseline_sources_cache = rebuilt
                snapshot_origin = "web_context_scraped_meta"
        except Exception:
            pass

    # Also count invalid snapshots for debug (if present)
    invalid_count = 0
    try:
        if isinstance(previous_data, dict):
            r = previous_data.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache_invalid"), list):
                invalid_count = len(r.get("baseline_sources_cache_invalid") or [])
    except Exception:
        invalid_count = 0

    # ---------- Prepare stable default output ----------
    output = {
        "status": "success",
        "message": "",
        "sources_checked": 0,
        "sources_fetched": 0,
        "numbers_extracted_total": 0,
        "stability_score": 0.0,
        "summary": {
            "total_metrics": 0,
            "metrics_found": 0,
            "metrics_increased": 0,
            "metrics_decreased": 0,
            "metrics_unchanged": 0,
        },
        "metric_changes": [],
        "source_results": [],
        "interpretation": "",
        "snapshot_origin": snapshot_origin,
        "valid_snapshot_count": len(baseline_sources_cache or []),
        "invalid_snapshot_count": int(invalid_count),
        "generated_at": _now(),
    }

    # =====================================================================
    # PATCH FIX35 (ADDITIVE): emit origin + hash debugging for process-of-elimination
    # - Always stamp CODE_VERSION into output
    # - Create output['debug'] container (non-breaking)
    # - Track fastpath eligibility + reason in a deterministic way
    # =====================================================================
    try:
        output["code_version"] = CODE_VERSION
    except Exception:
        pass
    try:
        if not isinstance(output.get("debug"), dict):
            output["debug"] = {}
        output["debug"].setdefault("fix35", {})
        output["debug"]["fix35"]["current_metrics_origin"] = "unknown"
        output["debug"]["fix35"]["fastpath_eligible"] = False
        output["debug"]["fix35"]["fastpath_reason"] = ""
    except Exception:
        pass

    # Attach debug flags (rehydration + snapshot_debug)
    try:
        if _prev_rehydrated:
            output["previous_data_rehydrated"] = True
            output["previous_data_full_store_ref"] = _prev_rehydrated_ref
    except Exception:
        pass
    try:
        if isinstance(_snapshot_debug, dict) and _snapshot_debug:
            output["snapshot_debug"] = _snapshot_debug
    except Exception:
        pass

    # =====================================================================
    # PATCH SS6 (ADDITIVE, REQUIRED): last-chance snapshot rehydration
    # =====================================================================
    try:
        if not baseline_sources_cache and isinstance(previous_data, dict):
            _ref = previous_data.get("snapshot_store_ref") or (previous_data.get("results") or {}).get("snapshot_store_ref")
            _hash = previous_data.get("source_snapshot_hash") or (previous_data.get("results") or {}).get("source_snapshot_hash")

            # ============================================================
            # PATCH FIX41I_SS6_STABLE (ADDITIVE): prefer v2/stable snapshot refs & hashes
            # Why:
            # - Analysis now emits stable/v2 snapshot hashes (source_snapshot_hash_v2 / _stable) and
            #   snapshot_store_ref_v2 pointing at the same Snapshots row key.
            # - Evolution must prefer these fields to keep fastpath alignment intact.
            # ============================================================
            try:
                _ref_v2 = previous_data.get("snapshot_store_ref_v2") or previous_data.get("snapshot_store_ref_stable")
                if (not _ref_v2) and isinstance(previous_data.get("results"), dict):
                    _ref_v2 = (previous_data.get("results") or {}).get("snapshot_store_ref_v2") or (previous_data.get("results") or {}).get("snapshot_store_ref_stable")
                if isinstance(_ref_v2, str) and _ref_v2:
                    _ref = _ref_v2  # prefer stable ref
            except Exception:
                pass

            try:
                _hash_stable = previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2")
                if (not _hash_stable) and isinstance(previous_data.get("results"), dict):
                    _hash_stable = (previous_data.get("results") or {}).get("source_snapshot_hash_stable") or (previous_data.get("results") or {}).get("source_snapshot_hash_v2")
                if isinstance(_hash_stable, str) and _hash_stable:
                    _hash = _hash_stable  # prefer stable hash
            except Exception:
                pass
            # ============================================================

            if isinstance(_ref, str) and _ref.startswith("gsheet:"):
                parts = _ref.split(":")
                _ws_title = parts[1] if len(parts) > 1 and parts[1] else "Snapshots"
                _h = parts[2] if len(parts) > 2 else ""
                baseline_sources_cache = load_full_snapshots_from_sheet(_h, worksheet_title=_ws_title) if _h else []
                if baseline_sources_cache:
                    output["snapshot_origin"] = "sheet_snapshot_store_ref"

            if not baseline_sources_cache and isinstance(_hash, str) and _hash:
                baseline_sources_cache = load_full_snapshots_from_sheet(_hash, worksheet_title="Snapshots")
                if baseline_sources_cache:
                    output["snapshot_origin"] = "sheet_source_snapshot_hash"

            if not baseline_sources_cache and isinstance(_ref, str) and _ref and not _ref.startswith("gsheet:"):
                baseline_sources_cache = load_full_snapshots_local(_ref)
                if baseline_sources_cache:
                    output["snapshot_origin"] = "local_snapshot_store_ref"

            if isinstance(baseline_sources_cache, list):
                output["valid_snapshot_count"] = len(baseline_sources_cache)
    except Exception:
        pass
    # =====================================================================

    # If no valid snapshots, return "not_found"
    if not baseline_sources_cache:
        output["status"] = "failed"
        output["message"] = "No valid snapshots available for source-anchored evolution. (No re-fetch / no heuristic matching performed.)"
        output["interpretation"] = "Snapshot-gated: evolution refused to fabricate matches without valid cached source text."
        return output

    # ---------- Use your existing deterministic metric diff helper ----------
    prev_response = (previous_data or {}).get("primary_response", {}) or {}

    # =====================================================================
    # PATCH HF6 (ADDITIVE): tolerate previous_data being the primary_response itself
    # =====================================================================
    try:
        if (not isinstance(prev_response, dict) or not prev_response) and isinstance(previous_data, dict):
            if isinstance(previous_data.get("primary_metrics_canonical"), dict) or isinstance(previous_data.get("metric_schema_frozen"), dict):
                prev_response = previous_data
    except Exception:
        pass
    # =====================================================================

    prev_metrics = prev_response.get("primary_metrics_canonical") or prev_response.get("primary_metrics") or {}

    # ============================================================
    # PATCH CSR_INPUTS1 (ADDITIVE): normalize prev schema/anchors/canon
    # (safe alias for prior `prev_analysis` usage)
    # ============================================================
    prev_analysis = previous_data  # PATCH CSR_INPUTS1_ALIAS (ADDITIVE)
    try:
        prev_schema = _first_present(prev_analysis, [
            ("metric_schema_frozen",),
            ("primary_response", "metric_schema_frozen"),
            ("results", "metric_schema_frozen"),
            ("results", "primary_response", "metric_schema_frozen"),
        ], default=None)

        prev_canon = _first_present(prev_analysis, [
            ("primary_metrics_canonical",),
            ("primary_response", "primary_metrics_canonical"),
            ("results", "primary_metrics_canonical"),
            ("results", "primary_response", "primary_metrics_canonical"),
        ], default=None)

        prev_anchors = _first_present(prev_analysis, [
            ("metric_anchors",),
            ("primary_response", "metric_anchors"),
            ("results", "metric_anchors"),
            ("results", "primary_response", "metric_anchors"),
        ], default=None)

        if (not isinstance(prev_schema, dict) or not prev_schema) and isinstance(prev_canon, dict) and prev_canon:
            try:
                fn = globals().get("freeze_metric_schema")
                if callable(fn):
                    prev_schema = fn(prev_canon)
            except Exception:
                pass
    except Exception:
        pass
    # ============================================================

    # Ensure schema/anchors are available inside prev_response (additive copies)
    try:
        if isinstance(prev_response, dict) and not isinstance(prev_response.get("metric_schema_frozen"), dict):
            if isinstance(previous_data.get("metric_schema_frozen"), dict):
                prev_response["metric_schema_frozen"] = previous_data.get("metric_schema_frozen")
    except Exception:
        pass
    try:
        if isinstance(prev_response, dict) and not isinstance(prev_response.get("metric_anchors"), dict):
            if isinstance(previous_data.get("metric_anchors"), dict):
                prev_response["metric_anchors"] = previous_data.get("metric_anchors")
    except Exception:
        pass

    # =====================================================================
    # PATCH FIX31 (ADDITIVE): authoritative fast-path when sources + data unchanged
    #
    # Principle:
    #   If the source snapshot inputs are proven unchanged, do NOT perform any
    #   anchor-based selection or rebuild "gymnastics". Reuse the already
    #   processed + schema-gated metrics from the previous analysis payload and
    #   publish directly.
    #
    # Implementation notes:
    #   - We compute a stable hash from baseline_sources_cache[*].extracted_numbers
    #     using a reduced, order-independent projection.
    #   - If it matches previous_data/source_snapshot_hash AND a prior processed
    #     canonical metrics dict exists, we set current_metrics to prev_metrics
    #     and force anchors to be ignored by short-circuiting _get_metric_anchors().
    #   - This is purely additive and does not remove legacy paths.
    # =====================================================================
    _fix31_authoritative_reuse = False
    try:
        import json as _fix31_json
        import hashlib as _fix31_hashlib

        def _fix31_stable_dumps(obj):
            try:
                return _fix31_json.dumps(obj, sort_keys=True, ensure_ascii=False, separators=(",", ":"))
            except Exception:
                # last resort
                return str(obj)

        def _fix31_snapshot_fingerprint(bsc):
            # Reduced projection: stable across benign field additions/ordering
            rows = []
            for sr in (bsc or []):
                if not isinstance(sr, dict):
                    continue
                u = sr.get("source_url") or sr.get("url") or ""
                nums = []
                for n in (sr.get("extracted_numbers") or []):
                    if not isinstance(n, dict):
                        continue
                    nums.append({
                        "anchor_hash": n.get("anchor_hash") or "",
                        "value_norm": n.get("value_norm"),
                        "unit_tag": n.get("unit_tag") or "",
                        "unit": n.get("unit") or n.get("unit_norm") or "",
                        "currency": n.get("currency") or n.get("currency_symbol") or "",
                        "is_percent": bool(n.get("is_percent") or n.get("has_percent")),
                        "is_junk": bool(n.get("is_junk")),
                    })
                # order-independent for candidates
                nums = sorted(nums, key=lambda x: (_fix31_stable_dumps(x)))
                rows.append({"source_url": u, "extracted_numbers": nums})
            rows = sorted(rows, key=lambda r: r.get("source_url") or "")
            payload = _fix31_stable_dumps(rows).encode("utf-8", errors="ignore")
            return _fix31_hashlib.sha256(payload).hexdigest()

        # =========================
        # PATCH FIX37 (ADD): stable snapshot hash for fastpath alignment
        # - Use the SAME hash function as analysis (compute_source_snapshot_hash_v2) whenever possible.
        # - Falls back to legacy compute_source_snapshot_hash, then to the reduced fingerprint.
        # =========================
        def _fix37_snapshot_hash_stable(bsc):
            try:
                if isinstance(bsc, list) and bsc:
                    try:
                        _h2 = compute_source_snapshot_hash_v2(bsc)
                        if _h2:
                            return str(_h2)
                    except Exception:
                        pass
                    try:
                        _h1 = compute_source_snapshot_hash(bsc)
                        if _h1:
                            return str(_h1)
                    except Exception:
                        pass
            except Exception:
                pass
            return _fix31_snapshot_fingerprint(bsc)

        _prev_hash = None
        _prev_hash_stable = None
        if isinstance(previous_data, dict):
            # =========================
            # PATCH FIX37 (ADD): prefer stable hash keys when available
            # =========================
            _prev_hash_stable = previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2")
            try:
                if not _prev_hash_stable and isinstance(previous_data.get("results"), dict):
                    _prev_hash_stable = (previous_data.get("results") or {}).get("source_snapshot_hash_stable") or (previous_data.get("results") or {}).get("source_snapshot_hash_v2")
            except Exception:
                pass
            _prev_hash = _prev_hash_stable or previous_data.get("source_snapshot_hash")
            # PATCH FIX41I_FASTPATH_PREF (ADDITIVE): explicit preferred hash (stable/v2 first)
            _prev_hash_pref = _prev_hash_stable or previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2") or _prev_hash

            try:
                if not _prev_hash and isinstance(previous_data.get("results"), dict):
                    _prev_hash = (previous_data.get("results") or {}).get("source_snapshot_hash")
            except Exception:
                pass

# PATCH FIX36 (ADDITIVE): populate explicit fastpath ineligibility reasons
        # - Record current/previous hashes even on mismatch
        # - Explain which prerequisite failed (no_prev_hash / no_prev_metrics / no_snapshots / hash_mismatch)
        # ============================================================
        _fix36_cur_hash = None
        _fix36_reason = ""

        # =====================================================================
        # PATCH FIX41AFC15 (ADDITIVE): Pre-hash merge of injected URL delta into baseline_sources_cache
        #
        # Why:
        # - In your latest evolution JSON, fastpath was correctly bypassed due to injected delta,
        #   but the current snapshot universe (baseline_sources_cache) still did NOT include the
        #   injected URL, so the stable hash still matched and downstream logic treated the run
        #   as "no delta" (no fetch, no rebuild, no injected lifecycle).
        #
        # Goal:
        # - BEFORE computing the current stable hash / fastpath eligibility, deterministically
        #   append placeholder snapshot rows for any injected URLs missing from the current
        #   baseline_sources_cache universe. This makes the hash differ (as it should when the
        #   source universe changes), forcing the normal rebuild/fetch pathways without changing
        #   the hashing algorithm itself.
        #
        # Safety:
        # - Purely additive.
        # - No effect when no injected URLs are present OR all injected URLs already exist in
        #   baseline_sources_cache.
        # =====================================================================
        try:
            _fx15_wc = web_context if isinstance(web_context, dict) else {}
            _fx15_extra = []
            # Prefer already-wired list fields
            if isinstance(_fx15_wc.get("extra_urls"), (list, tuple)):
                _fx15_extra = list(_fx15_wc.get("extra_urls") or [])
            elif isinstance(_fx15_wc.get("diag_extra_urls_ui"), (list, tuple)) and _fx15_wc.get("diag_extra_urls_ui"):
                _fx15_extra = list(_fx15_wc.get("diag_extra_urls_ui") or [])
            elif isinstance(_fx15_wc.get("diag_extra_urls_ui_raw"), str) and (_fx15_wc.get("diag_extra_urls_ui_raw") or "").strip():
                _raw = str(_fx15_wc.get("diag_extra_urls_ui_raw") or "")
                _parts = []
                for _line in _raw.splitlines():
                    _line = (_line or "").strip()
                    if not _line:
                        continue
                    for _p in _line.split(","):
                        _p = (_p or "").strip()
                        if _p:
                            _parts.append(_p)
                _fx15_extra = _parts

            _fx15_inj_norm = _inj_diag_norm_url_list(_fx15_extra) if _fx15_extra else []
            if _fx15_inj_norm and isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                _fx15_base_urls = []
                for _r in (baseline_sources_cache or []):
                    if not isinstance(_r, dict):
                        continue
                    _u = _r.get("source_url") or _r.get("url") or ""
                    if isinstance(_u, str) and _u:
                        _fx15_base_urls.append(_u)
                _fx15_base_set = set(_inj_diag_norm_url_list(_fx15_base_urls)) if _fx15_base_urls else set()
                _fx15_delta = [u for u in _fx15_inj_norm if u and u not in _fx15_base_set]
                if _fx15_delta:
                    # Append stable placeholders so the snapshot hash changes deterministically.
                    for _u in _fx15_delta:
                        baseline_sources_cache.append({
                            "source_url": _u,
                            "url": _u,
                            "status": "injected_pending",
                            "status_detail": "injected_url_placeholder_pre_hash",
                            "snapshot_text": "",
                            "extracted_numbers": [],
                            "numbers_found": 0,
                            "injected": True,
                            "injected_reason": "prehash_placeholder",
                        })
                    # Also ensure downstream sees a consistent universe via web_context["extra_urls"].
                    try:
                        if isinstance(_fx15_wc, dict):
                            _fx15_wc.setdefault("extra_urls", [])
                            if isinstance(_fx15_wc.get("extra_urls"), list):
                                # keep original order; append unique normalized
                                _seen = set(_inj_diag_norm_url_list(_fx15_wc.get("extra_urls") or []))
                                for _u in _fx15_delta:
                                    if _u not in _seen:
                                        _fx15_wc["extra_urls"].append(_u)
                                        _seen.add(_u)
                    except Exception:
                        pass
                    # Debug
                    try:
                        output.setdefault("debug", {})
                        if isinstance(output.get("debug"), dict):
                            output["debug"].setdefault("fix41afc15", {})
                            if isinstance(output["debug"].get("fix41afc15"), dict):
                                output["debug"]["fix41afc15"].update({
                                    "inj_norm_count": int(len(_fx15_inj_norm)),
                                    "inj_norm": list(_fx15_inj_norm),
                                    "inj_delta_count": int(len(_fx15_delta)),
                                    "inj_delta": list(_fx15_delta),
                                    "baseline_sources_cache_count_after_placeholder": int(len(baseline_sources_cache or [])),
                                })
                    except Exception:
                        pass
        except Exception:
            pass

        # =====================================================================
        # PATCH FIX41AFC16 (ADDITIVE): If injected URL placeholders exist, actually fetch+extract them
        #
        # Observed gap (from evolution JSON):
        #   - Injected URLs were present in ui/intake/hash_inputs, but remained:
        #       status = "injected_pending" / status_detail = "injected_url_placeholder_pre_hash"
        #   - So they never produced snapshot_text / extracted_numbers, and thus could not
        #     influence metric rebuild beyond a "hash universe" delta.
        #
        # Goal:
        #   - When injection is present, attempt to fetch+extract the injected URLs (delta-only),
        #     and update their baseline_sources_cache rows in-place so downstream rebuild sees them
        #     like normal fetched sources (or explicit failed reasons).
        #
        # Safety:
        #   - Purely additive.
        #   - No effect when no injected URLs are present.
        #   - Only touches rows that are injected placeholders (status == injected_pending) OR
        #     URLs that are injected_delta (not already in baseline).
        # =====================================================================
        try:
            _fx16_wc = web_context if isinstance(web_context, dict) else {}
            _fx16_extra = []
            if isinstance(_fx16_wc.get("extra_urls"), (list, tuple)) and _fx16_wc.get("extra_urls"):
                _fx16_extra = list(_fx16_wc.get("extra_urls") or [])
            elif isinstance(_fx16_wc.get("diag_extra_urls_ui"), (list, tuple)) and _fx16_wc.get("diag_extra_urls_ui"):
                _fx16_extra = list(_fx16_wc.get("diag_extra_urls_ui") or [])
            elif isinstance(_fx16_wc.get("diag_extra_urls_ui_raw"), str) and (_fx16_wc.get("diag_extra_urls_ui_raw") or "").strip():
                _raw = str(_fx16_wc.get("diag_extra_urls_ui_raw") or "")
                _parts = []
                for _line in _raw.splitlines():
                    _line = (_line or "").strip()
                    if not _line:
                        continue
                    for _p in _line.split(","):
                        _p = (_p or "").strip()
                        if _p:
                            _parts.append(_p)
                _fx16_extra = _parts

            _fx16_inj_norm = _inj_diag_norm_url_list(_fx16_extra) if _fx16_extra else []
            _fx16_base_urls = []
            if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                for _r in (baseline_sources_cache or []):
                    if not isinstance(_r, dict):
                        continue
                    _u = _r.get("source_url") or _r.get("url") or ""
                    if isinstance(_u, str) and _u:
                        _fx16_base_urls.append(_u)
            _fx16_base_set = set(_inj_diag_norm_url_list(_fx16_base_urls)) if _fx16_base_urls else set()
            _fx16_delta = [u for u in _fx16_inj_norm if u and u not in _fx16_base_set]

            # Identify placeholder rows that should be fetched
            _fx16_targets = []
            if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                for _r in (baseline_sources_cache or []):
                    if not isinstance(_r, dict):
                        continue
                    _u = _r.get("source_url") or _r.get("url") or ""
                    _u_norm = _inj_diag_norm_url_list([_u])[0] if isinstance(_u, str) and _u else ""
                    if not _u_norm:
                        continue
                    if _r.get("status") == "injected_pending":
                        _fx16_targets.append((_u_norm, _r, "placeholder_row"))
                    elif _u_norm in _fx16_delta:
                        _fx16_targets.append((_u_norm, _r, "delta_row"))

            # Also cover the case where placeholders were not appended (defensive)
            for _u in (_fx16_delta or []):
                if not isinstance(baseline_sources_cache, list):
                    continue
                if any((_inj_diag_norm_url_list([(_r.get("source_url") or _r.get("url") or "")])[0] if isinstance(_r, dict) else "") == _u for _r in (baseline_sources_cache or [])):
                    continue
                baseline_sources_cache.append({
                    "source_url": _u,
                    "url": _u,
                    "status": "injected_pending",
                    "status_detail": "injected_url_placeholder_pre_hash",
                    "snapshot_text": "",
                    "extracted_numbers": [],
                    "numbers_found": 0,
                    "injected": True,
                    "injected_reason": "fx16_defensive_placeholder",
                })
                _fx16_targets.append((_u, baseline_sources_cache[-1], "defensive_placeholder"))

            # Fetch+extract for targets (best-effort)
            _fx16_fetched = []
            _fx16_failed = []
            if _fx16_targets:
                for (_u_norm, _row, _why) in _fx16_targets:
                    # Skip if row already has text/numbers (idempotent)
                    try:
                        if isinstance(_row.get("snapshot_text"), str) and _row.get("snapshot_text").strip():
                            continue
                        if isinstance(_row.get("extracted_numbers"), list) and len(_row.get("extracted_numbers") or []) > 0:
                            continue
                    except Exception:
                        pass

                    _txt = None
                    _detail = ""
                    try:
                        _txt, _detail = fetch_url_content_with_status(_u_norm, timeout=25)
                    except Exception as _e:
                        _txt, _detail = None, f"exception:{type(_e).__name__}"

                    if _txt and isinstance(_txt, str) and len(_txt.strip()) >= 200:
                        try:
                            _nums = extract_numbers_with_context(_txt, source_url=_u_norm) or []
                        except Exception:
                            _nums = []
                        _row.update({
                            "status": "fetched",
                            "status_detail": (_detail or "success"),
                            "snapshot_text": _txt[:7000],
                            "extracted_numbers": _nums,
                            "numbers_found": int(len(_nums or [])),
                            "injected": True,
                            "injected_reason": _row.get("injected_reason") or "fx16_fetch_and_extract",
                        })
                        _fx16_fetched.append({"url": _u_norm, "why": _why, "numbers_found": int(len(_nums or [])), "status_detail": (_detail or "success")})
                    else:
                        _row.update({
                            "status": "failed",
                            "status_detail": (_detail or "failed:no_text"),
                            "snapshot_text": "",
                            "extracted_numbers": [],
                            "numbers_found": 0,
                            "injected": True,
                            "injected_reason": _row.get("injected_reason") or "fx16_fetch_failed",
                        })
                        _fx16_failed.append({"url": _u_norm, "why": _why, "status_detail": (_detail or "failed:no_text")})

            # Emit debug
            try:
                output.setdefault("debug", {})
                if isinstance(output.get("debug"), dict):
                    output["debug"].setdefault("fix41afc16", {})
                    if isinstance(output["debug"].get("fix41afc16"), dict):
                        output["debug"]["fix41afc16"].update({
                            "inj_norm_count": int(len(_fx16_inj_norm or [])),
                            "inj_delta_count": int(len(_fx16_delta or [])),
                            "fetch_target_count": int(len(_fx16_targets or [])),
                            "fetched_count": int(len(_fx16_fetched or [])),
                            "failed_count": int(len(_fx16_failed or [])),
                            "fetched": list(_fx16_fetched or []),
                            "failed": list(_fx16_failed or []),
                        })
            except Exception:
                pass
        except Exception:
            pass


        # =====================================================================
        # PATCH FIX41AFC17 (ADDITIVE): Pin fetched injected snapshots into canonical snapshot plumbing
        #
        # Observed gap (from evolution JSON after FIX41AFC16):
        #   - Injected URL reaches intake/admitted/attempted/hash_inputs, but snapshot_debug remains empty
        #     (origin none / raw_count 0), and downstream consumers appear to miss the fetched snapshot_text.
        #
        # Goal:
        #   - Ensure the same snapshot-carrier fields used by New Analysis are populated for Evolution,
        #     so that attach_source_snapshots_to_analysis() (and any downstream rebuild plumbing) can
        #     “see” the injected (and other) snapshots deterministically.
        #
        # Safety:
        #   - Purely additive wiring.
        #   - No effect if baseline_sources_cache is missing.
        #   - Does not alter hashing logic; only ensures snapshots are attached consistently.
        # =====================================================================
        try:
            if isinstance(baseline_sources_cache, list) and baseline_sources_cache and isinstance(web_context, dict):
                # Provide canonical aliases for current pool (additive; downstream may read any of these)
                web_context.setdefault("current_baseline_sources_cache", baseline_sources_cache)
                web_context.setdefault("baseline_sources_cache_current", baseline_sources_cache)
                web_context.setdefault("current_source_results", baseline_sources_cache)

                # Mirror into output for downstream consumers/debug (additive)
                try:
                    output.setdefault("baseline_sources_cache_current", baseline_sources_cache)
                except Exception:
                    pass
                try:
                    output.setdefault("baseline_sources_cache", baseline_sources_cache)
                except Exception:
                    pass
                try:
                    output.setdefault("results", {})
                    if isinstance(output.get("results"), dict):
                        output["results"].setdefault("baseline_sources_cache_current", baseline_sources_cache)
                        output["results"].setdefault("baseline_sources_cache", baseline_sources_cache)
                except Exception:
                    pass

                # Call the same snapshot attach helper used by analysis if present (best-effort)
                try:
                    _att_fn = globals().get("attach_source_snapshots_to_analysis")
                    if callable(_att_fn):
                        _att_fn(output, web_context)
                except Exception:
                    pass

                # Small debug breadcrumb
                try:
                    output.setdefault("debug", {})
                    if isinstance(output.get("debug"), dict):
                        output["debug"].setdefault("fix41afc17", {})
                        if isinstance(output["debug"].get("fix41afc17"), dict):
                            output["debug"]["fix41afc17"].update({
                                "attached_pool_count": int(len(baseline_sources_cache)),
                                "attach_called": bool(callable(globals().get("attach_source_snapshots_to_analysis"))),
                            })
                except Exception:
                    pass
        except Exception:
            pass
        # =====================================================================

        try:
            if not (isinstance(baseline_sources_cache, list) and baseline_sources_cache):
                _fix36_reason = "no_snapshots"
            elif not (isinstance(prev_metrics, dict) and prev_metrics):
                _fix36_reason = "no_prev_metrics"
            else:
                _fix36_cur_hash = _fix37_snapshot_hash_stable(baseline_sources_cache)
                if not (isinstance(_prev_hash, str) and _prev_hash):
                    _fix36_reason = "no_prev_hash"
                elif _fix36_cur_hash != _prev_hash:
                    _fix36_reason = "hash_mismatch"
                else:
                    _fix36_reason = "hash_match_and_prev_metrics_present"

            # =====================================================================
            # PATCH EVO_FASTPATH_BYPASS_ON_INJECTED_URL_DELTA_V1 (ADDITIVE)
            # Intent:
            #   If the Evolution UI supplies injected URLs that are NOT already part of the
            #   baseline source universe, bypass fastpath eligibility even when hashes match.
            #   This does NOT weaken fastpath for the locked/no-injection case; it only prevents
            #   "observed but inert" injections from being ignored when they materially change
            #   the intended source universe.
            #
            #   Key rule:
            #     - If injected_delta (normalized_injected_urls - normalized_baseline_urls) is non-empty
            #       and fastpath would otherwise be eligible, force _fix36_reason to a bypass reason so
            #       fastpath_eligible becomes False and rebuild path can run.
            # =====================================================================
            try:
                _evo_wc = web_context if isinstance(web_context, dict) else {}
                _evo_diag = _evo_wc.get("diag_injected_urls") if isinstance(_evo_wc.get("diag_injected_urls"), dict) else {}
                _evo_extra_urls = []
                # Prefer already-normalized intake/ui lists if present
                for _k in ("intake", "ui_norm", "ui", "extra_urls"):
                    _v = _evo_diag.get(_k)
                    if isinstance(_v, (list, tuple)) and _v:
                        _evo_extra_urls = list(_v)
                        break
                # Fall back to raw web_context extras if diag not populated
                if not _evo_extra_urls:
                    _v2 = _evo_wc.get("extra_urls")
                    if isinstance(_v2, (list, tuple)) and _v2:
                        _evo_extra_urls = list(_v2)


                # =====================================================================
                # PATCH EVO_FASTPATH_BYPASS_INJ_DELTA_V2 (ADDITIVE):
                #   Robustly recover injected/extra URLs for bypass detection even when
                #   diag_injected_urls is not populated yet (common on replay/fastpath).
                #   Sources (in order):
                #     - web_context["extra_urls"] if list
                #     - web_context["diag_extra_urls_ui"] if list
                #     - web_context["diag_extra_urls_ui_raw"] if str (newline/space separated)
                #   This is diagnostic-only: we ONLY use this to decide whether to bypass
                #   fastpath when hashes otherwise match, so injected URLs can be admitted
                #   via the rebuild path and become first-class inputs.
                # =====================================================================
                try:
                    if not _evo_extra_urls:
                        _v3 = _evo_wc.get("diag_extra_urls_ui")
                        if isinstance(_v3, (list, tuple)) and _v3:
                            _evo_extra_urls = list(_v3)
                    if not _evo_extra_urls:
                        _raw = _evo_wc.get("diag_extra_urls_ui_raw")
                        if isinstance(_raw, str) and _raw.strip():
                            # Split on newlines first; also allow commas/spaces as separators
                            _parts = []
                            for _line in _raw.splitlines():
                                _line = (_line or "").strip()
                                if not _line:
                                    continue
                                # allow comma-separated within a line
                                for _p in _line.split(","):
                                    _p = (_p or "").strip()
                                    if _p:
                                        _parts.append(_p)
                            if _parts:
                                _evo_extra_urls = _parts
                except Exception:
                    pass

                # =====================================================================
                # PATCH FIX41AFC2 (ADDITIVE): Ensure rebuild/fetch path receives injected URLs
                #   If we recovered injected URLs from Streamlit diagnostic fields (e.g.,
                #   diag_extra_urls_ui_raw) and web_context["extra_urls"] is empty, wire the
                #   recovered list into web_context["extra_urls"] so downstream admission/
                #   fetch/persist logic can see the same universe deterministically.
                #   No effect on no-injection runs.
                # =====================================================================
                try:
                    if isinstance(_evo_wc, dict):
                        _wc_extra = _evo_wc.get("extra_urls")
                        if (not isinstance(_wc_extra, (list, tuple)) or not _wc_extra) and isinstance(_evo_extra_urls, list) and _evo_extra_urls:
                            _evo_wc["extra_urls"] = list(_evo_extra_urls)
                except Exception:
                    pass

                _evo_inj_set = set(_inj_diag_norm_url_list(_evo_extra_urls)) if _evo_extra_urls else set()

                # Baseline universe = urls present in baseline_sources_cache (the same object used for hashing)
                _evo_base_urls = []
                if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                    for _row in baseline_sources_cache:
                        if isinstance(_row, dict) and isinstance(_row.get("source_url"), str) and _row.get("source_url"):
                            _evo_base_urls.append(_row.get("source_url"))
                _evo_base_set = set(_inj_diag_norm_url_list(_evo_base_urls)) if _evo_base_urls else set()

                _evo_inj_delta = sorted(list(_evo_inj_set - _evo_base_set)) if _evo_inj_set else []
                # =====================================================================
                # PATCH FIX41AFC2 (ADDITIVE): Latch bypass decision for later fastpath checks
                #   We persist a simple boolean flag in locals so the downstream FIX31
                #   authoritative reuse check can be disabled without refactoring.
                # =====================================================================
                _fix41af_inj_delta_present = bool(_evo_inj_delta)



                # Only bypass when hashes match and we would otherwise take fastpath
                if _evo_inj_delta and _fix36_reason == "hash_match_and_prev_metrics_present":
                    _fix36_reason = "hash_match_but_injected_urls_present_bypass_fastpath"
                    try:
                        if isinstance(output.get("debug"), dict) and isinstance(output.get("debug", {}).get("fix35"), dict):
                            output["debug"]["fix35"]["fastpath_bypass_injected_delta"] = _evo_inj_delta
                            output["debug"]["fix35"]["fastpath_bypass_injected_delta_count"] = len(_evo_inj_delta)
                    except Exception:
                        pass
            except Exception:
                # Never break evolution on diagnostics / bypass checks
                pass
            if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                # Preserve any earlier reason, but fill if empty
                if not output["debug"]["fix35"].get("fastpath_reason"):
                    output["debug"]["fix35"]["fastpath_reason"] = _fix36_reason
                output["debug"]["fix35"]["fastpath_eligible"] = bool(_fix36_reason == "hash_match_and_prev_metrics_present")
                if _fix36_cur_hash:
                    output["debug"]["fix35"]["source_snapshot_hash_current"] = _fix36_cur_hash
                    output["debug"]["fix35"]["source_snapshot_hash_current_alg"] = "fix37_stable_v2_preferred"
                if isinstance(_prev_hash, str) and _prev_hash:
                    output["debug"]["fix35"]["source_snapshot_hash_previous"] = (_prev_hash_pref if isinstance(locals().get("_prev_hash_pref"), str) and locals().get("_prev_hash_pref") else _prev_hash)
                # PATCH FIX37 (ADD): also expose stable-hash candidate if available
                try:
                    if isinstance(_prev_hash_stable, str) and _prev_hash_stable:
                        output["debug"]["fix35"]["source_snapshot_hash_previous_stable"] = _prev_hash_stable
                except Exception:
                    pass
        except Exception:
            pass
        # ============================================================

        # Only attempt fast-path if we have snapshots AND prior canonical metrics to reuse
        if isinstance(baseline_sources_cache, list) and baseline_sources_cache and isinstance(prev_metrics, dict) and prev_metrics:
            # ============================================================
            # PATCH FIX38 (ADDITIVE): align FIX31 authoritative reuse with FIX37 stable hash
            # - Previously FIX31 compared a v1 fingerprint against prev source_snapshot_hash,
            #   which could mismatch even when data was unchanged.
            # - We now prefer the same stable/v2 hash used by analysis & FIX37 debug.
            # ============================================================
            _cur_hash_v1 = _fix31_snapshot_fingerprint(baseline_sources_cache)
            try:
                _cur_hash = _fix37_snapshot_hash_stable(baseline_sources_cache)
            except Exception:
                _cur_hash = _cur_hash_v1

            # Prefer stable/v2 previous hash if present
            _prev_hash_pref = previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2") or _prev_hash

            # =====================================================================
            # PATCH FIX42 (ADDITIVE): prefer "current" snapshot pool when provided
            #
            # Goal:
            #   When hashes are unequal, rebuild should run on the SAME snapshot pool
            #   that "new analysis" just produced, not on the stale baseline cache
            #   embedded in the previous analysis payload.
            #
            # Where it comes from:
            #   - web_context["current_baseline_sources_cache"]  (preferred)
            #   - web_context["baseline_sources_cache_current"]
            #   - web_context["current_source_results"]         (fallback alias)
            #
            # Policy (additive, fastpath-safe):
            #   - If force_rebuild is asserted by UI/web_context, always use current pool.
            #   - Else, only switch to current pool if its stable hash != previous hash.
            #   - If hashes match, we keep existing behavior (but either pool is equivalent).
            # =====================================================================
            _fix42_used_current_pool = False
            _fix42_reason = ""
            _fix42_cur_pool_hash = None
            try:
                if isinstance(web_context, dict):
                    _cur_pool = (
                        web_context.get("current_baseline_sources_cache")
                        or web_context.get("baseline_sources_cache_current")
                        or web_context.get("current_source_results")
                        or web_context.get("current_source_results_cache")
                        or None
                    )
                    if isinstance(_cur_pool, list) and _cur_pool:
                        _force = bool(
                            web_context.get("force_rebuild")
                            or web_context.get("forced_rebuild")
                            or web_context.get("force_full_rebuild")
                            or web_context.get("force_metric_rebuild")
                        )
                        try:
                            _fix42_cur_pool_hash = _fix37_snapshot_hash_stable(_cur_pool)
                        except Exception:
                            _fix42_cur_pool_hash = None

                        _prev_hash_for_compare = _prev_hash_pref if (isinstance(_prev_hash_pref, str) and _prev_hash_pref) else _prev_hash
                        _hash_mismatch = bool(
                            (isinstance(_prev_hash_for_compare, str) and _prev_hash_for_compare and isinstance(_fix42_cur_pool_hash, str) and _fix42_cur_pool_hash)
                            and (_fix42_cur_pool_hash != _prev_hash_for_compare)
                        )

                        if _force or _hash_mismatch:
                            baseline_sources_cache = _cur_pool
                            snapshot_origin = (snapshot_origin or "analysis_cache") + "|fix42_current_pool"
                            _fix42_used_current_pool = True
                            _fix42_reason = "forced_rebuild" if _force else "hash_mismatch_use_current_pool"
            except Exception:
                pass

            # Attach FIX42 diagnostics (non-breaking)
            try:
                if _fix42_used_current_pool:
                    output["snapshot_origin"] = snapshot_origin
                if isinstance(output.get("debug"), dict):
                    output["debug"].setdefault("fix42", {})
                    output["debug"]["fix42"]["used_current_pool"] = bool(_fix42_used_current_pool)
                    output["debug"]["fix42"]["reason"] = _fix42_reason
                    if isinstance(_fix42_cur_pool_hash, str) and _fix42_cur_pool_hash:
                        output["debug"]["fix42"]["current_pool_hash_stable"] = _fix42_cur_pool_hash
            except Exception:
                pass
            # =====================================================================


            # =====================================================================
            # PATCH FIX41AFC2 (ADDITIVE): Enforce fastpath bypass on injected URL delta
            #   If an injected URL delta exists, we MUST NOT take FIX31 authoritative
            #   reuse (fastpath replay), even if hashes match. We do this additively by
            #   temporarily blanking _prev_hash_pref so the existing hash-match check
            #   remains unchanged for normal runs.
            # =====================================================================
            _fix41af_prev_hash_pref_saved = None
            try:
                if bool(locals().get("_fix41af_inj_delta_present")):
                    _fix41af_prev_hash_pref_saved = _prev_hash_pref
                    _prev_hash_pref = ""
                    try:
                        if isinstance(output.get("debug"), dict) and isinstance(output.get("debug", {}).get("fix35"), dict):
                            output["debug"]["fix35"]["fastpath_reason"] = "hash_match_but_injected_urls_present_bypass_fastpath"
                            output["debug"]["fix35"]["fastpath_eligible"] = False
                    except Exception:
                        pass
            except Exception:
                pass

            if isinstance(_prev_hash_pref, str) and _prev_hash_pref and _cur_hash == _prev_hash_pref:
                _fix31_authoritative_reuse = True
                # =====================================================================
                # PATCH FIX41AFC2 (ADDITIVE): Restore _prev_hash_pref after bypass guard
                # =====================================================================
                try:
                    if _fix41af_prev_hash_pref_saved is not None:
                        _prev_hash_pref = _fix41af_prev_hash_pref_saved
                except Exception:
                    pass

                try:
                    output["rebuild_skipped"] = True
                    output["rebuild_skipped_reason"] = "fix31_sources_unchanged_reuse_prev_metrics"
                    output["source_snapshot_hash_current"] = _cur_hash
                    output["source_snapshot_hash_previous"] = (_prev_hash_cmp if " _prev_hash_cmp" in locals() else _prev_hash)
                    try:
                        if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                            output["debug"]["fix35"]["fastpath_eligible"] = True
                            output["debug"]["fix35"]["fastpath_reason"] = "hash_match_and_prev_metrics_present"
                            output["debug"]["fix35"]["source_snapshot_hash_current"] = _cur_hash
                            output["debug"]["fix35"]["source_snapshot_hash_previous"] = (_prev_hash_pref if isinstance(locals().get("_prev_hash_pref"), str) and locals().get("_prev_hash_pref") else _prev_hash)
                            output["debug"]["fix35"]["current_metrics_origin"] = "reuse_processed_metrics_fastpath"
                    except Exception:
                        pass
                except Exception:
                    pass
    except Exception:
        _fix31_authoritative_reuse = False
        try:
            if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                if not output["debug"]["fix35"].get("fastpath_reason"):
                    output["debug"]["fix35"]["fastpath_reason"] = "fastpath_not_taken_or_exception"
        except Exception:
            pass
    # =====================================================================

    # Build a minimal current metrics dict from snapshots:
    current_metrics = {}
    # ============================================================
    # PATCH FIX31 (ADDITIVE): assign authoritative reused metrics now
    # ============================================================
    try:

        # =====================================================================
        # PATCH FIX41AFC2 (ADDITIVE): Ensure _prev_hash_pref restored if bypass guard blanked it
        #   (covers the case where hash-match condition was false and the inline restore
        #   inside the if-body did not execute).
        # =====================================================================
        try:
            if locals().get("_fix41af_prev_hash_pref_saved") is not None and not _prev_hash_pref:
                _prev_hash_pref = locals().get("_fix41af_prev_hash_pref_saved")
        except Exception:
            pass

        if _fix31_authoritative_reuse and isinstance(prev_metrics, dict) and prev_metrics:
            current_metrics = dict(prev_metrics)
            try:
                output["snapshot_origin"] = (output.get("snapshot_origin") or "") + "|fix31_reuse_prev_metrics"
            except Exception:
                pass
    except Exception:
        pass
    # ============================================================


    # Prefer metric_anchors to rebuild current_metrics (snapshot-gated)
    def _get_metric_anchors(prev: dict) -> dict:
        # ============================================================
        # PATCH FIX31 (ADDITIVE): if authoritative reuse is active, ignore anchors entirely
        # so the reused, schema-gated metrics remain untouched.
        # ============================================================
        try:
            if _fix31_authoritative_reuse:
                return {}
        except Exception:
            pass
        # ============================================================

        if not isinstance(prev, dict):
            return {}
        a = prev.get("metric_anchors")
        if isinstance(a, dict) and a:
            return a
        pr = prev.get("primary_response")
        if isinstance(pr, dict):
            a2 = pr.get("metric_anchors")
            if isinstance(a2, dict) and a2:
                return a2
        res = prev.get("results")
        if isinstance(res, dict):
            a3 = res.get("metric_anchors")
            if isinstance(a3, dict) and a3:
                return a3
        return {}

    def _canonicalize_candidate(n: dict) -> dict:
        try:
            fn = globals().get("canonicalize_numeric_candidate")
            if callable(fn):
                return fn(dict(n))
        except Exception:
            pass
        return dict(n)

    def _build_anchor_to_candidate_map(snapshots: list) -> dict:
        m = {}
        for sr in snapshots or []:
            if not isinstance(sr, dict):
                continue
            for n in (sr.get("extracted_numbers") or []):
                if not isinstance(n, dict):
                    continue
                nn = _canonicalize_candidate(n)
                ah = nn.get("anchor_hash")
                if not ah:
                    continue
                if ah not in m:
                    m[ah] = nn
        return m

    try:
        metric_anchors = _get_metric_anchors(previous_data)
        anchor_to_candidate = _build_anchor_to_candidate_map(baseline_sources_cache)

        if isinstance(metric_anchors, dict) and metric_anchors:
            for ckey, a in metric_anchors.items():
                if not isinstance(a, dict):
                    continue
                ah = a.get("anchor_hash") or a.get("anchor")
                if not ah:
                    continue
                cand = anchor_to_candidate.get(ah)
                if not isinstance(cand, dict):
                    continue

                base = prev_metrics.get(ckey) if isinstance(prev_metrics, dict) else None
                out_row = dict(base) if isinstance(base, dict) else {}
                out_row.update({
                    "canonical_key": ckey,
                    "anchor_hash": ah,
                    "anchor_used": True,
                    "anchor_confidence": a.get("anchor_confidence"),
                    "source_url": cand.get("source_url") or a.get("source_url"),
                    "raw": cand.get("raw"),
                    "value": cand.get("value"),
                    "unit": cand.get("unit"),
                    "value_norm": cand.get("value_norm"),
                    "context_snippet": cand.get("context_snippet") or cand.get("context") or "",
                    "candidate_id": cand.get("candidate_id") or a.get("candidate_id"),
                })
                current_metrics[ckey] = out_row
    except Exception:
        pass

    # Rebuild fallback only if anchors didn't produce metrics
    if not isinstance(current_metrics, dict) or not current_metrics:
        try:
            fn_rebuild = globals().get("rebuild_metrics_from_snapshots_schema_only") or globals().get("rebuild_metrics_from_snapshots")
            if callable(fn_rebuild):
                current_metrics = fn_rebuild(prev_response, baseline_sources_cache, web_context=web_context)
        except Exception:
            current_metrics = {}

    if not isinstance(current_metrics, dict) or not current_metrics:
        output["status"] = "failed"
        output["message"] = "Valid snapshots exist, but metric rebuild returned empty. No re-fetch / no heuristic matching performed."
        output["source_results"] = baseline_sources_cache[:50]
        output["sources_checked"] = len(baseline_sources_cache)
        output["sources_fetched"] = len(baseline_sources_cache)
        output["interpretation"] = "Snapshot-ready but metric rebuild not implemented or returned empty; add/verify rebuild_metrics_from_snapshots* hooks."
        return output


    # =====================================================================
    # PATCH FIX41AFC19 (ADDITIVE): Anchor-first FIX16 rebuild override (schema parity)
    #
    # Why:
    # - Latest evo JSON shows current metrics can be selected from non-matching units
    #   (e.g., unit_sales metric receiving a unitless/negative number; percent metric
    #   receiving a magnitude unit like 'B'). This leads the dashboard "Current" column
    #   to display the wrong metric values even though injection plumbing is progressing.
    # - The new analysis pipeline already relies on FIX16-style hard eligibility gates +
    #   anchor_hash deterministic rebuild. Evolution must use the same selection rules
    #   when fastpath is NOT taken (hash mismatch or injection-triggered rebuild).
    #
    # What:
    # - Right before diffing, attempt an anchor-first rebuild using:
    #     rebuild_metrics_from_snapshots_schema_only_fix16(prev_response, pool, web_context)
    #   when available.
    # - If it returns a non-empty dict, it *overrides* the previously computed
    #   current_metrics (additive override only when rebuild succeeded).
    # - Emits explicit debug fields for traceability.
    #
    # Non-negotiables:
    # - Does NOT alter fastpath logic.
    # - Only activates when fastpath is not taken (i.e., not authoritative reuse).
    # =====================================================================
    try:
        _fix41afc19_applied = False
        _fix41afc19_reason = ""
        _fix41afc19_fn_name = ""
        _fix41afc19_rebuilt_count = 0

        # Only consider override when fastpath is not active
        if not bool(locals().get("_fix31_authoritative_reuse")):
            # Attempt to locate the "current" snapshot pool (post-injection merge/attach)
            _fix41afc19_pool = (
                locals().get("baseline_sources_cache_current")
                or (output.get("baseline_sources_cache_current") if isinstance(output, dict) else None)
                or (output.get("results", {}).get("baseline_sources_cache_current") if isinstance(output, dict) else None)
                or locals().get("baseline_sources_cache")
                or locals().get("baseline_sources_cache_prefetched")
                or None
            )

            # =====================================================================
            # PATCH PH2B_S2 (ADDITIVE): Robust pool resolution for canonical rebuild
            # - Some pipelines store the post-attach merged pool under different locals()
            #   names (or only inside nested objects). If the pool is missed, FIX41AFC19
            #   appears "not applied" even on rebuild runs.
            # - We search locals() for any list-like baseline_sources_cache* variants and
            #   choose the largest plausible pool as a safe fallback.
            # =====================================================================
            if _fix41afc19_pool is None:
                try:
                    _cand_pools = []
                    for _k, _v in (locals() or {}).items():
                        if not isinstance(_k, str):
                            continue
                        if "baseline_sources_cache" in _k and isinstance(_v, list) and _v:
                            _cand_pools.append((_k, _v))
                    # Choose the largest pool (most likely post-attach merged universe)
                    if _cand_pools:
                        _cand_pools.sort(key=lambda kv: len(kv[1] or []), reverse=True)
                        _fix41afc19_pool = _cand_pools[0][1]
                        _fix41afc19_reason = (_fix41afc19_reason or "") + "|ph2b_s2_pool_fallback:" + str(_cand_pools[0][0])
                except Exception:
                    pass
            # =====================================================================

            # Prefer Analysis-canonical rebuild (Phase 2B hard-wire) when present; else fall back to FIX16 schema-only rebuild
            _fix41afc19_fn = globals().get("rebuild_metrics_from_snapshots_analysis_canonical_v1")
            if callable(_fix41afc19_fn):
                _fix41afc19_fn_name = "rebuild_metrics_from_snapshots_analysis_canonical_v1"
            else:
                _fix41afc19_fn = globals().get("rebuild_metrics_from_snapshots_schema_only_fix16")
                if callable(_fix41afc19_fn):
                    _fix41afc19_fn_name = "rebuild_metrics_from_snapshots_schema_only_fix16"
                else:
                    _fix41afc19_fn = None

            if callable(_fix41afc19_fn) and _fix41afc19_pool is not None:
                try:
                    _fix41afc19_rebuilt = _fix41afc19_fn(prev_response, _fix41afc19_pool, web_context=web_context)
                except TypeError:
                    # Backward-compat: older signature without web_context
                    _fix41afc19_rebuilt = _fix41afc19_fn(prev_response, _fix41afc19_pool)

                if isinstance(_fix41afc19_rebuilt, dict) and _fix41afc19_rebuilt:
                    current_metrics = dict(_fix41afc19_rebuilt)
                    _fix41afc19_applied = True
                    _fix41afc19_rebuilt_count = len(current_metrics)
                    _fix41afc19_reason = "override_current_metrics_with_fix16_anchor_rebuild"
                else:
                    _fix41afc19_reason = (_fix41afc19_reason or "") + "|rebuilt_empty_or_non_dict"
    except Exception:
        pass

    # Emit debug for FIX41AFC19 (non-breaking)
    try:
        if isinstance(output.get("debug"), dict):
            output["debug"].setdefault("fix41afc19", {})
            output["debug"]["fix41afc19"]["applied"] = bool(locals().get("_fix41afc19_applied"))
            output["debug"]["fix41afc19"]["reason"] = locals().get("_fix41afc19_reason") or ""
            output["debug"]["fix41afc19"]["fn"] = locals().get("_fix41afc19_fn_name") or ""
            output["debug"]["fix41afc19"]["rebuilt_count"] = int(locals().get("_fix41afc19_rebuilt_count") or 0)
    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH V19_HARDWIRE_EVO_CANONICAL (ADDITIVE)
    # Objective:
    # - Ensure Evolution diff "current" side is built from the same canonical semantics as Analysis
    #   by forcing a best-effort canonical rebuild for DISPLAY/DIFF, even when earlier logic
    #   skipped due to authoritative reuse or pool-resolution drift.
    # - Adds two minimal diagnostics:
    #   (2) debug.fix41afc19 truth table (attempted/applied/skip_reason + keys_sample + pool_count)
    #   (3) debug.evo_winner_trace_v1 for key EV metrics (winner provenance + top3 candidate glimpse)
    # Safety:
    # - Additive-only. Does not modify hashing inputs or snapshot attach. Affects only what diff renders.
    # =====================================================================
    _fix41afc19_attempted_v19 = False
    _fix41afc19_skip_reason_v19 = ""
    _fix41afc19_pool_count_v19 = 0
    _fix41afc19_keys_sample_v19 = []
    _fix41afc19_winner_trace_v19 = {}

    try:
        # Start with whatever earlier stage produced
        current_metrics_for_display = locals().get("current_metrics") if isinstance(locals().get("current_metrics"), dict) else {}

        # Force a display rebuild if earlier FIX41AFC19 did not apply or rebuilt_count==0
        _already_applied = bool(locals().get("_fix41afc19_applied"))
        _already_count = int(locals().get("_fix41afc19_rebuilt_count") or 0)

        if (not _already_applied) or (_already_count <= 0):
            _fix41afc19_attempted_v19 = True

            # Resolve the best available snapshot pool (post-attach merged universe)
            _pool = (
                locals().get("baseline_sources_cache_current")
                or (output.get("baseline_sources_cache_current") if isinstance(output, dict) else None)
                or (output.get("results", {}).get("baseline_sources_cache_current") if isinstance(output, dict) else None)
                or locals().get("baseline_sources_cache")
                or locals().get("baseline_sources_cache_prefetched")
                or None
            )

            if _pool is None:
                try:
                    _cand_pools = []
                    for _k, _v in (locals() or {}).items():
                        if not isinstance(_k, str):
                            continue
                        if "baseline_sources_cache" in _k and isinstance(_v, list) and _v:
                            _cand_pools.append((_k, _v))
                    if _cand_pools:
                        _cand_pools.sort(key=lambda kv: len(kv[1] or []), reverse=True)
                        _pool = _cand_pools[0][1]
                except Exception:
                    _pool = None

            if isinstance(_pool, list):
                _fix41afc19_pool_count_v19 = len(_pool or [])

            # Pick best available rebuild fn
            _fn = globals().get("rebuild_metrics_from_snapshots_analysis_canonical_v1")
            _fn_name = "rebuild_metrics_from_snapshots_analysis_canonical_v1"
            if not callable(_fn):
                _fn = globals().get("rebuild_metrics_from_snapshots_schema_only_fix16")
                _fn_name = "rebuild_metrics_from_snapshots_schema_only_fix16"

            if not callable(_fn):
                _fix41afc19_skip_reason_v19 = "fn_missing"
            elif _pool is None:
                _fix41afc19_skip_reason_v19 = "pool_missing"
            elif not isinstance(_pool, list) or not _pool:
                _fix41afc19_skip_reason_v19 = "pool_empty"
            else:
                try:
                    try:
                        _rebuilt = _fn(prev_response, _pool, web_context=web_context)
                    except TypeError:
                        _rebuilt = _fn(prev_response, _pool)
                except Exception as _e:
                    _rebuilt = None
                    _fix41afc19_skip_reason_v19 = "rebuild_exception:" + str(type(_e).__name__)

                if isinstance(_rebuilt, dict) and _rebuilt:
                    current_metrics_for_display = dict(_rebuilt)
                    _fix41afc19_skip_reason_v19 = "applied_v19_display_rebuild:" + str(_fn_name)
                    try:
                        _fix41afc19_keys_sample_v19 = list(current_metrics_for_display.keys())[:10]
                    except Exception:
                        _fix41afc19_keys_sample_v19 = []
                else:
                    _fix41afc19_skip_reason_v19 = _fix41afc19_skip_reason_v19 or "rebuilt_empty_or_non_dict"

        # Apply display override (used by diff below)
        locals()["current_metrics"] = current_metrics_for_display  # keep variable name used by downstream diff
        try:
            _fix41afc19_keys_sample_v19 = _fix41afc19_keys_sample_v19 or (list(current_metrics_for_display.keys())[:10] if isinstance(current_metrics_for_display, dict) else [])
        except Exception:
            pass

        # ---------------- Diagnostic (3): winner provenance trace for key EV metrics ----------------
        # Heuristic: pick the known canonical keys if present, else infer from schema/keys.
        _key_candidates = [
            "units_sold_2024__unit_sales",
            "market_share_2024__percent",
            "projected_market_share_2026__percent",
            "projected_market_share_2030__percent",
            "yoy_growth_rate_2024__percent",
            "cagr_2024__percent",
        ]

        try:
            prev_canon = (prev_response or {}).get("primary_metrics_canonical") if isinstance(prev_response, dict) else {}
            if not isinstance(prev_canon, dict):
                prev_canon = {}

            cur_canon = current_metrics_for_display if isinstance(current_metrics_for_display, dict) else {}

            # Infer projected-share keys if the exact ones are not present
            if isinstance(prev_canon, dict) and isinstance(cur_canon, dict):
                all_keys = set(list(prev_canon.keys()) + list(cur_canon.keys()))
                for k in sorted(all_keys):
                    lk = k.lower()
                    if ("project" in lk or "proj" in lk) and ("share" in lk or "market_share" in lk) and ("2026" in lk or "2030" in lk) and k not in _key_candidates:
                        _key_candidates.append(k)

            # Flatten snapshot candidates once (for top3 glimpse)
            flat = []
            try:
                _pool_for_flat = locals().get("baseline_sources_cache") if isinstance(locals().get("baseline_sources_cache"), list) else []
                for sr in _pool_for_flat or []:
                    if isinstance(sr, dict):
                        for c in (sr.get("extracted_numbers") or []):
                            if isinstance(c, dict):
                                flat.append(c)
            except Exception:
                flat = []

            def _cand_has_unit_evidence(c: dict) -> bool:
                try:
                    if (c.get("unit_tag") or c.get("unit") or c.get("unit_norm") or c.get("unit_raw") or "").strip():
                        return True
                    if (c.get("currency") or c.get("currency_symbol") or "").strip():
                        return True
                    if c.get("is_percent") or c.get("has_percent"):
                        return True
                    if (c.get("base_unit") or "").strip():
                        return True
                    if (c.get("unit_family") or "").strip():
                        return True
                    if isinstance(c.get("unit_tokens"), list) and c.get("unit_tokens"):
                        return True
                except Exception:
                    return False
                return False

            # Schema lookup (if available)
            schema = {}
            try:
                if isinstance(previous_data, dict):
                    pr = previous_data.get("primary_response") if isinstance(previous_data.get("primary_response"), dict) else previous_data
                    schema = (pr.get("metric_schema_frozen") or {}) if isinstance(pr, dict) else {}
            except Exception:
                schema = {}

            def _score_candidate_for_key(c: dict, ckey: str) -> int:
                try:
                    md = schema.get(ckey) if isinstance(schema, dict) else {}
                    kws = []
                    if isinstance(md, dict):
                        kws = md.get("keywords") or md.get("keyword_hints") or []
                    kws = [str(k).lower() for k in kws if str(k).strip()]
                    ctx = (c.get("context") or c.get("window") or c.get("context_window") or "").lower()
                    s = 0
                    for k in kws[:25]:
                        if k and k in ctx:
                            s += 1
                    if _cand_has_unit_evidence(c):
                        s += 5
                    # Prefer anchor-matching if both present
                    try:
                        if (c.get("anchor_hash") or "") and isinstance(md, dict) and (md.get("anchor_hash") or ""):
                            if str(c.get("anchor_hash")) == str(md.get("anchor_hash")):
                                s += 10
                    except Exception:
                        pass
                    return int(s)
                except Exception:
                    return 0

            # Build traces for up to 5 keys that exist in either prev/cur
            _keys_for_trace = []
            for k in _key_candidates:
                if k in (prev_canon or {}) or k in (cur_canon or {}):
                    _keys_for_trace.append(k)
                if len(_keys_for_trace) >= 5:
                    break
            if not _keys_for_trace:
                # fallback: first 5 keys from prev canonical
                _keys_for_trace = list(prev_canon.keys())[:5]

            for k in _keys_for_trace:
                pv = (prev_canon.get(k) or {}) if isinstance(prev_canon, dict) else {}
                cv = (cur_canon.get(k) or {}) if isinstance(cur_canon, dict) else {}

                winner_src = "unknown"
                if _fix41afc19_attempted_v19 and "applied_v19_display_rebuild" in (_fix41afc19_skip_reason_v19 or ""):
                    winner_src = "analysis_canonical_rebuild_v19"
                elif bool(locals().get("_fix41afc19_applied")):
                    winner_src = "analysis_canonical_rebuild"
                else:
                    winner_src = "fallback_snapshot_selector"

                # Top3 glimpse from flat pool
                top3 = []
                try:
                    scored = sorted(flat, key=lambda c: _score_candidate_for_key(c, k), reverse=True)[:3]
                    for t in scored:
                        top3.append({
                            "raw": t.get("raw"),
                            "value_norm": t.get("value_norm"),
                            "unit_tag": t.get("unit_tag") or t.get("unit") or "",
                            "has_unit_evidence": bool(_cand_has_unit_evidence(t)),
                            "anchor_hash": t.get("anchor_hash"),
                        })
                except Exception:
                    top3 = []

                _fix41afc19_winner_trace_v19[k] = {
                    "winner_source": winner_src,
                    "prev_value_norm": pv.get("value_norm"),
                    "prev_unit": pv.get("unit") or pv.get("unit_tag") or "",
                    "cur_value_norm": cv.get("value_norm"),
                    "cur_unit": cv.get("unit") or cv.get("unit_tag") or "",
                    "cur_has_unit_evidence": bool((cv.get("unit") or cv.get("unit_tag") or "").strip()),
                    "top3_candidates_glimpse": top3,
                }
        except Exception:
            pass

    except Exception:
        pass

    # ---------------- Diagnostic (2): FIX41AFC19 truth table ----------------
    try:
        if isinstance(output.get("debug"), dict):
            output["debug"].setdefault("fix41afc19", {})
            output["debug"]["fix41afc19"]["attempted"] = bool(_fix41afc19_attempted_v19)
            # Keep the pre-existing 'applied' field as-is, but add applied_v19_display_override
            output["debug"]["fix41afc19"]["applied_v19_display_override"] = bool(_fix41afc19_attempted_v19 and ("applied_v19_display_rebuild" in (_fix41afc19_skip_reason_v19 or "")))
            # Never allow blank reason: prefer v19 skip_reason when earlier reason is empty
            _prev_reason = str(output["debug"]["fix41afc19"].get("reason") or "")
            if (not _prev_reason.strip()) and (_fix41afc19_skip_reason_v19 or "").strip():
                output["debug"]["fix41afc19"]["reason"] = str(_fix41afc19_skip_reason_v19)
            output["debug"]["fix41afc19"]["skip_reason_v19"] = str(_fix41afc19_skip_reason_v19 or "")
            output["debug"]["fix41afc19"]["pool_count_v19"] = int(_fix41afc19_pool_count_v19 or 0)
            output["debug"]["fix41afc19"]["rebuilt_keys_sample_v19"] = list(_fix41afc19_keys_sample_v19 or [])
            if _fix41afc19_winner_trace_v19:
                output["debug"]["evo_winner_trace_v1"] = _fix41afc19_winner_trace_v19
    except Exception:
        pass
    # =====================================================================
    # =====================================================================
    # PATCH V20_CANONICAL_FOR_RENDER (ADDITIVE): make Evolution dashboard derive
    # "Current" from a canonical-for-render payload (analysis-aligned) WITHOUT
    # touching fastpath/hashing/snapshot-attach.
    #
    # Why:
    # - Evolution UI renders from diff rows (metric_changes), not analysis key-metrics.
    # - If diff rows source "Current" from raw extracted pools, unitless survivors
    #   (e.g., 170, 2) can win.
    # - We compute a late, render-only canonical dict from the frozen snapshot pool
    #   using the same rebuild semantics as analysis (best effort), then force the
    #   diff + row hydration to use it.
    #
    # Safety:
    # - Purely post-snapshot, post-hash: affects ONLY dashboard/diff rendering.
    # - Does NOT alter source selection, hashing inputs, injection lifecycle, fastpath.
    #
    # Diagnostics:
    # - output.debug.canonical_for_render_v1
    # - output.debug.canonical_for_render_row_audit_v1
    # =====================================================================
    _canonical_for_render_applied = False
    _canonical_for_render_reason = ""
    _canonical_for_render_fn = ""
    _canonical_for_render_count = 0
    _canonical_for_render_keys_sample = []
    _canonical_for_render_replaced_current_metrics = False

    canonical_for_render = {}
    try:
        # Default: use whatever current_metrics we already have
        canonical_for_render = current_metrics if isinstance(current_metrics, dict) else {}
        # =====================================================================
        # PATCH V30_CANONICAL_FOR_RENDER_SEED_DISABLE (ADDITIVE)
        # Goal:
        # - Stop seeding canonical_for_render from current_metrics because current_metrics may already
        #   contain year-like / unitless / junk winners (e.g., "2.0 B", "-6441").
        # - Force the downstream rebuild path (which is intended to be analysis-aligned) to run,
        #   while keeping fastpath/hashing/injection/snapshot attach untouched.
        #
        # Mechanism:
        # - If prev_response carries a frozen schema, disable the seed by default.
        # - Allow opt-out via env var EVO_CANONICAL_FOR_RENDER_ALLOW_SEED=1.
        # - Emit a small trace later via _canonical_for_render_reason tag.
        # =====================================================================
        try:
            _allow_seed = str(os.getenv("EVO_CANONICAL_FOR_RENDER_ALLOW_SEED", "") or "").strip() in ("1", "true", "True", "yes", "YES")
            _has_schema = isinstance(prev_response, dict) and isinstance(prev_response.get("metric_schema_frozen") or {}, dict) and bool(prev_response.get("metric_schema_frozen"))
            if _has_schema and not _allow_seed:
                canonical_for_render = {}
                _canonical_for_render_reason = "v30_seed_disabled_force_rebuild"
        except Exception:
            pass

        # PATCH V21_CANONICAL_FOR_RENDER_SUSPICION (ADDITIVE):
        # Even when current_metrics has "enough" keys, it can still be junk (year-like/unitless winners).
        # Detect suspicious existing canonical dict and force a render-only rebuild in that case.
        def _v21_yearlike(x):
            try:
                if x is None:
                    return False
                fx = float(x)
                if abs(fx - round(fx)) < 1e-9:
                    ix = int(round(fx))
                    return 1900 <= ix <= 2105
                return False
            except Exception:
                return False

        def _v21_metric_suspicious(m):
            try:
                if not isinstance(m, dict):
                    return True
                u = (m.get("unit") or m.get("unit_tag") or "").strip()
                vn = m.get("value_norm")
                if (not u) and (_v21_yearlike(vn) or vn is None):
                    return True
                return False
            except Exception:
                return True

        _suspicious_existing = False
        try:
            if isinstance(canonical_for_render, dict) and canonical_for_render:
                _keys_sample_chk = list(sorted(list(canonical_for_render.keys())))[:25]
                _sus = 0
                _tot = 0
                for _k in _keys_sample_chk:
                    _tot += 1
                    if _v21_metric_suspicious(canonical_for_render.get(_k)):
                        _sus += 1
                if _tot > 0:
                    _suspicious_existing = (_sus / float(_tot)) >= 0.30
        except Exception:
            _suspicious_existing = False

        # Best-effort: rebuild canonical-for-render from frozen snapshots using analysis-aligned builder.
        # Apply when current canonical is missing/suspiciously small.
        _need_render_rebuild = (not isinstance(canonical_for_render, dict)) or (len(canonical_for_render) < 3) or bool(_suspicious_existing)
        if _need_render_rebuild and isinstance(baseline_sources_cache, list) and baseline_sources_cache:
            # Resolve schema/anchors/canon from prev_response (analysis baseline)
            _schema = {}
            _anchors = {}
            _prev_canon = {}
            try:
                if isinstance(prev_response, dict):
                    _schema = prev_response.get("metric_schema_frozen") or {}
                    _anchors = prev_response.get("metric_anchors") or {}
                    _prev_canon = prev_response.get("primary_metrics_canonical") or {}
            except Exception:
                _schema, _anchors, _prev_canon = {}, {}, {}

            # Choose the best available analysis-aligned rebuild function
            _fn = globals().get("rebuild_metrics_from_snapshots_analysis_canonical_v1")
            if callable(_fn):
                try:
                    canonical_for_render = _fn(baseline_sources_cache, _schema, _anchors, _prev_canon)
                    _canonical_for_render_fn = "rebuild_metrics_from_snapshots_analysis_canonical_v1"
                except Exception:
                    canonical_for_render = {}
            if (not canonical_for_render) and callable(globals().get("rebuild_metrics_from_snapshots_with_anchors_fix16")):
                try:
                    _fn2 = globals().get("rebuild_metrics_from_snapshots_with_anchors_fix16")
                    canonical_for_render = _fn2(baseline_sources_cache, _schema, _anchors, _prev_canon)
                    _canonical_for_render_fn = "rebuild_metrics_from_snapshots_with_anchors_fix16"
                except Exception:
                    canonical_for_render = {}
            if (not canonical_for_render) and callable(globals().get("rebuild_metrics_from_snapshots_schema_only")):
                try:
                    _fn3 = globals().get("rebuild_metrics_from_snapshots_schema_only")
                    canonical_for_render = _fn3(baseline_sources_cache, _schema)
                    _canonical_for_render_fn = "rebuild_metrics_from_snapshots_schema_only"
                except Exception:
                    canonical_for_render = {}

            if isinstance(canonical_for_render, dict) and canonical_for_render:
                _canonical_for_render_applied = True
                _canonical_for_render_reason = "applied_render_only_rebuild" if not bool(_suspicious_existing) else "applied_render_only_rebuild_forced_suspicious"
                _canonical_for_render_count = int(len(canonical_for_render))
                _canonical_for_render_keys_sample = list(sorted(list(canonical_for_render.keys())))[:12]
                _canonical_for_render_replaced_current_metrics = True
                # PATCH V22_CANONICAL_FOR_RENDER_FN_GUARD (ADDITIVE): ensure fn label is never empty when rebuild succeeded
                try:
                    if not str(_canonical_for_render_fn or "").strip():
                        _canonical_for_render_fn = "unknown_rebuild_fn"
                except Exception:
                    pass
            else:
                _canonical_for_render_reason = "render_rebuild_failed_or_empty"
        else:
            _canonical_for_render_reason = "used_existing_current_metrics" if not bool(_suspicious_existing) else "forced_render_rebuild_due_to_suspicious_existing_failed"
            _canonical_for_render_count = int(len(canonical_for_render)) if isinstance(canonical_for_render, dict) else 0
            _canonical_for_render_keys_sample = list(sorted(list(canonical_for_render.keys())))[:12] if isinstance(canonical_for_render, dict) else []
    except Exception:
        canonical_for_render = current_metrics if isinstance(current_metrics, dict) else {}
        _canonical_for_render_reason = "exception_fallback_existing"


    # =====================================================================
    # PATCH V28_FORCE_ANCHOR_PICK_FOR_RENDER (ADDITIVE)
    # Problem observed:
    # - canonical_for_render rebuild may select junk numbers from the frozen pool
    #   (e.g., GlobeNewswire footer "2B" or email fragments "-6441") when anchors
    #   are not strictly enforced.
    #
    # Fix:
    # - If prev_response provides metric_anchors for a canonical_key, forcibly
    #   resolve the exact anchored candidate from baseline_sources_cache by matching
    #   anchor_hash and use that to populate canonical_for_render for that key.
    #
    # Scope / Safety:
    # - Render-only: affects ONLY canonical_for_render (dashboard "Current")
    # - Does NOT touch hashing, fastpath, injection lifecycle, or snapshot attach.
    #
    # Diagnostics:
    # - output.debug.canonical_for_render_anchor_enforce_v28 (summary)
    # - per-metric cm["diag"]["v28_anchor_enforced"] (when applied)
    # =====================================================================
    _v28_anchor_enforce = {
        "attempted": False,
        "schema_keys": 0,
        "anchors_keys": 0,
        "hits": 0,
        "misses": 0,
        "hit_keys_sample": [],
        "miss_keys_sample": [],
        "note": "render-only anchor enforcement by anchor_hash against frozen extracted_numbers pool",
    }

    def _v28_iter_numbers_from_sources_cache(_sources_cache):
        try:
            for _src in (_sources_cache or []):
                if not isinstance(_src, dict):
                    continue
                _url = _src.get("url") or _src.get("source_url") or ""
                nums = _src.get("extracted_numbers") or []
                if isinstance(nums, list):
                    for _n in nums:
                        if isinstance(_n, dict):
                            yield _url, _n
        except Exception:
            return

    def _v28_pick_by_anchor_hash(_sources_cache, _anchor_hash: str):
        try:
            ah = str(_anchor_hash or "").strip()
            if not ah or ah == "None":
                return None
            for _url, _n in _v28_iter_numbers_from_sources_cache(_sources_cache):
                try:
                    if str(_n.get("anchor_hash") or "").strip() == ah:
                        out = dict(_n)
                        if _url and (not out.get("source_url")):
                            out["source_url"] = _url
                        return out
                except Exception:
                    continue
            return None
        except Exception:
            return None

    def _v28_schema_unit_label(_schema_row: dict) -> str:
        try:
            if not isinstance(_schema_row, dict):
                return ""
            # Prefer schema unit_tag (human-friendly) then unit
            u = (_schema_row.get("unit_tag") or _schema_row.get("unit") or "").strip()
            # Small convenience mapping
            if u == "M":
                return "million units"
            if u == "B":
                return "billion"
            return u
        except Exception:
            return ""

    try:
        if isinstance(canonical_for_render, dict) and isinstance(baseline_sources_cache, list) and baseline_sources_cache:
            _v28_anchor_enforce["attempted"] = True
            _schema = {}
            _anchors = {}
            try:
                if isinstance(prev_response, dict):
                    _schema = prev_response.get("metric_schema_frozen") or {}
                    _anchors = prev_response.get("metric_anchors") or {}
            except Exception:
                _schema, _anchors = {}, {}
            _v28_anchor_enforce["schema_keys"] = int(len(_schema)) if isinstance(_schema, dict) else 0
            _v28_anchor_enforce["anchors_keys"] = int(len(_anchors)) if isinstance(_anchors, dict) else 0

            if isinstance(_anchors, dict) and _anchors:
                for _ckey, _ainfo in list(_anchors.items()):
                    try:
                        if not _ckey:
                            continue
                        if not isinstance(_ainfo, dict):
                            continue
                        _ah = _ainfo.get("anchor_hash") or _ainfo.get("anchor") or ""
                        if not str(_ah or "").strip() or str(_ah) == "None":
                            continue

                        cand = _v28_pick_by_anchor_hash(baseline_sources_cache, _ah)
                        if not isinstance(cand, dict):
                            _v28_anchor_enforce["misses"] += 1
                            if len(_v28_anchor_enforce["miss_keys_sample"]) < 12:
                                _v28_anchor_enforce["miss_keys_sample"].append(str(_ckey))
                            continue

                        # Build minimal schema-aligned canonical metric
                        srow = _schema.get(_ckey) if isinstance(_schema, dict) else None
                        unit_lbl = _v28_schema_unit_label(srow if isinstance(srow, dict) else {})
                        vnorm = cand.get("value_norm")
                        if vnorm is None:
                            vnorm = cand.get("value")
                        raw = (cand.get("raw") or "").strip()
                        if not raw:
                            try:
                                if vnorm is not None and unit_lbl:
                                    raw = f"{vnorm} {unit_lbl}".strip()
                                elif vnorm is not None:
                                    raw = str(vnorm)
                            except Exception:
                                raw = ""

                        cm = canonical_for_render.get(_ckey) if isinstance(canonical_for_render, dict) else None
                        if not isinstance(cm, dict):
                            cm = {}
                        cm["value_norm"] = vnorm
                        cm["unit"] = unit_lbl
                        cm["unit_tag"] = unit_lbl
                        if raw:
                            cm["raw"] = raw
                        # Provide evidence and source hint
                        cm["source_url"] = cand.get("source_url") or cand.get("url") or _ainfo.get("source_url") or ""
                        cm["context_snippet"] = cand.get("context_snippet") or cand.get("context") or _ainfo.get("context_snippet") or ""
                        cm["evidence"] = [cand]
                        cm.setdefault("diag", {})
                        if isinstance(cm.get("diag"), dict):
                            cm["diag"]["v28_anchor_enforced"] = True
                            cm["diag"]["v28_anchor_hash"] = str(_ah)
                            cm["diag"]["v28_anchor_candidate_raw"] = cand.get("raw")
                            cm["diag"]["v28_anchor_candidate_unit"] = cand.get("unit") or cand.get("unit_tag") or ""
                            cm["diag"]["v28_anchor_candidate_value_norm"] = cand.get("value_norm")

                        canonical_for_render[_ckey] = cm
                        _v28_anchor_enforce["hits"] += 1
                        if len(_v28_anchor_enforce["hit_keys_sample"]) < 12:
                            _v28_anchor_enforce["hit_keys_sample"].append(str(_ckey))
                    except Exception:
                        continue
    except Exception:
        pass

    try:
        if isinstance(output.get("debug"), dict):
            output["debug"]["canonical_for_render_anchor_enforce_v28"] = _v28_anchor_enforce
    except Exception:
        pass
    # =====================================================================
    # END PATCH V28_FORCE_ANCHOR_PICK_FOR_RENDER
    # =====================================================================

    # =====================================================================
    # PATCH V29_CANONICAL_FOR_RENDER_SCHEMA_GATE_AND_JUNK_REJECT (ADDITIVE)
    # =====================================================================
    # Problem:
    # - canonical_for_render can still select "junk" numerics (e.g., footer phone
    #   fragments like -6441 or marketing magnitudes like 2B) as Current, because
    #   the frozen extracted_numbers pool is noisy and some late selection paths
    #   lack strict schema gating.
    #
    # Fix (render-only):
    # - Apply a strict schema-compatibility gate for canonical_for_render values.
    # - Hard-reject phone/contact/email/footer-like contexts.
    # - If the existing canonical_for_render metric is suspicious (unitless for
    #   unit-required dimensions, or unit-incompatible like "B" for percent),
    #   attempt to replace it with the best compatible candidate from the frozen
    #   extracted_numbers pool.
    #
    # Safety:
    # - Render-only: affects ONLY canonical_for_render (dashboard Current).
    # - Does NOT touch fastpath replay, hashing universe, injection lifecycle,
    #   snapshot attach, or extraction.
    #
    # Diagnostics:
    # - output.debug.canonical_for_render_schema_gate_v29 (summary)
    # - per-metric cm["diag"]["v29_schema_gate_*"] flags (when applied)
    # =====================================================================
    _v29_schema_gate = {
        "attempted": False,
        "canonical_keys": 0,
        "suspicious": 0,
        "replaced": 0,
        "kept": 0,
        "candidates_checked": 0,
        "replaced_keys_sample": [],
        "suspicious_keys_sample": [],
        "note": "render-only schema gate + junk reject on canonical_for_render",
    }

    def _v29_s(_x):
        try:
            return str(_x or "")
        except Exception:
            return ""

    def _v29_lower(_x):
        try:
            return _v29_s(_x).lower()
        except Exception:
            return ""

    def _v29_get_text_blob(*parts):
        try:
            out = []
            for p in parts:
                if not p:
                    continue
                if isinstance(p, (list, tuple)):
                    out.extend([_v29_s(z) for z in p if z])
                else:
                    out.append(_v29_s(p))
            return " ".join([z for z in out if z]).strip()
        except Exception:
            return ""

    def _v29_phoneish(text):
        # Catch common phone patterns including "+1-888-600-6441" and fragments.
        try:
            import re
            t = _v29_s(text)
            if not t:
                return False
            if re.search(r"\+\d[\d\-\s]{7,}\d", t):
                return True
            if re.search(r"\b\d{3}[-\s]\d{3}[-\s]\d{4}\b", t):
                return True
            if re.search(r"\bext\.?\s*\d+\b", t, re.I):
                return True
            return False
        except Exception:
            return False

    def _v29_junk_context(text):
        try:
            t = _v29_lower(text)
            if not t:
                return False
            junk_terms = [
                "contact", "email", "phone", "tel", "telephone", "fax", "call us",
                "press release", "copyright", "all rights reserved", "subscribe",
                "unsubscribe", "privacy policy", "terms of use", "cookie", "newsletter",
                "about us", "follow us", "for media", "media contact"
            ]
            if any(w in t for w in junk_terms):
                return True
            if _v29_phoneish(text):
                return True
            # Many PR footers include an email address
            if "@" in t and "." in t:
                return True
            return False
        except Exception:
            return False

    def _v29_unit_str(obj):
        try:
            if isinstance(obj, dict):
                return _v29_s(obj.get("unit") or obj.get("unit_tag") or obj.get("unit_cmp") or obj.get("cur_unit_cmp") or "")
            return ""
        except Exception:
            return ""

    def _v29_value_norm(obj):
        try:
            if isinstance(obj, dict):
                v = obj.get("value_norm")
                if v is None:
                    v = obj.get("cur_value_norm")
                if v is None:
                    v = obj.get("value")
                return v
            return None
        except Exception:
            return None

    def _v29_has_unit_evidence(obj):
        try:
            # Conservative: unit evidence if unit string non-empty OR raw contains %/$/€ etc.
            if not isinstance(obj, dict):
                return False
            u = _v29_unit_str(obj).strip()
            if u:
                return True
            raw = _v29_s(obj.get("raw") or "")
            if "%" in raw or "$" in raw or "€" in raw or "£" in raw:
                return True
            return False
        except Exception:
            return False

    def _v29_expected_dimension(schema_row):
        try:
            if isinstance(schema_row, dict):
                # Prefer FIX16 helper if present
                if "_fix16_expected_dimension" in globals():
                    return _fix16_expected_dimension(schema_row)
                return schema_row.get("dimension") or schema_row.get("unit_family") or ""
            return ""
        except Exception:
            return ""

    def _v29_schema_requires_unit(schema_row):
        try:
            dim = _v29_lower(_v29_expected_dimension(schema_row))
            if dim in ("currency", "percent", "rate", "ratio"):
                return True
            uf = _v29_lower(_v29_s(schema_row.get("unit_family") if isinstance(schema_row, dict) else ""))
            if uf in ("currency", "percent", "rate", "ratio"):
                return True
            # unit_sales / unit counts should have some "unit-ness"
            if "unit" in dim or "unit" in uf:
                return True
            return False
        except Exception:
            return False

    def _v29_unit_compatible(schema_row, cand_obj):
        try:
            if isinstance(schema_row, dict) and isinstance(cand_obj, dict):
                if "_fix16_unit_compatible" in globals():
                    return bool(_fix16_unit_compatible(schema_row, cand_obj))
            # fallback heuristic
            dim = _v29_lower(_v29_expected_dimension(schema_row))
            u = _v29_lower(_v29_unit_str(cand_obj))
            raw = _v29_lower(_v29_s(cand_obj.get("raw") or ""))
            if dim == "percent":
                return ("%"
                        in u) or ("percent" in u) or ("%" in raw)
            if dim == "currency":
                return any(sym in raw for sym in ["$", "€", "£"]) or ("usd" in u) or ("eur" in u) or ("sgd" in u) or ("currency" in u)
            if "unit" in dim:
                # must not be percent/currency-like
                if "%" in raw or "%" in u:
                    return False
                if any(sym in raw for sym in ["$", "€", "£"]):
                    return False
                # prefer explicit unit words
                if "unit" in u or "vehicle" in u or "car" in u or "sales" in u:
                    return True
                # allow million/billion with implied units only if raw/context says units/sales
                ctx = _v29_lower(_v29_get_text_blob(cand_obj.get("context_snippet"), cand_obj.get("context")))
                if ("unit" in ctx) or ("sales" in ctx) or ("vehicle" in ctx):
                    return True
                return False
            return True
        except Exception:
            return False

    def _v29_is_suspicious_current(schema_row, cm):
        try:
            if not isinstance(cm, dict):
                return True
            dim = _v29_lower(_v29_expected_dimension(schema_row))
            requires_unit = _v29_schema_requires_unit(schema_row)
            u = _v29_lower(_v29_unit_str(cm))
            v = _v29_value_norm(cm)
            blob = _v29_get_text_blob(cm.get("raw"), cm.get("context_snippet"), cm.get("source_url"))
            if _v29_junk_context(blob):
                return True
            if requires_unit and not _v29_has_unit_evidence(cm):
                return True
            # Percent metrics must not carry magnitude units like B/M
            if dim == "percent" and ("b" in u or "m" in u) and "%" not in u:
                return True
            if dim == "percent" and isinstance(v, (int, float)) and abs(float(v)) > 1000:
                return True
            # Unit sales must not be negative (phone fragments, ids)
            if "unit" in dim and isinstance(v, (int, float)) and float(v) < 0:
                return True
            return not _v29_unit_compatible(schema_row, cm)
        except Exception:
            return True

    def _v29_keywords(schema_row):
        try:
            import re
            if not isinstance(schema_row, dict):
                return []
            nm = _v29_lower(schema_row.get("name") or schema_row.get("label") or schema_row.get("display_name") or "")
            toks = [t for t in re.split(r"[^a-z0-9]+", nm) if t and len(t) >= 4]
            # prune common filler
            bad = set(["global", "projected", "market", "share", "sales", "volume", "units", "unit", "year"])
            toks = [t for t in toks if t not in bad]
            return toks[:10]
        except Exception:
            return []

    def _v29_score_candidate(schema_row, cand_obj):
        try:
            score = 0
            u = _v29_lower(_v29_unit_str(cand_obj))
            raw = _v29_lower(_v29_s(cand_obj.get("raw") or ""))
            ctx = _v29_lower(_v29_get_text_blob(cand_obj.get("context_snippet"), cand_obj.get("context")))
            dim = _v29_lower(_v29_expected_dimension(schema_row))
            if dim == "percent":
                if "%" in raw or "%" in u or "percent" in u:
                    score += 5
                if "b" in u or "m" in u:
                    score -= 4
            if "unit" in dim:
                if "unit" in ctx or "sales" in ctx or "vehicle" in ctx:
                    score += 3
                if "%" in raw or "$" in raw:
                    score -= 3
            if not _v29_junk_context(ctx + " " + raw):
                score += 1
            for kw in _v29_keywords(schema_row):
                if kw and kw in ctx:
                    score += 1
            return score
        except Exception:
            return 0

    try:
        if isinstance(canonical_for_render, dict) and isinstance(prev_response, dict) and isinstance(baseline_sources_cache, list) and baseline_sources_cache:
            _v29_schema_gate["attempted"] = True
            _schema = prev_response.get("metric_schema_frozen") or {}
            _v29_schema_gate["canonical_keys"] = int(len(canonical_for_render)) if isinstance(canonical_for_render, dict) else 0

            for _ckey, _cm in list(canonical_for_render.items()):
                try:
                    if not _ckey:
                        continue
                    srow = _schema.get(_ckey) if isinstance(_schema, dict) else None
                    if not isinstance(srow, dict):
                        # without schema, we cannot safely gate; keep
                        _v29_schema_gate["kept"] += 1
                        continue

                    if _v29_is_suspicious_current(srow, _cm):
                        _v29_schema_gate["suspicious"] += 1
                        if len(_v29_schema_gate["suspicious_keys_sample"]) < 12:
                            _v29_schema_gate["suspicious_keys_sample"].append(str(_ckey))

                        best = None
                        best_score = -10**9
                        # search frozen pool for compatible candidates
                        for cand in _v28_iter_numbers_from_sources_cache(baseline_sources_cache):
                            _v29_schema_gate["candidates_checked"] += 1
                            if not isinstance(cand, dict):
                                continue
                            blob = _v29_get_text_blob(cand.get("raw"), cand.get("context_snippet"), cand.get("context"), cand.get("source_url") or cand.get("url"))
                            if _v29_junk_context(blob):
                                continue
                            # normalize candidate
                            cobj = dict(cand)
                            # ensure value_norm present if possible
                            if cobj.get("value_norm") is None and cobj.get("value") is not None:
                                cobj["value_norm"] = cobj.get("value")
                            if not _v29_unit_compatible(srow, cobj):
                                continue
                            if _v29_schema_requires_unit(srow) and not _v29_has_unit_evidence(cobj):
                                continue
                            sc = _v29_score_candidate(srow, cobj)
                            if sc > best_score:
                                best_score = sc
                                best = cobj
                        if isinstance(best, dict):
                            prior_v = _v29_value_norm(_cm if isinstance(_cm, dict) else {})
                            prior_u = _v29_unit_str(_cm if isinstance(_cm, dict) else {})
                            # overwrite with best candidate
                            if not isinstance(_cm, dict):
                                _cm = {}
                            _cm["value_norm"] = best.get("value_norm")
                            _cm["unit"] = _v28_schema_unit_label(srow) or (_v29_unit_str(best) or _v28_schema_unit_label(srow))
                            _cm["unit_tag"] = _cm.get("unit")
                            if best.get("raw"):
                                _cm["raw"] = best.get("raw")
                            _cm["source_url"] = best.get("source_url") or best.get("url") or _cm.get("source_url") or ""
                            _cm["context_snippet"] = best.get("context_snippet") or best.get("context") or _cm.get("context_snippet") or ""
                            _cm["evidence"] = [best]
                            _cm.setdefault("diag", {})
                            if isinstance(_cm.get("diag"), dict):
                                _cm["diag"]["v29_schema_gate_replaced"] = True
                                _cm["diag"]["v29_schema_gate_prior_value_norm"] = prior_v
                                _cm["diag"]["v29_schema_gate_prior_unit"] = prior_u
                                _cm["diag"]["v29_schema_gate_best_score"] = best_score
                                _cm["diag"]["v29_schema_gate_best_raw"] = best.get("raw")
                                _cm["diag"]["v29_schema_gate_best_unit"] = best.get("unit") or best.get("unit_tag") or ""
                            canonical_for_render[_ckey] = _cm
                            _v29_schema_gate["replaced"] += 1
                            if len(_v29_schema_gate["replaced_keys_sample"]) < 12:
                                _v29_schema_gate["replaced_keys_sample"].append(str(_ckey))
                        else:
                            _v29_schema_gate["kept"] += 1
                    else:
                        _v29_schema_gate["kept"] += 1
                except Exception:
                    continue
    except Exception:
        pass

    try:
        if isinstance(output.get("debug"), dict):
            output["debug"]["canonical_for_render_schema_gate_v29"] = _v29_schema_gate
    except Exception:
        pass
    # =====================================================================
    # END PATCH V29_CANONICAL_FOR_RENDER_SCHEMA_GATE_AND_JUNK_REJECT
    # =====================================================================


# PATCH V22_CANONICAL_FOR_RENDER_NORMALIZE (ADDITIVE): normalize canonical_for_render metric dicts so that
    # downstream row hydration does not overwrite Current with blanks when the rebuilt dict uses alternate fields.
    # - Derives value_norm/unit/raw from common alternate keys and evidence entries.
    # - Purely render-layer enrichment; does NOT alter selection/hashing.
    def _v22_extract_numeric(v):
        try:
            if v is None:
                return None
            if isinstance(v, (int, float)):
                return float(v)
            s = str(v).strip()
            if not s:
                return None
            # strip commas
            s2 = s.replace(",", "")
            return float(s2)
        except Exception:
            return None

    def _v22_norm_metric(cm: dict) -> dict:
        try:
            if not isinstance(cm, dict):
                return cm
            # value_norm
            vn = cm.get("value_norm")
            if vn is None:
                for k in ("value", "value_num", "value_float", "norm_value", "canonical_value_norm"):
                    if k in cm and cm.get(k) is not None:
                        vn = cm.get(k)
                        break
            # evidence-derived
            if vn is None and isinstance(cm.get("evidence"), list) and cm.get("evidence"):
                try:
                    ev0 = cm.get("evidence")[0]
                    if isinstance(ev0, dict):
                        vn = ev0.get("value_norm") if ev0.get("value_norm") is not None else ev0.get("value")
                except Exception:
                    pass
            vn_f = _v22_extract_numeric(vn)
            if vn_f is not None:
                cm["value_norm"] = vn_f

            # unit
            unit = (cm.get("unit") or cm.get("unit_tag") or cm.get("unit_label") or "").strip()
            if (not unit) and isinstance(cm.get("evidence"), list) and cm.get("evidence"):
                try:
                    ev0 = cm.get("evidence")[0]
                    if isinstance(ev0, dict):
                        unit = (ev0.get("unit") or ev0.get("unit_tag") or "").strip()
                except Exception:
                    pass
            if unit:
                cm["unit"] = unit

            # raw/display
            raw = (cm.get("raw") or cm.get("value_raw") or cm.get("raw_value") or cm.get("display") or "").strip()
            if not raw:
                try:
                    if cm.get("value_norm") is not None and (cm.get("unit") or ""):
                        raw = f"{cm.get('value_norm')} {cm.get('unit')}".strip()
                    elif cm.get("value_norm") is not None:
                        raw = str(cm.get("value_norm"))
                except Exception:
                    raw = ""
            if raw:
                cm["raw"] = raw
            cm.setdefault("diag", {})
            if isinstance(cm.get("diag"), dict):
                cm["diag"].setdefault("v22_norm", True)
            return cm
        except Exception:
            return cm

    try:
        if isinstance(canonical_for_render, dict) and canonical_for_render:
            for _k, _m in list(canonical_for_render.items()):
                if isinstance(_m, dict):
                    canonical_for_render[_k] = _v22_norm_metric(_m)
    except Exception:
        pass


    # =====================================================================
    # PATCH V30_STRICT_SCHEMA_UNIT_GATE (ADDITIVE)
    # Goal:
    # - Apply an analysis-like schema/unit compatibility gate at render-time.
    # - Explicitly reject obviously incompatible unit evidence (e.g. "2.0 B" for a % metric,
    #   or unitless negatives like "-6441" for a unit_sales metric).
    #
    # Notes:
    # - Render-only: does not affect extraction, snapshot attach, hashing, or fastpath replay.
    # - Best-effort: only runs if FIX16 helpers are present.
    # =====================================================================
    _v30_strict_gate = {"attempted": False, "dropped": 0, "dropped_keys_sample": []}
    try:
        _v30_strict_gate["attempted"] = True
        _schema = {}
        try:
            if isinstance(prev_response, dict):
                _schema = prev_response.get("metric_schema_frozen") or {}
        except Exception:
            _schema = {}

        _fix16_exp = globals().get("_fix16_expected_dimension")
        _fix16_comp = globals().get("_fix16_unit_compatible")
        _fix16_has_unit = globals().get("_fix16_candidate_has_any_unit")
        if callable(_fix16_exp) and callable(_fix16_comp) and isinstance(_schema, dict) and _schema:
            _dropped = []
            for _ck, _m in list((canonical_for_render or {}).items()):
                if not isinstance(_m, dict):
                    continue
                _defn = _schema.get(_ck) or {}
                try:
                    _expected = _fix16_exp(_defn)
                except Exception:
                    _expected = None

                # pull unit evidence in the same style analysis expects
                _unit_tag = str(_m.get("unit_tag") or _m.get("unit") or "").strip()
                _raw = str(_m.get("raw") or _m.get("raw_value") or "").strip()
                _has_any = False
                try:
                    if callable(_fix16_has_unit):
                        _has_any = bool(_fix16_has_unit(_m))
                except Exception:
                    _has_any = bool(_unit_tag) or ("% " in (_raw + " ") or "$" in _raw)

                _ok = True
                try:
                    _ok = bool(_fix16_comp(_expected, _unit_tag, _has_any))
                except Exception:
                    _ok = True

                if not _ok:
                    _dropped.append(_ck)
                    try:
                        canonical_for_render.pop(_ck, None)
                    except Exception:
                        pass

            _v30_strict_gate["dropped"] = len(_dropped)
            _v30_strict_gate["dropped_keys_sample"] = list(_dropped[:12])
    except Exception:
        pass

    # Diff using existing diff helper if present, but FORCE cur_response to canonical-for-render.
    metric_changes = []
    try:
        fn_diff = globals().get("diff_metrics_by_name")
        if callable(fn_diff):
            cur_resp_for_diff = {"primary_metrics_canonical": canonical_for_render}
            # =====================================================================
            # PATCH V34E_DIFF_METRIC_CHANGES_ANCHOR_INPUT (ADDITIVE)
            # Ensure the diff layer receives metric_anchors for CURRENT so that
            # v34 anchor-hash secondary join can resolve drifting canonical_keys.
            # - Deterministic, inference-free: only uses anchor_hash already present
            #   on canonical_for_render entries (if any).
            # =====================================================================
            try:
                _cur_metric_anchors = {}
                if isinstance(canonical_for_render, dict):
                    for _ckey, _m in canonical_for_render.items():
                        if not isinstance(_ckey, str):
                            continue
                        if not isinstance(_m, dict):
                            continue
                        _ah = _m.get("anchor_hash")
                        if not _ah and isinstance(_m.get("anchor"), dict):
                            _ah = _m.get("anchor", {}).get("anchor_hash")
                        if not _ah and isinstance(_m.get("anchor_meta"), dict):
                            _ah = _m.get("anchor_meta", {}).get("anchor_hash")
                        # Only emit when present and non-null-ish
                        if _ah and str(_ah).lower() not in ("none", "null", ""):
                            _cur_metric_anchors[_ckey] = {"anchor_hash": str(_ah)}
                if _cur_metric_anchors:
                    cur_resp_for_diff["metric_anchors"] = _cur_metric_anchors
            except Exception:
                pass
            # =====================================================================
            # END PATCH V34E_DIFF_METRIC_CHANGES_ANCHOR_INPUT
            # =====================================================================


            # =====================================================================
            # PATCH V27_DISABLE_NUMERIC_INFERENCE_FLAG (ADDITIVE)
            # Signal to diff layer: when canonical-for-render is active, do NOT
            # infer/parse numeric values for CURRENT from free-form strings.
            # =====================================================================
            try:
                cur_resp_for_diff["_disable_numeric_inference_v27"] = True
            except Exception:
                pass
            # =====================================================================
            # END PATCH V27_DISABLE_NUMERIC_INFERENCE_FLAG
            # =====================================================================
            # =====================================================================
            # PATCH V24_STRICT_CKEY_FLAG (ADDITIVE)
            # When using canonical-for-render, force strict canonical_key identity matching in diff layer.
            # This prevents cross-metric substitution (e.g., 2.0 B / 170.0 / year values) from fallback matchers.
            # =====================================================================
            try:
                cur_resp_for_diff["_ph2b_strict_ckey_v24"] = True
            except Exception:
                pass
            metric_changes, unchanged, increased, decreased, found = fn_diff(prev_response, cur_resp_for_diff)
            # =====================================================================
            # PATCH V34E_DIFF_JOIN_SUMMARY_SURFACE (ADDITIVE)
            # Surface v34 join summary (if produced by diff layer) into top-level debug
            # so it appears in the Evolution JSON for audit.
            # =====================================================================
            try:
                _dj = None
                if isinstance(cur_resp_for_diff, dict):
                    _dj = (cur_resp_for_diff.get("debug") or {}).get("diff_join_anchor_v34")
                if isinstance(_dj, dict):
                    try:
                        if "debug" not in output or not isinstance(output.get("debug"), dict):
                            output["debug"] = {}
                    except Exception:
                        output["debug"] = {}
                    try:
                        output["debug"]["diff_join_anchor_v34"] = _dj
                    except Exception:
                        pass
            except Exception:
                pass
            # =====================================================================
            # END PATCH V34E_DIFF_JOIN_SUMMARY_SURFACE
            # =====================================================================


        else:
            metric_changes, unchanged, increased, decreased, found = ([], 0, 0, 0, 0)
    except Exception:
        metric_changes, unchanged, increased, decreased, found = ([], 0, 0, 0, 0)

    # Post-process diff rows: ensure "Current" fields are derived from canonical-for-render.
    _row_audit = {
        "rows_total": int(len(metric_changes or [])),
        "rows_hydrated": 0,
        "rows_missing_canonical": 0,
        "rows_skipped_missing_fields": 0,
        "rows_with_prior_current_overridden": 0,
    }
    try:
        if isinstance(metric_changes, list) and isinstance(canonical_for_render, dict):
            for row in metric_changes:
                if not isinstance(row, dict):
                    continue
                ckey = row.get("canonical_key") or row.get("canonical") or row.get("canonical_id") or ""
                if not ckey:
                    continue
                cm = canonical_for_render.get(ckey)
                if not isinstance(cm, dict):
                    _row_audit["rows_missing_canonical"] += 1
                    continue

                # PATCH V22_ROW_HYDRATE_GUARD (ADDITIVE): only override if canonical metric has usable fields
                # Prevents overwriting a previously non-empty current with blanks when canon metric is sparse.
                _cm_vn = cm.get("value_norm")
                _cm_unit = (cm.get("unit") or cm.get("unit_tag") or "").strip()
                _cm_raw = (cm.get("raw") or cm.get("value_raw") or cm.get("raw_value") or "").strip()
                if _cm_vn is None and (not _cm_raw):
                    # no usable canonical payload to hydrate from
                    _row_audit["rows_skipped_missing_fields"] += 1
                    row.setdefault("diag", {})
                    if isinstance(row.get("diag"), dict):
                        row["diag"].setdefault("canonical_for_render_v1", {})
                        row["diag"]["canonical_for_render_v1"]["applied"] = False
                        row["diag"]["canonical_for_render_v1"]["reason"] = "skipped_missing_canonical_fields"
                        row["diag"]["canonical_for_render_v1"]["fn"] = _canonical_for_render_fn
                    continue

                # Capture prior values for audit if we are overriding
                prior = {
                    "current_value": row.get("current_value"),
                    "current_value_norm": row.get("current_value_norm"),
                    "cur_value_norm": row.get("cur_value_norm"),
                    "cur_unit_cmp": row.get("cur_unit_cmp"),
                    "current_unit": row.get("current_unit"),
                }

                # Hydrate from canonical metric
                vnorm = cm.get("value_norm")
                unit = (cm.get("unit") or cm.get("unit_tag") or "").strip()
                raw = (cm.get("raw") or cm.get("value_raw") or cm.get("raw_value") or "").strip()
                if not raw:
                    # Build a lightweight display string when raw isn't available
                    try:
                        if vnorm is not None and unit:
                            raw = f"{vnorm} {unit}".strip()
                        elif vnorm is not None:
                            raw = str(vnorm)
                    except Exception:
                        raw = row.get("current_value") or ""

                # Apply canonical-for-render to diff row
                row["current_value_norm"] = vnorm
                row["cur_value_norm"] = vnorm
                row["current_unit"] = unit
                row["cur_unit_cmp"] = unit
                row["current_value"] = raw

                # PATCH V22_CLEAR_UNIT_MISMATCH_ON_CANON (ADDITIVE): if canonical-for-render provides
                # a schema-aligned unit+value, clear any prior unit_mismatch that came from raw/fallback.
                try:
                    if (vnorm is not None) and str(unit or "").strip():
                        if row.get("unit_mismatch") is True:
                            row["unit_mismatch"] = False
                        if row.get("change_type") in ("unit_mismatch", "invalid_current"):
                            pv = row.get("previous_value")
                            pvf = _v22_extract_numeric(pv)
                            cvf = _v22_extract_numeric(vnorm)
                            if pvf is not None and cvf is not None:
                                if abs(cvf - pvf) < 1e-9:
                                    row["change_type"] = "unchanged"
                                elif cvf > pvf:
                                    row["change_type"] = "increased"
                                else:
                                    row["change_type"] = "decreased"
                except Exception:
                    pass

                # Range fields (if present in canonical)
                if isinstance(cm.get("value_range"), dict):
                    row["current_value_range"] = cm.get("value_range")
                if (cm.get("value_range_display") or "").strip():
                    row["current_value_range_display"] = cm.get("value_range_display")

                # Audit stamp
                row.setdefault("diag", {})
                if isinstance(row.get("diag"), dict):
                    row["diag"].setdefault("canonical_for_render_v1", {})
                    row["diag"]["canonical_for_render_v1"]["applied"] = True
                    row["diag"]["canonical_for_render_v1"]["fn"] = _canonical_for_render_fn
                    row["diag"]["canonical_for_render_v1"]["reason"] = _canonical_for_render_reason
                    row["diag"]["canonical_for_render_v1"]["prior_current"] = prior

                # =====================================================================
                # PATCH V26_LOCK_CANONICAL_CURRENT_FIELDS (ADDITIVE)
                # Goal: Once a row is hydrated from canonical-for-render, lock the "Current" fields
                #       so later post-processing (heuristics/sanitizers) cannot overwrite them.
                # This is render-only and does NOT affect hashing/fastpath/snapshot attach.
                # =====================================================================
                try:
                    row["_lock_current_v26"] = True
                    # ensure nested dict exists
                    row.setdefault("diag", {})
                    if isinstance(row.get("diag"), dict):
                        row["diag"].setdefault("canonical_for_render_v1", {})
                        if isinstance(row["diag"].get("canonical_for_render_v1"), dict):
                            row["diag"]["canonical_for_render_v1"]["locked_current_v26"] = {
                                "current_value": row.get("current_value"),
                                "current_value_norm": row.get("current_value_norm"),
                                "cur_value_norm": row.get("cur_value_norm"),
                                "cur_unit_cmp": row.get("cur_unit_cmp"),
                                "current_unit": row.get("current_unit"),
                                "current_value_range": row.get("current_value_range"),
                                "current_value_range_display": row.get("current_value_range_display"),
                            }
                except Exception:
                    pass
                # =====================================================================
                # END PATCH V26_LOCK_CANONICAL_CURRENT_FIELDS
                # =====================================================================




                # Determine if we actually changed the row
                if prior.get("cur_value_norm") != vnorm or str(prior.get("cur_unit_cmp") or "") != unit or str(prior.get("current_value") or "") != raw:
                    _row_audit["rows_with_prior_current_overridden"] += 1
                _row_audit["rows_hydrated"] += 1
    except Exception:
        pass

    # Attach diagnostics for auditability
    try:
        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            output["debug"]["canonical_for_render_v1"] = {
                "applied": bool(_canonical_for_render_applied),
                "reason": str(_canonical_for_render_reason or ""),
                "fn": str(_canonical_for_render_fn or ""),
                "rebuilt_count": int(_canonical_for_render_count or 0),
                "keys_sample": list(_canonical_for_render_keys_sample or []),
                "replaced_current_metrics_for_render": bool(_canonical_for_render_replaced_current_metrics),
            }
            output["debug"]["canonical_for_render_row_audit_v1"] = _row_audit
    except Exception:
        pass
    # =====================================================================
    # END PATCH V20_CANONICAL_FOR_RENDER
    # =====================================================================

    output["metric_changes"] = metric_changes or []
    output["summary"]["total_metrics"] = len(output["metric_changes"])
    output["summary"]["metrics_found"] = int(found or 0)
    output["summary"]["metrics_increased"] = int(increased or 0)
    output["summary"]["metrics_decreased"] = int(decreased or 0)
    output["summary"]["metrics_unchanged"] = int(unchanged or 0)

    total = max(1, len(output["metric_changes"]))
    output["stability_score"] = (output["summary"]["metrics_unchanged"] / total) * 100.0

    output["source_results"] = baseline_sources_cache[:50]
    output["sources_checked"] = len(baseline_sources_cache)
    output["sources_fetched"] = len(baseline_sources_cache)

    try:
        total_nums = 0
        for sr in baseline_sources_cache or []:
            if isinstance(sr, dict) and isinstance(sr.get("extracted_numbers"), list):
                total_nums += len(sr.get("extracted_numbers") or [])
        output["numbers_extracted_total"] = int(total_nums)
    except Exception:
        pass

    output["message"] = "Source-anchored evolution completed (snapshot-gated, analysis-aligned)."
    output["interpretation"] = "Evolution used cached source snapshots only; no brute-force candidate harvesting."

    # =====================================================================
    # PATCH FIX35 (ADDITIVE): attach bad-current traces for unit-required metrics
    # - If a diff row shows a year-like integer as current for a unit-required metric,
    #   emit a compact trace: origin, schema unit_family, current fields, and top candidates.
    # =====================================================================
    try:
        if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
            bad_traces = {}
            # Build a flattened candidate pool once (from snapshots only)
            flat = []
            for sr in baseline_sources_cache or []:
                if isinstance(sr, dict):
                    for c in (sr.get("extracted_numbers") or []):
                        if isinstance(c, dict):
                            flat.append(c)

            def _is_yearlike(v):
                try:
                    iv = int(float(v))
                    return 1900 <= iv <= 2100
                except Exception:
                    return False

            def _schema_unit_required(md: dict, ckey: str = "") -> bool:
                uf = ((md or {}).get("unit_family") or (md or {}).get("unit") or "").strip().lower()
                if uf in {"currency", "percent", "rate", "ratio"}:
                    return True
                ck = (ckey or "").lower().strip()
                return ck.endswith("__currency") or ck.endswith("__percent") or ck.endswith("__rate") or ck.endswith("__ratio")

            def _cand_unit_evidence(c: dict) -> bool:
                if not isinstance(c, dict):
                    return False
                if (c.get("unit_tag") or c.get("unit") or c.get("unit_norm") or c.get("unit_raw") or "").strip():
                    return True
                if (c.get("currency") or c.get("currency_symbol") or "").strip():
                    return True
                if c.get("is_percent") or c.get("has_percent"):
                    return True
                if (c.get("base_unit") or "").strip():
                    return True
                if (c.get("unit_family") or "").strip():
                    return True
                if isinstance(c.get("unit_tokens"), list) and c.get("unit_tokens"):
                    return True
                return False

            schema = {}
            try:
                if isinstance(previous_data, dict):
                    pr = previous_data.get("primary_response") if isinstance(previous_data.get("primary_response"), dict) else previous_data
                    schema = (pr.get("metric_schema_frozen") or {}) if isinstance(pr, dict) else {}
            except Exception:
                schema = {}

            for row in output.get("metric_changes") or []:
                try:
                    ckey = row.get("canonical_key") or row.get("canonical") or ""
                    md = schema.get(ckey) if isinstance(schema, dict) else None
                    if not _schema_unit_required(md or {}, ckey):
                        continue

                    cur_val = row.get("current_value_norm")
                    cur_unit = (row.get("cur_unit_cmp") or row.get("current_unit") or "").strip()
                    if cur_val is None:
                        continue
                    if not _is_yearlike(cur_val):
                        continue
                    if cur_unit:
                        continue

                    kws = []
                    if isinstance(md, dict):
                        kws = md.get("keywords") or md.get("keyword_hints") or []
                    kws = [str(k).lower() for k in kws if str(k).strip()]

                    def _hit_score(c):
                        ctx = (c.get("context") or c.get("window") or c.get("context_window") or "").lower()
                        score = 0
                        for k in kws[:25]:
                            if k and k in ctx:
                                score += 1
                        if _cand_unit_evidence(c):
                            score += 5
                        if _is_yearlike(c.get("value_norm")) and not _cand_unit_evidence(c):
                            score -= 5
                        return score

                    top = sorted(flat, key=_hit_score, reverse=True)[:10]
                    bad_traces[ckey or row.get("name") or "unknown_metric"] = {
                        "current_value_norm": cur_val,
                        "cur_unit_cmp": cur_unit,
                        "schema_unit_family": (md or {}).get("unit_family") if isinstance(md, dict) else "",
                        "origin": output["debug"]["fix35"].get("current_metrics_origin", "unknown"),
                        "top_candidates": [
                            {
                                "raw": t.get("raw"),
                                "value_norm": t.get("value_norm"),
                                "unit_tag": t.get("unit_tag"),
                                "unit_family": t.get("unit_family"),
                                "base_unit": t.get("base_unit"),
                                "has_unit_evidence": bool(_cand_unit_evidence(t)),
                                "anchor_hash": t.get("anchor_hash"),
                            }
                            for t in top
                        ],
                    }
                except Exception:
                    continue

            if bad_traces:
                output["debug"]["fix35"]["bad_current_traces"] = bad_traces
                output["debug"]["fix35"]["bad_current_trace_count"] = len(bad_traces)
    except Exception:
        pass

    # =====================================================================
    # PATCH INJ_TRACE_V1_EMIT_EVOLUTION (ADDITIVE): always emit canonical trace
    # - Mirrors to output.results.debug.inj_trace_v1 for a fixed location across modes
    # - Does NOT affect fastpath decisioning
    # =====================================================================
    try:
        _wc_diag = {}
        if isinstance(web_context, dict):
            _wc_diag = web_context.get("diag_injected_urls") or {}
        _hash_inputs = _inj_diag_hash_inputs_from_bsc(baseline_sources_cache)

        # Determine path from existing fix35 origin stamp
        _path = ""
        try:
            origin = ""
            if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                origin = str(output["debug"]["fix35"].get("current_metrics_origin") or "")
            if "fastpath" in origin:
                _path = "fastpath"
            elif "rebuild" in origin:
                _path = "rebuild"
            else:
                _path = "unknown"
        except Exception:
            _path = "unknown"

        # Rebuild "selected" URLs: unique source_url from current metrics (if present)
        _selected = []
        try:
            cm = output.get("current_metrics")
            if isinstance(cm, dict):
                for v in cm.values():
                    if isinstance(v, dict):
                        u = (v.get("source_url") or "").strip()
                        if u:
                            _selected.append(u)
            _selected = sorted(set(_selected))
        except Exception:
            _selected = []

        # For evolution, rebuild_pool is effectively the hash input URL universe available via snapshots
        # =====================================================================
        # PATCH INJ_HASH_V1_EVO (ADDITIVE): compute per-URL hash exclusion reasons in evolution
        # - If injected URLs exist but are not in hash_inputs, we record the most likely reason:
        #     * excluded_by_flag_default_off  (when inclusion switch is OFF)
        #     * missing_from_hash_inputs      (when switch ON but still absent)
        # =====================================================================
        _evo_hash_reasons = {}
        try:
            _evo_persisted = []
            if isinstance(_wc_diag, dict):
                _evo_persisted = _inj_diag_norm_url_list(_wc_diag.get("persisted_norm") or _wc_diag.get("persisted") or [])
            _evo_inj = _inj_diag_norm_url_list(
                (_wc_diag.get("intake_norm") if isinstance(_wc_diag, dict) else []) or
                (_wc_diag.get("ui_norm") if isinstance(_wc_diag, dict) else []) or
                []
            )
            _evo_targets = _evo_persisted or _evo_inj
            _hashset = set(_inj_diag_norm_url_list(_hash_inputs or []))
            _incl = _inj_hash_policy_should_include(_evo_targets)
            for u in _evo_targets:
                if u in _hashset:
                    _evo_hash_reasons[u] = "present_in_hash_inputs"
                else:
                    _evo_hash_reasons[u] = ("excluded_by_policy_disable" if _inj_hash_policy_explicit_disable() else ("excluded_by_legacy_switch_default_off" if not _incl else "missing_from_hash_inputs"))
        except Exception:
            _evo_hash_reasons = {}
        # =====================================================================

        # =====================================================================
        # PATCH INJ_TRACE_V1_EVO_ADMISSION_ALIGN_V1 (ADDITIVE)
        # Goal:
        # - Evolution often bypasses fetch_web_context(), so "admitted" may be unset even
        #   when URLs are actually in the current scrape/hash universe.
        # - This patch makes inj_trace_v1 "admitted_norm" reflect the same practical
        #   universe used for scraping/hashing (without changing any control flow).
        #
        # Policy (diagnostics only):
        # - If diag.admitted is empty but hash_inputs are present, treat hash_inputs as
        #   admitted for trace purposes.
        # - Prefer any explicit FIX24 evo merge set if present (urls_after_merge_norm).
        # =====================================================================
        try:
            if isinstance(_wc_diag, dict):
                _ad = _inj_diag_norm_url_list(_wc_diag.get("admitted") or _wc_diag.get("extra_urls_admitted") or [])
                if not _ad:
                    _pref = []
                    try:
                        _pref = _inj_diag_norm_url_list(_wc_diag.get("urls_after_merge_norm") or [])
                    except Exception:
                        _pref = []
                    if not _pref:
                        _pref = _inj_diag_norm_url_list(_hash_inputs or [])
                    if _pref:
                        _wc_diag["admitted"] = list(_pref)
                        _wc_diag.setdefault("admission_reason", "trace_fallback_to_hash_inputs_or_urls_after_merge")
        except Exception:
            pass
        # =====================================================================

        # =====================================================================
        # PATCH INJ_TRACE_V1_EVO_REBUILD_SELECTED_FALLBACK_V1 (ADDITIVE)
        # Goal:
        # - In fastpath/replay or when current_metrics lacks source_url fields,
        #   rebuild_selected_norm can be empty, creating misleading pool_minus_selected.
        #
        # Diagnostics-only fallback:
        # - If rebuild_selected is empty but rebuild_pool/hash_inputs exists, treat
        #   selected as the full pool for trace purposes.
        # =====================================================================
        try:
            if (not _selected) and _hash_inputs:
                _selected = list(_inj_diag_norm_url_list(_hash_inputs))
                if isinstance(_wc_diag, dict):
                    _wc_diag.setdefault("rebuild_selected_reason", "trace_fallback_to_hash_inputs_no_current_metric_sources")
        except Exception:
            pass
        # =====================================================================


        # =====================================================================
        # PATCH FIX41AFC12 (ADDITIVE): Admission-gate override for injected URLs + post-fetch trace
        #
        # Why:
        # - inj_trace_v1 shows injected URLs at intake but missing from admitted (unknown_rejected_pre_admission).
        # - We must "pin" injection at the admission boundary for evolution (delta-only), and emit a post-fetch trace
        #   because inj_trace_v1 may be emitted before the fetch/persist stage completes.
        #
        # What:
        # - If injected URLs are present (from web_context.extra_urls OR diag ui fields) we force-add them into
        #   _wc_diag["admitted"] so the trace reflects admission override deterministically (delta-only).
        # - Additionally, emit inj_trace_v2_postfetch using best-effort enrichment from scraped_meta / cur_bsc if available.
        #
        # Safety:
        # - Purely additive; never raises; does not modify fastpath rules or hashing.
        # =====================================================================
        try:
            _fx12_wc = web_context if isinstance(web_context, dict) else {}
            _fx12_diag = _wc_diag if isinstance(locals().get("_wc_diag"), dict) else (_fx12_wc.get("diag_injected_urls") if isinstance(_fx12_wc.get("diag_injected_urls"), dict) else {})
            _fx12_inj_raw = []
            try:
                _fx12_inj_raw = list(_fx12_wc.get("extra_urls") or [])
            except Exception:
                _fx12_inj_raw = []
            if not _fx12_inj_raw and isinstance(_fx12_diag, dict):
                try:
                    _fx12_inj_raw = list(_fx12_diag.get("ui_norm") or _fx12_diag.get("intake_norm") or [])
                except Exception:
                    _fx12_inj_raw = []
            _fx12_inj = _inj_diag_norm_url_list(_fx12_inj_raw or [])
            if _fx12_inj and isinstance(_wc_diag, dict):
                _fx12_prev_ad = _inj_diag_norm_url_list(_wc_diag.get("admitted") or [])
                _fx12_forced = sorted(list(set(_fx12_inj) - set(_fx12_prev_ad)))
                if _fx12_forced:
                    _wc_diag["admitted"] = list(_inj_diag_stable_dedupe_order((_fx12_prev_ad or []) + _fx12_forced))
                    _wc_diag.setdefault("forced_admit_reasons", {})
                    if isinstance(_wc_diag.get("forced_admit_reasons"), dict):
                        for _u in _fx12_forced:
                            _wc_diag["forced_admit_reasons"][_u] = "forced_admit_injected_url_override"
                    try:
                        output.setdefault("debug", {})
                        if isinstance(output.get("debug"), dict):
                            output["debug"].setdefault("fix41afc12", {})
                            if isinstance(output["debug"].get("fix41afc12"), dict):
                                output["debug"]["fix41afc12"].update({
                                    "forced_admit_injected_count": int(len(_fx12_forced)),
                                    "forced_admit_injected_urls": list(_fx12_forced),
                                })
                    except Exception:
                        pass
        except Exception:
            pass
        # =====================================================================
        _trace = _inj_trace_v1_build(
            diag_injected_urls=_wc_diag if isinstance(_wc_diag, dict) else {},
            hash_inputs=_hash_inputs,
            stage="evolution",
            path=_path,
            rebuild_pool=_hash_inputs,
            rebuild_selected=_selected,
            hash_exclusion_reasons=_evo_hash_reasons,
        )

        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            output["debug"]["inj_trace_v1"] = _trace

        # Fixed location mirror: results.debug.inj_trace_v1
        output.setdefault("results", {})
        if isinstance(output.get("results"), dict):
            output["results"].setdefault("debug", {})
            if isinstance(output["results"].get("debug"), dict):
                output["results"]["debug"]["inj_trace_v1"] = _trace

                # =====================================================================
                # PATCH FIX41AFC12_POSTFETCH (ADDITIVE): inj_trace_v2_postfetch
                #
                # Emit a second trace after best-effort enrichment from scraped_meta / baseline cache so that
                # attempted/persisted deltas reflect the true post-fetch state (inj_trace_v1 may be earlier).
                # =====================================================================
                try:
                    _fx12_diag2 = dict(_wc_diag) if isinstance(_wc_diag, dict) else {}
                    try:
                        _sm = locals().get("scraped_meta")
                        if isinstance(_sm, dict):
                            _fx12_diag2 = _inj_trace_v1_enrich_diag_from_scraped_meta(_fx12_diag2, _sm, (_inj_extra_urls or []))
                    except Exception:
                        pass
                    try:
                        _cb = locals().get("cur_bsc") or locals().get("baseline_sources_cache") or locals().get("baseline_sources_cache_current")
                        if isinstance(_cb, list):
                            _fx12_diag2 = _inj_trace_v1_enrich_diag_from_bsc(_fx12_diag2, _cb)
                    except Exception:
                        pass
                    _trace2 = _inj_trace_v1_build(
                        diag_injected_urls=_fx12_diag2 if isinstance(_fx12_diag2, dict) else {},
                        hash_inputs=_hash_inputs,
                        stage="evolution",
                        path=str(_path or "evolution") + "_postfetch",
                        rebuild_pool=_hash_inputs,
                        rebuild_selected=_selected,
                        hash_exclusion_reasons=_evo_hash_reasons,
                    )
                    output["debug"]["inj_trace_v2_postfetch"] = _trace2
                    if isinstance(output.get("results"), dict) and isinstance(output["results"].get("debug"), dict):
                        output["results"]["debug"]["inj_trace_v2_postfetch"] = _trace2
                except Exception:
                    pass
                # =====================================================================
    except Exception:
        pass
    # =====================================================================


    # =====================================================================
    # PATCH FIX41AFC19_V25 (ADDITIVE): Dashboard "Current" source audit + row sample
    #
    # Why:
    # - Conclusively identify which structure the Evolution dashboard reads for
    #   the "Current" column, and whether upstream patches are modifying that
    #   exact structure.
    #
    # What:
    # - Emit a compact debug payload that:
    #     * states the dashboard read-path ("results.metric_changes[].current_value")
    #     * samples the first N metric_changes rows (canonical_key, current_value, unit hints, diag keys)
    # - Purely additive: no selection, hashing, or fastpath behavior changes.
    # =====================================================================
    try:
        if isinstance(output, dict):
            _dbg = output.get("debug") if isinstance(output.get("debug"), dict) else {}
            if not isinstance(_dbg, dict):
                _dbg = {}
            rows = output.get("metric_changes") or []
            _sample = []
            if isinstance(rows, list):
                for _r in rows[:25]:
                    if not isinstance(_r, dict):
                        continue
                    _diag = _r.get("diag") if isinstance(_r.get("diag"), dict) else {}
                    _sample.append({
                        "canonical_key": _r.get("canonical_key"),
                        "name": _r.get("metric") or _r.get("name"),
                        "current_value": _r.get("current_value"),
                        "current_value_norm": (_r.get("current_value_norm") if _r.get("current_value_norm") is not None else _r.get("cur_value_norm")),
                        "cur_unit_cmp": (_r.get("cur_unit_cmp") if _r.get("cur_unit_cmp") is not None else _r.get("current_unit")),
                        "anchor_used": _r.get("anchor_used"),
                        "unit_mismatch": _r.get("unit_mismatch"),
                        "diag_keys": (list(_diag.keys()) if isinstance(_diag, dict) else []),
                    })
            _dbg["dashboard_current_source_v25"] = {
                "dashboard_reads": "results.metric_changes[].current_value",
                "rows_sample_n": len(_sample),
                "rows_sample": _sample,
            }
            _dbg["canonical_for_render_present_v25"] = bool(_dbg.get("canonical_for_render_v1"))

            # =====================================================================
            # PATCH V33_DIFF_PANEL_CANONICAL_PATHS_EMIT (ADDITIVE)
            # Surface which response-shape path diff used to hydrate "current".
            # =====================================================================
            try:
                _paths = None
                try:
                    _paths = cur_response.get("_diff_panel_canonical_paths_v33") if isinstance(cur_response, dict) else None
                except Exception:
                    _paths = None
                if isinstance(_paths, dict) and _paths:
                    _dbg["diff_panel_canonical_paths_v33"] = _paths
            except Exception:
                pass

            output["debug"] = _dbg
    except Exception:
        pass
    # =====================================================================
    # END PATCH FIX41AFC19_V25
    # =====================================================================


    # =====================================================================
    # PATCH V26_RESTORE_LOCKED_CURRENT_FIELDS (ADDITIVE)
    # If any later code overwrote current_* fields, restore the locked canonical-for-render
    # fields right before returning the evolution output.
    # =====================================================================
    try:
        _lock_dbg = {"rows_total": 0, "rows_locked": 0, "rows_restored": 0, "rows_missing_lock": 0, "restored_keys_sample": []}
        for _r in (output.get("metric_changes") or []):
            if not isinstance(_r, dict):
                continue
            _lock_dbg["rows_total"] += 1
            if not _r.get("_lock_current_v26"):
                continue
            _lock_dbg["rows_locked"] += 1
            _d = _r.get("diag") if isinstance(_r.get("diag"), dict) else {}
            _lc = None
            try:
                if isinstance(_d, dict):
                    _cfr = _d.get("canonical_for_render_v1") if isinstance(_d.get("canonical_for_render_v1"), dict) else {}
                    _lc = _cfr.get("locked_current_v26") if isinstance(_cfr, dict) else None
            except Exception:
                _lc = None
            if not isinstance(_lc, dict):
                _lock_dbg["rows_missing_lock"] += 1
                continue
            # If current fields differ from locked, restore
            _changed = False
            for _k in ("current_value", "current_value_norm", "cur_value_norm", "cur_unit_cmp", "current_unit",
                       "current_value_range", "current_value_range_display"):
                if _k in _lc:
                    if _r.get(_k) != _lc.get(_k):
                        _r[_k] = _lc.get(_k)
                        _changed = True
            if _changed:
                _lock_dbg["rows_restored"] += 1
                ck = _r.get("canonical_key") or _r.get("canonical") or ""
                if ck and len(_lock_dbg["restored_keys_sample"]) < 20:
                    _lock_dbg["restored_keys_sample"].append(str(ck))
        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            output["debug"]["lock_current_v26"] = _lock_dbg
    except Exception:
        pass
    # =====================================================================
    # END PATCH V26_RESTORE_LOCKED_CURRENT_FIELDS
    # =====================================================================


    return output
def rebuild_metrics_from_snapshots_schema_only(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    Minimal deterministic rebuild:
      - Uses ONLY: baseline_sources_cache + frozen schema (or derives schema from canonical metrics)
      - No re-fetch
      - No heuristic name fallback outside schema fields
      - Deterministic selection + ordering

    Returns a dict shaped like primary_metrics_canonical (by canonical_key).
    """
    import re

    if not isinstance(prev_response, dict):
        return {}

    # 1) Obtain frozen schema (required contract for schema-driven rebuild)
    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
    )

    # If schema is missing but canonical metrics exist, derive schema deterministically
    if not metric_schema:
        try:
            canon = (
                prev_response.get("primary_metrics_canonical")
                or (prev_response.get("primary_response") or {}).get("primary_metrics_canonical")
                or (prev_response.get("results") or {}).get("primary_metrics_canonical")
            )
            fn_freeze = globals().get("freeze_metric_schema")
            if canon and callable(fn_freeze):
                metric_schema = fn_freeze(canon)
        except Exception:
            metric_schema = None

    if not isinstance(metric_schema, dict) or not metric_schema:
        return {}

    # 2) Flatten candidates (must come from snapshots/cache, no re-fetch)
    candidates = []
    if isinstance(baseline_sources_cache, dict) and isinstance(baseline_sources_cache.get("snapshots"), list):
        source_entries = baseline_sources_cache.get("snapshots", [])
    elif isinstance(baseline_sources_cache, list):
        source_entries = baseline_sources_cache
    else:
        source_entries = []

    # Candidate normalization helpers
    def _norm(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def _unit_family_guess(unit: str) -> str:
        u = (unit or "").strip().lower()
        if u in ("%", "percent", "percentage"):
            return "percent"
        if any(x in u for x in ("usd", "$", "eur", "gbp", "jpy", "cny", "aud", "sgd")):
            return "currency"
        if any(x in u for x in ("unit", "units", "vehicle", "vehicles", "car", "cars", "kwh", "mwh", "gwh", "twh")):
            return "quantity"
        return ""

    # Prefer already-extracted numbers in snapshots; otherwise optionally extract from stored text if present.
    fn_extract = globals().get("extract_numbers_with_context")

    for s in source_entries:
        if not isinstance(s, dict):
            continue
        url = s.get("source_url") or s.get("url") or ""
        xs = s.get("extracted_numbers")
        if isinstance(xs, list) and xs:
            for c in xs:
                if isinstance(c, dict):
                    c2 = dict(c)
                    c2.setdefault("source_url", url)
                    candidates.append(c2)
            continue

        # Optional: if snapshot stores text, we can extract deterministically (no re-fetch).
        txt = s.get("text") or s.get("raw_text") or s.get("content_text") or ""
        if txt and callable(fn_extract):
            try:
                xs2 = fn_extract(txt, source_url=url)
                if isinstance(xs2, list):
                    for c in xs2:
                        if isinstance(c, dict):
                            c2 = dict(c)
                            c2.setdefault("source_url", url)
                            candidates.append(c2)
            except Exception:
                pass

    # Drop junk + enforce deterministic ordering
    def _cand_sort_key(c: dict):
        return (
            str(c.get("anchor_hash") or ""),
            str(c.get("source_url") or ""),
            int(c.get("start_idx") or 0),
            str(c.get("raw") or ""),
            str(c.get("unit") or ""),
            float(c.get("value_norm") or 0.0),
        )

    filtered = []
    for c in candidates:
        if not isinstance(c, dict):
            continue
        if c.get("is_junk") is True:
            continue
        filtered.append(c)

    filtered.sort(key=_cand_sort_key)

    # =====================================================================
    # PATCH FIX27 (ADDITIVE): Strict schema-only eligibility gate to prevent
    # bare-year tokens (e.g., "2024") from winning typed metrics (currency,
    # percent, unit_sales) due to keyword overlap.
    #
    # Key rules (token-level evidence):
    #   - If raw looks like a bare year (1900–2100) AND token has no unit
    #     evidence, reject for non-year metrics.
    #   - For currency metrics, require explicit currency evidence on the token.
    #   - For percent metrics, require explicit percent evidence on the token.
    #   - For unit/unit_sales metrics, require some unit evidence on the token.
    # NOTE: This is enforced inside schema-only selection, BEFORE scoring.
    # =====================================================================

    def _fix27_is_bare_year_token(raw_token: str, value_norm):
        try:
            s = (raw_token or "").strip()
            # Allow formats like "2024" or "2024.0"
            if re.fullmatch(r"\d{4}(?:\.0+)?", s):
                y = int(float(s))
                return 1900 <= y <= 2100
        except Exception:
            pass
        # fallback: numeric year-like
        try:
            if value_norm is not None:
                y = int(float(value_norm))
                if abs(float(value_norm) - float(y)) < 1e-9:
                    return 1900 <= y <= 2100
        except Exception:
            pass
        return False

    def _fix27_expected_kind(canonical_key: str, sch: dict) -> str:
        ck = (canonical_key or "").lower()
        dim = ((sch or {}).get("dimension") or (sch or {}).get("unit_family") or "").lower().strip()
        if ck.endswith("__percent") or "percent" in dim:
            return "percent"
        if ck.endswith("__currency") or "currency" in dim or "money" in dim:
            return "currency"
        if ck.endswith("__unit_sales") or "unit" in ck or "sales" in ck:
            return "unit"
        if "year" in dim or ck.endswith("__year"):
            return "year"
        return "other"

    def _fix27_has_currency_evidence(c: dict) -> bool:
        raw = (c.get("raw") or "")
        u = (c.get("unit") or "")
        bu = (c.get("base_unit") or "")
        ut = (c.get("unit_tag") or "")
        blob = f"{raw} {u} {bu} {ut}".lower()
        # Token-level currency signals
        if "$" in raw:
            return True
        for t in ("usd", "eur", "gbp", "sgd", "aud", "cad", "chf", "jpy", "cny", "rmb", "hkd", "inr", "krw"):
            if re.search(rf"\b{t}\b", blob):
                return True
        return False

    def _fix27_has_percent_evidence(c: dict) -> bool:
        raw = (c.get("raw") or "")
        u = (c.get("unit") or "")
        bu = (c.get("base_unit") or "")
        ut = (c.get("unit_tag") or "")
        blob = f"{raw} {u} {bu} {ut}".lower()
        return ("%" in raw) or ("%" in blob) or ("percent" in blob)

    def _fix27_has_any_unit_evidence(c: dict) -> bool:
        # Any unit evidence captured on the token (not schema defaults)
        return bool((c.get("unit") or "").strip() or (c.get("base_unit") or "").strip() or (c.get("unit_tag") or "").strip())
    # 3) Schema-driven selection
    rebuilt = {}
    for canonical_key, sch in metric_schema.items():
        if not isinstance(sch, dict):
            continue

        # Schema tokens/keywords
        name = sch.get("name") or canonical_key
        keywords = sch.get("keywords") or sch.get("keyword_hints") or []
        if isinstance(keywords, str):
            keywords = [keywords]
        kw_norm = [_norm(k) for k in keywords if k]

        # Expected family/dimension
        expected_dim = (sch.get("dimension") or sch.get("unit_family") or "").lower().strip()
        expected_kind = _fix27_expected_kind(canonical_key, sch)  # PATCH FIX27

        best = None
        best_score = None

        for c in filtered:
            ctx = _norm(c.get("context") or c.get("context_window") or "")
            raw = _norm(c.get("raw") or "")
            unit = c.get("unit") or ""
            fam = _unit_family_guess(unit)
            if expected_dim and fam and expected_dim not in (fam,):
                # strict family check when we can infer a family
                continue

            score = 0
            # keyword match only from schema-provided keywords
            for k in kw_norm:
                if k and (k in ctx or k in raw):
                    score += 10

            # Prefer candidates that have an anchor_hash (stability)
            if c.get("anchor_hash"):
                score += 1

            # Deterministic tie-breakers: earlier in list wins if equal score
            if best is None or score > best_score:
                best = c
                best_score = score

        if best is None or (best_score is not None and best_score <= 0):
            # No schema-consistent evidence found -> omit (or could mark proxy; keep minimal here)
            continue

        rebuilt[canonical_key] = {
            "canonical_key": canonical_key,
            "name": name,
            "value": best.get("value"),
            "unit": best.get("unit") or "",
            "value_norm": best.get("value_norm"),
            "source_url": best.get("source_url") or "",
            "anchor_hash": best.get("anchor_hash") or "",
            "evidence": [{
                "source_url": best.get("source_url") or "",
                "raw": best.get("raw") or "",
                "context_snippet": (best.get("context") or best.get("context_window") or "")[:400],
                "anchor_hash": best.get("anchor_hash") or "",
                "method": "schema_only_rebuild",
            }],
        }

    return rebuilt
# =================== END PATCH RMS_CORE1 (ADDITIVE) ===================





def extract_context_keywords(metric_name: str) -> List[str]:
    """
    General-purpose keyword extraction for matching metric names to page contexts.

    Goals:
    - Work for ANY topic (not tourism-specific)
    - Keep deterministic behavior
    - Extract years/quarters, key financial/stat terms, and meaningful tokens
    """
    if not metric_name:
        return []

    name = str(metric_name)
    n = name.lower()

    keywords: List[str] = []

    # Years (e.g., 2019, 2024)
    years = re.findall(r"\b(19\d{2}|20\d{2})\b", name)
    keywords.extend(years)

    # Quarters / time buckets
    q = re.findall(r"\bq[1-4]\b", n)
    keywords.extend([x.upper() for x in q])

    # Common metric concepts (broad, cross-industry)
    concept_phrases = [
        "market size", "revenue", "sales", "turnover", "profit", "operating profit",
        "ebit", "ebitda", "net income", "gross margin", "margin",
        "growth", "yoy", "cagr", "share", "penetration",
        "forecast", "projected", "projection", "estimate", "expected",
        "actual", "baseline", "target",
        "volume", "units", "shipments", "users", "subscribers", "visitors",
        "price", "asp", "arpu", "aov",
        "inflation", "gdp", "unemployment", "interest rate"
    ]
    for p in concept_phrases:
        if p in n:
            keywords.append(p)

    # Units / scales that help matching
    unit_hints = ["trillion", "billion", "million", "thousand", "%", "percent"]
    for u in unit_hints:
        if u in n:
            keywords.append(u)

    # Tokenize remaining meaningful words
    tokens = re.findall(r"[a-z0-9]+", n)
    stop = {
        "the","and","or","of","in","to","for","by","from","with","on","at","as",
        "total","overall","average","avg","number","rate","value","amount",
        "annual","year","years","monthly","month","daily","day","quarter","quarters"
    }
    for t in tokens:
        if t in stop:
            continue
        if len(t) <= 2:
            continue
        keywords.append(t)

    # De-dup, keep stable ordering
    seen = set()
    out = []
    for k in keywords:
        if k and k not in seen:
            seen.add(k)
            out.append(k)

    return out[:30]

def extract_numbers_with_context(text, source_url: str = "", max_results: int = 350):
    """
    Extract numeric candidates with context windows (analysis-aligned, hardened).

    Fixes / tightening:
    - ALWAYS returns a list (never None)  ✅ critical for snapshots & evolution
    - Strips HTML tags/scripts/styles if HTML-like
    - Nav/chrome/junk rejection (analytics, cookie banners, menus, footers, etc.)
    - Suppress year-only candidates (e.g., "2024") unless clearly a metric
    - Suppress ID-like long integers, phone-like patterns, DOI/ISBN-like contexts
    - Captures currency + scale + percent + common magnitude suffixes
    - Adds anchor_hash for stable matching
    """
    import re
    import hashlib

    if not text or not str(text).strip():
        return []

    raw = str(text)

    # ---------- helpers ----------
    def _sha1(s: str) -> str:
        return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()

    def _normalize_unit(u: str) -> str:
        u = (u or "").strip()
        if not u:
            return ""
        ul = u.lower().replace(" ", "")

        # Energy units (must come before magnitude)
        if "twh" in ul:
            return "TWh"
        if "gwh" in ul:
            return "GWh"
        if "mwh" in ul:
            return "MWh"
        if "kwh" in ul:
            return "kWh"
        if ul == "wh":
            return "Wh"

        # Magnitudes (case-insensitive; fix: accept single-letter suffixes)
        if ul in ("bn", "billion", "b"):
            return "B"
        if ul in ("mn", "mio", "million", "m"):
            return "M"
        if ul in ("k", "thousand", "000"):
            return "K"
        if ul in ("trillion", "tn", "t"):
            return "T"

        if ul in ("pct", "percent", "%"):
            return "%"

        return u

    def _looks_html(s: str) -> bool:
        sl = s.lower()
        return ("<html" in sl) or ("<div" in sl) or ("<p" in sl) or ("<script" in sl) or ("</" in sl)

    def _html_to_text(s: str) -> str:
        # Prefer BeautifulSoup if available
        try:
            from bs4 import BeautifulSoup  # type: ignore
            soup = BeautifulSoup(s, "html.parser")
            for tag in soup(["script", "style", "noscript", "svg", "canvas", "iframe", "header", "footer", "nav", "form"]):
                try:
                    tag.decompose()
                except Exception:
                    pass
            txt = soup.get_text(separator=" ", strip=True)
            txt = re.sub(r"\s+", " ", txt).strip()
            return txt
        except Exception:
            # fallback: cheap strip
            s2 = re.sub(r"(?is)<(script|style|noscript).*?>.*?</\1>", " ", s)
            s2 = re.sub(r"(?is)<[^>]+>", " ", s2)
            s2 = re.sub(r"\s+", " ", s2).strip()
            return s2

    def _is_phone_like(ctx: str, rawnum: str) -> bool:
        # strict phone pattern or phone keywords nearby
        if re.search(r"\b\d{3}-\d{3}-\d{4}\b", rawnum):
            return True
        c = (ctx or "").lower()
        if any(k in c for k in ["call", "phone", "tel:", "telephone", "contact us", "whatsapp"]):
            if re.search(r"\b\d{7,}\b", rawnum):
                return True
        return False

    def _is_id_like(val_str: str, ctx: str) -> bool:
        # very long digit strings typically IDs, unless explicitly monetary with symbols
        digits = re.sub(r"\D", "", val_str or "")
        if len(digits) >= 13:
            c = (ctx or "").lower()
            if any(k in c for k in ["isbn", "doi", "issn", "arxiv", "repec", "id:", "order", "invoice", "reference"]):
                return True
            # generic ID-like (too many digits)
            return True
        return False

    def _chrome_junk(ctx: str) -> bool:
        c = (ctx or "").lower()
        # common site chrome / analytics / cookie / nav junk
        bad = [
            "googleanalyticsobject", "gtag(", "googletagmanager", "analytics", "doubleclick",
            "cookie", "consent", "privacy", "terms", "copyright", "all rights reserved",
            "subscribe", "newsletter", "sign in", "login", "menu", "search", "breadcrumb",
            "share this", "follow us", "social media", "footer", "header", "nav", "sitemap"
        ]
        if any(b in c for b in bad):
            return True
        # css/js-like
        if any(b in c for b in ["function(", "var ", "const ", "let ", "webpack", "sourcemappingurl", ".css", "{", "};"]):
            return True
        # low alpha ratio
        if len(c) > 80:
            letters = sum(ch.isalpha() for ch in c)
            if letters / max(1, len(c)) < 0.18:
                return True
        return False

    def _year_only_suppression(num: float, unit: str, rawnum: str, ctx: str) -> bool:
        # suppress standalone 4-digit years like 2024 with no unit/currency
        if unit:
            return False
        s = (rawnum or "").strip()
        if re.fullmatch(r"\d{4}", s):
            year = int(s)
            if 1900 <= year <= 2099:
                c = (ctx or "").lower()
                allow_kw = ["cagr", "growth", "inflation", "gdp", "revenue", "market", "sales", "shipments", "capacity"]
                if not any(k in c for k in allow_kw):
                    return True
        return False

    # -------------------------------------------------------------------------
    # ADDITIVE (Patch A1): fix common "split year" artifact (e.g., "202 5" -> "2025")
    # Do this AFTER HTML->text and BEFORE regex extraction.
    # -------------------------------------------------------------------------

    # ---------- normalize to visible text ----------
    if _looks_html(raw):
        raw = _html_to_text(raw)

    # cap huge pages
    raw = raw[:250_000]

    # ---- ADDITIVE: fix common "split year" artifact (e.g., "202 5" -> "2025") ----
    raw = re.sub(r"\b((?:19|20)\d)\s+(\d)\b", r"\1\2", raw)
    # -----------------------------------------------------------------------------

    # -------------------------------------------------------------------------
    # PATCH A3 (ADDITIVE): year-range detector (tag-only, does NOT drop candidates)
    # -------------------------------------------------------------------------
    def _is_year_range_context(ctx: str) -> bool:
        return bool(re.search(r"\b(19|20)\d{2}\s*(?:-|–|—|to)\s*(19|20)\d{2}\b", ctx or "", flags=re.I))

    # -------------------------------------------------------------------------
    # ADDITIVE (Patch A2): non-destructive junk tagger
    # - We DO NOT filter here; we tag and downstream excludes by default.
    # -------------------------------------------------------------------------
    def _junk_tag(value: float, unit: str, raw_disp: str, ctx: str):
        """
        Non-destructive junk classifier.
        Returns (is_junk: bool, reason: str).
        """
        c = (ctx or "").lower()
        u = (unit or "").strip()

        # =========================
        # PATCH A3 (TAG ONLY): year-range endpoints are usually timeline metadata
        # =========================
        try:
            iv = int(float(value))
            if u == "" and 1900 <= iv <= 2099 and _is_year_range_context(ctx):
                return True, "year_range"
        except Exception:
            pass

        nav_hits = [
            "skip to content", "menu", "search", "login", "sign in", "sign up",
            "subscribe", "newsletter", "cookie", "privacy", "terms", "copyright",
            "all rights reserved", "back to top", "next", "previous", "page ",
            "home", "about", "contact", "sitemap", "breadcrumb"
        ]
        if any(h in c for h in nav_hits):
            try:
                if u == "" and abs(float(value)) <= 20:
                    return True, "nav_small_int"
            except Exception:
                pass

        if u == "":
            try:
                if abs(float(value)) <= 12:
                    if any(h in c for h in ["•", "–", "step", "chapter", "section", "item", "no."]):
                        return True, "enumeration_small_int"
            except Exception:
                pass

        if u == "":
            try:
                iv = int(abs(float(value)))
                if 190 <= iv <= 209:
                    if any(x in (raw_disp or "") for x in ["202", "203", "204", "205", "206", "207", "208", "209"]):
                        return True, "year_fragment_3digit"
            except Exception:
                pass

        return False, ""

    # -------------------------------------------------------------------------
    # PATCH M1 (ADDITIVE): semantic classifier for associations like "share" vs "units"
    # NOTE: moved OUTSIDE the loop for determinism + speed (no behavioral change).
    # Also emits a "measure_assoc" label that downstream can display easily.
    # -------------------------------------------------------------------------
    def _classify_measure(unit_tag: str, ctx: str):
        """
        Returns (measure_kind, measure_assoc):
          - measure_kind: stable internal tag (share_pct / growth_pct / count_units / money / etc.)
          - measure_assoc: human-meaning label ("share", "growth", "units", "money", "energy", etc.)
        """
        c = (ctx or "").lower()
        ut = (unit_tag or "").strip()

        if ut == "%":
            if any(k in c for k in ["market share", "share of", "share", "penetration", "portion", "contribution"]):
                return "share_pct", "share"
            if any(k in c for k in ["growth", "cagr", "increase", "decrease", "yoy", "mom", "qoq", "rate"]):
                return "growth_pct", "growth"
            return "percent_other", "percent"

        if ut in ("K", "M", "B", "T", ""):
            if any(k in c for k in ["units", "unit", "vehicles", "cars", "sold", "sales volume", "shipments", "deliveries", "registrations"]):
                return "count_units", "units"
            if any(k in c for k in ["revenue", "sales ($", "usd", "$", "market size", "valuation", "turnover"]):
                return "money", "money"
            return "magnitude_other", "magnitude"

        if ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
            return "energy", "energy"

        return "other", "other"
    # -------------------------------------------------------------------------

    # ---------- extraction pattern ----------
    # =========================
    # PATCH N1 (ADDITIVE, BUGFIX): currency tokens
    # - Fix US$ being parsed as S$ by matching US\$ first.
    # - Also accept "US$" as a single token (case-insensitive).
    # =========================
    pat = re.compile(
        r"(US\$|US\$(?!\w)|S\$|\$|USD|SGD|EUR|€|GBP|£)?\s*"
        # =========================
        # PATCH N2 (ADDITIVE, BUGFIX): avoid capturing negative year from year-range
        # - We'll still allow negatives generally, but we'll tag the special "2025-2030" case below.
        # (No behavior change for real negatives like -1.2% etc.)
        # =========================
        r"(-?\d{1,3}(?:,\d{3})*(?:\.\d+)?|-?\d+(?:\.\d+)?)(?!\d)\s*"
        # =========================
        # PATCH N3 (ADDITIVE, BUGFIX): capture 'tn' magnitude explicitly
        # - Keep your A5 safeguard: single-letter magnitudes only match if NOT followed by a letter.
        # =========================
        r"(TWh|GWh|MWh|kWh|Wh|tn|(?:T|B|M|K)(?![A-Za-z])|trillion|billion|million|bn|mn|%|percent)?",
        flags=re.I
    )

    out = []
    for m in pat.finditer(raw):
        cur = (m.group(1) or "").strip()
        num_s = (m.group(2) or "").strip()
        unit_s = (m.group(3) or "").strip()

        if not num_s:
            continue

        start = max(0, m.start() - 160)
        end = min(len(raw), m.end() + 160)
        ctx = raw[start:end].replace("\n", " ")
        ctx = re.sub(r"\s+", " ", ctx).strip()
        ctx_store = ctx[:240]

        # numeric parse
        try:
            val = float(num_s.replace(",", ""))
        except Exception:
            continue

        # normalize unit
        unit = _normalize_unit(unit_s)

        raw_disp = f"{cur} {num_s}{unit_s}".strip()
        raw_num_only = (cur + num_s).strip()

        if _chrome_junk(ctx_store):
            continue
        if _is_phone_like(ctx_store, raw_disp):
            continue
        if _is_id_like(raw_disp, ctx_store):
            continue
        if _year_only_suppression(val, unit, num_s, ctx_store):
            continue

        # =========================
        # PATCH N2b (ADDITIVE, BUGFIX): tag the "negative year from range" case as junk
        # Example: "CAGR 2025-2030" producing "-2030"
        # - Do NOT drop here (keep non-destructive policy); just tag.
        # =========================
        neg_year_from_range = False
        try:
            if num_s.startswith("-"):
                iv = int(abs(float(val)))
                if 1900 <= iv <= 2099:
                    # look immediately behind the match for a digit (the "2025" in "2025-2030")
                    if m.start() > 0 and raw[m.start() - 1].isdigit():
                        neg_year_from_range = True
        except Exception:
            neg_year_from_range = False
        # =========================

        anchor_hash = _sha1(f"{source_url}|{raw_disp}|{ctx_store}")
        is_junk, junk_reason = _junk_tag(val, unit, raw_disp, ctx_store)

        # =========================
        # PATCH N2c (ADDITIVE): override junk tagging reason when we confidently detect this bug
        # =========================
        if neg_year_from_range:
            is_junk = True
            junk_reason = "year_range_negative_endpoint"
        # =========================


            # =================================================================
            # PATCH YEAR_ONLY_V2 (ADDITIVE): suppress standalone years as datapoints
            # Why:
            # - Years (e.g., 2025) frequently appear in headings/ranges and should not
            #   compete with real metric values (currency, %, volumes) in evolution.
            # - We keep years only if there is strong metric context nearby.
            # Rules:
            # - If value is an integer-like 4-digit year in [1900..2100],
            #   unit is empty, and context lacks currency/%/magnitude cues => mark junk.
            # =================================================================
            try:
                if (not is_junk) and (not str(unit or "").strip()):
                    _v_int = None
                    try:
                        _v_int = int(float(val)) if val is not None else None
                    except Exception:
                        _v_int = None

                    if _v_int is not None and 1900 <= _v_int <= 2100:
                        _ctx = str(ctx_store or "")
                        _ctx_l = _ctx.lower()

                        # Strong numeric-metric cues that should keep the candidate
                        _keep_cue = False
                        try:
                            if re.search(r"[$€£¥]|\b(usd|sgd|eur|gbp|jpy|aud|cad|chf)\b", _ctx_l):
                                _keep_cue = True
                            elif re.search(r"%|\b(cagr|yoy|growth|increase|decrease)\b", _ctx_l):
                                _keep_cue = True
                            elif re.search(r"\b(million|billion|trillion|mn|bn|m|b)\b", _ctx_l):
                                _keep_cue = True
                            elif re.search(r"\b(kwh|mwh|gwh|twh|mw|gw|tw|kg|tonnes?|mt|kt)\b", _ctx_l):
                                _keep_cue = True
                            elif re.search(r"\b(revenue|sales|market\s*size|valuation|profit|earnings)\b", _ctx_l):
                                _keep_cue = True
                        except Exception:
                            _keep_cue = False

                        if not _keep_cue:
                            is_junk = True
                            junk_reason = "year_only"
            except Exception:
                pass
            # =================================================================
# semantic association tags
        measure_kind, measure_assoc = _classify_measure(unit, ctx_store)

        out.append({
            "value": val,
            "unit": unit,
            "raw": raw_disp,
            "source_url": source_url,
            "context": ctx_store,
            "context_snippet": ctx_store,
            "anchor_hash": anchor_hash,

            "is_junk": bool(is_junk),
            "junk_reason": junk_reason,
            "start_idx": int(m.start()),
            "end_idx": int(m.end()),

            "measure_kind": measure_kind,
            "measure_assoc": measure_assoc,
        })

        if len(out) >= int(max_results or 350):
            break

    return out



def extract_numbers_with_context_pdf(text):
    """
    PDF-specialized extractor wrapper.

    Tightening changes (v7.29+):
    - Inherit the year-only rejection from extract_numbers_with_context().
    - Keep boilerplate filters; prefer metric/table-like contexts.
    """
    import re

    if not text:
        return []

    base = extract_numbers_with_context(text) or []

    def _bad_pdf_context(ctx):
        c = (ctx or "").lower()
        bad = [
            "issn", "isbn", "doi", "catalogue", "legal notice",
            "all rights reserved", "reproduction is authorised",
            "printed by", "manuscript completed", "©", "copyright",
            "table of contents"
        ]
        return any(b in c for b in bad)

    def _good_pdf_context(ctx):
        c = (ctx or "").lower()
        # Lightweight heuristic: "table-ish" or "metric-ish"
        good = [
            "market", "revenue", "sales", "capacity", "generation", "growth",
            "cagr", "forecast", "projection", "increase", "decrease",
            "percent", "%", "billion", "million", "trillion", "usd", "eur", "gbp", "sgd"
        ]
        return any(g in c for g in good)

    filtered = []
    for n in base:
        if not isinstance(n, dict):
            continue
        ctx = n.get("context") or ""
        if _bad_pdf_context(ctx):
            continue
        filtered.append(n)

    # Prefer contexts that look "metric-like"
    preferred = [n for n in filtered if _good_pdf_context(n.get("context") or "")]

    # If we filtered too aggressively, fall back safely
    if preferred:
        return preferred
    if filtered:
        return filtered
    return base


def calculate_context_match(keywords: List[str], context: str) -> float:
    """Calculate how well keywords match the context (deterministic)."""
    if not context:
        return 0.0

    context_lower = context.lower()

    # If no keywords, give a small baseline (we'll rely more on value_score)
    if not keywords:
        return 0.25

    # Year keywords MUST match if present
    year_keywords = [kw for kw in keywords if re.fullmatch(r"20\d{2}", kw)]
    if year_keywords:
        if not any(y in context_lower for y in year_keywords):
            return 0.0

    matches = sum(1 for kw in keywords if kw.lower() in context_lower)

    # Instead of hard "matches < 2 = reject", scale smoothly:
    match_ratio = matches / max(len(keywords), 1)

    # If nothing matches, reject
    if matches == 0:
        return 0.0

    # Score between 0.35 and 1.0 depending on ratio
    return 0.35 + (match_ratio * 0.65)


def render_source_anchored_results(results, query: str):
    """Render source-anchored evolution results (guarded + backward compatible + tuned debug UI)."""
    import math
    import re
    from collections import Counter, defaultdict

    st.header("📈 Source-Anchored Evolution Analysis")
    st.markdown(f"**Query:** {query}")

    if not isinstance(results, dict):
        st.error("❌ Evolution returned an invalid result payload (not a dict).")
        st.write(results)
        return

    status = (results.get("status") or "").strip().lower()
    message = results.get("message") or ""

    def _safe_int(x, default=0) -> int:
        try:
            if x is None:
                return default
            return int(x)
        except Exception:
            return default

    def _safe_float(x, default=0.0) -> float:
        try:
            if x is None:
                return default
            return float(x)
        except Exception:
            return default

    def _fmt_pct(x, default="—") -> str:
        try:
            if x is None:
                return default
            v = float(x)
            if math.isnan(v):
                return default
            return f"{v:.0f}%"
        except Exception:
            return default

    def _fmt_change_pct(x) -> str:
        try:
            if x is None:
                return "-"
            v = float(x)
            if math.isnan(v):
                return "-"
            return f"{v:+.1f}%"
        except Exception:
            return "-"

    def _short(u: str, n: int = 95) -> str:
        if not u:
            return ""
        return (u[:n] + "…") if len(u) > n else u

    if status != "success":
        st.error(f"❌ {message or 'Evolution failed'}")
        sr = results.get("source_results") or []
        if isinstance(sr, list) and sr:
            st.subheader("🔗 Source Verification")
            for src in sr:
                if not isinstance(src, dict):
                    continue
                u = _short((src.get("url") or ""), 90)
                st.error(f"❌ {u} - {src.get('status_detail', 'Unknown error')}")
        return

    sources_checked = _safe_int(results.get("sources_checked"), 0)
    sources_fetched = _safe_int(results.get("sources_fetched"), 0)
    stability = _safe_float(results.get("stability_score"), 0.0)
    summary = results.get("summary") or {}
    if not isinstance(summary, dict):
        summary = {}

    metrics_inc = _safe_int(summary.get("metrics_increased"), 0)
    metrics_dec = _safe_int(summary.get("metrics_decreased"), 0)
    metrics_unch = _safe_int(summary.get("metrics_unchanged"), 0)

    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Sources Checked", sources_checked)
    col2.metric("Sources Fetched", sources_fetched)
    col3.metric("Stability", _fmt_pct(stability))
    if metrics_inc > metrics_dec:
        col4.success("📈 Trending Up")
    elif metrics_dec > metrics_inc:
        col4.error("📉 Trending Down")
    else:
        col4.info("➡️ Stable")

    if message:
        st.caption(message)

    st.markdown("---")

    # -------------------------
    # Source status
    # -------------------------
    st.subheader("🔗 Source Verification")
    src_results = results.get("source_results") or []
    if not isinstance(src_results, list):
        src_results = []

    # If everything failed, show breakdown
    if sources_checked > 0 and sources_fetched == 0 and src_results:
        reasons = []
        for s in src_results:
            if isinstance(s, dict):
                reasons.append((s.get("status_detail") or "unknown").split(":")[0])
        top = Counter(reasons).most_common(6)
        if top:
            st.warning("No sources were fetched successfully. Top failure types:")
            st.write({k: v for k, v in top})

    for src in src_results:
        if not isinstance(src, dict):
            continue
        url = src.get("url") or ""
        sstatus = src.get("status") or ""
        detail = src.get("status_detail") or ""
        ctype = src.get("content_type") or ""
        nfound = _safe_int(src.get("numbers_found"), 0)

        short = _short(url, 95)

        # show extra debug flags if present
        flags = []
        if src.get("snapshot_origin"):
            flags.append(f"origin={src.get('snapshot_origin')}")
        if src.get("is_homepage"):
            flags.append("homepage")
        if src.get("skip_reason"):
            flags.append(f"skip={src.get('skip_reason')}")
        if src.get("quality_score") is not None:
            try:
                flags.append(f"q={float(src.get('quality_score')):.2f}")
            except Exception:
                flags.append(f"q={src.get('quality_score')}")

        flag_txt = f" • {' • '.join(flags)}" if flags else ""

        if str(sstatus).startswith("fetched"):
            extra = f" ({nfound} nums)"
            if ctype:
                extra += f" • {ctype}"
            st.success(f"✅ {short}{extra}{flag_txt}")
        else:
            extra = f" - {detail}" if detail else ""
            if ctype:
                extra += f" • {ctype}"
            st.error(f"❌ {short}{extra}{flag_txt}")

    st.markdown("---")

    # -------------------------
    # Metric changes table
    # -------------------------
    st.subheader("💰 Metric Changes")
    rows = results.get("metric_changes") or []
    if not isinstance(rows, list) or not rows:
        st.info("No metric changes to display.")
        return

    table_rows = []
    for r in rows:
        if not isinstance(r, dict):
            continue

        metric_label = r.get("metric") or r.get("name") or ""
        status_label = r.get("status") or r.get("change_type") or ""

        table_rows.append({
            "Metric": metric_label,
            "Canonical Key": r.get("canonical_key", "") or "",
            "Match Stage": r.get("match_stage", "") or "",
            "Previous": r.get("previous_value", "") or "",
            "Current": r.get("current_value", "") or "",
            "Δ%": _fmt_change_pct(r.get("change_pct")),
            "Status": status_label,
            "Match": _fmt_pct(r.get("match_confidence")),
            "Score": ("" if r.get("match_score") is None else f"{_safe_float(r.get('match_score'), 0.0):.2f}"),
            "Anchor": "✅" if r.get("anchor_used") else "",
        })

    st.dataframe(table_rows, use_container_width=True)

    # -------------------------
    # Debug / tuning views
    # -------------------------
    # Aggregate rejection reasons across all metrics (quick tuning signal)
    agg_rej = Counter()
    for r in rows:
        if isinstance(r, dict) and isinstance(r.get("rejected_reason_counts"), dict):
            for k, v in r["rejected_reason_counts"].items():
                try:
                    agg_rej[k] += int(v or 0)
                except Exception:
                    pass

    if agg_rej:
        with st.expander("🧰 Tuning Summary (aggregate rejects across all metrics)"):
            st.write(dict(agg_rej.most_common(20)))

    # Full per-metric debug
    with st.expander("🧾 Per-metric match details (debug)"):
        for i, r in enumerate(rows, 1):
            if not isinstance(r, dict):
                continue

            metric_label = r.get("metric") or r.get("name") or f"metric_{i}"
            status_label = r.get("status") or r.get("change_type") or "unknown"

            canonical_key = r.get("canonical_key", "") or ""
            stage = r.get("match_stage", "") or ""
            conf = r.get("match_confidence", None)
            score = r.get("match_score", None)

            header = f"{i}. {metric_label} — {status_label}"
            meta_bits = []
            if canonical_key:
                meta_bits.append(f"ck={canonical_key}")
            if stage:
                meta_bits.append(f"stage={stage}")
            if conf is not None:
                meta_bits.append(f"conf={_fmt_pct(conf)}")
            if score is not None:
                try:
                    meta_bits.append(f"score={float(score):.2f}")
                except Exception:
                    meta_bits.append(f"score={score}")

            if meta_bits:
                header += f"  ({' • '.join(meta_bits)})"

            with st.expander(header):
                # Values
                st.write({
                    "previous_value": r.get("previous_value"),
                    "current_value": r.get("current_value"),
                    "change_pct": r.get("change_pct"),
                })

                # Candidate considered / rejects
                st.write("Candidates considered:", _safe_int(r.get("candidates_considered_count"), 0))

                rej = r.get("rejected_reason_counts")
                if isinstance(rej, dict) and rej:
                    # sort largest first
                    try:
                        rej_sorted = dict(sorted(((k, int(v or 0)) for k, v in rej.items()), key=lambda x: x[1], reverse=True))
                    except Exception:
                        rej_sorted = rej
                    st.write("Rejected reason counts:", rej_sorted)

                # Score breakdown (if present)
                sb = r.get("score_breakdown")
                if isinstance(sb, dict) and sb:
                    st.write("Score breakdown:", sb)

                # Matched candidate (new)
                mc = r.get("matched_candidate")
                if isinstance(mc, dict) and mc:
                    st.markdown("**Matched candidate**")
                    st.write({
                        "raw": mc.get("raw"),
                        "value": mc.get("value"),
                        "unit": mc.get("unit"),
                        "source_url": mc.get("source_url"),
                        "anchor_hash": mc.get("anchor_hash"),
                        "is_homepage": mc.get("is_homepage"),
                        "skip_reason": mc.get("skip_reason"),
                        "quality_score": mc.get("quality_score"),
                    })
                    ctx = mc.get("context_snippet")
                    if ctx:
                        st.write("Context:")
                        st.code(str(ctx))
                else:
                    # Backward-compatible fields
                    src = r.get("matched_source") or r.get("source_url")
                    ctx = r.get("matched_context") or r.get("context_snippet")
                    if src:
                        st.write("Source:", src)
                    if ctx:
                        st.write("Context:")
                        st.code(str(ctx))

                # Additional anchor hash compatibility
                if r.get("matched_anchor_hash"):
                    st.write("Matched Anchor Hash:", r.get("matched_anchor_hash"))

    st.markdown("---")


# =========================================================
# 9. DASHBOARD RENDERING
# =========================================================

def detect_x_label_dynamic(labels: list) -> str:
    """Enhanced X-axis detection with better region matching"""
    if not labels:
        return "Category"

    # Convert to lowercase for comparison
    label_texts = [str(l).lower().strip() for l in labels]
    all_text = ' '.join(label_texts)

    # 1. GEOGRAPHIC REGIONS (PRIORITY 1)
    region_keywords = [
        'north america', 'asia pacific', 'asia-pacific', 'apac', 'europe', 'emea',
        'latin america', 'latam', 'middle east', 'africa', 'oceania',
        'rest of world', 'row', 'china', 'usa', 'india', 'japan', 'germany'
    ]

    # Count how many labels contain region keywords
    region_matches = sum(
        1 for label in label_texts
        if any(keyword in label for keyword in region_keywords)
    )

    # If 40%+ of labels are regions → "Regions"
    if region_matches / len(labels) >= 0.4:
        return "Regions"

    # 2. YEARS (e.g., 2023, 2024, 2025)
    year_pattern = r'\b(19|20)\d{2}\b'
    year_count = sum(1 for label in label_texts if re.search(year_pattern, label))
    if year_count / len(labels) > 0.5:
        return "Years"

    # 3. QUARTERS (Q1, Q2, Q3, Q4)
    quarter_pattern = r'\bq[1-4]\b'
    quarter_count = sum(1 for label in label_texts if re.search(quarter_pattern, label, re.IGNORECASE))
    if quarter_count >= 2:
        return "Quarters"

    # 4. MONTHS
    months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']
    month_count = sum(1 for label in label_texts if any(month in label for month in months))
    if month_count >= 3:
        return "Months"

    # 5. COMPANIES (common suffixes)
    company_keywords = ['inc', 'corp', 'ltd', 'llc', 'gmbh', 'ag', 'sa', 'plc']
    company_count = sum(1 for label in label_texts if any(kw in label for kw in company_keywords))
    if company_count >= 2:
        return "Companies"

    # 6. PRODUCTS/SEGMENTS (if contains "segment", "product", "category")
    if any(word in all_text for word in ['segment', 'product line', 'category', 'type']):
        return "Segments"

    # Default
    return "Categories"

def detect_y_label_dynamic(values: list) -> str:
    """Fully dynamic Y-axis label based on magnitude + context"""
    if not values:
        return "Value"

    numeric_values = []
    for v in values:
        try:
            numeric_values.append(abs(float(v)))
        except (ValueError, TypeError):
            continue

    if not numeric_values:
        return "Value"

    avg_mag = np.mean(numeric_values)
    max_mag = max(numeric_values)

    # Non-overlapping ranges with clear boundaries
    # 1. BILLIONS (large market sizes)
    if max_mag > 100 or avg_mag > 50:
        return "USD B"

    # 2. MILLIONS (medium values)
    elif max_mag > 10 or avg_mag > 5:
        return "USD M"

    # 3. PERCENTAGES (typical 0-100 range, but also small decimals)
    elif max_mag <= 100 and avg_mag <= 50:
        # Check if values look like percentages (mostly 0-100)
        if all(0 <= v <= 100 for v in numeric_values):
            return "Percent %"
        else:
            return "USD K"

    # 4. Default
    else:
        return "Units"

# =========================================================
# 3A. QUESTION CATEGORIZATION + SIGNALS (DETERMINISTIC)
# =========================================================

def categorize_question_signals(query: str, qs: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Build a question_profile used for structured reporting.

    IMPORTANT:
      - category must follow query_structure if provided (single source of truth).
      - signals can be rich, but must not contradict the chosen category.
    """
    qs = qs or {}
    q = (query or "").strip()

    # Prefer category/main/side from query_structure when available
    category = (qs.get("category") or "").strip() or "unknown"
    main_q = (qs.get("main") or "").strip() or q
    side_qs = qs.get("side") if isinstance(qs.get("side"), list) else []

    # Deterministic signals (richer classifier)
    base = classify_question_signals(q) or {}

    # Force category + expected_metric_ids to match query_structure category
    # (but preserve other extracted info like years/regions/intents)
    signals: Dict[str, Any] = {}
    signals["category"] = category

    # Carry over extracted fields
    signals["years"] = base.get("years", []) or []
    signals["regions"] = base.get("regions", []) or []
    signals["intents"] = base.get("intents", []) or []

    # Keep raw signals for debugging
    raw_hits = list(base.get("signals") or [])
    signals["raw_signals"] = raw_hits

    def _signal_consistent_with_category(sig: str, cat: str) -> bool:
        s = (sig or "").lower()
        c = (cat or "").lower()
        if not s:
            return False

        # If final category is country, drop industry/company category-rule strings
        if c == "country":
            if "industry_keywords" in s or "mixed_signals_default_to_industry" in s or "company_keywords" in s:
                return False

        # If final category is industry, drop explicit country-rule strings
        if c == "industry":
            if "macro_outlook_bias_country" in s or "country_keywords" in s:
                return False

        return True

    signals["signals"] = [s for s in raw_hits if _signal_consistent_with_category(s, category)]

    # Expected metric IDs: always determined by the final category, then lightly enriched by intents (optional)
    expected_metric_ids: List[str] = []
    try:
        expected_metric_ids = get_expected_metric_ids_for_category(category) or []
    except Exception:
        expected_metric_ids = []

    # Optional: enrich with intent-based suggestions (won't remove anything)
    intent_metric_suggestions = {
        "market_size": ["market_size", "market_size_2024", "market_size_2025"],
        "growth_forecast": ["cagr", "market_size_2030"],
        "competitive_landscape": ["market_share", "top_players"],
        "pricing": ["avg_price", "asp"],
        "consumer_demand": ["users", "penetration", "arpu"],
        "supply_chain": ["capacity", "shipments"],
        "investment": ["capex", "profit", "ebitda"],
        "macro_outlook": ["gdp", "inflation", "interest_rate", "exchange_rate"],
    }

    intents = signals.get("intents") or []
    for intent in intents:
        for mid in intent_metric_suggestions.get(intent, []):
            if mid not in expected_metric_ids:
                expected_metric_ids.append(mid)

    signals["expected_metric_ids"] = expected_metric_ids

    profile: Dict[str, Any] = {
        "category": category,
        "signals": signals,
        "main_question": main_q,
        "side_questions": side_qs,
    }

    # Keep debug for traceability
    if qs.get("debug") is not None:
        profile["debug_query_structure"] = qs.get("debug")

    return profile




def render_dashboard(
    primary_json: str,
    final_conf: float,
    web_context: Dict,
    base_conf: float,
    user_question: str,
    veracity_scores: Optional[Dict] = None,
    source_reliability: Optional[List[str]] = None,
):
    """Render the analysis dashboard"""

    # -------------------------
    # Parse primary response
    # -------------------------

    # =========================
    # PATCH RD1 (ADDITIVE): safe preview helper
    # - Prevents slice errors when primary_json is dict/list/etc.
    # - Keeps original behavior for strings
    # =========================
    def _preview(x, limit: int = 1000) -> str:
        try:
            if isinstance(x, (dict, list)):
                s = json.dumps(x, ensure_ascii=False, indent=2, default=str)
            else:
                s = str(x)
        except Exception:
            s = repr(x)
        return s[:limit]
    # =========================

    try:
        # =========================
        # PATCH RD2 (ADDITIVE): accept dict/list directly
        # - If caller passes dict (primary_data), just use it
        # - If caller passes list, wrap it (keeps downstream dict access safe)
        # - Else try json.loads on string
        # =========================
        if isinstance(primary_json, dict):
            data = primary_json
        elif isinstance(primary_json, list):
            data = {"_list": primary_json}
        else:
            data = json.loads(primary_json)
        # =========================

    except Exception as e:
        st.error(f"❌ Cannot render dashboard: {e}")
        # =========================
        # PATCH RD1 (ADDITIVE): safe preview (no slicing crash)
        # =========================
        st.code(_preview(primary_json))
        # =========================
        return

    # -------------------------
    # Helper: metric value formatting (currency + compact units) + RANGE SUPPORT
    # -------------------------
    def _format_metric_value(m: Any) -> str:
        """
        Format metric values cleanly, with RANGE SUPPORT:
        - If value_range exists (min/max), show min–max using the same currency/unit rules
        - Otherwise show the point value as before
        """
        if not isinstance(m, dict):
            if m is None:
                return "N/A"
            return str(m)

        # -------------------------
        # Helper: format a single numeric endpoint (val+unit)
        # -------------------------
        def _format_point(val: Any, unit: str) -> str:
            if val is None or val == "":
                return "N/A"

            unit = (unit or "").strip()
            raw_val = str(val).strip()

            # Try parse numeric
            try:
                num = float(raw_val.replace(",", ""))
            except Exception:
                # If we can't parse as float, just glue value+unit neatly
                return f"{raw_val}{unit}".strip() if unit else raw_val

            # Normalize unit spacing
            unit = unit.replace(" ", "")
            currency_prefix = ""
            u_upper = unit.upper()

            # Common patterns: "S$B", "SGDB", "USD B", "$B"
            if u_upper.startswith("S$"):
                currency_prefix = "S$"
                unit = unit[2:]
            elif u_upper.startswith("SGD"):
                currency_prefix = "S$"
                unit = unit[3:]
            elif u_upper.startswith("USD"):
                currency_prefix = "$"
                unit = unit[3:]
            elif u_upper.startswith("$"):
                currency_prefix = "$"
                unit = unit[1:]

            unit = unit.strip()

            # Percent
            if unit == "%":
                return f"{num:.1f}%"

            # Compact units
            unit_upper = unit.upper()
            if unit_upper in ("B", "BILLION"):
                formatted = f"{num:.2f}".rstrip("0").rstrip(".") + "B"
                return f"{currency_prefix}{formatted}".strip()
            if unit_upper in ("M", "MILLION"):
                formatted = f"{num:.2f}".rstrip("0").rstrip(".") + "M"
                return f"{currency_prefix}{formatted}".strip()
            if unit_upper in ("K", "THOUSAND"):
                formatted = f"{num:.2f}".rstrip("0").rstrip(".") + "K"
                return f"{currency_prefix}{formatted}".strip()

            # Plain number formatting
            if abs(num) >= 1000:
                if float(num).is_integer():
                    formatted = f"{int(num):,}"
                else:
                    formatted = f"{num:,.2f}".rstrip("0").rstrip(".")
            else:
                formatted = f"{num:g}"

            # Unit glue
            if unit:
                formatted = f"{formatted} {unit}".strip()

            return f"{currency_prefix}{formatted}".strip()

        # -------------------------
        # RANGE: prefer value_range if present and meaningful
        # -------------------------
        unit = (m.get("unit") or "").strip()
        vr = m.get("value_range")

        if isinstance(vr, dict):
            vmin = vr.get("min")
            vmax = vr.get("max")
            if vmin is not None and vmax is not None:
                left = _format_point(vmin, unit)
                right = _format_point(vmax, unit)
                if left != "N/A" and right != "N/A" and left != right:
                    return f"{left}–{right}"

        # Precomputed range display (optional)
        vr_disp = m.get("value_range_display")
        if isinstance(vr_disp, str) and vr_disp.strip():
            return vr_disp.strip()

        # -------------------------
        # POINT VALUE fallback
        # -------------------------
        val = m.get("value")
        if val is None or val == "":
            return "N/A"

        return _format_point(val, unit)

    # -------------------------
    # Header + confidence row
    # -------------------------
    st.header("📊 Yureeka Market Report")
    st.markdown(f"**Question:** {user_question}")

    col1, col2, col3 = st.columns(3)
    col1.metric("Final Confidence", f"{float(final_conf):.1f}%")
    col2.metric("Base Model", f"{float(base_conf):.1f}%")
    if isinstance(veracity_scores, dict):
        col3.metric("Evidence", f"{float(veracity_scores.get('overall', 0) or 0):.1f}%")
    else:
        col3.metric("Evidence", "N/A")

    st.markdown("---")

    # -------------------------
    # Executive Summary
    # -------------------------
    st.subheader("📋 Executive Summary")
    st.markdown(f"**{data.get('executive_summary', 'No summary available')}**")

    # Optional: expand summary if side-questions exist
    side_questions = data.get("side_questions") or (data.get("question_profile", {}) or {}).get("side_questions", [])
    if side_questions:
        st.markdown("")
        st.markdown("**Also addressed:**")
        for sq in side_questions[:6]:
            if sq:
                st.markdown(f"- {sq}")

    st.markdown("---")

    # -------------------------
    # Key Metrics
    # -------------------------
    st.subheader("💰 Key Metrics")
    metrics = data.get("primary_metrics", {}) or {}

    question_category = data.get("question_category") or (data.get("question_profile", {}) or {}).get("category")
    question_signals = data.get("question_signals") or (data.get("question_profile", {}) or {}).get("signals", {})
    expected_ids = data.get("expected_metric_ids") or ((data.get("question_signals") or {}).get("expected_metric_ids") or [])

    metric_rows: List[Dict[str, str]] = []

    if question_category:
        metric_rows.append({"Metric": "Question Category", "Value": str(question_category)})
    if isinstance(question_signals, dict) and question_signals:
        metric_rows.append({"Metric": "Signals", "Value": ", ".join([str(x) for x in (question_signals.get("signals") or [])][:10])})
    if expected_ids:
        metric_rows.append({"Metric": "Expected Metrics", "Value": ", ".join([str(x) for x in expected_ids][:10])})

    # Render primary metrics
    if isinstance(metrics, dict) and metrics:
        for _, m in metrics.items():
            if isinstance(m, dict):
                name = m.get("name") or "Metric"
                metric_rows.append({"Metric": str(name), "Value": _format_metric_value(m)})

    # Display metrics table
    if metric_rows:
        try:
            import pandas as pd  # optional dependency in your environment
            df_metrics = pd.DataFrame(metric_rows)
            st.dataframe(df_metrics, use_container_width=True, hide_index=True)
        except Exception:
            for r in metric_rows:
                st.write(f"**{r.get('Metric','')}**: {r.get('Value','')}")

    st.markdown("---")

    # -------------------------
    # Key Findings
    # -------------------------
    st.subheader("🧠 Key Findings")
    kf = data.get("key_findings") or []
    if isinstance(kf, list) and kf:
        for item in kf[:12]:
            if item:
                st.markdown(f"- {item}")
    else:
        st.info("No key findings available.")

    st.markdown("---")

    # -------------------------
    # Trends / Forecast
    # -------------------------
    st.subheader("📈 Trends & Forecast")
    tf = data.get("trends_forecast") or []
    if isinstance(tf, list) and tf:
        for t in tf[:12]:
            if isinstance(t, dict):
                trend = t.get("trend") or ""
                direction = t.get("direction") or ""
                timeline = t.get("timeline") or ""
                st.markdown(f"- **{trend}** {direction} ({timeline})")
            elif t:
                st.markdown(f"- {t}")
    else:
        st.info("No trends forecast available.")

    st.markdown("---")

    # -------------------------
    # Sources / Web Context summary
    # -------------------------
    st.subheader("🔎 Sources & Evidence")
    sources = data.get("sources") or data.get("web_sources") or []
    if isinstance(sources, list) and sources:
        with st.expander(f"Show sources ({len(sources)})"):
            for s in sources[:50]:
                if s:
                    st.markdown(f"- {s}")
            if len(sources) > 50:
                st.markdown(f"... (+{len(sources)-50} more)")

    # Web context debug counters if present
    if isinstance(web_context, dict):
        dbg = web_context.get("debug_counts") or {}
        if isinstance(dbg, dict) and dbg:
            with st.expander("Collector debug counts"):
                st.json(dbg)

    # =====================================================================
    # PATCH UI_EXTRA_URLS_TRACE2 (ADDITIVE): show injected extra-URL trace (if any)
    # =====================================================================
    try:
        exdbg = {}
        if isinstance(web_context, dict):
            exdbg = web_context.get("extra_urls_debug") or {}
            # Back-compat: allow nested placement under debug_counts
            if (not exdbg) and isinstance(web_context.get("debug_counts"), dict):
                exdbg = (web_context.get("debug_counts") or {}).get("extra_urls_debug") or {}
        if isinstance(exdbg, dict) and exdbg:
            with st.expander("Extra URLs trace (injected sources)"):
                st.json(exdbg)
    except Exception:
        pass
    # =====================================================================

    # Source reliability badges (if provided)
    if isinstance(source_reliability, list) and source_reliability:
        with st.expander("Source reliability"):
            for line in source_reliability[:80]:
                st.write(line)



def render_native_comparison(baseline: Dict, compare: Dict):
    """Render a clean comparison between two analyses"""

    st.header("📊 Analysis Comparison")

    # Time info
    baseline_time = baseline.get('timestamp', '')
    compare_time = compare.get('timestamp', '')

    try:
        baseline_dt = datetime.fromisoformat(baseline_time.replace('Z', '+00:00'))
        compare_dt = datetime.fromisoformat(compare_time.replace('Z', '+00:00'))
        delta = compare_dt - baseline_dt
        if delta.days > 0:
            delta_str = f"{delta.days}d {delta.seconds // 3600}h"
        else:
            delta_str = f"{delta.seconds // 3600}h {(delta.seconds % 3600) // 60}m"
    except:
        delta_str = "Unknown"

    # Overview row
    col1, col2, col3 = st.columns(3)
    col1.metric("Baseline", baseline_time[:16] if baseline_time else "N/A")
    col2.metric("Current", compare_time[:16] if compare_time else "N/A")
    col3.metric("Time Delta", delta_str)

    st.markdown("---")

    # Extract metrics
    baseline_metrics = baseline.get('primary_response', {}).get('primary_metrics', {})
    compare_metrics = compare.get('primary_response', {}).get('primary_metrics', {})

    # Build metric diff table
    st.subheader("💰 Metric Changes")

    diff_rows = []
    stability_count = 0
    total_count = 0

    # Canonicalize metrics for stable matching
    baseline_canonical = canonicalize_metrics(baseline_metrics)
    compare_canonical = canonicalize_metrics(compare_metrics)

    # Build lookup by canonical ID
    baseline_by_id = {}
    compare_by_id = {}

    for cid, m in baseline_canonical.items():
        baseline_by_id[cid] = m

    for cid, m in compare_canonical.items():
        compare_by_id[cid] = m

    all_ids = set(baseline_by_id.keys()).intersection(compare_by_id.keys())

    for cid in sorted(all_ids):
        baseline_m = baseline_by_id.get(cid)
        compare_m = compare_by_id.get(cid)

        # Use canonical name for display, fallback to original
        display_name = cid
        if baseline_m and baseline_m.get('name'):
            display_name = baseline_m['name']


        if baseline_m and compare_m:
            old_val = baseline_m.get('value', 'N/A')
            new_val = compare_m.get('value', 'N/A')
            unit = compare_m.get('unit', baseline_m.get('unit', ''))

            old_num = parse_to_float(old_val)
            new_num = parse_to_float(new_val)

            if old_num is not None and new_num is not None and old_num != 0:
                change_pct = ((new_num - old_num) / abs(old_num)) * 100

                if abs(change_pct) < 1:
                    icon, reason = "➡️", "No change"
                    stability_count += 1
                elif abs(change_pct) < 5:
                    icon, reason = "➡️", "Minor change"
                    stability_count += 1
                elif change_pct > 0:
                    icon, reason = "📈", "Increased"
                else:
                    icon, reason = "📉", "Decreased"

                delta_str = f"{change_pct:+.1f}%"
            else:
                icon, delta_str, reason = "➡️", "-", "Non-numeric"
                stability_count += 1

            diff_rows.append({
                '': icon,
                'Metric': display_name,
                'Old': _fmt_currency_first(str(old_val), str(unit)),
                'New': _fmt_currency_first(str(new_val), str(unit)),
                'Δ': delta_str,
                'Reason': reason
            })
            total_count += 1

        elif baseline_m:
            old_val = baseline_m.get('value', 'N/A')
            unit = baseline_m.get('unit', '')
            diff_rows.append({
                '': '❌',
                'Metric': display_name,
                'Old': f"{old_val} {unit}".strip(),
                'New': '-',
                'Δ': '-',
                'Reason': 'Removed'
            })
            total_count += 1
        else:
            new_val = compare_m.get('value', 'N/A')
            unit = compare_m.get('unit', '')
            diff_rows.append({
                '': '🆕',
                'Metric': display_name,
                'Old': '-',
                'New': f"{new_val} {unit}".strip(),
                'Δ': '-',
                'Reason': 'New'
            })
            total_count += 1

    if diff_rows:
        st.dataframe(pd.DataFrame(diff_rows), hide_index=True, use_container_width=True)

        # Show canonical ID mapping for debugging
        with st.expander("🔧 Canonical ID Mapping (Debug)"):
            st.write("**How metrics were matched:**")

            baseline_canonical = canonicalize_metrics(baseline_metrics)
            compare_canonical = canonicalize_metrics(compare_metrics)

            col1, col2 = st.columns(2)

            with col1:
                st.write("**Baseline Metrics:**")
                for cid, m in baseline_canonical.items():
                    original = m.get('original_name', 'N/A')
                    canonical = m.get('name', 'N/A')
                    st.caption(f"`{cid}`")
                    st.write(f"  {original} → {canonical}")

            with col2:
                st.write("**Current Metrics:**")
                for cid, m in compare_canonical.items():
                    original = m.get('original_name', 'N/A')
                    canonical = m.get('name', 'N/A')
                    st.caption(f"`{cid}`")
                    st.write(f"  {original} → {canonical}")
    else:
        st.info("No metrics to compare")

    # Stability score
    stability_pct = (stability_count / total_count * 100) if total_count > 0 else 100

    st.markdown("---")
    st.subheader("📊 Stability Score")

    col1, col2, col3 = st.columns(3)
    col1.metric("Stable Metrics", f"{stability_count}/{total_count}")
    col2.metric("Stability", f"{stability_pct:.0f}%")

    if stability_pct >= 80:
        col3.success("🟢 Highly Stable")
    elif stability_pct >= 60:
        col3.warning("🟡 Moderate Changes")
    else:
        col3.error("🔴 Significant Drift")

    # Confidence comparison
    st.markdown("---")
    st.subheader("🎯 Confidence Change")

    col1, col2, col3 = st.columns(3)
    baseline_conf = baseline.get('final_confidence', 0)
    compare_conf = compare.get('final_confidence', 0)
    conf_change = compare_conf - baseline_conf if isinstance(baseline_conf, (int, float)) and isinstance(compare_conf, (int, float)) else 0

    col1.metric("Baseline", f"{baseline_conf:.1f}%" if isinstance(baseline_conf, (int, float)) else "N/A")
    col2.metric("Current", f"{compare_conf:.1f}%" if isinstance(compare_conf, (int, float)) else "N/A")
    col3.metric("Change", f"{conf_change:+.1f}%")

    # Download comparison
    st.markdown("---")
    comparison_output = {
        "comparison_timestamp": datetime.now().isoformat(),
        "baseline": baseline,
        "current": compare,
        "stability_score": stability_pct,
        "metrics_compared": total_count,
        "metrics_stable": stability_count
    }

    st.download_button(
        label="💾 Download Comparison Report",
        data=json.dumps(comparison_output, indent=2, ensure_ascii=False).encode('utf-8'),
        file_name=f"yureeka_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        mime="application/json"
    )

# =========================================================
# 10. MAIN APPLICATION
# =========================================================

# ==============================================================================
# PATCH FIX39 (ADDITIVE): Final publish/render unit-required hard gate
#
# Why:
# - Even if upstream selection is tightened, some paths (UI render, sheet publish,
#   legacy mappings) can still surface unit-less year-like integers (e.g., 2024/2025)
#   in the "Current" column for unit-required metrics (currency/percent/rate/ratio).
# - FIX39 enforces the invariant at the last mile: right before rendering/publishing.
#
# Behavior:
# - For each metric change row (dict-form) and each EvolutionDiff metric entry (object-form),
#   if schema indicates unit required (via unit_family or canonical_key suffix) AND
#   current value lacks token-level unit evidence, then:
#     * blank out Current/new_raw
#     * set unit_mismatch flag / change_type to "unit_mismatch" where possible
# - Purely additive; does not refactor upstream pipelines.
# ==============================================================================

def _fix39_schema_unit_required(metric_def: dict, canonical_key: str = "") -> bool:
    try:
        uf = str((metric_def or {}).get("unit_family") or (metric_def or {}).get("unit") or "").strip().lower()
        if uf in {"currency", "percent", "rate", "ratio"}:
            return True
    except Exception:
        pass
    ck = (canonical_key or "").strip().lower()
    if ck.endswith("__currency") or ck.endswith("__percent") or ck.endswith("__rate") or ck.endswith("__ratio"):
        return True
    # unit_tag explicit
    try:
        ut = str((metric_def or {}).get("unit_tag") or "").strip()
        if ut:
            # if schema explicitly wants a unit token, treat as required
            return True
    except Exception:
        pass
    return False

def _fix39_has_unit_evidence(metric_like: dict) -> bool:
    """Token-level unit evidence check (tolerant across shapes)."""
    try:
        m = metric_like if isinstance(metric_like, dict) else {}
        for k in ("unit", "unit_tag", "base_unit", "unit_family", "currency", "currency_symbol"):
            if str(m.get(k) or "").strip():
                return True
        if bool(m.get("is_percent") or m.get("has_percent")):
            return True
        # Some rows store comparator field
        if str(m.get("cur_unit_cmp") or "").strip():
            return True
        raw = str(m.get("raw") or m.get("value") or m.get("new_raw") or "")
        if raw and any(sym in raw for sym in ("$", "€", "£", "¥", "%")):
            return True
    except Exception:
        pass
    return False

def _fix39_sanitize_metric_change_rows(results_dict: dict) -> None:
    """Sanitize dict-based evolution results before publishing/rendering."""
    if not isinstance(results_dict, dict):
        return
    try:
        schema = results_dict.get("metric_schema_frozen") or results_dict.get("schema") or {}
        metric_changes = None
        # common nesting patterns
        if isinstance(results_dict.get("results"), dict) and isinstance(results_dict["results"].get("metric_changes"), list):
            metric_changes = results_dict["results"]["metric_changes"]
        elif isinstance(results_dict.get("metric_changes"), list):
            metric_changes = results_dict.get("metric_changes")

        if not isinstance(metric_changes, list):
            return

        bad = []
        for row in metric_changes:
            if not isinstance(row, dict):
                continue
            ck = row.get("canonical_key") or row.get("canonical") or row.get("key") or ""
            md = {}
            try:
                if isinstance(schema, dict) and ck in schema:
                    md = schema.get(ck) or {}
            except Exception:
                md = {}
            if _fix39_schema_unit_required(md, ck):
                # current fields may be in different keys
                cur_like = {
                    "unit": row.get("current_unit") or row.get("cur_unit") or row.get("unit") or "",
                    "unit_tag": row.get("current_unit_tag") or row.get("unit_tag") or "",
                    "unit_family": row.get("schema_unit_family") or "",
                    "cur_unit_cmp": row.get("cur_unit_cmp") or "",
                    "raw": row.get("current_value") or row.get("current_raw") or row.get("Current") or "",
                    "new_raw": row.get("new_raw") or "",
                    "currency_symbol": row.get("currency_symbol") or "",
                    "is_percent": row.get("is_percent") or False,
                }
                if not _fix39_has_unit_evidence(cur_like):
                    # invalidate
                    row["unit_mismatch"] = True
                    # prefer explicit fields if present
                    for k in ("current_value", "current_raw", "new_raw", "Current"):
                        if k in row:
                            row[k] = ""
                    if "current_value_norm" in row:
                        row["current_value_norm"] = None
                    if "cur_value_norm" in row:
                        row["cur_value_norm"] = None
                    # normalize change_type
                    if row.get("change_type") not in ("unit_mismatch", "invalid_current"):
                        row["change_type"] = "unit_mismatch"
                    bad.append(str(ck))
        # small debug marker
        dbg = results_dict.setdefault("debug", {})
        f39 = dbg.setdefault("fix39", {})
        f39["invalidated_count"] = len(bad)
        if bad:
            f39["invalidated_keys_sample"] = bad[:20]
    except Exception:
        return

def _fix39_sanitize_evolutiondiff_object(diff_obj, metric_schema_frozen: dict = None):
    """Sanitize object-based EvolutionDiff (used by Streamlit renderer)."""
    try:
        schema = metric_schema_frozen or {}
        mdiffs = getattr(diff_obj, "metric_diffs", None)
        if not mdiffs:
            return diff_obj
        bad = []
        for m in mdiffs:
            try:
                ck = getattr(m, "canonical_key", "") or getattr(m, "canonical", "") or ""
                md = schema.get(ck) if isinstance(schema, dict) else {}
                if _fix39_schema_unit_required(md or {}, ck):
                    unit = getattr(m, "unit", "") or ""
                    new_raw = getattr(m, "new_raw", None)
                    # basic evidence check: unit or symbol in new_raw
                    has_e = bool(str(unit).strip())
                    if not has_e:
                        s = str(new_raw or "")
                        if any(sym in s for sym in ("$", "€", "£", "¥", "%")):
                            has_e = True
                    if not has_e:
                        # invalidate
                        try: setattr(m, "new_raw", "")
                        except Exception: pass
                        try: setattr(m, "new_value", None)
                        except Exception: pass
                        try: setattr(m, "change_type", "unit_mismatch")
                        except Exception: pass
                        bad.append(str(ck))
            except Exception:
                continue
        try:
            dbg = getattr(diff_obj, "debug", None)
            if isinstance(dbg, dict):
                dbg.setdefault("fix39", {})["invalidated_count"] = len(bad)
        except Exception:
            pass
        return diff_obj
    except Exception:
        return diff_obj

def main():
    st.set_page_config(
        page_title="Yureeka Market Report",
        page_icon="💹",
        layout="wide"
    )

    st.title("💹 Yureeka Market Intelligence")

    # Info section
    col_info, col_status = st.columns([3, 1])
    with col_info:
        st.markdown("""
        **Yureeka** provides AI-powered market research and analysis for finance,
        economics, and business questions.
        Powered by evidence-based verification and real-time web search.

        *Currently in prototype stage.*
        """)

    # Create tabs
    tab1, tab2 = st.tabs(["🔍 New Analysis", "📈 Evolution Analysis"])

    # =====================
    # TAB 1: NEW ANALYSIS
    # =====================
    with tab1:
        query = st.text_input(
            "Enter your question about markets, industries, finance, or economics:",
            placeholder="e.g., What is the size of the global EV battery market?"
        )

        col_opt1, col_opt2 = st.columns(2)
        with col_opt1:
            use_web = st.checkbox(
                "Enable web search (recommended)",
                value=bool(SERPAPI_KEY),
                disabled=not SERPAPI_KEY
            )


            # ============================================================

            # PATCH UI_EXTRA_SOURCES_TAB1 (ADDITIVE)

            # - Add extra URL injection UI directly to TAB 1 (New Analysis)

            # - Does NOT alter behavior unless user supplies URLs

            # ============================================================

            extra_sources_text_tab1 = st.text_area(

                "Extra source URLs (optional, one per line)",

                placeholder="https://example.com/report\nhttps://another-source.com/page",

                help="Add these URLs to the admitted source list for this analysis run (useful for hash-mismatch tests).",

                height=90,

                key="ui_extra_sources_tab1",

            )

            # ============================================================



        if st.button("🔍 Analyze", type="primary") and query:
            if len(query.strip()) < 5:
                st.error("❌ Please enter a question with at least 5 characters")
                return

            query = query.strip()[:500]

            query_structure = extract_query_structure(query) or {}
            question_profile = categorize_question_signals(query, qs=query_structure)
            question_signals = question_profile.get("signals", {}) or {}

            web_context = {}
            if use_web:
                with st.spinner("🌐 Searching the web..."):

                    # ---- ADDITIVE: pass existing snapshots for reuse (Change #3 wiring) ----
                    existing_snapshots = None

                    # If you have an analysis dict already in scope, reuse its cache
                    try:
                        if isinstance(locals().get("analysis"), dict):
                            existing_snapshots = (
                                analysis.get("baseline_sources_cache")
                                or (analysis.get("results", {}) or {}).get("baseline_sources_cache")
                                or (analysis.get("results", {}) or {}).get("source_results")
                            )
                    except Exception:
                        existing_snapshots = None

                    # Optional: if you keep a prior analysis in session_state, reuse it
                    try:
                        prev = st.session_state.get("last_analysis")
                        if existing_snapshots is None and isinstance(prev, dict):
                            existing_snapshots = (
                                prev.get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("source_results")
                            )
                    except Exception:
                        pass

                                        # ============================================================
                    # PATCH UI_EXTRA_SOURCES2 (ADDITIVE): parse extra source URLs
                    # ============================================================
                    extra_urls = []
                    try:
                        for _l in str(extra_sources_text_tab1 or "").splitlines():
                            _u = _l.strip()
                            if not _u:
                                continue
                            if _u.startswith("http://") or _u.startswith("https://"):
                                extra_urls.append(_u)
                    except Exception:
                        extra_urls = []


                    # ============================================================
                    # PATCH INJ_DIAG_TAB1_CALL (ADDITIVE): correlate UI extra-URL input into fetch_web_context diagnostics
                    # ============================================================
                    _analysis_run_id = _inj_diag_make_run_id("analysis")
                    # ============================================================

                    web_context = fetch_web_context(
                        query,
                        num_sources=3,
                        existing_snapshots=existing_snapshots,
                        extra_urls=extra_urls,
                        diag_run_id=_analysis_run_id,
                        diag_extra_urls_ui_raw=(extra_sources_text_tab1 or ""),
                    )
                    # ----------------------------------------------------------------------

            if not web_context or not web_context.get("search_results"):
                st.info("💡 Using AI knowledge without web search")
                web_context = {
                    "search_results": [],
                    "scraped_content": {},
                    "summary": "",
                    "sources": [],
                    "source_reliability": []
                }

            with st.spinner("🤖 Analyzing query..."):
                primary_response = query_perplexity(query, web_context, query_structure=query_structure)

            if not primary_response:
                st.error("❌ Primary model failed to respond")
                return

            try:
                primary_data = json.loads(primary_response)
            except Exception as e:
                st.error(f"❌ Failed to parse primary response: {e}")
                st.code(primary_response[:1000])
                return

            with st.spinner("✅ Verifying evidence quality..."):
                veracity_scores = evidence_based_veracity(primary_data, web_context)

            base_conf = float(primary_data.get("confidence", 75))
            final_conf = calculate_final_confidence(base_conf, veracity_scores.get("overall", 0))

            # Optional: canonicalize + attribution + schema freeze (only if your codebase defines these)
            # Optional: canonicalize + attribution + schema freeze (only if your codebase defines these)
            try:
                # 1) canonicalize (unchanged)
                if primary_data.get("primary_metrics"):
                    primary_data["primary_metrics_canonical"] = canonicalize_metrics(
                        primary_data.get("primary_metrics", {}),
                        merge_duplicates_to_range=True,
                        question_text=query,
                        category_hint=str(primary_data.get("question_category", ""))
                    )

                # 2) freeze schema FIRST ✅ (so attribution can be schema-first)
                if primary_data.get("primary_metrics_canonical"):
                    primary_data["metric_schema_frozen"] = freeze_metric_schema(
                        primary_data["primary_metrics_canonical"]
                    )

                # 3) attribution using frozen schema ✅
                if primary_data.get("primary_metrics_canonical"):
                    primary_data["primary_metrics_canonical"] = add_range_and_source_attribution_to_canonical_metrics(
                        primary_data.get("primary_metrics_canonical", {}),
                        web_context,
                        metric_schema=(primary_data.get("metric_schema_frozen") or {}),
                    )

                # PATCH SV1/EG1 (ADDITIVE): validate frozen schema + enforce evidence gating (analysis-side)
                try:
                    fn = globals().get("apply_schema_validation_and_evidence_gating")
                    if callable(fn):
                        primary_data = fn(primary_data)
                except Exception:
                    pass

            except Exception:
                pass

            # Hash key findings (optional)
            try:
                if primary_data.get("key_findings"):
                    findings_with_hash = []
                    for finding in primary_data.get("key_findings", []):
                        if finding:
                            findings_with_hash.append({
                                "text": finding,
                                "semantic_hash": compute_semantic_hash(finding)
                            })
                    primary_data["key_findings_hashed"] = findings_with_hash
            except Exception:
                pass


            # Save baseline numeric cache if available (existing behavior)

            # Build output
            output = {
                "question": query,
                "question_profile": question_profile,
                "question_category": question_profile.get("category"),
                "question_signals": question_signals,
                "side_questions": question_profile.get("side_questions", []),
                "timestamp": now_utc().isoformat(),
                "primary_response": primary_data,
                "final_confidence": final_conf,
                "veracity_scores": veracity_scores,
                "web_sources": web_context.get("sources", []),
                "code_version": CODE_VERSION,
                }

            try:
                if isinstance(output.get("primary_response"), dict):
                    output["primary_response"]["code_version"] = CODE_VERSION
            except Exception:
                pass


            # ✅ NEW: attach analysis-aligned snapshots (from scraped_meta)
            # This is the stable cache evolution should reuse.
            try:
                output = attach_source_snapshots_to_analysis(output, web_context)
            except Exception:
                pass

            with st.spinner("💾 Saving to history..."):
                if add_to_history(output):
                    st.success("✅ Analysis saved to Google Sheets")
                else:
                    st.warning("⚠️ Saved to session only (Google Sheets unavailable)")

            json_bytes = json.dumps(output, indent=2, ensure_ascii=False).encode("utf-8")
            filename = f"yureeka_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

            st.download_button(
                label="💾 Download Analysis JSON",
                data=json_bytes,
                file_name=filename,
                mime="application/json"
            )

            render_dashboard(
            primary_data,
            final_conf,
            web_context,
            base_conf,
            query,
            veracity_scores,
            web_context.get("source_reliability", [])
            )


            with st.expander("🔧 Debug Information"):
                st.write("**Confidence Breakdown:**")
                st.json({
                    "base_confidence": base_conf,
                    "evidence_score": veracity_scores.get("overall", 0),
                    "final_confidence": final_conf,
                    "veracity_breakdown": veracity_scores
                })
                st.write("**Primary Model Response:**")
                st.json(primary_data)

    # =====================
    # TAB 2: EVOLUTION ANALYSIS
    # =====================
    with tab2:
        st.markdown("""
        ### 📈 Track the evolution of key metrics over time using **deterministic source-anchored analysis**.

        **How it works:**
        - Select a baseline from your history (stored in Google Sheets)
        - Re-fetches the **exact same sources** from that analysis
        - Extracts current numbers using regex (no LLM variance)
        - Computes deterministic diffs with context-aware matching
        """)

        with st.sidebar:
            st.subheader("📚 History")

            if st.button("🔄 Refresh"):
                st.cache_resource.clear()
                st.rerun()

            sheet = get_google_sheet()
            if sheet:
                st.success("✅ Google Sheets connected")
            else:
                st.warning("⚠️ Using session storage")

            # =====================================================================
            # PATCH FIX40 (ADDITIVE): Scenario B control — Force rebuild toggle
            # - Streamlit Cloud UI has no free-text question editing (dropdown-only).
            # - This toggle lets you intentionally bypass the unchanged fastpath so you
            #   can validate the rebuild path + FIX39 publish invariants.
            # - Pure UI flag; no logic changes unless explicitly enabled.
            # =====================================================================
            force_rebuild = st.checkbox(
                "🧪 Force rebuild (ignore snapshot fastpath)",
                value=False,
                key="fix41_force_rebuild_toggle",
                help="Debug only: forces evolution to rebuild even if sources+data are unchanged."
            )
            # =====================================================================

        # ✅ FIX: your codebase uses get_history(), not load_history()
        history = get_history()

        if not history:
            st.info("📭 No previous analyses found. Run an analysis in the 'New Analysis' tab first.")
            return

        baseline_options = [
            f"{i+1}. {h.get('question', 'N/A')}  ({h.get('timestamp', '')})"
            for i, h in enumerate(history)
        ]
        baseline_choice = st.selectbox("Select baseline analysis:", baseline_options)
        baseline_idx = int(baseline_choice.split(".")[0]) - 1
        baseline_data = history[baseline_idx]

        compare_method = st.selectbox(
            "Comparison method:",
            [
                "source-anchored evolution (re-fetch same sources)",
                "another saved analysis (deterministic)",
                "fresh analysis (volatile)"
            ]
        )

        # ============================================================
        # PATCH UI_EXTRA_SOURCES1 (ADDITIVE)
        # ============================================================
        extra_sources_text = st.text_area(
            "Extra source URLs (optional, one per line)",
            placeholder="https://example.com/report\nhttps://another-source.com/page",
            help="Adds these URLs to the admitted source list for this run. Useful to test hash-mismatch rebuilds.",
            height=110,
        )

        compare_data = None
        if "another saved analysis" in compare_method:
            compare_options = [
                f"{i+1}. {h.get('question', 'N/A')}  ({h.get('timestamp', '')})"
                for i, h in enumerate(history) if i != baseline_idx
            ]
            if compare_options:
                compare_choice = st.selectbox("Select comparison analysis:", compare_options)
                compare_idx = int(compare_choice.split(".")[0]) - 1
                compare_data = history[compare_idx]
            else:
                st.warning("No other saved analyses to compare with.")

        st.markdown("---")

        if st.button("🧬 Run Evolution Analysis", type="primary"):

            if "source-anchored evolution" in compare_method:
                evolution_query = baseline_data.get("question", "")
                if not evolution_query:
                    st.error("❌ No question found in baseline.")
                    return

                with st.spinner("🧬 Running source-anchored evolution..."):

                    try:


                        # ============================================================

                        # PATCH INJ_DIAG_EVO_UI (ADDITIVE): pass extra injected URLs + run_id into evolution

                        # ============================================================

                        _evo_run_id = _inj_diag_make_run_id("evo")

                        _extra_urls_evo = []

                        try:

                            for _l in str(extra_sources_text or "").splitlines():

                                _u = _l.strip()

                                if not _u:

                                    continue

                                if _u.startswith("http://") or _u.startswith("https://"):

                                    _extra_urls_evo.append(_u)

                        except Exception:

                            _extra_urls_evo = []


                        results = run_source_anchored_evolution(

                            baseline_data,

                            web_context={

                                "force_rebuild": bool(force_rebuild),

                                "extra_urls": _extra_urls_evo,

                                "diag_run_id": _evo_run_id,

                                "diag_extra_urls_ui_raw": (extra_sources_text or ""),

                            },

                        )

                        # ============================================================


                    except Exception as e:

                        st.error(f"❌ Evolution failed: {e}")

                        return


                interpretation = ""
                try:
                    if results and isinstance(results, dict):
                        interpretation = results.get("interpretation", "") or ""
                except Exception:
                    interpretation = ""

                evolution_output = {
                    "question": evolution_query,
                    "timestamp": datetime.now().isoformat(),
                    "analysis_type": "source_anchored",
                    "previous_timestamp": baseline_data.get("timestamp"),
                    "results": results,
                    "interpretation": {
                        "text": interpretation,
                        "authoritative": False,
                        "source": "llm_optional"
                    }
                }

                st.download_button(
                    label="💾 Download Evolution Report",
                    data=json.dumps(evolution_output, indent=2, ensure_ascii=False).encode("utf-8"),
                    file_name=f"yureeka_evolution_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json"
                )

                # ✅ FIX: guarded renderer to avoid stability_score=None formatting crashes
                render_source_anchored_results(results, evolution_query)

            elif "another saved analysis" in compare_method:
                if compare_data:
                    st.success("✅ Comparing two saved analyses (deterministic)")
                    render_native_comparison(baseline_data, compare_data)
                else:
                    st.error("❌ Please select a comparison analysis")

            else:
                st.warning("⚠️ Running fresh analysis - results may vary")

                query = baseline_data.get("question", "")
                if not query:
                    st.error("❌ No query found")
                    return

                with st.spinner("🌐 Fetching current data..."):
                    # ---- ADDITIVE: pass existing snapshots for reuse (Change #3 wiring) ----
                    existing_snapshots = None

                    try:
                        prev = st.session_state.get("last_analysis")
                        if isinstance(prev, dict):
                            existing_snapshots = (
                                prev.get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("source_results")
                            )
                    except Exception:
                        existing_snapshots = None

                    web_context = fetch_web_context(
                        query,
                        num_sources=3,
                        existing_snapshots=existing_snapshots,
                    )
                    # ----------------------------------------------------------------------


                if not web_context:
                    web_context = {
                        "search_results": [],
                        "scraped_content": {},
                        "summary": "",
                        "sources": [],
                        "source_reliability": []
                    }

                with st.spinner("🤖 Running analysis..."):
                    new_response = query_perplexity(query, web_context)

                if new_response:
                    try:
                        new_parsed = json.loads(new_response)
                        veracity = evidence_based_veracity(new_parsed, web_context)
                        base_conf = float(new_parsed.get("confidence", 75))
                        final_conf = calculate_final_confidence(base_conf, veracity.get("overall", 0))

                        compare_data = {
                            "question": query,
                            "timestamp": datetime.now().isoformat(),
                            "primary_response": new_parsed,
                            "final_confidence": final_conf,
                            "veracity_scores": veracity,
                            "web_sources": web_context.get("sources", [])
                        }

                        add_to_history(compare_data)
                        st.success("✅ Saved to history")

                        render_native_comparison(baseline_data, compare_data)
                    except Exception as e:
                        st.error(f"❌ Failed: {e}")
                else:
                    st.error("❌ Analysis failed")


# ======================================================================
# PATCH SV1/EG1 (ADDITIVE): Schema validation + Evidence gating (analysis)
# - Additive only: does not remove or refactor existing code.
# - Only applied in TAB 1 (New Analysis) via a small post-pass hook.
# - Does NOT alter evolution behavior (no changes to evolution functions).
# ======================================================================

def validate_metric_schema_frozen(metric_schema_frozen: dict) -> dict:
    """
    Validate frozen metric schema for internal consistency.
    Returns: {"ok": bool, "errors": [...], "warnings": [...], "by_key": {...}}
    """
    issues = {"ok": True, "errors": [], "warnings": [], "by_key": {}}

    def _add(kind: str, canonical_key: str, msg: str):
        issues["ok"] = issues["ok"] and (kind != "errors")
        issues[kind].append({"canonical_key": canonical_key, "message": msg})
        issues["by_key"].setdefault(canonical_key, {"errors": [], "warnings": []})
        issues["by_key"][canonical_key][kind].append(msg)

    if not isinstance(metric_schema_frozen, dict):
        _add("errors", "__schema__", "metric_schema_frozen missing or not a dict")
        return issues

    for canonical_key, spec in metric_schema_frozen.items():
        if not isinstance(spec, dict):
            _add("errors", canonical_key, "schema entry not a dict")
            continue

        dim = (spec.get("dimension") or spec.get("measure_kind") or "").lower().strip()
        unit = (spec.get("unit") or spec.get("unit_tag") or "").strip()
        unit_family = (spec.get("unit_family") or spec.get("unit_family_tag") or "").lower().strip()
        name = (spec.get("name") or canonical_key or "").lower()

        # Hard conflict: currency + percent
        if dim in ("currency", "revenue", "market_value", "value") and unit in ("%", "percent", "percentage"):
            _add("errors", canonical_key, "dimension=currency but unit is percent (%)")

        # Soft checks for percent metrics without percent unit
        if ("cagr" in name or dim in ("percent", "percentage", "growth_rate")) and unit and unit not in ("%", "percent", "percentage"):
            _add("warnings", canonical_key, f"percent-like metric but unit='{unit}' (expected '%')")

        # Common drift hazard: CAGR schema includes 'share'
        kw = " ".join([str(x) for x in (spec.get("keywords") or [])]).lower()
        if "cagr" in name and "share" in kw:
            _add("warnings", canonical_key, "CAGR schema keywords include 'share' (risk of mapping share% to CAGR)")

        # Unit family conflicts
        if dim == "currency" and unit_family and unit_family not in ("currency", "money"):
            _add("warnings", canonical_key, f"dimension=currency but unit_family='{unit_family}'")

    return issues


def _metric_evidence_list(metric: dict):
    ev = metric.get("evidence")
    if isinstance(ev, list):
        return ev
    return []


def _synthesize_evidence_from_examples(metric: dict, max_items: int = 5) -> list:
    """
    If metric has value_range.examples (from attribution pass), synthesize evidence records.
    This keeps JSON stable and makes evolution rebuild auditing possible.
    """
    examples = None
    vr = metric.get("value_range")
    if isinstance(vr, dict):
        examples = vr.get("examples")
    if not isinstance(examples, list) or not examples:
        return []

    # Try to use an existing anchor hash function if present
    anchor_fn = globals().get("compute_anchor_hash")
    out = []
    for ex in examples[:max_items]:
        if not isinstance(ex, dict):
            continue
        url = ex.get("source_url") or ex.get("url") or ""
        raw = ex.get("raw") or ""
        ctx = ex.get("context") or ex.get("context_window") or ex.get("snippet") or ""
        ah = ex.get("anchor_hash") or ""
        if not ah and callable(anchor_fn):
            try:
                ah = anchor_fn(url, ctx)
            except Exception:
                ah = ""
        out.append({
            "source_url": url,
            "raw": raw,
            "context_snippet": ctx[:500] if isinstance(ctx, str) else "",
            "anchor_hash": ah,
            "method": "value_range_examples",
        })
    return out


def ensure_metric_has_evidence(metric: dict) -> dict:
    """
    Evidence gating for a single metric:
    - If evidence already exists -> no change
    - Else synthesize from value_range.examples if available
    - Else mark as proxy (do not delete or zero the metric)
    """
    if not isinstance(metric, dict):
        return metric

    ev = _metric_evidence_list(metric)
    if ev:
        return metric

    synth = _synthesize_evidence_from_examples(metric)
    if synth:
        metric["evidence"] = synth
        return metric

    # No evidence at all: mark proxy (do not alter numeric payload)
    metric.setdefault("evidence", [])
    metric["is_proxy"] = True
    metric["proxy_type"] = "evidence_missing"
    metric["proxy_reason"] = "no_evidence_anchors_available"
    metric["proxy_confidence"] = float(metric.get("proxy_confidence") or 0.2)
    return metric


def enforce_evidence_gating(primary_metrics_canonical: dict) -> dict:
    """
    Apply evidence gating across canonical metrics.
    Returns the (mutated) dict for compatibility.
    """
    if not isinstance(primary_metrics_canonical, dict):
        return primary_metrics_canonical

    for k, m in list(primary_metrics_canonical.items()):
        if isinstance(m, dict):
            primary_metrics_canonical[k] = ensure_metric_has_evidence(m)

    return primary_metrics_canonical


def apply_schema_validation_and_evidence_gating(primary_data: dict) -> dict:
    """
    New Analysis post-pass hook:
    - validates metric_schema_frozen
    - evidence-gates primary_metrics_canonical
    - marks schema-conflict metrics as proxy (does not remove anything)
    """
    if not isinstance(primary_data, dict):
        return primary_data

    # Where schema is stored
    schema = (
        primary_data.get("metric_schema_frozen")
        or (primary_data.get("primary_response") or {}).get("metric_schema_frozen")
        or (primary_data.get("results") or {}).get("metric_schema_frozen")
        or {}
    )

    validation = validate_metric_schema_frozen(schema)
    primary_response = primary_data.setdefault("primary_response", {})
    primary_response["schema_validation"] = validation

    # Mark schema-conflict metrics as proxy (additive)
    pmc = primary_data.get("primary_metrics_canonical")
    if isinstance(pmc, dict) and validation.get("by_key"):
        for ck, iss in validation["by_key"].items():
            if ck in pmc and isinstance(pmc[ck], dict) and iss.get("errors"):
                pmc[ck]["is_proxy"] = True
                pmc[ck]["proxy_type"] = "schema_conflict"
                pmc[ck]["proxy_reason"] = "schema_validation_error"
                pmc[ck]["proxy_confidence"] = float(pmc[ck].get("proxy_confidence") or 0.15)
                pmc[ck]["schema_issues"] = {"errors": iss.get("errors", []), "warnings": iss.get("warnings", [])}

    # Evidence gating
    pmc2 = primary_data.get("primary_metrics_canonical")
    if isinstance(pmc2, dict):
        before = sum(1 for v in pmc2.values() if isinstance(v, dict) and _metric_evidence_list(v))
        enforce_evidence_gating(pmc2)
        after = sum(1 for v in pmc2.values() if isinstance(v, dict) and _metric_evidence_list(v))
        prox = sum(1 for v in pmc2.values() if isinstance(v, dict) and v.get("is_proxy"))
        primary_response["evidence_gating_summary"] = {
            "total_metrics": len(pmc2),
            "metrics_with_evidence_before": before,
            "metrics_with_evidence_after": after,
            "metrics_marked_proxy": prox,
        }

    return primary_data



# ===================== PATCH RMS_UNWRAP1 (ADDITIVE) =====================
def _normalize_prev_response_for_rebuild(previous_data):
    """Best-effort normalization of the loaded baseline object for rebuild dispatch.
    - If previous_data is a JSON string, parse it.
    - If it contains nested 'primary_response' as JSON string, parse it.
    - If it contains a top-level wrapper with 'data' or 'results', keep as dict.
    This is additive and only affects rebuild dispatch input normalization.
    """
    import json
    try:
        pd = previous_data
        if isinstance(pd, str) and pd.strip().startswith(("{","[")):
            try:
                pd = json.loads(pd)
            except Exception:
                pd = previous_data
        if isinstance(pd, dict):
            pr = pd.get("primary_response")
            if isinstance(pr, str) and pr.strip().startswith(("{","[")):
                try:
                    pd["primary_response"] = json.loads(pr)
                except Exception:
                    pass
            # Some callers store the main payload under 'data'
            d = pd.get("data")
            if isinstance(d, str) and d.strip().startswith(("{","[")):
                try:
                    pd["data"] = json.loads(d)
                except Exception:
                    pass
        return pd
    except Exception:
        return previous_data
# =================== END PATCH RMS_UNWRAP1 (ADDITIVE) ===================


if __name__ == "__main__":
    main()


# ===================== PATCH RMS_DISPATCH2 (ADDITIVE) =====================
def _get_metric_anchors_any(prev_response: dict) -> dict:
    """Best-effort retrieval of metric_anchors from any plausible location (additive helper)."""
    try:
        if not isinstance(prev_response, dict):
            return {}
        for path in (
            ("metric_anchors",),
            ("results", "metric_anchors"),
            ("primary_response", "metric_anchors"),
            ("primary_response", "results", "metric_anchors"),
        ):
            cur = prev_response
            ok = True
            for k in path:
                if isinstance(cur, dict) and k in cur:
                    cur = cur[k]
                else:
                    ok = False
                    break
            if ok and isinstance(cur, dict) and cur:
                return cur
        return {}
    except Exception:
        return {}

def _coerce_prev_response_any(previous_data):
    """Normalize previous_data into a dict-shaped 'prev_response' for rebuild dispatch (additive helper)."""
    try:
        return previous_data if isinstance(previous_data, dict) else {}
    except Exception:
        return {}
# =================== END PATCH RMS_DISPATCH2 (ADDITIVE) ===================

# =====================================================================
# PATCH FIX16 (ADDITIVE): close analysis↔evolution metric lock-down gaps
# Goals (deterministic, no re-architecture):
#   1) De-year schema keyword scoring for non-year metrics
#   2) Hard unit expectation gating (unitless years can't win currency/percent)
#   3) Absolute anchor priority when anchors exist
# Notes:
#   - Additive only: we define FIX16 rebuild functions and re-wire dispatch
#   - No refetch, no heuristics beyond schema/unit/anchors
# =====================================================================

def _fix16_is_year_token(s: str) -> bool:
    try:
        s2 = str(s or "").strip()
        return bool(re.fullmatch(r"(19\d{2}|20\d{2})", s2))
    except Exception:
        return False


def _fix16_metric_is_year_like(metric_spec: dict, canonical_key: str = "") -> bool:
    """Deterministic allow-list for metrics whose value is genuinely a year."""
    try:
        spec = metric_spec or {}
        blob = " ".join([
            str(canonical_key or ""),
            str(spec.get("name") or ""),
            str(spec.get("canonical_key") or spec.get("canonical_id") or ""),
            " ".join([str(x) for x in (spec.get("keywords") or spec.get("keyword_hints") or []) if x]),
            str(spec.get("dimension") or ""),
        ]).lower()
        # year-ish intents
        return any(k in blob for k in (" year", "year_", "founded", "since", "established", "launch year", "model year"))
    except Exception:
        return False


def _fix16_prune_year_keywords(keywords: list, metric_is_year_like: bool) -> list:
    """Remove YYYY tokens from keyword scoring unless the metric is year-like."""
    try:
        if metric_is_year_like:
            return list(keywords or [])
        out = []
        for k in (keywords or []):
            if _fix16_is_year_token(k):
                continue
            out.append(k)
        return out
    except Exception:
        return list(keywords or [])


def _fix16_expected_dimension(metric_spec: dict) -> str:
    try:
        spec = metric_spec or {}
        dim = (spec.get("dimension") or spec.get("unit_family") or spec.get("expected_unit_family") or "").strip().lower()
        return dim
    except Exception:
        return ""


def _fix16_candidate_has_any_unit(c: dict) -> bool:
    try:
        if not isinstance(c, dict):
            return False
        for k in ("base_unit", "unit", "unit_tag", "unit_family"):
            if str(c.get(k) or "").strip():
                return True
        # raw sometimes carries $ or % even if unit field blank
        raw = str(c.get("raw") or "")
        if "$" in raw or "%" in raw:
            return True
        return False
    except Exception:
        return False


def _fix16_unit_compatible(c: dict, expected_dim: str) -> bool:
    """Hard gate: if schema expects a unit family, candidate must be compatible."""
    try:
        if not expected_dim:
            return True
        if not isinstance(c, dict):
            return False

        # Treat these as requiring explicit unit-ness
        requires_unit = expected_dim in ("currency", "percent", "rate", "ratio")
        if requires_unit and not _fix16_candidate_has_any_unit(c):
            return False

        # If candidate has a unit_family, require match
        cand_fam = (c.get("unit_family") or "").strip().lower()
        if cand_fam:
            return cand_fam == expected_dim

        # Infer from unit/raw when unit_family missing
        u = (c.get("base_unit") or c.get("unit") or c.get("unit_tag") or "").strip().lower()
        raw = str(c.get("raw") or "").lower()

        if expected_dim == "percent":
            return ("%" in u) or ("percent" in u) or ("%" in raw)

        if expected_dim == "currency":
            # currency symbols/codes or magnitude suffix paired with a currency marker in raw
            if any(x in u for x in ("usd", "eur", "gbp", "jpy", "cny", "aud", "sgd", "$", "€", "£", "¥")):
                return True
            if "$" in raw or "usd" in raw or "sgd" in raw or "eur" in raw or "gbp" in raw:
                return True
            # if unit is magnitude only (M/B/K), require a currency marker in raw
            if u in ("m", "b", "k", "t", "mn", "bn", "million", "billion"):
                return ("$" in raw) or ("usd" in raw) or ("sgd" in raw) or ("eur" in raw) or ("gbp" in raw)
            return False

        # Quantity/rate/ratio are tricky; enforce only the unit-presence gate above.
        return True
    except Exception:
        return True


def _fix16_candidate_allowed(c: dict, metric_spec: dict, canonical_key: str = "") -> bool:
    """Compose fix15 exclusion + fix16 hard unit gate + year-token guard."""
    try:
        if not isinstance(c, dict):
            return False

        # Respect fix15 junk/year-only exclusion if present
        fn = globals().get("_candidate_disallowed_for_metric")
        if callable(fn):
            if fn(c, dict(metric_spec or {}, canonical_key=canonical_key)):
                return False

        expected_dim = _fix16_expected_dimension(metric_spec)
        if not _fix16_unit_compatible(c, expected_dim):
            return False

        # Extra deterministic guard: unitless year-like numbers should never compete
        # for non-year metrics even if upstream tagging missed them.
        if not _fix16_metric_is_year_like(metric_spec, canonical_key=canonical_key):
            # PATCH FIX2B_RANGE_SCHEMA_V1 (ADDITIVE): range uses schema-unit value when available
            v = c.get("value") if c.get("value") is not None else c.get("value_norm")
            u = (c.get("base_unit") or c.get("unit") or "").strip()
            if u == "" and isinstance(v, (int, float)):
                iv = int(v)
                if 1900 <= iv <= 2100:
                    return False

        return True
    except Exception:
        return True


def rebuild_metrics_from_snapshots_with_anchors_fix16(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    FIX16 anchor-aware rebuild:
      - Absolute anchor priority when anchor_hash exists in prev_response.metric_anchors
      - Hard disallow junk/year-like unitless candidates for non-year metrics
      - Hard unit expectation gating for currency/percent/rate/ratio dimensions
    """
    import re

    if not isinstance(prev_response, dict):
        return {}

    metric_anchors = (
        prev_response.get("metric_anchors")
        or (prev_response.get("primary_response") or {}).get("metric_anchors")
        or (prev_response.get("results") or {}).get("metric_anchors")
        or {}
    )
    if not isinstance(metric_anchors, dict) or not metric_anchors:
        return {}

    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )

    # Build deterministic candidate index (anchor_hash -> best candidate)
    fn_idx = globals().get("_es_build_candidate_index_deterministic")
    cand_index = fn_idx(baseline_sources_cache) if callable(fn_idx) else {}

    rebuilt = {}

    for canonical_key, a in (metric_anchors or {}).items():
        if not isinstance(a, dict):
            continue
        ah = a.get("anchor_hash") or a.get("anchor") or ""
        if not ah:
            continue

        spec = (metric_schema.get(canonical_key) if isinstance(metric_schema, dict) else None) or {}
        spec = dict(spec)
        spec.setdefault("name", a.get("name") or canonical_key)
        spec.setdefault("canonical_key", canonical_key)

        c = cand_index.get(ah)
        if not isinstance(c, dict):
            continue

        # FIX16 eligibility hard-gates
        if not _fix16_candidate_allowed(c, spec, canonical_key=canonical_key):
            continue

        rebuilt[canonical_key] = {
            "canonical_key": canonical_key,
            "name": spec.get("name") or canonical_key,
            "value": c.get("value"),
            "unit": c.get("unit") or "",
            "value_norm": c.get("value_norm"),
            "source_url": c.get("source_url") or "",
            "anchor_hash": c.get("anchor_hash") or ah,
            "evidence": [{
                "source_url": c.get("source_url") or "",
                "raw": c.get("raw") or "",
                "context_snippet": (c.get("context_snippet") or c.get("context") or c.get("context_window") or "")[:400],
                "anchor_hash": c.get("anchor_hash") or ah,
                "method": "anchor_hash_rebuild_fix16",
            }],
            "anchor_used": True,
        }

    return rebuilt


def rebuild_metrics_from_snapshots_schema_only_fix16(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    FIX16 schema-only rebuild:
      - Removes YYYY tokens from keyword scoring for non-year metrics
      - Hard unit expectation gating
      - Applies fix15 junk/year exclusion + fix16 extra year-token disallow
      - Deterministic selection/tie-breaks
    """
    import re

    if not isinstance(prev_response, dict):
        return {}

    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )
    if not isinstance(metric_schema, dict) or not metric_schema:
        return {}

    # Flatten snapshot candidates (no re-fetch)
    if isinstance(baseline_sources_cache, dict) and isinstance(baseline_sources_cache.get("snapshots"), list):
        sources = baseline_sources_cache.get("snapshots", [])
    elif isinstance(baseline_sources_cache, list):
        sources = baseline_sources_cache
    else:
        sources = []

    candidates = []
    for s in sources:
        if not isinstance(s, dict):
            continue
        url = s.get("source_url") or s.get("url") or ""
        xs = s.get("extracted_numbers")
        if isinstance(xs, list) and xs:
            for c in xs:
                if not isinstance(c, dict):
                    continue
                c2 = dict(c)
                c2.setdefault("source_url", url)
                candidates.append(c2)

    def _norm(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def _cand_sort_key(c: dict):
        try:
            return (
                str(c.get("anchor_hash") or ""),
                str(c.get("source_url") or ""),
                int(c.get("start_idx") or 0),
                str(c.get("raw") or ""),
                str(c.get("unit") or ""),
                float(c.get("value_norm") or 0.0),
            )
        except Exception:
            return ("", "", 0, "", "", 0.0)

    # Deterministic global ordering of candidates
    candidates.sort(key=_cand_sort_key)

    rebuilt = {}

    for canonical_key, sch in metric_schema.items():
        if not isinstance(sch, dict):
            continue

        spec = dict(sch)
        spec.setdefault("canonical_key", canonical_key)
        spec.setdefault("name", sch.get("name") or canonical_key)

        metric_is_year_like = _fix16_metric_is_year_like(spec, canonical_key=canonical_key)

        keywords = sch.get("keywords") or sch.get("keyword_hints") or []
        if isinstance(keywords, str):
            keywords = [keywords]
        keywords = _fix16_prune_year_keywords(list(keywords), metric_is_year_like)
        kw_norm = [_norm(k) for k in keywords if k]

        expected_dim = _fix16_expected_dimension(spec)

        best = None
        best_tie = None

        for c in candidates:
            # FIX16 hard eligibility gates
            if not _fix16_candidate_allowed(c, spec, canonical_key=canonical_key):
                continue

            # keyword relevance
            ctx = _norm(c.get("context_snippet") or c.get("context") or c.get("context_window") or "")
            raw = _norm(c.get("raw") or "")

            hits = 0
            for k in kw_norm:
                if k and (k in ctx or k in raw):
                    hits += 1

            # If there are no keyword hits at all, keep as weak fallback only if unit family matches strongly
            # but do not select zero-hit candidates over hit candidates.
            tie = (-hits,) + _cand_sort_key(c)
            if best is None or tie < best_tie:
                best = c
                best_tie = tie

        if not isinstance(best, dict):
            continue

        # Require at least one keyword hit unless the schema has no keywords
        if kw_norm:
            if best_tie is not None and isinstance(best_tie, tuple):
                try:
                    if (-best_tie[0]) <= 0:
                        continue
                except Exception:
                    pass

        rebuilt[canonical_key] = {
            "canonical_key": canonical_key,
            "name": spec.get("name") or canonical_key,
            "value": best.get("value"),
            "unit": best.get("unit") or "",
            "value_norm": best.get("value_norm"),
            "source_url": best.get("source_url") or "",
            "anchor_hash": best.get("anchor_hash") or "",
            "evidence": [{
                "source_url": best.get("source_url") or "",
                "raw": best.get("raw") or "",
                "context_snippet": (best.get("context_snippet") or best.get("context") or best.get("context_window") or "")[:400],
                "anchor_hash": best.get("anchor_hash") or "",
                "method": "schema_only_rebuild_fix16",
            }],
            "anchor_used": False,
        }

    return rebuilt


# =====================================================================
# PATCH FIX16 (ADDITIVE): wire FIX16 rebuilds into the existing dispatch
# - Keep names identical so evolution uses these as the LAST definitions
# - We expose both functions while preserving older ones for reference
# =====================================================================

def rebuild_metrics_from_snapshots_with_anchors(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:  # noqa: F811
    return rebuild_metrics_from_snapshots_with_anchors_fix16(prev_response, baseline_sources_cache, web_context=web_context)


def rebuild_metrics_from_snapshots_schema_only(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:  # noqa: F811
    return rebuild_metrics_from_snapshots_schema_only_fix16(prev_response, baseline_sources_cache, web_context=web_context)

# =====================================================================
# END PATCH FIX16
# =====================================================================


# =====================================================================
# PATCH PH2B_S1 (ADDITIVE): Extract a PURE analysis-canonical final selector (v1)
# Goal:
#   - Provide exactly ONE authoritative selector for dashboard-facing "Current"
#   - Treat candidate.value_norm as schema units (no base-unit assumption)
#   - Deterministic tie-breaks; NO IO; NO re-fetch; NO hashing changes
#
# Notes:
#   - This is a single-metric selector. Batch rebuild helpers may call it.
#   - Reuses FIX16 hard eligibility gates (_fix16_candidate_allowed) + FIX16 scoring.
#   - Adds optional preferred/anchor lock when anchors are present (stays in preferred source).
# =====================================================================

def _ph2b_norm_url(url: str) -> str:
    try:
        fn = globals().get("_normalize_url")
        if callable(fn):
            return str(fn(url or ""))
    except Exception:
        pass
    return str((url or "").strip())

def _analysis_canonical_final_selector_v1(
    canonical_key: str,
    schema_frozen: dict,
    candidates: list,
    anchors: dict = None,
    prev_metric: dict = None,
    web_context: dict = None,
) -> tuple:
    """Pure selector: returns (best_metric_or_None, meta_dict)."""
    import re

    meta = {
        "selector_used": "analysis_canonical_v1",
        "canonical_key": canonical_key or "",
        "anchor_used": False,
        "blocked_reason": "",
        "preferred_url": "",
        "chosen_source_url": "",
        "tie_break": "",
        "eligible_count": 0,
        "range_method": "",
    }

    spec = schema_frozen or {}
    if not isinstance(spec, dict) or not spec:
        meta["blocked_reason"] = "missing_schema"
        return None, meta

    # Determine preferred URL from anchors (strongest) or schema if present
    preferred_url = ""
    anchor = None
    if isinstance(anchors, dict) and canonical_key in anchors and isinstance(anchors.get(canonical_key), dict):
        anchor = anchors.get(canonical_key) or {}
        preferred_url = anchor.get("source_url") or ""
    preferred_url = preferred_url or (spec.get("preferred_url") or spec.get("source_url") or "")
    if preferred_url:
        meta["preferred_url"] = _ph2b_norm_url(preferred_url)

    # Prepare FIX16 keyword scoring (same as rebuild_metrics_from_snapshots_schema_only_fix16)
    def _norm(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def _cand_sort_key(c: dict):
        try:
            return (
                str(c.get("anchor_hash") or ""),
                str(c.get("source_url") or ""),
                int(c.get("start_idx") or 0),
                str(c.get("raw") or ""),
                str(c.get("unit") or ""),
                float(c.get("value_norm") or 0.0),
            )
        except Exception:
            return ("", "", 0, "", "", 0.0)

    # FIX16 keyword pruning helper if present; otherwise keep keywords as-is
    metric_is_year_like = False
    try:
        fn_year = globals().get("_fix16_metric_is_year_like")
        if callable(fn_year):
            metric_is_year_like = bool(fn_year(spec, canonical_key=canonical_key))
    except Exception:
        metric_is_year_like = False

    keywords = spec.get("keywords") or spec.get("keyword_hints") or []
    if isinstance(keywords, str):
        keywords = [keywords]
    try:
        fn_prune = globals().get("_fix16_prune_year_keywords")
        if callable(fn_prune):
            keywords = fn_prune(list(keywords), metric_is_year_like)
    except Exception:
        keywords = list(keywords) if isinstance(keywords, list) else []

    kw_norm = [_norm(k) for k in (keywords or []) if k]

    # Candidate filtering
    cands = [c for c in (candidates or []) if isinstance(c, dict)]
    # PATCH FIX2B_TRACE_V1 (ADDITIVE): candidate counts for trace
    try:
        meta["candidate_count_in"] = int(len(cands))
    except Exception:
        pass
    # Enforce preferred source lock when available (prevents cross-source hijack)
    if meta["preferred_url"]:
        pref = meta["preferred_url"]
        cands_pref = []
        for c in cands:
            cu = _ph2b_norm_url(c.get("source_url") or "")
            if cu and cu == pref:
                cands_pref.append(c)
        # If preferred exists but yields zero candidates, we keep empty (hard lock).
        cands = cands_pref
        # PATCH FIX2B_TRACE_V1 (ADDITIVE): preferred-locked candidate count
        try:
            meta["candidate_count_pref"] = int(len(cands))
        except Exception:
            pass

    eligible = []
    for c in cands:
        try:
            # =====================================================================
            # PATCH PH2B_UF1 (ADDITIVE): Fill missing unit_family/unit_cmp deterministically
            # Many snapshot candidates omit unit_family even when unit_tag/raw clearly indicates
            # magnitude/percent/currency. The analysis selector treats unit_family as authoritative
            # for schema gating; leaving it blank causes false ineligibility (empty Current).
            # =====================================================================
            try:
                if isinstance(c, dict) and not str(c.get("unit_family") or "").strip():
                    _raw = str(c.get("raw") or "")
                    _ut = str(c.get("unit_tag") or c.get("unit") or "")
                    _ctx = str(c.get("context_snippet") or "")
                    _blob = (" ".join([_raw, _ut, _ctx])).lower()
                    uf = ""
                    if "%" in _blob or "percent" in _blob or "percentage" in _blob:
                        uf = "percent"
                    elif any(tok in _blob for tok in ["usd", "sgd", "eur", "gbp", "$", "€", "£", "¥", "aud", "cad", "inr", "cny", "rmb"]):
                        uf = "currency"
                    else:
                        # Magnitude / counts (incl. unit sales)
                        if any(w in _blob for w in ["million", "billion", "thousand", "trillion"]) or re.search(r"[mbkt]", _blob):
                            uf = "magnitude"
                        elif str(c.get("measure_kind") or "").lower() in ("count_units", "count", "quantity"):
                            uf = "magnitude"
                        elif str(c.get("measure_assoc") or "").lower() in ("units", "unit_sales", "sales"):
                            uf = "magnitude"
                    if uf:
                        c["unit_family"] = uf
                        # unit_cmp hint (best-effort; used only for display/debug)
                        if not str(c.get("unit_cmp") or "").strip():
                            if uf == "percent":
                                c["unit_cmp"] = "%"
                            elif uf == "currency":
                                c["unit_cmp"] = "currency"
                            else:
                                c["unit_cmp"] = (_ut or "").strip()
            except Exception:
                pass

            if not _fix16_candidate_allowed(c, spec, canonical_key=canonical_key):
                continue

            # =====================================================================
            # PATCH FIX2B_SEL23 (ADDITIVE): unit-family hard gating + year suppression + scaled-magnitude unit evidence
            # - Rejects percent/currency evidence when schema expects magnitude (prevents % hijacks).
            # - Suppresses unitless bare years (e.g., 2030) as candidates.
            # - Enforces unit evidence for scaled magnitude schemas (million/billion/etc.).
            # =====================================================================
            try:
                _raw0 = str(c.get("raw") or "").strip()
                _raw0_l = _raw0.lower()
                _v0 = c.get("value_norm", None)
                if _v0 is None:
                    _v0 = c.get("value", None)

                _cand_family = str(c.get("unit_family") or "").strip().lower()
                _cand_ucmp = str(c.get("unit_cmp") or c.get("unit_tag") or "").strip().lower()
                _spec_family = str(spec.get("unit_family") or "").strip().lower()
                # =====================================================================
                # PATCH FIX2B_SCHEMAFAM_INFER_V1 (ADDITIVE):
                # If schema.unit_family is missing/blank, infer expected family deterministically
                # from schema.dimension / canonical_key suffixes using existing FIX17 helper.
                # This prevents silent bypass of percent/currency hard-gates.
                # =====================================================================
                try:
                    if not _spec_family:
                        _fn_exp = globals().get("_fix17_expected_dimension")
                        if callable(_fn_exp):
                            _exp = str(_fn_exp(spec, canonical_key=canonical_key) or "").strip().lower()
                            if _exp in ("percent", "currency", "magnitude", "rate", "ratio"):
                                _spec_family = _exp
                except Exception:
                    pass
                _spec_ut = str(spec.get("unit_tag") or spec.get("unit") or "").strip().lower()

                # evidence flags
                _has_pct = ("%" in _raw0) or ("percent" in _raw0_l) or ("%" in _cand_ucmp) or (_cand_family == "percent")
                _has_ccy = any(sym in _raw0 for sym in ("$", "€", "£", "¥")) or any(tok in _raw0_l for tok in ("usd", "eur", "sgd", "gbp", "jpy")) or (_cand_family == "currency")

                _has_unit_ev0 = False
                try:
                    for _k in ("base_unit", "unit", "unit_tag", "unit_family"):
                        if str(c.get(_k) or "").strip():
                            _has_unit_ev0 = True
                            break
                    if not _has_unit_ev0:
                        if any(tok in _raw0_l for tok in ("million", "billion", "trillion", "mn", "bn", "thousand")):
                            _has_unit_ev0 = True
                        if _has_pct or _has_ccy:
                            _has_unit_ev0 = True
                except Exception:
                    pass

                # unit-family hard gating (schema-driven)
                try:
                    if _spec_family == "percent":
                        if not _has_pct:
                            meta["blocked_reason"] = "percent_evidence_missing_hard_block"
                            continue
                    elif _spec_family == "currency":
                        if not _has_ccy:
                            meta["blocked_reason"] = "currency_evidence_missing_hard_block"
                            continue
                    elif _spec_family == "magnitude":
                        # reject percent/currency candidates outright
                        if _has_pct or _has_ccy:
                            meta["blocked_reason"] = "unit_mismatch_hard_block"
                            continue
                except Exception:
                    pass

                # year-only candidate suppression (strict): only when unit evidence is missing
                try:
                    if (not _has_unit_ev0) and isinstance(_v0, (int, float)):
                        _iv0 = int(float(_v0))
                        if 1900 <= _iv0 <= 2100:
                            import re as _re
                            if _re.fullmatch(r"(19\d{2}|20\d{2})", _raw0 or str(_iv0)):
                                continue
                except Exception:
                    pass

                # scaled magnitude requires unit evidence (schema implies million/billion/etc.)
                try:
                    _spec_nm = str(spec.get("name") or "").lower()
                    _scaled = any(t in _spec_ut for t in ("million", "billion", "trillion", "thousand")) or (_spec_ut in ("m", "b", "t", "k"))
                    _countish = any(t in _spec_nm for t in ("unit", "units", "sales", "deliveries", "shipments", "registrations", "volume"))
                    if _spec_family == "magnitude" and _scaled and _countish and (not _has_unit_ev0):
                        meta["blocked_reason"] = "unit_evidence_missing_hard_block"
                        continue
                except Exception:
                    pass

                # =====================================================================
                # PATCH FIX2B_SCALE_EV_V2 (2026-01-10)
                # Purpose: Fix broken scale/countish gating where earlier patch strings were truncated
                #          (e.g., "milli..." / "uni...") and therefore never matched.
                #          Enforce: if schema implies scaled magnitude (million/billion/etc.), unit evidence
                #          must exist in the candidate (unit_tag/unit_cmp/raw/context), else hard-block.
                # Safety: additive-only; does not change fastpath/hashing/injection/snapshot attach.
                # =====================================================================
                try:
                    _spec_nm2 = str(spec.get("name") or spec.get("label") or "").lower()
                    _spec_ut2 = str(_spec_ut or "").lower()

                    def _ph2b_has_scale_token(_s: str) -> bool:
                        try:
                            _s = str(_s or "").lower()
                        except Exception:
                            _s = ""
                        if not _s:
                            return False
                        toks = [
                            "million", "mn", "m", "millions",
                            "billion", "bn", "b", "billions",
                            "trillion", "tn", "t", "trillions",
                            "thousand", "k", "000",
                        ]
                        # require word-boundary-ish for single-letter tokens
                        if "million" in _s or "millions" in _s or "billion" in _s or "billions" in _s or "trillion" in _s or "trillions" in _s or "thousand" in _s:
                            return True
                        import re as _re2
                        if _re2.search(r"\b(mn|bn|tn|k)\b", _s):
                            return True
                        # "m" / "b" / "t" are too ambiguous; only accept when adjacent to "usd/units/sales" etc.
                        if _re2.search(r"\b(m|b|t)\b", _s) and _re2.search(r"(units?|sales|deliveries|shipments|vehicles|usd|eur|sgd|gbp|jpy|cny|aud|cad)", _s):
                            return True
                        return False

                    def _ph2b_has_unit_evidence_candidate(_c: dict) -> bool:
                        if not isinstance(_c, dict):
                            return False
                        if str(_c.get("unit_tag") or "").strip():
                            return True
                        if str(_c.get("unit_cmp") or "").strip():
                            return True
                        if str(_c.get("unit_family") or "").strip():
                            return True
                        # raw/context tokens
                        if _ph2b_has_scale_token(_c.get("raw")):
                            return True
                        if _ph2b_has_scale_token(_c.get("context_snippet") or _c.get("context")):
                            return True
                        return False

                    _schema_scaled = _ph2b_has_scale_token(_spec_ut2)
                    # If schema is scaled, we hard-require candidate unit evidence regardless of name.
                    if _spec_family == "magnitude" and _schema_scaled and (not _ph2b_has_unit_evidence_candidate(c0)):
                        meta["blocked_reason"] = "unit_evidence_missing_hard_block"
                        continue

                    # Extra guard: suppress obvious document-structure numbers (pages/figures) when schema is scaled.
                    try:
                        if _spec_family == "magnitude" and _schema_scaled:
                            _ctx = str((c0 or {}).get("context_snippet") or (c0 or {}).get("context") or "").lower()
                            if ("pages" in _ctx) or ("figures" in _ctx) or ("page " in _ctx):
                                # treat as ineligible rather than junking globally
                                continue
                    except Exception:
                        pass
                except Exception:
                    pass
                # =====================================================================
                # PATCH FIX2B_SCALE_EV_V2 END
                # =====================================================================

                # =====================================================================
                # PATCH FIX2B_SCALE_EV_V1 (ADDITIVE): require *scale* evidence for scaled magnitude schemas
                # Why:
                # - Earlier gating treated inferred unit_family='magnitude' as "unit evidence", allowing unitless
                #   integers (e.g., 170) to pass for schemas like "million units".
                # - For scaled schemas, we require explicit scale evidence in raw/unit_tag/unit_cmp (million/billion/etc.).
                # Safety:
                # - Only applies when schema implies a scale (million/billion/thousand/trillion or M/B/K/T).
                # - Does NOT change fastpath/hashing/injection/snapshot attach.
                # =====================================================================
                try:
                    _scaled2 = False
                    try:
                        _scaled2 = any(t in _spec_ut for t in ("million", "billion", "trillion", "thousand")) or (_spec_ut in ("m", "b", "t", "k"))
                    except Exception:
                        _scaled2 = False
                    if _spec_family == "magnitude" and _scaled2:
                        _blob = (" ".join([
                            str(c.get("raw") or ""),
                            str(c.get("unit_cmp") or ""),
                            str(c.get("unit_tag") or c.get("unit") or ""),
                            str(c.get("context_snippet") or c.get("context") or ""),
                        ])).lower()
                        _has_scale_ev = any(tok in _blob for tok in ("million", "billion", "trillion", "thousand", "mn", "bn")) or bool(re.search(r"\b[mbkt]\b", _blob))
                        # =================================================================
                        # PATCH FIX2B_SCALE_MATCH_V1 (ADDITIVE):
                        # For scaled schemas, require scale to *match* the schema (e.g., million vs billion).
                        # Prevents wrong-scale candidates from surviving for 'million units' schemas.
                        # =================================================================
                        try:
                            _schema_scale = ""
                            if "million" in _spec_ut or _spec_ut == "m":
                                _schema_scale = "m"
                            elif "billion" in _spec_ut or _spec_ut == "b":
                                _schema_scale = "b"
                            elif "thousand" in _spec_ut or _spec_ut == "k":
                                _schema_scale = "k"
                            elif "trillion" in _spec_ut or _spec_ut == "t":
                                _schema_scale = "t"

                            _cand_scale = ""
                            if any(t in _blob for t in ("million", "mn")) or bool(re.search(r"m", _blob)):
                                _cand_scale = "m"
                            elif any(t in _blob for t in ("billion", "bn")) or bool(re.search(r"b", _blob)):
                                _cand_scale = "b"
                            elif "thousand" in _blob or bool(re.search(r"k", _blob)):
                                _cand_scale = "k"
                            elif "trillion" in _blob or bool(re.search(r"t", _blob)):
                                _cand_scale = "t"

                            if _schema_scale and _cand_scale and _schema_scale != _cand_scale:
                                meta["blocked_reason"] = "scale_mismatch_hard_block"
                                continue
                        except Exception:
                            pass
                        if not _has_scale_ev:
                            meta["blocked_reason"] = "scale_evidence_missing_hard_block"
                            continue
                except Exception:
                    pass
            except Exception:
                pass
            # =====================================================================
        except Exception:
            continue
        eligible.append(c)

    meta["eligible_count"] = int(len(eligible) or 0)
    # PATCH FIX2B_TRACE_V1 (ADDITIVE): eligible candidate count
    try:
        meta["candidate_count_eligible"] = int(len(eligible))
    except Exception:
        pass

    if not eligible:
        meta["blocked_reason"] = "no_eligible_candidates_in_preferred_source" if meta["preferred_url"] else "no_eligible_candidates"
        return None, meta

    # Deterministic scoring: keyword hits + stable tie-break
    best = None
    best_tie = None
    for c in sorted(eligible, key=_cand_sort_key):
        ctx = _norm(c.get("context_snippet") or c.get("context") or c.get("context_window") or "")
        raw = _norm(c.get("raw") or "")
        hits = 0
        for k in kw_norm:
            if not k:
                continue
            if k in ctx:
                hits += 2
            if k in raw:
                hits += 1

        # Prefer anchored candidate_id/anchor_hash when anchors exist
        anchor_bonus = 0
        if isinstance(anchor, dict) and anchor:
            ah = str(anchor.get("anchor_hash") or "")
            cid = str(anchor.get("candidate_id") or "")
            if ah and str(c.get("anchor_hash") or "") == ah:
                anchor_bonus += 10
            if cid and str(c.get("candidate_id") or "") == cid:
                anchor_bonus += 10

        score = hits + anchor_bonus

        # Tie-break: higher score, then earlier occurrence, then stable sort key
        tie = (int(score), -int(c.get("start_idx") or 0), _cand_sort_key(c))
        if best is None or tie > best_tie:
            best = c
            best_tie = tie

    if best is None:
        meta["blocked_reason"] = "no_winner_after_scoring"
        return None, meta

    # Build value_range in schema units (NO double divide)
    try:
        vals = []
        for c in eligible:
            v = c.get("value_norm")
            if v is None:
                # fallback to parsing 'value'/'raw' in the unit of the candidate/spec
                try:
                    fn_parse = globals().get("_parse_num")
                    if callable(fn_parse):
                        v = fn_parse(c.get("value"), c.get("unit") or "") or fn_parse(c.get("raw"), c.get("unit") or "")
                except Exception:
                    v = None
            if v is None:
                continue
            try:
                vals.append(float(v))
            except Exception:
                pass
        if len(vals) >= 2:
            vmin = min(vals); vmax = max(vals)
            meta["value_range"] = {"min": vmin, "max": vmax, "n": len(vals), "method": "ph2b_schema_unit_range_v2|fix2b_range4"}
            meta["range_method"] = "ph2b_schema_unit_range_v2|fix2b_range4"
    except Exception:
        pass

    out = {
        "name": spec.get("name") or spec.get("label") or canonical_key,
        "canonical_key": canonical_key,
        "value": best.get("value"),
        "unit": best.get("unit") or spec.get("unit_tag") or "",
        "value_norm": best.get("value_norm"),
        "source_url": best.get("source_url") or "",
        "anchor_hash": best.get("anchor_hash") or "",
        "candidate_id": best.get("candidate_id") or "",
        "context_snippet": best.get("context_snippet") or best.get("context") or "",
        "anchor_used": bool(isinstance(anchor, dict) and anchor and (
            (anchor.get("anchor_hash") and str(best.get("anchor_hash") or "") == str(anchor.get("anchor_hash")))
            or (anchor.get("candidate_id") and str(best.get("candidate_id") or "") == str(anchor.get("candidate_id")))
        )),
        "evidence": [{
            "source_url": best.get("source_url") or "",
            "raw": best.get("raw") or "",
            "context_snippet": (best.get("context_snippet") or best.get("context") or best.get("context_window") or "")[:400],
            "anchor_hash": best.get("anchor_hash") or "",
            "method": "analysis_canonical_selector_v1",
        }],
    }

    meta["anchor_used"] = bool(out.get("anchor_used"))
    meta["chosen_source_url"] = _ph2b_norm_url(out.get("source_url") or "")
    # =====================================================================

    # =====================================================================
    # PATCH FIX2B_TRACE_V2 (ADDITIVE): richer trace fields (NO behavior change)
    # - Adds winner_candidate_debug + would_block_reason for scaled schemas.
    # =====================================================================
    try:
        _winner = out if isinstance(out, dict) else {}
        meta["winner_candidate_debug"] = {
        "canonical_key": str(canonical_key),
        "source_url": str(_winner.get("source_url") or ""),
        "source_url_norm": _ph2b_norm_url(_winner.get("source_url") or ""),
        "candidate_id": str(_winner.get("candidate_id") or _winner.get("id") or _winner.get("anchor_hash") or ""),
        "value_norm": _winner.get("value_norm"),
        "raw": str(_winner.get("raw") or _winner.get("value") or ""),
        "unit_cmp": str(_winner.get("unit_cmp") or ""),
        "unit_family": str(_winner.get("unit_family") or ""),
        "unit_tag": str(_winner.get("unit_tag") or ""),
        "context_snippet": str(_winner.get("context_snippet") or _winner.get("context") or ""),
        }

    # "Would block" diagnostic: scaled magnitude schema but chosen candidate lacks unit evidence.
        _spec_unit = str((schema_frozen or {}).get("unit_tag") or (schema_frozen or {}).get("unit") or "")
        _spec_unit_l = _spec_unit.lower()
        _is_scaled = any(tok in _spec_unit_l for tok in ["million", "billion", "trillion", "thousand", "mn", "bn", "m ", "b ", "k "]) or (_spec_unit.strip() in ["M", "B", "K", "T"])
        _winner_unit_cmp = str(_winner.get("unit_cmp") or "")
        _winner_unit_tag = str(_winner.get("unit_tag") or "")
        _winner_raw = str(_winner.get("raw") or _winner.get("value") or "")
        _winner_has_scale = any(tok in (_winner_raw.lower()) for tok in ["million", "billion", "trillion", "thousand"]) or (_winner_unit_tag.strip().upper() in ["M", "B", "K", "T"]) or (_winner_unit_cmp.strip().upper() in ["M", "B", "K", "T", "%"])
        if _is_scaled and not _winner_has_scale and not _winner_unit_cmp:
            meta["would_block_reason"] = "unit_evidence_missing_for_scaled_schema"
        else:
            meta["would_block_reason"] = ""
    except Exception:
        pass

    # PATCH FIX2B_TRACE_V1 (ADDITIVE): emit selector trace payload
    # - Does NOT change selection; purely diagnostic.
    # =====================================================================
    try:
        meta["analysis_selector_trace_v1"] = {
            "selector_used": meta.get("selector_used"),
            "preferred_url": meta.get("preferred_url"),
            "chosen_source_url": meta.get("chosen_source_url"),
            "n_candidates_in": int(meta.get("candidate_count_in") or 0),
            "n_candidates_pref": int(meta.get("candidate_count_pref") or 0),
            "n_candidates_eligible": int(meta.get("candidate_count_eligible") or 0),
            "blocked_reason": meta.get("blocked_reason") or "",
            "anchor_used": bool(meta.get("anchor_used")),
            "would_block_reason": meta.get("would_block_reason") or "",
            "winner_candidate_debug": dict(meta.get("winner_candidate_debug") or {}) if isinstance(meta.get("winner_candidate_debug"), dict) else {},
        }
    except Exception:
        pass
    return out, meta


def rebuild_metrics_from_snapshots_analysis_canonical_v1(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """Batch rebuild using the extracted canonical selector (pure, deterministic)."""
    if not isinstance(prev_response, dict):
        return {}
    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )
    if not isinstance(metric_schema, dict) or not metric_schema:
        return {}

    metric_anchors = (
        prev_response.get("metric_anchors")
        or (prev_response.get("results") or {}).get("metric_anchors")
        or {}
    )
    if not isinstance(metric_anchors, dict):
        metric_anchors = {}

    # Flatten candidates from snapshots
    if isinstance(baseline_sources_cache, dict) and isinstance(baseline_sources_cache.get("snapshots"), list):
        sources = baseline_sources_cache.get("snapshots", [])
    elif isinstance(baseline_sources_cache, list):
        sources = baseline_sources_cache
    else:
        sources = []

    candidates = []
    for s in sources:
        if not isinstance(s, dict):
            continue
        url = s.get("source_url") or s.get("url") or ""
        xs = s.get("extracted_numbers")
        if isinstance(xs, list) and xs:
            for c in xs:
                if not isinstance(c, dict):
                    continue
                c2 = dict(c)
                if url and not c2.get("source_url"):
                    c2["source_url"] = url
                candidates.append(c2)

    rebuilt = {}
    debug_sel = {}
    for canonical_key, spec in metric_schema.items():
        # =====================================================================
        # PATCH FIX2B_PREFLOCK_V1 (ADDITIVE): per-metric preferred source lock
        # - Keeps canonicals anchored to their preferred URL (anchor.source_url or schema.source_url).
        # - Prevents cross-source hijack from injected/other sources during rebuild.
        # =====================================================================
        try:
            _pref = ""
            if isinstance(metric_anchors, dict) and isinstance(metric_anchors.get(canonical_key), dict):
                _pref = metric_anchors.get(canonical_key).get("source_url") or ""
            _pref = _pref or (spec.get("preferred_url") or spec.get("source_url") or "")
            _pref_n = _ph2b_norm_url(_pref) if _pref else ""
            if _pref_n:
                candidates_for_metric = [c for c in candidates if _ph2b_norm_url(c.get("source_url") or "") == _pref_n]
            else:
                candidates_for_metric = candidates
        except Exception:
            candidates_for_metric = candidates
        best, meta = _analysis_canonical_final_selector_v1(
            canonical_key=canonical_key,
            schema_frozen=spec,
            candidates=candidates_for_metric,
            anchors=metric_anchors,
            prev_metric=(prev_response.get("primary_metrics_canonical") or {}).get(canonical_key) if isinstance(prev_response.get("primary_metrics_canonical"), dict) else None,
            web_context=web_context,
        )
        if isinstance(best, dict) and best:
            # Attach value_range if available in meta
            if isinstance(meta, dict) and isinstance(meta.get("value_range"), dict):
                best["value_range"] = dict(meta.get("value_range"))
                try:
                    unit_disp = best.get("unit") or ""
                    best["value_range_display"] = f"{best['value_range']['min']:g}–{best['value_range']['max']:g} {unit_disp}".strip()
                except Exception:
                    pass
            # Breadcrumb
            best["selector_used"] = meta.get("selector_used") if isinstance(meta, dict) else "analysis_canonical_v1"
            # PATCH FIX2B_TRACE_V1 (ADDITIVE): attach trace to metric dict
            try:
                if isinstance(meta, dict) and isinstance(meta.get("analysis_selector_trace_v1"), dict):
                    best["analysis_selector_trace_v1"] = dict(meta.get("analysis_selector_trace_v1"))
            except Exception:
                pass
            # PATCH FIX2B_TRACE_V2 (ADDITIVE): attach richer trace fields (NO behavior change)
            try:
                if isinstance(meta, dict) and isinstance(meta.get("winner_candidate_debug"), dict):
                    best["winner_candidate_debug"] = dict(meta.get("winner_candidate_debug"))
                if isinstance(meta, dict) and isinstance(meta.get("would_block_reason"), str):
                    best["would_block_reason"] = meta.get("would_block_reason") or ""
            except Exception:
                pass
            rebuilt[canonical_key] = best
        debug_sel[canonical_key] = meta

    # Attach batch debug for audit (non-breaking)
    try:
        if isinstance(prev_response.get("debug"), dict):
            prev_response["debug"].setdefault("ph2b_selector_v1", {})
            prev_response["debug"]["ph2b_selector_v1"]["note"] = "batch_debug_in_prev_response_only"
    except Exception:
        pass

    return rebuilt

# =================== END PATCH PH2B_S1 (ADDITIVE) ===================

# =====================================================================
# PATCH FIX17 (ADDITIVE): end the "circles" by making fallback explicit,
# tightening unit gating (even when schema dimension is missing),
# and logging anchor rejection reasons deterministically.
#
# Goals:
#   1) If anchors exist but are not used, record *why* (no silent fallback)
#   2) Enforce "unit-required" for currency/percent/rate/ratio even when
#      schema.dimension is blank by inferring expected_dim from schema/name/key
#   3) Make "bare year" candidates (1900-2100) absolutely ineligible for
#      non-year metrics, even if upstream tagging/unit_family misfires.
#
# Notes:
#   - Additive only: we introduce FIX17 helpers + rebuild functions,
#     then re-wire the public rebuild names at the end (last definition wins).
#   - No refetch. No heuristics beyond schema/keywords/units/anchors.
# =====================================================================

def _fix17_expected_dimension(metric_spec: dict, canonical_key: str = "") -> str:
    """
    Infer expected dimension when schema doesn't provide it.
    Deterministic rules:
      - if schema.dimension/unit_family present -> use it
      - else infer from canonical_key/name/keywords/unit strings
    """
    try:
        spec = metric_spec or {}
        dim = (spec.get("dimension") or spec.get("unit_family") or spec.get("expected_unit_family") or "").strip().lower()
        if dim:
            return dim

        blob = " ".join([
            str(canonical_key or ""),
            str(spec.get("canonical_key") or spec.get("canonical_id") or ""),
            str(spec.get("name") or ""),
            " ".join([str(x) for x in (spec.get("keywords") or spec.get("keyword_hints") or []) if x]),
            str(spec.get("unit") or spec.get("units") or ""),
        ]).lower()

        # canonical suffixes commonly used in your registry
        if "__currency" in blob or " currency" in blob or "usd" in blob or "sgd" in blob or "eur" in blob or "gbp" in blob:
            return "currency"
        if "__percent" in blob or " percent" in blob or "percentage" in blob or "%" in blob or "yoy" in blob or "cagr" in blob or "growth" in blob:
            return "percent"
        if "__rate" in blob or " rate" in blob:
            return "rate"
        if "__ratio" in blob or " ratio" in blob:
            return "ratio"

        return ""
    except Exception:
        return ""


def _fix17_candidate_has_explicit_unit(c: dict) -> bool:
    """
    Stricter than fix16: we only consider the candidate 'unit-bearing' if it has
    a unit/base_unit/unit_tag/unit_family OR raw contains explicit $/%/currency code.
    """
    try:
        if not isinstance(c, dict):
            return False

        for k in ("base_unit", "unit", "unit_tag", "unit_family"):
            if str(c.get(k) or "").strip():
                return True

        raw = str(c.get("raw") or "")
        raw_l = raw.lower()
        if any(sym in raw for sym in ("$", "€", "£", "¥", "%")):
            return True
        if any(code in raw_l for code in (" usd", "sgd", " eur", " gbp", " aud", " jpy", " cny", " cad")):
            return True

        return False
    except Exception:
        return False


def _fix17_is_bare_year_candidate(c: dict) -> bool:
    """Detect a naked year token candidate, regardless of measure_kind/unit_family."""
    try:
        if not isinstance(c, dict):
            return False
        v = c.get("value_norm")
        if not isinstance(v, (int, float)):
            return False
        iv = int(v)
        if not (1900 <= iv <= 2100):
            return False

        raw = str(c.get("raw") or "").strip()
        # If raw isn't a clean year token, treat it as not "bare year"
        if not re.fullmatch(r"(19\d{2}|20\d{2})", raw):
            return False

        # Must be unitless / not explicitly unit-bearing
        if _fix17_candidate_has_explicit_unit(c):
            return False

        return True
    except Exception:
        return False


def _fix17_metric_is_year_like(metric_spec: dict, canonical_key: str = "") -> bool:
    """Reuse fix16 year-like classifier if available; else fall back to fix16's rule."""
    try:
        fn = globals().get("_fix16_metric_is_year_like")
        if callable(fn):
            return bool(fn(metric_spec, canonical_key=canonical_key))
    except Exception:
        pass
    try:
        spec = metric_spec or {}
        blob = " ".join([
            str(canonical_key or ""),
            str(spec.get("name") or ""),
            str(spec.get("canonical_key") or spec.get("canonical_id") or ""),
            " ".join([str(x) for x in (spec.get("keywords") or spec.get("keyword_hints") or []) if x]),
        ]).lower()
        return any(k in blob for k in (" year", "year_", "founded", "since", "established", "launch year", "model year"))
    except Exception:
        return False


def _fix17_candidate_allowed_with_reason(c: dict, metric_spec: dict, canonical_key: str = "") -> tuple:
    """
    Returns: (allowed: bool, reason: str)
    reason is deterministic and suitable for debug logs.
    """
    try:
        if not isinstance(c, dict):
            return (False, "cand_not_dict")

        # Respect fix15 junk exclusion (if present) with a reason
        fn_dis = globals().get("_candidate_disallowed_for_metric")
        if callable(fn_dis):
            try:
                if fn_dis(c, dict(metric_spec or {}, canonical_key=canonical_key)):
                    # expose the most actionable indicator
                    if c.get("is_junk") is True:
                        return (False, "disallowed:is_junk")
                    if c.get("junk_reason"):
                        return (False, f"disallowed:junk_reason:{c.get('junk_reason')}")
                    return (False, "disallowed:fix15")
            except Exception:
                pass

        # Absolute bare-year ban for non-year metrics
        if _fix17_is_bare_year_candidate(c) and not _fix17_metric_is_year_like(metric_spec, canonical_key=canonical_key):
            return (False, "disallowed:bare_year_non_year_metric")

        expected_dim = _fix17_expected_dimension(metric_spec, canonical_key=canonical_key)

        # Hard "requires unit" gate even if schema didn't set dimension
        if expected_dim in ("currency", "percent", "rate", "ratio"):
            if not _fix17_candidate_has_explicit_unit(c):
                return (False, f"disallowed:missing_unit_for:{expected_dim}")

        # Reuse fix16 compatibility gate when available, but with expected_dim from fix17
        fn_u = globals().get("_fix16_unit_compatible")
        if callable(fn_u):
            if not fn_u(c, expected_dim):
                return (False, f"disallowed:unit_incompatible_for:{expected_dim}")

        return (True, "")
    except Exception:
        return (True, "")


def rebuild_metrics_from_snapshots_with_anchors_fix17(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    FIX17 anchor-aware rebuild:
      - Absolute anchor priority
      - If anchor cannot be used, log deterministic rejection reason
      - Applies strict unit gating & bare-year exclusion
    """
    if not isinstance(prev_response, dict):
        return {}

    metric_anchors = (
        prev_response.get("metric_anchors")
        or (prev_response.get("primary_response") or {}).get("metric_anchors")
        or (prev_response.get("results") or {}).get("metric_anchors")
        or {}
    )
    if not isinstance(metric_anchors, dict) or not metric_anchors:
        return {}

    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )

    # Deterministic candidate index (anchor_hash -> best candidate)
    fn_idx = globals().get("_es_build_candidate_index_deterministic")
    cand_index = fn_idx(baseline_sources_cache) if callable(fn_idx) else {}

    # Debug sink (additive mutation; safe if ignored)
    dbg = prev_response.setdefault("_evolution_rebuild_debug", {})
    rej = dbg.setdefault("anchor_rejects_fix17", [])
    used = dbg.setdefault("anchor_used_fix17", [])

    rebuilt = {}

    for canonical_key, a in (metric_anchors or {}).items():
        if not isinstance(a, dict):
            continue
        ah = a.get("anchor_hash") or a.get("anchor") or ""
        if not ah:
            rej.append({"canonical_key": canonical_key, "reason": "anchor_missing"})
            continue

        spec = (metric_schema.get(canonical_key) if isinstance(metric_schema, dict) else None) or {}
        spec = dict(spec)
        spec.setdefault("name", a.get("name") or canonical_key)
        spec.setdefault("canonical_key", canonical_key)

        c = cand_index.get(ah)
        if not isinstance(c, dict):
            rej.append({"canonical_key": canonical_key, "anchor_hash": ah, "reason": "anchor_not_found_in_index"})
            continue

        ok, reason = _fix17_candidate_allowed_with_reason(c, spec, canonical_key=canonical_key)
        if not ok:
            rej.append({"canonical_key": canonical_key, "anchor_hash": ah, "reason": reason})
            continue

        rebuilt[canonical_key] = {
            "canonical_key": canonical_key,
            "name": spec.get("name") or canonical_key,
            "value": c.get("value"),
            "unit": c.get("unit") or "",
            "value_norm": c.get("value_norm"),
            "source_url": c.get("source_url") or "",
            "anchor_hash": c.get("anchor_hash") or ah,
            "evidence": [{
                "source_url": c.get("source_url") or "",
                "raw": c.get("raw") or "",
                "context_snippet": (c.get("context_snippet") or c.get("context") or c.get("context_window") or "")[:400],
                "anchor_hash": c.get("anchor_hash") or ah,
                "method": "anchor_hash_rebuild_fix17",
            }],
            "anchor_used": True,
        }
        used.append({"canonical_key": canonical_key, "anchor_hash": ah, "source_url": c.get("source_url") or ""})

    return rebuilt


def rebuild_metrics_from_snapshots_schema_only_fix17(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    FIX17 schema-only rebuild:
      - Uses fix16 keyword de-yearing
      - Applies strict fix17 unit requirement inference (even if schema dim missing)
      - Applies fix15 junk exclusion + bare-year exclusion
      - Deterministic tie-breaks
    """
    import re

    if not isinstance(prev_response, dict):
        return {}

    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )
    if not isinstance(metric_schema, dict) or not metric_schema:
        return {}

    # Flatten snapshot candidates (no re-fetch)
    if isinstance(baseline_sources_cache, dict) and isinstance(baseline_sources_cache.get("snapshots"), list):
        sources = baseline_sources_cache.get("snapshots", [])
    elif isinstance(baseline_sources_cache, list):
        sources = baseline_sources_cache
    else:
        sources = []

    candidates = []
    for s in sources:
        if not isinstance(s, dict):
            continue
        url = s.get("source_url") or s.get("url") or ""
        xs = s.get("extracted_numbers")
        if isinstance(xs, list) and xs:
            for c in xs:
                if not isinstance(c, dict):
                    continue
                c2 = dict(c)
                c2.setdefault("source_url", url)
                candidates.append(c2)

    def _norm(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def _cand_sort_key(c: dict):
        try:
            return (
                str(c.get("anchor_hash") or ""),
                str(c.get("source_url") or ""),
                int(c.get("start_idx") or 0),
                str(c.get("raw") or ""),
                str(c.get("unit") or ""),
                float(c.get("value_norm") or 0.0),
            )
        except Exception:
            return ("", "", 0, "", "", 0.0)

    candidates.sort(key=_cand_sort_key)

    # Debug sink
    dbg = prev_response.setdefault("_evolution_rebuild_debug", {})
    dbg.setdefault("schema_only_zero_hit_metrics_fix17", [])

    rebuilt = {}

    for canonical_key, sch in metric_schema.items():
        if not isinstance(sch, dict):
            continue

        spec = dict(sch)
        spec.setdefault("canonical_key", canonical_key)
        spec.setdefault("name", sch.get("name") or canonical_key)

        # Use fix16 year-like & keyword pruning if available
        metric_is_year_like = _fix17_metric_is_year_like(spec, canonical_key=canonical_key)

        keywords = sch.get("keywords") or sch.get("keyword_hints") or []
        if isinstance(keywords, str):
            keywords = [keywords]

        fn_prune = globals().get("_fix16_prune_year_keywords")
        if callable(fn_prune):
            keywords2 = fn_prune(list(keywords), metric_is_year_like)
        else:
            keywords2 = list(keywords)

        kw_norm = [_norm(k) for k in (keywords2 or []) if k]

        best = None
        best_tie = None
        best_hits = 0

        for c in candidates:
            ok, _reason = _fix17_candidate_allowed_with_reason(c, spec, canonical_key=canonical_key)
            if not ok:
                continue

            ctx = _norm(c.get("context_snippet") or c.get("context") or c.get("context_window") or "")
            raw = _norm(c.get("raw") or "")

            hits = 0
            for k in kw_norm:
                if k and (k in ctx or k in raw):
                    hits += 1

            tie = (-hits,) + _cand_sort_key(c)
            if best is None or tie < best_tie:
                best = c
                best_tie = tie
                best_hits = hits

        if not isinstance(best, dict):
            continue

        # If schema has keywords, require at least one hit.
        if kw_norm and best_hits <= 0:
            dbg["schema_only_zero_hit_metrics_fix17"].append({"canonical_key": canonical_key, "reason": "no_keyword_hits"})
            continue

        rebuilt[canonical_key] = {
            "canonical_key": canonical_key,
            "name": spec.get("name") or canonical_key,
            "value": best.get("value"),
            "unit": best.get("unit") or "",
            "value_norm": best.get("value_norm"),
            "source_url": best.get("source_url") or "",
            "anchor_hash": best.get("anchor_hash") or "",
            "evidence": [{
                "source_url": best.get("source_url") or "",
                "raw": best.get("raw") or "",
                "context_snippet": (best.get("context_snippet") or best.get("context") or best.get("context_window") or "")[:400],
                "anchor_hash": best.get("anchor_hash") or "",
                "method": "schema_only_rebuild_fix17",
            }],
            "anchor_used": False,
        }

    return rebuilt


# =====================================================================
# PATCH FIX17 (ADDITIVE): wire FIX17 rebuilds into the existing dispatch
# - Keep names identical so evolution uses these as the LAST definitions
# =====================================================================

def rebuild_metrics_from_snapshots_with_anchors(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:  # noqa: F811
    return rebuild_metrics_from_snapshots_with_anchors_fix17(prev_response, baseline_sources_cache, web_context=web_context)


def rebuild_metrics_from_snapshots_schema_only(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:  # noqa: F811
    return rebuild_metrics_from_snapshots_schema_only_fix17(prev_response, baseline_sources_cache, web_context=web_context)

# =====================================================================
# END PATCH FIX17
# =====================================================================



# =====================================================================
# PATCH FIX18 (ADDITIVE): Anchor-authoritative rebuild (no schema fallback)
# Goal:
#   - If analysis emitted an anchor for a canonical metric, evolution MUST NOT
#     fall back to schema-only selection when the anchor cannot be used.
#   - This closes the final loophole where unitless year tokens win schema-only
#     scoring after an anchor rejection.
# Implementation:
#   - Provide a FIX18 schema-only rebuild that:
#       (1) rebuilds anchored metrics via rebuild_metrics_from_snapshots_with_anchors_fix17
#       (2) rebuilds ONLY unanchored metrics via rebuild_metrics_from_snapshots_schema_only_fix17
#       (3) merges results deterministically (anchored first)
#   - Re-wire the public rebuild_metrics_from_snapshots_schema_only to FIX18.
# Notes:
#   - Fully deterministic; no refetch; no LLM.
#   - Additive only: leaves FIX17 implementations intact.
# =====================================================================

def rebuild_metrics_from_snapshots_schema_only_fix18(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    FIX18 rebuild:
      - Anchored metrics: ONLY from anchor-aware rebuild (fix17), or skipped if rejected.
      - Unanchored metrics: schema-only rebuild (fix17) as before.
      - Never allows schema-only fallback to fill an anchored canonical_key.
    """
    if not isinstance(prev_response, dict):
        return {}

    # Anchored part (authoritative when present)
    fn_anchor = globals().get("rebuild_metrics_from_snapshots_with_anchors_fix17")
    anchored = fn_anchor(prev_response, baseline_sources_cache, web_context=web_context) if callable(fn_anchor) else {}

    # Identify anchored keys (only those with a concrete anchor_hash)
    metric_anchors_any = globals().get("_get_metric_anchors_any")
    metric_anchors = metric_anchors_any(prev_response) if callable(metric_anchors_any) else (
        prev_response.get("metric_anchors") or {}
    )
    anchored_keys = set()
    try:
        for k, a in (metric_anchors or {}).items():
            if isinstance(a, dict) and (a.get("anchor_hash") or a.get("anchor")):
                anchored_keys.add(k)
    except Exception:
        anchored_keys = set()

    # Build a shallow prev_response copy where schema-only sees ONLY unanchored metrics
    pr2 = dict(prev_response)
    ms = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )
    if isinstance(ms, dict) and anchored_keys:
        ms2 = {k: v for k, v in ms.items() if k not in anchored_keys}
        pr2["metric_schema_frozen"] = ms2

    # Unanchored part
    fn_schema = globals().get("rebuild_metrics_from_snapshots_schema_only_fix17")
    unanchored = fn_schema(pr2, baseline_sources_cache, web_context=web_context) if callable(fn_schema) else {}

    # Deterministic merge: anchored first, then unanchored
    rebuilt = {}
    if isinstance(anchored, dict):
        rebuilt.update(anchored)
    if isinstance(unanchored, dict):
        for k, v in unanchored.items():
            if k not in rebuilt:
                rebuilt[k] = v
    return rebuilt


# Re-wire schema-only entrypoint to FIX18 (keep names identical for evolution dispatch)
def rebuild_metrics_from_snapshots_schema_only(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:  # noqa: F811
    return rebuild_metrics_from_snapshots_schema_only_fix18(prev_response, baseline_sources_cache, web_context=web_context)

# =====================================================================
# END PATCH FIX18
# =====================================================================


# ==============================================================================
# PATCH FIX24 (ADDITIVE): Sheets-first replay for unchanged sources+data, and
# scrape+hash gate for evolution to prevent any rebuild/picking when unchanged.
#
# Goals:
#   1) Evolution ALWAYS performs a current scrape/fetch pass to compute a "current"
#      snapshot hash (v2 preferred).
#   2) If current hash == prior analysis hash (v2 preferred), evolution stops and
#      replays the prior FULL analysis payload rehydrated from Google Sheets,
#      publishing metrics to the dashboard WITHOUT any metric rebuild/selection.
#   3) If hashes differ, evolution proceeds via the existing deterministic path
#      (same rebuild/anchors logic as used elsewhere in this codebase).
#
# This patch is purely additive:
#   - Preserves original run_source_anchored_evolution as run_source_anchored_evolution_BASE
#   - Adds helper functions prefixed _fix24_*
#   - Overrides run_source_anchored_evolution by re-defining it below
# ==============================================================================

try:
    run_source_anchored_evolution_BASE = run_source_anchored_evolution  # type: ignore
except Exception:
    run_source_anchored_evolution_BASE = None  # type: ignore


def _fix24_get_prev_full_payload(previous_data: dict) -> dict:
    """
    Load the FULL prior analysis payload from Google Sheets if possible.
    Falls back to previous_data if already full.
    """
    try:
        if not isinstance(previous_data, dict):
            return {}
        # If it already looks like a full payload (contains canonical metrics), return as-is
        if isinstance(previous_data.get("primary_metrics_canonical"), dict) and previous_data["primary_metrics_canonical"]:
            return previous_data

        # Preferred: explicit snapshot_store_ref / full_store_ref
        ref = previous_data.get("full_store_ref") or previous_data.get("snapshot_store_ref") or ""
        # Fallback: sheet id
        if (not ref) and isinstance(previous_data.get("_sheet_id"), str) and previous_data.get("_sheet_id"):
            # Assume HistoryFull
            ref = f"gsheet:HistoryFull:{previous_data.get('_sheet_id')}"

        if isinstance(ref, str) and ref.startswith("gsheet:"):
            parts = ref.split(":")
            ws_title = parts[1] if len(parts) > 1 and parts[1] else "HistoryFull"
            aid = parts[2] if len(parts) > 2 else ""
            if aid:
                fn = globals().get("load_full_history_payload_from_sheet")
                if callable(fn):
                    full = fn(aid, worksheet_title=ws_title)
                    if isinstance(full, dict) and full:
                        return full
    except Exception:
        pass

    return previous_data if isinstance(previous_data, dict) else {}


def _fix24_extract_source_urls(prev_full: dict) -> list:
    """
    Determine the URL list to fetch for current-hash computation.
    Uses analysis 'sources' if available, else URLs from baseline_sources_cache.
    """
    urls = []
    try:
        if isinstance(prev_full, dict):
            s = prev_full.get("sources")
            if isinstance(s, list) and s:
                urls = [str(u) for u in s if isinstance(u, str) and u.strip()]
            if not urls:
                # Try results.source_results urls
                r = prev_full.get("results") if isinstance(prev_full.get("results"), dict) else {}
                sr = r.get("source_results") if isinstance(r, dict) else None
                if isinstance(sr, list):
                    for item in sr:
                        if isinstance(item, dict):
                            u = item.get("url") or item.get("source_url")
                            if u:
                                urls.append(str(u))
            if not urls:
                # Try baseline_sources_cache urls
                r = prev_full.get("results") if isinstance(prev_full.get("results"), dict) else {}
                bsc = None
                if isinstance(r, dict):
                    bsc = r.get("baseline_sources_cache")
                if not isinstance(bsc, list):
                    bsc = prev_full.get("baseline_sources_cache")
                if isinstance(bsc, list):
                    for item in bsc:
                        if isinstance(item, dict):
                            u = item.get("source_url") or item.get("url")
                            if u:
                                urls.append(str(u))
    except Exception:
        pass

    # Stable de-dupe order
    seen = set()
    out = []
    for u in urls:
        uu = (u or "").strip()
        if not uu or uu in seen:
            continue
        seen.add(uu)
        out.append(uu)
    return out[:25]


def _fix24_build_scraped_meta(urls: list, max_chars_per_source: int = 180000) -> dict:
    """
    Fetch each URL (deterministically) and return scraped_meta in the same shape
    attach_source_snapshots_to_analysis expects: {url: {"status":..., "text":..., "extracted_numbers":[...]}}
    """
    scraped_meta = {}
    fetch_fn = globals().get("fetch_url_content_with_status") or globals().get("fetch_url_content")
    extract_fn = globals().get("extract_numbers_with_context")

    for u in urls or []:
        url = str(u or "").strip()
        if not url:
            continue
        try:
            if callable(fetch_fn) and fetch_fn.__name__.endswith("_with_status"):
                text, status = fetch_fn(url)
            elif callable(fetch_fn):
                text = fetch_fn(url)
                status = "success_direct" if (text and str(text).strip()) else "empty"
            else:
                text, status = (None, "no_fetch_fn")

            txt = "" if text is None else str(text)
            if max_chars_per_source and len(txt) > int(max_chars_per_source):
                txt = txt[: int(max_chars_per_source)]

            nums = []
            if callable(extract_fn) and txt.strip():
                try:
                    nums = extract_fn(txt, source_url=url)
                    if nums is None:
                        nums = []
                except Exception:
                    nums = []

            scraped_meta[url] = {
                "status": status,
                "text": txt,
                "extracted_numbers": nums if isinstance(nums, list) else [],
            }
        except Exception as e:
            scraped_meta[url] = {"status": f"exception:{type(e).__name__}", "text": "", "extracted_numbers": []}

    return scraped_meta


def _fix24_baseline_sources_cache_from_scraped_meta(scraped_meta: dict) -> list:
    """
    Use attach_source_snapshots_to_analysis (existing deterministic normalizer) to produce
    baseline_sources_cache from scraped_meta, ensuring value_norm/unit_tag fields are present.
    """
    try:
        fn = globals().get("attach_source_snapshots_to_analysis")
        if not callable(fn):
            return []
        dummy = {"results": {}}
        web_context = {"scraped_meta": scraped_meta or {}}
        fn(dummy, web_context)
        r = dummy.get("results") if isinstance(dummy.get("results"), dict) else {}
        bsc = r.get("baseline_sources_cache") if isinstance(r, dict) else None
        return bsc if isinstance(bsc, list) else []
    except Exception:
        return []


def _fix24_get_prev_hashes(prev_full: dict) -> dict:
    """
    Extract prior snapshot hashes (v2 preferred) from a full analysis payload.
    """
    out = {"v2": "", "v1": ""}
    try:
        if not isinstance(prev_full, dict):
            return out
        out["v2"] = str(prev_full.get("source_snapshot_hash_v2") or "")
        out["v1"] = str(prev_full.get("source_snapshot_hash") or "")
        r = prev_full.get("results") if isinstance(prev_full.get("results"), dict) else {}
        if isinstance(r, dict):
            out["v2"] = out["v2"] or str(r.get("source_snapshot_hash_v2") or "")
            out["v1"] = out["v1"] or str(r.get("source_snapshot_hash") or "")
    except Exception:
        pass
    return out


def _fix24_compute_current_hashes(baseline_sources_cache: list) -> dict:
    """
    Compute current snapshot hashes (v2 preferred).
    """
    out = {"v2": "", "v1": ""}
    try:
        fn1 = globals().get("compute_source_snapshot_hash")
        fn2 = globals().get("compute_source_snapshot_hash_v2")
        if callable(fn2):
            out["v2"] = str(fn2(baseline_sources_cache) or "")
        if callable(fn1):
            out["v1"] = str(fn1(baseline_sources_cache) or "")
    except Exception:
        pass
    return out


def _fix24_make_replay_output(prev_full: dict, hashes: dict) -> dict:
    """
    Build a minimal evolution payload for the dashboard that reflects the prior analysis
    payload verbatim (no rebuild). This avoids the diff panel showing years by ensuring
    the "evolution column" is sourced from stored canonical metrics.
    """
    pmc = prev_full.get("primary_metrics_canonical") if isinstance(prev_full, dict) else {}
    pmc = pmc if isinstance(pmc, dict) else {}

    # Build a deterministic "no-change" metric_changes list WITHOUT re-selecting metrics.
    metric_changes = []
    try:
        for ckey in sorted(pmc.keys()):
            m = pmc.get(ckey) if isinstance(pmc.get(ckey), dict) else {}
            name = str(m.get("name") or m.get("metric_name") or ckey)
            v = m.get("value_norm", m.get("value"))
            unit = m.get("base_unit") or m.get("unit_tag") or m.get("unit") or ""
            metric_changes.append({
                "canonical_key": ckey,
                "name": name,
                "previous_value": v,
                "current_value": v,
                "previous_unit": unit,
                "current_unit": unit,
                "change_type": "unchanged",
                "confidence": 1.0,
            })
    except Exception:
        metric_changes = []

    return {
        "status": "ok",
        "mode": "replay_unchanged_fix24",
        "message": "Sources + data unchanged (hash match). Replaying prior analysis snapshot from Sheets.",
        "sources_checked": int(len(prev_full.get("sources") or [])) if isinstance(prev_full, dict) else 0,
        "sources_fetched": 0,
        "sources_failed": 0,
        "sources_skipped": 0,
        "source_results": [],
        "metric_changes": metric_changes,
        "change_stats": {
            "metrics_increased": 0,
            "metrics_decreased": 0,
            "metrics_unchanged": len(metric_changes),
            "metrics_total": len(metric_changes),
        },
        "debug": {
            "fix24": True,
            "prev_source_snapshot_hash_v2": hashes.get("prev_v2",""),
            "cur_source_snapshot_hash_v2": hashes.get("cur_v2",""),
            "prev_source_snapshot_hash": hashes.get("prev_v1",""),
            "cur_source_snapshot_hash": hashes.get("cur_v1",""),
            "hash_equal_v2": bool(hashes.get("prev_v2") and hashes.get("cur_v2") and hashes.get("prev_v2")==hashes.get("cur_v2")),
            "hash_equal_v1": bool(hashes.get("prev_v1") and hashes.get("cur_v1") and hashes.get("prev_v1")==hashes.get("cur_v1")),
        },
        # Provide the replay payload so the dashboard can render canonical metrics directly if desired
        "replay_analysis_payload": prev_full,
    }


# PATCH FIX41G: removed misplaced top-level web_context normalization block (was causing NameError)


def run_source_anchored_evolution(previous_data: dict, web_context: dict = None) -> dict:
    """
    PATCH FIX24 (ADDITIVE): Evolution flow is:
      1) Rehydrate prior full analysis payload from Sheets (HistoryFull)
      2) Scrape/fetch current sources to build scraped_meta + baseline_sources_cache_current
      3) Compute current snapshot hash (v2 preferred)
      4) If hash matches prior analysis: STOP and replay from Sheets (no rebuild/selection)
      5) If changed: proceed with the existing deterministic evolution path, but ensure
         it routes through the same snapshot/anchor deterministic plumbing used elsewhere.

    Note: This does NOT refactor existing evolution code; it wraps it.
    """
    # Step 1: Rehydrate prior payload
    prev_full = _fix24_get_prev_full_payload(previous_data or {})
    prev_hashes = _fix24_get_prev_hashes(prev_full)

    # Step 2: Build current scraped_meta by fetching the same URLs used previously
    urls = _fix24_extract_source_urls(prev_full)
    # =====================================================================
    # PATCH FIX41AFC3 (ADDITIVE): Recover Evolution injected URLs into web_context['extra_urls']
    #
    # Purpose:
    # - Streamlit may provide injected URLs only via diagnostic fields
    #   (diag_extra_urls_ui / diag_extra_urls_ui_raw).
    # - Downstream evolution admission & fetch logic keys off web_context['extra_urls'].
    #
    # Behavior:
    # - If web_context['extra_urls'] is empty/missing, recover from (in order):
    #     1) web_context['diag_extra_urls_ui']     (list)
    #     2) web_context['diag_extra_urls_ui_raw'] (str; newline/comma separated)
    # - Normalize/canonicalize via _inj_diag_norm_url_list (tracking params stripped).
    #
    # Safety:
    # - Purely additive wiring; no effect when no injection is present.
    # - Never raises; falls back silently.
    # =====================================================================
    try:
        if isinstance(web_context, dict):
            _wc_extra0 = web_context.get('extra_urls')
            _needs = (not isinstance(_wc_extra0, (list, tuple)) or not _wc_extra0)
            if _needs:
                _recovered = []
                _v_list = web_context.get('diag_extra_urls_ui')
                if isinstance(_v_list, (list, tuple)) and _v_list:
                    _recovered = list(_v_list)
                if not _recovered:
                    _raw = web_context.get('diag_extra_urls_ui_raw')
                    if isinstance(_raw, str) and _raw.strip():
                        _parts = []
                        for _line in _raw.splitlines():
                            _line = (_line or '').strip()
                            if not _line:
                                continue
                            for _p in _line.split(','):
                                _p = (_p or '').strip()
                                if _p:
                                    _parts.append(_p)
                        if _parts:
                            _recovered = _parts
                if _recovered:
                    _recovered_norm = _inj_diag_norm_url_list(_recovered)
                    if _recovered_norm:
                        web_context['extra_urls'] = list(_recovered_norm)
                        web_context.setdefault('debug', {})
                        if isinstance(web_context.get('debug'), dict):
                            web_context['debug'].setdefault('fix41afc3', {})
                            if isinstance(web_context['debug'].get('fix41afc3'), dict):
                                web_context['debug']['fix41afc3'].update({
                                    'extra_urls_recovered': True,
                                    'extra_urls_recovered_count': int(len(_recovered_norm)),
                                })
    except Exception:
        pass
    # =====================================================================


    # =====================================================================
    # PATCH EVO_ROUTE_INJECTED_URLS_THROUGH_FWC_V1 (ADDITIVE)
    #
    # Goal:
    # - When Evolution UI provides injected URLs, route them through the SAME
    #   admission/normalization/dedupe logic used by analysis (fetch_web_context),
    #   but in IDENTITY-ONLY mode (no scraping).
    #
    # Why:
    # - Previously, injected URLs could appear in ui_norm/intake_norm but never
    #   reach the admission gate, yielding empty admission_decisions.
    #
    # Behavior:
    # - Only active when web_context['extra_urls'] is non-empty.
    # - Does NOT change fastpath logic directly; it only defines the "current URL
    #   universe" inputs (urls) used for hashing/scrape_meta building.
    #
    # Safety:
    # - identity_only=True prevents any network scrape inside fetch_web_context.
    # - Purely additive; if anything fails, it falls back to existing urls list.
    # =====================================================================
    try:
        _evo_extra_urls_raw = (web_context or {}).get("extra_urls") or []
        _evo_extra_urls_norm = _inj_diag_norm_url_list(_evo_extra_urls_raw)
        if _evo_extra_urls_norm:
            _baseline_urls_for_fwc = _fix24_extract_source_urls(prev_full) or []
            _baseline_urls_for_fwc_norm = _inj_diag_norm_url_list(_baseline_urls_for_fwc)
            _q_for_fwc = str((prev_full or {}).get("question") or (previous_data or {}).get("question") or "").strip()
            _fwc = fetch_web_context(
                _q_for_fwc or "evolution_identity_only",
                num_sources=int(min(12, max(1, len(_baseline_urls_for_fwc_norm) + len(_evo_extra_urls_norm)))),
                fallback_mode=True,
                fallback_urls=_baseline_urls_for_fwc_norm,
                existing_snapshots=(prev_full or {}).get("baseline_sources_cache") or (prev_full or {}).get("baseline_sources_cache_v2") or None,
                extra_urls=_evo_extra_urls_norm,
                diag_run_id=str((web_context or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                diag_extra_urls_ui_raw=(web_context or {}).get("diag_extra_urls_ui_raw"),
                identity_only=True,
            ) or {}
            _fwc_admitted = _fwc.get("web_sources") or _fwc.get("sources") or []
            if isinstance(_fwc_admitted, list) and _fwc_admitted:
                urls = list(_fwc_admitted)
            # Attach the admission decisions to web_context for unified inj_trace reporting
            if isinstance(web_context, dict):
                if isinstance(_fwc.get("diag_injected_urls"), dict):
                    web_context.setdefault("diag_injected_urls", {})
                    if isinstance(web_context.get("diag_injected_urls"), dict):
                        # do not clobber if already present
                        for _k, _v in _fwc.get("diag_injected_urls").items():
                            web_context["diag_injected_urls"].setdefault(_k, _v)
                web_context.setdefault("debug", {})
                if isinstance(web_context.get("debug"), dict):
                    web_context["debug"].setdefault("evo_fwc_identity_only", {})
                    if isinstance(web_context["debug"].get("evo_fwc_identity_only"), dict):
                        web_context["debug"]["evo_fwc_identity_only"].update({
                            "called": True,
                            "baseline_urls_count": int(len(_baseline_urls_for_fwc_norm)),
                            "extra_urls_count": int(len(_evo_extra_urls_norm)),
                            "admitted_count": int(len(urls or [])),
                            "admitted_set_hash": _inj_diag_set_hash(_inj_diag_norm_url_list(urls or [])),
                        })
    except Exception:
        pass
    # =====================================================================
    # =====================================================================


    # =====================================================================
        # =====================================================================
    # PATCH FIX41AFC9 (ADDITIVE): Merge injected URLs into `urls` universe BEFORE scrape_meta build
    #
    # Problem observed (inj_trace_v1):
    # - Injected URL shows up in ui_norm/intake_norm, but can still vanish from admitted_norm/hash_inputs_norm.
    # - Root cause: the identity-only fetch_web_context() step may replace `urls` with an admitted list
    #   that excludes injected URLs, and later injected logic may read from a different variable/path.
    #
    # Goal:
    # - If injected URLs are present in web_context['extra_urls'], ensure they are ALWAYS merged into the
    #   local `urls` list used by _fix24_build_scraped_meta(), so the injected URLs are at least
    #   attempted (scraped_meta populated) and can become part of current hash identity when successful.
    #
    # Safety:
    # - Only active when injection is present.
    # - Purely additive: does not change hashing algorithm or fastpath rules; it only ensures the URL
    #   universe includes the injected URLs when the user provided them.
    # - Never raises.
    # =====================================================================
    try:
        _fx9_wc = web_context if isinstance(web_context, dict) else {}
        _fx9_inj = _inj_diag_norm_url_list((_fx9_wc or {}).get("extra_urls") or [])
        if _fx9_inj and isinstance(urls, list):
            _fx9_seen = set(_inj_diag_norm_url_list([(_d.get("url") if isinstance(_d, dict) else _d) for _d in urls]))
            _fx9_added = []
            for _u in _fx9_inj:
                if _u in _fx9_seen:
                    continue
                _fx9_seen.add(_u)
                urls.append(_u)
                _fx9_added.append(_u)

            if isinstance(_fx9_wc, dict):
                _fx9_wc.setdefault("debug", {})
                if isinstance(_fx9_wc.get("debug"), dict):
                    _fx9_wc["debug"].setdefault("fix41afc9", {})
                    if isinstance(_fx9_wc["debug"].get("fix41afc9"), dict):
                        _fx9_wc["debug"]["fix41afc9"].update({
                            "merged_into_urls_universe": True,
                            "injected_urls_count": int(len(_fx9_inj)),
                            "added_to_urls_count": int(len(_fx9_added)),
                            "added_to_urls": list(_fx9_added),
                            "urls_count_after_merge": int(len(urls)),
                        })
    except Exception:
        pass
    # =====================================================================

# PATCH FIX41AFC6 (ADDITIVE): When injected URL delta exists, actually FETCH it
    #
    # Observation (from inj_trace_v1 in evolution JSON):
    # - Injected URL appears in ui_norm/intake_norm, but attempted/persisted remain empty,
    #   and the injected URL never reaches hash_inputs because evolution never performs a
    #   fetch cycle for the injected delta (it only replays cached snapshots).
    #
    # Goal:
    # - ONLY when injected URLs introduce a true delta vs the baseline source universe,
    #   run fetch_web_context() in normal mode (identity_only=False) so the injected URL
    #   is actually attempted/persisted and can become a first-class current source
    #   (and thus can affect downstream identity/hash inputs).
    #
    # Safety:
    # - No effect on no-injection runs.
    # - No effect when injection is empty or introduces no delta.
    # - Uses existing_snapshots to avoid re-fetching baseline sources.
    # - Never raises; falls back to existing behavior.
    # =====================================================================
    try:
        _fix41afc6_wc = web_context if isinstance(web_context, dict) else {}
        _fix41afc6_inj = _inj_diag_norm_url_list((_fix41afc6_wc or {}).get("extra_urls") or [])
        _fix41afc6_base = _inj_diag_norm_url_list(_fix24_extract_source_urls(prev_full) or [])
        _fix41afc6_delta = sorted(list(set(_fix41afc6_inj) - set(_fix41afc6_base))) if _fix41afc6_inj else []
        if _fix41afc6_delta:
            _fix41afc6_q = str((prev_full or {}).get("question") or (previous_data or {}).get("question") or "").strip()
            _fix41afc6_prev_snap = (prev_full or {}).get("baseline_sources_cache") or (prev_full or {}).get("baseline_sources_cache_v2") or None

            _fix41afc6_fwc = fetch_web_context(
                _fix41afc6_q or "evolution_injection_fetch",
                num_sources=int(min(12, max(1, len(_fix41afc6_base) + len(_fix41afc6_inj)))),
                fallback_mode=True,
                fallback_urls=_fix41afc6_base,
                existing_snapshots=_fix41afc6_prev_snap,
                extra_urls=_fix41afc6_inj,
                diag_run_id=str((_fix41afc6_wc or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                diag_extra_urls_ui_raw=(_fix41afc6_wc or {}).get("diag_extra_urls_ui_raw"),
                # PATCH FIX41AFC8 (ADDITIVE): force scrape injected extras even if not admitted
                force_scrape_extra_urls=True,
                force_admit_extra_urls=True,
                identity_only=False,
            ) or {}

            # Prefer the admitted list from fetch_web_context (it includes injected URLs that pass admission)
            _fix41afc6_admitted = _fix41afc6_fwc.get("web_sources") or _fix41afc6_fwc.get("sources") or []
            if isinstance(_fix41afc6_admitted, list) and _fix41afc6_admitted:
                urls = list(_fix41afc6_admitted)

            # Bubble up a small marker so inj_trace_v1 can report whether evolution actually called FWC
            if isinstance(web_context, dict):
                web_context["evolution_calls_fetch_web_context"] = True
                web_context.setdefault("debug", {})
                if isinstance(web_context.get("debug"), dict):
                    web_context["debug"].setdefault("fix41afc6", {})
                    if isinstance(web_context["debug"].get("fix41afc6"), dict):
                        web_context["debug"]["fix41afc6"].update({
                            "called_fetch_web_context": True,
                            "injected_delta_count": int(len(_fix41afc6_delta)),
                            "injected_delta": list(_fix41afc6_delta),
                            "admitted_count": int(len(_fix41afc6_admitted or [])) if isinstance(_fix41afc6_admitted, list) else 0,
                        })

                    # =====================================================================
                    # PATCH FIX41AFC8 (ADDITIVE): Emit forced-fetch diagnostics for injected delta
                    # =====================================================================
                    try:
                        web_context.setdefault("debug", {})
                        if isinstance(web_context.get("debug"), dict):
                            web_context["debug"].setdefault("fix41afc8", {})
                            if isinstance(web_context["debug"].get("fix41afc8"), dict):
                                # delta URLs are what we intend to force-attempt
                                _fx8_delta_urls = list(_fix41afc6_delta or [])
                                # attempted/persist outcomes can be inferred from fetch_web_context scraped_meta
                                _fx8_results = {}
                                try:
                                    _fx8_sm = _fix41afc6_fwc.get("scraped_meta") or {}
                                    if isinstance(_fx8_sm, dict):
                                        for _u in _fx8_delta_urls:
                                            meta = _fx8_sm.get(_u) or {}
                                            if isinstance(meta, dict) and meta:
                                                _fx8_results[_u] = meta.get("status") or meta.get("fetch_status") or meta.get("reason") or "attempted"
                                            else:
                                                _fx8_results[_u] = "not_in_scraped_meta"
                                except Exception:
                                    pass
                                web_context["debug"]["fix41afc8"].update({
                                    "forced_fetch_reason": "injected_delta_present_force_fetch_even_if_not_admitted",
                                    "forced_fetch_urls": _fx8_delta_urls,
                                    "forced_fetch_count": int(len(_fx8_delta_urls)),
                                    "forced_fetch_results": _fx8_results,
                                })
                    except Exception:
                        pass

    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH FIX41AFC7 (ADDITIVE): Early recovery + latching of injected URLs into web_context["extra_urls"]
    #
    # Problem observed in evolution JSON:
    # - ui_norm/intake_norm contains the injected URL (from Streamlit textarea),
    #   but web_context["extra_urls"] can still be empty at evolution core, which
    #   causes downstream "fetch injected delta" and "forced admit" patches to see
    #   an empty injected set and skip.
    #
    # Goal:
    # - If web_context["extra_urls"] is empty, recover injected URLs from the same
    #   Streamlit diagnostic fields used at intake:
    #     1) web_context["diag_extra_urls_ui"] (list)
    #     2) web_context["diag_extra_urls_ui_raw"] (string, newline/comma separated)
    # - Normalize/canonicalize deterministically via _inj_diag_norm_url_list().
    # - Latch the recovered list back into web_context["extra_urls"] so ALL later
    #   injected URL logic (fetch + forced admit + hash identity) sees the same set.
    #
    # Safety:
    # - Purely additive. No effect when extra_urls already present.
    # - Never raises; falls back silently.
    # =====================================================================
    try:
        if isinstance(web_context, dict):
            _fix41afc7_norm = []
            _existing = web_context.get("extra_urls")
            _need = (not isinstance(_existing, (list, tuple)) or not list(_existing))
            if _need:
                _raw = []
                _v_list = web_context.get("diag_extra_urls_ui")
                if isinstance(_v_list, (list, tuple)) and _v_list:
                    _raw = list(_v_list)
                if not _raw:
                    _v_raw = web_context.get("diag_extra_urls_ui_raw")
                    if isinstance(_v_raw, str) and _v_raw.strip():
                        _parts = []
                        for _line in _v_raw.splitlines():
                            _line = (_line or "").strip()
                            if not _line:
                                continue
                            for _p in _line.split(","):
                                _p = (_p or "").strip()
                                if _p:
                                    _parts.append(_p)
                        if _parts:
                            _raw = _parts

                _fix41afc7_norm = _inj_diag_norm_url_list(_raw)
                if _fix41afc7_norm:
                    web_context["extra_urls"] = list(_fix41afc7_norm)

            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].setdefault("fix41afc7", {})
                if isinstance(web_context["debug"].get("fix41afc7"), dict):
                    web_context["debug"]["fix41afc7"].update({
                        "recovery_needed": bool(_need),
                        "recovered_extra_urls_count": int(len(_fix41afc7_norm or [])),
                        "recovered_extra_urls": list(_fix41afc7_norm or [])[:20],
                        "extra_urls_present_after_recovery": bool(isinstance(web_context.get("extra_urls"), (list, tuple)) and list(web_context.get("extra_urls") or [])),
                    })
    except Exception:
        pass
    # =====================================================================

# PATCH INJ_DIAG_EVO_CORE (ADDITIVE): allow optional injected URLs (Scenario B)
    # - Only active if caller provides web_context['extra_urls']
    # - Does NOT affect default fastpath behavior.
    # =====================================================================
    _inj_diag_run_id = ""
    _inj_extra_urls = []
    try:
        _inj_diag_run_id = str((web_context or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo")
        _inj_extra_urls = _inj_diag_norm_url_list((web_context or {}).get("extra_urls") or [])
    except Exception:
        _inj_diag_run_id = _inj_diag_make_run_id("evo")
        _inj_extra_urls = []

    try:
        if _inj_extra_urls:
            _u_seen = set([str(u or "").strip() for u in (urls or []) if str(u or "").strip()])
            for _u in _inj_extra_urls:
                if _u not in _u_seen:
                    _u_seen.add(_u)
                    urls.append(_u)
    except Exception:
        pass
    # =====================================================================


    # =====================================================================

    # =====================================================================
    # PATCH FIX41AFC4 (ADDITIVE): Force-admit injected URL deltas into evolution URL universe
    #
    # Problem (observed in evolution JSON):
    # - Injected URL appears in ui_norm/intake_norm but is missing from admitted_norm,
    #   so it never reaches attempted/persisted/hash_inputs.
    # - This typically happens when admission/allowlist logic rejects injected URLs
    #   before the fetch loop, leaving attempted empty.
    #
    # Goal:
    # - ONLY when injection is present AND it introduces a true delta vs the baseline
    #   source universe, ensure the injected URLs are included in `urls` (the universe
    #   FIX24 uses for scrape_meta building).
    #
    # Safety:
    # - Purely additive; no effect when no injection or no delta.
    # - Does not modify fastpath logic/hashing; it only ensures injected URLs are
    #   present in the post-intake universe when delta exists.
    # - Never raises; falls back silently.
    # =====================================================================
    try:
        _fix41afc4_inj_norm = _inj_diag_norm_url_list(_inj_extra_urls or [])
        _fix41afc4_base_norm = _inj_diag_norm_url_list(_fix24_extract_source_urls(prev_full) or [])
        _fix41afc4_delta = sorted(list(set(_fix41afc4_inj_norm) - set(_fix41afc4_base_norm))) if _fix41afc4_inj_norm else []
        _fix41afc4_applied = False

        if _fix41afc4_delta:
            # Determine expected URL container shape (strings vs dicts)
            _urls_list = urls if isinstance(urls, list) else []
            _urls_are_dicts = bool(_urls_list) and isinstance(_urls_list[0], dict)

            # Build a normalized "seen" set from existing urls
            if _urls_are_dicts:
                _seen_norm = set(_inj_diag_norm_url_list([(_d.get("url") if isinstance(_d, dict) else "") for _d in _urls_list]))
            else:
                _seen_norm = set(_inj_diag_norm_url_list(_urls_list))

            for _u in _fix41afc4_delta:
                if _u in _seen_norm:
                    continue
                _seen_norm.add(_u)
                if _urls_are_dicts:
                    _urls_list.append({"url": _u})
                else:
                    _urls_list.append(_u)
                _fix41afc4_applied = True

            urls = _urls_list  # rebind defensively

        if isinstance(web_context, dict):
            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].setdefault("fix41afc4", {})
                if isinstance(web_context["debug"].get("fix41afc4"), dict):
                    web_context["debug"]["fix41afc4"].update({
                        "forced_admit_applied": bool(_fix41afc4_applied),
                        "forced_admit_injected_urls_count": int(len(_fix41afc4_delta)),
                        "forced_admit_injected_urls": list(_fix41afc4_delta),
                        "baseline_urls_count": int(len(_fix41afc4_base_norm)),
                        "urls_after_forced_admit_count": int(len(_inj_diag_norm_url_list([(_d.get("url") if isinstance(_d, dict) else _d) for _d in (urls or [])] if isinstance(urls, list) else []))),
                    })
    except Exception:
        pass
    # =====================================================================


# PATCH EVO_INJ_ADMISSION_TRACE_V1 (ADDITIVE): pinpoint where injected URLs are dropped
    #
    # Why:
    # - When a URL appears in ui_norm/intake_norm but not in admitted_norm, we need
    #   an explicit reason before we change any behavior.
    #
    # What this records (debug only):
    # - Whether evolution is using fetch_web_context (it is not in FIX24 path)
    # - The pre- and post-injection URL universe
    # - Per-injected-URL admission decision + reason codes
    #
    # Safety:
    # - Does NOT alter control flow, fastpath eligibility, scraping, hashing, or selection.
    # =====================================================================
    try:
        _urls_prev_full = _fix24_extract_source_urls(prev_full)
        _urls_prev_full_norm = _inj_diag_norm_url_list(_urls_prev_full or [])
        _urls_after_merge_norm = _inj_diag_norm_url_list(urls or [])

        _admission_decisions = {}
        for _u in (_inj_extra_urls or []):
            if _u in set(_urls_after_merge_norm):
                _admission_decisions[_u] = {
                    "decision": "admitted",
                    "reason_code": "merged_into_urls_for_scrape",
                }
            else:
                _admission_decisions[_u] = {
                    "decision": "rejected",
                    "reason_code": "not_present_in_urls_after_merge",
                }

        if isinstance(web_context, dict):
            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].setdefault("evo_injection_trace", {})
                if isinstance(web_context["debug"].get("evo_injection_trace"), dict):
                    web_context["debug"]["evo_injection_trace"].update({
                        "uses_fetch_web_context": False,
                        "urls_prev_full_count": int(len(_urls_prev_full_norm)),
                        "urls_prev_full_set_hash": _inj_diag_set_hash(_urls_prev_full_norm),
                        "urls_after_merge_count": int(len(_urls_after_merge_norm)),
                        "urls_after_merge_set_hash": _inj_diag_set_hash(_urls_after_merge_norm),
                        "inj_extra_urls_norm": list(_inj_extra_urls or []),
                        "inj_merge_applied": bool(_inj_extra_urls),
                        "inj_admission_decisions": _admission_decisions,
                    })

            # Also attach to diag_injected_urls for unified downstream reporting
            web_context.setdefault("diag_injected_urls", {})
            if isinstance(web_context.get("diag_injected_urls"), dict):
                web_context["diag_injected_urls"].setdefault("admission_decisions", {})
                if isinstance(web_context["diag_injected_urls"].get("admission_decisions"), dict):
                    web_context["diag_injected_urls"]["admission_decisions"].update(_admission_decisions)
                web_context["diag_injected_urls"].setdefault("urls_prev_full_norm", _urls_prev_full_norm)
                web_context["diag_injected_urls"].setdefault("urls_after_merge_norm", _urls_after_merge_norm)
    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH FIX41AFC11 (ADDITIVE): Injection admission override + must-fetch lane (delta-only)
    #
    # Problem:
    # - Injected URLs can appear in UI intake but get dropped pre-admission, resulting in:
    #     attempted=0, persisted_norm=0, hash_inputs_norm unchanged.
    #
    # Goal:
    # - When injected URL DELTA exists (vs current urls baseline), deterministically:
    #     1) Force-admit injected delta into local `urls` universe (so downstream meta sees it)
    #     2) Force-fetch injected delta via fetch_web_context(force_scrape_extra_urls=True),
    #        so we get attempted/persisted entries or an explicit failure reason.
    #
    # Safety:
    # - No effect when no injection / no delta.
    # - Does not weaken normal fastpath logic (already bypassed upstream when delta exists).
    # =====================================================================
    try:
        _fix41afc11_wc = web_context if isinstance(web_context, dict) else {}
        # Robust recovery (order required)
        _fix41afc11_extra = []
        if isinstance(_fix41afc11_wc.get("extra_urls"), (list, tuple)) and _fix41afc11_wc.get("extra_urls"):
            _fix41afc11_extra = list(_fix41afc11_wc.get("extra_urls") or [])
        elif isinstance(_fix41afc11_wc.get("diag_extra_urls_ui"), (list, tuple)) and _fix41afc11_wc.get("diag_extra_urls_ui"):
            _fix41afc11_extra = list(_fix41afc11_wc.get("diag_extra_urls_ui") or [])
        elif isinstance(_fix41afc11_wc.get("diag_extra_urls_ui_raw"), str) and (_fix41afc11_wc.get("diag_extra_urls_ui_raw") or "").strip():
            _raw = str(_fix41afc11_wc.get("diag_extra_urls_ui_raw") or "")
            _parts = []
            for _line in _raw.splitlines():
                _line = (_line or "").strip()
                if not _line:
                    continue
                for _p in _line.split(","):
                    _p = (_p or "").strip()
                    if _p:
                        _parts.append(_p)
            _fix41afc11_extra = _parts

        _fix41afc11_inj_norm = _inj_diag_norm_url_list(_fix41afc11_extra) if _fix41afc11_extra else []
        _fix41afc11_urls_norm = _inj_diag_norm_url_list(urls) if urls else []
        _fix41afc11_inj_set = set(_fix41afc11_inj_norm or [])
        _fix41afc11_base_set = set(_fix41afc11_urls_norm or [])
        _fix41afc11_delta = sorted(list(_fix41afc11_inj_set - _fix41afc11_base_set)) if _fix41afc11_inj_set else []

        if _fix41afc11_delta:
            # (1) Force-admit delta into local urls universe deterministically
            _added = []
            if urls and isinstance(urls[0], dict):
                _seen = set(_fix41afc11_urls_norm or [])
                for _u in _fix41afc11_delta:
                    if _u in _seen:
                        continue
                    urls.append({"url": _u, "source": "injected_force_admit", "is_injected": True})
                    _seen.add(_u)
                    _added.append(_u)
            else:
                _seen = set(_fix41afc11_urls_norm or [])
                for _u in _fix41afc11_delta:
                    if _u in _seen:
                        continue
                    urls.append(_u)
                    _seen.add(_u)
                    _added.append(_u)

            # (2) Must-fetch lane: force scrape extra urls even if not admitted by normal filter
            _q = str((prev_full or {}).get("question") or (previous_data or {}).get("question") or "evolution_injection_force_fetch").strip()
            _prev_snap = (prev_full or {}).get("baseline_sources_cache") or (prev_full or {}).get("baseline_sources_cache_v2") or None
            try:
                _fwc = fetch_web_context(
                    _q or "evolution_injection_force_fetch",
                    num_sources=int(min(12, max(1, len(_fix41afc11_urls_norm or []) + len(_fix41afc11_inj_norm or [])))),
                    fallback_mode=True,
                    fallback_urls=list(_fix41afc11_urls_norm or []),
                    existing_snapshots=_prev_snap,
                    extra_urls=list(_fix41afc11_inj_norm or []),
                    diag_run_id=str((_fix41afc11_wc or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                    diag_extra_urls_ui_raw=(_fix41afc11_wc or {}).get("diag_extra_urls_ui_raw"),
                    identity_only=False,
                    force_scrape_extra_urls=True,
                force_admit_extra_urls=True,
                ) or {}
            except TypeError:
                # Backward-compat: older fetch_web_context without force_scrape_extra_urls
                _fwc = fetch_web_context(
                    _q or "evolution_injection_force_fetch",
                    num_sources=int(min(12, max(1, len(_fix41afc11_urls_norm or []) + len(_fix41afc11_inj_norm or [])))),
                    fallback_mode=True,
                    fallback_urls=list(_fix41afc11_urls_norm or []),
                    existing_snapshots=_prev_snap,
                    extra_urls=list(_fix41afc11_inj_norm or []),
                    diag_run_id=str((_fix41afc11_wc or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                    diag_extra_urls_ui_raw=(_fix41afc11_wc or {}).get("diag_extra_urls_ui_raw"),
                    identity_only=False,
                ) or {}

            # If fetch_web_context returns a concrete web_sources list, prefer it for downstream scraped_meta
            _fwc_sources = _fwc.get("web_sources") or _fwc.get("sources") or None
            if isinstance(_fwc_sources, list) and _fwc_sources:
                urls = list(_fwc_sources)

            # Emit explicit debug fields
            if isinstance(web_context, dict):
                web_context.setdefault("debug", {})
                if isinstance(web_context.get("debug"), dict):
                    web_context["debug"].setdefault("fix41afc11", {})
                    if isinstance(web_context["debug"].get("fix41afc11"), dict):
                        web_context["debug"]["fix41afc11"].update({
                            "inj_force_admit_applied": True,
                            "inj_force_admit_count": int(len(_added)),
                            "inj_force_admit_urls": list(_added),
                            "inj_delta_count": int(len(_fix41afc11_delta)),
                            "inj_delta": list(_fix41afc11_delta),
                            "inj_must_fetch_called": True,
                            "inj_must_fetch_sources_count": int(len(_fwc_sources or [])) if isinstance(_fwc_sources, list) else 0,
                        })
    except Exception:
        pass
    # =====================================================================



    scraped_meta = _fix24_build_scraped_meta(urls)

    # Step 3: Normalize into baseline_sources_cache and hash
    cur_bsc = _fix24_baseline_sources_cache_from_scraped_meta(scraped_meta)

    # =====================================================================
    # PATCH EVO_INJECTED_URLS_AS_CURRENT_SOURCES_V1 (ADDITIVE):
    # Policy + wiring alignment for Evolution UI injected URLs
    #
    # Goal:
    # - Treat Evolution-tab injected URLs as part of the *current source universe*
    #   for hash identity *when they fetch successfully*, consistent with baseline
    #   sources (successful snapshots contribute to identity).
    #
    # Behavior (safe):
    # - If an injected URL was provided (web_context['extra_urls']) and its scrape
    #   status is success, it must appear in cur_bsc so that hash inputs can include it.
    # - If success-but-missing occurs (unexpected), we add a synthetic url-only entry
    #   tagged for hash identity (debug only; no numbers).
    # - If fetch failed, we do NOT force hash mismatch (consistent with policy),
    #   but we record explicit attempted status + reason into diagnostics.
    #
    # Safety:
    # - Does NOT alter fastpath eligibility logic directly; it only ensures that
    #   the identity inputs reflect the actual successfully fetched current sources.
    # - Purely additive; never removes or refactors existing logic.
    # =====================================================================
    try:
        _inj_sm = scraped_meta if isinstance(scraped_meta, dict) else {}
        _inj_attempted_rows = []
        _inj_success_urls = set()
        for _u in (_inj_extra_urls or []):
            _m = _inj_sm.get(_u) if isinstance(_inj_sm.get(_u), dict) else {}
            _st = str(_m.get('status') or _m.get('fetch_status') or '')
            _reason = str(_m.get('status_detail') or _m.get('fail_reason') or '')
            _clen = _m.get('clean_text_len') or _m.get('content_len') or 0
            _inj_attempted_rows.append({
                'url': _u,
                'status': _st or 'unknown',
                'reason': _reason,
                'content_len': int(_clen) if str(_clen).isdigit() else 0,
            })
            if (_st or '').lower() in ('success','ok','fetched'):
                _inj_success_urls.add(_u)

        # Ensure success injected urls are represented in cur_bsc (hash identity)
        if _inj_success_urls and isinstance(cur_bsc, list):
            _bsc_urls = set()
            for _row in cur_bsc:
                if isinstance(_row, dict):
                    _bu = str(_row.get('url') or _row.get('source_url') or '').strip()
                    if _bu:
                        _bsc_urls.add(_bu)
            _missing_success = sorted(list(_inj_success_urls - _bsc_urls))
            for _u in _missing_success:
                cur_bsc.append({
                    'url': _u,
                    'status': 'success',
                    'status_detail': 'synthetic_success_missing_in_bsc',
                    'clean_text': '',
                    'clean_text_len': 0,
                    'extracted_numbers': [],
                    'numbers_found': 0,
                    'fingerprint': 'synthetic_url_only_for_hash',
                    'is_synthetic_for_hash': True,
                })

        # Write attempted status into web_context diagnostics for transparency
        if isinstance(web_context, dict):
            web_context.setdefault('diag_injected_urls', {})
            if isinstance(web_context.get('diag_injected_urls'), dict):
                web_context['diag_injected_urls'].setdefault('attempted', [])
                # Only overwrite if empty to avoid clobbering richer traces
                if not web_context['diag_injected_urls'].get('attempted'):
                    web_context['diag_injected_urls']['attempted'] = _inj_attempted_rows
                web_context['diag_injected_urls']['success_urls'] = sorted(list(_inj_success_urls))
    except Exception:
        pass
    # =====================================================================

    cur_hashes = _fix24_compute_current_hashes(cur_bsc)

    # =====================================================================

    # PATCH INJ_DIAG_EVO_DEBUG (ADDITIVE): record injected URL lifecycle (B1)
    # =====================================================================
    try:
        _hash_inputs = _inj_diag_hash_inputs_from_bsc(cur_bsc)
        if isinstance(web_context, dict):
            web_context.setdefault("diag_injected_urls", {})
            if isinstance(web_context.get("diag_injected_urls"), dict):
                web_context["diag_injected_urls"].update({
                    "run_id": _inj_diag_run_id,
                    "ui_raw": (web_context or {}).get("diag_extra_urls_ui_raw") or "",
                    "ui_norm": _inj_extra_urls,
                    "intake_norm": _inj_extra_urls,
                    "admitted": list(urls or []),
                    "hash_inputs": _hash_inputs,
                    "injected_in_hash_inputs": sorted(list(set(_inj_extra_urls) & set(_hash_inputs))),
                    "set_hashes": {
                        "hash_inputs": _inj_diag_set_hash(_hash_inputs),
                        "admitted": _inj_diag_set_hash(list(urls or [])),
                    }
                })
    except Exception:
        pass
    # =====================================================================



    # Step 4: Compare (v2 preferred)
    equal_v2 = bool(prev_hashes.get("v2") and cur_hashes.get("v2") and prev_hashes["v2"] == cur_hashes["v2"])
    equal_v1 = bool(prev_hashes.get("v1") and cur_hashes.get("v1") and prev_hashes["v1"] == cur_hashes["v1"])
    unchanged = equal_v2 or (not prev_hashes.get("v2") and equal_v1)

    # =====================================================================
    # PATCH FIX40 (ADDITIVE): Force rebuild override (Scenario B)
    # If the UI (or caller) requests force_rebuild, we intentionally bypass
    # the unchanged fastpath even if hashes match, to exercise rebuild logic.
    # =====================================================================
    _force_rebuild = False
    try:
        _force_rebuild = bool((web_context or {}).get("force_rebuild"))
    except Exception:
        _force_rebuild = False
    if _force_rebuild:
        unchanged = False
        _fix41_force_rebuild_honored = True
    else:
        _fix41_force_rebuild_honored = False
    # =====================================================================

    if unchanged:
        hashes = {
            "prev_v2": prev_hashes.get("v2",""),
            "cur_v2": cur_hashes.get("v2",""),
            "prev_v1": prev_hashes.get("v1",""),
            "cur_v1": cur_hashes.get("v1",""),
        }
        out_replay = _fix24_make_replay_output(prev_full, hashes)
        # =====================================================================
        # PATCH FIX41 (ADDITIVE): Attach force-rebuild debug to replay output
        # =====================================================================
        try:
            if isinstance(out_replay, dict):
                out_replay.setdefault("code_version", CODE_VERSION)
                out_replay.setdefault("debug", {}).setdefault("fix41", {})
                out_replay["debug"]["fix41"].update({
                    "force_rebuild_seen": bool(_fix41_force_rebuild_seen),
                    "force_rebuild_honored": bool(locals().get("_fix41_force_rebuild_honored", False)),
                    "path": "replay_unchanged",
                })
        except Exception:
            pass

        # =====================================================================
        # PATCH EVO_INJ_TRACE_REPLAY1 (ADDITIVE): emit inj_trace_v1 even on replay fastpath
        # Why:
        # - The FIX24 replay path returns early (skipping compute_source_anchored_diff),
        #   which previously meant results.debug.inj_trace_v1 might be missing.
        # - We need injected-URL lifecycle visibility even when hashes match (fastpath/replay)
        #   to validate UI wiring and to explain why a mismatch did/did not occur.
        # Safety:
        # - Pure debug emission only; does NOT affect hash logic, scraping, or fastpath decisions.
        # =====================================================================
        try:
            _wc = web_context if isinstance(web_context, dict) else {}
            _diag = _wc.get("diag_injected_urls") if isinstance(_wc.get("diag_injected_urls"), dict) else {}

            # =====================================================================
            # PATCH INJ_TRACE_V1_ENRICH_EVOLUTION_REPLAY_ARTIFACTS (ADDITIVE)
            # Populate attempted/persisted for injected URLs from scraped_meta/cur_bsc
            # even when replay fastpath returns early.
            # Also attach an explicit reason when injected URLs are present but not
            # admitted/hashed due to replay semantics.
            # =====================================================================
            try:
                if isinstance(_diag, dict):
                    # Enrich from scraped_meta (injected only) and from BSC (all)
                    _diag = _inj_trace_v1_enrich_diag_from_scraped_meta(_diag, scraped_meta, (_inj_extra_urls or []))
                    _diag = _inj_trace_v1_enrich_diag_from_bsc(_diag, cur_bsc if isinstance(cur_bsc, list) else [])
                    # Explain replay semantics when UI extras exist
                    _ui_norm = _inj_diag_norm_url_list(_diag.get("ui_norm") or [])
                    if _ui_norm:
                        _diag.setdefault("admission_reason", "fastpath_replay_no_rebuild_no_admission")
                        _diag.setdefault("injection_effective", False)
            except Exception:
                pass
            # =====================================================================

            _hash_inputs_replay = _inj_diag_hash_inputs_from_bsc(cur_bsc)
            _trace_replay = _inj_trace_v1_build(
                diag_injected_urls=_diag,
                hash_inputs=_hash_inputs_replay,
                stage="evolution",
                path="fastpath_replay",
                rebuild_pool=None,
                rebuild_selected=None,
            )
            out_replay.setdefault("results", {})
            if isinstance(out_replay.get("results"), dict):
                out_replay["results"].setdefault("debug", {})
                if isinstance(out_replay["results"].get("debug"), dict):
                    out_replay["results"]["debug"]["inj_trace_v1"] = _trace_replay
        except Exception:
            pass
        # =====================================================================
        # END PATCH EVO_INJ_TRACE_REPLAY1
        # =====================================================================
        return out_replay

    # Step 5: Changed -> run deterministic evolution diff using existing machinery.
    # Provide web_context with scraped_meta so compute_source_anchored_diff can reconstruct snapshots deterministically.
    wc = {"scraped_meta": scraped_meta}
    # =====================================================================
    # PATCH FIX40 (ADDITIVE): Preserve caller flags (e.g., force_rebuild) into web_context
    # so downstream diff/rebuild logic can record provenance if needed.
    # =====================================================================
    try:
        if isinstance(web_context, dict):
            wc.update({k: v for k, v in web_context.items() if k != "scraped_meta"})
    except Exception:
        pass
    # =====================================================================



    if callable(run_source_anchored_evolution_BASE):
        try:
            out = run_source_anchored_evolution_BASE(prev_full, web_context=wc)
            if isinstance(out, dict):
                out.setdefault("debug", {})
                if isinstance(out["debug"], dict):
                    out["debug"]["fix24"] = True
                    out["debug"]["fix24_mode"] = "recompute_changed"
                    # =====================================================================
                    # PATCH FIX40 (ADDITIVE): record Scenario B override
                    # =====================================================================
                    out["debug"]["fix40_force_rebuild"] = bool(_force_rebuild)
                    # =====================================================================
            # =====================================================================
            # =====================================================================
            # =====================================================================
            # =====================================================================
                    out["debug"]["prev_source_snapshot_hash_v2"] = prev_hashes.get("v2","")
                    out["debug"]["cur_source_snapshot_hash_v2"] = cur_hashes.get("v2","")
                    out["debug"]["prev_source_snapshot_hash"] = prev_hashes.get("v1","")
                    out["debug"]["cur_source_snapshot_hash"] = cur_hashes.get("v1","")
            return out
        except Exception as e:
            # Fall through to original behavior if anything unexpected
            pass

    # Ultimate fallback: call compute_source_anchored_diff directly if base runner not available
    fn = globals().get("compute_source_anchored_diff")
    if callable(fn):
        try:
            out_changed = fn(prev_full, web_context=wc)
            # =====================================================================
            # PATCH FIX41 (ADDITIVE): Attach force-rebuild debug to changed output
            # =====================================================================
            try:
                if isinstance(out_changed, dict):
                    out_changed.setdefault("code_version", CODE_VERSION)
                    out_changed.setdefault("debug", {}).setdefault("fix41", {})
                    out_changed["debug"]["fix41"].update({
                        "force_rebuild_seen": bool(_fix41_force_rebuild_seen),
                        "force_rebuild_honored": bool(locals().get("_fix41_force_rebuild_honored", False)) or bool(_fix41_force_rebuild_seen),
                        "path": "changed_compute_source_anchored_diff",
                    })
            except Exception:
                pass
            return out_changed
        except Exception:
            pass

    return {
        "status": "failed",
        "message": "FIX24: Evolution recompute failed (no callable base evolution runner).",
        "sources_checked": len(urls),
        "sources_fetched": len(urls),
        "metric_changes": [],
        "debug": {"fix24": True, "fix24_mode": "recompute_failed"},
    }


# ==============================================================================
# FIX32 (ADDITIVE): Unit-required hard gate in evolution diff rendering
#
# Problem:
# - Evolution diff table can show unit-less / year-like integers (e.g. 2024, 2033, 1500)
#   in the "Current" column for metrics whose schema requires currency/percent/rate/ratio.
# - This happens when upstream selection or LLM delta yields a numeric candidate that lacks
#   token-level unit evidence (unit_tag/unit_family/base_unit empty), and the diff layer
#   currently treats it as a valid numeric current value.
#
# Target invariant:
# - For any metric with unit_family in {currency, percent, rate, ratio} (or with a non-empty
#   unit_tag/unit), a unit-less candidate must be treated as ineligible and must not be
#   rendered as a valid "Current" value in the evolution table/output.
#
# Approach (purely additive):
# - Preserve existing diff implementation as diff_metrics_by_name_FIX31_BASE.
# - Override diff_metrics_by_name with a wrapper that post-processes each row:
#     * if schema says unit is required AND current unit evidence is missing,
#       then mark unit_mismatch=True and render current_value="N/A" (and cur_value_norm=None).
# - No refactors; does not change upstream extraction/building logic, only prevents
#   unit-less values from appearing as "valid" in evolution output.
# ==============================================================================
try:
    diff_metrics_by_name_FIX31_BASE = diff_metrics_by_name  # type: ignore
except Exception:
    diff_metrics_by_name_FIX31_BASE = None  # type: ignore

def _fix32_metric_requires_unit(metric_def: dict) -> bool:
    """Return True if schema implies this metric requires unit evidence."""
    try:
        spec = metric_def if isinstance(metric_def, dict) else {}
        uf = str(spec.get("unit_family") or "").strip().lower()
        ut = str(spec.get("unit_tag") or spec.get("unit") or "").strip()
        # Core families we treat as unit-required
        if uf in ("currency", "percent", "rate", "ratio"):
            return True
        # If schema explicitly declares a unit tag/unit, treat as required (except year/time-ish)
        if ut:
            # Avoid requiring "year" units
            blob = (uf + " " + ut + " " + str(spec.get("name") or "")).lower()
            if "year" in blob or "time" in blob:
                return False
            return True
    except Exception:
        pass
    return False

def _fix32_has_token_unit_evidence(metric_row: dict) -> bool:
    """
    Token-level unit evidence heuristic:
    - Prefer structured fields: base_unit/unit/unit_tag/unit_family
    - Fall back to raw token containing '$' or '%' or currency code immediately adjacent.
    Deterministic; does NOT attempt any NLP.
    """
    try:
        m = metric_row if isinstance(metric_row, dict) else {}
        for k in ("base_unit", "unit", "unit_tag", "unit_family"):
            if str(m.get(k) or "").strip():
                return True

        raw = str(m.get("raw") or m.get("value") or "")
        if not raw:
            return False
        r = raw.strip()
        rl = r.lower()

        # direct symbol evidence on the token
        if any(sym in r for sym in ("$", "€", "£", "¥", "%")):
            return True

        # compact currency codes adjacent to the token
        # NOTE: keep conservative (requires code in same token string)
        if any(code in rl for code in ("usd", "sgd", "eur", "gbp", "aud", "cad", "jpy", "cny")):
            return True

    except Exception:
        return False
    return False

def diff_metrics_by_name(prev_response: dict, cur_response: dict):  # noqa: F811
    """
    FIX32 wrapper: calls existing diff, then enforces the unit-required hard gate at render time.
    """
    if not callable(diff_metrics_by_name_FIX31_BASE):
        # Fallback: nothing we can do
        return ([], 0, 0, 0, 0)

    metric_changes, unchanged, increased, decreased, found = diff_metrics_by_name_FIX31_BASE(prev_response, cur_response)

    # =====================================================================
    # PATCH FIX2B_EVO_CURFIELDS_V2 (ADDITIVE): disabled due to indentation corruption; FIX2B_EVO_CURFIELDS_V1 handles canonical-first enrichment
    # =====================================================================


    try:
        if not isinstance(metric_changes, list):
            return metric_changes, unchanged, increased, decreased, found

        for row in metric_changes:
            if not isinstance(row, dict):
                continue

            md = row.get("metric_definition") or {}
            if not _fix32_metric_requires_unit(md):
                continue

            cur_unit_cmp = str(row.get("cur_unit_cmp") or "").strip()
            # If diff already says mismatch, keep it; we only add the missing-unit mismatch
            if cur_unit_cmp:
                continue

            # Determine whether the current-side metric row shows any unit evidence at all
            # We use the current canonical row when present; else fall back to row fields.
            cur_metrics = (cur_response or {}).get("primary_metrics_canonical") or {}
            ck = row.get("canonical_key") or ""
            cm = cur_metrics.get(ck) if isinstance(cur_metrics, dict) else None

            has_ev = _fix32_has_token_unit_evidence(cm or {})
            if not has_ev:
                # Enforce: missing unit where required => mismatch + no current value rendered
                row["unit_mismatch"] = True
                row["cur_unit_cmp"] = ""
                row["current_value"] = "N/A"
                row["cur_value_norm"] = None

                # Make the reason machine-detectable (non-breaking extra field)
                row.setdefault("guardrail_reason", "unit_required_missing_current_unit")

                # Re-classify change as unit_mismatch (keeps deterministic reporting)
                row["change_type"] = "unit_mismatch"
                row["change_pct"] = None

    except Exception:
        pass

    return metric_changes, unchanged, increased, decreased, found

# ==============================================================================
# END FIX32
# ==============================================================================
# =====================================================================
# PATCH V23_CANONICAL_FOR_RENDER_FORCE_CLEAR (ADDITIVE)
# Goal: When the current evolution canonical dict is "present but junk" (year-like / unitless winners),
# force the canonical-for-render rebuild path to actually run.
#
# Root cause (observed in v22 outputs):
# - canonical_for_render is initially assigned from current_metrics (non-empty)
# - _need_render_rebuild can become True due to suspiciousness
# - BUT rebuild attempts are guarded by `if (not canonical_for_render)` so they never execute
#   when canonical_for_render is already non-empty.
#
# Fix (render-only):
# - Wrap diff_metrics_by_name so that when we detect a suspicious current canonical dict,
#   we pass an empty current canonical map into the base diff function.
# - This triggers the existing v21/v22 rebuild ladder to actually run.
# - Fully auditable: adds diag markers at row-level and (optionally) at top-level.
# =====================================================================
try:
    diff_metrics_by_name_FIX32_V22_BASE
except Exception:
    diff_metrics_by_name_FIX32_V22_BASE = None

try:
    diff_metrics_by_name_FIX31_BASE
except Exception:
    diff_metrics_by_name_FIX31_BASE = None


def _ph2b_v23_yearlike(_x):
    try:
        if _x is None:
            return False
        fx = float(_x)
        if abs(fx - round(fx)) < 1e-9:
            ix = int(round(fx))
            return 1900 <= ix <= 2105
        return False
    except Exception:
        return False


def _ph2b_v23_metric_suspicious(_m):
    try:
        if not isinstance(_m, dict):
            return True
        u = (str(_m.get('unit') or _m.get('unit_tag') or '').strip())
        vn = _m.get('value_norm')
        # Most problematic pattern: unitless + yearlike/None
        if (not u) and (_ph2b_v23_yearlike(vn) or vn is None):
            return True
        # Also treat empty raw+unit as suspicious when vn is present but unitless
        raw = (str(_m.get('raw') or '').strip())
        if vn is not None and (not u) and (not raw):
            return True
        return False
    except Exception:
        return True


def _ph2b_v23_current_metrics_suspicious(_cm_map):
    try:
        if not isinstance(_cm_map, dict) or not _cm_map:
            return True
        ks = list(sorted(list(_cm_map.keys())))[:25]
        if not ks:
            return True
        sus = 0
        tot = 0
        for k in ks:
            tot += 1
            if _ph2b_v23_metric_suspicious(_cm_map.get(k)):
                sus += 1
        if tot <= 0:
            return True
        return (sus / float(tot)) >= 0.30
    except Exception:
        return False


def diff_metrics_by_name_FIX33_V23_CANONICAL_CLEAR(prev_response, cur_response, *args, **kwargs):
    """Wrapper around the existing diff implementation to force render-only rebuild when current canonical is suspicious."""
    if not callable(diff_metrics_by_name_FIX31_BASE):
        # If we can't find the base function, fall back to any existing diff impl
        if callable(diff_metrics_by_name_FIX32_V22_BASE):
            return diff_metrics_by_name_FIX32_V22_BASE(prev_response, cur_response, *args, **kwargs)
        return []

    _cur = cur_response
    _forced_clear = False
    _sus = False
    try:
        cm = None
        if isinstance(cur_response, dict):
            cm = cur_response.get('primary_metrics_canonical')
        _sus = bool(_ph2b_v23_current_metrics_suspicious(cm))
        if _sus:
            _forced_clear = True
            # Shallow-copy cur_response and clear primary_metrics_canonical only.
            _cur = dict(cur_response) if isinstance(cur_response, dict) else cur_response
            if isinstance(_cur, dict):
                # Preserve original for audit
                _cur.setdefault('diag', {})
                if isinstance(_cur.get('diag'), dict):
                    _cur['diag'].setdefault('ph2b_v23_force_clear_current_metrics', True)
                    try:
                        _cur['diag'].setdefault('ph2b_v23_force_clear_reason', 'suspicious_current_metrics_triggered_render_rebuild')
                    except Exception:
                        pass
                # Force empty so v21/v22 rebuild ladder actually runs
                _cur['primary_metrics_canonical'] = {}
    except Exception:
        _cur = cur_response
        _forced_clear = False

    rows = diff_metrics_by_name_FIX31_BASE(prev_response, _cur, *args, **kwargs)

    # PATCH V23_ROW_DIAG (ADDITIVE): mark every returned row so we can confirm v23 wrapper ran
    try:
        if _forced_clear and isinstance(rows, list):
            for r in rows:
                if isinstance(r, dict):
                    r.setdefault('diag', {})
                    if isinstance(r.get('diag'), dict):
                        r['diag'].setdefault('ph2b_v23_force_clear_applied', True)
                        r['diag'].setdefault('ph2b_v23_force_clear_suspicious', bool(_sus))
    except Exception:
        pass

    return rows


# PATCH V23_WIRE (ADDITIVE): Replace the public diff entrypoint used by evolution with the v23 wrapper.
try:
    if callable(diff_metrics_by_name_FIX33_V23_CANONICAL_CLEAR):
        diff_metrics_by_name = diff_metrics_by_name_FIX33_V23_CANONICAL_CLEAR
except Exception:
    pass

# PATCH V23_VERSION_BUMP (ADDITIVE): bump CODE_VERSION for audit
try:
    CODE_VERSION = 'fix41afc19_evo_fix16_anchor_rebuild_override_v1_fix2b_hardwire_v23'
except Exception:
    pass



# =====================================================================
# PATCH V24_STRICT_CANONICAL_KEY_MATCH (ADDITIVE)
# Goal: When canonical-for-render is active, enforce strict canonical_key identity for current-side lookup.
# This blocks all name/heuristic fallback matching that can substitute unrelated canon metrics (e.g., 2.0 B, 170.0, 2030.0).
#
# Activation: cur_response contains _ph2b_strict_ckey_v24 == True.
#
# Safety: render/diff-layer only. Does not touch fastpath/hashing/injection/snapshot attach.
# =====================================================================
try:
    diff_metrics_by_name_V24_BASE = diff_metrics_by_name  # type: ignore
except Exception:
    diff_metrics_by_name_V24_BASE = None  # type: ignore

def _v24_num(x):
    try:
        if x is None:
            return None
        if isinstance(x, (int, float)):
            return float(x)
        s = str(x).strip()
        if not s:
            return None
        # remove common commas
        s = s.replace(",", "")
        return float(s)
    except Exception:
        return None

def diff_metrics_by_name_FIX34_V24_STRICT(prev_response: dict, cur_response: dict):
    """Strict diff: current metric is looked up ONLY by canonical_key when v24 strict flag is set."""
    # If not in strict mode, fall back to existing diff implementation.
    try:
        if not (isinstance(cur_response, dict) and cur_response.get("_ph2b_strict_ckey_v24")):
            if callable(diff_metrics_by_name_V24_BASE):
                return diff_metrics_by_name_V24_BASE(prev_response, cur_response)
            return ([], 0, 0, 0, 0)
    except Exception:
        if callable(diff_metrics_by_name_V24_BASE):
            return diff_metrics_by_name_V24_BASE(prev_response, cur_response)
        return ([], 0, 0, 0, 0)

    prev_can = (prev_response or {}).get("primary_metrics_canonical") or {}
    cur_can = (cur_response or {}).get("primary_metrics_canonical") or {}

    metric_changes = []
    unchanged = increased = decreased = found = 0

    try:
        if not isinstance(prev_can, dict):
            prev_can = {}
        if not isinstance(cur_can, dict):
            cur_can = {}

        # Use prev keys as the authoritative set for diffing (matches Analysis behavior).
        for ckey in prev_can.keys():
            pm = prev_can.get(ckey) if isinstance(prev_can.get(ckey), dict) else (prev_can.get(ckey) or {})
            cm = cur_can.get(ckey) if isinstance(cur_can.get(ckey), dict) else (cur_can.get(ckey) or None)

            row = {
                "canonical_key": ckey,
                "metric_name": (pm.get("name") if isinstance(pm, dict) else "") or ckey,
                "previous_value": (pm.get("value_norm") if isinstance(pm, dict) else None),
                "current_value": None,
                "prev_unit_cmp": (pm.get("unit") if isinstance(pm, dict) else "") or "",
                "cur_unit_cmp": "",
                "prev_value_norm": (pm.get("value_norm") if isinstance(pm, dict) else None),
                "cur_value_norm": None,
                "unit_mismatch": False,
                "change_type": "not_found",
                "confidence": pm.get("confidence") if isinstance(pm, dict) else None,
                "metric_definition": pm.get("metric_definition") if isinstance(pm, dict) else None,
            }

            if isinstance(cm, dict) and cm:
                found += 1
                cvn = cm.get("value_norm")
                cunit = (cm.get("unit") or cm.get("unit_tag") or "").strip()
                craw = (cm.get("raw") or "").strip()
                if not craw:
                    try:
                        if cvn is not None and cunit:
                            craw = f"{cvn} {cunit}".strip()
                        elif cvn is not None:
                            craw = str(cvn)
                    except Exception:
                        craw = ""
                row["current_value"] = craw
                row["cur_value_norm"] = cvn
                row["current_value_norm"] = cvn
                row["cur_unit_cmp"] = cunit
                row["current_unit"] = cunit

                pv = _v24_num(row.get("previous_value"))
                cv = _v24_num(cvn)
                if pv is not None and cv is not None:
                    if abs(cv - pv) < 1e-9:
                        row["change_type"] = "unchanged"
                        unchanged += 1
                    elif cv > pv:
                        row["change_type"] = "increased"
                        increased += 1
                    else:
                        row["change_type"] = "decreased"
                        decreased += 1
                else:
                    # We found a row but cannot compare numerically
                    row["change_type"] = "found"

                # Attach v24 audit
                row.setdefault("diag", {})
                if isinstance(row.get("diag"), dict):
                    row["diag"].setdefault("canonical_key_strict_v1", {})
                    row["diag"]["canonical_key_strict_v1"]["enabled"] = True
                    row["diag"]["canonical_key_strict_v1"]["used_key"] = ckey
                    row["diag"]["canonical_key_strict_v1"]["fallback_blocked"] = True
            else:
                # Not found: keep blank. Still attach audit.
                row.setdefault("diag", {})
                if isinstance(row.get("diag"), dict):
                    row["diag"].setdefault("canonical_key_strict_v1", {})
                    row["diag"]["canonical_key_strict_v1"]["enabled"] = True
                    row["diag"]["canonical_key_strict_v1"]["used_key"] = ckey
                    row["diag"]["canonical_key_strict_v1"]["fallback_blocked"] = True
                    row["diag"]["canonical_key_strict_v1"]["not_found"] = True

            metric_changes.append(row)

    except Exception:
        # On failure, revert to existing diff
        if callable(diff_metrics_by_name_V24_BASE):
            return diff_metrics_by_name_V24_BASE(prev_response, cur_response)
        return ([], 0, 0, 0, 0)

    return metric_changes, unchanged, increased, decreased, found

# PATCH V24_WIRE (ADDITIVE): override diff_metrics_by_name entrypoint with v24 strict-aware wrapper.
try:
    if callable(diff_metrics_by_name_FIX34_V24_STRICT):
        diff_metrics_by_name = diff_metrics_by_name_FIX34_V24_STRICT  # type: ignore
except Exception:
    pass

# PATCH V24_VERSION_BUMP (ADDITIVE)
try:
    CODE_VERSION = 'fix41afc19_evo_fix16_anchor_rebuild_override_v1_fix2b_hardwire_v24'
except Exception:
    pass


# =====================================================================
# PATCH FIX41AFC19_V25 (ADDITIVE): CODE_VERSION bump (audit)
# =====================================================================
try:
    CODE_VERSION = "fix41afc19_evo_fix16_anchor_rebuild_override_v1_fix2b_hardwire_v25"
except Exception:
    pass
# =====================================================================
# END PATCH FIX41AFC19_V25
# =====================================================================


# =====================================================================
# PATCH CODE_VERSION_V26 (ADDITIVE)
# =====================================================================
CODE_VERSION = 'fix41afc19_evo_fix16_anchor_rebuild_override_v1_fix2b_hardwire_v26'
# =====================================================================
# END PATCH CODE_VERSION_V26
# =====================================================================

# =====================================================================
# PATCH V27_VERSION_BUMP (ADDITIVE)
# =====================================================================
try:
    CODE_VERSION = 'fix41afc19_evo_fix16_anchor_rebuild_override_v1_fix2b_hardwire_v27'
except Exception:
    pass
# =====================================================================
# END PATCH V27_VERSION_BUMP
# =====================================================================

# =====================================================================
# PATCH V28_VERSION_BUMP (ADDITIVE): bump CODE_VERSION for audit
# =====================================================================
try:
    CODE_VERSION = 'fix41afc19_evo_fix16_anchor_rebuild_override_v1_fix2b_hardwire_v28'
except Exception:
    pass
# =====================================================================
# END PATCH V28_VERSION_BUMP
# =====================================================================


# =====================================================================
# PATCH V29_CODE_VERSION_BUMP (ADDITIVE)
# =====================================================================
CODE_VERSION = 'fix41afc19_evo_fix16_anchor_rebuild_override_v1_fix2b_hardwire_v29'
# =====================================================================
# END PATCH V29_CODE_VERSION_BUMP
# =====================================================================


# =====================================================================
# PATCH V32_PREFER_CUR_PRIMARY_METRICS_CANONICAL (ADDITIVE)
# Goal: For Evolution diff panel, prefer cur_response['primary_metrics_canonical'][canonical_key] as the source of CURRENT
#       when present. This avoids render-only snapshot rebuild empties and eliminates raw numeric pool fallbacks.
# Safety: diff/render-layer only. Does not touch fastpath replay / hashing universe / injection lifecycle / snapshot attach / extraction.
#
# Adds per-row audit:
#   row['diag']['v32_current_source_v1'] = {
#       'cur_has_pmc': bool,
#       'used_current_source': 'cur_primary_metrics_canonical' | 'base_diff' | 'none',
#       'ckey': canonical_key,
#   }
# Also adds a tiny debug counter into cur_response (harmless):
#   cur_response['_debug_v32_pmc_used_count'] (int)
# =====================================================================

try:
    diff_metrics_by_name_V32_BASE = diff_metrics_by_name  # type: ignore
except Exception:
    diff_metrics_by_name_V32_BASE = None  # type: ignore

def _v32_safe_str(x):
    try:
        if x is None:
            return ""
        return str(x)
    except Exception:
        return ""

def _v32_pick_canon_metric(cur_pmc: dict, ckey: str):
    try:
        if not isinstance(cur_pmc, dict):
            return None
        m = cur_pmc.get(ckey)
        return m if isinstance(m, dict) else None
    except Exception:
        return None

def _v32_extract_value_unit_raw(cm: dict):
    '''
    Extract (value_norm, unit, raw_display) without numeric inference.
    Uses a few known canonical fields and then evidence[0] as a last resort.
    '''
    try:
        if not isinstance(cm, dict):
            return (None, "", "")
        cvn = cm.get("value_norm", None)
        unit = (cm.get("unit") or cm.get("unit_tag") or cm.get("unit_cmp") or "").strip()
        raw = (cm.get("value_display") or cm.get("value_range_display") or cm.get("raw") or "").strip()
        if (cvn is None or unit == "") and isinstance(cm.get("evidence"), list) and cm["evidence"]:
            ev0 = cm["evidence"][0] if isinstance(cm["evidence"][0], dict) else None
            if isinstance(ev0, dict):
                if cvn is None:
                    cvn = ev0.get("value_norm", ev0.get("value", None))
                if unit == "":
                    unit = (ev0.get("unit") or ev0.get("unit_tag") or ev0.get("unit_cmp") or "").strip()
                if not raw:
                    raw = (ev0.get("raw") or "").strip()
        if not raw:
            try:
                if cvn is not None and unit:
                    raw = f"{cvn} {unit}".strip()
                elif cvn is not None:
                    raw = str(cvn)
            except Exception:
                raw = ""
        return (cvn, unit, raw)
    except Exception:
        return (None, "", "")

def diff_metrics_by_name_FIX40_V32_PREFER_PMC(prev_response: dict, cur_response: dict):
    '''
    Wrapper around existing diff_metrics_by_name:
      - Computes base metric_changes using existing logic
      - Then, if cur_response['primary_metrics_canonical'] contains an entry for the same canonical_key,
        overwrites CURRENT fields from that canonical metric (NO numeric inference).
      - Emits per-row audit under row['diag']['v32_current_source_v1'].
    '''
    # Run base implementation first
    if callable(diff_metrics_by_name_V32_BASE):
        metric_changes, unchanged, increased, decreased, found = diff_metrics_by_name_V32_BASE(prev_response, cur_response)
    else:
        return ([], 0, 0, 0, 0)

    try:
        cur_pmc = (cur_response or {}).get("primary_metrics_canonical") if isinstance(cur_response, dict) else None
        cur_has_pmc = isinstance(cur_pmc, dict) and len(cur_pmc) > 0
        used_count = 0

        if isinstance(metric_changes, list):
            for row in metric_changes:
                try:
                    if not isinstance(row, dict):
                        continue
                    ckey = row.get("canonical_key") or row.get("ckey") or ""
                    ckey = _v32_safe_str(ckey).strip()
                    row.setdefault("diag", {})
                    if isinstance(row.get("diag"), dict):
                        row["diag"].setdefault("v32_current_source_v1", {})
                        row["diag"]["v32_current_source_v1"]["cur_has_pmc"] = bool(cur_has_pmc)
                        row["diag"]["v32_current_source_v1"]["ckey"] = ckey

                    if not (cur_has_pmc and ckey):
                        if isinstance(row.get("diag"), dict):
                            row["diag"]["v32_current_source_v1"]["used_current_source"] = "base_diff"
                        continue

                    cm = _v32_pick_canon_metric(cur_pmc, ckey)
                    if not isinstance(cm, dict) or not cm:
                        if isinstance(row.get("diag"), dict):
                            row["diag"]["v32_current_source_v1"]["used_current_source"] = "base_diff"
                        continue

                    cvn, unit, raw = _v32_extract_value_unit_raw(cm)
                    if cvn is None and not raw:
                        if isinstance(row.get("diag"), dict):
                            row["diag"]["v32_current_source_v1"]["used_current_source"] = "base_diff"
                        continue

                    # Override CURRENT fields
                    row["current_value"] = raw
                    row["cur_value_norm"] = cvn
                    row["current_value_norm"] = cvn
                    if unit:
                        row["cur_unit_cmp"] = unit
                        row["current_unit"] = unit

                    # Soft-clear unit_mismatch if both indicate percent
                    if unit and isinstance(row.get("prev_unit_cmp"), str) and row.get("prev_unit_cmp").strip() == "%" and unit.strip() == "%":
                        row["unit_mismatch"] = False

                    # Update change_type if numeric comparable (no inference)
                    try:
                        pv = row.get("prev_value_norm", row.get("previous_value", None))
                        pv_num = float(pv) if isinstance(pv, (int, float)) else None
                        cv_num = float(cvn) if isinstance(cvn, (int, float)) else None
                        if pv_num is not None and cv_num is not None:
                            if abs(cv_num - pv_num) < 1e-9:
                                row["change_type"] = "unchanged"
                            elif cv_num > pv_num:
                                row["change_type"] = "increased"
                            else:
                                row["change_type"] = "decreased"
                    except Exception:
                        pass

                    if isinstance(row.get("diag"), dict):
                        row["diag"]["v32_current_source_v1"]["used_current_source"] = "cur_primary_metrics_canonical"
                    used_count += 1
                except Exception:
                    continue

        # attach a tiny debug counter into cur_response for audit (harmless)
        try:
            if isinstance(cur_response, dict):
                cur_response["_debug_v32_pmc_used_count"] = used_count
        except Exception:
            pass

    except Exception:
        pass

    return metric_changes, unchanged, increased, decreased, found

# PATCH V32_WIRE (ADDITIVE): override diff_metrics_by_name entrypoint with v32 wrapper.
try:
    if callable(diff_metrics_by_name_FIX40_V32_PREFER_PMC):
        diff_metrics_by_name = diff_metrics_by_name_FIX40_V32_PREFER_PMC  # type: ignore
except Exception:
    pass

# PATCH V32_VERSION_BUMP (ADDITIVE)
try:
    CODE_VERSION = 'fix41afc19_evo_fix16_anchor_rebuild_override_v1_fix2b_hardwire_v32'
except Exception:
    pass

# =====================================================================
# END PATCH V32_PREFER_CUR_PRIMARY_METRICS_CANONICAL
# =====================================================================


# =====================================================================
# PATCH V34_EVOLUTION_DIFF_ANCHOR_JOIN (ADDITIVE)
# Goal: Fix Evolution Diff Metrics panel by adding a STRICT, ORDERED secondary join on anchor_hash.
#       - Primary join remains canonical_key equality (unchanged).
#       - Secondary join (NEW): if current missing, map prev_anchor_hash -> current canonical_key via cur metric_anchors.
#       - Current value sourcing: ONLY from cur_response['primary_metrics_canonical'][resolved_cur_ckey].
#       - No other fallback: no name similarity, no unit-family matching, no numeric inference, no raw numeric pools.
# Safety: Evolution render/diff layer ONLY. Does not touch fastpath replay / hashing universe / injection lifecycle /
#         snapshot attach / extraction / Analysis rebuild logic.
#
# Adds per-row diagnostics (exact requested shape):
#   row['diag']['diff_join_trace_v1']
#   row['diag']['diff_current_source_trace_v1']
# Adds top-level debug summary:
#   cur_response['debug']['diff_join_anchor_v34']
# =====================================================================

try:
    diff_metrics_by_name_V34_BASE = diff_metrics_by_name  # type: ignore
except Exception:
    diff_metrics_by_name_V34_BASE = None  # type: ignore


def _v34_safe_str(x):
    try:
        if x is None:
            return ""
        return str(x)
    except Exception:
        return ""


def _v34_get_anchor_hash(metric_anchors: dict, ckey: str):
    try:
        if not isinstance(metric_anchors, dict) or not ckey:
            return None
        a = metric_anchors.get(ckey)
        if not isinstance(a, dict):
            return None
        ah = a.get("anchor_hash")
        ahs = _v34_safe_str(ah).strip()
        if not ahs or ahs.lower() == "none":
            return None
        return ahs
    except Exception:
        return None


def _v34_build_cur_anchor_index(cur_metric_anchors: dict):
    """Return map anchor_hash -> deterministic canonical_key (lexicographically smallest)"""
    idx = {}
    try:
        if not isinstance(cur_metric_anchors, dict):
            return idx
        for ckey, a in cur_metric_anchors.items():
            if not isinstance(ckey, str):
                ckey = _v34_safe_str(ckey)
            ckey_s = ckey.strip()
            if not ckey_s:
                continue
            if not isinstance(a, dict):
                continue
            ah = _v34_safe_str(a.get("anchor_hash")).strip()
            if not ah or ah.lower() == "none":
                continue
            prev = idx.get(ah)
            if prev is None or ckey_s < prev:
                idx[ah] = ckey_s
    except Exception:
        pass
    return idx


def diff_metrics_by_name_FIX41_V34_ANCHOR_JOIN(prev_response: dict, cur_response: dict):
    """
    Evolution Diff Metrics join fix:
      1) Run base diff to get row set (keeps primary ckey join behavior).
      2) For rows that are missing CURRENT due to canonical_key drift, attempt strict anchor_hash join.
      3) If resolved_cur_ckey found, source CURRENT ONLY from cur primary_metrics_canonical[resolved_cur_ckey].
      4) Emit requested diagnostics and top-level debug summary.
    """
    if not callable(diff_metrics_by_name_V34_BASE):
        return ([], 0, 0, 0, 0)

    metric_changes, unchanged, increased, decreased, found = diff_metrics_by_name_V34_BASE(prev_response, cur_response)

    # Build anchor indices
    prev_ma = (prev_response or {}).get("metric_anchors") if isinstance(prev_response, dict) else None
    cur_ma = (cur_response or {}).get("metric_anchors") if isinstance(cur_response, dict) else None
    cur_pmc = (cur_response or {}).get("primary_metrics_canonical") if isinstance(cur_response, dict) else None

    prev_ma = prev_ma if isinstance(prev_ma, dict) else {}
    cur_ma = cur_ma if isinstance(cur_ma, dict) else {}
    cur_pmc = cur_pmc if isinstance(cur_pmc, dict) else {}

    cur_anchor_idx = _v34_build_cur_anchor_index(cur_ma)

    joined_by_ckey = 0
    joined_by_anchor = 0
    not_found = 0
    sample_anchor_joins = []

    def _is_missing_current(row: dict):
        try:
            # Treat "" / None / "N/A" as missing.
            v = row.get("current_value")
            if v is None:
                return True
            vs = _v34_safe_str(v).strip()
            if not vs or vs.upper() == "N/A":
                return True
            # Some codepaths may store numeric current_value_norm only.
            cvn = row.get("cur_value_norm", row.get("current_value_norm", None))
            if (cvn is None) and (not vs):
                return True
            return False
        except Exception:
            return True

    try:
        if isinstance(metric_changes, list):
            for row in metric_changes:
                if not isinstance(row, dict):
                    continue

                row.setdefault("diag", {})
                if not isinstance(row.get("diag"), dict):
                    row["diag"] = {}

                # Determine prev_ckey (what the diff row is keyed on)
                prev_ckey = row.get("canonical_key") or row.get("ckey") or row.get("canonical") or ""
                prev_ckey = _v34_safe_str(prev_ckey).strip()

                # Primary join (ckey): if base diff already has a non-missing CURRENT, call that "ckey".
                resolved_cur_ckey = prev_ckey if prev_ckey else None
                method = "none"
                prev_anchor_hash = _v34_get_anchor_hash(prev_ma, prev_ckey) if prev_ckey else None
                cur_anchor_hash = None

                if prev_ckey and (not _is_missing_current(row)):
                    method = "ckey"
                    joined_by_ckey += 1
                    # cur_anchor_hash is best-effort for trace
                    cur_anchor_hash = _v34_get_anchor_hash(cur_ma, prev_ckey)
                else:
                    # Secondary join (anchor_hash)
                    if prev_anchor_hash and prev_anchor_hash in cur_anchor_idx:
                        resolved_cur_ckey = cur_anchor_idx.get(prev_anchor_hash)
                        if resolved_cur_ckey:
                            method = "anchor_hash"
                            joined_by_anchor += 1
                            cur_anchor_hash = _v34_get_anchor_hash(cur_ma, resolved_cur_ckey) or prev_anchor_hash
                            # Capture a small sample for debug
                            if len(sample_anchor_joins) < 6:
                                sample_anchor_joins.append({
                                    "prev_ckey": prev_ckey,
                                    "resolved_cur_ckey": resolved_cur_ckey,
                                    "anchor_hash": prev_anchor_hash,
                                })
                    else:
                        resolved_cur_ckey = None
                        method = "none"
                        not_found += 1

                # Per-row diagnostics (exact requested keys)
                try:
                    row["diag"]["diff_join_trace_v1"] = {
                        "prev_ckey": prev_ckey or None,
                        "resolved_cur_ckey": resolved_cur_ckey if resolved_cur_ckey else None,
                        "method": method,
                        "prev_anchor_hash": prev_anchor_hash,
                        "cur_anchor_hash": cur_anchor_hash,
                    }
                except Exception:
                    pass

                # Current sourcing (strict)
                used_path = "none"
                cur_value_norm = None
                cur_unit_tag = None

                if method in ("ckey", "anchor_hash") and resolved_cur_ckey:
                    cm = cur_pmc.get(resolved_cur_ckey)
                    if isinstance(cm, dict) and cm:
                        # Prefer normalized values directly from canonical metric
                        cur_value_norm = cm.get("value_norm", cm.get("value", None))
                        cur_unit_tag = (cm.get("unit_tag") or cm.get("unit") or cm.get("unit_cmp") or "")
                        cur_unit_tag = _v34_safe_str(cur_unit_tag).strip() or None

                        # Update row CURRENT fields ONLY if we have something concrete
                        try:
                            if cur_value_norm is not None:
                                # build display similar to v32 helper but without inference
                                if cur_unit_tag:
                                    row["current_value"] = f"{cur_value_norm} {cur_unit_tag}".strip()
                                    row["current_unit"] = cur_unit_tag
                                    row["cur_unit_cmp"] = cur_unit_tag
                                else:
                                    row["current_value"] = _v34_safe_str(cur_value_norm)
                                row["cur_value_norm"] = cur_value_norm
                                row["current_value_norm"] = cur_value_norm
                                used_path = "primary_metrics_canonical"
                        except Exception:
                            pass

                # If still missing after strict sourcing, do NOT substitute.
                if used_path != "primary_metrics_canonical" and _is_missing_current(row):
                    # Ensure canonical not_found semantics stay as blank/N/A
                    used_path = "none"

                try:
                    row["diag"]["diff_current_source_trace_v1"] = {
                        "current_source_path_used": used_path,
                        "current_value_norm": cur_value_norm if used_path == "primary_metrics_canonical" else None,
                        "current_unit_tag": cur_unit_tag if used_path == "primary_metrics_canonical" else None,
                        "inference_disabled": True,
                    }
                except Exception:
                    pass

    except Exception:
        pass

    # Top-level debug summary (on cur_response)
    try:
        if isinstance(cur_response, dict):
            cur_response.setdefault("debug", {})
            if isinstance(cur_response.get("debug"), dict):
                cur_response["debug"]["diff_join_anchor_v34"] = {
                    "rows_total": len(metric_changes) if isinstance(metric_changes, list) else 0,
                    "joined_by_ckey": joined_by_ckey,
                    "joined_by_anchor_hash": joined_by_anchor,
                    "not_found": not_found,
                    "sample_anchor_joins": sample_anchor_joins,
                }
    except Exception:
        pass

    return metric_changes, unchanged, increased, decreased, found


# PATCH V34_WIRE (ADDITIVE): override diff_metrics_by_name entrypoint with v34 anchor join wrapper.
try:
    if callable(diff_metrics_by_name_FIX41_V34_ANCHOR_JOIN):
        diff_metrics_by_name = diff_metrics_by_name_FIX41_V34_ANCHOR_JOIN  # type: ignore
except Exception:
    pass

# PATCH V34_VERSION_BUMP (ADDITIVE)
try:
    CODE_VERSION = 'fix41afc19_evo_fix16_anchor_rebuild_override_v1_fix2b_hardwire_v34'
except Exception:
    pass

# =====================================================================
# END PATCH V34_EVOLUTION_DIFF_ANCHOR_JOIN
# =====================================================================


# =====================================================================
# PATCH V34_REBUILD_FN_ALIAS (ADDITIVE):
# Evolution compute_source_anchored_diff expects these function names:
#   - rebuild_metrics_from_snapshots_analysis_canonical_v1
#   - rebuild_metrics_from_snapshots_schema_only_fix16
# Some branches only define rebuild_metrics_from_snapshots_schema_only.
# Provide safe aliases so display-rebuild can run and populate Current values,
# without touching hashing/extraction/fastpath.
# =====================================================================

try:
    _v34_base_rebuild = globals().get("rebuild_metrics_from_snapshots_schema_only")
except Exception:
    _v34_base_rebuild = None

def rebuild_metrics_from_snapshots_schema_only_fix16(prev_response: dict, snapshot_pool: list, web_context: dict = None):  # noqa: F811
    """Alias wrapper for evolution display rebuild (FIX16 semantics live in base)."""
    try:
        fn = _v34_base_rebuild
        if callable(fn):
            try:
                return fn(prev_response, snapshot_pool, web_context=web_context)
            except TypeError:
                return fn(prev_response, snapshot_pool)
    except Exception:
        pass
    return {}

def rebuild_metrics_from_snapshots_analysis_canonical_v1(prev_response: dict, snapshot_pool: list, web_context: dict = None):  # noqa: F811
    """Alias wrapper; prefer schema-only rebuild to preserve deterministic behavior."""
    try:
        fn = globals().get("rebuild_metrics_from_snapshots_schema_only_fix16")
        if callable(fn):
            return fn(prev_response, snapshot_pool, web_context=web_context)
    except Exception:
        pass
    return {}

# PATCH V34_VERSION_BUMP (ADDITIVE)
try:
    CODE_VERSION = 'fix41afc19_evo_fix16_anchor_rebuild_override_v1_fix2b_hardwire_v34'
except Exception:
    pass

# =====================================================================
# PATCH V34C_DIFF_RESPONSE_UNWRAP (ADDITIVE)
# Purpose:
#   Evolution diff panel sometimes receives wrapper objects rather than the
#   direct payload containing primary_metrics_canonical / metric_anchors.
#   This patch adds a deterministic unwrapping shim and wires a v34c wrapper
#   that runs the existing v34 anchor-hash join logic against the unwrapped
#   payloads, while preserving strict no-fallback semantics.
# Non-negotiables:
#   - Evolution render/diff layer only
#   - No changes to fastpath, hashing, injection, snapshot attach, extraction, Analysis
# =====================================================================

def _v34c_unwrap_for_diff(resp):
    """
    Deterministically unwrap common wrapper shapes to obtain the payload dict that
    actually contains 'primary_metrics_canonical' and 'metric_anchors'.

    Returns: (payload_dict, path_str)
      - payload_dict is always a dict (may be the original dict)
      - path_str indicates the unwrap path used (for debugging only)
    """
    try:
        if not isinstance(resp, dict):
            return ({}, "non_dict")

        # If it already looks like the payload, stop.
        if isinstance(resp.get("primary_metrics_canonical"), dict) or isinstance(resp.get("metric_anchors"), dict):
            return (resp, "self")

        # Candidate unwrap paths (ordered, deterministic)
        candidates = [
            ("primary_response", ["primary_response"]),
            ("results.primary_response", ["results", "primary_response"]),
            ("results.response", ["results", "response"]),
            ("results.payload", ["results", "payload"]),
            ("payload", ["payload"]),
            ("response", ["response"]),
            ("data", ["data"]),
            ("result", ["result"]),
        ]

        for label, path in candidates:
            cur = resp
            ok = True
            for k in path:
                if isinstance(cur, dict) and (k in cur):
                    cur = cur.get(k)
                else:
                    ok = False
                    break
            if not ok:
                continue
            if isinstance(cur, dict):
                if isinstance(cur.get("primary_metrics_canonical"), dict) or isinstance(cur.get("metric_anchors"), dict):
                    return (cur, label)

        # Last resort: one-level scan for a dict that looks like a payload
        try:
            for k, v in list(resp.items()):
                if isinstance(v, dict) and (isinstance(v.get("primary_metrics_canonical"), dict) or isinstance(v.get("metric_anchors"), dict)):
                    return (v, f"scan.{k}")
        except Exception:
            pass

        return (resp, "self_no_payload_keys")
    except Exception:
        return ({}, "error")


def diff_metrics_by_name_FIX41_V34C_UNWRAP(prev_response: dict, cur_response: dict):
    """
    v34c wrapper:
      - unwrap prev/cur to the canonical payload dicts
      - run the existing v34 anchor-hash join wrapper logic against unwrapped payloads
      - write top-level debug summary onto the *outer* cur_response (and inner payload too, if different)
    """
    # Ensure we can call the v34 wrapper (wired in v34).
    if not callable(diff_metrics_by_name_FIX41_V34_ANCHOR_JOIN):
        # Fallback to base if present (keeps prior behavior)
        if callable(diff_metrics_by_name_V34_BASE):
            return diff_metrics_by_name_V34_BASE(prev_response, cur_response)
        return ([], 0, 0, 0, 0)

    prev_payload, prev_path = _v34c_unwrap_for_diff(prev_response)
    cur_payload, cur_path = _v34c_unwrap_for_diff(cur_response)

    # Run v34 join on the unwrapped payloads.
    out = diff_metrics_by_name_FIX41_V34_ANCHOR_JOIN(prev_payload, cur_payload)

    # Attach an additional small debug note on the outer response (does not alter required v34 key)
    try:
        if isinstance(cur_response, dict):
            cur_response.setdefault("debug", {})
            if isinstance(cur_response.get("debug"), dict):
                cur_response["debug"]["diff_join_anchor_v34c_unwrap"] = {
                    "prev_unwrap_path": prev_path,
                    "cur_unwrap_path": cur_path,
                    "prev_payload_has_pmc": bool(isinstance(prev_payload, dict) and isinstance(prev_payload.get("primary_metrics_canonical"), dict)),
                    "cur_payload_has_pmc": bool(isinstance(cur_payload, dict) and isinstance(cur_payload.get("primary_metrics_canonical"), dict)),
                    "prev_payload_has_metric_anchors": bool(isinstance(prev_payload, dict) and isinstance(prev_payload.get("metric_anchors"), dict)),
                    "cur_payload_has_metric_anchors": bool(isinstance(cur_payload, dict) and isinstance(cur_payload.get("metric_anchors"), dict)),
                }
    except Exception:
        pass

    # If payload is a different dict, mirror the same note for convenience (no harm if same).
    try:
        if isinstance(cur_payload, dict) and (cur_payload is not cur_response):
            cur_payload.setdefault("debug", {})
            if isinstance(cur_payload.get("debug"), dict):
                cur_payload["debug"]["diff_join_anchor_v34c_unwrap"] = {
                    "prev_unwrap_path": prev_path,
                    "cur_unwrap_path": cur_path,
                    "prev_payload_has_pmc": bool(isinstance(prev_payload, dict) and isinstance(prev_payload.get("primary_metrics_canonical"), dict)),
                    "cur_payload_has_pmc": bool(isinstance(cur_payload, dict) and isinstance(cur_payload.get("primary_metrics_canonical"), dict)),
                    "prev_payload_has_metric_anchors": bool(isinstance(prev_payload, dict) and isinstance(prev_payload.get("metric_anchors"), dict)),
                    "cur_payload_has_metric_anchors": bool(isinstance(cur_payload, dict) and isinstance(cur_payload.get("metric_anchors"), dict)),
                }
    except Exception:
        pass

    return out


# PATCH V34C_WIRE (ADDITIVE): override diff_metrics_by_name entrypoint with v34c unwrap wrapper.
try:
    if callable(diff_metrics_by_name_FIX41_V34C_UNWRAP):
        diff_metrics_by_name = diff_metrics_by_name_FIX41_V34C_UNWRAP  # type: ignore
except Exception:
    pass

# PATCH V34C_VERSION_BUMP (ADDITIVE)
try:
    CODE_VERSION = 'fix41afc19_evo_fix16_anchor_rebuild_override_v1_fix2b_hardwire_v34c'
except Exception:
    pass

# =====================================================================
# END PATCH V34C_DIFF_RESPONSE_UNWRAP
# =====================================================================


# =====================================================================
# PATCH V34F_VERSION_BUMP (ADDITIVE)
# =====================================================================
try:
    CODE_VERSION = 'fix41afc19_evo_fix16_anchor_rebuild_override_v1_fix2b_hardwire_v34f'
except Exception:
    pass

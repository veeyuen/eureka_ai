# YUREEKA AI RESEARCH ASSISTANT v7.41
# With Web Search, Evidence-Based Verification, Confidence Scoring
# SerpAPI Output with Evolution Layer Version
# Updated SerpAPI parameters for stable output
# Deterministic Output From LLM
# Deterministic Evolution Core Using Python Diff Engine
# Anchored Evolution Analysis Using JSON As Input Into Model
# Implementation of Source-Based Evolution
# Saving of JSON output Files into Google Sheets
# Canonical Metric Registry + Semantic Hashing of Findings
# Removal of Evolution Decisions from LLM
# Further Enhancements to Minimize Evolution Drift (Metric)
# Saving of Extraction Cache in JSON
# Prioritize High Quality Sources With Source Freshness Tracking
# Timestamps = Timezone Naive
# Improved Stability of Handling of Duplicate Canonicalized IDs
# Deterministic Main and Side Topic Extractor
# Range Aware Canonical Metrics
# Range + Source Attribution
# Proxy Labeler + Geo Tagging
# Improved Main Topic + Side Topic Extractor Using Deterministic-->NLP-->LLM layer
# Guardrails For Main + Side Topic Handling
# Numeric Consistency Scores
# Multi-Side Enumerations
# Dashboard Unit Presentation Fixes (Main + Evolution)
# Domain-Agnostic Question Profiling
# Baseline Caching Contains HTTP Validators + Numeric Data
# URL canonicalization
# Evolution Layer Leverage On New Analysis Pipeline to Minimise Volatility
# Canonicalization of Evolution Layer Metrics To Match Analysis Layer
# Fix URL/path Collapese Issue Causing + Tighten Evolution Extraction (Topic Gating)
# canonical-key-first matching
# Evolution Pipeline to Consume analysis upstream artifacts
# safety-net hard gates (minimal) before matching
# Tighten canonical identity + unit-family constraints
# Fingerprint freshness gating to evolution
# Fix SerpAPI access and fetching
# Keeps your snapshot-friendly scraped_meta (with extracted numbers + fingerprint fields)
# Safe fallback scraper when ScrapingDog is unavailable
# Prevent caching “empty results” from SerpAPI (no poisoned cache)
# Restoration of Range Estimates For Metrics
# Improved Junk Tagging and Rejection
# One Canononical Operator for Analysis + Evolution Layers
# Metric Aware Range Construction Everywhere
# Anchor Matching Correctness
# Unit Measure + Attribute Association e.g. M + units (sold)
# Enriched metric_schema_frozen (analysis side)
# THIS VERSION HAS THE PLUMBING LOCKED-DOWN
# ONLY THE METRIC EXTRACTION LAYER FOR EVOLUTION REQUIRES WORK

import io
import os
import re
import json
import requests
import pandas as pd
import plotly.express as px
import streamlit as st


# REFACTOR108: Shared Sheets read-cache (used by sheets_get_all_values_cached). Keep TTL short to prevent stale baseline selection.
_SHEETS_READ_CACHE = {}
_SHEETS_READ_CACHE_TTL_SEC = 45
_SHEETS_LAST_READ_ERROR = None


_YUREEKA_SHEETS_SCOPES_DEBUG_V1 = {}  # REFACTOR100

def _coerce_google_oauth_scopes(scopes) -> list:
    """Return a de-duplicated list of string OAuth scopes (drops non-strings).

    This is a defensive guard for Google Sheets auth. If any non-string value
    (e.g., a dict) contaminates the scopes list, Google auth may crash with:
      'sequence item N: expected str instance, dict found'
    """
    raw = []
    try:
        raw = list(scopes) if scopes is not None else []
    except Exception:
        raw = []

    out = []
    dropped = 0
    for s in raw:
        if isinstance(s, str):
            out.append(s)
        else:
            dropped += 1

    # de-dup while preserving order
    seen = set()
    deduped = []
    for s in out:
        if s in seen:
            continue
        seen.add(s)
        deduped.append(s)

    if not deduped:
        # sensible fallback
        deduped = [
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive",
        ]

        # REFACTOR100: capture a structured debug block once when scopes are contaminated.
    try:
        if dropped and (not _YUREEKA_SHEETS_SCOPES_DEBUG_V1.get('sheets_scopes_v1')):
            _YUREEKA_SHEETS_SCOPES_DEBUG_V1['sheets_scopes_v1'] = {
                'raw_types': [type(x).__name__ for x in (raw or [])],
                'sanitized': list(deduped or []),
            }
            try:
                st.session_state.setdefault('debug', {})
                st.session_state['debug']['sheets_scopes_v1'] = dict(_YUREEKA_SHEETS_SCOPES_DEBUG_V1['sheets_scopes_v1'])
            except Exception:
                pass
    except Exception:
        pass

    # warn once per session (non-fatal)
    try:
        if dropped:
            _k = "_coerce_google_oauth_scopes_warned_v1"
            if hasattr(st, "session_state") and not st.session_state.get(_k):
                st.session_state[_k] = True
                st.warning(f"⚠️ Dropped {dropped} invalid Google OAuth scope(s) from configuration.")
    except Exception:
        pass

    return deduped



# Enabled only if caller sets web_context["enable_spine_shadow_fix2d64"]=True or env ENABLE_SPINE_SHADOW_FIX2D64=1.
try:
    import canonical_identity_spine as _canonical_identity_spine_fix2d64
except Exception:
    pass
    _canonical_identity_spine_fix2d64 = None
import base64
import hashlib
import numpy as np
import difflib
import gspread
from pypdf import PdfReader
from pathlib import Path
from google.oauth2.service_account import Credentials
from dataclasses import dataclass
from typing import Dict, List, Optional, Any, Union, Tuple
from sentence_transformers import SentenceTransformer, util
from transformers import pipeline
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup
from collections import Counter
from pydantic import BaseModel, Field, ValidationError, ConfigDict

# VERSION STAMP (LOCKED)
# REFACTOR12: single-source-of-truth version lock.
# - All JSON outputs must stamp using _yureeka_get_code_version().
# - The getter is intentionally "frozen" via a default arg to prevent late overrides.
_YUREEKA_CODE_VERSION_LOCK = "REFACTOR124"
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK

# REFACTOR111 escape hatch + selector gate
DISABLE_FASTPATH_FOR_NOW = True
FORCE_LATEST_PREV_SNAPSHOT_V1 = True


# - Downsizing step 1: remove accumulated per-patch try/append scaffolding.
# - Registers a canonical entries list idempotently at import time.

_PATCH_TRACKER_CANONICAL_ENTRIES_V1 = [{'patch_id': 'REFACTOR124', 'date': '2026-02-05', 'summary': 'Fix diffing regression: preserve Analysis wrapper fields (question/timestamp/profile) when attach_source_snapshots_to_analysis triggers schema-only rebuild by merging rebuilt PMC/schema into the wrapper rather than replacing it. Harden HistoryFull baseline picker so newest snapshots with missing question remain eligible if they contain any non-null PMC values, preventing fallback to older all-null baselines. Adds debug beacons analysis_wrapper_shape_v1 and prev_snapshot_match_mode_v1. No schema/key grammar changes; strict comparability preserved; Streamlit-safe.', 'files': ['REFACTOR124.py'], 'supersedes': ['REFACTOR123']}, {'patch_id': 'REFACTOR123', 'date': '2026-02-04', 'summary': "Fix diffing by seeding + schema-only rebuild for Analysis baseline (so previous_value is non-null). Improve diff change_type to treat null baseline/current as missing_baseline/missing_current, and restore diff-row source_url fallback.", 'files': ['REFACTOR123.py'], 'supersedes': ['REFACTOR122'], 'acceptance_notes': "Analysis baseline should yield non-null schema values (>=2/4). Evolution should compute deltas with previous_value populated. metric_changes rows should include source_url."}, {'patch_id': 'REFACTOR122', 'date': '2026-02-04', 'summary': "Restore missing helper _fix17_candidate_allowed_with_reason used by rebuild_metrics_from_snapshots_analysis_canonical_v1. Fixes NameError that prevents post-seed canonical rebuild (fix41afc19) from running, blocking prod schema values.", 'files': ['REFACTOR122.py'], 'supersedes': ['REFACTOR121'], 'acceptance_notes': "Expect results.debug.fix41afc19.applied==True and rebuilt_count>0 in prod evolution. At least 2/4 schema keys should be non-null in prod evolution. Year-anchor top3 should include seeded sources."}, {'patch_id': 'REFACTOR121', 'date': '2026-02-04', 'summary': 'Restore year-anchor backstop by storing lightweight snapshot_text_excerpt in baseline_sources_cache (incl. seeds) and indexing it for year-token detection; also broaden candidate context used in year-anchor scan. Adds debug beacons snapshot_excerpt_coverage_v1 + year_anchor_page_text_v1. No schema/key grammar changes; Streamlit-safe.', 'files': ['REFACTOR121.py'], 'supersedes': ['REFACTOR120'], 'acceptance_notes': 'Expect snapshot_excerpt_coverage_v1.sources_with_excerpt > 0 and year_anchor_page_text_v1.indexed_text_sources > 0. Year-anchor provenance for investment_2040 should improve because required year tokens can be detected anywhere on the page via excerpt backstop, while preserving Δt gating, PDF skip, and fast-path behavior.'},{'patch_id': 'REFACTOR120', 'date': '2026-02-04', 'summary': 'Fix schema-seed numeric extraction by cleaning HTML to visible text in fetch_url_content_with_status before downstream extraction; this prevents HTML-heavy pages from yielding zero extracted_numbers and unlocks seeded candidates (incl. $/bn/trn figures) for year-anchor gating and schema selection. Also bumps code version + adds tracker entry. No schema/key grammar changes; Streamlit-safe.', 'files': ['REFACTOR120.py'], 'supersedes': ['REFACTOR119'], 'acceptance_notes': 'Expect schema_seed_extract_v1 to show seeds_extracted_nonzero > 0; baseline_sources_cache seeded entries should have numbers_found > 0 (at least some). Production should ideally fill charging_investment_2040 from WoodMac/Electrek/PV Mag seeds while preserving Δt gating, PDF skip, and fast-path behavior.'},
{'patch_id': 'REFACTOR119', 'date': '2026-02-04', 'summary': 'Make schema-seeded production sources actually extract numeric candidates by running extract_numbers_with_context on fetched snapshot_text and storing extracted_numbers/numbers_found (+ fingerprint) in baseline_sources_cache; add debug beacons schema_seed_extract_v1 and year_anchor_seed_coverage_v1 to confirm seed extraction and year-anchor gating coverage. No schema/key grammar changes; strict comparability preserved; Streamlit-safe.', 'files': ['REFACTOR119.py'], 'supersedes': ['REFACTOR118']}, {'patch_id': 'REFACTOR118', 'date': '2026-02-03', 'summary': 'Fix remaining REFACTOR117 gaps: guard optional _fix17_metric_is_year_like to prevent rebuild NameError; repair REFACTOR100 year-anchor page-text indexing bug (local normalizer used before definition) so year gating can see required tokens in snapshot_text; strengthen investment(2040) binding via schema-key-specific keyword synonym bridge; and dedupe year-anchor top3 debug summaries by URL. Schema/key grammar unchanged; strict comparability preserved; Streamlit-safe.', 'files': ['REFACTOR118.py'], 'supersedes': ['REFACTOR117']}, {'patch_id': 'REFACTOR117', 'date': '2026-02-03', 'summary': 'Fix Evolution Δt definitively by deriving the effective analysis timestamp from the selected baseline snapshot (wrapper.previous_timestamp / debug.prev_snapshot_pick_v1) and applying post-hoc run_timing + per-row deltas on the wrapper object. Also ensure DISABLE_FASTPATH_FOR_NOW actually disables FIX31 fast-path reuse, expand schema seed sources to include explicit investment-by-2040 coverage, and add investment-key keyword synonyms to improve candidate binding. Schema/key grammar unchanged; strict comparability preserved; Streamlit-safe.', 'files': ['REFACTOR117.py'], 'supersedes': ['REFACTOR116']}, {'patch_id': 'REFACTOR116', 'date': '2026-02-03', 'summary': 'Fix Evolution Δt to use the effective baseline snapshot timestamp actually selected (prev_snapshot_pick_v1/previous_timestamp), and restore per-row analysis_evolution_delta_* emission with injection-source suppression based on a robust injected URL set. Adds debug beacons run_timing_effective_v3 and row_delta_gating_v3. No schema/key grammar changes; strict comparability preserved; Streamlit-safe.', 'files': ['REFACTOR116.py'], 'supersedes': ['REFACTOR115']}, {'patch_id': 'REFACTOR115', 'date': '2026-02-03', 'summary': 'Fix Evolution Δt to use the selected baseline snapshot timestamp (effective timestamp), restore row-level injection Δt suppression using an injected URL set, add deterministic schema-seeded production sources to stabilize year-anchor gating, downgrade missing PDF dependencies to skipped (non-noisy), and gate FIX31 fast-path to avoid masking progress during triad validation/injection/null-schema scenarios. Schema/key grammar unchanged; strict comparability preserved; Streamlit-safe.', 'files': ['REFACTOR115.py'], 'supersedes': ['REFACTOR114']}, {'patch_id': 'REFACTOR25', 'date': '2026-01-24', 'summary': 'Add production-only Analysis→Evolution run delta column for metric changes. Standardize top-level timestamps to UTC (+00:00), compute/stamp run_timing_v1 (delta_seconds/human) in Evolution results, and gate per-row delta display when current metric is sourced from injected URLs.', 'files': ['REFACTOR25_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR24']}, {'patch_id': 'REFACTOR26', 'date': '2026-01-24', 'summary': 'Tighten and centralize current metric source_url attribution for reliable row-level injection gating. Add a hydrator that fills primary_metrics_canonical[*].source_url from evidence/provenance, expose current_source_url fields on diff rows, and enhance per-row injection detection to prefer row-attributed URLs before falling back to canonical maps.', 'files': ['REFACTOR26_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR25']}, {'patch_id': 'REFACTOR27', 'date': '2026-01-24', 'summary': "Harden unit comparability and candidate eligibility for currency metrics. Reject date-fragment currency candidates (e.g., 'July 01, 2025') during schema-only rebuild, and strengthen currency unit mismatch detection so mixed scale/code representations do not emit nonsense deltas. Also expose current_source_url on diff rows for clearer row-level injection attribution.", 'files': ['REFACTOR27_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR26']}, {'patch_id': 'REFACTOR28', 'date': '2026-01-24', 'summary': "Consolidate schema-only rebuild authority to eliminate stale wrapper capture chains and ensure REFACTOR27 candidate filters (especially currency date-fragment rejection) are active at runtime. This prevents day-of-month tokens like '01' from binding as currency values, restoring comparable currency baselines while preserving percent-year poisoning sanitization.", 'files': ['REFACTOR28_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR27']}, {'patch_id': 'REFACTOR29', 'date': '2026-01-24', 'summary': 'Refine REFACTOR25 run-delta harness and diagnostics: replace overly-strict global injection assertion with per-row gating stats (injected rows must have blank delta, production rows should show delta when available). Persist row_delta_gating_v1 into run_timing_v1 for easier debugging, without changing schema/key grammar or diff behavior.', 'files': ['REFACTOR29_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR28']}, {'patch_id': 'REFACTOR30', 'date': '2026-01-24', 'summary': 'Fix REFACTOR29 run_timing_v1 row_delta_gating_v1 double-counting: apply per-row Analysis→Evolution delta stamping once (primary metric_changes list) and propagate to metric_changes_v2 by canonical_key, so diagnostic counts match the displayed table while keeping injection gating behavior unchanged.', 'files': ['REFACTOR30_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR29']}, {'patch_id': 'REFACTOR31', 'date': '2026-01-24', 'summary': 'Add runtime_identity_v1 stamp (code_version lock + __file__ + SHA1) to Analysis/Evolution debug for diagnosing stale-version runs; and harden run_timing_v1 row_delta_gating_v1 stats to count unique canonical_keys only (prevents double-counting when both metric_changes and metric_changes_v2 exist). No schema/key-grammar changes.', 'files': ['REFACTOR31_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR30']}, {'patch_id': 'REFACTOR32', 'date': '2026-01-24', 'summary': "Clarify injected URL semantics: treat only UI-provided extra_urls_ui(_raw) (or explicit internal marker) as injected for debug + run-delta gating, preventing production source URLs from being misclassified as injected. Add __yureeka_extra_urls_are_injection_v1 markers when Evolution wires injected URLs into web_context['extra_urls']. No schema/key-grammar changes.", 'files': ['REFACTOR32_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR31']}, {'patch_id': 'REFACTOR33', 'date': '2026-01-24', 'summary': 'Downsize footprint by deleting shadowed duplicate top-level function definitions and redundant metric_changes_legacy preservation block, keeping only the final authoritative implementations. No schema/key-grammar changes.', 'files': ['REFACTOR33_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR32']}, {'patch_id': 'REFACTOR34', 'date': '2026-01-24', 'summary': 'Fix a missing return in rebuild_metrics_from_snapshots_schema_only_fix17 that caused schema-only rebuilds to return None, breaking Analysis primary_metrics_canonical persistence and Evolution diffing after REFACTOR33 deletions. No schema/key-grammar changes.', 'files': ['REFACTOR34_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR33']}, {'patch_id': 'REFACTOR24', 'date': '2026-01-23', 'summary': 'Fix REFACTOR23 syntax regression (mis-indented try block) and make currency-aware unit_cmp construction consistent across all get_canonical_value_and_unit() definitions (USD:B, EUR:B, etc.) so cross-currency deltas are vetoed deterministically.', 'files': ['REFACTOR24_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR23']}, {'patch_id': 'REFACTOR23', 'date': '2026-01-23', 'summary': 'Unit consistency hardening: carry currency_code through candidate canonicalization & schema-only rebuild; include currency_code in diff unit_cmp token for currency metrics (detect USD vs EUR rather than silently comparing); and fix a small anchor-rebuild NameError to keep anchor path safe.', 'files': ['REFACTOR23_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR22']}, {'patch_id': 'REFACTOR22', 'date': '2026-01-23', 'summary': "Fix unit-family noise for yearlike tokens: normalize_unit_family() no longer infers percent/currency/magnitude from surrounding context when unit_tag is empty and raw token is a plain 4-digit year (1900–2100). This reduces unit inconsistencies in baseline_sources_cache and prevents misleading 'percent_tag' traces on year/range endpoints, without changing canonical binding or diff behavior.", 'files': ['REFACTOR23_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR21']}, {'patch_id': 'REFACTOR21', 'date': '2026-01-23', 'summary': "Harden unit inference against year/range artifacts: mark 4-digit year tokens as junk (year_token) when unitless, prevent context-driven unit backfill for yearlike candidates, and tag negative endpoints produced by hyphen ranges (e.g., '151-300' -> '-300') as junk (hyphen_range_negative_endpoint). Reduces unit inconsistencies and percent/currency 'poisoning' from nearby context.", 'files': ['REFACTOR21_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR20']}, {'patch_id': 'REFACTOR20', 'date': '2026-01-23', 'summary': "Fix false currency evidence hits caused by substring matches (e.g., 'eur'/'euro' inside 'Europe'). Introduce boundary-aware currency detection helper and use it in infer_unit_tag_from_context() and normalize_unit_family(), preventing 'million units' candidates from being misclassified as currency in Europe contexts.", 'files': ['REFACTOR20_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR19']}, {'patch_id': 'REFACTOR19', 'date': '2026-01-23', 'summary': 'Restore Analysis/Evolution parity for schema_only_rebuild outputs by including unit_tag, unit_family, base_unit, and multiplier_to_base (plus raw) on rebuilt primary_metrics_canonical entries. This keeps diff comparability deterministic and prevents downstream re-parsing of current values.', 'files': ['REFACTOR19_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR18']}, {'patch_id': 'REFACTOR18', 'date': '2026-01-23', 'summary': 'Harden authoritative diff binding signal: ensure diff_metrics_by_name always carries a reliable __YUREEKA_AUTHORITATIVE_BINDING__ tag (with globals fallback when callable wrappers reject setattr). Make binding_manifest_v1 self-contained (use local bound_from values) and update harness report IDs to the current refactor version for cleaner diagnostics.', 'files': ['REFACTOR18_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR17']}, {'patch_id': 'REFACTOR17', 'date': '2026-01-23', 'summary': 'Add concise in-file debug playbook and surface an authoritative binding manifest for Diff Panel V2 entrypoint (plus legacy diff_metrics_by_name when available). Improves manifest resilience when Streamlit triggers execution before later defs.', 'files': ['REFACTOR17_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR16']}, {'patch_id': 'REFACTOR16', 'date': '2026-01-23', 'summary': 'Hard-lock CODE_VERSION to REFACTOR16 across legacy override sites; expand FINAL BINDINGS candidate set to include _yureeka_diff_metrics_by_name_v24; make binding_manifest_v1 resolve/report the actual diff entrypoint even when Streamlit executes before later diff wrapper defs.', 'files': ['REFACTOR16_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR15']}, {'patch_id': 'REFACTOR02', 'date': '2026-01-21', 'summary': 'Add refactor regression harness (Analysis→Evolution) gated by explicit flag; emits JSON report + asserts diff invariants (both_count > 0, no prev-metrics sentinel, percent-year token rule).', 'files': ['REFACTOR02_full_codebase_streamlit_safe.py'], 'supersedes': ['FIX2D86']}, {'patch_id': 'REFACTOR03', 'date': '2026-01-21', 'summary': 'Fix REFACTOR02 regression: enforce unit-family + scale eligibility in schema-only rebuild; detect unit mismatch in diff panel and mark as unit_mismatch (avoid bogus B vs M diffs).', 'files': ['REFACTOR03_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR02']}, {'patch_id': 'REFACTOR10', 'date': '2026-01-21', 'summary': 'Fix false unit_mismatch caused by context_snippet percent leakage; enrich schema-anchored rebuilt PMC with unit_tag/unit_family/multiplier_to_base; tighten unit-family evidence checks to prefer token/raw evidence over broad context for magnitude keys; update refactor harness labels to REFACTOR04.', 'files': ['REFACTOR04_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR03']}, {'patch_id': 'REFACTOR05', 'date': '2026-01-21', 'summary': 'Selection gating + harness fix: block currency/percent candidates from __unit_* keys, promote raw/unit metadata into PMC for parity, and make harness read evidence lists.', 'files': ['REFACTOR05_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR04']}, {'patch_id': 'REFACTOR07', 'date': '2026-01-22', 'summary': 'Freeze versioning as single-source-of-truth using a refactor version lock; ensure JSON outputs use the locked version; add binding_manifest_v1 and harness assertions for version + authoritative diff binding; update FINAL BINDINGS and harness report labels to REFACTOR07.', 'files': ['REFACTOR07_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR06']}, {'patch_id': 'REFACTOR08', 'date': '2026-01-22', 'summary': 'Enhance refactor regression harness with consistent REFACTOR08 labels/versioning, dynamic authoritative diff binding expectation, and summary consistency checks (rows_total/partition/found/not_found/key_overlap). Update FINAL BINDINGS tag and locked CODE_VERSION to REFACTOR08.', 'files': ['REFACTOR08_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR07']}, {'patch_id': 'REFACTOR06', 'date': '2026-01-22', 'summary': 'Freeze authoritative runtime bindings: add FINAL BINDINGS section for diff_metrics_by_name, tag authoritative function for harness verification, and update harness report/version labels.', 'files': ['REFACTOR06_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR05']}, {'patch_id': 'REFACTOR46', 'date': '2026-01-25', 'summary': 'Prevent refactor harness from terminating Streamlit runtime (disable harness under Streamlit; double-guard EOF harness dispatch).', 'files': ['REFACTOR46_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR45']}, {'patch_id': 'FIX2D71', 'date': '2026-01-19', 'summary': 'Commit schema-keyed baseline canonical metrics during Analysis: if schema authority selection yields no winners but baseline_schema_metrics_v1 is non-empty, promote that schema-keyed (auditable proxy) map into primary_metrics_canonical so Evolution has prev canonical metrics for metric_changes_v2 diffing.', 'files': ['FIX2D71_full_codebase.py'], 'supersedes': ['FIX2D70']}, {'patch_id': 'FIX2D70', 'date': '2026-01-19', 'summary': 'Controlled schema-candidate reconciliation during schema-anchored rebuild: relax key-year matching (±1 for single-year keys, overlap for ranges) and keyword gating only when strict prefilter yields zero candidates, while emitting FIX2D70 rejection counts and relax flags for audit. This closes the last-mile binding gap without reintroducing heuristic matching.', 'files': ['FIX2D70_full_codebase.py'], 'supersedes': ['FIX2D69B']}, {'patch_id': 'FIX2D69', 'date': '2026-01-19', 'summary': 'Hard-wire numeric extraction on injected snapshot_text: when injected placeholders are fetched (FIX41AFC16), convert HTML to plain text if needed, always extract numbers from the non-empty text, and store content_len/clean_text_len plus FIX2D68 extraction diagnostics and errors. Also defensively initialize observed_rows_filtered_noninjected to prevent UnboundLocalError in Diff Panel V2 summary.', 'files': ['FIX2D69_full_codebase.py'], 'supersedes': ['FIX2D68']}, {'patch_id': 'FIX2D66G', 'date': '2026-01-19', 'summary': 'Google Sheets write resiliency: always mirror saved analyses into session_state history; if Sheets write fails, set a flag and record _SHEETS_LAST_WRITE_ERROR so get_history() falls back to session_state when Sheet reads are empty. Prevents Evolution from being blocked by transient Sheets save failures. No changes to extraction/diffing.', 'files': ['FIX2D66G_full_codebase.py'], 'supersedes': ['FIX2D66']}, {'patch_id': 'FIX2D66', 'date': '2026-01-19', 'summary': 'Deterministic injected-URL admission: promote UI raw/diag injection fields into web_context.extra_urls and synthesize diag_injected_urls when missing, so inj_diag/inj_trace_v1 reliably reflect injected URLs in snapshot pool and hash inputs (auditable). No UI/diff changes.', 'files': ['FIX2D66_full_codebase.py'], 'supersedes': ['FIX2D65D']}, {'patch_id': 'FIX2D65A', 'date': '2026-01-19', 'summary': "Hotfix for FIX2D65: repair syntax-corrupted duplicate selector block; make yearlike prune non-fatal (never empties pool); make 'rebuild empty with snapshots' non-fatal so Evolution can still emit JSON diagnostics.", 'files': ['FIX2D65A_full_codebase.py'], 'supersedes': ['FIX2D65']}, {'patch_id': 'FIX2D44', 'date': '2026-01-17', 'summary': 'Fix Analysis baseline schema baseline materialisation: define _core in attach_source_snapshots_to_analysis so FIX2D31/FIX2D38 baseline_schema_metrics_v1 builder executes; emit results.baseline_schema_metrics_v1 for Evolution diff join.', 'files': ['FIX2D44.py'], 'supersedes': ['FIX2D43']}, {'patch_id': 'FIX2D59', 'summary': 'Canonical identity resolver v1: define identity tuple + schema-first resolver; route canonical key minting through resolver to align Analysis and Evolution key authority.'}, {'patch_id': 'FIX2D60', 'summary': 'Enforce schema-only canonical store (Analysis) and hard-reject bare-year candidates for unit/count keys at schema_only_rebuild commit point (Evolution).'}, {'patch_id': 'FIX2D61', 'summary': 'Option A schema extension: generate promotion proposals from primary_metrics_provisional and (optionally) promote them into metric_schema_frozen with full audit metadata; enables closing remaining coverage gaps without reintroducing heuristic canonical minting.'}, {'patch_id': 'FIX2D62', 'summary': 'Normalize time tokens into identity tuple (year/YTD/forecast) + schema-first resolver uses metric_token+time_scope to match schema; prevents 2024/2025 contamination of metric_token and improves Analysis/Evolution convergence.'}, {'patch_id': 'FIX2D63', 'summary': 'Harden schema_only_rebuild_fix17 against injected-year pollution for unit/count metrics: fix _c variable typo in FIX2D2U gate and reject yearlike candidates without unit evidence upstream.'}, {'patch_id': 'FIX2D64', 'summary': 'Add Canonical Identity Spine V1 module (shadow mode only) + minimal regressions: centralizes identity tuple, schema-first resolver contract, and value selection with yearlike rejection immune to context unit backfill.', 'files': ['canonical_identity_spine.py', 'FIX2D64_full_codebase.py'], 'supersedes': ['FIX2D63']}, {'patch_id': 'FIX2D65', 'date': '2026-01-19', 'summary': 'Authority takeover: make Canonical Identity Spine V1 the only key-resolution authority (Analysis+Evolution) and add hard gates; prune yearlike candidates for unit/count metrics immune to context unit backfill.', 'files': ['canonical_identity_spine.py', 'FIX2D65_full_codebase.py'], 'supersedes': ['FIX2D64']}, {'patch_id': 'FIX2D40', 'summary': 'Analysis: when schema is frozen, remap best-fit baseline metrics from generic canonical keys onto schema canonical keys (one-to-one) to enable baseline diffing; stamps explicit schema_remap audit fields; retains FIX2D39 hard unit/dimension rejection.', 'files': ['FIX2D40.py'], 'supersedes': ['FIX2D39']}, {'patch_id': 'FIX2D32', 'date': '2026-01-17', 'summary': 'Diff Panel V2: treat anchor_hash mismatches as still diffable when canonical_key + unit-family gates pass; stamp row-level anchor_mismatch_diffable_v1 diagnostics and count such joins for audit.', 'files': ['FIX2D32.py'], 'supersedes': ['FIX2D31']}, {'patch_id': 'FIX2D33', 'date': '2026-01-17', 'summary': 'Analysis schema-primary rebuild: baseline commitment for schema keys by backfilling missing value_norm from raw/value via deterministic parser when selector chooses a candidate but value_norm is None; improves baseline comparability without weakening semantic gates.', 'files': ['FIX2D33.py'], 'supersedes': ['FIX2D32']}, {'patch_id': 'FIX2D25', 'date': '2026-01-16', 'summary': 'Re-enable Analysis→Evolution diffing by adding deterministic, unit-family-guarded inference for baseline keys in Diff Panel V2 when ckey/anchor joins miss; keep FIX2D20/FIX2D24 tracing and yearlike current blocking.', 'files': ['FIX2D25.py'], 'supersedes': ['FIX2D23']}, {'patch_id': 'FIX2D2D', 'date': '2026-01-16', 'summary': 'Fix Diff Panel V2 crash (prev_v/cur_v NameError) by using correctly scoped norm variables in traces; simplify end-of-file version stamping to a single final override.', 'files': ['FIX2D2D.py'], 'supersedes': ['FIX2D2C']}, {'patch_id': 'FIX2D2E', 'date': '2026-01-16', 'summary': 'Make Diff Panel V2 binding inference authoritative in the active FIX2J override path by committing inferred current_value/current_value_norm/current_source/current_method when joins miss; add explicit inference_commit trace; keep FIX2D24 year blocking and unit-first eligibility.', 'files': ['FIX2D2E.py'], 'supersedes': ['FIX2D2D']}, {'patch_id': 'FIX2D2I', 'date': '2026-01-16', 'summary': 'Enable binding inference fallback when a joined current value is blocked as unitless yearlike; add unit-family backfill for extracted_numbers pool candidates and trace the backfill/override in Diff Panel V2 __rows.', 'files': ['FIX2D2I.py'], 'supersedes': ['FIX2D2G']}, {'patch_id': 'FIX2D2J', 'date': '2026-01-16', 'summary': 'Deterministic unit/measure classifier for extracted numeric candidates: backfill unit_family from unit_tag and currency evidence in context; correct measure_kind/measure_assoc for currency-like candidates; attach classifier trace fields.', 'files': ['FIX2D2J.py'], 'supersedes': ['FIX2D2I']}, {'patch_id': 'FIX2D2K', 'date': '2026-01-16', 'summary': 'Context-driven unit backfill when unit_tag is empty, plus unit_family/measure_kind corrections trace (context_unit_backfill_v1).'}, {'patch_id': 'FIX2D2Z', 'date': '2026-01-17', 'summary': 'Make injected candidates first-class for Diff Panel inference by unwrapping injected scraped_meta into extracted_numbers pools; enforce hard unit-family rejection (percent/currency/units/magnitude) in fallback inference scoring to prevent leakage.', 'files': ['FIX2D2Z.py'], 'supersedes': ['FIX2D2Y']}, {'patch_id': 'FIX2D30', 'date': '2026-01-17', 'summary': "Contextual unit-family correction: prevent magnitude tags (e.g., M/million) in 'million units / units sold / chargers / vehicles' contexts from being misclassified as currency; remove keyword-only currency upgrades to enable clean baseline comparability without weakening hard unit-family rejection.", 'files': ['FIX2D30.py'], 'supersedes': ['FIX2D2Z']}, {'patch_id': 'FIX2D31', 'date': '2026-01-17', 'summary': 'Option A schema authority: when metric_schema_frozen is present in Analysis, rebuild primary_metrics_canonical by running the authoritative Analysis selector (_analysis_canonical_final_selector_v1) constrained to the frozen schema keys. This makes Analysis emit schema-aligned baseline metrics so Evolution injection can overlap and Diff Panel V2 can activate without weakening semantics.', 'files': ['FIX2D31.py'], 'supersedes': ['FIX2D30']}, {'patch_id': 'FIX2D2U', 'date': '2026-01-17', 'summary': 'Introduce shared semantic eligibility gate (Analysis parity) using local context_snippet; enforce it in Evolution schema-only rebuild(s) and Analysis selector to prevent cross-metric pollution (e.g., China sales value mapping into chargers 2040).', 'files': ['FIX2D2U.py'], 'supersedes': ['FIX2D2T']}, {'patch_id': 'FIX2D26', 'date': '2026-01-16', 'summary': 'Unit-first, context-bound inference candidate picker for Diff Panel V2 (Analysis→Evolution). Prefers percent/units/currency matches with keyword binding; rejects bare-year tokens pre-score; adds per-row trace counters.', 'files': ['FIX2D26.py'], 'supersedes': ['FIX2D25']}, {'patch_id': 'FIX2D28', 'date': '2026-01-16', 'summary': 'Close Diff Panel V2 binding gap: when inference selects a current value, commit it into UI/diff-read fields (current_value, current_value_norm, current_source, current_method) and mark baseline_is_comparable once all guards pass.', 'files': ['FIX2D28.py'], 'supersedes': ['FIX2D27']}, {'patch_id': 'FIX2D29', 'date': '2026-01-16', 'summary': 'Fix FIX2D28 insertion placement and complete write-through: commit inference/joined current values into metric_changes fields used by UI/diff (current_value, current_value_norm, current_source, current_method) and set baseline_is_comparable when numeric.', 'files': ['FIX2D29.py'], 'supersedes': ['FIX2D28']}, {'patch_id': 'FIX2D2A', 'date': '2026-01-16', 'summary': 'Enable guarded inference in Diff Panel V2 regardless of join mode; add explicit inference gate + attempted traces so binding inference can commit current values.', 'files': ['FIX2D2A.py']}, {'patch_id': 'FIX2D2B', 'date': '2026-01-16', 'summary': 'Correct version stamping for FIX2D2A runtime by bumping CODE_VERSION and adding final end-of-file override to prevent legacy late assignments from masking patch id.', 'files': ['FIX2D2B.py'], 'supersedes': ['FIX2D2A']}, {'patch_id': 'FIX2D2C', 'date': '2026-01-16', 'summary': 'Fix Diff Panel V2 NameError by defining guarded inference gate in the active builder (build_diff_metrics_panel_v2) and emitting explicit inference gate trace; no heuristic changes.', 'files': ['FIX2D2C.py'], 'supersedes': ['FIX2D2B']}, {'patch_id': 'FIX2D18', 'date': '2026-01-15', 'summary': 'Re-enable schema-only rebuild eligibility gates (domain token + unit-family) and strengthen unit-sales expectations to prevent bare-year (e.g., 2030) contamination; improves baseline comparables for Analysis→Evolution diffing.', 'files': ['FIX2D18.py']}, {'patch_id': 'FIX2D19', 'date': '2026-01-16', 'summary': 'Harden schema_only_rebuild_fix17 with required domain-token binding (prevents generic keyword matches) and add deterministic baseline soft-match fallback in Diff Panel V2 to enable Analysis→Evolution comparable diffs when strict joins fail.', 'files': ['FIX2D19.py']}, {'patch_id': 'FIX2D20', 'date': '2026-01-15', 'summary': 'Diagnostic-first trace: record every year-like (1900-2100) value committed to primary_metrics_canonical, including callsite tags and metric object metadata; also disable FIX2D18/FIX2D19 logic while tracing.', 'files': ['FIX2D20.py']}, {'patch_id': 'FIX2D24', 'date': '2026-01-16', 'summary': 'Last-mile guard for dashboard Current: block unitless year-like values (1900-2100, including 2030.0) from metric_changes hydration for non-year metrics; keep FIX2D20 tracing; supersedes FIX2D23 observed-only filter.', 'files': ['FIX2D24.py']}, {'patch_id': 'FIX2D21', 'date': '2026-01-16', 'summary': 'Evolution baseline-key schema: derive metric_schema_frozen from Analysis primary_metrics_canonical keys, and fix bare-year detection to reject tokens like 2030.0/2024.0; keep FIX2D20 year-commit tracing for verification.', 'files': ['FIX2D21.py']}, {'patch_id': 'FIX2D22', 'date': '2026-01-16', 'summary': 'Schema-only rebuild: enforce *eligibility-before-scoring* (hard reject bare-year tokens incl 2024/2030 and require unit-family + required domain tokens) so years cannot win; supersedes FIX2D21 selector hardening but retains baseline-key schema derivation.', 'files': ['FIX2D22.py'], 'supersedes': ['FIX2D21']}, {'patch_id': 'FIX2D11c', 'date': '2026-01-15', 'summary': 'Fix indentation/scope of FIX2D11 render fallback by ensuring it remains inside compute_source_anchored_diff (4-space indent) to avoid parse-time try/indent errors.', 'files': ['FIX2D11c.py']}, {'patch_id': 'FIX2D12', 'date': '2026-01-15', 'summary': 'Fix Diff Panel V2 premature return that prevented row emission; ensure UNION mode can emit current-only rows (added) and populate Current column.', 'files': ['FIX2D12.py']}, {'patch_id': 'FIX2D15', 'date': '2026-01-15', 'summary': 'Reject bare-year tokens (e.g., 2030) during schema-only rebuild for non-year metrics; add diagnostics to prevent year pollution and restore stable baseline comparables.', 'files': ['FIX2D15.py']}, {'patch_id': 'FIX2D16', 'date': '2026-01-15', 'summary': 'Add soft-match fallback to fill current values for baseline rows when anchor/ckey join fails; add last-mile bare-year reject for schema-only rebuild promotions. Disable FIX2D15 gating.', 'files': ['FIX2D16.py']}, {'patch_id': 'FIX2D17', 'date': '2026-01-15', 'summary': 'Harden canonical selector: reject bare-year tokens as metric values and require domain keyword overlap to prevent cross-metric pollution; deprecate FIX2D16 soft-match/year guards.', 'files': ['FIX2D17.py']}, {'patch_id': 'FIX2D13', 'date': '2026-01-15', 'summary': 'Add baseline-focused diff semantics to Diff Panel V2: classify rows as comparable/added/not_found and emit baseline delta fields + summary counters without requiring injected URLs in Analysis.', 'files': ['FIX2D13.py']}, {'patch_id': 'FIX2D11b', 'date': '2026-01-15', 'summary': 'Syntax-safe render-gate fallback: union-mode unanchored canonical_for_render without introducing nested try blocks.', 'files': ['FIX2D11b.py']}, {'patch_id': 'FIX2D11', 'date': '2026-01-15', 'summary': 'Render gate fallback in union mode: if V28 anchor-enforce yields 0 hits, populate canonical_for_render from current primary_metrics_canonical and label as unanchored-for-render.', 'files': ['FIX2D11.py']}, {'patch_id': 'FIX2D73', 'date': '2026-01-20', 'summary': 'HistoryFull persistence gap: promote baseline primary_metrics_canonical into rehydrated previous_data + ensure compute_source_anchored_diff prev_response carries canonical metrics so Diff Panel V2 can compute deltas.', 'files': ['FIX2D73_full_codebase.py']}, {'patch_id': 'FIX2D75', 'date': '2026-01-20', 'summary': 'Option B fork: materialize Analysis baseline primary_metrics_canonical (schema-anchored rebuild) and persist it (incl. primary_response) so HistoryFull replay exposes previous canonical values for diffing.', 'files': ['FIX2D75_full_codebase.py'], 'supersedes': ['FIX2D73']}, {'patch_id': 'FIX2D10', 'date': '2026-01-15', 'summary': 'Materialize output_debug.canonical_for_render_v1 from results.primary_metrics_canonical so dashboard can hydrate Current and diagnostics stop falsely flagging missing.', 'files': ['FIX2D10.py']}, {'patch_id': 'FIX2D9', 'date': '2026-01-15', 'summary': 'Schema-anchored current-side canonical rebuild for diff/render: prefer schema_only rebuild so keys overlap and Current can populate.', 'files': ['FIX2D9.py']}, {'patch_id': 'FIX2D8', 'date': '2026-01-15', 'summary': 'Normalize output shape by promoting nested results.results.* into results.* for diff/render Current hydration.', 'files': ['FIX2D8_fixed.py']}, {'patch_id': 'FIX2D1', 'date': '2026-01-15', 'summary': 'Alias canonical rebuild functions to avoid fn_missing; harden Diff Panel V2 wrapper to prevent unbound summary crash.', 'files': ['fix41afc19_evo_fix16_anchor_rebuild_override_v1_fix2af_fetch_failure_visibility_and_hardening_v1.py']}, {'patch_id': 'FIX2D3', 'date': '2026-01-15', 'summary': 'Fix FIX41AFC19 v19 display-rebuild pool resolution + callable lookup; harden Diff Panel V2 injected-set detection (support cur_response without debug wrapper).', 'files': ['FIX2D3.py']}, {'patch_id': 'FIX2D4', 'date': '2026-01-15', 'summary': 'Add debug.key_overlap_v1 to explicitly report prev/cur canonical key counts, overlap, and target key presence for deterministic diff feasibility checks.', 'files': ['FIX2D4.py']}, {'patch_id': 'FIX2D5', 'date': '2026-01-15', 'summary': 'Mirror canonical_for_render_v1 diagnostics into results.debug so dashboard/diff diagnostics can see it; additive only.', 'files': ['FIX2D5.py']}, {'patch_id': 'FIX2D6', 'date': '2026-01-15', 'summary': 'Option B engine completeness: Diff Panel V2 row universe can be prev∪cur (union) behind EVO_DIFF_JOIN_MODE flag; adds added/removed change_type and summary counts; default remains strict.', 'files': ['FIX2D6.py']}, {'patch_id': 'FIX2D6_HARDCODE', 'date': '2026-01-15', 'summary': 'Hardcode diff join mode override via FORCE_DIFF_JOIN_MODE and route Diff Panel V2 join-mode selection through helper.', 'files': ['FIX2D6.py']}, {'patch_id': 'FIX2D2', 'date': '2026-01-15', 'summary': 'Anchor-fill current_metrics for diff/display when schema_frozen is misaligned; add rebuild-fn name fallbacks to prevent fn_missing.', 'files': ['FIX2D2.py']}, {'patch_id': 'FIX2D2M', 'date': '2026-01-16', 'summary': 'Injected-first current-value inference: two-pass selection (injected-only pool then global fallback) with explicit trace fields and authoritative commit into metric_changes.current_value(_norm).', 'files': ['FIX2D2M.py']}, {'patch_id': 'FIX2D2N', 'date': '2026-01-16', 'summary': 'Baseline-keyed current mapping: for each Analysis baseline canonical key, synthesize a current metric via injected-first, unit-family-guarded inference from extracted_numbers pools when Evolution canonical keys diverge; enables deterministic Analysis→Evolution diff joins even under key parity gaps.', 'files': ['FIX2D2N.py'], 'supersedes': ['FIX2D2M']}, {'patch_id': 'FIX2D2O', 'date': '2026-01-16', 'summary': 'Persist baseline-keyed current mapping for diffing: when baseline keys are synthesized into cur_can, expose them as primary_metrics_canonical_for_diff and mirror into primary_metrics_canonical so Evolution output keys align with Analysis for the diff demo.', 'files': ['FIX2D2O.py'], 'supersedes': ['FIX2D2N']}, {'patch_id': 'FIX2D2Q', 'date': '2026-01-16', 'summary': 'Baseline-aligned current selection for diffing: injected-first with optional base fallback; stamps provenance fields (source_type, selection_mode) to prevent confusion while preserving union pool behavior.', 'files': ['FIX2D2Q.py'], 'supersedes': ['FIX2D2O']}, {'patch_id': 'FIX2D2R', 'date': '2026-01-16', 'summary': 'Rebuild parity guard: prevent schema-only rebuild paths from committing bare-year tokens when a unit-qualified sibling candidate exists in the same snippet; improves Analysis/Evolution parity for injected content.', 'files': ['FIX2D2R.py'], 'supersedes': ['FIX2D2Q']}, {'patch_id': 'FIX2D2S', 'date': '2026-01-16', 'summary': 'Schema-only rebuild hardening: when non-year candidates exist for a schema key, skip bare-year tokens during winner selection (down-rank/skip) and record diagnostics; reduces year-token pollution before downstream year-blocking.', 'files': ['FIX2D2S.py'], 'supersedes': ['FIX2D2R']}, {'patch_id': 'FIX2D42', 'date': '2026-01-17', 'summary': 'Serialize/promote baseline_schema_metrics_v1 into Analysis primary_response/results so Evolution diff can consume it; extend nested results promotion to mirror baseline_schema_metrics_v1.', 'files': ['FIX2D42.py']}, {'patch_id': 'FIX2D2T', 'summary': 'Add explicit baseline->current projection in diff layer: if baseline row is missing CURRENT but cur_response has same canonical_key in primary_metrics_canonical_for_diff/primary_metrics_canonical, project into metric_changes current_value/_norm and recompute diff counters; attach debug summary.', 'ts': '2026-01-16'}, {'patch_id': 'FIX2D2W', 'date': '2026-01-17', 'summary': 'Fix parity leak: ensure schema_only_rebuild commit-time semantic gate is always active (avoid NameError when _FIX2D2U_ENABLE defined later) and fix year-token extraction regex for required-year checks; bump CODE_VERSION.', 'files': ['FIX2D2W.py'], 'supersedes': ['FIX2D2V']}, {'patch_id': 'FIX2D2X', 'date': '2026-01-17', 'summary': 'Parity patch: replace Evolution schema-only slot filling for baseline-key current with Analysis authoritative selector (_analysis_canonical_final_selector_v1) using injected-first two-pass selection and synthesized keyword hints from canonical_key/name; prevents cross-metric misassignment (e.g., China sales -> chargers 2040) and aligns gating with Analysis.', 'files': ['FIX2D2X.py'], 'supersedes': ['FIX2D2W', 'FIX2D2V', 'FIX2D2U']}, {'patch_id': 'FIX2D2Y', 'date': '2026-01-17', 'summary': 'Hardwire Evolution rebuild_metrics_from_snapshots_analysis_canonical_v1 to use the shared Analysis final selector for baseline-keyed diff current metrics (fix41afc19 parity); eliminates disjoint keyset that blocks diff activation.', 'files': ['FIX2D2Y.py'], 'supersedes': ['FIX2D2X']}, {'patch_id': 'FIX2D65B', 'date': '2026-01-19', 'summary': 'Force canonical pipeline materialisation when injected URLs exist (seed schema via deterministic extensions so FIX2D31 schema-authority rebuild can run even for narrative queries).', 'files': ['FIX2D65B_full_codebase.py'], 'supersedes': ['FIX2D65A']}, {'patch_id': 'FIX2D65C', 'date': '2026-01-19', 'summary': 'Restore analysis->evolution diff contract: broaden injected URL detection (ui_raw + legacy keys) so schema seeding and FIX2D31 schema-authority rebuild reliably run when injection is used; bump version.', 'files': ['FIX2D65C_full_codebase.py'], 'supersedes': ['FIX2D65B']}, {'patch_id': 'FIX2D65D', 'date': '2026-01-19', 'summary': 'Restore analysis->evolution diff contract by always serializing/seeding metric_schema_frozen (deterministic schema extensions), so Evolution schema-only rebuild has a stable keyspace even when LLM emits no primary_metrics; bump version.', 'files': ['FIX2D65D_full_codebase.py'], 'supersedes': ['FIX2D65C']}, {'patch_id': 'FIX2D66H', 'date': '2026-01-19', 'summary': "Fix Google Sheets history save return semantics: add_to_history() now returns True on successful Sheets append and False on failure (previously fell through as None, triggering spurious 'Saved to session only' warning). Keeps session fallback and captures last Sheets error for diagnostics.", 'files': ['FIX2D66H_full_codebase.py']}, {'patch_id': 'FIX2D67', 'date': '2026-01-19', 'summary': 'Fix injected numeric extraction missing-link: fetch_web_context() now calls numeric extractor with correct parameter name (source_url vs url), preventing silent TypeError and empty extracted_numbers; restores injected HTML numbers into snapshot pools feeding schema-only rebuild.', 'files': ['FIX2D67_full_codebase.py'], 'supersedes': ['FIX2D66H']}, {'patch_id': 'FIX2D77', 'date': '2026-01-20', 'summary': 'Percent-schema guardrail: prevent schema-only rebuild from binding year-like tokens (e.g., 2040) to __percent keys by requiring percent evidence and rejecting yearlike-without-percent raw; fixes incorrect prev value for CAGR percent metrics.', 'files': ['FIX2D77_full_codebase.py'], 'supersedes': ['FIX2D76']}, {'patch_id': 'FIX2D82', 'date': '2026-01-20', 'summary': 'Definitive fix for year-token leakage into __percent metrics: drop year-token values when raw token is YYYY regardless of context % signs; force-apply at schema_only rebuild (Analysis baseline materialize) and sanitize previous_data before diff join (cleans old HistoryFull snapshots). Adds debug: fix2d82_percent_sanitize_schema_only / fix2d82_prev_percent_sanitize.', 'files': ['FIX2D82_full_codebase.py'], 'supersedes': ['FIX2D80']}, {'patch_id': 'FIX2D83', 'date': '2026-01-20', 'summary': 'Cleanup consolidation: remove obsolete percent-key guard wrappers (FIX2D78/FIX2D79) now superseded by definitive FIX2D82 sanitizer; stabilize CODE_VERSION with final override to avoid stale late assignments; reduce patch clutter without changing diff behavior.', 'files': ['FIX2D83_full_codebase.py'], 'supersedes': ['FIX2D78', 'FIX2D79', 'FIX2D80']}, {'patch_id': 'REFACTOR45', 'date': '2026-01-25', 'summary': 'Evolution: fix Diff Panel V2 empty rows by hardening the V2 builder call against RecursionError (minimal wrappers, traceback capture, preserve existing rows, strict canonical-join fallback).', 'files': ['REFACTOR45_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR44']}, {'patch_id': 'REFACTOR11', 'date': '2026-01-23', 'summary': 'Fix evolution JSON/UI counters: recompute summary (increased/decreased/unchanged/added) from final metric_changes rows; recompute stability_score accordingly; bump final binding/version locks to REFACTOR11 for hygiene.', 'files': ['REFACTOR11_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR10']}, {'patch_id': 'REFACTOR13', 'date': '2026-01-23', 'summary': 'Summary/stability correctness: recompute evolution results.summary and stability_score from canonical-first diff rows (metric_changes_v2/metric_changes). Add graded stability fallback (100 - mean abs % change) when discrete unchanged/small-change scoring would yield 0, and mirror counts into diff_panel_v2_summary for auditability.', 'files': ['REFACTOR13_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR12']}, {'patch_id': 'REFACTOR14', 'date': '2026-01-23', 'summary': 'Diff engine consolidation: eliminate diff_metrics_by_name override chain by renaming legacy impls and introducing a single public wrapper entrypoint. Preserve legacy/fix31/v24 impls under stable names; update base capture vars and keep binding manifest stable/streamlit-safe.', 'files': ['REFACTOR14_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR13']}, {'patch_id': 'REFACTOR15', 'date': '2026-01-23', 'summary': "Restore diffing after REFACTOR14: harden diff_metrics_by_name resolution in FINAL BINDINGS, eliminate V2 UnboundLocalError (cur_resp_for_diff), and add safe fallback extraction for canonical_for_render/current PMC when nested under output['results'].", 'files': ['REFACTOR15_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR14']}, {'patch_id': 'REFACTOR12', 'date': '2026-01-23', 'summary': 'Truth-lock version stamping + binding manifest hygiene: freeze _yureeka_get_code_version() via locked default arg; re-assert globals for observability; ensure FINAL BINDINGS tag + diff function authoritative tag always present; standardize canonical_for_render_v1 debug block to avoid stale missing diagnostics.', 'files': ['REFACTOR12_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR11']}, {'patch_id': 'REFACTOR09', 'date': '2026-01-22', 'summary': 'Introduce a single authoritative diff engine wrapper (_refactor09_diff_metrics_by_name) and bind diff_metrics_by_name to it in final bindings; prepare for safe removal of legacy duplicate diff definitions.', 'files': ['REFACTOR09_full_codebase_streamlit_safe.py']}, {'patch_id': 'REFACTOR60', 'date': '2026-01-26', 'summary': "Fix REFACTOR09 diff wrapper regression by robustly capturing a callable legacy diff implementation (and adding a signature-safe base fallback). Restores Evolution metric_changes so the dashboard no longer shows 'no metrics to display'.", 'files': ['REFACTOR60.py']}, {'patch_id': 'REFACTOR35', 'summary': 'Move main() invocation to EOF so late refactor defs/overrides are active during runs; add schema-key filter to baseline PMC materialization to prevent debug-key leakage.', 'files': ['REFACTOR35_full_codebase_streamlit_safe.py']}, {'patch_id': 'REFACTOR36', 'summary': "Fix Evolution fatal 'NoneType has no attribute get' by hardening add_to_history() against None callers and coercing run_source_anchored_evolution inputs (previous_data/web_context) to dicts.", 'files': ['REFACTOR36_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR35']}, {'patch_id': 'REFACTOR37', 'summary': 'Add a final crash-proof wrapper for run_source_anchored_evolution to prevent fatal NoneType.get exceptions from aborting Streamlit Evolution; coerce inputs to dict and return renderer-safe failed payload with traceback.', 'files': ['REFACTOR37_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR36']}, {'patch_id': 'REFACTOR38', 'summary': 'Fix FIX24 helper regressions causing None.get crashes in Evolution: ensure _fix24_get_prev_full_payload/_fix24_get_prev_hashes/_fix24_compute_current_hashes always return dicts; enhance REFACTOR37 evolution wrapper to persist full traceback under debug.error_traceback, surface a callsite hint in message/debug, and guard against non-dict impl returns.', 'files': ['REFACTOR38_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR37']}, {'patch_id': 'REFACTOR39', 'summary': 'Fix source-anchored evolution snapshot gating regression: when baseline_sources_cache is omitted from HistoryFull payload (Sheets cell limit), rehydrate snapshots deterministically via snapshot_store_ref/snapshot_store_ref_v2 and source_snapshot_hash_v2/source_snapshot_hash (Snapshots worksheet and local snapshot store). Attach snapshot_store_debug into output.debug on failure for fast diagnosis. No heuristic matching added; remains strict snapshot-gated without valid cached source text.', 'files': ['REFACTOR39_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR38']}, {'patch_id': 'REFACTOR83', 'date': '2026-01-28', 'summary': 'Hardening + clarity pass: (1) normalize Evolution output so baseline_sources_cache is never accidentally reduced to an injected-only row when baseline_sources_cache_current contains the full current pool (keeps UI/harness consistent), and (2) fix a latent NameError in canonical_for_render_v1 diagnostic extension (baseline_sources_cache_prev_rows) so diag_ext reliably populates. No schema/key-grammar changes; no unit conversion changes; Streamlit-safe.', 'files': ['REFACTOR83.py'], 'supersedes': ['REFACTOR82']}, {'patch_id': 'REFACTOR84', 'date': '2026-01-28', 'summary': 'Bump the frozen code-version lock to REFACTOR84 so JSON stamping (code_version) matches the active patch and the harness version self-check stops flagging false mismatches. Add patch tracker entry for REFACTOR84. No schema/key-grammar changes; Streamlit-safe.', 'files': ['REFACTOR84.py'], 'supersedes': ['REFACTOR83']}, {'patch_id': 'REFACTOR85', 'date': '2026-01-28', 'summary': "Optional high-value hardening: (1) handle PDF sources in scrape_url (extract text instead of failed:no_text), (2) add explicit last-good snapshot fallback when extractor returns 0 numbers on blocked/placeholder pages, and (3) normalize evolution source caches even when payload contains a nested 'results' mirror dict. No schema/key-grammar changes; no unit conversion changes; Streamlit-safe.", 'files': ['REFACTOR85.py'], 'supersedes': ['REFACTOR84']}, {'patch_id': 'REFACTOR40', 'summary': "Fix recent snapshot retrievability and partial snapshot corruption in Snapshots sheet store. load_full_snapshots_from_sheet now bypasses stale cache on hash miss, and selects the most recent *complete* write batch (grouped by created_at) to avoid mixed/partial merges. store_full_snapshots_to_sheet no longer treats any existing rows as 'complete'; it validates loadability first and attempts repair writes when prior batch is incomplete, and invalidates snapshot worksheet cache after successful writes.", 'files': ['REFACTOR40_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR39']}, {'patch_id': 'REFACTOR41', 'date': '2026-01-25', 'summary': 'Fix recent snapshot rehydration failures by (1) preventing fake snapshot_store_ref_v2 (gsheet:Snapshots:<hash>) from being emitted unless the mirror-write actually succeeded, (2) emitting snapshot_store_ref_stable pointing to a verified store (v2 sheet > v1 sheet > local) plus a compact snapshot_store_write_v1 debug manifest, and (3) making store_full_snapshots_to_sheet more reliable via smaller default chunk size and batched append_rows to reduce API payload size / rate-limit failures.', 'files': ['REFACTOR41_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR40']}, {'patch_id': 'REFACTOR42', 'date': '2026-01-25', 'summary': "Fix snapshot-gate failures caused by large baseline_sources_cache writes silently failing under Sheets rate limits. Snapshots sheet store now compresses very large payloads (zlib+base64 with 'zlib64:' prefix) to drastically reduce chunk count and API calls, adds a small throttle between batch appends for very large writes, and the loader transparently detects/decompresses compressed payloads while remaining backward-compatible with existing uncompressed snapshots.", 'files': ['REFACTOR42_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR41']}, {'patch_id': 'REFACTOR43', 'date': '2026-01-25', 'summary': "BUGFIX: make Snapshots sheet loader actually decode REFACTOR42 compressed payloads ('zlib64:' prefix). Previously store_full_snapshots_to_sheet could write compressed snapshots but load_full_snapshots_from_sheet attempted json.loads() on the compressed string and returned empty, causing Evolution to be snapshot-gated for recent runs while older (uncompressed) snapshots still loaded.", 'files': ['REFACTOR43_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR42']}, {'patch_id': 'REFACTOR44', 'date': '2026-01-25', 'summary': 'BUGFIX: Fix local snapshot persistence path creation. _snapshot_store_dir() previously omitted a return on the success path, returning None and causing local snapshot store/load to fail (os.path.join(None,...)). Wrapped local snapshot write call in add_to_history with guards to prevent snapshot persistence block from aborting. Also hardened store_full_snapshots_local to compute path safely. This restores reliable snapshot persistence for recent runs when Sheets snapshot store is unavailable/partial, eliminating Evolution snapshot-gate failures caused by missing baseline_sources_cache.', 'files': ['REFACTOR44_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR43']}, {'patch_id': 'REFACTOR47', 'date': '2026-01-25', 'summary': 'Fix Diff Panel V2 recursion (maximum recursion depth exceeded) caused by FIX2D2I wrapper chains capturing already-wrapped __rows implementations. Provide a deterministic, non-recursive canonical-first join builder (strict unit comparability + percent/year poisoning containment) and rebind build_diff_metrics_panel_v2__rows (and FIX2D2I aliases) as last-wins entrypoint so Evolution no longer sets diff_panel_v2_error or falls back to strict_fallback_v2.', 'files': ['REFACTOR47_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR46']}, {'patch_id': 'REFACTOR48', 'date': '2026-01-25', 'summary': 'Fix source-anchored Metric Changes table to render metric_changes_v2 fields (delta_abs/delta_pct/comparability/method) while keeping legacy fallback; bump version lock to REFACTOR48.'}, {'patch_id': 'REFACTOR49', 'date': '2026-01-25', 'summary': 'Eliminate Diff Panel V2 RecursionError by making FIX2D2I-style __rows wrapper idempotent and non-recursive across duplicate wrapper blocks and Streamlit reruns. Store a stable base __rows implementation, avoid re-wrapping an already wrapped function, and keep trace augmentation without affecting schema/key grammar or diff semantics.', 'files': ['REFACTOR49_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR48']}, {'patch_id': 'REFACTOR50', 'date': '2026-01-25', 'summary': "Fix Evolution stability calculation: prevent unchanged rows from being double-counted as 'small change' (<10%), clamp discrete stability to 0–100, and compute stable/small counts from comparable rows only. Removes impossible 150% stability when all rows are unchanged; no schema/key-grammar changes.", 'files': ['REFACTOR50_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR49']}, {'patch_id': 'REFACTOR51', 'date': '2026-01-25', 'summary': "Fix evolution stability graded fallback: cap per-row abs % change at 100 before averaging so injected/outlier deltas don't force 0% stability. Add debug fields mean_abs_pct_raw/capped.", 'files': ['REFACTOR51_full_codebase_streamlit_safe.py']}, {'patch_id': 'REFACTOR52', 'date': '2026-01-25', 'summary': 'Add authority_manifest_v1 (runtime last-wins introspection) into binding_manifest_v1 to make refactor deletions safer. No schema/key-grammar changes; no behavior changes.', 'files': ['REFACTOR52_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR51']}, {'patch_id': 'REFACTOR53', 'date': '2026-01-25', 'summary': 'Make metric_changes rows self-attributing for injected-vs-production gating. Extend source_url extraction to include provenance.best_candidate.source_url; stamp rows with cur/current/source_url fields; change Δt gating to suppress only when row source URL matches injected URL set (missing attribution no longer treated as injected). Add debug counters rows_with_source_url/rows_missing_source_url/rows_suppressed_by_injection.', 'files': ['REFACTOR53_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR52']}, {'patch_id': 'REFACTOR54', 'date': '2026-01-25', 'summary': 'Safe downsizing + durability diagnostics: remove duplicated FIX2D2I Diff Panel V2 wrapper block (redundant after recursion hardening); add snapshot_roundtrip_v1 (best-effort readback of snapshot_store_ref_stable) into Analysis persistence debug to catch recent snapshot save/retrieve issues early. No schema/key-grammar changes.', 'files': ['REFACTOR54_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR53']}, {'patch_id': 'REFACTOR55', 'date': '2026-01-25', 'summary': "Consolidate metric changes outputs to a single canonical feed: output['metric_changes'] is authoritative; output['metric_changes_v2'] mirrors the same list for backward/UI compatibility; drop output['metric_changes_legacy'] (no longer maintained). Add a late-stage consolidation block in compute_source_anchored_diff_BASE to enforce invariants. No schema/key-grammar changes.", 'files': ['REFACTOR55_full_codebase_streamlit_safe.py'], 'supersedes': ['REFACTOR54']}, {'patch_id': 'REFACTOR56', 'date': '2026-01-26', 'summary': 'Controlled downsizing: stop emitting metric_changes_legacy entirely (remove late-stage re-add), add end-of-function safety rail to pop it before returning. No schema/key-grammar changes; Diff Panel V2 remains authoritative and metric_changes_v2 continues to mirror metric_changes.', 'files': ['REFACTOR56.py'], 'supersedes': ['REFACTOR55']}, {'patch_id': 'REFACTOR57', 'date': '2026-01-26', 'summary': 'Stabilize diff harness prior to further downsizing: prefer REFACTOR47 V2 row builder when present; suppress emission of debug.diff_panel_v2_error/traceback (legacy V2 builder can fail before late rebinds). Canonical-first strict fallback remains authoritative; no schema/key-grammar changes.', 'files': ['REFACTOR57.py'], 'supersedes': ['REFACTOR56']}, {'patch_id': 'REFACTOR58', 'date': '2026-01-26', 'summary': "Fix latent UnboundLocalError in Diff Panel V2 row builder by initializing 'effective' locals on loop entry and removing a duplicate 'current_value_norm' key in an early fallback row. This eliminates recurring debug.diff_panel_v2_traceback noise while preserving canonical-first diffing semantics. No schema/key-grammar changes.", 'files': ['REFACTOR58.py'], 'supersedes': ['REFACTOR57']}, {'patch_id': 'REFACTOR59', 'date': '2026-01-26', 'summary': 'Controlled downsizing: removed redundant legacy Diff Panel V2 builders/wrappers (incl. Fix2K inference path + FIX2D2I wrapper) and deleted an identical duplicated mid-file block that redefined FIX2D47/FIX2D48/FIX2D49-era helpers. REFACTOR47 canonical-first join remains the sole authoritative diff-row builder. No schema/key-grammar changes; preserves strict unit comparability and percent-year poisoning guardrails.', 'files': ['REFACTOR59.py'], 'supersedes': ['REFACTOR58']}, {'patch_id': 'REFACTOR61', 'date': '2026-01-26', 'summary': 'Restore minimal Diff Panel V2 canonical-first join row builder (build_diff_metrics_panel_v2__rows_refactor47) so Evolution always populates metric_changes_v2 (and thus metric_changes) even when Streamlit triggers execution before late diff wrapper defs. Strict unit comparability preserved; no schema/key-grammar changes.', 'files': ['REFACTOR61.py'], 'supersedes': ['REFACTOR60']}, {'patch_id': 'REFACTOR62', 'date': '2026-01-26', 'summary': "Add an early, schema-agnostic Diff Panel V2 failsafe canonical-first row builder to prevent Streamlit ordering hazards from producing 'no metrics to display' during controlled downsizing. Also remove obsolete commented CODE_VERSION bump blocks (comment-only clutter). No schema/key-grammar changes; strict unit comparability preserved.", 'files': ['REFACTOR62.py'], 'supersedes': ['REFACTOR61']}, {'patch_id': 'REFACTOR63', 'date': '2026-01-26', 'summary': 'Controlled downsizing: remove the unused FIX2D47 Diff Panel V2 builder + shadow helper defs, while retaining (and hardening) the deterministic collision resolver used by schema/key remapping. No schema/key-grammar changes; diffing + unit comparability preserved.', 'files': ['REFACTOR63.py'], 'supersedes': ['REFACTOR62']}, {'patch_id': 'REFACTOR64', 'date': '2026-01-26', 'summary': 'Controlled downsizing (low-risk): remove redundant early Diff Panel V2 failsafe builder (REFACTOR62) and delete a dead post-main REFACTOR47 rebind block that never affects runtime diffing. No schema/key-grammar changes; preserves strict unit comparability + canonical-first diffing.', 'files': ['REFACTOR64.py'], 'supersedes': ['REFACTOR63']}, {'patch_id': 'REFACTOR65', 'date': '2026-01-26', 'summary': 'Controlled downsizing (low-risk): remove unused preserved BASE implementations (diff_metrics_by_name_BASE and compute_source_anchored_diff_BASE) along with their wrap scaffolding. Add code_version to Evolution report export so all JSON artifacts carry the patch ID. No schema/key-grammar changes; preserves canonical-first diffing + strict unit comparability + snapshot rehydration.', 'files': ['REFACTOR65.py'], 'supersedes': ['REFACTOR64']}, {'patch_id': 'REFACTOR66', 'date': '2026-01-26', 'summary': "Controlled downsizing + safety rail: de-duplicate the redundant nested output['results'] mirror in Evolution payloads by shrinking it to a lightweight compatibility stub (prevents baseline_sources_cache duplication and reduces JSON/Sheets footprint). No schema/key-grammar changes; preserves canonical-first diffing, unit comparability, snapshot rehydration, Δt injection gating, and stability scoring.", 'files': ['REFACTOR66.py'], 'supersedes': ['REFACTOR65']}, {'patch_id': 'REFACTOR67', 'date': '2026-01-26', 'summary': 'Controlled downsizing: remove legacy run_source_anchored_evolution_BASE preservation path and the old pre-FIX24 evolution wrapper. FIX24 changed-case recompute now routes directly through compute_source_anchored_diff (single authoritative path), while retaining FIX24 debug markers, snapshot hash trace, Δt injection gating, and stability scoring. No schema/key-grammar changes.', 'files': ['REFACTOR67.py'], 'supersedes': ['REFACTOR66']}, {'patch_id': 'REFACTOR68', 'date': '2026-01-26', 'summary': 'Fix Evolution stability + summary regression: stop overwriting stability_score with legacy (unchanged/total) formula and recompute results.summary + stability_score from the final canonical-first metric_changes rows (V2-first). Restores correct increased/decreased/unchanged counters and non-zero graded stability for injection runs, while keeping strict unit comparability and schema/key-grammar freeze.', 'files': ['REFACTOR68.py'], 'supersedes': ['REFACTOR67']}, {'patch_id': 'REFACTOR69', 'date': '2026-01-26', 'summary': "Controlled downsizing + safety fix: promote _fmt_currency_first to a shared top-level helper (prevents NameError in render_native_comparison and removes an unnecessary nested duplicate). Also make the Source-Anchored Evolution Metric Changes table prefer output['metric_changes'] (authoritative) while retaining metric_changes_v2 as a mirror for compatibility. No schema/key-grammar changes.", 'files': ['REFACTOR69.py'], 'supersedes': ['REFACTOR68']}, {'patch_id': 'REFACTOR70', 'date': '2026-01-27', 'summary': 'Safety rail + simplification: add a late-stage output bridge in compute_source_anchored_diff to enforce a single authoritative metric_changes list (mirrored to metric_changes_v2) and a clamped top-level stability_score, while hard-removing metric_changes_legacy. This stabilizes the Evolution UI/export path during controlled downsizing. No schema/key-grammar changes.', 'files': ['REFACTOR70.py'], 'supersedes': ['REFACTOR69']}, {'patch_id': 'REFACTOR71', 'date': '2026-01-27', 'summary': 'Safety rail: stamp harness_invariants_v1 into Evolution results to prevent silent degradation when external sources flake (e.g., failed:no_text). Records schema-frozen key count vs baseline/current canonical counts, missing keys, and source failure summaries, plus an additive harness_warning_v1 banner string. No schema/key-grammar changes.', 'files': ['REFACTOR71.py'], 'supersedes': ['REFACTOR70']}, {'patch_id': 'REFACTOR72', 'date': '2026-01-27', 'summary': 'Completeness-first diffs: upgrade Diff Panel V2 strict canonical join to prefer frozen schema keys (when metric_schema_frozen is available) and emit explicit completeness rows when either side is missing (missing_baseline / missing_current / missing_both). Keeps strict unit comparability and delta computation only for unit-matching pairs; no schema/key-grammar changes; Streamlit-safe.', 'files': ['REFACTOR72.py'], 'supersedes': ['REFACTOR71']}, {'patch_id': 'REFACTOR73', 'date': '2026-01-27', 'summary': 'Fix REFACTOR72 indentation regression in _refactor13_recompute_summary_and_stability_v1 (rows loop escaped to module scope, causing runtime error). Restores Streamlit-safe execution and preserves completeness-first change_type counting.'}, {'patch_id': 'REFACTOR74', 'date': '2026-01-27', 'summary': 'Completeness-first diffs hardening: guarantee schema-complete Metric Changes rows even if the Diff Panel V2 builder errors by upgrading the strict fallback to iterate frozen schema keys (or union fallback) and emit explicit missing_baseline/missing_current/missing_both rows. Also extend harness_invariants_v1 to record metric_changes row_count vs schema size and surface row_count_mismatch in harness_warning_v1 when violated. No schema/key-grammar changes; Streamlit-safe.', 'files': ['REFACTOR74.py'], 'supersedes': ['REFACTOR73']}, {'patch_id': 'REFACTOR75', 'date': '2026-01-27', 'summary': 'High-value harness hardening: add explicit last-good snapshot fallback inside fetch_web_context for failed:no_text and scrape exceptions using existing_snapshots, with clear provenance fields (fallback_used/status_detail=fallback:last_good_snapshot) so extraction remains complete under source flakiness; extend harness_invariants_v1 to record and surface baseline/current fallbacks in both debug and harness_warning_v1; and restore the invariant that Evolution injection runs suppress Δt.', 'files': ['REFACTOR75.py'], 'supersedes': ['REFACTOR74']}, {'patch_id': 'REFACTOR76', 'date': '2026-01-27', 'summary': 'Fix injection-mode detection and completeness-first harness guards: suppress results.run_delta_seconds/human whenever injected URLs are present using canonical debug.inj_trace_v1 signals (not legacy lists); prefer inj_trace_v1 for per-row injection gating; extend harness_invariants_v1 with schema-key coverage checks (missing/extra/duplicate canonical keys) and warning-only change_type integrity validation for missing_baseline/missing_current rows, surfacing count-only warning banners.', 'files': ['REFACTOR76.py'], 'supersedes': ['REFACTOR75']}, {'patch_id': 'REFACTOR77', 'date': '2026-01-27', 'summary': 'Fix version stamping and add a self-check: bump the locked code version stamp to REFACTOR77, and extend harness_invariants_v1 with a warning-only comparison of output.code_version vs the latest REFACTOR patch_id in PATCH_TRACKER_V1, surfacing version_mismatch in harness_warning_v1 if they diverge (catches stale version locks / wrong file deployments).', 'files': ['REFACTOR77.py'], 'supersedes': ['REFACTOR76']}, {'patch_id': 'REFACTOR80', 'date': '2026-01-27', 'summary': 'Fix injection-mode detection so production runs do not get falsely flagged as suppressed_by_injection; suppress run-delta only when true UI/intake injection URLs exist. Also improve row_delta_gating_v1 source-attribution counters (rows_with_source_url/rows_missing_source_url) even when no injection is present. No schema/key-grammar changes; Streamlit-safe.', 'files': ['REFACTOR80.py'], 'supersedes': ['REFACTOR79']}, {'patch_id': 'REFACTOR81', 'date': '2026-01-27', 'summary': "Last-good snapshot fallback hardening: expand existing_snapshots lookup to match URL variants (scheme/www/trailing slash) and fall back not only on failed:no_text/exception but also when extraction yields zero numbers (explicit status_detail=fallback:last_good_snapshot, never silent). Add telemetry debug_counts.fallback_last_good_snapshot_used(+urls). Also fill units for schema-complete missing rows from metric_schema_frozen when safe (avoid currency placeholder unit 'U'). Streamlit-safe; no schema/key-grammar changes.", 'files': ['REFACTOR81.py'], 'supersedes': ['REFACTOR80']}, {'patch_id': 'REFACTOR82', 'date': '2026-01-27', 'summary': 'Fix patch tracker/version self-check false positives by registering the current REFACTOR patch before main() executes (Streamlit load-order safe). Also make the main() crash banner use the active code version instead of a hardcoded patch id. No schema/key-grammar changes; Streamlit-safe.', 'files': ['REFACTOR82.py'], 'supersedes': ['REFACTOR81']}, {'patch_id': 'REFACTOR86', 'date': '2026-01-28', 'summary': 'Controlled downsizing (step 1): collapse the accumulated patch-tracker try/append blocks into a single consolidated registry. Keep all existing patch metadata as a static canonical entries list, register idempotently at import-time, and remove redundant patch-tracker scaffolding to reduce file size and risk of syntax/indentation drift. No schema/key-grammar changes; no unit conversion changes; Streamlit-safe; core pipeline unchanged.', 'files': ['REFACTOR86.py'], 'supersedes': ['REFACTOR85']}, {'patch_id': 'REFACTOR87', 'date': '2026-01-28', 'summary': 'Controlled downsizing (step 2): remove a large, inactive legacy diff/patch ladder (FIX32→FIX2D77) that is no longer referenced by the authoritative Evolution diff path. Keep Diff Panel V2 (build_diff_metrics_panel_v2__rows_refactor47) and compute_source_anchored_diff unchanged. This reduces file size and lowers syntax/indentation risk while preserving all refactor invariants (schema frozen, strict unit comparability, percent-year poisoning guards, Streamlit-safe).', 'files': ['REFACTOR87.py'], 'supersedes': ['REFACTOR86']}, {'patch_id': 'REFACTOR88', 'date': '2026-01-28', 'summary': 'Hotfix after downsizing: restore/guard the FIX2D55 prev-lift helper used by the FIX24 changed-case evolution recompute path. Prevents the “FIX24: Evolution recompute failed (compute_source_anchored_diff path)” banner caused by missing helper after pruning legacy ladder. No schema/key-grammar changes; Streamlit-safe.', 'files': ['REFACTOR88.py'], 'supersedes': ['REFACTOR87']}, {'patch_id': 'REFACTOR89', 'date': '2026-01-28', 'summary': 'Restore baseline hydration into diff: add robust PMC locator and define the Diff Panel V2 unwrap helper to prevent silent baseline drop; treat empty baseline dict as a failure requiring fallback; broaden HF5 HistoryFull rehydrate trigger to also rehydrate when baseline primary_metrics_canonical is missing; add deterministic FIX24 rehydrate fallback and emit debug.prev_payload_probe_v1 + debug.baseline_missing_reason_v1 for explicit diagnostics. No schema/key-grammar changes; strict unit comparability preserved; Streamlit-safe.', 'files': ['REFACTOR89.py'], 'supersedes': ['REFACTOR88']}, {'id': 'REFACTOR90', 'date': '2026-01-28', 'summary': 'Fix missing baseline for investment metric by improving PDF coverage and supporting space-separated thousands in numeric extraction.', 'notes': ['PDF fetch now extracts first 10 pages plus an evenly-spaced spread sample up to ~50 pages total, to reach late-report tables (IEA PDFs).', 'Number regex now accepts space as a thousands separator (e.g., "2 131.89") and numeric parsing strips both commas and spaces.', 'Extractor fingerprint bumped to invalidate cached extracted_numbers and force recompute.'], 'files': ['REFACTOR90.py'], 'supersedes': ['REFACTOR89']}, {'id': 'REFACTOR90A', 'date': '2026-01-28', 'summary': 'Hotfix: fix indentation error in PDF sampling block (no logic changes).', 'notes': ['Corrects an IndentationError after a with-statement in fetch_url_content_with_status PDF handling.', 'No intended behavioral changes beyond making the REFACTOR90 PDF sampling code runnable.'], 'files': ['REFACTOR90A.py'], 'supersedes': ['REFACTOR90']}, {'id': 'REFACTOR91', 'date': '2026-01-28', 'summary': 'Add toggle to hide missing_both (coverage gap) rows in Metric Changes table by default.', 'notes': ['Adds a Streamlit checkbox: “Show missing-both rows (coverage gaps)”. Default off.', 'Shows a coverage gaps counter so users know rows are hidden.', 'Filtering is presentation-only; diff output remains unchanged.'], 'files': ['REFACTOR91.py'], 'supersedes': ['REFACTOR90A']}, {'patch_id': 'REFACTOR92', 'id': 'REFACTOR92', 'date': '2026-01-29', 'summary': 'Harden PDF ingestion to restore 4/4 schema coverage: add force-PDF path for .pdf URLs, extract pdfplumber tables alongside text sampling, and preserve PDF page coverage in status_detail to prevent front-matter-only misses (IEA Global EV Outlook PDF).', 'files': ['REFACTOR92.py'], 'supersedes': ['REFACTOR91'], 'notes': 'No schema/key grammar changes. Diff semantics unchanged; improves upstream evidence extraction for charging investment metric.'}, {'patch_id': 'REFACTOR93', 'id': 'REFACTOR93', 'date': '2026-01-29', 'summary': 'UI polish: hide missing_both (coverage gap) rows by default in legacy Metric Changes table, with a toggle and coverage counter (presentation-only).', 'notes': ['Adds a Streamlit checkbox near the legacy Metric Changes table to hide rows where both previous and current are missing.', 'Default hides missing_both; toggle reveals them.', 'No changes to schema, key grammar, diff semantics, or JSON outputs.'], 'files': ['REFACTOR93.py'], 'supersedes': ['REFACTOR92']}, {'patch_id': 'REFACTOR94', 'id': 'REFACTOR94', 'date': '2026-01-29', 'summary': 'UI defaults: hide missing_both (coverage gap) rows by default everywhere and hide the legacy Metric Changes table behind an opt-in toggle (presentation-only UI downsizing step).', 'notes': ['Adds a checkbox to show/hide the legacy Metric Changes (diff.metric_diffs) table; default is hidden to reduce clutter and prevent confusion with authoritative metric_changes_v2.', 'Keeps existing missing_both hide toggle defaults (off) and coverage counters.', 'No changes to schema, key grammar, diff semantics, or JSON outputs.'], 'files': ['REFACTOR94.py'], 'supersedes': ['REFACTOR93']}, {'patch_id': 'REFACTOR95', 'id': 'REFACTOR95', 'date': '2026-01-29', 'summary': 'Downsizing step 2: remove legacy Metric Changes (non-authoritative) table from the main UI and delete the unused non-canonical compute_metric_diffs() implementation (canonical path remains). No changes to authoritative metric_changes/metric_changes_v2 output schema or strict comparability rules.', 'notes': ['Legacy metric diffs (diff.metric_diffs) are still computed for stability, but the legacy table UI is removed to reduce clutter.', 'compute_metric_diffs() was dead code (no callers) and is removed.'], 'files': ['REFACTOR95.py'], 'supersedes': ['REFACTOR94']}, {'patch_id': 'REFACTOR96', 'id': 'REFACTOR96', 'date': '2026-01-29', 'summary': 'Downsizing step 3: remove obsolete early alias wrappers (shadowed by later authoritative implementations) for schema-only and analysis-canonical rebuild entrypoints. No behavior change.', 'notes': ['Deletes the redundant early alias definitions of rebuild_metrics_from_snapshots_schema_only_fix16 and rebuild_metrics_from_snapshots_analysis_canonical_v1 that were overwritten later in the file.', 'Reduces F811 redefinition noise; preserves final authoritative implementations.'], 'files': ['REFACTOR96.py'], 'supersedes': ['REFACTOR95']}, {'patch_id': 'REFACTOR97', 'date': '2026-01-29', 'summary': 'Controlled downsizing (low-risk): remove legacy diff capture plumbing and unused fix31 variant; bind REFACTOR09 diff wrapper directly to _yureeka_diff_metrics_by_name_wrap1. No behavior change.', 'notes': ['Deletes the REFACTOR60 _YUREEKA_DIFF_METRICS_BY_NAME_LEGACY capture block and the unused _yureeka_diff_metrics_by_name_fix31 implementation.', 'Simplifies diff wiring to reduce surface area while preserving canonical-first behavior.'], 'files': ['REFACTOR97.py'], 'supersedes': ['REFACTOR96']}, {'patch_id': 'REFACTOR98', 'date': '2026-01-29', 'summary': 'Controlled downsizing (low-risk): prune patch-banner comment scaffolding and separator noise to reduce file size and lower syntax/indentation risk. No behavior change.', 'notes': ['Removes only comment-only banners/separators outside string literals.', 'Runtime logic, schema/key grammar, diff semantics, and strict unit comparability unchanged.'], 'files': ['REFACTOR98.py'], 'supersedes': ['REFACTOR97']},
    {'patch_id': 'REFACTOR99', 'id': 'REFACTOR99', 'date': '2026-01-29', 'summary': 'Fix Evolution baseline selection to use latest Analysis payload only; add FIX31 fastpath safety gate when prev PMC is too sparse; improve FIX41AFC19 rebuild_exception diagnostics.', 'files': ['REFACTOR99.py'], 'supersedes': ['REFACTOR98']},
    {'patch_id': 'REFACTOR99A', 'id': 'REFACTOR99A', 'date': '2026-01-29', 'summary': 'Hotfix: remove accidental non-string entry from Google Sheets SCOPES (restores Sheets snapshot persistence). No other logic changes.', 'files': ['REFACTOR99A.py'], 'supersedes': ['REFACTOR99']},
    {'patch_id': 'REFACTOR99B', 'id': 'REFACTOR99B', 'date': '2026-01-29', 'summary': 'Hardening: sanitize OAuth scopes to strings-only at Google Sheets auth callsites to prevent crashes if SCOPES is contaminated.', 'files': ['REFACTOR99B.py'], 'supersedes': ['REFACTOR99A']},
    {'patch_id': 'REFACTOR99C', 'id': 'REFACTOR99C', 'date': '2026-01-29', 'summary': 'Hotfix: define _sanitize_scopes helper (fix NameError) while keeping scope sanitization for Google Sheets auth.', 'files': ['REFACTOR99C.py'], 'supersedes': ['REFACTOR99B']},
    {'patch_id': 'REFACTOR99D', 'id': 'REFACTOR99D', 'date': '2026-01-29', 'summary': 'Hotfix: harden Google Sheets OAuth scopes by sanitizing at call-site (no dependency on _sanitize_scopes).', 'files': ['REFACTOR99D.py'], 'supersedes': ['REFACTOR99C']}
, {'patch_id': 'REFACTOR100', 'date': '2026-01-30', 'summary': 'Enforce year-anchor gating in authoritative schema-only candidate selection: derive required year tokens from canonical_key, prefer year-matching candidates (best_strong) and fall back with used_fallback_weak + structured provenance debug (selection_year_anchor_v1). Also harden Google Sheets auth scopes by coercing to strings-only and keeping _sanitize_scopes as a compatibility alias.', 'files': ['REFACTOR100.py'], 'supersedes': ['REFACTOR99D']}, {'patch_id': 'REFACTOR101', 'date': '2026-01-30', 'summary': 'Fix year-anchor gating for underscore-separated canonical keys and ensure gating runs by disabling hash fast-path reuse when code version changes. Year-anchored metrics now emit selection_year_anchor_v1 provenance with required/found years and used_fallback_weak.', 'files': ['REFACTOR101.py'], 'supersedes': ['REFACTOR100']}, {'patch_id': 'REFACTOR102', 'date': '2026-01-30', 'summary': 'Fix baseline freshness after saving Analysis to Google Sheets by invalidating History worksheet get_all_values cache on successful append_row. Ensures Evolution baseline selector sees the most recent Analysis run immediately in the same session (prevents stale Yahoo-baseline reuse after REFACTOR101).', 'files': ['REFACTOR102.py'], 'supersedes': ['REFACTOR101']}, {'patch_id': 'REFACTOR103', 'date': '2026-01-30', 'summary': 'Fix stale baseline selection when latest Analysis row in Sheet1 is sheets-safe/truncated wrapper: allow wrapper baselines (rehydratable via HistoryFull/full_store_ref/_sheet_id) to appear in the Evolution baseline selector, so Evolution always diffs against the most recent Analysis run. (Rehydration already occurs inside source-anchored evolution via HistoryFull.)', 'files': ['REFACTOR103.py'], 'supersedes': ['REFACTOR102']}, {'patch_id': 'REFACTOR104', 'date': '2026-01-30', 'summary': 'Baseline freshness hardening: prefer in-session last_analysis as Evolution baseline when Sheets history is stale/cached, reset baseline selectbox on newest-timestamp change (keyed widget), and emit diag_baseline_freshness_v1 for traceability. This prevents Evolution from diffing against an older snapshot after a new Analysis run in the same session.', 'files': ['REFACTOR104.py'], 'supersedes': ['REFACTOR103']}, {'patch_id': 'REFACTOR105', 'date': '2026-01-30', 'summary': 'Fix Evolution reading old baseline snapshots by making get_history() cache-proof: after successful Sheets writes, set a write-dirty flag and bypass cached History reads once; merge in-session analysis_history + last_analysis into history even when Sheets is available, with de-dup and limit enforcement. Ensures Evolution baseline selector always sees and defaults to the latest Analysis run in-session.', 'files': ['REFACTOR105.py'], 'supersedes': ['REFACTOR104']}, {'patch_id': 'REFACTOR106', 'date': '2026-01-30', 'summary': 'Fix Evolution baseline still reading stale snapshots by adding a final run-time baseline autobump guard: when the baseline selector is left on default (index 0) but a newer in-session or freshly-scanned Sheets Analysis exists for the same question, automatically use that latest Analysis payload as baseline and stamp diag_baseline_autobump_v1 for traceability. No schema/key-grammar changes; Streamlit-safe.', 'files': ['REFACTOR106.py'], 'supersedes': ['REFACTOR105']}, {'patch_id': 'REFACTOR107', 'date': '2026-01-30', 'summary': 'Baseline freshness hardening v2: fix Evolution still reading old Analysis snapshots by (a) treating Analysis payloads as valid even when primary_metrics_canonical is nested under primary_response, (b) accepting sheets-safe/truncated wrapper baselines via full_store_ref/_sheet_id/snapshot refs, and (c) sorting merged history by nested timestamps so the newest Analysis is truly default. Strengthens the REFACTOR106 baseline autobump guard without schema/key-grammar changes.', 'files': ['REFACTOR107.py'], 'supersedes': ['REFACTOR106']}, {'patch_id': 'REFACTOR108', 'date': '2026-01-30', 'summary': 'Fix baseline snapshot recency: return history newest-first (reverse chronological) so Evolution default baseline is truly latest across sessions; add shared Sheets read-cache globals + short TTL to prevent stale reads. No schema/key-grammar changes; Streamlit-safe.', 'files': ['REFACTOR108.py'], 'supersedes': ['REFACTOR107']},
    {'patch_id': 'REFACTOR109', 'date': '2026-01-30', 'summary': 'Wire Evolution rebuilt current_baseline_sources_cache into compute_source_anchored_diff web_context so FIX42 can use true current pool (fixes old snapshot as current issue).', 'files': ['REFACTOR109.py'], 'supersedes': ['REFACTOR108']},
    {'patch_id': 'REFACTOR110', 'date': '2026-01-31', 'summary': 'Fix HistoryFull baseline rehydration against cached-empty reads: force direct worksheet refresh when cached rows are empty/short; invalidate HistoryFull cache after write_full_history_payload_to_sheet; add session dirty-flag bypass for immediate post-write rehydrate. Prevents Evolution from falling back to older snapshots when full_store_ref rehydration transiently fails.', 'files': ['REFACTOR110.py'], 'supersedes': ['REFACTOR109']}
    , {
  'patch_id': 'REFACTOR111',
  'date': '2026-02-02',
  'summary': 'Deterministic previous snapshot selection (HistoryFull max timestamp) + debug.prev_snapshot_pick_v1; break stale fastpath loop via debug.fastpath_gate_v1 and DISABLE_FASTPATH_FOR_NOW; Evolution previous_timestamp can be overridden from selected snapshot.',
  'files': ['REFACTOR111.py'],
  'supersedes': ['REFACTOR110']
}

    , {
  'patch_id': 'REFACTOR112',
  'date': '2026-02-03',
  'summary': 'Fix HistoryFull row schema mismatch: write 7-column rows (created_at, code_version) to match loader; add timestamp fallback from analysis_id when created_at is non-ISO; add sheet header/row-length probe fields to prev_snapshot_pick_v1; bump code version.',
  'files': ['REFACTOR112.py'],
  'supersedes': ['REFACTOR111']
}

, {'patch_id': 'REFACTOR113', 'date': '2026-02-03', 'summary': 'Hard enforce year-anchor gating for year-stamped schema keys (no weak fallback); emit missing_reason_v1 for blocked/missing year anchors; normalize source row status when status_detail indicates success.', 'files': ['REFACTOR113.py'], 'supersedes': ['REFACTOR112']}, {'patch_id': 'REFACTOR114', 'date': '2026-02-03', 'summary': 'Fix injected URL semantics: stop treating forced/seed source lists as injection; do not auto-fill diag_extra_urls_ui_raw; remove legacy fix2d65b_injected_urls fallbacks from injection detection + per-row delta gating so production runs regain Δt and only true UI injections suppress it.', 'files': ['REFACTOR114.py'], 'supersedes': ['REFACTOR113']}]

def _yureeka_register_patch_tracker_v1(_entries=_PATCH_TRACKER_CANONICAL_ENTRIES_V1):
    try:
        PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
        if not isinstance(PATCH_TRACKER_V1, list):
            PATCH_TRACKER_V1 = []
        _existing = set()
        for _e in PATCH_TRACKER_V1:
            if isinstance(_e, dict) and _e.get("patch_id") is not None:
                _existing.add(str(_e.get("patch_id")))
        for _e in (_entries or []):
            if not isinstance(_e, dict):
                continue
            _pid = _e.get("patch_id")
            if _pid is None:
                continue
            _pid_s = str(_pid)
            if _pid_s in _existing:
                continue
            PATCH_TRACKER_V1.append(dict(_e))
            _existing.add(_pid_s)
        globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
    except Exception:
        pass

# Register immediately (Streamlit rerun-safe, idempotent).
_yureeka_register_patch_tracker_v1()


def _yureeka_get_code_version(_lock=_YUREEKA_CODE_VERSION_LOCK):
    try:
        return str(_lock)
    except Exception:
        return "UNKNOWN"


def _yureeka_authority_manifest_v1() -> dict:
    """Additive debug helper: capture which 'last-wins' definitions are actually active at runtime.

    This is used during the downsizing phase to ensure deletions don't accidentally swap authority.
    Streamlit-safe: pure introspection (no IO/network).
    """
    def _fn_meta(name: str):
        try:
            fn = globals().get(name)
            if fn is None:
                return {"present": False}
            meta = {"present": True, "type": str(type(fn))}
            if callable(fn):
                try:
                    meta.update({
                        "name": str(getattr(fn, "__name__", "") or ""),
                        "qualname": str(getattr(fn, "__qualname__", "") or ""),
                        "module": str(getattr(fn, "__module__", "") or ""),
                        "id": str(id(fn)),
                    })
                except Exception:
                    pass
                try:
                    c = getattr(fn, "__code__", None)
                    if c is not None:
                        meta["firstlineno"] = int(getattr(c, "co_firstlineno", -1) or -1)
                        meta["filename"] = str(getattr(c, "co_filename", "") or "")
                except Exception:
                    pass
            return meta
        except Exception as e:
            return {"present": False, "error": f"{e}"}

    try:
        keys = [
            # evolution / snapshots
            "run_source_anchored_evolution",
            "compute_source_anchored_diff",
            "attach_source_snapshots_to_analysis",
            # diff panel + join engine
            "build_diff_metrics_panel_v2__rows_fix2d2i",
            "build_diff_metrics_panel_v2__rows",
            "build_diff_metrics_panel_v2",
            "diff_metrics_by_name",
            "_refactor09_diff_metrics_by_name",
            "metric_changes_v2",
        ]
    except Exception:
        keys = []

    out = {"code_version": str(globals().get("_YUREEKA_CODE_VERSION_LOCK") or ""), "targets": {}}
    for k in keys:
        out["targets"][k] = _fn_meta(k)
    return out
def _yureeka_runtime_identity_v1():
    """Additive debug helper: identify the running script reliably (helps diagnose stale-version runs)."""
    try:
        import os, sys, platform, hashlib, datetime
        out = {
            "code_version": _yureeka_get_code_version(),
            "authority_manifest_v1": _yureeka_authority_manifest_v1(),
            "code_version_lock": globals().get("_YUREEKA_CODE_VERSION_LOCK"),
        }
        try:
            out["__file__"] = __file__
        except Exception:
            out["__file__"] = None
        try:
            out["cwd"] = os.getcwd()
        except Exception:
            out["cwd"] = None
        try:
            out["pid"] = os.getpid()
        except Exception:
            out["pid"] = None
        try:
            out["python"] = sys.version.split()[0]
        except Exception:
            out["python"] = None
        try:
            out["platform"] = platform.platform()
        except Exception:
            out["platform"] = None
        try:
            out["now_utc"] = datetime.datetime.now(datetime.timezone.utc).isoformat()
        except Exception:
            out["now_utc"] = None

        # File signature (best-effort; safe in Streamlit)
        try:
            p = out.get("__file__")
            if isinstance(p, str) and p and os.path.exists(p):
                with open(p, "rb") as f:
                    b = f.read()
                out["file_sha1_12"] = hashlib.sha1(b).hexdigest()[:12]
                out["file_bytes"] = int(len(b))
        except Exception:
            pass

        return out
    except Exception:
        return {"code_version": _yureeka_get_code_version(),
            "authority_manifest_v1": _yureeka_authority_manifest_v1(), "code_version_lock": globals().get("_YUREEKA_CODE_VERSION_LOCK")}


def _yureeka_lock_version_globals_v1():
    """Re-assert global version vars for observability (does not affect the frozen getter)."""
    try:
        v = _yureeka_get_code_version()
        globals()["_YUREEKA_CODE_VERSION_LOCK"] = v
        globals()["CODE_VERSION"] = v
    except Exception:
        pass


def _yureeka_set_authoritative_binding_v1(fn, tag: str) -> bool:
    """Best-effort: stamp the authoritative binding tag onto the callable.
    Falls back to globals if the callable doesn't support attribute assignment.
    """
    ok = False
    try:
        setattr(fn, "__YUREEKA_AUTHORITATIVE_BINDING__", str(tag))
        ok = True
    except Exception:
        ok = False
    try:
        globals()["_YUREEKA_DIFF_METRICS_BY_NAME_AUTHORITATIVE"] = fn
    except Exception:
        pass
    try:
        globals()["_YUREEKA_DIFF_METRICS_BY_NAME_AUTHORITATIVE_TAG"] = str(tag)
    except Exception:
        pass
    return bool(ok)

def _yureeka_get_authoritative_binding_tag_v1(fn) -> str:
    """Return the authoritative binding tag for the given callable (attr first, then globals fallback)."""
    try:
        v = getattr(fn, "__YUREEKA_AUTHORITATIVE_BINDING__", None)
        if v:
            return str(v)
    except Exception:
        pass
    try:
        v = globals().get("_YUREEKA_DIFF_METRICS_BY_NAME_AUTHORITATIVE_TAG")
        if v:
            return str(v)
    except Exception:
        pass
    return ""

def _yureeka_ensure_final_bindings_v1():
    """Ensure FINAL BINDINGS tags are always present and consistent with the version lock.

    REFACTOR18: make the diff binding authority signal *non-optional*:
      - Try to stamp __YUREEKA_AUTHORITATIVE_BINDING__ on the callable.
      - Always mirror the tag into globals as a fallback (some callable wrappers reject setattr).
    """
    try:
        v = _yureeka_get_code_version()
        globals()["_YUREEKA_FINAL_BINDINGS_VERSION"] = v
    except Exception:
        pass

    # Resolve active diff entrypoint (best-effort)
    fn = None
    bound_from = ""
    try:
        fn = globals().get("diff_metrics_by_name")
    except Exception:
        fn = None

    if callable(fn):
        bound_from = "diff_metrics_by_name"
    else:
        for _cand_name in [
            "_yureeka_diff_metrics_by_name_v24",
            "diff_metrics_by_name_V24_BASE",
            "diff_metrics_by_name",
            "_refactor09_diff_metrics_by_name",
            "diff_metrics_by_name_FIX41_V34C_UNWRAP",
            "diff_metrics_by_name_FIX41_V34_ANCHOR_JOIN",
            "diff_metrics_by_name_FIX40_V32_PREFER_PMC",
            "diff_metrics_by_name_FIX34_V24_STRICT",
            "diff_metrics_by_name_FIX33_V23_CANONICAL_CLEAR",
            "diff_metrics_by_name_FIX2D34",
        ]:
            try:
                _cand = globals().get(_cand_name)
            except Exception:
                _cand = None
            if callable(_cand):
                fn = _cand
                bound_from = _cand_name
                try:
                    globals()["diff_metrics_by_name"] = fn
                except Exception:
                    pass
                break

    try:
        globals()["_YUREEKA_DIFF_METRICS_BY_NAME_BOUND_FROM"] = bound_from
    except Exception:
        pass

    try:
        if callable(fn):
            _yureeka_set_authoritative_binding_v1(fn, _yureeka_get_code_version())
    except Exception:
        pass


# assert globals early for debugging (safe no-op)

_yureeka_lock_version_globals_v1()
_yureeka_ensure_final_bindings_v1()


_YUREEKA_DEBUG_PLAYBOOK_MD_V1 = """## Debug Playbook (REFACTOR22)

This file is **single-file Streamlit-safe** and is intentionally refactored in small, testable steps.
The refactor harness (when enabled) is the authority for “did we preserve behavior?”.

### What to check first (fast triage)
1) **code_version** in both Analysis and Evolution JSON must match this file’s `_YUREEKA_CODE_VERSION_LOCK`.
2) Evolution must show **previous_data_rehydrated: true** when running after an Analysis baseline.
3) Diff Panel V2 must emit **metric_changes_v2** rows with both **prev_value_norm** and **cur_value_norm** for “both-side” metrics.

### If metric_changes_v2 is empty
- Confirm Analysis was run first and produced **primary_metrics_canonical**.
- Confirm Evolution JSON has **previous_data_rehydrated: true** and the expected **previous_timestamp**.
- Inspect `results.debug.diff_panel_v2_trace_v1` (if present) and row-level `diag.diff_join_trace_v1`.

### If code_version looks wrong
- Streamlit can reuse an older loaded module if the process isn’t restarted.
- This refactor uses a **version lock**: outputs stamp via `_yureeka_get_code_version()` (not `CODE_VERSION`).

### Key debug fields
- `debug.binding_manifest_v1`: Which entrypoints were resolved (Diff Panel V2 + legacy diff).
- `debug.canonical_for_render_v1`: Presence indicator for canonical-for-render debug (should be present).
- `metric_changes_v2[*].diag`: Per-row join + current-source trace.

"""


def _yureeka_show_debug_playbook_in_streamlit_v1():
    """Streamlit-safe helper: show an optional playbook expander without affecting JSON outputs."""
    try:
        if not hasattr(st, "sidebar"):
            return
        with st.sidebar.expander("Debug playbook", expanded=False):
            st.markdown(_YUREEKA_DEBUG_PLAYBOOK_MD_V1)
    except Exception:
        pass


# Invocation:
#   - python REFACTOR02_full_codebase_streamlit_safe.py --run_refactor_harness
#   - or set RUN_REFACTOR_HARNESS=1
# NOTE:
#   - Under Streamlit runtime, the harness is forcibly disabled to prevent sys.exit()
#     from terminating the Streamlit server / failing health checks.
try:
    import os as _rf01_os
    import sys as _rf01_sys

    def _rf01__running_under_streamlit() -> bool:
        try:
            argv = _rf01_sys.argv or []
            if any("streamlit" in str(a).lower() for a in argv[:5]):
                return True
        except Exception:
            pass
        try:
            if "streamlit" in _rf01_sys.modules:
                return True
            if "streamlit.runtime.scriptrunner" in _rf01_sys.modules:
                return True
        except Exception:
            pass
        try:
            for _k in (
                "STREAMLIT_SERVER_PORT",
                "STREAMLIT_SERVER_ADDRESS",
                "STREAMLIT_SERVER_HEADLESS",
                "STREAMLIT_BROWSER_GATHER_USAGE_STATS",
            ):
                if _rf01_os.getenv(_k) is not None:
                    return True
        except Exception:
            pass
        return False

    _rf01__is_streamlit = _rf01__running_under_streamlit()

    _REFACTOR01_HARNESS_REQUESTED = (
        (("--run_refactor_harness" in (_rf01_sys.argv or []))
         or (str(_rf01_os.getenv("RUN_REFACTOR_HARNESS", "")).strip().lower() in ("1", "true", "yes", "y")))
        and (not _rf01__is_streamlit)
    )
except Exception:
    _REFACTOR01_HARNESS_REQUESTED = False


# - Introduces a single deterministic identity tuple and a schema-first resolver.
# - Cuts old direct key-mint paths by routing canonical_key assignment through the resolver.
# - Analysis: after identity rekey, keep ONLY schema-bound metrics in primary_metrics_canonical.
# - Evolution schema_only rebuild: never allow a bare year token to commit for expected_kind=='unit' keys.
# - Propose schema entries from primary_metrics_provisional
# - Allow deterministic promotion into metric_schema_frozen (analysis-side)
# - Record proposals and promotions for audit
# - Split embedded year/YTD/forecast tokens out of metric_token and into time_scope.
# - Resolver matches schema on metric_token + '_' + time_scope, preventing 2024/2025 being glued into metric_token.
# - Also adds required except/pass closure for early patch-tracker try block.
# - Fixes a variable typo that could disable FIX2D2U gating in the prefilter.
# - Rejects yearlike numeric candidates for unit/count schema keys unless they carry unit evidence.
# - Records reject counts under _evolution_rebuild_debug.fix2d63_reject_yearlike_no_unit_evidence.
# - Introduces canonical_identity_spine.py as the single future authority for identity normalization,
#   schema-first key resolution, and value selection (incl. yearlike hard rejection immune to window backfill).
# - Adds minimal regression tests as callable self-checks (no runtime behavior change unless explicitly enabled).
# - Rewire Analysis + Evolution to resolve canonical keys via canonical_identity_spine.resolve_key_v1 (schema-first)
# - Enforce no-canonical-outside-spine gate at primary_metrics_canonical commit and schema_only_rebuild selection
# - Prune yearlike candidates for unit/count metrics even when unit evidence was context/window backfilled
_FIX2D20_DISABLE_FIX2D18 = False
_FIX2D20_DISABLE_FIX2D19 = False

# - We have repeated evidence of year tokens (e.g., 2024/2030) being committed
#   with method/unit missing. This patch records *where* and *what* is being
#   committed so we can fix the correct choke point without more speculation.

def _fix2d20_is_yearish_value(v):
    try:
        if v is None:
            return False
        fv = float(v)
        iv = int(fv)
        if abs(fv - iv) > 1e-6:
            return False
        return 1900 <= iv <= 2100
    except Exception:
        return False


def _fix2d20_is_yearish_token(raw: str) -> bool:
    try:
        s = str(raw or '').strip()
        if not s:
            return False
        # allow "2030" or "2030.0"
        if re.fullmatch(r"\d{4}(?:\.0+)?", s):
            iv = int(float(s))
            return 1900 <= iv <= 2100
        return False
    except Exception:
        return False


def _fix2d20_trace_year_like_commits(output: dict, stage: str, callsite: str) -> None:
    try:
        if not isinstance(output, dict):
            return
        results = output.get('results')
        if not isinstance(results, dict):
            return
        pmc = results.get('primary_metrics_canonical')
        if not isinstance(pmc, dict) or not pmc:
            return
        dbg = results.setdefault('debug', {})
        if not isinstance(dbg, dict):
            return
        trace = dbg.setdefault('fix2d20_year_commit_trace_v1', {})
        if not isinstance(trace, dict):
            return
        trace.setdefault('stage', str(stage or ''))
        trace.setdefault('events', [])
        trace.setdefault('callsite_counts', {})
        events = trace.get('events')
        if not isinstance(events, list):
            events = []
            trace['events'] = events
        ccounts = trace.get('callsite_counts')
        if not isinstance(ccounts, dict):
            ccounts = {}
            trace['callsite_counts'] = ccounts

        max_events = 80
        for k, mobj in pmc.items():
            if not isinstance(mobj, dict):
                continue
            v = mobj.get('value_norm')
            raw = mobj.get('raw')
            if not (_fix2d20_is_yearish_value(v) or _fix2d20_is_yearish_token(raw)):
                continue
            unit = mobj.get('unit_tag')
            method = mobj.get('method')
            src = mobj.get('source_url')
            ev = {
                'canonical_key': str(k),
                'value_norm': v,
                'raw': raw,
                'unit_tag': unit,
                'method': method,
                'source_url': src,
                'callsite': str(callsite or ''),
            }
            # small context sample if present
            evd = mobj.get('evidence')
            if isinstance(evd, dict):
                ctx = evd.get('context_snippet') or evd.get('context')
                if ctx:
                    ev['context_snippet'] = str(ctx)[:240]
                ev['evidence_method'] = evd.get('method')
                ev['evidence_raw'] = evd.get('raw')
            diag = mobj.get('diag')
            if isinstance(diag, dict):
                ev['diag_keys'] = sorted([str(x) for x in diag.keys()])[:12]
            events.append(ev)
            ccounts[str(callsite or '')] = int(ccounts.get(str(callsite or ''), 0) or 0) + 1
            if len(events) >= max_events:
                break

        trace['events'] = events
        trace['callsite_counts'] = ccounts
    except Exception:
        return

# NOTE: FIX2D15 supersedes and removes FIX2D15 year-token guard implementation (replaced with stricter schema-only eligibility gates).
# Purpose:
#   The Evolution dashboard diagnostics expect output_debug.canonical_for_render_v1
#   to exist. Some code paths populate current canonical under results.primary_metrics_canonical
#   (or nested results.results.*). This patch materializes output_debug.canonical_for_render_v1
#   from results.primary_metrics_canonical (post-promotion) to eliminate false 'missing' signals
#   and allow Current column hydration.
def _fix2d10_materialize_output_debug_canonical_for_render_v1(output_obj):
    diag = {
        "applied": False,
        "source": None,
        "count": 0,
        "keys_sample": [],
    }
    try:
        if not isinstance(output_obj, dict):
            return diag
        results = output_obj.get("results")
        if not isinstance(results, dict):
            return diag
        # prefer top-level results.primary_metrics_canonical
        pmc = results.get("primary_metrics_canonical")
        src_name = "results.primary_metrics_canonical"
        if not (isinstance(pmc, dict) and pmc):
            nested = results.get("results")
            if isinstance(nested, dict) and isinstance(nested.get("primary_metrics_canonical"), dict) and nested.get("primary_metrics_canonical"):
                pmc = nested.get("primary_metrics_canonical")
                src_name = "results.results.primary_metrics_canonical"
        if not (isinstance(pmc, dict) and pmc):
            diag["source"] = "none"
            return diag

        # materialize output_debug.canonical_for_render_v1
        od = output_obj.get("output_debug")
        if not isinstance(od, dict):
            od = {}
            output_obj["output_debug"] = od
        od["canonical_for_render_v1"] = dict(pmc)
        diag["applied"] = True
        diag["source"] = src_name
        diag["count"] = len(pmc)
        try:
            diag["keys_sample"] = list(pmc.keys())[:10]
        except Exception:
            pass
            diag["keys_sample"] = []

        try:
            results.setdefault("debug", {})
            results["debug"]["fix2d10_materialize_output_debug_canonical_for_render_v1"] = diag
        except Exception:
            return diag
    except Exception:
        return diag


# Purpose:
#   - HistoryFull rehydrate can return a full prior analysis payload where baseline
#     canonical metrics live under nested containers (e.g., results.primary_metrics_canonical).
#   - Evolution diffing (Diff Panel V2) expects previous_data.primary_metrics_canonical
#     (and/or previous_data.primary_response.primary_metrics_canonical) to exist.
# What:
#   - Promote nested baseline canonical metrics + schema into top-level keys on the
#     rehydrated previous payload, and mirror into primary_response.
#   - Emit compact diagnostics counts (safe, additive).

def _fix2d73_promote_rehydrated_prevdata_v1(prev_full: dict) -> dict:
    diag = {
        "applied": False,
        "pmc_before": 0,
        "pmc_after": 0,
        "pmc_source": None,
        "notes": [],
    }
    try:
        if not isinstance(prev_full, dict):
            return prev_full

        # Count before
        try:
            if isinstance(prev_full.get("primary_metrics_canonical"), dict):
                diag["pmc_before"] = int(len(prev_full.get("primary_metrics_canonical") or {}))
        except Exception:
            pass

        # Find candidate pmc in common nested locations
        pmc = None
        src = None
        if isinstance(prev_full.get("primary_metrics_canonical"), dict) and prev_full.get("primary_metrics_canonical"):
            pmc = prev_full.get("primary_metrics_canonical")
            src = "prev_full.primary_metrics_canonical"
        elif isinstance(prev_full.get("primary_response"), dict) and isinstance(prev_full["primary_response"].get("primary_metrics_canonical"), dict) and prev_full["primary_response"].get("primary_metrics_canonical"):
            pmc = prev_full["primary_response"].get("primary_metrics_canonical")
            src = "prev_full.primary_response.primary_metrics_canonical"
        elif isinstance(prev_full.get("results"), dict) and isinstance(prev_full["results"].get("primary_metrics_canonical"), dict) and prev_full["results"].get("primary_metrics_canonical"):
            pmc = prev_full["results"].get("primary_metrics_canonical")
            src = "prev_full.results.primary_metrics_canonical"
        elif isinstance(prev_full.get("results"), dict) and isinstance(prev_full["results"].get("primary_response"), dict) and isinstance(prev_full["results"]["primary_response"].get("primary_metrics_canonical"), dict) and prev_full["results"]["primary_response"].get("primary_metrics_canonical"):
            pmc = prev_full["results"]["primary_response"].get("primary_metrics_canonical")
            src = "prev_full.results.primary_response.primary_metrics_canonical"
        elif isinstance(prev_full.get("results"), dict) and isinstance(prev_full["results"].get("results"), dict) and isinstance(prev_full["results"]["results"].get("primary_metrics_canonical"), dict) and prev_full["results"]["results"].get("primary_metrics_canonical"):
            pmc = prev_full["results"]["results"].get("primary_metrics_canonical")
            src = "prev_full.results.results.primary_metrics_canonical"

        # Promote to top-level + mirror into primary_response
        if isinstance(pmc, dict) and pmc:
            if not (isinstance(prev_full.get("primary_metrics_canonical"), dict) and prev_full.get("primary_metrics_canonical")):
                prev_full["primary_metrics_canonical"] = dict(pmc)
                diag["notes"].append("promoted_top_level_primary_metrics_canonical")
                diag["applied"] = True
            if not isinstance(prev_full.get("primary_response"), dict):
                prev_full["primary_response"] = {}
                diag["notes"].append("created_primary_response")
            if isinstance(prev_full.get("primary_response"), dict):
                if not (isinstance(prev_full["primary_response"].get("primary_metrics_canonical"), dict) and prev_full["primary_response"].get("primary_metrics_canonical")):
                    prev_full["primary_response"]["primary_metrics_canonical"] = dict(prev_full.get("primary_metrics_canonical") or {})
                    diag["notes"].append("filled_primary_response.primary_metrics_canonical")
                    diag["applied"] = True

            diag["pmc_source"] = src

        # Count after
        try:
            if isinstance(prev_full.get("primary_metrics_canonical"), dict):
                diag["pmc_after"] = int(len(prev_full.get("primary_metrics_canonical") or {}))
        except Exception:
            pass

        # Attach diag
        try:
            prev_full.setdefault("debug", {})
            if isinstance(prev_full.get("debug"), dict):
                prev_full["debug"]["fix2d73_historyfull_load_counts"] = dict(diag)
        except Exception:
            pass

        return prev_full
    except Exception:
        return prev_full


# Purpose:
#   Force current-side canonical rebuild to be schema-anchored
#   to the Analysis (prev_response) schema universe, so keys
#   overlap and "Current" can populate.
#
# Strategy:
#   Prefer rebuild_metrics_from_snapshots_schema_only_fix16 if
#   callable; fallback to rebuild_metrics_from_snapshots_analysis_canonical_v1.
#
#   Render/diff-facing only; does not change hashing/snapshots.

def _fix2d9_schema_anchored_rebuild_current_metrics_v1(prev_response, pool, web_context=None):
    diag = {
        "applied": False,
        "fn": None,
        "count": 0,
        "keys_sample": [],
        "reason": None,
    }
    try:
        if pool is None:
            diag["reason"] = "pool_none"
            return None, diag
        if not isinstance(pool, list) or not pool:
            diag["reason"] = "pool_empty"
            return None, diag

        fn = globals().get("rebuild_metrics_from_snapshots_schema_only_fix16")
        fn_name = "rebuild_metrics_from_snapshots_schema_only_fix16"
        if not callable(fn):
            fn = globals().get("rebuild_metrics_from_snapshots_analysis_canonical_v1")
            fn_name = "rebuild_metrics_from_snapshots_analysis_canonical_v1"

        if not callable(fn):
            diag["reason"] = "fn_missing"
            return None, diag

        diag["fn"] = fn_name

        try:
            rebuilt = fn(prev_response, pool, web_context=web_context)
        except TypeError:
            rebuilt = fn(prev_response, pool)


        # REFACTOR04: enrich rebuilt PMC metrics with unit_tag/unit_family/multiplier_to_base for parity + diffing.
        try:
            if isinstance(rebuilt, dict) and rebuilt:
                rebuilt = _refactor04_enrich_pmc_units_v1(rebuilt, prev_response=prev_response)
        except Exception:
            pass

        if isinstance(rebuilt, dict) and rebuilt:
            diag["applied"] = True
            diag["count"] = len(rebuilt)
            try:
                diag["keys_sample"] = list(rebuilt.keys())[:10]
            except Exception:
                pass
                diag["keys_sample"] = []
            return dict(rebuilt), diag

        diag["reason"] = "rebuilt_empty_or_non_dict"
        return None, diag
    except Exception as _e:
        diag["reason"] = "exception:" + str(type(_e).__name__)
        return None, diag


# Purpose:
#   Normalize output shape by promoting nested results.results.*
#   up into results.* so downstream diff/render can see current
#   canonical metrics in the expected location.
#   (Additive, deterministic)
def _fix2d8_promote_nested_results_v1(output_obj):
    diag = {
        "applied": False,
        "promoted_primary_metrics_canonical": 0,
        "promoted_primary_response": 0,
        "notes": []
    }
    try:
        if not isinstance(output_obj, dict):
            return diag
        results = output_obj.get("results")
        if not isinstance(results, dict):
            return diag
        nested = results.get("results")
        if not isinstance(nested, dict):
            return diag

        nested_pmc = nested.get("primary_metrics_canonical")
        top_pmc = results.get("primary_metrics_canonical")
        if isinstance(nested_pmc, dict) and nested_pmc and (not isinstance(top_pmc, dict) or not top_pmc):
            results["primary_metrics_canonical"] = dict(nested_pmc)
            diag["promoted_primary_metrics_canonical"] = len(nested_pmc)
            diag["applied"] = True

        nested_pr = nested.get("primary_response")
        top_pr = results.get("primary_response")
        if isinstance(nested_pr, dict) and nested_pr and (not isinstance(top_pr, dict) or not top_pr):
            results["primary_response"] = dict(nested_pr)
            diag["promoted_primary_response"] = 1
            diag["applied"] = True

        # Ensure primary_response.primary_metrics_canonical exists
        try:
            if isinstance(results.get("primary_response"), dict) and isinstance(results.get("primary_metrics_canonical"), dict):
                if "primary_metrics_canonical" not in results["primary_response"]:
                    results["primary_response"]["primary_metrics_canonical"] = results["primary_metrics_canonical"]
                    diag["notes"].append("filled_primary_response.primary_metrics_canonical_from_results")
                    diag["applied"] = True
        except Exception:
            pass


        # FIX2D42: Ensure baseline_schema_metrics_v1 is promoted and visible under primary_response (and results)
        try:
            # Promote baseline_schema_metrics_v1 from nested if present
            _nested_bsm = None
            try:
                _nested_bsm = nested.get("baseline_schema_metrics_v1")
            except Exception:
                pass
                _nested_bsm = None
            if isinstance(_nested_bsm, dict) and _nested_bsm and (not isinstance(results.get("baseline_schema_metrics_v1"), dict) or not results.get("baseline_schema_metrics_v1")):
                results["baseline_schema_metrics_v1"] = dict(_nested_bsm)
                diag["notes"].append("promoted_results.baseline_schema_metrics_v1_from_nested")
                diag["applied"] = True

            # Ensure primary_response.baseline_schema_metrics_v1 exists when available
            if isinstance(results.get("primary_response"), dict):
                _bsm_top = results.get("baseline_schema_metrics_v1")
                if not isinstance(_bsm_top, dict):
                    _bsm_top = None
                if isinstance(_bsm_top, dict) and _bsm_top and ("baseline_schema_metrics_v1" not in results["primary_response"]):
                    results["primary_response"]["baseline_schema_metrics_v1"] = _bsm_top
                    diag["notes"].append("filled_primary_response.baseline_schema_metrics_v1_from_results")
                    diag["applied"] = True
        except Exception:
            pass
        # FIX2D42: Ensure baseline_schema_metrics_v1 is promoted and visible under primary_response (and results)
        try:
            # Promote baseline_schema_metrics_v1 from nested if present
            _nested_bsm = None
            try:
                _nested_bsm = nested.get("baseline_schema_metrics_v1")
            except Exception:
                pass
                _nested_bsm = None
            if isinstance(_nested_bsm, dict) and _nested_bsm and (not isinstance(results.get("baseline_schema_metrics_v1"), dict) or not results.get("baseline_schema_metrics_v1")):
                results["baseline_schema_metrics_v1"] = dict(_nested_bsm)
                diag["notes"].append("promoted_results.baseline_schema_metrics_v1_from_nested")
                diag["applied"] = True
            # If analysis builder stored it at results.baseline_schema_metrics_v1, mirror into primary_response
            if isinstance(results.get("primary_response"), dict) and isinstance(results.get("baseline_schema_metrics_v1"), dict):
                if "baseline_schema_metrics_v1" not in results["primary_response"]:
                    results["primary_response"]["baseline_schema_metrics_v1"] = results.get("baseline_schema_metrics_v1")
                    diag["notes"].append("filled_primary_response.baseline_schema_metrics_v1_from_results")
                    diag["applied"] = True
        except Exception:
            pass


        try:
            results.setdefault("debug", {})
            results["debug"]["fix2d8_promote_nested_results_v1"] = diag
        except Exception:
            return diag
    except Exception as _e:
        try:
            diag["notes"].append("exception:" + str(type(_e).__name__))
        except Exception:
            return diag


# Purpose:
#   - Assert the running code version at runtime (fail fast if wrong file imported)
#   - Emit execution stamp into results.debug so JSON proves which code ran
#   - Emit join mode into results.debug
EXPECTED_CODE_VERSION_FIX2D6 = "FIX2D6"

def _fix2d6_assert_and_stamp_runtime_v1(output_obj, join_mode=None):
    """Fail-fast version assert + JSON-visible execution stamp."""
    # Hard assert: if this file isn't the one executing, stop immediately.
    if str(globals().get("CODE_VERSION", "")).strip() != EXPECTED_CODE_VERSION_FIX2D6:
        raise RuntimeError(
            "FIX2D6 runtime assert failed: CODE_VERSION=%r expected=%r"
            % (globals().get("CODE_VERSION"), EXPECTED_CODE_VERSION_FIX2D6)
        )
    try:
        if isinstance(output_obj, dict):
            output_obj.setdefault("results", {}).setdefault("debug", {})
            output_obj["results"]["debug"]["__exec_code_version"] = globals().get("CODE_VERSION")
            try:
                output_obj["results"]["debug"]["__exec_join_mode"] = join_mode
            except Exception:
                pass
    except Exception:
        pass


# Purpose: Construct diff row key universe (strict vs union)
def _build_diff_key_universe(prev_keys, cur_keys):
    join_mode = None
    try:
        join_mode = _fix2d6_get_diff_join_mode_v1()
    except Exception:
        pass
        join_mode = "strict"
    if join_mode == "union":
        return sorted(set(prev_keys) | set(cur_keys)), join_mode
    return sorted(set(prev_keys)), join_mode


# Purpose:
#   Allow a hardcoded override for diff join mode (demo/debug).
#   If FORCE_DIFF_JOIN_MODE is set (e.g. "union"), it overrides
#   EVO_DIFF_JOIN_MODE environment variable.
FORCE_DIFF_JOIN_MODE = "union"   # set to None to restore env-based behavior

def _fix2d6_get_diff_join_mode_v1():
    try:
        if FORCE_DIFF_JOIN_MODE:
            return str(FORCE_DIFF_JOIN_MODE).strip().lower()
    except Exception:
        pass
    try:
        import os as _os
        return str(_os.getenv("EVO_DIFF_JOIN_MODE", "strict")).strip().lower()
    except Exception:
        return "strict"

# Purpose:
#   Emit explicit canonical key overlap diagnostics between previous and
#   current canonical metrics to make diff feasibility observable.
#   (Additive, no behavior change)

def _emit_key_overlap_debug_v1(prev_metrics, cur_metrics, target_key=None):
    try:
        prev_keys = set(prev_metrics.keys()) if isinstance(prev_metrics, dict) else set()
        cur_keys = set(cur_metrics.keys()) if isinstance(cur_metrics, dict) else set()
        overlap = prev_keys.intersection(cur_keys)
        return {
            "prev_count": len(prev_keys),
            "cur_count": len(cur_keys),
            "overlap_count": len(overlap),
            "overlap_sample": list(sorted(overlap))[:10],
            "target_key": target_key,
            "target_present_prev": (target_key in prev_keys) if target_key else None,
            "target_present_cur": (target_key in cur_keys) if target_key else None,
        }
    except Exception as _e:
        return {
            "error": "key_overlap_exception",
            "exception": str(type(_e).__name__),
        }

# - Adds best-effort aliases so FIX41AFC19 display rebuild can find a callable
# - Wraps Diff Panel V2 _impl call so summary is always defined (no UnboundLocalError)
def _fix2d1_first_callable_name(candidates):
    try:
        for name in candidates:
            fn = globals().get(name)
            if callable(fn):
                return name
    except Exception:
        return None
    return None

def _fix2d1_find_callable_by_contains(substr, deny_exact=None):
    try:
        for k, v in globals().items():
            if deny_exact and k == deny_exact:
                continue
            if substr in k and callable(v):
                return k
    except Exception:
        return None
    return None

# Ensure canonical rebuild symbol exists under the exact names FIX41AFC19 expects
try:
    if not callable(globals().get("rebuild_metrics_from_snapshots_analysis_canonical_v1")):
        _alt = _fix2d1_first_callable_name([
            "rebuild_metrics_from_snapshots_analysis_canonical_v1_IMPL",
            "rebuild_metrics_from_snapshots_analysis_canonical_v1_base",
            "rebuild_metrics_from_snapshots_analysis_canonical_v1_BASE",
        ]) or _fix2d1_find_callable_by_contains("rebuild_metrics_from_snapshots_analysis_canonical", deny_exact="rebuild_metrics_from_snapshots_analysis_canonical_v1")
        if _alt and callable(globals().get(_alt)):
            globals()["rebuild_metrics_from_snapshots_analysis_canonical_v1"] = globals()[_alt]

    if not callable(globals().get("rebuild_metrics_from_snapshots_schema_only_fix16")):
        _alt = _fix2d1_first_callable_name([
            "rebuild_metrics_from_snapshots_schema_only_fix16_IMPL",
            "rebuild_metrics_from_snapshots_schema_only_fix16_base",
            "rebuild_metrics_from_snapshots_schema_only_fix16_BASE",
        ]) or _fix2d1_find_callable_by_contains("rebuild_metrics_from_snapshots_schema_only_fix16", deny_exact="rebuild_metrics_from_snapshots_schema_only_fix16")
        if _alt and callable(globals().get(_alt)):
            globals()["rebuild_metrics_from_snapshots_schema_only_fix16"] = globals()[_alt]
except Exception:
    pass

# - URL shape normalizer (boundary before scraping)
# - Scrape ledger keyed by url_norm w/ stage+reason
# - Scraped text accessor to avoid meta-key drift
# - Fetch-failure visibility (status/textlen/classification)

def _fix2af_norm_url(u: str) -> str:
    try:
        s = str(u or "").strip()
        if not s:
            return ""
        _norm = globals().get("_inj_diag_norm_url_list")
        if callable(_norm):
            try:
                out = _norm([s])
                if out and isinstance(out, list):
                    return str(out[0] or "")
            except Exception:
                pass
        if s.startswith("http://"):
            s = "https://" + s[len("http://"):]
        if "#" in s:
            s = s.split("#", 1)[0]
        return s.rstrip("/")
    except Exception:
        return ""

def _fix2af_normalize_url_items(urls):
    diag = {
        "input_type": type(urls).__name__,
        "input_len": 0,
        "flattened_len": 0,
        "string_urls": 0,
        "dict_urls": 0,
        "dropped_non_url": 0,
        "mixed_shape": False,
        "nested_lists": 0,
        "samples_dropped": [],
    }
    out = []
    def _emit(u):
        nu = _fix2af_norm_url(u)
        if nu:
            out.append(nu)
        else:
            diag["dropped_non_url"] += 1
            if len(diag["samples_dropped"]) < 10:
                diag["samples_dropped"].append(str(u)[:200])
    def _walk(x):
        if x is None:
            return
        if isinstance(x, (list, tuple)):
            diag["nested_lists"] += 1
            for y in x:
                _walk(y)
            return
        if isinstance(x, dict):
            diag["dict_urls"] += 1
            for k in ("url", "href", "link"):
                if k in x and x.get(k):
                    _emit(x.get(k))
                    return
            diag["dropped_non_url"] += 1
            if len(diag["samples_dropped"]) < 10:
                diag["samples_dropped"].append(str(x)[:200])
            return
        diag["string_urls"] += 1
        _emit(x)

    try:
        if urls is None:
            diag["input_len"] = 0
        elif isinstance(urls, (list, tuple)):
            diag["input_len"] = len(urls)
            for it in urls:
                _walk(it)
        else:
            diag["input_len"] = 1
            _walk(urls)
    except Exception:
        pass

    diag["flattened_len"] = len(out)
    diag["mixed_shape"] = (diag["dict_urls"] > 0 and diag["string_urls"] > 0)

    seen = set()
    dedup = []
    for u in out:
        if u in seen:
            continue
        seen.add(u)
        dedup.append(u)
    return dedup, diag

def _fix2af_scraped_text_accessor(x):
    try:
        if x is None:
            return ""
        if isinstance(x, str):
            return x
        if isinstance(x, bytes):
            try:
                return x.decode("utf-8", errors="ignore")
            except Exception:
                return ""
        if isinstance(x, dict):
            for k in ("text", "clean_text", "content", "body", "html"):
                v = x.get(k)
                if isinstance(v, str) and v.strip():
                    return v
                if isinstance(v, bytes):
                    try:
                        return v.decode("utf-8", errors="ignore")
                    except Exception:
                        pass
            if "data" in x and isinstance(x["data"], dict):
                return _fix2af_scraped_text_accessor(x["data"])
            return ""
        return str(x)
    except Exception:
        return ""

def _fix2af_classify_fetch_failure(status, txt):
    try:
        s = str(status or "").lower()
        tlen = len(txt or "")
        if (not s or s == "success_direct") and tlen > 0:
            return "ok"
        if "timeout" in s:
            return "timeout"
        if "captcha" in s or "forbidden" in s or "blocked" in s or "403" in s:
            return "blocked"
        if "paywall" in s:
            return "paywall"
        if "pdf" in s and (tlen == 0 or "no_text" in s):
            return "pdf_no_text"
        if "no_text" in s or tlen == 0 or "empty" in s:
            return "no_text"
        if "redirect" in s:
            return "redirect"
        if "error" in s or "exception" in s or "fail" in s:
            return "error"

        # - Some sources label unit counts as "million units" while metric_name is "... Sales ...".
        # - If we see sales language AND a magnitude-like unit, bind to unit_sales.
        try:
            if ("sales" in n or "sold" in n) and any(tok in u for tok in ("million", "billion", "thousand", "units", "unit", "vehicles", "pcs", "pieces")):
                return "unit_sales"
        except Exception:
            return "unknown"
    except Exception:
        return "unknown"

def _fix2af_ledger_put(ledger: dict, url_raw: str, stage: str, reason: str = "", extra: dict = None):
    try:
        if ledger is None:
            return
        u = _fix2af_norm_url(url_raw)
        if not u:
            return
        rec = ledger.get(u) or {"url_norm": u, "stages": []}
        rec["stages"].append({
            "stage": str(stage or ""),
            "reason": str(reason or ""),
            "extra": extra if isinstance(extra, dict) else {}, })
        ledger[u] = rec
    except Exception:
        pass

_fix2af_last_scrape_ledger = {}

ENDSTATE_FINAL_VERSION = "v7_41_endstate_final_1"
INJ_TRACE_PATCH_VERSION = "fix41q_inj_trace_v1_always_emit"

# - Deterministic sorting / tie-breaking helpers
# - Deterministic candidate index builder (anchor_hash -> best candidate)
# - Lightweight schema + universe hashing for convergence checks
# - One-button end-state validation harness (callable)
# NOTE: Additive only; existing logic remains intact.
import hashlib as _es_hashlib

def _es_hash_text(s: str) -> str:
    try:
        return _es_hashlib.sha256((s or "").encode("utf-8")).hexdigest()
    except Exception:
        return ""

def _es_stable_sort_key(v):
    """
    Deterministic sort key that never relies on Python's randomized hash().
    Keeps ordering stable across runs for mixed types.
    """
    try:
        if v is None:
            return (0, "")
        if isinstance(v, (int, float)):
            return (1, f"{v:.17g}")
        if isinstance(v, str):
            return (2, v)
        if isinstance(v, bytes):
            return (3, v.decode("utf-8", "ignore"))
        if isinstance(v, dict):
            items = sorted(((str(k), _es_stable_sort_key(vv)) for k, vv in v.items()), key=lambda x: x[0])
            return (4, str(items))
        if isinstance(v, (list, tuple, set)):
            lst = list(v)
            try:
                lst.sort(key=_es_stable_sort_key)
            except Exception:
                pass
                lst = sorted(lst, key=lambda x: str(x))
            return (5, str([_es_stable_sort_key(x) for x in lst]))
        return (9, str(v))
    except Exception:
        return (9, str(v))

def _es_sorted_pairs_from_sources_cache(baseline_sources_cache):
    pairs = []
    for sr in (baseline_sources_cache or []):
        if not isinstance(sr, dict):
            continue
        u = (sr.get("source_url") or sr.get("url") or "").strip()
        fp = (sr.get("source_fingerprint") or sr.get("fingerprint") or sr.get("content_fingerprint") or "").strip()
        if u and fp:
            pairs.append((u, fp))
    pairs.sort(key=lambda t: (t[0], t[1]))
    return pairs

def _es_compute_canonical_universe_hash(primary_metrics_canonical: dict, metric_schema_frozen: dict) -> str:
    try:
        keys = set()
        if isinstance(primary_metrics_canonical, dict):
            keys.update([str(k) for k in primary_metrics_canonical.keys()])
        if isinstance(metric_schema_frozen, dict):
            keys.update([str(k) for k in metric_schema_frozen.keys()])
        return _es_hash_text("|".join(sorted(keys)))
    except Exception:
        return ""

def _es_compute_schema_hash(metric_schema_frozen: dict) -> str:
    """
    Deterministic hash of schema fields that affect numeric comparisons.
    Keeps it lightweight: tolerances + units + scale hints only.
    """
    try:
        if not isinstance(metric_schema_frozen, dict):
            return ""
        rows = []
        for k in sorted(metric_schema_frozen.keys()):
            s = metric_schema_frozen.get(k) or {}
            if not isinstance(s, dict):
                continue
            abs_eps = s.get("abs_eps", s.get("ABS_EPS"))
            rel_eps = s.get("rel_eps", s.get("REL_EPS"))
            unit = s.get("unit") or s.get("units") or ""
            scale = s.get("scale") or s.get("magnitude") or ""
            rows.append(f"{k}::abs={abs_eps}::rel={rel_eps}::unit={unit}::scale={scale}")
        return _es_hash_text("|".join(rows))
    except Exception:
        return ""

def _es_build_candidate_index_deterministic(baseline_sources_cache):
    """
    Deterministically build anchor_hash -> candidate map.
    If multiple candidates share the same anchor_hash, choose the best by a stable
    tie-breaker that prefers:
      - higher anchor_confidence
      - longer context_snippet (more evidence)
      - stable context_hash / numeric value / unit
      - stable source_url
    """
    try:
        buckets = {}
        for sr in (baseline_sources_cache or []):
            if not isinstance(sr, dict):
                continue
            su = sr.get("source_url") or sr.get("url") or ""
            for cand in (sr.get("extracted_numbers") or []):
                if not isinstance(cand, dict):
                    continue
                ah = cand.get("anchor_hash") or cand.get("anchor") or ""
                if not ah:
                    continue
                c2 = dict(cand)
                if "source_url" not in c2:
                    c2["source_url"] = su
                buckets.setdefault(ah, []).append(c2)

        out = {}
        for ah in sorted(buckets.keys()):
            cands = buckets.get(ah) or []
            def _cand_key(c):
                try:
                    conf = c.get("anchor_confidence")
                    conf_key = -(float(conf) if conf is not None else 0.0)
                except Exception:
                    pass
                    conf_key = 0.0
                ctx = (c.get("context_snippet") or c.get("context") or "")
                ctx_len = -len(str(ctx))
                ctx_hash = c.get("context_hash") or ""
                val = c.get("value")
                unit = c.get("unit") or ""
            # Reject bare-year tokens for non-year metrics when there is no token unit evidence.
            if expected_kind != "year":
                raw_token = (c.get("raw") or "").strip()
                if _fix27_is_bare_year_token(raw_token, c.get("value_norm")) and not _fix27_has_any_unit_evidence(c):
                    continue
            # Typed metrics require explicit token-level unit evidence
            if expected_kind == "currency" and not _fix27_has_currency_evidence(c):
                continue
            if expected_kind == "percent" and not _fix27_has_percent_evidence(c):
                continue
            if expected_kind == "unit" and not _fix27_has_any_unit_evidence(c):
                continue
                su = c.get("source_url") or ""
                return (conf_key, ctx_len, str(ctx_hash), _es_stable_sort_key(val), str(unit), str(su))
            cands_sorted = sorted(cands, key=_cand_key)
            out[ah] = cands_sorted[0] if cands_sorted else None
        return out
    except Exception:
        return {}

def end_state_validation_harness(baseline_analysis: dict, evolution_output: dict, min_stability: float = 99.9) -> dict:
    """
    PATCH ES9 (ADDITIVE): one-button end-state validation (warn-only helper)
    Use this to assert drift=0 on identical inputs.

    Returns a dict with pass/fail booleans and diagnostic fields.
    This does NOT mutate inputs.
    """
    report = {
        "passed": False,
        "checks": {},
        "notes": [],
    }
    try:
        base_prev = baseline_analysis or {}
        evo = evolution_output or {}

        # Snapshot hash
        base_snap = base_prev.get("source_snapshot_hash") or base_prev.get("results", {}).get("source_snapshot_hash")
        evo_snap = evo.get("source_snapshot_hash")

        # Universe + schema hashes
        base_uni = base_prev.get("canonical_universe_hash") or base_prev.get("results", {}).get("canonical_universe_hash")
        base_sch = base_prev.get("schema_hash") or base_prev.get("results", {}).get("schema_hash")
        evo_uni = evo.get("canonical_universe_hash")
        evo_sch = evo.get("schema_hash")

        report["checks"]["snapshot_hash_match"] = bool(base_snap and evo_snap and base_snap == evo_snap)
        report["checks"]["canonical_universe_hash_match"] = bool(base_uni and evo_uni and base_uni == evo_uni)
        report["checks"]["schema_hash_match"] = bool(base_sch and evo_sch and base_sch == evo_sch)

        # Stability threshold (warn-only semantics: "passed" includes match + stability)
        try:
            st = float(evo.get("stability_score") or 0.0)
        except Exception:
            pass
            st = 0.0
        report["checks"]["stability_meets_threshold"] = bool(st + 1e-9 >= float(min_stability))

        # Drift suspicion flag (if your pipeline sets it)
        report["checks"]["drift_suspected_flag_false"] = (evo.get("drift_suspected") is False)

        # Final pass condition
        report["passed"] = (
            report["checks"]["snapshot_hash_match"]
            and report["checks"]["canonical_universe_hash_match"]
            and report["checks"]["schema_hash_match"]
            and report["checks"]["stability_meets_threshold"]
        )

        if not report["passed"]:
            report["notes"].append("If hashes match but stability is low, inspect candidate tie-breaks and ordering.")
    except Exception:
        pass
        report["notes"].append("Validation harness encountered an exception (non-fatal).")
    return report


# GOOGLE SHEETS HISTORY STORAGE

SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",
]
MAX_HISTORY_ITEMS = 50

def _sanitize_scopes(scopes) -> list:
    """Compatibility alias for older callsites.

    REFACTOR100: Always route through _coerce_google_oauth_scopes to ensure the
    scopes list contains strings-only (drops non-strings deterministically).
    """
    try:
        return _coerce_google_oauth_scopes(scopes)
    except Exception:
        return _coerce_google_oauth_scopes([])

@st.cache_resource
def get_google_sheet():
    """Connect to Google Sheet (cached connection)"""
    try:
        creds = Credentials.from_service_account_info(
            dict(st.secrets["gcp_service_account"]),
            scopes=_coerce_google_oauth_scopes(SCOPES)
        )
        client = gspread.authorize(creds)

        # Why:
        # - Your spreadsheet contains multiple tabs (e.g., "New Analysis", "History", "HistoryFull", "Snapshots")
        # - sheet1 is often NOT "History", so get_history() reads the wrong tab and sees "no analyses"
        # Behavior:
        # - Default worksheet_title = "History" (override via secrets: google_sheets.history_worksheet)
        # - Fallback to sheet1 only if the worksheet doesn't exist
        spreadsheet_name = (
            st.secrets.get("google_sheets", {}).get("spreadsheet_name", "Yureeka_JSON")
        )
        ss = client.open(spreadsheet_name)

        worksheet_title = st.secrets.get("google_sheets", {}).get("history_worksheet", "History")
        try:
            sheet = ss.worksheet(worksheet_title)
        except Exception:
            pass
            sheet = ss.sheet1

        # Ensure headers exist - handle response object
        try:
            headers = sheet.row_values(1)
            if not headers or len(headers) == 0 or headers[0] != "id":
                # update() returns a response object in newer gspread - ignore it
                _ = sheet.update('A1:E1', [["id", "timestamp", "question", "confidence", "data"]])
        except gspread.exceptions.APIError:
            _ = sheet.update('A1:E1', [["id", "timestamp", "question", "confidence", "data"]])
        except Exception:
            pass  # Headers probably already exist

        return sheet

    except gspread.exceptions.SpreadsheetNotFound:
        st.error("❌ Spreadsheet not found. Create 'Yureeka_JSON' (or your configured name) and share with service account.")
        return None
    except Exception as e:
        error_str = str(e)
        # Ignore Response [200] - it's actually success
        if "Response [200]" in error_str:
            # This means the connection worked, try to return the sheet anyway
            try:
                creds = Credentials.from_service_account_info(
                    dict(st.secrets["gcp_service_account"]),
                    scopes=_coerce_google_oauth_scopes(SCOPES)
                )
                client = gspread.authorize(creds)

                spreadsheet_name = st.secrets.get("google_sheets", {}).get("spreadsheet_name", "Yureeka_JSON")
                ss = client.open(spreadsheet_name)
                worksheet_title = st.secrets.get("google_sheets", {}).get("history_worksheet", "History")
                try:
                    return ss.worksheet(worksheet_title)
                except Exception:
                    return ss.sheet1
            except:
                pass
        st.error(f"❌ Failed to connect to Google Sheets: {e}")
        return None

def generate_analysis_id() -> str:
    """Generate unique ID for analysis"""
    return f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hashlib.md5(str(datetime.now().timestamp()).encode()).hexdigest()[:6]}"


# Why:
# - Evolution/diff are now anchor-driven; analysis must persist a deterministic
#   canonical_key -> anchor_hash mapping for drift=0 convergence.
# - Some UI/Sheets wrappers omit anchors unless explicitly emitted.
# Determinism:
# - Only uses existing evidence/candidates already present in the analysis payload.
# - No re-fetching; no heuristic matching.
def _emit_metric_anchors_in_analysis_payload(analysis_obj: dict) -> dict:
    try:
        if not isinstance(analysis_obj, dict):
            return analysis_obj

        # If already present and non-empty, keep as-is
        existing = analysis_obj.get("metric_anchors")
        if isinstance(existing, dict) and existing:
            return analysis_obj

        # Identify the primary response container (some payloads store it nested)
        pr = analysis_obj.get("primary_response") if isinstance(analysis_obj.get("primary_response"), dict) else None
        pr = pr or analysis_obj

        # Canonical metrics (preferred)
        pmc = pr.get("primary_metrics_canonical") if isinstance(pr, dict) else None
        if not isinstance(pmc, dict) or not pmc:
            pmc = analysis_obj.get("primary_metrics_canonical") if isinstance(analysis_obj.get("primary_metrics_canonical"), dict) else {}

        # Candidate lookup table from baseline snapshots (if present)
        bsc = None
        try:
            r = analysis_obj.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache"), list):
                bsc = r.get("baseline_sources_cache")
        except Exception:
            pass
            bsc = None
        if bsc is None and isinstance(analysis_obj.get("baseline_sources_cache"), list):
            bsc = analysis_obj.get("baseline_sources_cache")

        def _safe_str(x):
            try:
                return str(x).strip()
            except Exception:
                return ""

        # Build (anchor_hash -> best candidate) index deterministically
        anchor_to_candidate = {}
        cand_to_candidate = {}
        try:
            if isinstance(bsc, list):
                for sr in bsc:
                    if not isinstance(sr, dict):
                        continue
                    surl = sr.get("source_url") or sr.get("url")
                    for n in (sr.get("extracted_numbers") or []):
                        if not isinstance(n, dict):
                            continue
                        ah = _safe_str(n.get("anchor_hash"))
                        cid = _safe_str(n.get("candidate_id"))
                        if ah and ah not in anchor_to_candidate:
                            anchor_to_candidate[ah] = dict(n, source_url=n.get("source_url") or surl)
                        if cid and cid not in cand_to_candidate:
                            cand_to_candidate[cid] = dict(n, source_url=n.get("source_url") or surl)
        except Exception:
            pass

        metric_anchors = {}

        # Deterministic iteration for stable JSON output
        for ckey in sorted([str(k) for k in (pmc or {}).keys()]):
            m = pmc.get(ckey)
            if not isinstance(m, dict):
                continue

            # Pick anchor identifiers from evidence first (most authoritative)
            ev = m.get("evidence") or []
            if not isinstance(ev, list):
                ev = []

            best = None
            for e in ev:
                if not isinstance(e, dict):
                    continue
                ah = _safe_str(e.get("anchor_hash") or e.get("anchor"))
                cid = _safe_str(e.get("candidate_id"))
                if ah or cid:
                    best = e
                    break

            # Fallback: sometimes metric row carries anchor_hash directly
            if best is None:
                best = {
                    "anchor_hash": m.get("anchor_hash") or m.get("anchor"),
                    "candidate_id": m.get("candidate_id"),
                    "source_url": m.get("source_url") or m.get("url"),
                    "context_snippet": m.get("context_snippet") or m.get("context"),
                    "anchor_confidence": m.get("anchor_confidence"),
                }

            ah = _safe_str(best.get("anchor_hash") or best.get("anchor"))
            cid = _safe_str(best.get("candidate_id"))
            surl = best.get("source_url") or best.get("url")
            ctx = best.get("context_snippet") or best.get("context")
            aconf = best.get("anchor_confidence")

            # Enrich from candidate index if needed
            if (not surl) or (not ctx):
                cand = None
                if ah and ah in anchor_to_candidate:
                    cand = anchor_to_candidate.get(ah)
                elif cid and cid in cand_to_candidate:
                    cand = cand_to_candidate.get(cid)
                if isinstance(cand, dict):
                    surl = surl or (cand.get("source_url") or cand.get("url"))
                    ctx = ctx or (cand.get("context_snippet") or cand.get("context"))

            # Only emit if we actually have an anchor id
            if not (ah or cid):
                continue

            try:
                if isinstance(ctx, str):
                    ctx = ctx.strip()[:220]
                else:
                    ctx = None
            except Exception:
                pass
                ctx = None

            try:
                aconf = float(aconf) if aconf is not None else None
            except Exception:
                pass
                aconf = None

            metric_anchors[ckey] = {
                "canonical_key": ckey,
                "anchor_hash": ah or None,
                "candidate_id": cid or None,
                "source_url": surl or None,
                "context_snippet": ctx,
                "anchor_confidence": aconf,
            }

            try:
                if ah and not _safe_str(m.get("anchor_hash")):
                    m["anchor_hash"] = ah
                if cid and not _safe_str(m.get("candidate_id")):
                    m["candidate_id"] = cid
                if surl and not (m.get("source_url") or m.get("url")):
                    m["source_url"] = surl
                if ctx and not (m.get("context_snippet") or m.get("context")):
                    m["context_snippet"] = ctx
                if aconf is not None and m.get("anchor_confidence") is None:
                    m["anchor_confidence"] = aconf
            except Exception:
                pass

        if metric_anchors:
            try:
                analysis_obj["metric_anchors"] = metric_anchors
            except Exception:
                pass
            try:
                if isinstance(pr, dict):
                    pr.setdefault("metric_anchors", metric_anchors)
            except Exception:
                pass
            try:
                analysis_obj.setdefault("results", {})
                if isinstance(analysis_obj["results"], dict):
                    analysis_obj["results"].setdefault("metric_anchors", metric_anchors)
            except Exception:
                pass

        return analysis_obj
    except Exception:
        return analysis_obj

def add_to_history(analysis: dict) -> bool:
    """
    Save analysis to Google Sheet (or session fallback).

    ADDITIVE end-state wiring:
      - If a baseline source cache exists, build & store:
          * evidence_records (structured, cached)
          * metric_anchors (baseline metrics anchored to evidence)
      - Prevent Google Sheets 50,000-char single-cell limit errors by shrinking only
        the JSON payload written into the single "analysis json" cell when necessary.

    Backward compatible:
      - Only adds keys; does not remove existing fields.
      - Never blocks saving if enrichment fails.
      - If Sheets unavailable, falls back to session_state.
    """

    # REFACTOR36: harden against None callers (prevents NoneType.get crashes)
    if not isinstance(analysis, dict):
        return False


    try:
        analysis = _emit_metric_anchors_in_analysis_payload(analysis)
    except Exception:
        pass


    import json
    import re
    import streamlit as st
    from datetime import datetime

    SHEETS_CELL_LIMIT = 50000

    # - Added primary_response.baseline_sources_cache as extra fallback
    baseline_cache = (
        analysis.get("baseline_sources_cache")
        or (analysis.get("primary_response", {}) or {}).get("baseline_sources_cache")
        or (analysis.get("results", {}) or {}).get("baseline_sources_cache")
        or (analysis.get("results", {}) or {}).get("source_results")
    )

    def _build_evidence_records_from_baseline_cache(baseline_cache_obj):
        records = []
        if not isinstance(baseline_cache_obj, list):
            return records

        # helper: safe sha1 fallback if needed
        def _sha1(s: str) -> str:
            try:
                import hashlib
                return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()
            except Exception:
                return ""

        for sr in baseline_cache_obj:
            if not isinstance(sr, dict):
                continue
            url = sr.get("url") or ""
            fp = sr.get("fingerprint")
            fetched_at = sr.get("fetched_at")

            nums = sr.get("extracted_numbers") or []
            clean_nums = []

            if isinstance(nums, list):
                for n in nums:
                    if not isinstance(n, dict):
                        continue

                    # optional canonicalization hook
                    try:
                        fn = globals().get("canonicalize_numeric_candidate")
                        if callable(fn):
                            n = fn(dict(n))
                    except Exception:
                        pass
                        n = dict(n)

                    raw = (n.get("raw") or "").strip()
                    ctx = (n.get("context_snippet") or n.get("context") or "").strip()
                    anchor_hash = n.get("anchor_hash") or _sha1(f"{url}|{raw}|{ctx[:240]}")

                    clean_nums.append({
                        "value": n.get("value"),
                        "unit": n.get("unit"),
                        "unit_tag": n.get("unit_tag"),
                        "unit_family": n.get("unit_family"),
                        "base_unit": n.get("base_unit"),
                        "multiplier_to_base": n.get("multiplier_to_base"),
                        "value_norm": n.get("value_norm"),

                        "raw": raw,
                        "context_snippet": ctx[:240],
                        "anchor_hash": anchor_hash,
                "candidate_id": hashlib.sha1(str(anchor_hash or "").encode("utf-8")).hexdigest()[:16] if anchor_hash else None,

            # - candidate_id is a stable short id derived from anchor_hash
            # - anchor_basis documents what the anchor_hash was built from
            "candidate_id": (str(anchor_hash)[:16] if anchor_hash else None),
            "anchor_basis": "url|raw|context",
                        "source_url": n.get("source_url") or url,

                        "start_idx": n.get("start_idx"),
                        "end_idx": n.get("end_idx"),

                        "is_junk": bool(n.get("is_junk")) if isinstance(n.get("is_junk"), bool) else False,
                        "junk_reason": n.get("junk_reason") or "",

                        "measure_kind": n.get("measure_kind"),
                        "measure_assoc": n.get("measure_assoc"),
                    })

            # stable ordering (prefer your helper if present)
            try:
                if "sort_snapshot_numbers" in globals() and callable(globals()["sort_snapshot_numbers"]):
                    clean_nums = sort_snapshot_numbers(clean_nums)
                else:
                    clean_nums = sorted(
                        clean_nums,
                        key=lambda x: (str(x.get("anchor_hash") or ""), str(x.get("raw") or ""))
                    )
            except Exception:
                pass

            records.append({
                "url": url,
                "fetched_at": fetched_at,
                "fingerprint": fp,
                "numbers": clean_nums,
            })

        # stable ordering (prefer helper if present)
        try:
            if "sort_evidence_records" in globals() and callable(globals()["sort_evidence_records"]):
                records = sort_evidence_records(records)
            else:
                records = sorted(records, key=lambda r: str(r.get("url") or ""))
        except Exception:
            return records

    def _build_metric_anchors(primary_metrics_canonical, evidence_records):
        """
        Build a deterministic metric_anchors mapping for drift=0.

        PATCH AI1 (ADDITIVE): Anchor integrity
        - Prefer the anchor_hash/candidate_id already chosen during analysis (metric["evidence"]).
        - Fall back to scanning evidence_records for a candidate with the same anchor_hash/candidate_id.
        - As a last resort, pick a best candidate deterministically by (abs(value_norm-target), context length).
        - NEVER invent anchors; if no usable candidate, omit the anchor for that metric.
        """
        anchors = {}
        if not isinstance(primary_metrics_canonical, dict) or not primary_metrics_canonical:
            return anchors
        if not isinstance(evidence_records, list):
            evidence_records = []

        import hashlib

        def _sha1(s: str) -> str:
            try:
                return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()
            except Exception:
                return ""

        def _ensure_anchor_fields(c: dict, source_url: str = "") -> dict:
            c = c if isinstance(c, dict) else {}
            # context snippet normalization
            ctx = c.get("context_snippet") or c.get("context") or ""
            if isinstance(ctx, str):
                ctx = ctx.strip()[:240]
            else:
                ctx = ""
            raw = c.get("raw")
            if raw is None:
                # stable raw representation
                v = c.get("value_norm") if c.get("value_norm") is not None else c.get("value")
                u = c.get("base_unit") or c.get("unit") or ""
                raw = f"{v}{u}"
            raw = str(raw)[:120]

            ah = c.get("anchor_hash") or c.get("anchor") or ""
            if not ah:
                ah = _sha1(f"{source_url}|{raw}|{ctx}")
                if ah:
                    c["anchor_hash"] = ah

            if not c.get("candidate_id") and ah:
                c["candidate_id"] = str(ah)[:16]

            # keep normalized ctx/source_url for downstream
            if source_url and not c.get("source_url"):
                c["source_url"] = source_url
            if ctx and not c.get("context_snippet"):
                c["context_snippet"] = ctx

            return c

        # Pre-index evidence_records by (anchor_hash, candidate_id)
        anchor_index = {}
        candidate_index = {}
        value_index = {}  # ckey -> list of candidates (for fallback)

        for rec in evidence_records:
            if not isinstance(rec, dict):
                continue
            url = rec.get("source_url") or rec.get("url") or ""
            for c in (rec.get("candidates") or rec.get("extracted_numbers") or []):
                if not isinstance(c, dict):
                    continue
                c = _ensure_anchor_fields(c, url)
                ah = c.get("anchor_hash")
                cid = c.get("candidate_id")
                if ah and ah not in anchor_index:
                    anchor_index[ah] = c
                if cid and cid not in candidate_index:
                    candidate_index[cid] = c

        # Determine anchors per metric
        for ckey, m in primary_metrics_canonical.items():
            if not isinstance(m, dict):
                continue

            # --- Preferred: use the analysis-chosen evidence (integrity) ---
            chosen = None
            ev = m.get("evidence") or []
            if isinstance(ev, list) and ev:
                # pick first usable evidence deterministically
                for e in ev:
                    if not isinstance(e, dict):
                        continue
                    url = e.get("source_url") or e.get("url") or ""
                    e2 = _ensure_anchor_fields(dict(e), url)
                    ah = e2.get("anchor_hash")
                    cid = e2.get("candidate_id")
                    if ah or cid:
                        chosen = e2
                        break

            # --- Fallback 1: resolve by anchor_hash/candidate_id in evidence_records ---
            if isinstance(chosen, dict):
                ah = chosen.get("anchor_hash")
                cid = chosen.get("candidate_id")
                if ah and ah in anchor_index:
                    chosen = dict(anchor_index[ah])
                elif cid and cid in candidate_index:
                    chosen = dict(candidate_index[cid])

            # --- Fallback 2: deterministic best-by-value in the same source_url (if any) ---
            if not isinstance(chosen, dict) or not (chosen.get("anchor_hash") or chosen.get("candidate_id")):
                # gather candidates from evidence_records that match the metric's preferred source_url (if known)
                preferred_url = ""
                try:
                    if isinstance(ev, list) and ev:
                        preferred_url = str((ev[0] or {}).get("source_url") or (ev[0] or {}).get("url") or "")
                except Exception:
                    pass
                    preferred_url = ""

                target = m.get("value_norm")
                try:
                    target = float(target) if target is not None else None
                except Exception:
                    pass
                    target = None

                pool = []
                for rec in evidence_records:
                    if not isinstance(rec, dict):
                        continue
                    url = str(rec.get("source_url") or rec.get("url") or "")
                    if preferred_url and url != preferred_url:
                        continue
                    for c in (rec.get("candidates") or rec.get("extracted_numbers") or []):
                        if not isinstance(c, dict):
                            continue
                        cc = _ensure_anchor_fields(dict(c), url)
                        pool.append(cc)

                if pool:
                    def _score(cc):
                        ctx = cc.get("context_snippet") or ""
                        try:
                            v = cc.get("value_norm")
                            v = float(v) if v is not None else None
                        except Exception:
                            pass
                            v = None
                        dv = abs(v - target) if (v is not None and target is not None) else 1e30
                        return (dv, -len(str(ctx)), str(cc.get("anchor_hash") or ""), str(url))
                    pool.sort(key=_score)
                    chosen = pool[0]

            if not isinstance(chosen, dict):
                continue

            # emit anchor record (stable shape)
            anchors[ckey] = {
                "canonical_key": ckey,
                "anchor_hash": chosen.get("anchor_hash"),
# Why:
# - Provides a stable, inspectable signature tying the anchor to a specific
#   candidate (url + anchor_hash + value_norm + base_unit).
# - Helps detect silent anchor drift across analysis/evolution.
"anchor_integrity": {
    "candidate_id": chosen.get("candidate_id"),
    "value_norm": chosen.get("value_norm"),
    "base_unit": chosen.get("base_unit") or chosen.get("unit"),
    "fingerprint": chosen.get("fingerprint"),
    "integrity_hash": _es_hash_text(
        f"{ckey}|{chosen.get('anchor_hash')}|{chosen.get('source_url') or chosen.get('url') or ''}|{chosen.get('value_norm')}|{chosen.get('base_unit') or chosen.get('unit') or ''}"
    ) if callable(globals().get("_es_hash_text")) else None,
},
                "candidate_id": chosen.get("candidate_id"),
                "source_url": chosen.get("source_url") or chosen.get("url"),
                "context_snippet": chosen.get("context_snippet") or chosen.get("context"),
                "anchor_confidence": chosen.get("anchor_confidence") or chosen.get("confidence"),
            }

        # deterministic ordering (stable JSON)
        try:
            anchors = dict(sorted(anchors.items(), key=lambda kv: str(kv[0])))
        except Exception:
            return anchors
        def _tokenize(s: str):
            return [t for t in re.findall(r"[a-z0-9]+", (s or "").lower()) if len(t) > 2]

        def _to_float(x):
            try:
                return float(x)
            except Exception:
                return None

        # - Needed because many currency metrics appear as magnitude-tagged numbers (e.g., "40.7M")
        #   with currency implied in nearby context ("USD", "revenue", "$", etc.)
        def _has_currency_evidence(raw: str, ctx: str) -> bool:
            r = (raw or "")
            c = (ctx or "").lower()
            if any(s in r for s in ["$", "S$", "€", "£"]):
                return True
            if any(code in c for code in [" usd", "sgd", " eur", " gbp", " aud", " cad", " jpy", " cny", " rmb"]):
                return True
            strong_kw = [
                "revenue", "turnover", "valuation", "valued at", "market value", "market size",
                "sales value", "net profit", "operating profit", "gross profit",
                "ebitda", "earnings", "income", "capex", "opex"
            ]
            if any(k in c for k in strong_kw):
                return True
            return False

        # flatten candidates
        all_nums = []
        for rec in evidence_records:
            if not isinstance(rec, dict):
                continue
            for n in (rec.get("numbers") or []):
                if isinstance(n, dict):
                    all_nums.append(n)

        _norm_tag_fn = globals().get("normalize_unit_tag")
        _unit_family_fn = globals().get("unit_family")

        for ckey, m in primary_metrics_canonical.items():
            if not isinstance(m, dict):
                continue

            schema = (metric_schema_frozen or {}).get(ckey) if isinstance(metric_schema_frozen, dict) else None
            expected_family = (schema.get("unit_family") or "").lower().strip() if isinstance(schema, dict) else ""
            expected_unit = (schema.get("unit") or "").strip() if isinstance(schema, dict) else ""
            expected_dim = (schema.get("dimension") or "").lower().strip() if isinstance(schema, dict) else ""

            # tokens: schema keywords + metric name tokens
            toks = []
            if isinstance(schema, dict):
                for k in (schema.get("keywords") or []):
                    toks.extend(_tokenize(str(k)))
            toks.extend(_tokenize(m.get("name") or m.get("original_name") or ""))
            toks = list(dict.fromkeys(toks))[:40]

            best = None
            best_key = None

            m_val = _to_float(m.get("value_norm") if m.get("value_norm") is not None else m.get("value"))

            exp_tag = expected_unit
            try:
                if callable(_norm_tag_fn):
                    exp_tag = _norm_tag_fn(expected_unit)
            except Exception:
                pass

            m_tag = (m.get("unit_tag") or "").strip()

            for cand in all_nums:
                if cand.get("is_junk") is True:
                    continue

                ctx = cand.get("context_snippet") or ""
                c_ut = (cand.get("unit_tag") or "").strip()
                c_fam = (cand.get("unit_family") or "").lower().strip()

                # - prevents leakage when unit_family wasn't populated upstream
                if not c_fam:
                    try:
                        if callable(_unit_family_fn):
                            c_fam = str(_unit_family_fn(c_ut or "") or "").lower().strip()
                    except Exception:
                        pass

                # - helps older snapshots where unit_tag/unit may be empty but raw/context carries scale ("million", "%")
                cand_tag = c_ut
                try:
                    if callable(_norm_tag_fn):
                        cand_tag = _norm_tag_fn(c_ut or cand.get("unit") or cand.get("raw") or ctx)
                except Exception:
                    pass

                # - Currency metrics often appear as magnitude candidates ("40.7M") + currency evidence in context.
                # - We allow cand_fam == "magnitude" for expected_family == "currency" ONLY when currency evidence exists.
                if expected_family in ("percent", "currency", "magnitude", "energy"):
                    if expected_family == "currency":
                        if c_fam not in ("currency", "magnitude"):
                            continue
                        if c_fam == "magnitude" and not _has_currency_evidence(cand.get("raw", ""), ctx):
                            continue
                    else:
                        if (c_fam or "") != expected_family:
                            continue

                # dimension/meaning gate using measure_kind when present (soft but helpful)
                mk = cand.get("measure_kind")
                if expected_dim == "percent" and mk and mk not in ("share_pct", "growth_pct", "percent_other"):
                    continue
                if expected_dim == "currency" and mk and mk == "count_units":
                    continue

                c_tokens = set(_tokenize(ctx))
                overlap = sum(1 for t in toks if t in c_tokens) if toks else 0
                score = overlap / max(1, len(toks))

                bonus = 0.0

                if exp_tag and cand_tag and cand_tag == exp_tag:
                    bonus += 0.07
                # keep a small legacy bonus if exact unit string matches too
                if expected_unit and (str(cand.get("unit") or "").strip() == expected_unit):
                    bonus += 0.03

                # - Only apply when units are comparable (tag match or both use value_norm).
                # - Prevents misleading closeness when one side is normalized and the other isn't.
                c_val = _to_float(cand.get("value_norm") if cand.get("value_norm") is not None else cand.get("value"))
                comparable = False
                if m_tag and cand_tag and m_tag == cand_tag:
                    comparable = True
                elif (m.get("value_norm") is not None) and (cand.get("value_norm") is not None):
                    comparable = True

                if comparable and m_val is not None and c_val is not None:
                    denom = max(1e-9, abs(m_val))
                    rel_err = abs(c_val - m_val) / denom
                    if rel_err <= 0.02:
                        bonus += 0.06
                    elif rel_err <= 0.10:
                        bonus += 0.03

                score = float(score + bonus)

                # stable tie-breaker
                key = (
                    score,
                    str(cand.get("source_url") or ""),
                    str(cand.get("anchor_hash") or ""),
                    str(cand.get("raw") or ""),
                )

                if best_key is None or key > best_key:
                    best_key = key
                    best = cand

            if best and best_key and best_key[0] >= 0.10:
                anchors[ckey] = {
                    "metric_id": ckey,
                    "metric_name": (m.get("name") or m.get("original_name") or ckey),

                    "canonical_key": ckey,
                    "anchor_hash": best.get("anchor_hash"),
                    "source_url": best.get("source_url"),
                    "raw": best.get("raw"),
                    "unit": best.get("unit"),
                    "unit_tag": best.get("unit_tag"),
                    "unit_family": best.get("unit_family"),
                    "base_unit": best.get("base_unit"),
                    "value": best.get("value"),
                    "value_norm": (best.get("value") if best.get("value") is not None else best.get("value_norm")),
                    "measure_kind": best.get("measure_kind"),
                    "measure_assoc": best.get("measure_assoc"),
                    "context_snippet": (best.get("context_snippet") or "")[:220],
                    "anchor_confidence": float(min(100.0, best_key[0] * 100.0)),

                    # - Useful later for evolution/debugging; harmless if missing.
                    "fingerprint": best.get("fingerprint"),
                }
            else:
                anchors[ckey] = {
                    "metric_id": ckey,
                    "metric_name": (m.get("name") or m.get("original_name") or ckey),

                    "canonical_key": ckey,
                    "anchor_hash": None,
                    "source_url": None,
                    "raw": None,
                    "anchor_confidence": 0.0,

                    "fingerprint": None,
                }

        # stable ordering (prefer helper if present)
        try:
            if "sort_metric_anchors" in globals() and callable(globals()["sort_metric_anchors"]):
                ordered = sort_metric_anchors(list(anchors.values()))
                anchors = {
                    a.get("canonical_key"): a
                    for a in ordered
                    if isinstance(a, dict) and a.get("canonical_key")
                }
        except Exception:
            pass

        # Why:
        # - Drift=0 requires prev metrics to carry anchor_hash so diff can compare
        #   prev_anchor_hash vs cur_anchor_hash deterministically.
        # - We ONLY copy existing anchor_hash from anchors map; no fabrication.
        try:
            if isinstance(primary_metrics_canonical, dict) and isinstance(anchors, dict):
                for _ck, _a in anchors.items():
                    if not isinstance(_a, dict):
                        continue
                    _ah = _a.get("anchor_hash") or _a.get("anchor")
                    if not _ah:
                        continue
                    _mrow = primary_metrics_canonical.get(_ck)
                    if isinstance(_mrow, dict) and not _mrow.get("anchor_hash"):
                        _mrow["anchor_hash"] = _ah
        except Exception:
            pass
        return anchors

    try:
        if isinstance(baseline_cache, list) and baseline_cache:
            evidence_records = _build_evidence_records_from_baseline_cache(baseline_cache)

            # - Use CODE_VERSION if available; else keep numeric fallback
            try:
                cv = globals().get("CODE_VERSION")
                analysis.setdefault("evidence_layer_version", cv or 1)
            except Exception:
                pass
                analysis.setdefault("evidence_layer_version", 1)
            analysis.setdefault("evidence_layer_schema_version", 1)

            # stash on analysis (additive)
            analysis["evidence_records"] = evidence_records

            # build anchors using canonical metrics + frozen schema if present
            primary_resp = analysis.get("primary_response") or {}
            if isinstance(primary_resp, dict):
                pmc = primary_resp.get("primary_metrics_canonical") or analysis.get("primary_metrics_canonical") or {}
                schema = primary_resp.get("metric_schema_frozen") or analysis.get("metric_schema_frozen") or {}
            else:
                pmc = analysis.get("primary_metrics_canonical") or {}
                schema = analysis.get("metric_schema_frozen") or {}

            metric_anchors = _build_metric_anchors(pmc, schema, evidence_records)
            # Why:
            # - Evolution (and diff) expects anchors to be discoverable without guessing.
            # - Some storage paths wrap/summarize analysis objects; we persist anchors
            #   at multiple stable locations to survive those wrappers.
            # Determinism:
            # - Anchors are derived only from existing evidence_records / schema / pmc.
            # - No re-fetching; no heuristic matching.
            try:
                if isinstance(metric_anchors, dict) and metric_anchors:
                    # Top-level (preferred)
                    if not isinstance(analysis.get("metric_anchors"), dict):
                        analysis["metric_anchors"] = metric_anchors

                    # Under primary_response (common for older shapes)
                    pr = analysis.get("primary_response")
                    if isinstance(pr, dict) and not isinstance(pr.get("metric_anchors"), dict):
                        pr["metric_anchors"] = metric_anchors

                    # Under results (some evolution lookups)
                    res = analysis.get("results")
                    if isinstance(res, dict) and not isinstance(res.get("metric_anchors"), dict):
                        res["metric_anchors"] = metric_anchors

                    # Lightweight debug hint for wrappers
                    try:
                        dbg = analysis.get("debug")
                        if not isinstance(dbg, dict):
                            dbg = {}
                            analysis["debug"] = dbg
                        dbg.setdefault("metric_anchor_count", int(len(metric_anchors)))
                    except Exception:
                        pass
            except Exception:
                pass

            analysis["metric_anchors"] = metric_anchors
            # Why:
            # - Detect duplicate anchor_hash values across canonical keys.
            # - Detect missing anchor_hash on anchors and on baseline canonical metrics.
            # - Provide non-breaking debug/audit fields for drift investigations.
            # Notes:
            # - Purely additive; does not mutate anchors beyond attaching audit metadata.
            try:
                if isinstance(analysis, dict):
                    _ma = analysis.get('metric_anchors')
                    _pmc = analysis.get('primary_metrics_canonical')
                    _dup = {}  # anchor_hash -> [canonical_key,...]
                    _missing_anchor = []
                    _missing_metric_anchor = []
                    if isinstance(_ma, dict):
                        for _ck, _a in _ma.items():
                            if not isinstance(_a, dict):
                                _missing_anchor.append(str(_ck))
                                continue
                            _ah = _a.get('anchor_hash') or _a.get('anchor')
                            if not _ah:
                                _missing_anchor.append(str(_ck))
                                continue
                            _ah = str(_ah)
                            _dup.setdefault(_ah, []).append(str(_ck))
                    # anchors missing on baseline metrics (best-effort diagnostic)
                    if isinstance(_pmc, dict):
                        for _ck, _m in _pmc.items():
                            if not isinstance(_m, dict):
                                continue
                            if not (_m.get('anchor_hash') or _m.get('anchor') or _m.get('candidate_id')):
                                _missing_metric_anchor.append(str(_ck))
                    _dup_only = {k: v for k, v in _dup.items() if isinstance(v, list) and len(v) > 1}
                    analysis['anchor_integrity_audit'] = {
                        'anchor_count': int(len(_ma)) if isinstance(_ma, dict) else 0,
                        'duplicate_anchor_hash_count': int(len(_dup_only)),
                        'duplicate_anchor_hash_examples': dict(list(_dup_only.items())[:10]) if _dup_only else {},
                        'missing_anchor_hash_count': int(len(_missing_anchor)),
                        'missing_anchor_hash_examples': _missing_anchor[:20],
                        'metrics_missing_any_anchor_id_count': int(len(_missing_metric_anchor)),
                        'metrics_missing_any_anchor_id_examples': _missing_metric_anchor[:20],
                    }
            except Exception:
                pass

    # Existing Google Sheet save behavior (guarded)
    except Exception:
        pass

    # Why:
    # - HistoryFull replay/diff requires Analysis baseline canonical values (not just schema).
    # - Prior runs persisted metric_schema_frozen but had no primary_metrics_canonical map.
    # - Option B: compute once during Analysis and persist the decided map.
    # Behavior:
    # - If primary_metrics_canonical is missing/empty, rebuild deterministically from
    #   metric_schema_frozen + baseline_sources_cache using the schema-anchored rebuild.
    # - Mirror into analysis.primary_response.primary_metrics_canonical so Sheets minimal
    #   fallback still carries it.
    # - Emit debug counts for verification.
    try:
        if isinstance(analysis, dict):
            _already = None
            if isinstance(analysis.get('primary_metrics_canonical'), dict) and analysis.get('primary_metrics_canonical'):
                _already = 'analysis.primary_metrics_canonical'
            elif isinstance(analysis.get('primary_response'), dict) and isinstance(analysis['primary_response'].get('primary_metrics_canonical'), dict) and analysis['primary_response'].get('primary_metrics_canonical'):
                _already = 'analysis.primary_response.primary_metrics_canonical'
            elif isinstance(analysis.get('results'), dict) and isinstance(analysis['results'].get('primary_metrics_canonical'), dict) and analysis['results'].get('primary_metrics_canonical'):
                _already = 'analysis.results.primary_metrics_canonical'

            if not _already:
                _pool = None
                try:
                    if isinstance(analysis.get('baseline_sources_cache'), list):
                        _pool = analysis.get('baseline_sources_cache')
                    elif isinstance(analysis.get('results'), dict) and isinstance(analysis['results'].get('baseline_sources_cache'), list):
                        _pool = analysis['results'].get('baseline_sources_cache')
                except Exception:
                    _pool = None

                _schema_ok = isinstance(analysis.get('metric_schema_frozen'), dict) and bool(analysis.get('metric_schema_frozen'))

                _rebuilt = None
                _diag = None
                if _schema_ok and isinstance(_pool, list) and _pool:
                    try:
                        _rebuilt, _diag = _fix2d9_schema_anchored_rebuild_current_metrics_v1(
                            analysis,
                            _pool,
                            web_context=analysis.get('web_context') if isinstance(analysis.get('web_context'), dict) else None,
                        )

                        # FIX2D86: sanitize rebuilt baseline PMC so __percent keys cannot bind bare year tokens (e.g., 2040)
                        try:
                            if isinstance(_rebuilt, dict) and _rebuilt:
                                _schema_fix2d86 = analysis.get("metric_schema_frozen") if isinstance(analysis.get("metric_schema_frozen"), dict) else {}
                                _rebuilt2, _dbg_fix2d86 = _fix2d86_sanitize_pmc_percent_year_tokens_v1(
                                    pmc=_rebuilt,
                                    metric_schema_frozen=_schema_fix2d86,
                                    label="fix2d75_materialize_pmc",
                                )
                                _rebuilt = _rebuilt2
                                try:
                                    analysis.setdefault("debug", {})
                                    if isinstance(analysis.get("debug"), dict):
                                        analysis["debug"]["fix2d86_percent_year_token_sanitize_materialize"] = _dbg_fix2d86
                                except Exception:
                                    pass
                        except Exception:
                            pass


                        # FIX2D86: sanitize rebuilt baseline PMC so __percent keys cannot bind bare year tokens (e.g., 2040)
                        try:
                            if isinstance(_rebuilt, dict) and _rebuilt:
                                _schema_fix2d86 = analysis.get("metric_schema_frozen") if isinstance(analysis.get("metric_schema_frozen"), dict) else {}
                                _rebuilt2, _dbg_fix2d86 = _fix2d86_sanitize_pmc_percent_year_tokens_v1(
                                    pmc=_rebuilt,
                                    metric_schema_frozen=_schema_fix2d86,
                                    label="fix2d75_materialize_pmc",
                                )
                                _rebuilt = _rebuilt2
                                try:
                                    analysis.setdefault("debug", {})
                                    if isinstance(analysis.get("debug"), dict):
                                        analysis["debug"]["fix2d86_percent_year_token_sanitize_materialize"] = _dbg_fix2d86
                                except Exception:
                                    pass
                        except Exception:
                            pass


                        # FIX2D86: sanitize rebuilt baseline PMC so __percent keys cannot bind bare year tokens (e.g., 2040)
                        try:
                            if isinstance(_rebuilt, dict) and _rebuilt:
                                _schema_fix2d86 = analysis.get("metric_schema_frozen") if isinstance(analysis.get("metric_schema_frozen"), dict) else {}
                                _rebuilt2, _dbg_fix2d86 = _fix2d86_sanitize_pmc_percent_year_tokens_v1(
                                    pmc=_rebuilt,
                                    metric_schema_frozen=_schema_fix2d86,
                                    label="fix2d75_materialize_pmc",
                                )
                                _rebuilt = _rebuilt2
                                try:
                                    analysis.setdefault("debug", {})
                                    if isinstance(analysis.get("debug"), dict):
                                        analysis["debug"]["fix2d86_percent_year_token_sanitize_materialize"] = _dbg_fix2d86
                                except Exception:
                                    pass
                        except Exception:
                            pass

                    except Exception:
                        _rebuilt, _diag = (None, None)

                # REFACTOR35: guard against schema-only rebuilds leaking debug keys into PMC
                # Keep only keys that exist in the frozen schema.
                try:
                    _schema_keys = set((analysis.get('metric_schema_frozen') or {}).keys()) if isinstance(analysis.get('metric_schema_frozen'), dict) else set()
                    if isinstance(_rebuilt, dict) and _schema_keys:
                        _rebuilt = {k: v for (k, v) in _rebuilt.items() if isinstance(k, str) and k in _schema_keys and isinstance(v, dict)}
                except Exception:
                    pass

                if isinstance(_rebuilt, dict) and _rebuilt:
                    try:
                        analysis['primary_metrics_canonical'] = dict(_rebuilt)
                    except Exception:
                        pass
                    try:
                        analysis.setdefault('primary_response', {})
                        if isinstance(analysis.get('primary_response'), dict):
                            analysis['primary_response']['primary_metrics_canonical'] = dict(_rebuilt)
                    except Exception:
                        pass
                    try:
                        analysis.setdefault('results', {})
                        if isinstance(analysis.get('results'), dict):
                            analysis['results']['primary_metrics_canonical'] = dict(_rebuilt)
                    except Exception:
                        pass

                # Debug
                try:
                    analysis.setdefault('debug', {})
                    if isinstance(analysis.get('debug'), dict):
                        analysis['debug']['fix2d75_materialize_pmc'] = {
                            'had_existing': bool(_already),
                            'existing_source': str(_already or ''),
                            'schema_present': bool(_schema_ok),
                            'pool_count': int(len(_pool)) if isinstance(_pool, list) else 0,
                            'rebuilt_count': int(len(_rebuilt)) if isinstance(_rebuilt, dict) else 0,
                            'rebuilt_diag': _diag if isinstance(_diag, dict) else {},
                        }
                except Exception:
                    pass
            else:
                try:
                    analysis.setdefault('debug', {})
                    if isinstance(analysis.get('debug'), dict):
                        analysis['debug']['fix2d75_materialize_pmc'] = {
                            'had_existing': True,
                            'existing_source': str(_already),
                        }
                except Exception:
                    pass
    except Exception:
        pass

    # Why:
    # - Drift=0 depends on analysis and evolution sharing the SAME anchor IDs.
    # - Some downstream code paths expect anchor_hash on the metric row itself
    #   and/or inside evidence entries (not only in analysis["metric_anchors"]).
    # - This patch copies existing anchor metadata only (no fabrication, no refetch).
    try:
        import re
        import hashlib

        def _norm_ctx(s: str) -> str:
            try:
                return re.sub(r"\s+", " ", (s or "").strip())
            except Exception:
                return (s or "").strip()

        def _compute_anchor_hash_fallback(url: str, ctx: str) -> str:
            try:
                u = (url or "").strip()
                c = _norm_ctx(ctx or "")
                if not u or not c:
                    return ""
                return hashlib.sha1((u + "||" + c).encode("utf-8")).hexdigest()[:16]
            except Exception:
                return ""

        def _compute_anchor_hash(url: str, ctx: str) -> str:
            try:
                fn = globals().get("compute_anchor_hash")
                if callable(fn):
                    return str(fn(url, ctx) or "")
            except Exception:
                return _compute_anchor_hash_fallback(url, ctx)

        # Locate canonical metrics dict (prefer primary_response)
        _pmc = None
        _pr0 = analysis.get("primary_response") if isinstance(analysis, dict) else None
        if isinstance(_pr0, dict) and isinstance(_pr0.get("primary_metrics_canonical"), dict):
            _pmc = _pr0.get("primary_metrics_canonical")
        if _pmc is None and isinstance(analysis, dict) and isinstance(analysis.get("primary_metrics_canonical"), dict):
            _pmc = analysis.get("primary_metrics_canonical")

        if isinstance(metric_anchors, dict) and isinstance(_pmc, dict):
            for _ckey, _a in metric_anchors.items():
                if not isinstance(_ckey, str) or not _ckey:
                    continue
                if not isinstance(_a, dict) or not _a:
                    continue

                _m = _pmc.get(_ckey)
                if not isinstance(_m, dict):
                    continue

                _ah = str(_a.get("anchor_hash") or _a.get("anchor") or "").strip()
                _src = str(_a.get("source_url") or _a.get("url") or "").strip()
                _ctx = _a.get("context_snippet") or _a.get("context") or ""
                _ctx = _ctx.strip() if isinstance(_ctx, str) else ""

                # Copy onto metric row (only if missing)
                if _ah and not _m.get("anchor_hash"):
                    _m["anchor_hash"] = _ah
                if _src and not (_m.get("source_url") or _m.get("url")):
                    _m["source_url"] = _src
                if _ctx and not (_m.get("context_snippet") or _m.get("context")):
                    _m["context_snippet"] = _ctx[:220]

                # Pass through extra metadata if present (additive)
                if _a.get("anchor_confidence") is not None and _m.get("anchor_confidence") is None:
                    _m["anchor_confidence"] = _a.get("anchor_confidence")
                if _a.get("candidate_id") and not _m.get("candidate_id"):
                    _m["candidate_id"] = _a.get("candidate_id")
                if _a.get("fingerprint") and not _m.get("fingerprint"):
                    _m["fingerprint"] = _a.get("fingerprint")

                # Ensure evidence entries carry anchor_hash (deterministic; no new evidence)
                _ev = _m.get("evidence")
                if isinstance(_ev, list) and _ev:
                    for _e in _ev:
                        if not isinstance(_e, dict):
                            continue
                        if _e.get("anchor_hash"):
                            continue
                        _e_url = str(_e.get("url") or _e.get("source_url") or _src or "").strip()
                        _e_ctx = _e.get("context_snippet") or _e.get("context") or _ctx or ""
                        _e_ctx = _e_ctx.strip() if isinstance(_e_ctx, str) else ""
                        _eh = _compute_anchor_hash(_e_url, _e_ctx)
                        if _eh:
                            _e["anchor_hash"] = _eh
    except Exception:
        pass


    def _try_make_sheet_json(obj: dict) -> str:
        try:
            fn = globals().get("make_sheet_safe_json")
            if callable(fn):
                return fn(obj)
        except Exception:
            return json.dumps(obj, ensure_ascii=False, default=str)

    def _shrink_for_sheets(original: dict) -> dict:
        base_copy = dict(original)
        s = _try_make_sheet_json(base_copy)
        if isinstance(s, str) and len(s) <= SHEETS_CELL_LIMIT:
            return base_copy

        reduced = dict(base_copy)
        removed = []

        for k in [
            "evidence_records",
            "baseline_sources_cache",
            "metric_anchors",
            "source_results",
            "web_context",
            "scraped_meta",
            "raw_sources",
            "raw_text",
            "debug",
        ]:
            if k in reduced:
                reduced.pop(k, None)
                removed.append(k)

        reduced.setdefault("_sheet_write", {})
        if isinstance(reduced["_sheet_write"], dict):
            reduced["_sheet_write"]["truncated"] = True
            reduced["_sheet_write"]["removed_keys"] = removed[:50]

        s2 = _try_make_sheet_json(reduced)
        if isinstance(s2, str) and len(s2) <= SHEETS_CELL_LIMIT:
            return reduced

        return {
            "question": original.get("question"),
            "timestamp": original.get("timestamp"),
            "final_confidence": original.get("final_confidence"),
            "question_profile": original.get("question_profile"),
            "primary_response": original.get("primary_response") or {},
            "_sheet_write": {
                "truncated": True,
                "mode": "minimal_fallback",
                "note": "Full analysis too large for single Google Sheets cell (50k limit).",
            },
        }

    # Try Sheets
    try:
        sheet = get_google_sheet()
    except Exception:
        pass
        sheet = None

    if not sheet:
        if "analysis_history" not in st.session_state:
            st.session_state.analysis_history = []
        st.session_state.analysis_history.append(analysis)
        try:
            st.session_state["last_analysis"] = analysis
        except Exception:
            return False

    try:
        analysis_id = generate_analysis_id()


        # - If full baseline_sources_cache exists (list-shaped), store it outside
        #   Sheets keyed by source_snapshot_hash, and attach pointer fields into
        #   analysis/results for deterministic evolution rehydration.
        # - Pure enrichment only (no refetch, no heuristics).
        try:
            _bsc = None
            if isinstance(analysis, dict):
                _bsc = analysis.get("results", {}).get("baseline_sources_cache") or analysis.get("baseline_sources_cache")


            # rebuild minimal snapshot shape from evidence_records (deterministic).
            # This enables snapshot persistence even when baseline_sources_cache
            # is a summary dict in the main analysis object.
            try:
                if (not isinstance(_bsc, list)) and isinstance(analysis, dict):
                    _er = None
                    # prefer nested results evidence_records first
                    if isinstance(analysis.get("results"), dict):
                        _er = analysis["results"].get("evidence_records")
                    if _er is None:
                        _er = analysis.get("evidence_records")
                    _rebuilt = build_baseline_sources_cache_from_evidence_records(_er)
                    if isinstance(_rebuilt, list) and _rebuilt:
                        _bsc = _rebuilt
            except Exception:
                pass

            if isinstance(_bsc, list) and _bsc:
                _ssh = compute_source_snapshot_hash(_bsc)

                _ssh_v2 = None
                try:
                    _ssh_v2 = compute_source_snapshot_hash_v2(_bsc)
                except Exception:
                    pass
                    _ssh_v2 = None
                if _ssh:
                    # - Persists full baseline_sources_cache in a dedicated worksheet tab.
                    # - Falls back to local snapshot_store file if Sheets snapshot store unavailable.
                    # - Pointer ref stored as 'gsheet:Snapshots:<hash>' when successful.
                    _gs_ref = ""
                    _gs_ref_v2 = ""
                    try:
                        _gs_ref = store_full_snapshots_to_sheet(_bsc, _ssh, worksheet_title="Snapshots")
                        if _ssh_v2 and isinstance(_ssh_v2, str) and _ssh_v2 != _ssh:
                            try:
                                _gs_ref_v2 = store_full_snapshots_to_sheet(_bsc, _ssh_v2, worksheet_title="Snapshots")
                            except Exception:
                                _gs_ref_v2 = ""
                    except Exception:
                        _gs_ref = ""
                        _gs_ref_v2 = ""

                    _ref = ""
                    try:
                        _ref = store_full_snapshots_local(_bsc, _ssh)
                    except Exception:
                        _ref = ""

                    analysis["source_snapshot_hash"] = analysis.get("source_snapshot_hash") or _ssh
                    analysis.setdefault("results", {})
                    if isinstance(analysis["results"], dict):
                        analysis["results"]["source_snapshot_hash"] = analysis["results"].get("source_snapshot_hash") or _ssh
                        try:
                            if _ssh_v2:
                                analysis["results"]["source_snapshot_hash_v2"] = analysis["results"].get("source_snapshot_hash_v2") or _ssh_v2
                                # - Prefer v2 (stable) when present; fall back to legacy v1.
                                try:
                                    _ssh_stable = _ssh_v2 or _ssh
                                    if _ssh_stable:
                                        analysis["source_snapshot_hash_stable"] = analysis.get("source_snapshot_hash_stable") or _ssh_stable
                                        analysis["results"]["source_snapshot_hash_stable"] = analysis["results"].get("source_snapshot_hash_stable") or _ssh_stable
                                except Exception:
                                    pass
                        except Exception:
                            pass

                    if _ref:
                        analysis["snapshot_store_ref"] = analysis.get("snapshot_store_ref") or _ref
                        if isinstance(analysis["results"], dict):
                            analysis["results"]["snapshot_store_ref"] = analysis["results"].get("snapshot_store_ref") or _ref
                            try:
                                if _ssh_v2 and _gs_ref_v2:
                                    analysis["results"]["snapshot_store_ref_v2"] = analysis["results"].get("snapshot_store_ref_v2") or _gs_ref_v2
                            except Exception:
                                pass
                    try:
                        if _gs_ref:
                            analysis["snapshot_store_ref"] = _gs_ref
                            if isinstance(analysis.get("results"), dict):
                                analysis["results"]["snapshot_store_ref"] = _gs_ref
                    except Exception:
                        pass

                    # - Avoid advertising a v2 gsheet ref unless it was actually written successfully.
                    # - Provide a stable ref that always points to a verified store (v2 sheet > v1 sheet > local).
                    # - Emit a compact debug manifest for diagnosing snapshot write failures.
                    try:
                        _stable_ref = (_gs_ref_v2 or _gs_ref or (analysis.get("snapshot_store_ref") if isinstance(analysis, dict) else "") or (_ref if "_ref" in locals() else "") or "")
                        if _stable_ref and isinstance(analysis, dict):
                            analysis["snapshot_store_ref_stable"] = analysis.get("snapshot_store_ref_stable") or _stable_ref
                            if isinstance(analysis.get("results"), dict):
                                analysis["results"]["snapshot_store_ref_stable"] = analysis["results"].get("snapshot_store_ref_stable") or _stable_ref
                    except Exception:
                        pass

                    try:
                        if isinstance(analysis, dict) and isinstance(analysis.get("results"), dict):
                            _dbg = analysis["results"].get("debug")
                            if not isinstance(_dbg, dict):
                                _dbg = {}
                                analysis["results"]["debug"] = _dbg
                            _dbg["snapshot_store_write_v1"] = {
                                "ssh_v1": str(_ssh or ""),
                                "ssh_v2": str(_ssh_v2 or ""),
                                "gs_ref_v1": str(_gs_ref or ""),
                                "gs_ref_v2": str(_gs_ref_v2 or ""),
                                "local_ref_v1": str(_ref or ""),
                                "final_snapshot_store_ref": str((analysis.get("snapshot_store_ref") or "") if isinstance(analysis, dict) else ""),
                                "final_snapshot_store_ref_stable": str((analysis.get("snapshot_store_ref_stable") or "") if isinstance(analysis, dict) else ""),
                            }
                            # - After writing snapshot_store_ref_stable, attempt to load it back
                            #   (sheet or local path) and record basic success/failure stats.
                            # - Best-effort only; never blocks persistence.
                            try:
                                import os as _os, json as _json
                                _stable_ref_rt = str(analysis.get("snapshot_store_ref_stable") or "")
                                _rt = {
                                    "stable_ref": _stable_ref_rt,
                                    "origin": "",
                                    "expected_count": 0,
                                    "loaded_count": 0,
                                    "ok": False,
                                    "note": "",
                                }
                                try:
                                    _rt["expected_count"] = int(len(_bsc)) if isinstance(_bsc, list) else 0
                                except Exception:
                                    _rt["expected_count"] = 0

                                _loaded = None
                                try:
                                    if _stable_ref_rt.startswith("gsheet:Snapshots:"):
                                        _h = ""
                                        try:
                                            _h = _stable_ref_rt.split(":", 2)[-1].strip()
                                        except Exception:
                                            _h = ""
                                        if _h:
                                            _loaded = load_full_snapshots_from_sheet(_h, worksheet_title="Snapshots")
                                            _rt["origin"] = "sheet"
                                except Exception:
                                    _loaded = None

                                try:
                                    if _loaded is None and _stable_ref_rt and _os.path.exists(_stable_ref_rt):
                                        with open(_stable_ref_rt, "r", encoding="utf-8") as _f:
                                            _loaded = _json.load(_f)
                                        _rt["origin"] = "path"
                                except Exception:
                                    _loaded = None

                                try:
                                    _rt["loaded_count"] = int(len(_loaded)) if isinstance(_loaded, list) else 0
                                except Exception:
                                    _rt["loaded_count"] = 0

                                try:
                                    if _rt["loaded_count"] > 0:
                                        if _rt["expected_count"] > 0:
                                            _rt["ok"] = bool(_rt["loaded_count"] == _rt["expected_count"])
                                            if not _rt["ok"]:
                                                _rt["note"] = "count_mismatch"
                                        else:
                                            _rt["ok"] = True
                                    else:
                                        _rt["ok"] = False
                                        _rt["note"] = "empty_or_unreadable"
                                except Exception:
                                    pass

                                _dbg["snapshot_roundtrip_v1"] = _rt
                            except Exception:
                                pass
                    except Exception:
                        pass

        except Exception:
            pass

        payload_for_sheets = _shrink_for_sheets(analysis)
        payload_json = _try_make_sheet_json(payload_for_sheets)

        # - Previous hard truncation produced non-JSON (prefix + random suffix),
        #   causing history loaders (json.loads) to skip the row entirely.
        # - This wrapper guarantees valid JSON even when we must truncate.
        if isinstance(payload_json, str) and len(payload_json) > SHEETS_CELL_LIMIT:
            try:
                payload_json = json.dumps(
                    {
                        "_sheet_write": {
                            "truncated": True,
                            "mode": "hard_truncation_wrapper",
                            "note": "Payload exceeded Google Sheets single-cell limit; stored preview only.",
                        },
                        # keep a preview for debugging/UI; still parseable JSON
                        "preview": payload_json[: max(0, SHEETS_CELL_LIMIT - 600)],
                        "analysis_id": analysis_id,
                        "timestamp": analysis.get("timestamp", _yureeka_now_iso_utc()),
                        "question": (analysis.get("question", "") or "")[:200],
                    },
                    ensure_ascii=False,
                    default=str,
                )
            except Exception:
                pass
                # ultra-safe fallback: still valid JSON
                payload_json = '{"_sheet_write":{"truncated":true,"mode":"hard_truncation_wrapper","note":"json.dumps failed"}}'
        # Why:
        # - Evolution rebuild requires schema/anchors which may be lost in a sheets-safe wrapper
        # - HistoryFull stores the full JSON keyed by analysis_id for later rehydration
        # Behavior:
        # - If payload_json indicates truncation/wrapper OR is very large, write full payload to HistoryFull
        # - Attach a pointer full_store_ref to both analysis and the wrapper object (when possible)
        try:
            is_truncated = False
            try:
                if isinstance(payload_json, str) and ('"_sheet_write"' in payload_json or '"_sheets_safe"' in payload_json):
                    # quick signal; parse if possible
                    try:
                        _pj = json.loads(payload_json)
                        sw = _pj.get("_sheet_write") if isinstance(_pj, dict) else None
                        if isinstance(sw, dict) and sw.get("truncated") is True:
                            is_truncated = True
                        if _pj.get("_sheets_safe") is True:
                            is_truncated = True
                    except Exception:
                        pass
                        # if we can't parse and it's huge, treat as truncated risk
                        if len(payload_json) > 45000:
                            is_truncated = True
                elif isinstance(payload_json, str) and len(payload_json) > 45000:
                    is_truncated = True
            except Exception:
                pass

            if is_truncated:
                full_payload_json = ""
                # FIX2D73: save-side debug counts for baseline canonical metrics persistence
                try:
                    _fix2d73_pmc_count = 0
                    _fix2d73_pmc_src = None
                    if isinstance(analysis, dict):
                        if isinstance(analysis.get("primary_metrics_canonical"), dict) and analysis.get("primary_metrics_canonical"):
                            _fix2d73_pmc_count = len(analysis.get("primary_metrics_canonical") or {})
                            _fix2d73_pmc_src = "analysis.primary_metrics_canonical"
                        elif isinstance(analysis.get("primary_response"), dict) and isinstance(analysis["primary_response"].get("primary_metrics_canonical"), dict) and analysis["primary_response"].get("primary_metrics_canonical"):
                            _fix2d73_pmc_count = len(analysis["primary_response"].get("primary_metrics_canonical") or {})
                            _fix2d73_pmc_src = "analysis.primary_response.primary_metrics_canonical"
                        elif isinstance(analysis.get("results"), dict) and isinstance(analysis["results"].get("primary_metrics_canonical"), dict) and analysis["results"].get("primary_metrics_canonical"):
                            _fix2d73_pmc_count = len(analysis["results"].get("primary_metrics_canonical") or {})
                            _fix2d73_pmc_src = "analysis.results.primary_metrics_canonical"
                    analysis.setdefault("debug", {})
                    if isinstance(analysis.get("debug"), dict):
                        analysis["debug"]["fix2d73_historyfull_save_counts"] = {
                            "primary_metrics_canonical_count": int(_fix2d73_pmc_count),
                            "primary_metrics_canonical_source": str(_fix2d73_pmc_src or ""),
                        }
                except Exception:
                    pass
                try:
                    full_payload_json = json.dumps(analysis, ensure_ascii=False, default=str)
                except Exception:
                    pass
                    full_payload_json = ""

                if full_payload_json:
                    ok_full = write_full_history_payload_to_sheet(analysis_id, full_payload_json, worksheet_title="HistoryFull")
                    if ok_full:
                        ref = f"gsheet:HistoryFull:{analysis_id}"
                        try:
                            analysis["full_store_ref"] = ref
                        except Exception:
                            pass
                        # If payload_json is a wrapper dict, embed ref too
                        try:
                            _pj2 = json.loads(payload_json)
                            if isinstance(_pj2, dict):
                                _pj2["full_store_ref"] = ref
                                sw2 = _pj2.get("_sheet_write")
                                if isinstance(sw2, dict):
                                    sw2["full_store_ref"] = ref
                                    _pj2["_sheet_write"] = sw2
                                payload_json = json.dumps(_pj2, ensure_ascii=False, default=str)
                        except Exception:
                            pass
        except Exception:
            pass


        row = [
            analysis_id,
            analysis.get("timestamp", _yureeka_now_iso_utc()),
            (analysis.get("question", "") or "")[:100],
            str(analysis.get("final_confidence", "")),
            payload_json,
        ]
        sheet.append_row(row, value_input_option="RAW")

        # REFACTOR102: invalidate History get_all_values cache so get_history() sees the newly appended row immediately.
        try:
            _cache = globals().get("_SHEETS_READ_CACHE")
            if isinstance(_cache, dict):
                _ws_title = getattr(sheet, "title", "") or "Sheet1"
                _cache.pop(f"get_all_values:History::{_ws_title}", None)
                # Defensive: drop any other cached History::* reads (worksheet rename / prior cache keys).
                for _k in list(_cache.keys()):
                    if isinstance(_k, str) and _k.startswith("get_all_values:History::"):
                        _cache.pop(_k, None)
        except Exception:
            pass

        # REFACTOR105: mark History as dirty after a successful Sheets append so the next get_history() bypasses cached reads.
        try:
            import time as _time
            st.session_state["_history_dirty_v1"] = float(_time.time())
            st.session_state["_history_dirty_reason_v1"] = "append_row"
        except Exception:
            pass

        # - This prevents Evolution from being blocked when a Sheets write succeeds/fails intermittently.
        try:
            if "analysis_history" not in st.session_state:
                st.session_state.analysis_history = []
            st.session_state.analysis_history.append(analysis)
            # If Sheets succeeded, clear any prior write-failure forcing.
            st.session_state.pop("fix2d66_force_session_history", None)
        except Exception:
            pass

        try:
            st.session_state["last_analysis"] = analysis
        except Exception:
            pass

        return True

    except Exception as e:
        st.warning(f"⚠️ Failed to save to Google Sheets: {e}")
        try:
            globals()["_SHEETS_LAST_WRITE_ERROR"] = str(e)
        except Exception:
            pass
        try:
            st.session_state["fix2d66_force_session_history"] = True
        except Exception:
            pass
        if "analysis_history" not in st.session_state:
            st.session_state.analysis_history = []
        st.session_state.analysis_history.append(analysis)
        try:
            st.session_state["last_analysis"] = analysis
        except Exception:
            pass

        return False


def normalize_unit_tag(unit_str: str) -> str:
    """
    Canonical unit tags used for drift=0 comparisons.
    """
    u = (unit_str or "").strip()
    if not u:
        return ""
    ul = u.lower().replace(" ", "")

    # energy units
    if ul == "twh":
        return "TWh"
    if ul == "gwh":
        return "GWh"
    if ul == "mwh":
        return "MWh"
    if ul == "kwh":
        return "kWh"
    if ul == "wh":
        return "Wh"

    # magnitudes
    if ul in ("t", "trillion", "tn"):
        return "T"
    if ul in ("b", "bn", "billion"):
        return "B"
    if ul in ("m", "mn", "mio", "million"):
        return "M"
    if ul in ("k", "thousand", "000"):
        return "K"


    # composite phrases (e.g. "million units")
    # Some extractors pass unit strings like "million units" as a single tag.
    # Normalize these into the same magnitude tags used elsewhere (M/B/T/K) so
    # unit_family can be typed deterministically.
    if ("unit" in ul) or ("units" in ul):
        if ("trillion" in ul) or ul.startswith("tn") or ul.startswith("t") and ul.endswith("units"):
            return "T"
        if ("billion" in ul) or ul.startswith("bn"):
            return "B"
        if ("million" in ul) or ul.startswith("mn") or ul.startswith("mio"):
            return "M"
        if ("thousand" in ul) or ul.startswith("k"):
            return "K"

    # percent
    if ul in ("%", "pct", "percent"):
        return "%"

    return u


def unit_family(unit_tag: str) -> str:
    """
    Unit family classifier for gating.
    """
    ut = (unit_tag or "").strip()

    if ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
        return "energy"
    if ut == "%":
        return "percent"
    if ut in ("T", "B", "M", "K"):
        return "magnitude"

    return ""

# - Many extracted candidates arrive with unit_family='' due to legacy drift.
# - Provide a stable, analysis/evolution-shared unit_family normalizer.
# - Currency requires context evidence; caller may pass ctx/raw for upgrade.


# - Some sources yield numbers without an attached unit token (unit_tag="").
# - We conservatively infer unit_tag/unit_family from nearby context text.
# - This does NOT weaken FIX2D24 year-blocking; it only restores missing unit metadata.
import re as _re_fix2d2k

# REFACTOR20 (BUGFIX): boundary-aware currency evidence detector
# - Prevent false positives like 'eur'/'euro' inside 'Europe' from upgrading unit_family to currency.
# - Treat currency codes/words as tokens (word-boundary), while allowing symbol markers ($, €, £, ¥).
def _yureeka_has_currency_evidence_v1(text: str) -> bool:
    try:
        t = (text or "").lower()
    except Exception:
        t = str(text or "")
        t = t.lower()

    # strong symbol markers
    if any(sym in t for sym in ("$", "€", "£", "¥")):
        return True

    # common composite tokens
    if "us$" in t or "s$" in t:
        return True

    # currency codes as tokens (avoid substrings inside other words)
    try:
        if _re_fix2d2k.search(r"\b(usd|sgd|eur|gbp|jpy|cny|rmb|aud|cad|inr|krw|chf|hkd|nzd)\b", t):
            return True
        if _re_fix2d2k.search(r"\b(dollar|dollars|euro|euros|pound|pounds|yen|yuan|rupee|rupees)\b", t):
            return True
    except Exception:
        pass

    return False


def infer_unit_tag_from_context(ctx: str, raw: str = ""):
    """Return (unit_tag, unit_family, matched_phrase, excerpt)."""
    c = (ctx or "")
    r = (raw or "")
    cl = (c + " " + r).lower()

    # percent signals
    if "%" in cl or "percent" in cl or "pct" in cl or "market share" in cl or "share" in cl:
        return "%", "percent", "percent/market_share", (c[:160] if c else r[:160])

    # currency signals
    if _yureeka_has_currency_evidence_v1(cl):
        return "USD", "currency", "currency_marker", (c[:160] if c else r[:160])
    if any(k in cl for k in ["revenue", "market size", "market value", "valuation", "valued", "worth", "price"]):
        # keyword-only currency is weaker; require a magnitude word to reduce false positives
        if any(w in cl for w in ["billion", "bn", "million", "mn", "trillion", "tn"]):
            return "USD", "currency", "currency_keyword", (c[:160] if c else r[:160])

    # magnitude / unit-sales style signals
    # detect explicit magnitude words, and also "units".
    if "million" in cl or " mn" in cl or "mio" in cl:
        if "unit" in cl or "vehicle" in cl or "sales" in cl:
            return "M", "magnitude", "million_units", (c[:160] if c else r[:160])
        return "M", "magnitude", "million", (c[:160] if c else r[:160])
    if "billion" in cl or " bn" in cl:
        return "B", "magnitude", "billion", (c[:160] if c else r[:160])
    if "trillion" in cl or " tn" in cl:
        return "T", "magnitude", "trillion", (c[:160] if c else r[:160])
    if "thousand" in cl or "k " in cl:
        return "K", "magnitude", "thousand", (c[:160] if c else r[:160])

    return "", "", "", (c[:160] if c else r[:160])
def normalize_unit_family(unit_tag: str, ctx: str = "", raw: str = "") -> str:
    """
    Deterministic unit-family normalization.

    Goals (FIX2D30):
    - Keep 'M'/'million' candidates in "million units / units sold / chargers" contexts as *magnitude* (or unit-count/sales downstream),
      preventing false 'currency' upgrades that block baseline comparability.
    - Only label a candidate as 'currency' when explicit currency evidence exists (symbols/codes/words), not just generic keywords like "market".

    Notes:
    - This helper is intentionally conservative.
    - Hard unit-family rejection (Diff Panel inference) remains the enforcement point; this just fixes upstream family typing.
    """
    ut = (unit_tag or "").strip()
    fam = unit_family(ut)

    # REFACTOR22: Do not infer unit family from surrounding context for plain yearlike tokens
    # when unit_tag is missing. Year/range endpoints (e.g., '2026–2040') commonly sit next to
    # '%' or currency symbols and can be mis-typed as percent/currency, creating noisy
    # unit inconsistencies in baseline_sources_cache. This is behavior-preserving for binding,
    # since yearlike tokens are not legitimate metric values for __percent/__currency keys.
    if fam == "" and ut == "":
        try:
            import re as _re
            _rs = (raw or "").strip()
            if _re.fullmatch(r"(19|20)\d{2}", _rs or ""):
                # Only allow inference if the raw token itself contains explicit unit evidence.
                if not _re.search(r"[%$€£¥]", _rs):
                    return ""
        except Exception:
            pass


    if fam == "" and ut == "":
        try:
            _itag, ifam, _phr, _ex = infer_unit_tag_from_context(ctx or "", raw or "")
        except Exception:
            pass
            _itag, ifam, _phr, _ex = "", "", "", ""
        if ifam:
            return ifam

    if fam == "magnitude":
        c = ((ctx or "") + " " + (raw or "")).lower()

        # Strong unit-count / unit-sales evidence: keep as magnitude.
        # This blocks the legacy false-positive path where 'M' + 'market' upgraded to currency even when the phrase is "million units".
        unit_evidence = [
            "million units",
            "units sold",
            "unit sales",
            "vehicles sold",
            "ev sales",
            "sales ytd",
            "ytd",
            "chargers",
            "charger",
            "charging points",
            "charging stations",
            "stations",
            "units",
        ]
        has_unit_evidence = any(u in c for u in unit_evidence)

        # Explicit currency markers only (symbols/codes/words)
        # REFACTOR20: boundary-aware currency detection (avoid "Europe" → "euro" false positives)
        has_currency_markers = _yureeka_has_currency_evidence_v1(c)

        if has_unit_evidence and not has_currency_markers:
            return "magnitude"

        if has_currency_markers:
            return "currency"

        # FIX2D30: Remove keyword-only currency upgrades (e.g., 'market', 'valuation') because they cause false positives
        # for phrases like "million units" that also mention "market".

    return fam
def infer_currency_code_from_text_v1(text: str) -> str:
    """
    Best-effort, deterministic currency code inference from raw/context strings.
    Returns an ISO-ish code (e.g., USD/EUR/GBP/JPY/SGD/AUD/CAD/HKD/CNY/KRW/INR) or "".
    """
    try:
        s = (text or "").strip().lower()
        if not s:
            return ""
        # Explicit codes first
        for code in ("usd","eur","gbp","jpy","cny","rmb","aud","cad","sgd","hkd","krw","inr","chf","sek","nok","dkk","nzd"):
            if re.search(r"\b" + re.escape(code) + r"\b", s):
                return "CNY" if code == "rmb" else code.upper()
        # Prefixed symbols
        if "us$" in s or "u.s.$" in s:
            return "USD"
        if "s$" in s:
            return "SGD"
        if "a$" in s:
            return "AUD"
        if "c$" in s:
            return "CAD"
        if "hk$" in s:
            return "HKD"
        # Unicode currency symbols
        if "€" in s:
            return "EUR"
        if "£" in s:
            return "GBP"
        if "¥" in s:
            return "JPY"
        if "₹" in s:
            return "INR"
        if "₩" in s:
            return "KRW"
        # Plain "$" is ambiguous; assume USD only if US markers exist; otherwise leave blank.
        if "$" in s and ("united states" in s or "u.s." in s or " us " in s or " usa" in s):
            return "USD"
    except Exception:
        pass
    return ""


def canonicalize_numeric_candidate(candidate: dict) -> dict:


    """


    Additive: attach canonical numeric fields to a candidate dict.


    Safe to call multiple times.


    PATCH AI4 (ADDITIVE): anchor integrity


    - Ensures anchor_hash + candidate_id are present when possible (derived if missing).


    - Does not change extraction behavior; only enriches fields.


    """


    import hashlib


    if not isinstance(candidate, dict):


        return {}


    v_raw = candidate.get("value_norm")


    v = None


    if v_raw is not None:


        try:


            v = float(v_raw)


        except Exception:
            pass


            v = None


    if v is None:


        try:


            v0 = candidate.get("value")


            if v0 is None:


                return candidate


            v = float(v0)


        except Exception:
            return candidate


    try:


        ut = normalize_unit_tag(candidate.get("unit_tag") or candidate.get("unit") or "")


    except Exception:
        pass


        ut = str(candidate.get("unit_tag") or candidate.get("unit") or "").strip()

    # - Some sources yield numbers without an attached unit token (unit_tag="").
    # - Infer unit_tag/unit_family from nearby context_snippet/raw without weakening FIX2D24.
    # - Attach per-candidate trace: context_unit_backfill_v1.
    ctx_s = (candidate.get("context") or candidate.get("context_snippet") or "")
    raw_s = (candidate.get("raw") or candidate.get("display_value") or "")
    context_unit_backfill_v1 = {"applied": False}
    if (ut or "").strip() == "":
        # Guard: do not infer %/currency units from surrounding context for plain year tokens.
        _skip_backfill_yearlike = False
        try:
            _vi = int(float(v)) if v is not None else None
            if _vi is not None and float(_vi) == float(v) and 1900 <= _vi <= 2100:
                import re as _re
                _rs = (raw_s or "").strip().lower()
                if not _re.search(r"[%$€£¥]|\b(us\$|s\$|usd|sgd|eur|gbp|jpy|aud|cad|chf)\b", _rs):
                    _skip_backfill_yearlike = True
        except Exception:
            _skip_backfill_yearlike = False

        if _skip_backfill_yearlike:
            context_unit_backfill_v1 = {"applied": False, "skipped": True, "reason": "yearlike_no_backfill"}
        else:
            itag, ifam, phr, ex = ("", "", "", "")
            try:
                itag, ifam, phr, ex = infer_unit_tag_from_context(ctx_s, raw_s)
            except Exception:
                pass
                itag, ifam, phr, ex = ("", "", "", "")
            if itag or ifam:
                if itag:
                    ut = itag
                    candidate["unit_tag"] = itag
                context_unit_backfill_v1 = {
                    "applied": True,
                    "matched_phrase": phr,
                    "inferred_unit_family": ifam,
                    "inferred_unit_tag": itag,
                    "window_excerpt": ex,
                }
    try:
        candidate["context_unit_backfill_v1"] = context_unit_backfill_v1
    except Exception:
        pass

    # - Backfill unit_family using unit_tag and currency evidence in context
    # - Correct measure_kind/measure_assoc for currency-like candidates
    # - Attach small per-candidate trace fields for audit
    ctx_s = (candidate.get("context") or candidate.get("context_snippet") or "")
    raw_s = (candidate.get("raw") or candidate.get("display_value") or "")
    existing_fam = (candidate.get("unit_family") or "")

    try:
        fam = normalize_unit_family(ut, ctx=ctx_s, raw=raw_s)
    except Exception:
        pass
        fam = ""

    unit_family_backfilled = False
    if (not existing_fam) and fam:
        candidate["unit_family"] = fam
        unit_family_backfilled = True
    elif fam:
        # keep existing if set, but normalize obvious empties/whitespace
        if str(existing_fam).strip() == "":
            candidate["unit_family"] = fam
            unit_family_backfilled = True

    # REFACTOR24: infer/carry currency_code for currency candidates (used later for unit comparability)
    try:
        if fam == "currency" and not str(candidate.get("currency_code") or "").strip():
            _cc = infer_currency_code_from_text_v1((raw_s or "") + " " + (ctx_s or ""))
            if _cc:
                candidate["currency_code"] = _cc
    except Exception:
        pass

    # currency kind correction (only when evidence exists)
    measure_kind_corrected = False
    measure_assoc_corrected = False
    classifier_reason = ""
    if fam == "currency":
        classifier_reason = "currency_evidence"
        mk0 = str(candidate.get("measure_kind") or "").strip()
        if mk0 in ("", "count_units"):
            candidate["measure_kind"] = "currency"
            measure_kind_corrected = True
        ma0 = str(candidate.get("measure_assoc") or "").strip()
        cxl = (ctx_s or "").lower()
        assoc = None
        if "revenue" in cxl:
            assoc = "revenue"
        elif "market" in cxl and any(k in cxl for k in ["size", "value", "valuation"]):
            assoc = "market_value"
        elif "price" in cxl:
            assoc = "price"
        if assoc and (ma0 in ("", "units")):
            candidate["measure_assoc"] = assoc
            measure_assoc_corrected = True
    elif fam == "percent":
        classifier_reason = "percent_tag"
        mk0 = str(candidate.get("measure_kind") or "").strip()
        if mk0 == "":
            candidate["measure_kind"] = "percent"
            measure_kind_corrected = True

    # attach trace (compact)
    try:
        candidate["unit_measure_classifier_trace_v1"] = {
            "unit_tag": ut,
            "unit_family": candidate.get("unit_family") or fam,
            "context_unit_backfill_applied": bool((candidate.get("context_unit_backfill_v1") or {}).get("applied")),
            "unit_family_backfilled": bool(unit_family_backfilled),
            "measure_kind": candidate.get("measure_kind"),
            "measure_kind_corrected": bool(measure_kind_corrected),
            "measure_assoc": candidate.get("measure_assoc"),
            "measure_assoc_corrected": bool(measure_assoc_corrected),
            "reason": classifier_reason or ("from_unit_tag" if fam else "unknown"),
        }
    except Exception:
        pass


    # If candidate already has base_unit/multiplier_to_base, respect them


    base_unit = candidate.get("base_unit")


    mult = candidate.get("multiplier_to_base")


    try:


        mult = float(mult) if mult is not None else None


    except Exception:
        pass


        mult = None


    # Minimal deterministic mapping (extend as needed)


    if (not base_unit) or (mult is None):


        base_unit = ""


        mult = 1.0


        # percents


        if ut in ("%", "pct"):


            base_unit, mult = "%", 1.0


        # energy


        elif ut == "MWh":


            base_unit, mult = "Wh", 1e6


        elif ut == "kWh":


            base_unit, mult = "Wh", 1e3


        elif ut == "Wh":


            base_unit, mult = "Wh", 1.0


        # power


        elif ut == "GW":


            base_unit, mult = "W", 1e9


        elif ut == "MW":


            base_unit, mult = "W", 1e6


        elif ut == "kW":


            base_unit, mult = "W", 1e3


        elif ut == "W":


            base_unit, mult = "W", 1.0


        # mass


        elif ut in ("Mt", "million_tonnes", "million_tons"):


            base_unit, mult = "t", 1e6


        elif ut in ("kt", "kilo_tonnes", "kilo_tons"):


            base_unit, mult = "t", 1e3


        elif ut in ("t", "tonne", "tonnes", "ton", "tons"):


            base_unit, mult = "t", 1.0


        # count-ish


        elif ut in ("vehicles", "units", "count"):


            base_unit, mult = ut, 1.0


        else:


            # unknown unit: treat as-is


            base_unit, mult = (ut or str(candidate.get("unit") or "").strip()), 1.0


    # Only set defaults to avoid overriding existing enriched fields


    candidate.setdefault("unit_tag", ut)


    candidate.setdefault("unit_family", fam)


    candidate.setdefault("base_unit", base_unit)


    candidate.setdefault("multiplier_to_base", mult)


    # value_norm: if already present, do not overwrite


    if candidate.get("value_norm") is None:


        try:


            candidate["value_norm"] = float(v) * float(mult)


        except Exception:


            pass


    def _sha1(s: str) -> str:


        try:


            return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()


        except Exception:
            return ""


    ah = candidate.get("anchor_hash") or candidate.get("anchor")


    if not ah:


        # attempt deterministic derive if fields exist


        src = candidate.get("source_url") or candidate.get("url") or ""


        ctx = candidate.get("context_snippet") or candidate.get("context") or ""


        if isinstance(ctx, str):


            ctx = ctx.strip()[:240]


        else:


            ctx = ""


        raw = candidate.get("raw")


        if raw is None:


            raw = f"{candidate.get('value')}{candidate.get('unit') or ''}"


        ah = _sha1(f"{src}|{str(raw)[:120]}|{ctx}") if (src or ctx) else ""


        if ah:


            candidate["anchor_hash"] = ah


    if not candidate.get("candidate_id") and ah:


        candidate["candidate_id"] = str(ah)[:16]


    return candidate

def rebuild_metrics_from_snapshots(
    prev_response: dict,
    baseline_sources_cache: list,
    web_context: dict = None
) -> dict:
    """
    Deterministic rebuild using cached snapshots only.
    If sources unchanged, rebuilt metrics converge with analysis.

    Behavior:
      1) Primary: anchor_hash match via prev_response.metric_anchors
      2) Fallback: schema-first deterministic selection when anchor missing
         using metric_schema_frozen + context match + deterministic tie-break.

    NOTE: Dead/unreachable legacy code previously below an early return has been removed
    (explicitly approved).
    """
    import re
    import hashlib

    # - Prevents NameError if typing symbols are not imported globally.
    from typing import Dict, Any, List

    prev_response = prev_response if isinstance(prev_response, dict) else {}

    # - Backward compatible: does not change existing behavior if metric_anchors exists.
    prev_anchors = (
        prev_response.get("metric_anchors")
        or prev_response.get("anchors")
        or {}
    )

    if not isinstance(prev_anchors, dict):
        prev_anchors = {}

    rebuilt: Dict[str, Any] = {}

    metric_schema = prev_response.get("metric_schema_frozen") or {}
    if not isinstance(metric_schema, dict):
        metric_schema = {}

    # - Handles cases where history rows store only a summarized baseline_sources_cache, but full snapshots exist
    #   in the Snapshots sheet (referenced by snapshot_store_ref / source_snapshot_hash).
    try:
        if (not isinstance(baseline_sources_cache, list)) or (isinstance(baseline_sources_cache, dict) and baseline_sources_cache.get("_summary") is True):
            # Prefer already-rehydrated cache on prev_response["results"]["baseline_sources_cache"]
            _maybe = (prev_response.get("results", {}) or {}).get("baseline_sources_cache")
            if isinstance(_maybe, list) and _maybe:
                baseline_sources_cache = _maybe
            else:
                store_ref = prev_response.get("snapshot_store_ref") or (prev_response.get("results", {}) or {}).get("snapshot_store_ref")
                source_hash = prev_response.get("source_snapshot_hash") or (prev_response.get("results", {}) or {}).get("source_snapshot_hash")
                if (not store_ref) and source_hash:
                    store_ref = f"gsheet:Snapshots:{source_hash}"
                if isinstance(store_ref, str) and store_ref.startswith("gsheet:Snapshots:"):
                    _hash = store_ref.split(":")[-1]
                    _full = load_full_snapshots_from_sheet(_hash)
                    if isinstance(_full, list) and _full:
                        baseline_sources_cache = _full
    except Exception:
        pass

    prev_can = prev_response.get("primary_metrics_canonical") or {}
    if not isinstance(prev_can, dict):
        prev_can = {}

    # - Important: some metrics may not have anchors yet; we still must rebuild them
    #   (otherwise evolution "misses" metrics and diffs become unstable).
    metric_key_universe = set()
    try:
        metric_key_universe.update(list(prev_can.keys()))
        metric_key_universe.update(list(prev_anchors.keys()))
    except Exception:
        pass
        metric_key_universe = set(prev_can.keys()) if isinstance(prev_can, dict) else set()

    def _candidate_id(c: dict) -> str:
        try:
            url = str(c.get("source_url") or c.get("url") or "")
            ah = str(c.get("anchor_hash") or "")
            vn = c.get("value_norm")
            bu = str(c.get("base_unit") or c.get("unit") or c.get("unit_tag") or "")
            mk = str(c.get("measure_kind") or "")
            vn_s = ""
            if vn is not None:
                try:
                    vn_s = f"{float(vn):.12g}"
                except Exception:
                    pass
                    vn_s = str(vn)
            s = f"{url}|{ah}|{vn_s}|{bu}|{mk}"
            return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()
        except Exception:
            return ""

    # - Ensures we consistently carry anchor/evidence fields onto rebuilt metrics.
    # - Purely additive; never affects selection logic.
    def _extract_evidence_fields(c: dict) -> dict:
        if not isinstance(c, dict):
            return {}
        ctx = (c.get("context_snippet") or c.get("context") or "").strip()
        return {
            "raw": c.get("raw"),
            "candidate_id": c.get("candidate_id") or _candidate_id(c),
            "context_snippet": ctx[:240] if isinstance(ctx, str) else None,
            "measure_kind": c.get("measure_kind"),
            "measure_assoc": c.get("measure_assoc"),
            "start_idx": c.get("start_idx"),
            "end_idx": c.get("end_idx"),
            # optional passthroughs if upstream provides them
            "fingerprint": c.get("fingerprint"),
        }

    # - Pull anchor_confidence (and any other safe fields) from prev_anchors entry.
    # - Helps diff/UI show confidence without recomputing.
    def _anchor_meta(anchor_obj) -> dict:
        if isinstance(anchor_obj, dict):
            out = {}
            if anchor_obj.get("anchor_confidence") is not None:
                try:
                    out["anchor_confidence"] = float(anchor_obj.get("anchor_confidence"))
                except Exception:
                    pass
            # optional passthroughs if present
            if anchor_obj.get("source_url"):
                out["anchor_source_url"] = anchor_obj.get("source_url")
            if anchor_obj.get("raw"):
                out["anchor_raw"] = anchor_obj.get("raw")
            if anchor_obj.get("candidate_id"):
                out["anchor_candidate_id"] = anchor_obj.get("candidate_id")
            return out
        return {}

    anchor_to_candidate: Dict[str, Dict[str, Any]] = {}
    all_candidates: List[Dict[str, Any]] = []

    for src in baseline_sources_cache or []:
        if not isinstance(src, dict):
            continue
        src_url = src.get("url") or src.get("source_url") or ""

        # - Helps later debugging and “same source” proofs.
        src_fp = src.get("fingerprint")

        for c in (src.get("extracted_numbers") or []):
            if not isinstance(c, dict):
                continue

            # canonicalize if available (safe if repeated)
            try:
                c = canonicalize_numeric_candidate(dict(c))
            except Exception:
                pass
                c = dict(c)

            # ensure stable url carried through
            if not c.get("source_url"):
                c["source_url"] = src_url

            if src_fp and not c.get("fingerprint"):
                c["fingerprint"] = src_fp

            ah = c.get("anchor_hash")
            if ah:
                if ah not in anchor_to_candidate:
                    anchor_to_candidate[ah] = c
                else:
                    old = anchor_to_candidate[ah]
                    if old.get("is_junk") and not c.get("is_junk"):
                        anchor_to_candidate[ah] = c

            all_candidates.append(c)

    def _schema_for_key(metric_key: str) -> dict:
        d = metric_schema.get(metric_key)
        return d if isinstance(d, dict) else {}

    def _expected_from_schema(metric_key: str):
        d = _schema_for_key(metric_key)

        unit_family_s = str(d.get("unit_family") or "").strip().lower()
        dim_s = str(d.get("dimension") or "").strip().lower()
        unit_s = str(d.get("unit") or "").strip()
        name_l = str(d.get("name") or "").lower()

        expected_family = ""
        if unit_family_s in ("percent", "currency", "energy"):
            expected_family = unit_family_s
        else:
            ut = normalize_unit_tag(unit_s)
            if ut == "%":
                expected_family = "percent"
            elif ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
                expected_family = "energy"
            elif dim_s == "currency":
                expected_family = "currency"

        currencyish = (unit_family_s == "currency" or dim_s == "currency")

        expected_kind = None
        if expected_family == "percent":
            if any(k in name_l for k in ["growth", "cagr", "increase", "decrease", "yoy", "qoq", "mom", "rate"]):
                expected_kind = "growth_pct"
            else:
                expected_kind = "share_pct"
        if currencyish or expected_family == "currency":
            expected_kind = "money"
        if expected_kind is None and any(k in name_l for k in [
            "units", "unit sales", "vehicle sales", "vehicles sold", "sold",
            "deliveries", "shipments", "registrations", "volume"
        ]):
            expected_kind = "count_units"

        kw = d.get("keywords")
        schema_keywords = [str(x).strip() for x in kw] if isinstance(kw, list) else []
        schema_keywords = [x for x in schema_keywords if x]

        return expected_family, currencyish, expected_kind, schema_keywords, unit_s

    def _ctx_match_score(tokens: List[str], ctx: str) -> float:
        fn = globals().get("calculate_context_match")
        if callable(fn):
            try:
                return float(fn(tokens, ctx))
            except Exception:
                pass

        c = (ctx or "").lower()
        toks = [t.lower() for t in (tokens or []) if t and len(t) >= 2]
        if not toks:
            return 0.0
        hit = sum(1 for t in toks if t in c)
        return hit / max(1, len(toks))

    def _currency_evidence(raw: str, ctx: str) -> bool:
        r = (raw or "")
        c = (ctx or "").lower()
        if any(s in r for s in ["$", "S$", "€", "£"]):
            return True
        if any(code in c for code in [" usd", "sgd", " eur", " gbp", " aud", " cad", " jpy", " cny", " rmb"]):
            return True
        if any(k in c for k in ["revenue", "turnover", "valuation", "market size", "market value", "profit", "earnings", "ebitda"]):
            return True
        return False

    def _is_yearish_value(v) -> bool:
        try:
            iv = int(float(v))
            return 1900 <= iv <= 2099
        except Exception:
            return False

    # - Keeps metric identity fields (name/canonical_key/dimension/etc.) stable for diffing.
    # - Only overwrites value-ish/source-ish fields with rebuilt candidate data.
    def _overlay_base(metric_key: str, patch: dict) -> dict:
        base = {}
        try:
            if isinstance(prev_can.get(metric_key), dict):
                base = dict(prev_can.get(metric_key) or {})
        except Exception:
            pass
            base = {}
        out = dict(base)
        try:
            if isinstance(patch, dict):
                out.update(patch)
        except Exception:
            return out

    rebuilt_by_anchor = set()

    for metric_key, anchor in prev_anchors.items():
        ah = None
        if isinstance(anchor, dict):
            ah = anchor.get("anchor_hash") or anchor.get("anchor")
        elif isinstance(anchor, str):
            ah = anchor

        if ah and ah in anchor_to_candidate:
            c = anchor_to_candidate[ah]

            # - Keeps canonical identity fields intact for downstream diffs/UI.
            rebuilt[metric_key] = _overlay_base(metric_key, {
                "value": c.get("value"),
                "unit": c.get("unit"),
                "value_norm": c.get("value_norm"),
                "base_unit": c.get("base_unit"),
                "unit_tag": c.get("unit_tag"),
                "unit_family": c.get("unit_family"),
                "anchor_hash": ah,
                "source_url": c.get("source_url"),
                "context_snippet": (c.get("context_snippet") or c.get("context") or "")[:240],
                "measure_kind": c.get("measure_kind"),
                "measure_assoc": c.get("measure_assoc"),
                "rebuild_method": "anchor",

                # - candidate_id used as stable ID for UI/debugging
                # - anchor_confidence helps diff/UI set match_confidence
                **_extract_evidence_fields(c),
                **_anchor_meta(anchor),
            })

            rebuilt_by_anchor.add(metric_key)

    # NOTE: existing loop only iterated prev_anchors.keys(); we keep it as-is,
    for metric_key in prev_anchors.keys():
        if metric_key in rebuilt_by_anchor:
            continue

        expected_family, currencyish, expected_kind, schema_keywords, schema_unit = _expected_from_schema(metric_key)

        # conservative fallback if schema is thin
        if not expected_family and metric_key in prev_can and isinstance(prev_can.get(metric_key), dict):
            pm = prev_can.get(metric_key) or {}
            ut = normalize_unit_tag(pm.get("unit") or schema_unit or "")
            if ut == "%":
                expected_family = "percent"
            elif ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
                expected_family = "energy"

        # tokens for context scoring
        tokens = []
        if schema_keywords:
            tokens = schema_keywords
        else:
            # fallback to build_metric_keywords(schema_name)
            schema_name = ""
            try:
                schema_name = str(_schema_for_key(metric_key).get("name") or "")
            except Exception:
                pass
                schema_name = ""
            fn_bmk = globals().get("build_metric_keywords")
            if callable(fn_bmk):
                try:
                    tokens = fn_bmk(schema_name or metric_key) or []
                except Exception:
                    pass
                    tokens = []
            else:
                tokens = []

        best = None
        best_key = None
        best_score = -1.0

        for c in all_candidates:
            if not isinstance(c, dict):
                continue

            # fallback skips junk (anchor path already handled above)
            if c.get("is_junk") is True:
                continue

            ctx = (c.get("context") or c.get("context_snippet") or "").strip()
            if not ctx:
                continue

            # stop timeline years contaminating non-year metrics
            if expected_family not in ("percent", "energy") and not (currencyish or expected_family == "currency"):
                if (c.get("unit_tag") in ("", None)) and _is_yearish_value(c.get("value")):
                    continue
            # Why:
            #   - Prevent "2024"/"2025" from being selected as metric values (especially count/magnitude_other)
            #   - Applies regardless of expected_family, but only when the candidate is unitless/non-percent.
            # Determinism:
            #   - Pure filtering; stable ordering; no refetch.
            try:
                _vnorm = c.get("value_norm", None)
                if _vnorm is None:
                    _vnorm = c.get("value", None)
                _is_year = _is_yearish_value(_vnorm)
                _cand_ut0 = (c.get("unit_tag") or normalize_unit_tag(c.get("unit") or "") or "").strip()
                _cand_fam0 = (c.get("unit_family") or unit_family(_cand_ut0) or "").strip().lower()
                _mk0 = str(c.get("measure_kind") or "").strip().lower()
                _is_pct0 = bool(c.get("is_percent") or c.get("has_percent") or (_cand_ut0 == "%") or (_cand_fam0 == "percent"))
                _has_curr0 = bool(str(c.get("currency_symbol") or c.get("currency") or "").strip())
                _has_unit_ev0 = bool(_cand_ut0 or _cand_fam0 or _is_pct0 or _has_curr0)
                # year-only guard (unitless, non-percent, non-currency)
                if _is_year and (not _has_unit_ev0) and (not _is_pct0) and (not _has_curr0) and (_mk0 in ("magnitude_other", "count_units", "count", "number", "")):
                    try:
                        _fix41afc5_dbg["rejected_year_only"] = int(_fix41afc5_dbg.get("rejected_year_only", 0) or 0) + 1
                    except Exception:
                        pass
                    continue
                # magnitude_other guard (unitless, non-percent, non-currency)
                if (_mk0 == "magnitude_other") and (not _has_unit_ev0) and (not _is_pct0) and (not _has_curr0):
                    try:
                        _fix41afc5_dbg["rejected_magnitude_other_unitless"] = int(_fix41afc5_dbg.get("rejected_magnitude_other_unitless", 0) or 0) + 1
                    except Exception:
                        pass
                    continue
            except Exception:
                pass

            cand_ut = c.get("unit_tag") or normalize_unit_tag(c.get("unit") or "")
            cand_fam = (c.get("unit_family") or unit_family(cand_ut) or "").strip().lower()
            mk = c.get("measure_kind")

            # unit-family gating
            if expected_family == "percent":
                if cand_fam != "percent" and cand_ut != "%":
                    continue
            elif expected_family == "energy":
                if cand_fam != "energy":
                    continue
            elif currencyish or expected_family == "currency":
                if cand_fam not in ("currency", "magnitude"):
                    continue
                if not _currency_evidence(c.get("raw", ""), ctx):
                    continue
                if mk == "count_units":
                    continue

            # measure-kind gating (only if candidate provides it)
            if expected_kind and mk and mk != expected_kind:
                continue

            # normalize value for ranking
            try:
                c2 = canonicalize_numeric_candidate(dict(c))
            except Exception:
                pass
                c2 = c

            val_norm = c2.get("value_norm")
            if val_norm is None:
                try:
                    val_norm = float(c2.get("value"))
                except Exception:
                    pass
                    continue

            ctx_score = _ctx_match_score(tokens, ctx)
            if ctx_score <= 0.0:
                continue

            url = str(c2.get("source_url") or c2.get("url") or "")
            cid = c2.get("candidate_id") or _candidate_id({**c2, "value_norm": val_norm})

            # deterministic tie-break (max)
            key = (
                float(ctx_score),
                float(val_norm),
                url,
                str(cid),
            )

            if best_key is None or key > best_key:
                best_key = key
                best_score = float(ctx_score)
                best = {**c2, "value_norm": val_norm, "candidate_id": cid}

        if best:
            rebuilt[metric_key] = _overlay_base(metric_key, {
                "value": best.get("value"),
                "unit": best.get("unit") or best.get("unit_tag"),
                "value_norm": best.get("value_norm"),
                "base_unit": best.get("base_unit"),
                "unit_tag": best.get("unit_tag"),
                "unit_family": best.get("unit_family"),
                "anchor_hash": best.get("anchor_hash"),
                "source_url": best.get("source_url") or best.get("url"),
                "context_snippet": (best.get("context_snippet") or best.get("context") or "")[:240],
                "measure_kind": best.get("measure_kind"),
                "measure_assoc": best.get("measure_assoc"),
                "rebuild_method": "schema_fallback",
                "fallback_ctx_score": round(best_score, 6),
                "candidate_id": best.get("candidate_id"),

                # - Ensures candidate_id/raw/context are always present when possible.
                # - Adds anchor_confidence derived from fallback_ctx_score.
                **_extract_evidence_fields(best),
                "anchor_confidence": float(min(100.0, max(0.0, best_score) * 100.0)) if best_score is not None else 0.0,
            })

    # - Your existing fallback loop only iterates prev_anchors.keys().
    # - This loop covers the remaining canonical metrics (prev_can keys) that are missing
    #   from prev_anchors, using the SAME schema-first logic (copied, not refactored).
    # - Additive: does not alter prior behavior for anchored metrics.
    for metric_key in (metric_key_universe or set()):
        if metric_key in rebuilt:
            continue

        expected_family, currencyish, expected_kind, schema_keywords, schema_unit = _expected_from_schema(metric_key)

        if not expected_family and metric_key in prev_can and isinstance(prev_can.get(metric_key), dict):
            pm = prev_can.get(metric_key) or {}
            ut = normalize_unit_tag(pm.get("unit") or schema_unit or "")
            if ut == "%":
                expected_family = "percent"
            elif ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
                expected_family = "energy"

        tokens = []
        if schema_keywords:
            tokens = schema_keywords
        else:
            schema_name = ""
            try:
                schema_name = str(_schema_for_key(metric_key).get("name") or "")
            except Exception:
                pass
                schema_name = ""
            fn_bmk = globals().get("build_metric_keywords")
            if callable(fn_bmk):
                try:
                    tokens = fn_bmk(schema_name or metric_key) or []
                except Exception:
                    pass
                    tokens = []
            else:
                tokens = []

        best = None
        best_key = None
        best_score = -1.0

        for c in all_candidates:
            if not isinstance(c, dict):
                continue
            if c.get("is_junk") is True:
                continue

            ctx = (c.get("context") or c.get("context_snippet") or "").strip()
            if not ctx:
                continue

            if expected_family not in ("percent", "energy") and not (currencyish or expected_family == "currency"):
                if (c.get("unit_tag") in ("", None)) and _is_yearish_value(c.get("value")):
                    continue

            cand_ut = c.get("unit_tag") or normalize_unit_tag(c.get("unit") or "")
            cand_fam = (c.get("unit_family") or unit_family(cand_ut) or "").strip().lower()
            mk = c.get("measure_kind")

            if expected_family == "percent":
                if cand_fam != "percent" and cand_ut != "%":
                    continue
            elif expected_family == "energy":
                if cand_fam != "energy":
                    continue
            elif currencyish or expected_family == "currency":
                if cand_fam not in ("currency", "magnitude"):
                    continue
                if not _currency_evidence(c.get("raw", ""), ctx):
                    continue
                if mk == "count_units":
                    continue

            if expected_kind and mk and mk != expected_kind:
                continue

            try:
                c2 = canonicalize_numeric_candidate(dict(c))
            except Exception:
                pass
                c2 = c

            val_norm = c2.get("value_norm")
            if val_norm is None:
                try:
                    val_norm = float(c2.get("value"))
                except Exception:
                    pass
                    continue

            ctx_score = _ctx_match_score(tokens, ctx)
            if ctx_score <= 0.0:
                continue

            url = str(c2.get("source_url") or c2.get("url") or "")
            cid = c2.get("candidate_id") or _candidate_id({**c2, "value_norm": val_norm})

            key = (
                float(ctx_score),
                float(val_norm),
                url,
                str(cid),
            )

            if best_key is None or key > best_key:
                best_key = key
                best_score = float(ctx_score)
                best = {**c2, "value_norm": val_norm, "candidate_id": cid}

        if best:
            rebuilt[metric_key] = _overlay_base(metric_key, {
                "value": best.get("value"),
                "unit": best.get("unit") or best.get("unit_tag"),
                "value_norm": best.get("value_norm"),
                "base_unit": best.get("base_unit"),
                "unit_tag": best.get("unit_tag"),
                "unit_family": best.get("unit_family"),
                "anchor_hash": best.get("anchor_hash"),
                "source_url": best.get("source_url") or best.get("url"),
                "context_snippet": (best.get("context_snippet") or best.get("context") or "")[:240],
                "measure_kind": best.get("measure_kind"),
                "measure_assoc": best.get("measure_assoc"),
                "rebuild_method": "schema_fallback_no_anchor",
                "fallback_ctx_score": round(best_score, 6),
                "candidate_id": best.get("candidate_id"),

                **_extract_evidence_fields(best),
                "anchor_confidence": float(min(100.0, max(0.0, best_score) * 100.0)) if best_score is not None else 0.0,
            })
        else:
            # stable placeholder (do not fabricate)
            if isinstance(prev_can.get(metric_key), dict):
                rebuilt[metric_key] = _overlay_base(metric_key, {
                    "rebuild_method": "not_found_in_snapshots",

                    "anchor_hash": None,
                    "source_url": None,
                    "context_snippet": None,
                    "raw": None,
                    "candidate_id": None,
                    "anchor_confidence": 0.0,
                })

    # Why:
    #   - Source-anchored evolution is snapshot-gated; if snapshots exist but rebuild fails
    #     (missing anchors/schema mismatch/edge cases), returning {} causes evolution to hard-fail.
    #   - For determinism + drift-0 testing, we prefer a safe fallback that preserves the
    #     canonical metric universe from the previous analysis while emitting an explicit flag.
    #
    # Behavior:
    #   - If 'rebuilt' is empty/non-dict, fall back to prev_response['primary_metrics_canonical'].
    #   - Marks each metric with '_rebuild_fallback_used': True (additive field).
    #   - DOES NOT fabricate new values; it reuses previous canonical values only.
    try:
        if not isinstance(rebuilt, dict) or not rebuilt:
            prev_universe = {}
            if isinstance(prev_response, dict):
                prev_universe = prev_response.get("primary_metrics_canonical") or {}
            if isinstance(prev_universe, dict) and prev_universe:
                rebuilt = {}
                for ck in sorted(prev_universe.keys()):
                    m = prev_universe.get(ck)
                    if isinstance(m, dict):
                        mm = dict(m)
                        mm["_rebuild_fallback_used"] = True
                        # Ensure ES7 fields exist (pure enrichment)
                        mm.setdefault("canonical_key", ck)
                        mm.setdefault("anchor_used", False)
                        mm.setdefault("anchor_confidence", 0.0)
                        rebuilt[ck] = mm
                # Add top-level marker (additive)
                try:
                    rebuilt["_rebuild_status"] = "fallback_prev_primary_metrics_canonical"
                except Exception:
                    pass
    except Exception:
        pass

    try:
        if isinstance(rebuilt, dict):
            rebuilt.setdefault("_fix41afc5_debug", {})
            if isinstance(rebuilt.get("_fix41afc5_debug"), dict):
                rebuilt["_fix41afc5_debug"].update(dict(_fix41afc5_dbg))

    except Exception:
        pass

    # Purpose: Explain precisely why an injected URL may be "admitted" (diag) but not "attempted" (fetch).
    # Emits a small deterministic debug object into web_context['diag_injected_urls'].
    # - No behavior changes; debug only.
    # - Compares: raw extra_urls -> normalized extra_urls -> merged urls list.
    try:
        if isinstance(web_context, dict):
            web_context.setdefault("diag_injected_urls", {})
            if isinstance(web_context.get("diag_injected_urls"), dict):
                _fx2ad_diag = web_context.get("diag_injected_urls")
                _fx2ad_raw_extra = []
                try:
                    _fx2ad_raw_extra = list((web_context or {}).get("extra_urls") or [])
                except Exception:
                    pass
                    _fx2ad_raw_extra = []
                _fx2ad_norm_extra = []
                try:
                    _fx2ad_norm_extra = _inj_diag_norm_url_list(_fx2ad_raw_extra)
                except Exception:
                    pass
                    _fx2ad_norm_extra = []
                _fx2ad_urls_list = []
                try:
                    _fx2ad_urls_list = [str(u or "").strip() for u in (urls or []) if str(u or "").strip()]
                except Exception:
                    pass
                    _fx2ad_urls_list = []
                _fx2ad_norm_urls_list = []
                try:
                    _fx2ad_norm_urls_list = _inj_diag_norm_url_list(_fx2ad_urls_list)
                except Exception:
                    pass
                    _fx2ad_norm_urls_list = []

                # If "admitted" exists (from earlier trace), compare it to extra_urls.
                _fx2ad_admitted_norm = []
                try:
                    _fx2ad_admitted_norm = _inj_diag_norm_url_list(_fx2ad_diag.get("admitted") or _fx2ad_diag.get("extra_urls_admitted") or [])
                except Exception:
                    pass
                    _fx2ad_admitted_norm = []

                _fx2ad_rows = []
                try:
                    for _u in _fx2ad_admitted_norm[:80]:
                        _row = {
                            "url": _u,
                            "in_web_context_extra_urls_norm": (_u in set(_fx2ad_norm_extra)),
                            "in_urls_after_merge_norm": (_u in set(_fx2ad_norm_urls_list)),
                        }
                        # Best-effort reason classification
                        if not _row["in_web_context_extra_urls_norm"]:
                            _row["gate_reason"] = "not_in_web_context_extra_urls"
                        elif not _row["in_urls_after_merge_norm"]:
                            _row["gate_reason"] = "not_merged_into_urls_list"
                        else:
                            _row["gate_reason"] = "present_in_urls_list"
                        _fx2ad_rows.append(_row)
                except Exception:
                    pass
                    _fx2ad_rows = _fx2ad_rows

                _fx2ad_diag["fix2ad_inj_attempt_gating_v1"] = {
                    "raw_extra_urls_count": len(_fx2ad_raw_extra),
                    "norm_extra_urls_count": len(_fx2ad_norm_extra),
                    "urls_list_count": len(_fx2ad_urls_list),
                    "urls_list_norm_count": len(_fx2ad_norm_urls_list),
                    "admitted_norm_count": len(_fx2ad_admitted_norm),
                    "admitted_gate_rows": _fx2ad_rows,
                    "sample_norm_extra_urls": _fx2ad_norm_extra[:20],
                }
    except Exception:
        pass


    try:
        if isinstance(web_context, dict):
            web_context["fix2y_candidate_autopsy_v1"] = _fix2y_autopsy
    except Exception:
        pass

    return rebuilt


# Goal:
#   - Provide a deterministic, evolution-safe metric rebuild that uses ONLY:
#       (a) baseline_sources_cache snapshots (and their extracted_numbers)
#       (b) frozen metric schema (metric_schema_frozen)
#   - No re-fetch, no LLM inference, no heuristic "best guess" beyond schema fields.
#
# Contract:
#   - Returns a dict shaped like primary_metrics_canonical:
#       { canonical_key: { ...metric fields... } }
#   - Deterministic tie-break ordering.


#   - Enforce that ANY candidate flagged as junk is excluded from:
#       * candidate indexing
#       * candidate scoring
#       * final metric assignment
#   - Additionally, suppress "year-like" unitless tokens (e.g., 2024/2025) for
#     non-year metrics (currency/percent/rate/ratio/growth/etc.) to prevent
#     year fixation during evolution.
#   - Purely deterministic: no LLM, no refetch, no heuristics outside schema cues.

def _candidate_disallowed_for_metric(_cand: dict, _spec: dict = None) -> bool:
    """Return True if a snapshot candidate must not be used to assign a metric value."""
    if not isinstance(_cand, dict):
        return True

    # 1) Hard exclusion: explicit junk flags / reasons from extraction phase
    if _cand.get("is_junk") is True:
        return True
    jr = str(_cand.get("junk_reason") or "").strip().lower()
    if jr:
        # If a junk_reason exists, treat it as non-selectable deterministically.
        return True

    # 2) Deterministic anti-year-fixation: unitless year-like tokens are disallowed
    #    for most numeric metrics (unless schema clearly indicates a "year" metric).
    try:
        v = _cand.get("value_norm", _cand.get("value"))
        unitish = str(_cand.get("base_unit") or _cand.get("unit_tag") or _cand.get("unit") or "").strip()
        if unitish == "" and isinstance(v, (int, float)):
            if abs(float(v) - round(float(v))) < 1e-9:
                vi = int(round(float(v)))
                if 1900 <= vi <= 2100:
                    if isinstance(_spec, dict):
                        nm = str(_spec.get("name") or "").lower()
                        cid = str(_spec.get("canonical_id") or _spec.get("canonical_key") or "").lower()
                        kws = _spec.get("keywords") or []
                        kws_s = " ".join([str(k).lower() for k in kws]) if isinstance(kws, list) else str(kws).lower()

                        # Allow explicit year metrics
                        if ("year" in nm) or ("year" in cid) or ("founded" in nm) or ("since" in nm) or ("year" in kws_s):
                            return False

                        uf = str(_spec.get("unit_family") or "").lower().strip()
                        ut = str(_spec.get("unit_tag") or _spec.get("unit") or "").lower().strip()

                        # For common non-year metric families, exclude year-like tokens.
                        if uf in ("currency", "percent", "rate", "ratio", "growth", "share"):
                            return True
                        if "%" in ut:
                            return True
                        if any(w in nm for w in ("cagr", "revenue", "growth", "market", "sales", "profit", "margin", "volume")):
                            return True

                    # Default: unitless year-like token is not a valid metric value.
                    return True
    except Exception:
        return False


# Unit-family + scale eligibility guardrails for schema-only rebuild
# and unit-mismatch detection for Diff Panel V2.
#
# Motivation (from REFACTOR02 JSONs):
# - A currency token like "US$ 996.3bn" was being selected for a magnitude/count schema key
#   (e.g., global_ev_chargers_2040__unit_count), causing nonsensical diffs (B vs M).
# - We fix this *at selection time* and also *at diff time* (so any future regressions are
#   surfaced as unit_mismatch rather than as a bogus increased/decreased classification).
# Determinism:
# - Pure filtering + stable logic; no refetch; no randomness.



def _fix17_candidate_allowed_with_reason(candidate: dict, metric_spec: dict = None, canonical_key: str = None) -> tuple:
    """
    FIX17 helper used by rebuild_metrics_from_snapshots_analysis_canonical_v1.

    Returns (allowed: bool, reason: str).
    Must remain deterministic and lightweight.

    Why:
      - REFACTOR121 shows fix41afc19 rebuild fails with:
        NameError: _fix17_candidate_allowed_with_reason not defined
      - This blocks post-seed canonical rebuild and keeps prod all-null.
    """
    try:
        if not isinstance(candidate, dict):
            return False, "not_dict"

        # Hard junk flags
        if candidate.get("is_junk") is True:
            jr = str(candidate.get("junk_reason") or "junk").strip()
            return False, ("junk:" + jr)[:120]

        jr = str(candidate.get("junk_reason") or "").strip()
        if jr:
            return False, ("junk_reason:" + jr)[:120]

        # Generic candidate disallow rules (years-as-values, etc.)
        try:
            if callable(globals().get("_candidate_disallowed_for_metric")) and isinstance(metric_spec, dict):
                if _candidate_disallowed_for_metric(candidate, metric_spec):
                    return False, "candidate_disallowed_for_metric"
        except Exception:
            pass

        # Unit-family eligibility (currency/percent poisoning guard)
        try:
            if callable(globals().get("_refactor03_candidate_rejected_by_unit_family_v1")) and isinstance(metric_spec, dict):
                if _refactor03_candidate_rejected_by_unit_family_v1(candidate, metric_spec):
                    return False, "unit_family_gate"
        except Exception:
            pass

        # Currency date-fragment filter (day-of-month tokens)
        try:
            if callable(globals().get("_refactor27_candidate_rejected_currency_date_fragment_v1")) and isinstance(metric_spec, dict):
                if _refactor27_candidate_rejected_currency_date_fragment_v1(candidate, metric_spec):
                    return False, "currency_date_fragment"
        except Exception:
            pass

        return True, "ok"

    except Exception as e:
        return False, ("exception:" + str(type(e).__name__))[:120]

def _refactor03_has_currency_evidence_v1(text: str) -> bool:
    try:
        t = (text or "").lower()
        if not t:
            return False
        # Common currency markers; keep conservative to avoid false positives.
        markers = ["us$", "usd", "eur", "€", "gbp", "£", "sgd", "s$", "aud", "cad", "jpy", "¥", "$"]
        return any(m in t for m in markers)
    except Exception:
        return False


def _refactor03_has_percent_evidence_v1(text: str) -> bool:
    try:
        t = (text or "").lower()
        if not t:
            return False
        return ("%" in t) or ("percent" in t) or ("pct" in t)
    except Exception:
        return False


def _refactor03_extract_text_from_metric_v1(metric: dict) -> str:
    try:
        if not isinstance(metric, dict):
            return ""
        parts = []
        # direct fields
        for k in ("raw", "context_snippet", "source_url", "name", "canonical_key"):
            v = metric.get(k)
            if v:
                parts.append(str(v))
        # evidence list
        ev = metric.get("evidence")
        if isinstance(ev, list):
            for e in ev[:5]:
                if isinstance(e, dict):
                    if e.get("raw"):
                        parts.append(str(e.get("raw")))
                    if e.get("context_snippet"):
                        parts.append(str(e.get("context_snippet")))
        return " | ".join([p for p in parts if p])
    except Exception:
        return ""


def _refactor04_unit_evidence_text_from_metric_v1(metric: dict, include_context: bool = False) -> str:
    """Return a *narrow* evidence string for unit-family checks.

    Rationale:
      - context_snippet can include nearby unrelated % / currency (e.g. CAGR lines),
        causing false unit_mismatch for magnitude/count metrics.
      - For magnitude keys we prefer token/raw evidence only.
    """
    try:
        if not isinstance(metric, dict):
            return ""
        parts = []
        # Prefer direct raw-ish fields first
        for k in ("raw", "value_raw", "value_text", "value_str"):
            v = metric.get(k)
            if v:
                parts.append(str(v))
        # Unit tags themselves (helps currency/percent keys when raw is short)
        for k in ("unit_tag", "unit", "base_unit", "unit_family"):
            v = metric.get(k)
            if v:
                parts.append(str(v))
        # Evidence list: always include evidence.raw; include context only if requested
        ev = metric.get("evidence")
        if isinstance(ev, list):
            for e in ev[:5]:
                if not isinstance(e, dict):
                    continue
                if e.get("raw"):
                    parts.append(str(e.get("raw")))
                if include_context and e.get("context_snippet"):
                    parts.append(str(e.get("context_snippet")))
        # Optionally include metric context_snippet (but keep it last)
        if include_context and metric.get("context_snippet"):
            parts.append(str(metric.get("context_snippet")))
        return " | ".join([p for p in parts if p])
    except Exception:
        return ""


def _refactor04_scale_multiplier_from_unit_tag_v1(unit_tag: str) -> float:
    """Convert common magnitude tags to a scale multiplier (K/M/B/T)."""
    try:
        t = str(unit_tag or "").upper().strip()
        if t == "K":
            return 1e3
        if t == "M":
            return 1e6
        if t == "B":
            return 1e9
        if t == "T":
            return 1e12
    except Exception:
        pass
    return 1.0


def _refactor04_get_metric_schema_frozen_v1(obj: dict) -> dict:
    """Best-effort retrieval of metric_schema_frozen from common nesting patterns."""
    try:
        if not isinstance(obj, dict):
            return {}
        if isinstance(obj.get("metric_schema_frozen"), dict):
            return obj.get("metric_schema_frozen") or {}
        pr = obj.get("primary_response")
        if isinstance(pr, dict) and isinstance(pr.get("metric_schema_frozen"), dict):
            return pr.get("metric_schema_frozen") or {}
        res = obj.get("results")
        if isinstance(res, dict):
            if isinstance(res.get("metric_schema_frozen"), dict):
                return res.get("metric_schema_frozen") or {}
            pr2 = res.get("primary_response")
            if isinstance(pr2, dict) and isinstance(pr2.get("metric_schema_frozen"), dict):
                return pr2.get("metric_schema_frozen") or {}
        return {}
    except Exception:
        return {}


def _refactor04_enrich_pmc_units_v1(pmc: dict, prev_response: dict = None) -> dict:
    """Ensure PMC rows carry unit_tag/unit_family/multiplier_to_base (and base_unit) for parity + diffing."""
    try:
        if not isinstance(pmc, dict) or not pmc:
            return pmc
        schema = _refactor04_get_metric_schema_frozen_v1(prev_response) if isinstance(prev_response, dict) else {}
        for ckey, m in list(pmc.items()):
            if not isinstance(m, dict):
                continue
            spec = schema.get(ckey) if isinstance(schema, dict) else None
            spec = spec if isinstance(spec, dict) else {}
            # unit_tag
            ut = m.get("unit_tag") or m.get("base_unit") or m.get("unit") or spec.get("unit_tag") or spec.get("unit") or ""
            ut = str(ut or "").strip()
            # unit_family
            uf = m.get("unit_family") or spec.get("unit_family") or _refactor03_unit_family_from_ckey_v1(ckey)
            uf = str(uf or "").strip()
            # multiplier_to_base (scale)
            mult = m.get("multiplier_to_base")
            if mult is None:
                mult = _refactor04_scale_multiplier_from_unit_tag_v1(ut)
            try:
                mult = float(mult)
            except Exception:
                mult = _refactor04_scale_multiplier_from_unit_tag_v1(ut)

            # write back (do not delete existing fields)
            if ut and not m.get("unit"):
                m["unit"] = ut
            if ut and not m.get("unit_tag"):
                m["unit_tag"] = ut
            if ut and not m.get("base_unit"):
                m["base_unit"] = ut
            if uf and not m.get("unit_family"):
                m["unit_family"] = uf
            if m.get("multiplier_to_base") is None and mult is not None:
                m["multiplier_to_base"] = mult
    except Exception:
        return pmc
    return pmc


def _refactor03_unit_family_from_ckey_v1(canonical_key: str) -> str:
    try:
        ck = str(canonical_key or "").lower()
        if "__currency" in ck:
            return "currency"
        if "__percent" in ck:
            return "percent"
        # treat all "__unit_*" as magnitude/count family
        if "__unit_" in ck:
            return "magnitude"
        return "unknown"
    except Exception:
        return "unknown"


def _refactor03_candidate_rejected_by_unit_family_v1(cand: dict, spec: dict = None) -> bool:
    """Return True if candidate is incompatible with schema's unit family / tag."""
    if not isinstance(cand, dict):
        return True
    if not isinstance(spec, dict):
        return False  # no schema => don't over-filter

    try:
        uf = str(spec.get("unit_family") or "").lower().strip()
        ut = str(spec.get("unit_tag") or spec.get("unit") or "").strip()
        unit_tag = str(cand.get("unit_tag") or cand.get("unit") or cand.get("base_unit") or "").strip()

        raw_core = str(cand.get("raw") or "")


        raw_ctx = " ".join([


            raw_core,


            str(cand.get("context_snippet") or ""),


            str(cand.get("context") or ""),


        ])


        # For magnitude/count metrics, avoid broad context leakage (%/currency nearby).


        raw_for = raw_ctx


        try:


            if uf not in ("currency", "money", "percent", "rate", "ratio", "growth", "share"):


                raw_for = raw_core.strip() or raw_ctx


        except Exception:


            raw_for = raw_core.strip() or raw_ctx


        is_cur = _refactor03_has_currency_evidence_v1(raw_for)


        is_pct = _refactor03_has_percent_evidence_v1(raw_for)

        # 1) unit-family gating
        if uf in ("currency", "money"):
            if not is_cur:
                return True
        elif uf in ("percent", "rate", "ratio", "growth", "share"):
            if not is_pct:
                return True
        else:
            # magnitude/count: reject obvious currency/percent
            if is_cur or is_pct:
                return True

        # 2) unit-tag scale gating for magnitude/count metrics (K/M/B)
        try:
            ut_up = ut.upper().strip()
            unit_up = unit_tag.upper().strip()
            if ut_up in ("K", "M", "B", "T") and unit_up in ("K", "M", "B", "T") and ut_up != unit_up:
                return True
        except Exception:
            pass

        return False
    except Exception:
        return False


def _refactor27_candidate_rejected_currency_date_fragment_v1(cand: dict, spec: dict = None) -> bool:
    """Reject date-fragment candidates like '01' in contexts such as 'July 01, 2025' for currency-ish metrics.

    Rationale:
      - Some news pages include datelines (e.g., 'July 01, 2025') near genuine currency values.
      - Weak context-based currency evidence can cause day-of-month tokens to outscore real values.
    Determinism:
      - Pure filter; does not invent candidates or refetch content.
    """
    try:
        if not isinstance(cand, dict) or not isinstance(spec, dict):
            return False
        uf = str(spec.get("unit_family") or "").lower().strip()
        if uf not in ("currency", "money"):
            return False

        raw = str(cand.get("raw") or "").strip()
        if not raw:
            return False

        # Only target tiny integer tokens that look like day-of-month (01..31)
        try:
            v = cand.get("value_norm")
            if v is None:
                v = cand.get("value")
            iv = int(float(v))
        except Exception:
            return False

        if iv < 1 or iv > 31:
            return False

        if not re.fullmatch(r"0?\d{1,2}", raw):
            return False

        ctx = " ".join([
            str(cand.get("context_snippet") or ""),
            str(cand.get("context") or ""),
        ]).lower()

        if not ctx:
            return False

        # Month + year pattern indicates this is very likely a dateline token
        months = (
            "jan", "january", "feb", "february", "mar", "march", "apr", "april",
            "may", "jun", "june", "jul", "july", "aug", "august", "sep", "sept", "september",
            "oct", "october", "nov", "november", "dec", "december",
        )
        if any(m in ctx for m in months) and re.search(r"\b(19|20)\d{2}\b", ctx):
            # If the raw itself directly carries currency markers, do not reject
            raw_l = raw.lower()
            if any(sym in raw_l for sym in ("$", "usd", "eur", "gbp", "sgd", "aud", "cad", "hk$", "us$")):
                return False
            return True

        return False
    except Exception:
        return False


def _refactor03_diff_unit_mismatch_v1(prev_key: str, prev_metric: dict, cur_metric: dict, prev_unit: str = None, cur_unit: str = None) -> bool:
    """Return True if the prev/current pair is not comparable due to unit family or scale mismatch."""
    try:
        expected = _refactor03_unit_family_from_ckey_v1(prev_key)

        # REFACTOR04: avoid unit-family false positives due to broad context_snippet leakage.
        # - For magnitude/count metrics, prefer token/raw evidence only.
        # - For currency/percent metrics, allow context to help detect markers.
        if str(expected or "").lower().strip() in ("currency", "percent"):
            prev_txt = _refactor04_unit_evidence_text_from_metric_v1(prev_metric, include_context=True)
            cur_txt = _refactor04_unit_evidence_text_from_metric_v1(cur_metric, include_context=True)
        else:
            prev_txt = _refactor04_unit_evidence_text_from_metric_v1(prev_metric, include_context=False)
            cur_txt = _refactor04_unit_evidence_text_from_metric_v1(cur_metric, include_context=False)
            # If raw evidence is missing, fall back to context-inclusive extraction.
            try:
                if not str(prev_txt or "").strip():
                    prev_txt = _refactor04_unit_evidence_text_from_metric_v1(prev_metric, include_context=True)
                if not str(cur_txt or "").strip():
                    cur_txt = _refactor04_unit_evidence_text_from_metric_v1(cur_metric, include_context=True)
            except Exception:
                pass

        prev_is_cur = _refactor03_has_currency_evidence_v1(prev_txt)
        cur_is_cur = _refactor03_has_currency_evidence_v1(cur_txt)
        prev_is_pct = _refactor03_has_percent_evidence_v1(prev_txt)
        cur_is_pct = _refactor03_has_percent_evidence_v1(cur_txt)

        # scale mismatch (K/M/B/T)
        try:
            pu = str(prev_unit or "").upper().strip()
            cu = str(cur_unit or "").upper().strip()
            if pu in ("K", "M", "B", "T") and cu in ("K", "M", "B", "T") and pu != cu:
                return True
        except Exception:
            pass

        try:
            if expected == "currency":
                pu = str(prev_unit or "").upper().strip()
                cu = str(cur_unit or "").upper().strip()

                def _split_code_scale(u: str):
                    u = str(u or "").upper().strip()
                    if not u:
                        return ("", "")
                    # Composite like 'USD:B'
                    if ":" in u:
                        parts = [p.strip() for p in u.split(":") if p is not None]
                        if len(parts) >= 2:
                            code = parts[0].upper()
                            sc = parts[1].upper()
                            if sc in ("K", "M", "B", "T"):
                                return (code, sc)
                            return (code, "")
                    # Composite like 'USD_B'
                    if "_" in u:
                        parts = [p.strip() for p in u.split("_") if p is not None]
                        if len(parts) >= 2:
                            code = parts[0].upper()
                            sc = parts[-1].upper()
                            if sc in ("K", "M", "B", "T"):
                                return (code, sc)
                            return (code, "")
                    # Pure scale token
                    if u in ("K", "M", "B", "T"):
                        return ("", u)
                    # Pure currency code
                    try:
                        if re.fullmatch(r"[A-Z]{3}", u):
                            return (u, "")
                    except Exception:
                        pass
                    return ("", "")

                p_code, p_sc = _split_code_scale(pu)
                c_code, c_sc = _split_code_scale(cu)

                # Prefer explicit unit-encoded code, else infer from evidence text
                p_code = p_code or infer_currency_code_from_text_v1(prev_txt)
                c_code = c_code or infer_currency_code_from_text_v1(cur_txt)

                if p_code and c_code and p_code != c_code:
                    return True

                # If only one side has a magnitude scale token (K/M/B/T), treat as mismatch.
                has_ps = p_sc in ("K", "M", "B", "T")
                has_cs = c_sc in ("K", "M", "B", "T")
                if (has_ps and (not has_cs)) or (has_cs and (not has_ps)):
                    return True

                # Both sides have a scale token and they differ.
                if has_ps and has_cs and p_sc != c_sc:
                    return True
        except Exception:
            pass


        # family mismatch
        if expected == "currency":
            if isinstance(prev_metric, dict) and prev_metric and (not prev_is_cur):
                return True
            if isinstance(cur_metric, dict) and cur_metric and (not cur_is_cur):
                return True
            return False
        if expected == "percent":
            if isinstance(prev_metric, dict) and prev_metric and (not prev_is_pct):
                return True
            if isinstance(cur_metric, dict) and cur_metric and (not cur_is_pct):
                return True
            return False

        # expected magnitude/count
        if prev_is_cur or cur_is_cur or prev_is_pct or cur_is_pct:
            return True

        return False
    except Exception:
        return False


def get_history(limit: int = MAX_HISTORY_ITEMS) -> List[Dict]:
    """Load analysis history from Google Sheet"""
    sheet = get_google_sheet()
    # This prevents Evolution from being blocked by transient Sheets failures.
    try:
        if st.session_state.get("fix2d66_force_session_history"):
            return st.session_state.get('analysis_history', [])
    except Exception:
        pass
    if not sheet:
        # Fallback to session state
        return st.session_state.get('analysis_history', [])

    try:
        # Why:
        # - Your sheet names are: 'Sheet1', 'Snapshots', 'HistoryFull'
        # - There is no worksheet called 'History'
        # - Using cache_key='History' can cache empty reads under the wrong key.
        _ws_title = getattr(sheet, "title", "") or "Sheet1"
        _cache_key = f"History::{_ws_title}"

        # Get all rows (skip header)
        values = []
        _r105_used_direct = False

        # REFACTOR105: If we just wrote to Sheets, bypass cached reads once to avoid stale History reads.
        try:
            _dirty = st.session_state.get("_history_dirty_v1")
        except Exception:
            _dirty = None

        try:
            import time as _time
            if isinstance(_dirty, (int, float)) and (_time.time() - float(_dirty) < 120):
                direct = sheet.get_all_values()
                if direct and len(direct) >= 2:
                    values = direct
                    _r105_used_direct = True
                    try:
                        _cache = globals().get("_SHEETS_READ_CACHE")
                        if isinstance(_cache, dict):
                            _cache.pop(f"get_all_values:{_cache_key}", None)
                            # Defensive: drop any cached History::* reads.
                            for _k in list(_cache.keys()):
                                if isinstance(_k, str) and _k.startswith("get_all_values:History::"):
                                    _cache.pop(_k, None)
                    except Exception:
                        pass
                    try:
                        st.session_state.pop("_history_dirty_v1", None)
                        st.session_state.pop("_history_dirty_reason_v1", None)
                    except Exception:
                        pass
        except Exception:
            pass

        if not _r105_used_direct:
            try:
                values = sheets_get_all_values_cached(sheet, cache_key=_cache_key)
            except Exception:
                pass
                values = []

        # Why:
        # - If a prior transient read/429 produced an empty cached value,
        #   evolution may temporarily see no history even though rows exist.
        if not values or len(values) < 2:
            try:
                direct = sheet.get_all_values()
                if direct and len(direct) >= 2:
                    values = direct
            except Exception:
                pass

        all_rows = values[1:] if values and len(values) >= 2 else []

        try:
            if (not all_rows) and st.session_state.get('analysis_history'):
                return st.session_state.get('analysis_history', [])
        except Exception:
            pass


        try:
            if (not all_rows) and globals().get("_SHEETS_LAST_READ_ERROR"):
                if ("RESOURCE_EXHAUSTED" in str(_SHEETS_LAST_READ_ERROR)
                    or "Quota exceeded" in str(_SHEETS_LAST_READ_ERROR)
                    or "429" in str(_SHEETS_LAST_READ_ERROR)):
                    return st.session_state.get('analysis_history', [])
        except Exception:
            pass

        # Parse and return most recent
        history = []
        for row in all_rows[-limit:]:
            if len(row) >= 5:
                raw_cell = row[4]
                try:
                    data = json.loads(raw_cell)
                    data['_sheet_id'] = row[0]  # Keep track of sheet row ID

                    # (your existing GH2 / ES1G / GH1 / GH3 logic unchanged)
                    # ...
                    history.append(data)

                except json.JSONDecodeError:
                    # (your existing GH1 rescue logic unchanged)
                    continue


        # REFACTOR108: Ensure history is newest-first (Sheet rows are chronological).
        try:
            history = list(reversed(history))
        except Exception:
            pass

# REFACTOR105: Merge in-session history (analysis_history + last_analysis) to avoid stale Sheets reads.
        try:
            _sess_hist = st.session_state.get("analysis_history") or []
        except Exception:
            _sess_hist = []
        try:
            _sess_last = st.session_state.get("last_analysis")
        except Exception:
            _sess_last = None

        def _r105_hist_key(_h):
            try:
                if not isinstance(_h, dict):
                    return None
                _t = _h.get("timestamp") or (_h.get("results") or {}).get("timestamp") or ""
                _q = (_h.get("question") or (_h.get("results") or {}).get("question") or "").strip()
                if not _t and not _q:
                    return None
                return (_t, _q)
            except Exception:
                return None

        try:
            _merged = []
            _seen = set()

            for _h in (history or []):
                _k = _r105_hist_key(_h)
                if _k and _k not in _seen:
                    _seen.add(_k)
                    _merged.append(_h)

            if isinstance(_sess_hist, list):
                for _h in _sess_hist:
                    _k = _r105_hist_key(_h)
                    if _k and _k not in _seen:
                        _seen.add(_k)
                        _merged.append(_h)

            if isinstance(_sess_last, dict):
                _k = _r105_hist_key(_sess_last)
                if _k and _k not in _seen:
                    _seen.add(_k)
                    _merged.append(_sess_last)

            try:
                def _ts(_h):
                    try:
                        _t = (
                            (_h.get("timestamp") if isinstance(_h, dict) else "")
                            or ((_h.get("results") or {}).get("timestamp") if isinstance(_h.get("results"), dict) else "")
                            or ((_h.get("primary_response") or {}).get("timestamp") if isinstance(_h.get("primary_response"), dict) else "")
                            or ""
                        )
                        _dt = _parse_iso_dt(_t) if _t else None
                        return _dt.timestamp() if _dt else 0.0
                    except Exception:
                        return 0.0
                _merged.sort(key=_ts, reverse=True)
                if isinstance(limit, int) and limit > 0 and len(_merged) > limit:
                    _merged = _merged[:limit]
            except Exception:
                pass

            history = _merged
        except Exception:
            pass


        # (your existing GH3 sort unchanged)
        return history

    except Exception as e:
        st.warning(f"⚠️ Failed to load from Google Sheets: {e}")
        return st.session_state.get('analysis_history', [])


def get_analysis_by_id(analysis_id: str) -> Optional[Dict]:
    """Get a specific analysis by ID"""
    sheet = get_google_sheet()
    if not sheet:
        return None

    try:
        # Find row with matching ID
        cell = sheet.find(analysis_id)
        if cell:
            row = sheet.row_values(cell.row)
            if len(row) >= 5:
                return json.loads(row[4])
    except Exception as e:
        st.warning(f"⚠️ Failed to find analysis: {e}")

    return None

def delete_from_history(analysis_id: str) -> bool:
    """Delete an analysis from history"""
    sheet = get_google_sheet()
    if not sheet:
        return False

    try:
        cell = sheet.find(analysis_id)
        if cell:
            sheet.delete_rows(cell.row)
            return True
    except Exception as e:
        st.warning(f"⚠️ Failed to delete: {e}")

    return False

def clear_history() -> bool:
    """Clear all history (keep headers)"""
    sheet = get_google_sheet()
    if not sheet:
        st.session_state.analysis_history = []
        return True

    try:
        # Get row count
        all_rows = sheets_get_all_values_cached(sheet, cache_key="History")
        if len(all_rows) > 1:
            # Delete all rows except header
            sheet.delete_rows(2, len(all_rows))
        return True
    except Exception as e:
        st.warning(f"⚠️ Failed to clear history: {e}")
        return False

def format_history_label(analysis: Dict) -> str:
    """Format a history item for dropdown display"""
    timestamp = analysis.get('timestamp', '')
    question = analysis.get('question', 'Unknown query')[:40]
    confidence = analysis.get('final_confidence', '')

    try:
        dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
        now = datetime.now()
        delta = now - dt.replace(tzinfo=None)

        if delta.total_seconds() < 3600:
            time_str = f"{int(delta.total_seconds() / 60)}m ago"
        elif delta.total_seconds() < 86400:
            time_str = f"{int(delta.total_seconds() / 3600)}h ago"
        elif delta.days == 1:
            time_str = "Yesterday"
        elif delta.days < 7:
            time_str = f"{delta.days}d ago"
        else:
            time_str = dt.strftime("%b %d")
    except:
        time_str = timestamp[:10] if timestamp else "Unknown"

    conf_str = f" ({confidence:.0f}%)" if isinstance(confidence, (int, float)) else ""
    return f"{time_str}: {question}...{conf_str}"

def get_history_options() -> List[Tuple[str, int]]:
    """Get formatted history options for dropdown"""
    history = get_history()
    options = []
    for i, analysis in enumerate(reversed(history)):  # Most recent first
        label = format_history_label(analysis)
        actual_index = len(history) - 1 - i
        options.append((label, actual_index))
    return options

# 1. CONFIGURATION & API KEY VALIDATION

def load_api_keys():
    """Load and validate API keys from secrets or environment"""

    _fix41afc5_dbg2 = {"rejected_year_only": 0, "rejected_unitless": 0, "rejected_magnitude_other_unitless": 0}

    _fix41afc5_dbg = {"rejected_year_only": 0, "rejected_unitless": 0, "rejected_magnitude_other_unitless": 0}
    try:
        PERPLEXITY_KEY = st.secrets.get("PERPLEXITY_API_KEY") or os.getenv("PERPLEXITY_API_KEY", "")
        GEMINI_KEY = st.secrets.get("GEMINI_API_KEY") or os.getenv("GEMINI_API_KEY", "")
        SERPAPI_KEY = st.secrets.get("SERPAPI_KEY") or os.getenv("SERPAPI_KEY", "")
        SCRAPINGDOG_KEY = st.secrets.get("SCRAPINGDOG_KEY") or os.getenv("SCRAPINGDOG_KEY", "")
    except Exception:
        pass
        PERPLEXITY_KEY = os.getenv("PERPLEXITY_API_KEY", "")
        GEMINI_KEY = os.getenv("GEMINI_API_KEY", "")
        SERPAPI_KEY = os.getenv("SERPAPI_KEY", "")
        SCRAPINGDOG_KEY = os.getenv("SCRAPINGDOG_KEY", "")

    # Validate critical keys
    if not PERPLEXITY_KEY or len(PERPLEXITY_KEY) < 10:
        st.error("❌ PERPLEXITY_API_KEY is missing or invalid")
        st.stop()

    if not GEMINI_KEY or len(GEMINI_KEY) < 10:
        st.error("❌ GEMINI_API_KEY is missing or invalid")
        st.stop()

    return PERPLEXITY_KEY, GEMINI_KEY, SERPAPI_KEY, SCRAPINGDOG_KEY

PERPLEXITY_KEY, GEMINI_KEY, SERPAPI_KEY, SCRAPINGDOG_KEY = load_api_keys()
PERPLEXITY_URL = "https://api.perplexity.ai/chat/completions"

# Configure Gemini
#genai.configure(api_key=GEMINI_KEY)
#gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')

# 2. PYDANTIC MODELS

class MetricDetail(BaseModel):
    """Individual metric with name, value, and unit"""
    name: str = Field(..., description="Metric name")
    value: Union[float, int, str] = Field(..., description="Metric value")
    unit: str = Field(default="", description="Unit of measurement")
    model_config = ConfigDict(extra='ignore')

class TopEntityDetail(BaseModel):
    """Entity in top_entities list"""
    name: str = Field(..., description="Entity name")
    share: Optional[str] = Field(None, description="Market share")
    growth: Optional[str] = Field(None, description="Growth rate")
    model_config = ConfigDict(extra='ignore')

class TrendForecastDetail(BaseModel):
    """Trend forecast item"""
    trend: str = Field(..., description="Trend description")
    direction: Optional[str] = Field(None, description="Direction indicator")
    timeline: Optional[str] = Field(None, description="Timeline")
    model_config = ConfigDict(extra='ignore')

class VisualizationData(BaseModel):
    chart_labels: List[str] = Field(default_factory=list)
    chart_values: List[Union[float, int]] = Field(default_factory=list)
    chart_title: Optional[str] = Field("Trend Analysis")
    chart_type: Optional[str] = Field("line")
    x_axis_label: Optional[str] = None
    y_axis_label: Optional[str] = None
    model_config = ConfigDict(extra='ignore')

class ComparisonBar(BaseModel):
    """Comparison bar chart data"""
    title: str = Field("Comparison", description="Chart title")
    categories: List[str] = Field(default_factory=list)
    values: List[Union[float, int]] = Field(default_factory=list)
    model_config = ConfigDict(extra='ignore')

class BenchmarkTable(BaseModel):
    """Benchmark table row"""
    category: str
    value_1: Union[float, int, str] = Field(default=0, description="Numeric value or string")
    value_2: Union[float, int, str] = Field(default=0, description="Numeric value or string")
    model_config = ConfigDict(extra='ignore')

class Action(BaseModel):
    """Investment/action recommendation"""
    recommendation: str = Field("Neutral", description="Buy/Hold/Sell/Neutral")
    confidence: str = Field("Medium", description="High/Medium/Low")
    rationale: str = Field("", description="Reasoning")
    model_config = ConfigDict(extra='ignore')

class LLMResponse(BaseModel):
    """Complete LLM response schema"""
    executive_summary: str = Field(..., description="High-level summary")
    primary_metrics: Dict[str, MetricDetail] = Field(default_factory=dict)
    key_findings: List[str] = Field(default_factory=list)
    top_entities: List[TopEntityDetail] = Field(default_factory=list)
    trends_forecast: List[TrendForecastDetail] = Field(default_factory=list)
    visualization_data: Optional[VisualizationData] = None
    comparison_bars: Optional[ComparisonBar] = None
    benchmark_table: Optional[List[BenchmarkTable]] = None
    sources: List[str] = Field(default_factory=list)
    confidence: Union[float, int] = Field(default=75)
    freshness: Optional[str] = Field(None)
    action: Optional[Action] = None
    model_config = ConfigDict(extra='ignore')

# 3. PROMPTS

RESPONSE_TEMPLATE = """
{
  "executive_summary": "3-4 sentence high-level answer",
  "primary_metrics": {
    "metric_1": {"name": "Key Metric 1", "value": 25.5, "unit": "%"},
    "metric_2": {"name": "Key Metric 2", "value": 623, "unit": "$B"}
  },
  "key_findings": [
    "Finding 1 with quantified impact",
    "Finding 2 explaining drivers"
  ],
  "top_entities": [
    {"name": "Entity 1", "share": "25%", "growth": "15%"}
  ],
  "trends_forecast": [
    {"trend": "Trend description", "direction": "↑", "timeline": "2025-2027"}
  ],
  "visualization_data": {
    "chart_labels": ["2023", "2024", "2025"],
    "chart_values": [100, 120, 145],
    "chart_title": "Market Growth",
    "chart_type": "line"
  },
  "comparison_bars": {
    "title": "Market Share",
    "categories": ["A", "B", "C"],
    "values": [45, 30, 25]
  },
  "benchmark_table": [
    {"category": "Company A", "value_1": 25.5, "value_2": 623}
  ],
  "sources": ["source1.com"],
  "confidence": 87,
  "freshness": "Dec 2024"
}
"""


SYSTEM_PROMPT = f"""You are a professional market research analyst.

CRITICAL RULES:
1. Return ONLY valid JSON. NO markdown, NO code blocks, NO extra text.
2. NO citation references like [1][2] inside strings.
3. Use double quotes for all keys and string values.
4. NO trailing commas in arrays or objects.
5. Escape internal quotes with backslash.
6. If the prompt includes "Query Structure", you MUST follow it:
   - Treat "MAIN QUESTION" as the primary topic and address it FIRST.
   - Treat "SIDE QUESTIONS" as secondary topics and address them AFTER the main topic.
   - Do NOT let a side question replace the main question just because it is more specific.
   - In executive_summary, clearly separate: "Main:" then "Side:" when side questions exist.


NUMERIC FIELD RULES (IMPORTANT):
- In benchmark_table: value_1 and value_2 MUST be numbers (never "N/A", "null", or text)
- If data unavailable, use 0 for benchmark_table values
- In primary_metrics: values can be numbers or strings with units (e.g., "25.5" or "25.5 billion")
- In top_entities: share and growth can be strings (e.g., "25%")

REQUIRED FIELDS (provide substantive data):

**executive_summary** - MUST be 4-6 complete sentences covering:
  • Sentence 1: Direct answer with specific quantitative data (market size, revenue, units, etc.)
  • Sentence 2: Major players or regional breakdown with percentages/numbers
  • Sentence 3: Key growth drivers or market dynamics
  • Sentence 4: Future outlook with projected CAGR, timeline, or target values
  • Sentence 5 (optional): Challenge, risk, or competitive dynamic

  BAD (too short): "The EV market is growing rapidly due to government policies."

  GOOD: "The global electric vehicle market reached 14.2 million units sold in 2023, representing 18% of total auto sales. China dominates with 60% market share, followed by Europe (25%) and North America (10%). Growth is driven by battery cost reductions (down 89% since 2010), expanding charging infrastructure, and stricter emission regulations in over 20 countries. The market is projected to grow at 21% CAGR through 2030, reaching 40 million units annually. However, supply chain constraints for lithium and cobalt remain key challenges."

- primary_metrics (3+ metrics with numbers)
- key_findings (3+ findings with quantitative details)
- top_entities (3+ companies/countries with market share %)
- trends_forecast (2+ trends with timelines)
- visualization_data (MUST have chart_labels and chart_values)
- benchmark_table (if included, value_1 and value_2 must be NUMBERS, not "N/A")

Even if web data is sparse, use your knowledge to provide complete, detailed analysis.

Output ONLY this JSON structure:
{RESPONSE_TEMPLATE}
"""

EVOLUTION_PROMPT_TEMPLATE = """You are a market research analyst performing an UPDATE ANALYSIS.

You have been given a PREVIOUS ANALYSIS from {time_ago}. Your task is to:
1. Search for CURRENT data on the same metrics and entities
2. Identify what has CHANGED vs what has STAYED THE SAME
3. Provide updated values where data has changed
4. Flag any metrics/entities that are no longer relevant or have new entries

PREVIOUS ANALYSIS:
==================
Question: {previous_question}
Timestamp: {previous_timestamp}

Previous Executive Summary:
{previous_summary}

Previous Key Metrics:
{previous_metrics}

Previous Top Entities:
{previous_entities}

Previous Key Findings:
{previous_findings}
==================

CRITICAL RULES:
1. Return ONLY valid JSON. NO markdown, NO code blocks.
2. For EACH metric, indicate if it INCREASED, DECREASED, or stayed UNCHANGED
3. Keep the SAME metric names as previous analysis for easy comparison
4. If a metric is no longer available, mark it as "discontinued"
5. If there's a NEW important metric, add it with status "new"


REQUIRED OUTPUT FORMAT:
{{
  "executive_summary": "Updated 4-6 sentence summary noting key changes since last analysis",
  "analysis_delta": {{
    "time_since_previous": "{time_ago}",
    "overall_trend": "improving/declining/stable",
    "major_changes": ["Change 1", "Change 2"],
    "data_freshness": "Q4 2024"
  }},
  "primary_metrics": {{
    "metric_key": {{
      "name": "Same metric name as before",
      "previous_value": 100,
      "current_value": 110,
      "unit": "$B",
      "change_pct": 10.0,
      "direction": "increased/decreased/unchanged",
      "status": "updated/discontinued/new"
    }}
  }},
  "key_findings": [
    "[UNCHANGED] Finding that remains true",
    "[UPDATED] Finding with new data",
    "[NEW] Completely new finding",
    "[REMOVED] Finding no longer relevant - reason"
  ],
  "top_entities": [
    {{
      "name": "Company A",
      "previous_share": "25%",
      "current_share": "27%",
      "previous_rank": 1,
      "current_rank": 1,
      "change": "increased",
      "status": "updated"
    }}
  ],
  "trends_forecast": [
    {{"trend": "Trend description", "direction": "↑", "timeline": "2025-2027", "confidence": "high/medium/low"}}
  ],
  "visualization_data": {{
    "chart_labels": ["Previous", "Current"],
    "chart_values": [100, 110],
    "chart_title": "Market Size Evolution"
  }},
  "sources": ["source1.com", "source2.com"],
  "confidence": 85,
  "freshness": "Dec 2024",
  "drift_summary": {{
    "metrics_changed": 2,
    "metrics_unchanged": 3,
    "entities_reshuffled": 1,
    "findings_updated": 4,
    "overall_stability_pct": 75
  }}
}}

NOW, search for CURRENT information to UPDATE the previous analysis.
Focus on finding CHANGES to the metrics and entities listed above.

User Question: {query}
"""

# 4. MODEL LOADING

@st.cache_resource(show_spinner="🔧 Loading AI models...")
def load_models():
    """Load and cache sentence transformer and classifier"""
    try:
        classifier = pipeline(
            "zero-shot-classification",
            model="facebook/bart-large-mnli",
            device=-1
        )
        embedder = SentenceTransformer("all-MiniLM-L6-v2")
        return classifier, embedder
    except Exception as e:
        st.error(f"❌ Model loading failed: {e}")
        st.stop()

domain_classifier, embedder = load_models()

# 5. JSON REPAIR FUNCTIONS

def repair_llm_response(data: dict) -> dict:
    """
    Repair common LLM JSON structure issues:

    - Convert primary_metrics from list -> dict (stable keys)
    - Normalize MetricDetail fields so currency+unit do NOT get lost:
        "29.8 S$B" / "S$29.8B" / "S$29.8 billion" -> value=29.8, unit="S$B"
        "$204.7B" -> value=204.7, unit="$B"
        "9.8%" -> value=9.8, unit="%"
    - Ensure top_entities and trends_forecast are lists
    - Fix visualization_data legacy keys (labels/values)
    - Fix benchmark_table numeric values
    - Remove 'action' block entirely (no longer used)
    - Add minimal required fields if missing

    NOTE: This function is intentionally conservative: it normalizes obvious formatting
    without trying to "invent" missing values.
    """
    if not isinstance(data, dict):
        return {}

    def _to_list(x):
        if x is None:
            return []
        if isinstance(x, list):
            return x
        if isinstance(x, dict):
            return list(x.values())
        return []

    def _coerce_number(s: str):
        try:
            return float(str(s).replace(",", "").strip())
        except Exception:
            return None

    def _normalize_metric_item(item: dict) -> dict:
        """
        Normalize a single metric dict in-place-ish and return it.

        Goal: preserve currency + magnitude in `unit`, keep `value` numeric when possible.
        """
        if not isinstance(item, dict):
            return {"name": "N/A", "value": "N/A", "unit": ""}

        name = item.get("name")
        if not isinstance(name, str) or not name.strip():
            name = "N/A"
        item["name"] = name

        raw_val = item.get("value")
        raw_unit = item.get("unit")

        unit = (raw_unit or "")
        if not isinstance(unit, str):
            unit = str(unit)

        # If already numeric and unit looks okay, keep as-is
        if isinstance(raw_val, (int, float)) and isinstance(unit, str):
            item["unit"] = unit.strip()
            return item

        # Try to parse string value forms like:
        # "S$29.8B", "29.8 S$B", "$ 204.7 billion", "9.8%", "12 percent"
        if isinstance(raw_val, str):
            txt = raw_val.strip()

            # Also allow unit to carry the number sometimes (rare but happens)
            # e.g. value="29.8", unit="S$B" is already fine.
            # But if unit is empty and txt contains unit, we extract.
            # Percent detection
            if re.search(r'(%|\bpercent\b)', txt, flags=re.I):
                num = _coerce_number(re.sub(r'[^0-9\.\-\,]+', '', txt))
                if num is not None:
                    item["value"] = num
                    item["unit"] = "%"
                    return item

            # Currency detection
            currency = ""
            # Normalize currency tokens in either value or unit
            combo = f"{txt} {unit}".strip()

            if re.search(r'\bSGD\b', combo, flags=re.I) or "S$" in combo.upper():
                currency = "S$"
            elif re.search(r'\bUSD\b', combo, flags=re.I) or "$" in combo:
                currency = "$"

            # Magnitude detection
            # Accept: T/B/M/K, or words
            mag = ""
            if re.search(r'\btrillion\b', combo, flags=re.I):
                mag = "T"
            elif re.search(r'\bbillion\b', combo, flags=re.I):
                mag = "B"
            elif re.search(r'\bmillion\b', combo, flags=re.I):
                mag = "M"
            elif re.search(r'\bthousand\b', combo, flags=re.I):
                mag = "K"
            else:
                m = re.search(r'([TBMK])\b', combo.replace(" ", ""), flags=re.I)
                if m:
                    mag = m.group(1).upper()

            # Extract numeric
            num = _coerce_number(re.sub(r'[^0-9\.\-\,]+', '', txt))
            if num is not None:
                # If unit was present and meaningful (and already includes %), keep it
                if unit.strip() == "%":
                    item["value"] = num
                    item["unit"] = "%"
                    return item

                # Build unit as currency+magnitude when any found
                # If neither found, keep existing unit (may be e.g. "years", "points")
                if currency or mag:
                    item["value"] = num
                    item["unit"] = f"{currency}{mag}".strip()
                    return item

                # No currency/mag detected: keep unit if provided; else blank
                item["value"] = num
                item["unit"] = unit.strip()
                return item

            # If we can’t parse into a number, at least preserve the original text
            item["value"] = txt
            item["unit"] = unit.strip()
            return item

        # Non-string, non-numeric (None, dict, list, etc.)
        if raw_val is None or raw_val == "":
            item["value"] = "N/A"
        else:
            item["value"] = str(raw_val)

        item["unit"] = unit.strip()
        return item

    # primary_metrics normalization
    metrics = data.get("primary_metrics")

    # list -> dict
    if isinstance(metrics, list):
        new_metrics = {}
        for i, item in enumerate(metrics):
            if not isinstance(item, dict):
                continue
            item = _normalize_metric_item(item)

            raw_name = item.get("name", f"metric_{i+1}")
            key = re.sub(r'[^a-z0-9_]', '', str(raw_name).lower().replace(" ", "_")).strip("_")
            if not key:
                key = f"metric_{i+1}"

            original_key = key
            j = 1
            while key in new_metrics:
                key = f"{original_key}_{j}"
                j += 1

            new_metrics[key] = item

        data["primary_metrics"] = new_metrics

    elif isinstance(metrics, dict):
        # Normalize each metric dict entry
        cleaned = {}
        for k, v in metrics.items():
            if isinstance(v, dict):
                cleaned[str(k)] = _normalize_metric_item(v)
            else:
                # If someone stored a scalar, wrap it
                cleaned[str(k)] = _normalize_metric_item({"name": str(k), "value": v, "unit": ""})
        data["primary_metrics"] = cleaned

    else:
        data["primary_metrics"] = {}

    # list-like fields
    data["top_entities"] = _to_list(data.get("top_entities"))
    data["trends_forecast"] = _to_list(data.get("trends_forecast"))
    data["key_findings"] = _to_list(data.get("key_findings"))

    # Ensure strings in key_findings
    data["key_findings"] = [str(x) for x in data["key_findings"] if x is not None and str(x).strip()]

    # visualization_data legacy keys
    if isinstance(data.get("visualization_data"), dict):
        viz = data["visualization_data"]
        if "labels" in viz and "chart_labels" not in viz:
            viz["chart_labels"] = viz.pop("labels")
        if "values" in viz and "chart_values" not in viz:
            viz["chart_values"] = viz.pop("values")

        # Coerce chart_labels/values types gently
        if "chart_labels" in viz and not isinstance(viz["chart_labels"], list):
            viz["chart_labels"] = [str(viz["chart_labels"])]
        if "chart_values" in viz and not isinstance(viz["chart_values"], list):
            viz["chart_values"] = [viz["chart_values"]]

    # benchmark_table numeric cleaning
    if isinstance(data.get("benchmark_table"), list):
        cleaned_table = []
        for row in data["benchmark_table"]:
            if not isinstance(row, dict):
                continue

            if "category" not in row:
                row["category"] = "Unknown"

            for key in ["value_1", "value_2"]:
                if key not in row:
                    row[key] = 0
                    continue

                val = row.get(key)
                if isinstance(val, str):
                    val_upper = val.upper().strip()
                    if val_upper in ["N/A", "NA", "NULL", "NONE", "", "-", "—"]:
                        row[key] = 0
                    else:
                        try:
                            cleaned = re.sub(r'[^\d.-]', '', val)
                            row[key] = float(cleaned) if '.' in cleaned else int(cleaned) if cleaned else 0
                        except Exception:
                            pass
                            row[key] = 0
                elif isinstance(val, (int, float)):
                    pass
                else:
                    row[key] = 0

            cleaned_table.append(row)

        data["benchmark_table"] = cleaned_table

    # Remove action block entirely
    data.pop("action", None)

    # Minimal required top-level fields
    if not isinstance(data.get("executive_summary"), str) or not data.get("executive_summary", "").strip():
        data["executive_summary"] = "No executive summary provided."

    if not isinstance(data.get("sources"), list):
        data["sources"] = []

    if "confidence" not in data:
        data["confidence"] = 60

    if not isinstance(data.get("freshness"), str) or not data.get("freshness", "").strip():
        data["freshness"] = "Current"

    return data


def validate_numeric_fields(data: dict, context: str = "LLM Response") -> None:
    """
    Guardrail logger (and gentle coercer) for numeric lists used in charts/tables.

    We keep this lightweight: warn when strings appear where numbers are expected,
    and attempt to coerce when safe.
    """
    if not isinstance(data, dict):
        return

    # Check benchmark_table
    if "benchmark_table" in data and isinstance(data["benchmark_table"], list):
        for i, row in enumerate(data["benchmark_table"]):
            if isinstance(row, dict):
                for key in ["value_1", "value_2"]:
                    val = row.get(key)
                    if isinstance(val, str):
                        st.warning(
                            f"⚠️ {context}: benchmark_table[{i}].{key} is string: '{val}' (coercing to 0 if invalid)"
                        )
                        try:
                            cleaned = re.sub(r"[^\d\.\-]", "", val)
                            row[key] = float(cleaned) if cleaned else 0
                        except Exception:
                            pass
                            row[key] = 0

    # Check visualization_data chart_values
    viz = data.get("visualization_data")
    if isinstance(viz, dict):
        vals = viz.get("chart_values")
        if isinstance(vals, list):
            new_vals = []
            for j, v in enumerate(vals):
                if isinstance(v, (int, float)):
                    new_vals.append(v)
                elif isinstance(v, str):
                    try:
                        cleaned = re.sub(r"[^\d\.\-]", "", v)
                        new_vals.append(float(cleaned) if cleaned else 0.0)
                        st.warning(f"⚠️ {context}: visualization_data.chart_values[{j}] is string: '{v}' (coerced)")
                    except Exception:
                        pass
                        new_vals.append(0.0)
                else:
                    new_vals.append(0.0)
            viz["chart_values"] = new_vals


def preclean_json(raw: str) -> str:
    """
    Remove markdown fences and common citation markers before JSON parsing.
    Conservative: tries not to destroy legitimate JSON content.
    """
    if not raw or not isinstance(raw, str):
        return ""

    text = raw.strip()

    # Remove leading/trailing code fences (```json ... ```)
    text = re.sub(r'^\s*```(?:json)?\s*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\s*```\s*$', '', text)

    text = text.strip()

    # Remove common citation formats the model may append
    # [web:1], [1], (1) etc. (but avoid killing array syntax by being specific)
    text = re.sub(r'\[web:\d+\]', '', text, flags=re.IGNORECASE)
    text = re.sub(r'(?<!")\[\d+\](?!")', '', text)   # not inside quotes
    text = re.sub(r'(?<!")\(\d+\)(?!")', '', text)   # not inside quotes

    return text.strip()


def parse_json_safely(json_str: str, context: str = "LLM") -> dict:
    """
    Parse JSON with aggressive error recovery:
    1) Pre-clean markdown/citations
    2) Extract the *first* JSON object
    3) Repair common issues (unquoted keys, trailing commas, True/False/Null)
    4) Try parsing; if it fails, attempt a small set of pragmatic fixes
    """
    if json_str is None:
        return {}
    if not isinstance(json_str, str):
        json_str = str(json_str)

    if not json_str.strip():
        return {}

    cleaned = preclean_json(json_str)

    # Extract first JSON object (most LLM outputs are one object)
    match = re.search(r'\{.*\}', cleaned, flags=re.DOTALL)
    if not match:
        st.warning(f"⚠️ No JSON object found in {context} response")
        return {}

    json_content = match.group(0)

    # Structural repairs
    try:
        # Fix unquoted keys: {key: -> {"key":
        json_content = re.sub(
            r'([\{\,]\s*)([a-zA-Z_][a-zA-Z0-9_\-]*)(\s*):',
            r'\1"\2"\3:',
            json_content
        )

        # Remove trailing commas
        json_content = re.sub(r',\s*([\]\}])', r'\1', json_content)

        # Fix boolean/null capitalization
        json_content = re.sub(r':\s*True\b', ': true', json_content)
        json_content = re.sub(r':\s*False\b', ': false', json_content)
        json_content = re.sub(r':\s*Null\b', ': null', json_content)

    except Exception as e:
        st.warning(f"⚠️ {context}: Regex repair failed: {e}")

    # Attempt parse with a few passes
    attempts = 0
    last_err = None

    while attempts < 6:
        try:
            return json.loads(json_content)
        except json.JSONDecodeError as e:
            last_err = e
            msg = (e.msg or "").lower()

            # Pass 1: replace smart quotes
            if attempts == 0:
                json_content = (
                    json_content.replace("“", '"')
                                .replace("”", '"')
                                .replace("’", "'")
                )

            # Pass 2: single-quote keys/strings -> double quotes (limited)
            elif attempts == 1:
                # Only do this if it looks like single quotes dominate
                if json_content.count("'") > json_content.count('"'):
                    json_content = re.sub(r"\'", '"', json_content)

            # Pass 3: try removing control characters
            elif attempts == 2:
                json_content = re.sub(r"[\x00-\x1F\x7F]", "", json_content)

            # Pass 4: if unterminated string, try escaping a quote near the error
            elif "unterminated string" in msg or "unterminated" in msg:
                pos = e.pos
                # Try escaping a quote a bit before pos
                for i in range(pos - 1, max(0, pos - 200), -1):
                    if i < len(json_content) and json_content[i] == '"':
                        if i == 0 or json_content[i - 1] != "\\":
                            json_content = json_content[:i] + '\\"' + json_content[i + 1:]
                            break

            # Pass 5+: give up
            attempts += 1
            continue

    st.error(f"❌ Failed to parse JSON from {context}: {str(last_err)[:180] if last_err else 'unknown error'}")
    return {}


def parse_query_structure_safe(json_str: str, user_question: str) -> Dict:
    """
    Parse LLM-derived query structure with guaranteed deterministic fallback.
    Never raises, never returns empty dict.
    """
    parsed = parse_json_safely(json_str, context="LLM Query Structure")

    if isinstance(parsed, dict) and parsed:
        # Minimal schema validation
        if "main" in parsed or "category" in parsed:
            return parsed

    # 🔒 Deterministic fallback (NO LLM)
    return {
        "category": "unknown",
        "category_confidence": 0.0,
        "main": user_question,
        "side": []
    }


def extract_json_object(text: str) -> Optional[Dict]:
    """
    Best-effort extraction of the first JSON object from a string.
    Returns dict or None.
    """
    if not text or not isinstance(text, str):
        return None

    # Common cleanup
    cleaned = text.strip()
    cleaned = cleaned.replace("```json", "").replace("```", "").strip()

    # Fast path
    try:
        obj = json.loads(cleaned)
        if isinstance(obj, dict):
            return obj
    except Exception:
        pass

    # Regex: first {...} block (non-greedy)
    try:
        m = re.search(r"\{.*\}", cleaned, flags=re.DOTALL)
        if not m:
            return None
        candidate = m.group(0)
        obj = json.loads(candidate)
        if isinstance(obj, dict):
            return obj
    except Exception:
        return None

    return None


# 6. WEB SEARCH FUNCTIONS
#   SERPAPI STABILITY CONFIGURATION

# Fixed parameters to prevent geo/personalization variance

SERPAPI_STABILITY_CONFIG = {
    "gl": "us",                    # Fixed country
    "hl": "en",                    # Fixed language
    "google_domain": "google.com", # Fixed domain
    "nfpr": "1",                   # No auto-query correction
    "safe": "active",              # Consistent safe search
    "device": "desktop",           # Fixed device type
    "no_cache": "false",           # Allow Google caching (more stable)
}

# Preferred domains for consistent sourcing (sorted by priority)
PREFERRED_SOURCE_DOMAINS = [
    "statista.com", "reuters.com", "bloomberg.com", "imf.org", "wsj.com", "bcg.com", "opec.org",
    "worldbank.org", "mckinsey.com", "deloitte.com", "spglobal.com", "ft.com", "pwc.com", "semiconductors.org",
    "ft.com", "economist.com", "wsj.com", "forbes.com", "cnbc.com", "kpmg.com", "eia.org"
]

# Search results cache
_search_cache: Dict[str, Tuple[List[Dict], datetime]] = {}
SEARCH_CACHE_TTL_HOURS = 24

def get_search_cache_key(query: str) -> str:
    """Generate stable cache key for search query"""
    normalized = re.sub(r'\s+', ' ', query.lower().strip())
    normalized = re.sub(r'\b(today|current|latest|now|recent)\b', '', normalized)
    return hashlib.md5(normalized.encode()).hexdigest()[:16]

def get_cached_search_results(query: str) -> Optional[List[Dict]]:
    """
    Get cached search results if still valid.

    IMPORTANT:
    - Never treat cached empty results as valid.
      Returning [] here "poisons" the pipeline for hours and makes SerpAPI look broken.
    """
    try:
        cache_key = get_search_cache_key(query)
        if cache_key in _search_cache:
            cached_results, cached_time = _search_cache[cache_key]
            if datetime.now() - cached_time < timedelta(hours=SEARCH_CACHE_TTL_HOURS):
                # ✅ Do not reuse empty cache entries
                if isinstance(cached_results, list) and len(cached_results) == 0:
                    return None
                return cached_results
            # expired
            del _search_cache[cache_key]
    except Exception:
        return None
    return None


def cache_search_results(query: str, results: List[Dict]):
    """
    Cache search results.

    IMPORTANT:
    - Do NOT cache empty lists
    - Do NOT cache lists that contain no usable URLs
      (prevents "poisoned cache" that makes SerpAPI appear broken)
    """
    try:
        if not isinstance(query, str) or not query.strip():
            return
        if not isinstance(results, list) or not results:
            return

        # Require at least one usable url/link
        has_url = False
        for r in results:
            if isinstance(r, dict):
                u = (r.get("link") or r.get("url") or "").strip()
                if u:
                    has_url = True
                    break
            elif isinstance(r, str) and r.strip():
                has_url = True
                break

        if not has_url:
            return

        cache_key = get_search_cache_key(query)
        _search_cache[cache_key] = (results, datetime.now())
    except Exception:
        return


# LLM RESPONSE CACHE - Prevents variance on identical inputs
_llm_cache: Dict[str, Tuple[str, datetime]] = {}
LLM_CACHE_TTL_HOURS = 24  # Cache LLM responses for 24 hours

def get_llm_cache_key(query: str, web_context: Dict) -> str:
    """Generate cache key from query + source URLs"""
    # Include source URLs so cache invalidates if sources change
    source_urls = sorted(web_context.get("sources", [])[:5])
    cache_input = f"{query.lower().strip()}|{'|'.join(source_urls)}"
    return hashlib.md5(cache_input.encode()).hexdigest()[:20]

def get_cached_llm_response(query: str, web_context: Dict) -> Optional[str]:
    """Get cached LLM response if still valid"""
    cache_key = get_llm_cache_key(query, web_context)
    if cache_key in _llm_cache:
        cached_response, cached_time = _llm_cache[cache_key]
        if datetime.now() - cached_time < timedelta(hours=LLM_CACHE_TTL_HOURS):
            return cached_response
        del _llm_cache[cache_key]
    return None

def cache_llm_response(query: str, web_context: Dict, response: str):
    """Cache LLM response"""
    cache_key = get_llm_cache_key(query, web_context)
    _llm_cache[cache_key] = (response, datetime.now())


def sort_results_deterministically(results: List[Dict]) -> List[Dict]:
    """Sort results for consistent ordering"""
    def sort_key(r):
        link = r.get("link", "").lower()
        # Priority: preferred domains first, then alphabetical
        priority = 999
        for i, domain in enumerate(PREFERRED_SOURCE_DOMAINS):
            if domain in link:
                priority = i
                break
        return (priority, link)
    return sorted(results, key=sort_key)


def classify_source_reliability(source: str) -> str:
    """Classify source as High/Medium/Low quality"""
    source = source.lower() if isinstance(source, str) else ""

    high = ["gov", "imf", "worldbank", "central bank", "fed", "ecb", "reuters", "spglobal", "economist", "mckinsey", "bcg", "cognitive market research",
            "financial times", "wsj", "oecd", "bloomberg", "tradingeconomics", "deloitte", "hsbc", "imarc", "booz allen", "bakerinstitute.org", "wef",
           "kpmg", "semiconductors.org", "eu", "iea", "world bank", "opec", "jpmorgan", "citibank", "goldmansachs", "j.p. morgan", "oecd",
           "world bank", "sec", "federalreserve", "bls", "bea"]
    medium = ["wikipedia", "forbes", "cnbc", "yahoo", "ceic", "statista", "trendforce", "digitimes", "idc", "gartner", "marketwatch", "fortune", "investopedia"]
    low = ["blog", "medium.com", "wordpress", "ad", "promo"]

    for h in high:
        if h in source:
            return "✅ High"
    for m in medium:
        if m in source:
            return "⚠️ Medium"
    for l in low:
        if l in source:
            return "❌ Low"

    return "⚠️ Medium"

def source_quality_score(sources: List[str]) -> float:
    """Calculate average source quality (0-100)"""
    if not sources:
        return 50.0  # Lower default when no sources

    weights = {"✅ High": 100, "⚠️ Medium": 60, "❌ Low": 30}
    scores = [weights.get(classify_source_reliability(s), 60) for s in sources]
    return sum(scores) / len(scores) if scores else 50.0

@st.cache_data(ttl=3600, show_spinner=False)
def search_serpapi(query: str, num_results: int = 10) -> List[Dict]:
    """Search Google via SerpAPI with stability controls"""
    if not SERPAPI_KEY:
        return []

    # Check cache first (this is the ONLY cache we use - removed @st.cache_data to avoid conflicts)
    cached = get_cached_search_results(query)
    if cached:
        st.info("📦 Using cached search results")
        return cached

    # Aggressive query normalization for consistent searches
    query_normalized = query.lower().strip()

    # Remove temporal words that cause variance
    query_normalized = re.sub(r'\b(latest|current|today|now|recent|new|upcoming|this year|this month)\b', '', query_normalized)

    # Normalize whitespace
    query_normalized = re.sub(r'\s+', ' ', query_normalized).strip()

    # Add year for consistency
    if not re.search(r'\b20\d{2}\b', query_normalized):
        query_normalized = f"{query_normalized} 2024"

    # Build search terms
    query_lower = query_normalized
    industry_kw = ["industry", "market", "sector", "size", "growth", "players"]

    if any(kw in query_lower for kw in industry_kw):
        search_terms = f"{query_normalized} market size growth statistics"
        tbm, tbs = "", ""  # Organic results (more stable than news)
    else:
        search_terms = f"{query_normalized} finance economics data"
        tbm, tbs = "", ""  # Use organic for stability

    params = {
        "engine": "google",
        "q": search_terms,
        "api_key": SERPAPI_KEY,
        "num": num_results,
        "tbm": tbm,
        "tbs": tbs,
        **SERPAPI_STABILITY_CONFIG  # Add fixed location params
    }

    try:
        resp = requests.get("https://serpapi.com/search", params=params, timeout=10)
        resp.raise_for_status()
        data = resp.json()

        results = []

        # Prefer organic results (more stable than news)
        for item in data.get("organic_results", [])[:num_results]:
            results.append({
                "title": item.get("title", ""),
                "link": item.get("link", ""),
                "snippet": item.get("snippet", ""),
                "date": item.get("date", ""),
                "source": item.get("source", "")
            })

        # Fall back to news only if no organic results
        if not results:
            for item in data.get("news_results", [])[:num_results]:
                src = item.get("source", {})
                source_name = src.get("name", "") if isinstance(src, dict) else str(src)
                results.append({
                    "title": item.get("title", ""),
                    "link": item.get("link", ""),
                    "snippet": item.get("snippet", ""),
                    "date": item.get("date", ""),
                    "source": source_name
                })

        # Sort deterministically
        results = sort_results_deterministically(results)
        results = results[:num_results]

        # Cache results
        if results:
            cache_search_results(query, results)

        return results

    except Exception as e:
        st.warning(f"⚠️ SerpAPI error: {e}")
        return []


# - Pure helpers (no control-flow changes)
# - Used to trace injected extra URLs across: UI -> intake -> scrape -> snapshots -> hashing -> rebuild
def _inj_diag_make_run_id(prefix: str = "run") -> str:
    """Short correlation id for a single analysis/evolution run."""
    try:
        import os, time, hashlib
        seed = f"{prefix}|{time.time()}|{os.getpid()}|{os.urandom(8).hex()}"
        return hashlib.sha256(seed.encode("utf-8")).hexdigest()[:12]
    except Exception:
        pass
        try:
            import random
            return f"{prefix}_{random.randint(100000,999999)}"
        except Exception:
            return f"{prefix}_unknown"


# - Strips common tracking/query parameters from injected URLs ONLY
# - Keeps scheme/host/path; preserves non-tracking query params (sorted)
# - Adds deterministic canonical form for stable admission/dedupe/hashing
def _canonicalize_injected_url(url: str) -> str:
    """Canonicalize injected URLs by stripping known tracking params.

    This is intentionally conservative and applied only to user-injected URLs
    (extra URLs), not to SERP-derived URLs.
    """
    try:
        from urllib.parse import urlsplit, urlunsplit, parse_qsl, urlencode
        u = str(url or "").strip()
        if not u:
            return ""
        if not (u.startswith("http://") or u.startswith("https://")):
            return u

        parts = urlsplit(u)
        # Normalize scheme/host case
        scheme = (parts.scheme or "").lower()
        netloc = (parts.netloc or "").lower()
        path = parts.path or ""
        fragment = ""  # drop fragments for stability

        # Tracking params to drop (exact match)
        drop_exact = {
            "guccounter", "guce_referrer", "guce_referrer_sig",
            "gclid", "fbclid", "msclkid", "mc_cid", "mc_eid",
            "ref", "ref_src",
        }
        # Drop prefixes (utm_*, etc.)
        drop_prefixes = ("utm_",)

        qs = []
        for k, v in parse_qsl(parts.query or "", keep_blank_values=True):
            kk = (k or "").strip()
            if not kk:
                continue
            k_lower = kk.lower()
            if k_lower in drop_exact:
                continue
            if any(k_lower.startswith(p) for p in drop_prefixes):
                continue
            qs.append((kk, v))

        # Sort query params for determinism
        qs_sorted = sorted(qs, key=lambda kv: (kv[0].lower(), str(kv[1])))

        query = urlencode(qs_sorted, doseq=True) if qs_sorted else ""
        return urlunsplit((scheme, netloc, path, query, fragment))
    except Exception:
        pass
        try:
            return str(url or "").strip()
        except Exception:
            return ""

def _inj_diag_norm_url_list(extra_urls: Any) -> list:
    """Normalize/dedupe injected URL list (http/https only) with canonicalization.

    NOTE: This is used for injected/extra URL diagnostics and admission wiring only.
    It canonicalizes by stripping known tracking params for stability.
    """
    out = []
    try:
        if extra_urls is None:
            return []
        items = extra_urls
        if isinstance(items, str):
            items = [u.strip() for u in items.splitlines()]
        if not isinstance(items, (list, tuple, set)):
            items = [str(items)]
        seen = set()
        for u in items:
            uu = str(u or "").strip()
            if not uu:
                continue
            if not (uu.startswith("http://") or uu.startswith("https://")):
                continue
            cu = _canonicalize_injected_url(uu) or uu
            if cu in seen:
                continue
            seen.add(cu)
            out.append(cu)
    except Exception:
        return []
    return out

# =========================
# REFACTOR115 helpers (additive; deterministic; schema/key grammar unchanged)
# =========================

_REFACTOR115_SCHEMA_KEYS_V1 = [
    "global_ev_chargers_2040__unit_count",
    "global_ev_charging_investment_2040__currency",
    "global_ev_chargers_cagr_2026_2040__percent",
    "global_ev_sales_ytd_2025__unit_sales",
]

# Deterministic "known-good" seed URLs to stabilize year-anchor gating in production pools.
# These are only unioned into production (non-injection) runs.
_REFACTOR115_SCHEMA_SEED_URLS_V1 = [
    # Wood Mackenzie press release contains 2026–2040 CAGR, 2040 port totals, and 2040 spend ($).
    "https://www.woodmac.com/press-releases/global-ev-charging-ports-to-increase-cagr-of-12.3-from-2026-2040-reaching-206.6m-total-ports",
    # Secondary syndications of the same Wood Mackenzie facts (often easier to parse / less scripting).
    "https://www.aa.com.tr/en/energy/electricity/global-ev-charging-ports-forecast-to-surpass-206-million-by-2040/51226",
    "https://www.evinfrastructurenews.com/ev-networks/wood-mackenzie-global-ev-chargers-206-6m-2040",
    # YTD sales anchors for 2025 (Rho Motion).
    "https://rhomotion.com/news/global-ev-sales-reach-18-5-million-units-growing-by-21-ytd-in-november-2025/",
    # IEA HTML (non-PDF) for broader charging context.
    "https://www.iea.org/reports/global-ev-outlook-2024/trends-in-electric-vehicle-charging",

    # Additional syndicated/secondary pages with explicit 'investment' / '$... by 2040' phrasing (improves investment_2040 binding).
    "https://pv-magazine-usa.com/2025/08/18/global-ev-charging-ports-to-grow-12-annually-through-2040-says-wood-mackenzie/",
    "https://www.pv-magazine-india.com/2024/07/12/the-ev-charging-boom-a-1-trillion-opportunity-by-2040/",
    "https://www.sustainabletimes.co.uk/post/global-ev-charging-network-on-track-to-surpass-200-million-by-2040",
    "https://electrek.co/2025/08/18/home-charging-rules-as-global-ev-ports-soar-to-206-million-by-2040/",
    "https://energynews.pro/en/global-ev-charging-points-to-reach-206-6-million-by-2040/",
    "https://bolt.earth/blog/how-india-s-ev-infrastructure-rise-will-reshape-mobility-mindsets-and-market-dynamics-by-2040",
]

def _refactor115_collect_injection_urls_v1(results: Any, web_context: Any) -> list:
    """Collect injected URLs robustly from web_context + debug fields.

    This is intentionally defensive because multiple historical debug fields exist.
    Returned URLs are normalized/deduped via _inj_diag_norm_url_list.
    """
    urls = []
    try:
        wc = web_context if isinstance(web_context, dict) else {}
        # Common web_context locations
        for k in ("extra_urls", "injected_urls", "injected_urls_ui_norm", "ui_extra_urls"):
            v = wc.get(k)
            if isinstance(v, str):
                urls.extend([s.strip() for s in v.splitlines() if s.strip()])
            elif isinstance(v, (list, tuple, set)):
                urls.extend([str(x).strip() for x in v if isinstance(x, str) and str(x).strip()])
        # Diag containers
        dv = wc.get("diag_injected_urls") or wc.get("debug_injected_urls") or {}
        if isinstance(dv, dict):
            for kk in ("ui_norm", "intake_norm", "all_norm"):
                vv = dv.get(kk)
                if isinstance(vv, (list, tuple, set)):
                    urls.extend([str(x).strip() for x in vv if isinstance(x, str) and str(x).strip()])
        # Results/debug legacy fields
        if isinstance(results, dict):
            dbg = results.get("debug") if isinstance(results.get("debug"), dict) else {}
            # inj_trace variants
            for trace_k in ("inj_trace_v1", "inj_trace_v2_postfetch", "inj_trace_v2"):
                it = dbg.get(trace_k)
                if isinstance(it, dict):
                    for kk in ("ui_norm", "intake_norm"):
                        vv = it.get(kk)
                        if isinstance(vv, (list, tuple, set)):
                            urls.extend([str(x).strip() for x in vv if isinstance(x, str) and str(x).strip()])
            # Legacy lists
            for kk in ("injected_urls_v1", "fix2d65b_injected_urls", "fix2d66_injected_urls", "injected_urls"):
                vv = dbg.get(kk) or results.get(kk)
                if isinstance(vv, (list, tuple, set)):
                    urls.extend([str(x).strip() for x in vv if isinstance(x, str) and str(x).strip()])
    except Exception:
        pass

    try:
        return _inj_diag_norm_url_list(urls)
    except Exception:
        # best-effort fallback
        out = []
        seen = set()
        for u in urls:
            uu = str(u or "").strip()
            if uu and uu not in seen:
                seen.add(uu)
                out.append(uu)
        return out


# REFACTOR116: Robust post-hoc timing + per-row Δt stamping.
# Some runs pass a stale previous_data.timestamp into evolution while the evolution
# engine correctly selects a newer baseline snapshot (see debug.prev_snapshot_pick_v1).
# This helper recomputes run_timing_v1 and per-row analysis_evolution_delta_* from the
# *effective* baseline timestamp actually used, and blanks row-level Δt for injected rows.

def _refactor116_locate_evolution_results_dict_v1(evo_obj: Any) -> tuple:
    """Return (results_dict, wrapper_dict) for an evolution payload.

    - If evo_obj already looks like the results dict (has metric_changes/source_results), return it.
    - Else if evo_obj['results'] looks like the results dict, return that plus the wrapper.
    """
    if not isinstance(evo_obj, dict):
        return ({}, {})
    # Direct results dict
    if ("metric_changes" in evo_obj) and ("source_results" in evo_obj):
        return (evo_obj, evo_obj)
    inner = evo_obj.get("results")
    if isinstance(inner, dict) and (("metric_changes" in inner) or ("source_results" in inner)):
        return (inner, evo_obj)
    return (evo_obj, evo_obj)

def _refactor116_effective_baseline_timestamp_v1(results_dict: dict, wrapper_dict: dict, previous_data: Any) -> str:
    """Choose the effective baseline timestamp actually used for diffing."""
    ts = None
    try:
        if isinstance(wrapper_dict, dict):
            ts = wrapper_dict.get("previous_timestamp") or wrapper_dict.get("baseline_timestamp")
    except Exception:
        ts = None
    if not ts:
        try:
            ts = _first_present(
                results_dict or {},
                [
                    ["debug", "prev_snapshot_pick_v1", "selected_timestamp"],
                    ["debug", "prev_snapshot_pick_v2", "selected_timestamp"],
                    ["debug", "baseline_selector_v1", "selected_timestamp"],
                ],
                default=None,
            )
        except Exception:
            ts = None
    if not ts:
        try:
            if isinstance(previous_data, dict):
                ts = previous_data.get("timestamp")
        except Exception:
            ts = None
    return ts or ""

def _refactor116_compute_run_timing_v1(results_dict: dict, wrapper_dict: dict, previous_data: Any) -> tuple:
    """Return (analysis_ts_norm, evolution_ts_norm, delta_seconds, delta_human, warnings)."""
    warnings = []
    evo_ts_raw = None
    try:
        evo_ts_raw = None
        if isinstance(wrapper_dict, dict):
            evo_ts_raw = wrapper_dict.get("timestamp") or wrapper_dict.get("generated_at")
        if not evo_ts_raw and isinstance(results_dict, dict):
            evo_ts_raw = results_dict.get("generated_at") or results_dict.get("timestamp")
    except Exception:
        evo_ts_raw = None
    evo_ts_raw = evo_ts_raw or _yureeka_now_iso_utc()

    base_ts_raw = _refactor116_effective_baseline_timestamp_v1(results_dict, wrapper_dict, previous_data)
    dt_a = _parse_iso_dt(base_ts_raw) if base_ts_raw else None
    dt_e = _parse_iso_dt(evo_ts_raw) if evo_ts_raw else None

    analysis_ts_norm = dt_a.isoformat() if dt_a else (base_ts_raw or None)
    evo_ts_norm = dt_e.isoformat() if dt_e else (evo_ts_raw or None)

    delta_seconds = None
    delta_human = ""
    try:
        if dt_a and dt_e:
            ds = (dt_e - dt_a).total_seconds()
            if ds < 0:
                warnings.append("delta_negative_clamped_to_zero")
                ds = 0.0
            delta_seconds = float(ds)
            delta_human = _yureeka_humanize_seconds_v1(delta_seconds)
        else:
            warnings.append("delta_uncomputed_missing_timestamp")
    except Exception:
        warnings.append("delta_uncomputed_exception")
    return (analysis_ts_norm, evo_ts_norm, delta_seconds, delta_human, warnings)

def _refactor116_apply_effective_timing_and_row_deltas_v1(evo_obj: Any, previous_data: Any, web_context: Any = None) -> None:
    """Mutate the evolution payload to ensure Δt + per-row delta fields are correct."""
    try:
        res, wrap = _refactor116_locate_evolution_results_dict_v1(evo_obj)
        if not isinstance(res, dict):
            return
        wc = web_context if isinstance(web_context, dict) else {}

        analysis_ts_norm, evo_ts_norm, delta_seconds, delta_human, warnings = _refactor116_compute_run_timing_v1(res, wrap, previous_data)

        # Ensure debug container
        dbg = res.get("debug")
        if not isinstance(dbg, dict):
            dbg = {}
            res["debug"] = dbg

        # Beacon: effective timing (v3)
        dbg["run_timing_effective_v3"] = {
            "raw_previous_data_timestamp": (previous_data or {}).get("timestamp") if isinstance(previous_data, dict) else None,
            "previous_timestamp_field": (wrap or {}).get("previous_timestamp") if isinstance(wrap, dict) else None,
            "analysis_timestamp_effective": analysis_ts_norm,
            "evolution_timestamp": evo_ts_norm,
            "delta_seconds_effective": delta_seconds,
            "delta_human_effective": delta_human,
            "source": "refactor116_posthoc",
        }

        # REFACTOR117: duplicate beacon (v4) to make it unambiguous that wrapper.previous_timestamp is being honored.
        try:
            dbg["run_timing_effective_v4"] = {
                "raw_previous_data_timestamp": (previous_data or {}).get("timestamp") if isinstance(previous_data, dict) else None,
                "previous_timestamp_field": (wrap or {}).get("previous_timestamp") if isinstance(wrap, dict) else None,
                "analysis_timestamp_effective": analysis_ts_norm,
                "evolution_timestamp": evo_ts_norm,
                "delta_seconds_effective": delta_seconds,
                "delta_human_effective": delta_human,
                "baseline_ts_source": (
                    "wrapper.previous_timestamp" if ((wrap or {}).get("previous_timestamp") if isinstance(wrap, dict) else None)
                    else "debug.prev_snapshot_pick_v1_or_previous_data"
                ),
            }
        except Exception:
            pass


        # Overwrite run_timing_v1 with effective values
        rt = dbg.get("run_timing_v1") if isinstance(dbg.get("run_timing_v1"), dict) else {}
        rt.update({
            "analysis_timestamp": analysis_ts_norm,
            "evolution_timestamp": evo_ts_norm,
            "delta_seconds": delta_seconds,
            "delta_human": delta_human,
            "warnings": list(warnings or []),
        })
        # Record injection presence (but do not suppress run-level Δt)
        try:
            inj_urls_probe = _refactor115_collect_injection_urls_v1(res, wc)
            rt["injection_present_v3"] = bool(inj_urls_probe)
        except Exception:
            rt["injection_present_v3"] = False
        dbg["run_timing_v1"] = rt

        # Copy run delta fields
        res["run_delta_seconds"] = delta_seconds
        res["run_delta_human"] = delta_human
        # Copy into nested analysis payload (res["results"]) if present
        try:
            if isinstance(res.get("results"), dict):
                res["results"]["run_delta_seconds"] = delta_seconds
                res["results"]["run_delta_human"] = delta_human
                dbg2 = res["results"].get("debug")
                if not isinstance(dbg2, dict):
                    dbg2 = {}
                    res["results"]["debug"] = dbg2
                dbg2["run_timing_v1"] = dict(rt)
        except Exception:
            pass

        # Row-level delta stamping
        inj_urls = []
        try:
            inj_urls = _refactor115_collect_injection_urls_v1(res, wc)
        except Exception:
            inj_urls = []
        inj_norm = None
        try:
            inj_norm = _inj_diag_norm_url_list(inj_urls)
        except Exception:
            inj_norm = None
        inj_set = set([str(u).strip() for u in (inj_norm or inj_urls) if isinstance(u, str) and str(u).strip()])

        gating = {
            "inj_set_size": int(len(inj_set)),
            "inj_urls_sample": list(list(inj_set)[:3]),
            "rows_total": 0,
            "injected_rows_total": 0,
            "injected_rows_blank_delta": 0,
            "production_rows_total": 0,
            "production_rows_with_delta": 0,
            "rows_with_source_url": 0,
            "rows_missing_source_url": 0,
        }

        def _stamp(rows: Any):
            if not isinstance(rows, list):
                return
            for r in rows:
                if not isinstance(r, dict):
                    continue
                gating["rows_total"] += 1
                su = r.get("source_url")
                if not su:
                    gating["rows_missing_source_url"] += 1
                else:
                    gating["rows_with_source_url"] += 1
                is_injected = False
                if su and inj_set:
                    su_norm = su
                    try:
                        tmp = _inj_diag_norm_url_list([su])
                        if isinstance(tmp, list) and tmp:
                            su_norm = str(tmp[0] or su).strip()
                    except Exception:
                        su_norm = su
                    is_injected = (su_norm in inj_set)
                if is_injected:
                    gating["injected_rows_total"] += 1
                    r["analysis_evolution_delta_human"] = ""
                    r["analysis_evolution_delta_seconds"] = None
                    gating["injected_rows_blank_delta"] += 1
                else:
                    gating["production_rows_total"] += 1
                    r["analysis_evolution_delta_human"] = delta_human or ""
                    r["analysis_evolution_delta_seconds"] = delta_seconds
                    if (delta_human or delta_seconds is not None):
                        gating["production_rows_with_delta"] += 1

        _stamp(res.get("metric_changes"))
        _stamp(res.get("metric_changes_v2"))

        dbg["row_delta_gating_v3"] = dict(gating)
        # Also surface as row_delta_gating_v2 for backward compatibility with dashboards
        dbg["row_delta_gating_v2"] = dict(gating)
        try:
            if isinstance(res.get("results"), dict):
                dbg2 = res["results"].get("debug")
                if not isinstance(dbg2, dict):
                    dbg2 = {}
                    res["results"]["debug"] = dbg2
                dbg2["row_delta_gating_v3"] = dict(gating)
        except Exception:
            pass
    except Exception:
        return

def _refactor115_all_schema_values_null_in_payload_v1(payload: Any) -> bool:
    """Return True if all schema keys are missing or null in primary_metrics_canonical/metric_schema_frozen."""
    try:
        if not isinstance(payload, dict):
            return False
        pmc = payload.get("primary_metrics_canonical")
        if not isinstance(pmc, dict) and isinstance(payload.get("results"), dict):
            pmc = payload["results"].get("primary_metrics_canonical")
        ms = payload.get("metric_schema_frozen")
        if not isinstance(ms, dict) and isinstance(payload.get("results"), dict):
            ms = payload["results"].get("metric_schema_frozen")
        # helper to read a value-ish field
        def _val_from_metric(m):
            if not isinstance(m, dict):
                return None
            for k in ("value", "point_estimate", "numeric_value", "raw_value"):
                if k in m:
                    return m.get(k)
            return m.get("value")
        any_non_null = False
        for ck in _REFACTOR115_SCHEMA_KEYS_V1:
            v = None
            if isinstance(pmc, dict) and ck in pmc:
                v = _val_from_metric(pmc.get(ck))
            if v is None and isinstance(ms, dict) and ck in ms:
                v = _val_from_metric(ms.get(ck))
            if v is None or v == "":
                continue
            any_non_null = True
            break
        return (not any_non_null)
    except Exception:
        return False

def _refactor115_is_triad_validation_mode_v1(results: Any, web_context: Any) -> bool:
    try:
        if isinstance(web_context, dict) and bool(web_context.get("triad_validation_mode")):
            return True
        if isinstance(results, dict):
            dbg = results.get("debug") if isinstance(results.get("debug"), dict) else {}
            if bool(dbg.get("triad_validation_mode")):
                return True
        if bool(globals().get("_YUREEKA_TRIAD_VALIDATION_MODE_V1")):
            return True
    except Exception:
        return False
    return False



def _yureeka_extract_injected_urls_v1(web_context: Any) -> list:
    """Extract UI-injected URLs deterministically.

    Contract:
      - Prefer Streamlit/UI fields: diag_extra_urls_ui(_raw), extra_urls_ui(_raw), and list variants.
      - If Evolution wired injected URLs into web_context['extra_urls'], it MUST also set
        __yureeka_extra_urls_are_injection_v1 / __yureeka_injected_urls_v1 so we can safely
        treat extra_urls as injected without misclassifying production source lists.
    """
    out: list = []
    try:
        if not isinstance(web_context, dict):
            return []
        # list variants (UI)
        _cand = web_context.get("diag_extra_urls_ui") or web_context.get("extra_urls_ui") or []
        if isinstance(_cand, str):
            # allow simple newline/comma separated
            for part in _cand.replace(",", "\n").split():
                if part.startswith("http://") or part.startswith("https://"):
                    out.append(part.strip())
            _cand = []
        if isinstance(_cand, (list, tuple)):
            out.extend([u for u in _cand if isinstance(u, str)])

        # ui_raw string variants
        _ui_raw = web_context.get("diag_extra_urls_ui_raw") or web_context.get("extra_urls_ui_raw") or ""
        if isinstance(_ui_raw, str) and _ui_raw.strip():
            for part in _ui_raw.replace(",", "\n").split():
                if part.startswith("http://") or part.startswith("https://"):
                    out.append(part.strip())

        # explicit internal marker fallback (set by Evolution wiring)
        _marked = bool(web_context.get("__yureeka_extra_urls_are_injection_v1"))
        _marked_list = web_context.get("__yureeka_injected_urls_v1")
        if _marked and isinstance(_marked_list, (list, tuple)):
            out.extend([u for u in _marked_list if isinstance(u, str)])
        if _marked and isinstance(web_context.get("extra_urls"), (list, tuple)):
            out.extend([u for u in web_context.get("extra_urls") if isinstance(u, str)])

        # normalize / de-dup (keep original strings, but stable ordering)
        out = [u.strip() for u in out if isinstance(u, str) and u.strip()]
        _seen = set()
        _uniq = []
        for u in out:
            if u not in _seen:
                _seen.add(u)
                _uniq.append(u)
        out = _uniq

        # canonicalize for stability where possible (strip tracking params)
        try:
            _norm = _inj_diag_norm_url_list(out)
            if isinstance(_norm, list) and _norm:
                # keep normalized, but only if it doesn't erase all
                out = _norm
        except Exception:
            pass

        return out
    except Exception:
        return []


def _inj_diag_set_hash(urls: list) -> str:
    """Stable sha256 of sorted URL list (for compact logging)."""
    try:
        import hashlib
        lst = [str(u or "").strip() for u in (urls or []) if str(u or "").strip()]
        lst = sorted(set(lst))
        payload = "|".join(lst)
        return hashlib.sha256(payload.encode("utf-8")).hexdigest()
    except Exception:
        return ""

def _inj_diag_hash_inputs_from_bsc(baseline_sources_cache: Any) -> list:
    """Extract deterministic URL inputs used by snapshot hashing (v1/v2 both include URL)."""
    urls = []
    try:
        if not isinstance(baseline_sources_cache, list):
            return []
        for sr in baseline_sources_cache:
            if not isinstance(sr, dict):
                continue
            u = (sr.get("source_url") or sr.get("url") or "").strip()
            if u:
                urls.append(u)
    except Exception:
        return []
    return sorted(set(urls))

# Default behavior is OFF to avoid disrupting locked fastpath.
#
# When enabled, injected URLs that were persisted (per diag_injected_urls.persisted*)
# but are missing from baseline_sources_cache will be added as *synthetic* source
# records (url-only) so that:
#   - source_snapshot_hash (v1/v2) reflects injected sources deterministically
#   - evolution rebuild sees the same snapshot pool and hash identity via persistence
#
# Safety:
#   - Does NOT modify fastpath logic.
#   - Does NOT change metric selection (synthetic records have no extracted_numbers).
#   - Only activates when INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH is True.
INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH = False  # ✅ default OFF (locked fastpath safe)
CODE_VERSION_INJ_HASH_V1 = "fix41r_inj_hash_optional_include"  # additive version marker

def _inj_hash_should_include() -> bool:
    """Single switch for inclusion; additive-only. Supports env override."""
    try:
        import os
        v = os.getenv("YUREEKA_INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH", "").strip().lower()
        if v in ("1", "true", "yes", "y", "on"):
            return True
        if v in ("0", "false", "no", "n", "off"):
            return False
    except Exception:
        return bool(globals().get("INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH", False))

# Goal:
#   - Align injected URL "new data" identity semantics with baseline sources:
#       If an injected URL is PERSISTED as a successful snapshot, it should
#       participate in snapshot hash inputs by default (unless explicitly disabled).
#   - Preserve existing safety switch INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH
#     and its env override for emergency forcing.
#
# Controls:
#   - Default behavior (policy-aligned): ON when persisted injected URLs exist.
#   - Explicit disable: env YUREEKA_EXCLUDE_INJECTED_URLS_FROM_SNAPSHOT_HASH=1
#   - Explicit force include: env YUREEKA_INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH=1
#
# Notes:
#   - Fastpath logic is NOT modified.
#   - This only affects hash identity input construction; metric selection remains unchanged.
INJECTED_URL_HASH_POLICY_ALIGN_WITH_BASELINE = True  # ✅ default ON (policy-aligned)

def _inj_hash_policy_explicit_disable() -> bool:
    try:
        import os
        v = os.getenv("YUREEKA_EXCLUDE_INJECTED_URLS_FROM_SNAPSHOT_HASH", "").strip().lower()
        return v in ("1", "true", "yes", "y", "on")
    except Exception:
        return False

def _inj_hash_policy_should_include(persisted_injected_urls) -> bool:
    """Policy-aligned include decision for injected URLs in hash identity.

    - If explicitly disabled via env, returns False.
    - If explicitly forced via existing switch/env, returns True.
    - Otherwise, when policy-align is enabled and persisted injected URLs exist, returns True.
    - Else, falls back to legacy _inj_hash_should_include().
    """
    try:
        if _inj_hash_policy_explicit_disable():
            return False
        # Respect existing forcing mechanism first
        if _inj_hash_should_include():
            return True
        if bool(globals().get("INJECTED_URL_HASH_POLICY_ALIGN_WITH_BASELINE", True)) and (persisted_injected_urls or []):
            return True
    except Exception:
        return _inj_hash_should_include()

def _inj_hash_add_synthetic_sources(
    baseline_sources_cache: Any,
    injected_persisted_urls: list,
    now_iso: str = ""
) -> tuple:
    """
    Return (bsc_augmented, added_urls, reasons_by_url) without mutating the original list.
    Synthetic records are url-only, deterministic, and safe for selection logic.
    """
    reasons = {}
    added = []
    try:
        bsc = list(baseline_sources_cache) if isinstance(baseline_sources_cache, list) else []
        inj = _inj_diag_norm_url_list(injected_persisted_urls or [])
        if not inj:
            return (bsc, added, reasons)

        existing = set(_inj_diag_hash_inputs_from_bsc(bsc))
        for u in inj:
            if u in existing:
                reasons[u] = "present_in_bsc"
                continue
            # Add synthetic source record (no numbers) so hash includes the URL deterministically
            added.append(u)
            reasons[u] = "added_synthetic_for_hash"
            bsc.append({
                "url": u,
                "source_url": u,
                "status": "fetched",
                "status_detail": "synthetic_injected_for_hash",
                "numbers_found": 0,
                "fetched_at": now_iso or "",
                "fingerprint": "",
                "extracted_numbers": [],
                "__inj_synthetic": True,
            })

        # Keep deterministic ordering identical to existing conventions
        bsc = sorted(bsc, key=lambda x: str((x or {}).get("url") or ""))
        return (bsc, added, reasons)
    except Exception:
        pass
        try:
            return (list(baseline_sources_cache) if isinstance(baseline_sources_cache, list) else [], [], {})
        except Exception:
            return ([], [], {})


# Purpose:
# - Make injected URL admission deterministic & auditable across Analysis/Evolution.
# - Promote UI raw/diag fields into web_context['extra_urls'] (the admission input).
# - Synthesize a minimal web_context['diag_injected_urls'] when fetch_web_context was bypassed.
# - Pure wiring/diagnostics only: no scraping, no selection changes.
_URL_RE_FIX2D66 = None

def _fix2d66_extract_urls_from_text(text: str) -> list:
    """Extract http(s) URLs from arbitrary text (stable, conservative)."""
    global _URL_RE_FIX2D66
    try:
        if _URL_RE_FIX2D66 is None:
            import re as _re
            _URL_RE_FIX2D66 = _re.compile(r"https?://[^\s\]\)\"\'<>]+", _re.I)
        if not isinstance(text, str):
            return []
        return [u.strip() for u in _URL_RE_FIX2D66.findall(text) if isinstance(u, str) and u.strip()]
    except Exception:
        return []


def _fix2d66_collect_injected_urls(web_context: dict, question_text: str = "") -> list:
    """Collect **only user-intended** injected URLs.

    Important: do NOT treat generic wc['extra_urls'] (often used for seed/forced source pools)
    as injection unless Evolution explicitly marked it with __yureeka_extra_urls_are_injection_v1.
    """
    try:
        wc = web_context if isinstance(web_context, dict) else {}
        found: list = []
        # Primary: dedicated injected-url extractor (REFACTOR32 semantics)
        try:
            found.extend(_yureeka_extract_injected_urls_v1(wc))
        except Exception:
            pass
        # Last resort: URL pasted into the prompt/question text
        if isinstance(question_text, str) and question_text.strip():
            try:
                found.extend(_fix2d66_extract_urls_from_text(question_text))
            except Exception:
                pass
        # Normalize + stable de-dup
        try:
            norm = _inj_diag_norm_url_list(found)
        except Exception:
            norm = [u for u in found if isinstance(u, str) and (u.startswith("http://") or u.startswith("https://"))]
        out: list = []
        seen = set()
        for u in norm:
            uu = str(u or "").strip()
            if not uu or uu in seen:
                continue
            seen.add(uu)
            out.append(uu)
        return out
    except Exception:
        return []


def _fix2d66_promote_injected_urls(web_context: dict, question_text: str = "", stage: str = "") -> dict:
    """Promote injected URLs into web_context in a way that cannot poison production runs.

    - Only runs when **true user injection URLs** are present (via UI fields / explicit marker / prompt URL).
    - Does NOT auto-fill diag_extra_urls_ui_raw (that field is UI-owned; writing to it can make
      production runs look like injection runs).
    """
    try:
        if not isinstance(web_context, dict):
            return web_context
        inj = _fix2d66_collect_injected_urls(web_context or {}, question_text=question_text)
        if not inj:
            return web_context

        # Mark for downstream semantics (REFACTOR32 contract)
        try:
            web_context["__yureeka_extra_urls_are_injection_v1"] = True
            web_context["__yureeka_injected_urls_v1"] = list(inj)
        except Exception:
            pass

        # Promote into extra_urls (merge, stable)
        try:
            cur = web_context.get("extra_urls")
            cur_list = list(cur) if isinstance(cur, (list, tuple)) else []
            merged = _inj_diag_norm_url_list(cur_list + list(inj))
            seen = set()
            out = []
            for u in merged:
                uu = str(u or "").strip()
                if not uu or uu in seen:
                    continue
                seen.add(uu)
                out.append(uu)
            web_context["extra_urls"] = out
        except Exception:
            try:
                web_context["extra_urls"] = list(inj)
            except Exception:
                pass

        # Ensure diag payload exists for traceability, WITHOUT mutating UI fields
        try:
            _d = web_context.get("diag_injected_urls")
            if not isinstance(_d, dict):
                _d = {}
                web_context["diag_injected_urls"] = _d
            if isinstance(_d, dict):
                try:
                    _d.setdefault("run_id", web_context.get("diag_run_id") or _inj_diag_make_run_id(stage or "run"))
                except Exception:
                    pass
                # prefer actual UI raw if present; otherwise synthesize
                _ui_raw = ""
                try:
                    _ui_raw = str(web_context.get("diag_extra_urls_ui_raw") or web_context.get("extra_urls_ui_raw") or "")
                except Exception:
                    _ui_raw = ""
                if not _ui_raw.strip():
                    _ui_raw = "\n".join(list(inj))
                _d.setdefault("ui_raw", _ui_raw)
                _d.setdefault("ui_norm", list(inj))
                _d.setdefault("intake_norm", list(inj))
                _d.setdefault("admitted", list(inj))
        except Exception:
            pass

        # lightweight debug
        try:
            web_context["__fix2d66_injected_urls_promoted_v1"] = True
            web_context["__fix2d66_injected_urls_count_v1"] = int(len(inj))
            web_context["__fix2d66_stage_v1"] = str(stage or "")
        except Exception:
            pass

        return web_context
    except Exception:
        return web_context


def _fix2d66_extract_urls_from_text(text: str) -> list:
    try:
        if not isinstance(text, str):
            return []
        t = text.strip()
        if not t:
            return []
        # conservative URL matcher
        urls = re.findall(r"https?://[^\s\]\)\}\>\"']+", t)
        return [u.strip() for u in urls if isinstance(u, str) and u.strip()]
    except Exception:
        return []


def _fix2d66_collect_injected_urls_from_wc(web_context: dict, question: str = "") -> dict:
    """Return {ui_raw, ui_norm, intake_norm} for **true injection URLs** only.

    NOTE: This MUST NOT treat seed/forced source pools (often carried in wc['extra_urls'])
    as injection unless Evolution explicitly marked them with __yureeka_extra_urls_are_injection_v1.
    """
    wc = web_context if isinstance(web_context, dict) else {}

    # UI-owned raw injection field (do not synthesize from other fields)
    ui_raw = ""
    for k in ("diag_extra_urls_ui_raw", "extra_urls_ui_raw"):
        v = wc.get(k)
        if isinstance(v, str) and v.strip():
            ui_raw = v
            break

    candidates: list = []
    try:
        candidates.extend(_yureeka_extract_injected_urls_v1(wc))
    except Exception:
        pass

    # Also allow URLs embedded in the question text (rare, but helpful for CLI/API use)
    if isinstance(question, str) and question.strip():
        try:
            candidates.extend(_fix2d66_extract_urls_from_text(question))
        except Exception:
            pass

    try:
        norm = _inj_diag_norm_url_list(candidates)
    except Exception:
        norm = [x for x in candidates if isinstance(x, str) and x.strip()]

    return {
        "ui_raw": ui_raw,
        "ui_norm": list(norm),
        "intake_norm": list(norm),
    }




def _fix2d66_promote_injection_in_web_context(web_context: dict, question: str = "") -> dict:
    """Mutate web_context in-place: ensure extra_urls + diag_injected_urls reflect UI/raw injection."""
    wc = web_context if isinstance(web_context, dict) else {}
    info = _fix2d66_collect_injected_urls_from_wc(wc, question=question)
    intake = info.get('intake_norm') or []


    # Mark for downstream semantics (REFACTOR32 contract)
    try:
        if intake:
            wc['__yureeka_extra_urls_are_injection_v1'] = True
            wc['__yureeka_injected_urls_v1'] = list(intake)
    except Exception:
        pass

    # Promote into extra_urls when missing/empty
    try:
        cur = wc.get('extra_urls')
        needs = (not isinstance(cur, (list, tuple)) or not cur)
        if needs and intake:
            wc['extra_urls'] = list(intake)
    except Exception:
        pass

    # Ensure diag_injected_urls exists even if fetch_web_context wasn't called
    try:
        wc.setdefault('diag_injected_urls', {})
        if isinstance(wc.get('diag_injected_urls'), dict):
            d = wc['diag_injected_urls']
            # Fill minimally if absent
            d.setdefault('run_id', wc.get('diag_run_id') or _inj_diag_make_run_id('inj'))
            if info.get('ui_raw') and not d.get('ui_raw'):
                d['ui_raw'] = info.get('ui_raw')
            d.setdefault('ui_norm', info.get('ui_norm') or [])
            d.setdefault('intake_norm', intake)
            # If no admitted present, treat intake as admitted universe (diagnostic only)
            if not isinstance(d.get('admitted'), list) or not d.get('admitted'):
                d['admitted'] = list(intake)
    except Exception:
        pass

    # breadcrumb
    try:
        wc.setdefault('debug', {})
        if isinstance(wc.get('debug'), dict):
            wc['debug'].setdefault('fix2d66_injection', {})
            if isinstance(wc['debug'].get('fix2d66_injection'), dict):
                wc['debug']['fix2d66_injection'].update({
                    'promoted': bool(intake),
                    'intake_count': int(len(intake)),
                    'intake_set_hash': _inj_diag_set_hash(intake) if intake else '',
                })
    except Exception:
        return wc

# Purpose:
# - Populate inj_trace_v1 attempted/persisted fields from *real* artifacts when
#   the upstream diag_injected_urls payload is partial (common in baseline/no-injection
#   or fastpath replay scenarios).
# - Pure diagnostics only: does NOT alter control flow, hashing, scraping, or selection.
#
# Artifacts supported:
#   - baseline_sources_cache (BSC): list of per-url snapshot dicts
#   - scraped_meta: dict keyed by url with status/status_detail/clean_text_len

def _inj_trace_v1_enrich_diag_from_bsc(diag: dict, baseline_sources_cache: list) -> dict:
    """Add attempted/persisted evidence into diag_injected_urls from baseline_sources_cache."""
    try:
        d = diag if isinstance(diag, dict) else {}
        bsc = baseline_sources_cache if isinstance(baseline_sources_cache, list) else []
        # If attempted already present, do not overwrite (avoid clobbering richer traces).
        if not isinstance(d.get("attempted"), list) or not d.get("attempted"):
            attempted = []
            for row in bsc:
                if not isinstance(row, dict):
                    continue
                u = str(row.get("url") or row.get("source_url") or "").strip()
                if not u:
                    continue
                st = str(row.get("status") or row.get("fetch_status") or "").strip() or "unknown"
                rs = str(row.get("status_detail") or row.get("fail_reason") or "").strip()
                clen = row.get("clean_text_len") or row.get("content_len") or 0
                try:
                    clen_i = int(clen)
                except Exception:
                    pass
                    clen_i = 0
                attempted.append({"url": u, "status": st, "reason": rs, "content_len": clen_i})
            if attempted:
                d["attempted"] = attempted

        # Persisted: if missing, derive from successful snapshot rows in BSC.
        if not isinstance(d.get("persisted"), (list, str)) or not d.get("persisted"):
            persisted = []
            for row in bsc:
                if not isinstance(row, dict):
                    continue
                u = str(row.get("url") or row.get("source_url") or "").strip()
                if not u:
                    continue
                st = str(row.get("status") or row.get("fetch_status") or "").lower().strip()
                if st in ("success", "ok", "fetched"):
                    persisted.append(u)
            if persisted:
                d["persisted"] = _inj_diag_norm_url_list(persisted)

        return d
    except Exception:
        return diag if isinstance(diag, dict) else {}

def _inj_trace_v1_enrich_diag_from_scraped_meta(diag: dict, scraped_meta: dict, extra_urls: list) -> dict:
    """Add attempted/persisted evidence into diag_injected_urls from scraped_meta (evolution-side)."""
    try:
        d = diag if isinstance(diag, dict) else {}
        sm = scraped_meta if isinstance(scraped_meta, dict) else {}
        xs = _inj_diag_norm_url_list(extra_urls or [])
        if not xs:
            return d

        # attempted rows for injected urls
        if not isinstance(d.get("attempted"), list) or not d.get("attempted"):
            attempted = []
            for u in xs:
                m = sm.get(u) if isinstance(sm.get(u), dict) else {}
                st = str(m.get("status") or m.get("fetch_status") or "").strip() or "unknown"
                rs = str(m.get("status_detail") or m.get("fail_reason") or "").strip()
                clen = m.get("clean_text_len") or m.get("content_len") or 0
                try:
                    clen_i = int(clen)
                except Exception:
                    pass
                    clen_i = 0
                attempted.append({"url": u, "status": st, "reason": rs, "content_len": clen_i})
            d["attempted"] = attempted

        # persisted success urls (only for injected)
        if not isinstance(d.get("persisted"), (list, str)) or not d.get("persisted"):
            persisted = []
            for a in (d.get("attempted") or []):
                if not isinstance(a, dict):
                    continue
                st = str(a.get("status") or "").lower().strip()
                if st in ("success", "ok", "fetched"):
                    u = str(a.get("url") or "").strip()
                    if u:
                        persisted.append(u)
            d["persisted"] = _inj_diag_norm_url_list(persisted)

        return d
    except Exception:
        return diag if isinstance(diag, dict) else {}


def scrape_url(url: str) -> Optional[str]:
    """
    Scrape webpage content.

    Priority:
      1) ScrapingDog (if SCRAPINGDOG_KEY is present)
      2) Safe fallback: direct requests + BeautifulSoup visible-text extraction

    Returns:
      - Clean visible text (<= 3000 chars) or None
    """
    import re

    url_s = (url or "").strip()
    if not url_s:
        return None

    def _clean_html_to_text(html: str) -> str:
        try:
            from bs4 import BeautifulSoup  # type: ignore
            soup = BeautifulSoup(html or "", "html.parser")
            for tag in soup(["script", "style", "nav", "footer", "header", "aside", "noscript", "form"]):
                try:
                    tag.decompose()
                except Exception:
                    pass
            txt = soup.get_text(separator="\n")
        except Exception:
            pass
            # fallback: strip tags
            txt = re.sub(r"(?is)<(script|style|noscript).*?>.*?</\1>", " ", html or "")
            txt = re.sub(r"(?is)<[^>]+>", " ", txt)
        # normalize whitespace
        lines = [ln.strip() for ln in (txt or "").splitlines() if ln.strip()]
        out = "\n".join(lines)
        out = re.sub(r"\n{3,}", "\n\n", out)
        return out.strip()

    def _direct_fetch(u: str) -> Optional[str]:
        try:
            headers = {
                "User-Agent": (
                    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/120.0.0.0 Safari/537.36"
                ),
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9",
                "Cache-Control": "no-cache",
                "Pragma": "no-cache",
            }
            resp = requests.get(u, headers=headers, timeout=12, allow_redirects=True)
            if resp.status_code >= 400:
                return None

            ctype = (resp.headers.get("Content-Type") or "").lower()


            ctype = (resp.headers.get("Content-Type") or "").lower()
            # REFACTOR85: handle PDFs (IEA/others) instead of treating them as failed:no_text
            if ("application/pdf" in ctype) or u.lower().endswith(".pdf"):
                try:
                    _pdf_txt = _extract_pdf_text_from_bytes(resp.content, max_pages=8, max_chars=7000)
                    if isinstance(_pdf_txt, str) and _pdf_txt.strip():
                        return _pdf_txt.strip()[:7000]
                except Exception:
                    return None
                return None

            cleaned = _clean_html_to_text(resp.text or "")
            cleaned = cleaned.strip()
            if not cleaned:
                return None
            return cleaned[:3000]
        except Exception:
            return None

    # 1) ScrapingDog path (if configured)
    if globals().get("SCRAPINGDOG_KEY"):
        try:
            params = {"api_key": SCRAPINGDOG_KEY, "url": url_s, "dynamic": "false"}
            resp = requests.get("https://api.scrapingdog.com/scrape", params=params, timeout=15)
            if resp.status_code < 400:
                cleaned = _clean_html_to_text(resp.text or "").strip()
                if cleaned:
                    return cleaned[:3000]
        except Exception:
            pass  # fall through to direct fetch

    # 2) Safe fallback
    return _direct_fetch(url_s)


def fetch_web_context(
    query: str,
    num_sources: int = 3,
    *,
    fallback_mode: bool = False,
    fallback_urls: list = None,
    existing_snapshots: Any = None,   # <-- ADDITIVE
    extra_urls: Any = None,
    diag_run_id: str = "",
    diag_extra_urls_ui_raw: Any = None,
    identity_only: bool = False,
    # - Default False: no behavior change.
    # - When True: normalized extra URLs will be appended to admitted list for scraping.
    force_scrape_extra_urls: bool = False,
    # - Default False: no behavior change.
    # - When True: normalized extra URLs will be appended to admitted list (not just scrape list),
    #   enabling deterministic admission of injected URLs when delta exists.
    force_admit_extra_urls: bool = False,
) -> dict:

    """
    Web context collector used by BOTH analysis + evolution.

    Enhancements:
    - Dashboard telemetry (sources found / HQ / admitted / scraped / success)
    - Keeps snapshot-friendly scraped_meta (fingerprint + extracted_numbers + numbers_found)
    - Uses scrape_url() which now has ScrapingDog + safe fallback scraper
    - Restores legacy contract: web_context["sources"] AND ["web_sources"]
    """
    import re
    from datetime import datetime, timezone

    try:
        _qtxt = str((analysis or {}).get('question') or (analysis or {}).get('query') or '')
        web_context = _fix2d66_promote_injected_urls(web_context or {}, question_text=_qtxt, stage='analysis_attach')
    except Exception:
        pass

    def _now_iso() -> str:
        return datetime.now(timezone.utc).isoformat()

    def _is_probably_url(s: str) -> bool:
        if not s or not isinstance(s, str):
            return False
        t = s.strip()
        if " " in t:
            return False
        if re.match(r"^https?://", t, flags=re.I):
            return True
        if re.match(r"^[a-z0-9.-]+\.[a-z]{2,}(/.*)?$", t, flags=re.I):
            return True
        return False

    def _normalize_url(s: str) -> str:
        t = (s or "").strip()
        if not t:
            return ""
        if re.match(r"^https?://", t, flags=re.I):
            return t
        if re.match(r"^[a-z0-9.-]+\.[a-z]{2,}(/.*)?$", t, flags=re.I):
            return "https://" + t
        return ""


    out = {
        "query": query,
        "sources": [],        # ✅ legacy key many downstream blocks expect
        "web_sources": [],    # ✅ newer key used by evolution/snapshots
        "search_results": [],
        "scraped_meta": {},
        "scraped_content": {},
        "errors": [],
        "status": "ok",
        "status_detail": "",
        "fetched_at": _now_iso(),
        "debug_counts": {},   # ✅ telemetry for dashboard + JSON debugging
    }

    # Record explicit last-good snapshot fallback usage (never silent).
    def _record_last_good_fallback(_url: str) -> None:
        try:
            if not isinstance(_url, str) or not _url.strip():
                return
            out.setdefault("debug_counts", {})
            out["debug_counts"]["fallback_last_good_snapshot_used"] = int(out["debug_counts"].get("fallback_last_good_snapshot_used") or 0) + 1
            out["debug_counts"].setdefault("fallback_last_good_snapshot_used_urls", [])
            if _url not in out["debug_counts"]["fallback_last_good_snapshot_used_urls"]:
                out["debug_counts"]["fallback_last_good_snapshot_used_urls"].append(_url)
        except Exception:
            pass

    snap_lookup = {}
    try:
        def _snap_variants(u: str) -> list:
            try:
                if not isinstance(u, str):
                    return []
                u0 = u.strip()
                if not u0:
                    return []
                nu = _normalize_url(u0) or u0
                cands = []
                for x in (u0, nu):
                    if isinstance(x, str) and x.strip():
                        cands.append(x.strip())

                outv = []
                for x in cands:
                    x2 = x.rstrip("/")
                    if x2 and x2 not in outv:
                        outv.append(x2)
                    if x2 and (x2 + "/") not in outv:
                        outv.append(x2 + "/")

                more = []
                for x in list(outv):
                    if x.lower().startswith("https://"):
                        more.append("http://" + x[8:])
                    elif x.lower().startswith("http://"):
                        more.append("https://" + x[7:])
                for x in more:
                    if x and x not in outv:
                        outv.append(x)

                final = []
                for x in outv:
                    final.append(x)
                    try:
                        if x.lower().startswith("https://www."):
                            final.append("https://" + x[len("https://www."):])
                        elif x.lower().startswith("http://www."):
                            final.append("http://" + x[len("http://www."):])
                    except Exception:
                        pass

                seen = set()
                uniq = []
                for x in final:
                    if not isinstance(x, str):
                        continue
                    t = x.strip()
                    if not t or t in seen:
                        continue
                    seen.add(t)
                    uniq.append(t)
                return uniq
            except Exception:
                return [u] if isinstance(u, str) else []

        def _choose_better(a: dict, b: dict) -> dict:
            """Pick the better snapshot (prefer more numbers; then newer fetched_at)."""
            try:
                if not isinstance(a, dict):
                    return b
                if not isinstance(b, dict):
                    return a
                an = int(a.get("numbers_found") or 0)
                bn = int(b.get("numbers_found") or 0)
                if bn != an:
                    return b if bn > an else a
                af = str(a.get("fetched_at") or "")
                bf = str(b.get("fetched_at") or "")
                if bf and af and bf != af:
                    return b if bf > af else a
            except Exception:
                pass
            return a

        _cands = []
        if isinstance(existing_snapshots, dict):
            for _k, _v in (existing_snapshots or {}).items():
                if isinstance(_v, dict):
                    if not _v.get("url") and isinstance(_k, str) and _k.strip():
                        _vv = dict(_v)
                        _vv["url"] = _k.strip()
                        _v = _vv
                    if _v.get("url"):
                        _cands.append(_v)
        elif isinstance(existing_snapshots, list):
            for _v in (existing_snapshots or []):
                if isinstance(_v, dict) and _v.get("url"):
                    _cands.append(_v)

        for s in _cands:
            try:
                u0 = str(s.get("url") or "").strip()
                if not u0:
                    continue
                for key in _snap_variants(u0):
                    if not key:
                        continue
                    if key not in snap_lookup:
                        snap_lookup[key] = s
                    else:
                        snap_lookup[key] = _choose_better(snap_lookup.get(key), s)
            except Exception:
                continue

        def _get_prev_snapshot(u: str):
            try:
                if not isinstance(u, str) or not u.strip():
                    return None
                if not isinstance(snap_lookup, dict):
                    return None
                for key in _snap_variants(u):
                    if key in snap_lookup:
                        return snap_lookup.get(key)
                return snap_lookup.get(u.strip())
            except Exception:
                return None
    except Exception:
        snap_lookup = snap_lookup if isinstance(snap_lookup, dict) else {}
        def _get_prev_snapshot(u: str):
            try:
                return snap_lookup.get(u) if isinstance(snap_lookup, dict) else None
            except Exception:
                return None

    extractor_fp = get_extractor_fingerprint()


    q = (query or "").strip()
    if not q:
        out["status"] = "no_query"
        out["status_detail"] = "empty_query"
        return out

    # 1) Search (SerpAPI) OR fallback_urls
    search_results = []
    urls_raw = []

    if not fallback_mode:
        try:
            sr = search_serpapi(q, num_results=10) or []
            if isinstance(sr, list):
                search_results = sr
        except Exception as e:
            out["errors"].append(f"search_failed:{type(e).__name__}")
            search_results = []

        out["search_results"] = search_results

        # Extract urls from results
        for r in (search_results or []):
            if isinstance(r, dict):
                u = (r.get("link") or r.get("url") or "").strip()
                if _is_probably_url(u):
                    urls_raw.append(u)
            elif isinstance(r, str):
                if _is_probably_url(r):
                    urls_raw.append(r.strip())

    else:
        # Evolution fallback: use provided URLs
        if isinstance(fallback_urls, list):
            for u in fallback_urls:
                if isinstance(u, str) and _is_probably_url(u.strip()):
                    urls_raw.append(u.strip())

    # 2) Compute "HQ" counts (like old version)
    total_found = len(search_results) if not fallback_mode else len(urls_raw)
    hq_count = 0

    try:
        fn_rel = globals().get("classify_source_reliability")
        if callable(fn_rel) and not fallback_mode:
            for r in (search_results or []):
                if not isinstance(r, dict):
                    continue
                u = (r.get("link") or "").strip()
                if not u:
                    continue
                label = fn_rel(u) or ""
                if "✅" in str(label):
                    hq_count += 1
    except Exception:
        pass
        hq_count = 0

    # 3) Sanitize + normalize + dedupe
    normed = []
    seen = set()
    for u in (urls_raw or []):
        nu = _normalize_url(u)
        if not nu:
            continue
        if nu in seen:
            continue
        seen.add(nu)
        normed.append(nu)

    # admitted for scraping (top N)
    try:
        n = int(num_sources or 3)
    except Exception:
        pass
        n = 3
    n = max(1, min(12, n))
    admitted = normed[:n] if not fallback_mode else normed  # fallback_mode typically wants all

    #
    # Why:
    # - In evolution injection scenarios, extra URLs may be deliberately outside the normal
    #   admitted universe (domain allowlists, heuristics, etc.), but the user's intent is
    #   to attempt a fetch so the run can either persist a snapshot or fail with a concrete reason.
    #
    # Behavior:
    # - When force_scrape_extra_urls=True and normalized extras exist, append them into the
    #   admitted list (deduped, stable order) so downstream scraping attempts occur.
    #
    # Safety:
    # - Default is False (no change for normal runs).
    # - Never raises.
    try:
        if bool(force_scrape_extra_urls):
            _fx8_extras = []
            if "_extras" in locals() and isinstance(_extras, list):
                _fx8_extras = [u for u in _extras if isinstance(u, str) and u.strip()]
            if _fx8_extras and isinstance(admitted, list):
                _seen = set([u for u in admitted if isinstance(u, str)])
                for _u in _fx8_extras:
                    if _u not in _seen:
                        admitted.append(_u)
                        _seen.add(_u)
                # breadcrumb for diagnostics
                try:
                    out.setdefault("debug_counts", {})
                    if isinstance(out.get("debug_counts"), dict):
                        out["debug_counts"].setdefault("fix41afc8", {})
                        if isinstance(out["debug_counts"].get("fix41afc8"), dict):
                            out["debug_counts"]["fix41afc8"].update({
                                "force_scrape_extra_urls": True,
                                "force_scrape_extra_urls_count": int(len(_fx8_extras)),
                            })
                except Exception:
                    pass
    except Exception:
        pass


    try:
        _extras_in = extra_urls or []
        _extras = []
        _canon_map = {}
        if isinstance(_extras_in, str):
            _extras_in = [u.strip() for u in _extras_in.splitlines()]
        if isinstance(_extras_in, (list, tuple)):
            for u in _extras_in:
                u = str(u or "").strip()
                if not u:
                    continue
                if not (u.startswith("http://") or u.startswith("https://")):
                    continue
                _canon = _canonicalize_injected_url(u) or u
                _extras.append(_canon)
                try:
                    _canon_map[u] = _canon
                except Exception:
                    pass
        _seen = set()
        merged = []
        for u in _extras + (admitted or []):
            if u in _seen:
                continue
            _seen.add(u)
            merged.append(u)
        admitted = merged
        out.setdefault("debug", {})
        if isinstance(out.get("debug"), dict):
            out["debug"].setdefault("fwc_extra_urls", {})
            out["debug"]["fwc_extra_urls"]["extra_urls_count"] = int(len(_extras))
            out["debug"]["fwc_extra_urls"]["admitted_count_after_merge"] = int(len(admitted or []))
            out["debug"]["fwc_extra_urls"]["extra_urls"] = _extras[:20]
    except Exception:
        pass


    # Records: UI->intake->admitted, and later enriches with scrape outcomes.
    try:
        _diag_run = str(diag_run_id or "") or _inj_diag_make_run_id("analysis")
        out["diag_run_id"] = out.get("diag_run_id") or _diag_run

        _ui_raw = diag_extra_urls_ui_raw if diag_extra_urls_ui_raw is not None else extra_urls
        _ui_norm = _inj_diag_norm_url_list(_ui_raw)
        _intake_norm = list(_extras or []) if "_extras" in locals() and isinstance(_extras, list) else _inj_diag_norm_url_list(extra_urls)

        out["diag_injected_urls"] = {
            "run_id": _diag_run,
            "ui_raw": _ui_raw if isinstance(_ui_raw, (str, list, tuple)) else str(_ui_raw or ""),
            "ui_norm": _ui_norm,
            "intake_norm": _intake_norm,
            "admitted": list(admitted or []),
            "attempted": [],
            "persisted": [],
            "hash_inputs": [],
            "rebuild_pool": [],
            "rebuild_selected": [],
            "set_hashes": {
                "ui_norm": _inj_diag_set_hash(_ui_norm),
                "intake_norm": _inj_diag_set_hash(_intake_norm),
                "admitted": _inj_diag_set_hash(list(admitted or [])),
            },
            "canon_map": dict(_canon_map) if "_canon_map" in locals() else {},
            "deltas": {
                "ui_minus_intake": sorted(list(set(_ui_norm) - set(_intake_norm))),
                "intake_minus_admitted": sorted(list(set(_intake_norm) - set(list(admitted or [])))),
            },
        }
    except Exception:
        pass


    #
    # Goal:
    # - When force_admit_extra_urls is True, ensure normalized extra_urls are INCLUDED in the
    #   admitted list itself (not only the scrape list). This prevents injected URLs from dying
    #   at "intake_minus_admitted" and allows deterministic fetch/persist behavior.
    #
    # Safety:
    # - Default flag False => no behavior change.
    # - Never raises.
    try:
        if force_admit_extra_urls:
            _fix41afc13_extra = _inj_diag_norm_url_list(extra_urls) if extra_urls else []
            if _fix41afc13_extra:
                _fix41afc13_before = list(admitted or [])
                _fix41afc13_set = set(_inj_diag_norm_url_list(_fix41afc13_before))
                _fix41afc13_added = []
                for _u in _fix41afc13_extra:
                    if _u and _u not in _fix41afc13_set:
                        _fix41afc13_before.append(_u)
                        _fix41afc13_set.add(_u)
                        _fix41afc13_added.append(_u)
                if _fix41afc13_added:
                    admitted = _fix41afc13_before
                    out.setdefault("debug_counts", {})
                    if isinstance(out.get("debug_counts"), dict):
                        out["debug_counts"].update({
                            "forced_admit_extra_urls_count": int(len(_fix41afc13_added)),
                        })
                    out.setdefault("debug", {})
                    if isinstance(out.get("debug"), dict):
                        out["debug"].setdefault("fix41afc13", {})
                        if isinstance(out["debug"].get("fix41afc13"), dict):
                            out["debug"]["fix41afc13"].update({
                                "forced_admit_applied": True,
                                "forced_admit_added": list(_fix41afc13_added),
                                "forced_admit_total_extra": int(len(_fix41afc13_extra)),
                            })
    except Exception:
        pass

    out["sources"] = admitted
    out["web_sources"] = admitted

    # Telemetry before scrape
    out["debug_counts"].update({
        "total_found": int(total_found),
        "high_quality": int(hq_count),
        "admitted_for_scraping": int(len(admitted)),
        "fallback_mode": bool(fallback_mode),
    })

    # Dashboard info (restored)
    try:
        if not fallback_mode:
            st.info(
                f"🔍 Sources Found: **{out['debug_counts']['total_found']} total** | "
                f"**{out['debug_counts']['high_quality']} high-quality** | "
                f"Scraping **{out['debug_counts']['admitted_for_scraping']}**"
            )
        else:
            st.info(
                f"🧩 Fallback Sources: **{out['debug_counts']['admitted_for_scraping']}** (no SerpAPI search)"
            )
    except Exception:
        pass

    if not admitted:
        out["status"] = "no_sources"
        out["status_detail"] = "empty_sources_after_filter"
        return out

    try:
        if bool(identity_only):
            out["status"] = out.get("status") or "ok"
            out["status_detail"] = out.get("status_detail") or "identity_only"
            return out
    except Exception:
        pass


    # 4) Scrape + extract numbers (snapshot-friendly scraped_meta)
    fn_fp = globals().get("fingerprint_text")
    fn_extract = globals().get("extract_numbers_with_context") or globals().get("extract_numeric_candidates") or globals().get("extract_numbers_from_text")

    scraped_attempted = 0
    scraped_ok_text = 0
    scraped_ok_numbers = 0
    scraped_failed = 0

    # optional progress bar
    progress = None
    try:
        progress = st.progress(0)
    except Exception:
        pass
        progress = None

    for i, url in enumerate(admitted):
        scraped_attempted += 1

        meta = {
            "url": url,
            "fetched_at": _now_iso(),
            "status": "failed",
            "status_detail": "",
            "content_type": "",
            "content_len": 0,
            "clean_text_len": 0,
            "fingerprint": None,
            "numbers_found": 0,
            "extracted_numbers": [],
            "content": "",
            "clean_text": "",
        }

        try:
            text = scrape_url(url)  # ✅ ScrapingDog + fallback inside scrape_url
            if not text or not str(text).strip():
                meta["status"] = "failed"
                meta["status_detail"] = "failed:no_text"

                # Why:
                # - External flakiness can yield failed:no_text even for stable sources.
                # - If existing_snapshots contains a last-good snapshot for this URL,
                #   reuse its extracted_numbers with explicit provenance (never silent).
                try:
                    _prev = _get_prev_snapshot(url) if isinstance(snap_lookup, dict) else None
                    _prev_nums = _prev.get("extracted_numbers") if isinstance(_prev, dict) else None
                    if isinstance(_prev_nums, list) and _prev_nums:
                        meta["status"] = "fetched"
                        meta["status_detail"] = "fallback:last_good_snapshot"
                        meta["fallback_used"] = True
                        meta["fallback_reason"] = "failed:no_text"
                        meta["fallback_source"] = "existing_snapshots"
                        meta["fallback_snapshot_fetched_at"] = _prev.get("fetched_at") if isinstance(_prev, dict) else None
                        meta["reused_snapshot"] = True

                        # Reuse snapshot identity so downstream snapshot hashes remain deterministic.
                        meta["fingerprint"] = _prev.get("fingerprint") if isinstance(_prev, dict) else meta.get("fingerprint")
                        meta["extractor_fingerprint"] = _prev.get("extractor_fingerprint") if isinstance(_prev, dict) else meta.get("extractor_fingerprint")
                        if not meta.get("extractor_fingerprint"):
                            meta["extractor_fingerprint"] = extractor_fp

                        meta["extracted_numbers"] = list(_prev_nums)
                        meta["numbers_found"] = len(_prev_nums)

                        # Best-effort: keep content fields non-empty if the snapshot had them.
                        _pc = (_prev.get("content") if isinstance(_prev, dict) else "") or ""
                        _pt = (_prev.get("clean_text") if isinstance(_prev, dict) else "") or ""
                        meta["content"] = _pc or _pt or ""
                        meta["clean_text"] = _pt or _pc or ""
                        meta["content_len"] = len(meta.get("content") or "")
                        meta["clean_text_len"] = len(meta.get("clean_text") or "")

                        out["scraped_meta"][url] = meta
                        out["scraped_content"][url] = meta.get("clean_text") or meta.get("content") or ""

                        scraped_ok_text += 1
                        if meta.get("numbers_found", 0) > 0:
                            scraped_ok_numbers += 1
                        # REFACTOR75: we recovered; do not count as scraped_failed.
                        try:
                            _record_last_good_fallback(url)
                        except Exception:
                            pass
                        continue
                except Exception:
                    pass
                scraped_failed += 1
                out["scraped_meta"][url] = meta
            else:
                cleaned = str(text).strip()
                meta["status"] = "fetched"
                meta["status_detail"] = "success"
                meta["content"] = cleaned
                meta["clean_text"] = cleaned
                meta["content_len"] = len(cleaned)
                meta["clean_text_len"] = len(cleaned)

                # fingerprint
                try:
                    if callable(fn_fp):
                        meta["fingerprint"] = fn_fp(cleaned)
                    else:
                        meta["fingerprint"] = fingerprint_text(cleaned) if callable(globals().get("fingerprint_text")) else None
                except Exception:
                    pass
                    meta["fingerprint"] = None

                meta["extractor_fingerprint"] = extractor_fp
                prev = _get_prev_snapshot(url) if isinstance(snap_lookup, dict) else None
                if isinstance(prev, dict):
                    if prev.get("fingerprint") == meta.get("fingerprint") and prev.get("extractor_fingerprint") == extractor_fp:
                        prev_nums = prev.get("extracted_numbers")
                        if isinstance(prev_nums, list) and prev_nums:
                            meta["extracted_numbers"] = prev_nums
                            meta["numbers_found"] = len(prev_nums)
                            meta["reused_snapshot"] = True

                            out["scraped_meta"][url] = meta
                            out["scraped_content"][url] = cleaned

                            scraped_ok_text += 1
                            if meta["numbers_found"] > 0:
                                scraped_ok_numbers += 1

                            if progress:
                                try:
                                    progress.progress((i + 1) / max(1, len(admitted)))
                                except Exception:
                                    pass

                            continue
                meta["reused_snapshot"] = False

                # numeric extraction (analysis-aligned if fn exists)

                def _fix2d69a_norm_extraction_result_REMOVED(_res):
                    """Return a list of number dicts from extractor result.

                    Accepts:
                      - list
                      - tuple/list of (numbers, meta) where numbers is list
                      - None / other -> []
                    """
                    try:
                        if _res is None:
                            return []
                        if isinstance(_res, list):
                            return _res
                        if isinstance(_res, tuple) or isinstance(_res, list):
                            # e.g. (nums, meta)
                            if len(_res) >= 1 and isinstance(_res[0], list):
                                return _res[0]
                        # sometimes a dict wrapper
                        if isinstance(_res, dict) and isinstance(_res.get('numbers'), list):
                            return _res.get('numbers') or []
                    except Exception:
                        pass
                    return []

                nums = []
                meta["fix2d68_extract_attempted"] = bool(callable(fn_extract))
                meta["fix2d68_extract_input_len"] = int(len(cleaned) if isinstance(cleaned, str) else 0)
                try:
                    meta["fix2d68_extract_input_head"] = (cleaned[:200] if isinstance(cleaned, str) else "")
                except Exception:
                    meta["fix2d68_extract_input_head"] = ""

                _fix2d68_errors = []
                if callable(fn_extract):
                    # Robust dispatcher: try source_url, then url, then plain. Do not fail silently.
                    for _mode in ("source_url", "url", "plain"):
                        try:
                            _tmp = None
                            if _mode == "source_url":
                                _tmp = fn_extract(cleaned, source_url=url)
                            elif _mode == "url":
                                _tmp = fn_extract(cleaned, url=url)
                            else:
                                _tmp = fn_extract(cleaned)
                            # FIX2D69A: normalize extractor return (list | (list, meta) | dict | None)
                            if _tmp is None:
                                nums = []
                            elif isinstance(_tmp, list):
                                nums = _tmp
                            elif isinstance(_tmp, tuple) and len(_tmp) >= 1 and isinstance(_tmp[0], list):
                                nums = _tmp[0]
                            elif isinstance(_tmp, dict) and isinstance(_tmp.get("extracted_numbers"), list):
                                nums = _tmp.get("extracted_numbers") or []
                            else:
                                nums = []
                            meta["fix2d68_extract_call_mode"] = _mode
                            break
                        except Exception as _e:
                            _fix2d68_errors.append({"mode": _mode, "error": repr(_e)})
                            nums = []

                if _fix2d68_errors:
                    meta["fix2d68_extract_errors"] = _fix2d68_errors

                # FIX2D69A: normalize extractor return (list | (list, meta) | None)
                _nums_norm = []
                try:
                    if nums is None:
                        _nums_norm = []
                    elif isinstance(nums, list):
                        _nums_norm = nums
                    elif isinstance(nums, tuple) and len(nums) >= 1 and isinstance(nums[0], list):
                        _nums_norm = nums[0]
                    elif isinstance(nums, dict) and isinstance(nums.get("extracted_numbers"), list):
                        _nums_norm = nums.get("extracted_numbers") or []
                    else:
                        _nums_norm = []
                except Exception:
                    _nums_norm = []
                nums = _nums_norm


                if isinstance(nums, list):
                    meta["extracted_numbers"] = nums
                    meta["numbers_found"] = len(nums)
                    # REFACTOR85: last-good numbers fallback when fetch succeeded but extractor yields 0 numbers
                    # (common when a source returns a bot-wall / placeholder page that still has text).
                    if int(meta.get("numbers_found") or 0) == 0:
                        try:
                            _prev = _get_prev_snapshot(url, existing_snapshots)
                        except Exception:
                            _prev = None
                        try:
                            _prev_nums = (_prev.get("extracted_numbers") or []) if isinstance(_prev, dict) else []
                        except Exception:
                            _prev_nums = []
                        if _prev_nums:
                            try:
                                _low = (cleaned or "").lower()
                            except Exception:
                                _low = ""
                            _looks_blocked = False
                            try:
                                if len(cleaned or "") < 500:
                                    _looks_blocked = True
                            except Exception:
                                pass
                            if not _looks_blocked:
                                for _tok in ("captcha", "access denied", "enable javascript", "cloudflare", "unusual traffic", "verify you are", "blocked", "please wait"):
                                    if _tok in _low:
                                        _looks_blocked = True
                                        break
                            if _looks_blocked:
                                try:
                                    meta["status_detail_original"] = str(meta.get("status_detail") or "")
                                except Exception:
                                    pass
                                try:
                                    meta["status_detail"] = "fallback:last_good_snapshot_numbers"
                                except Exception:
                                    pass
                                try:
                                    meta["fallback_used"] = True
                                    meta["reused_snapshot"] = True
                                    meta["fallback_reason"] = "zero_numbers_blocked_or_short_text"
                                    meta["fallback_snapshot_fetched_at"] = (_prev.get("fetched_at") if isinstance(_prev, dict) else None)
                                except Exception:
                                    pass
                                try:
                                    meta["extracted_numbers"] = _prev_nums
                                    meta["numbers_found"] = len(_prev_nums)
                                except Exception:
                                    pass

                    urlv = meta.get("url") or url
                    fpv = meta.get("fingerprint") or ""

                    for n in (meta["extracted_numbers"] or []):
                        if isinstance(n, dict):
                            if "extracted_number_id" not in n:
                                n["extracted_number_id"] = make_extracted_number_id(urlv, fpv, n)
                            if not n.get("source_url"):
                                n["source_url"] = urlv

                    meta["extracted_numbers"] = sort_snapshot_numbers(meta["extracted_numbers"])
                    meta["numbers_found"] = len(meta["extracted_numbers"])

                # If extraction yields zero numbers but a last-good snapshot has numbers,
                # reuse them with explicit provenance (never silent).
                try:
                    if int(meta.get("numbers_found") or 0) <= 0:
                        _prev = _get_prev_snapshot(url) if callable(locals().get("_get_prev_snapshot")) else (snap_lookup.get(url) if isinstance(snap_lookup, dict) else None)
                        _prev_nums = _prev.get("extracted_numbers") if isinstance(_prev, dict) else None
                        if isinstance(_prev_nums, list) and _prev_nums:
                            meta["status"] = "fetched"
                            meta["status_detail"] = "fallback:last_good_snapshot"
                            meta["fallback_used"] = True
                            meta["fallback_reason"] = "numbers_found=0"
                            meta["fallback_source"] = "existing_snapshots"
                            meta["fallback_snapshot_fetched_at"] = _prev.get("fetched_at") if isinstance(_prev, dict) else None
                            meta["reused_snapshot"] = True

                            meta["fingerprint"] = _prev.get("fingerprint") if isinstance(_prev, dict) else meta.get("fingerprint")
                            meta["extractor_fingerprint"] = _prev.get("extractor_fingerprint") if isinstance(_prev, dict) else meta.get("extractor_fingerprint")
                            if not meta.get("extractor_fingerprint"):
                                meta["extractor_fingerprint"] = extractor_fp

                            meta["extracted_numbers"] = list(_prev_nums)
                            meta["numbers_found"] = len(_prev_nums)

                            _pc = (_prev.get("content") if isinstance(_prev, dict) else "") or ""
                            _pt = (_prev.get("clean_text") if isinstance(_prev, dict) else "") or ""
                            if not (meta.get("content") or "").strip():
                                meta["content"] = _pc or _pt or ""
                            if not (meta.get("clean_text") or "").strip():
                                meta["clean_text"] = _pt or _pc or ""
                            meta["content_len"] = len(meta.get("content") or "")
                            meta["clean_text_len"] = len(meta.get("clean_text") or "")

                            try:
                                _record_last_good_fallback(url)
                            except Exception:
                                pass
                except Exception:
                    pass

                out["scraped_meta"][url] = meta
                out["scraped_content"][url] = cleaned

                scraped_ok_text += 1
                if meta["numbers_found"] > 0:
                    scraped_ok_numbers += 1

        except Exception as e:
            meta["status"] = "failed"
            meta["status_detail"] = f"failed:exception:{type(e).__name__}"

            # Attempt snapshot fallback on scrape exceptions.
            try:
                _prev = _get_prev_snapshot(url) if isinstance(snap_lookup, dict) else None
                _prev_nums = _prev.get("extracted_numbers") if isinstance(_prev, dict) else None
                if isinstance(_prev_nums, list) and _prev_nums:
                    meta["status"] = "fetched"
                    meta["status_detail"] = "fallback:last_good_snapshot"
                    meta["fallback_used"] = True
                    meta["fallback_reason"] = meta.get("status_detail") or f"failed:exception:{type(e).__name__}"
                    meta["fallback_source"] = "existing_snapshots"
                    meta["fallback_snapshot_fetched_at"] = _prev.get("fetched_at") if isinstance(_prev, dict) else None
                    meta["reused_snapshot"] = True

                    meta["fingerprint"] = _prev.get("fingerprint") if isinstance(_prev, dict) else meta.get("fingerprint")
                    meta["extractor_fingerprint"] = _prev.get("extractor_fingerprint") if isinstance(_prev, dict) else meta.get("extractor_fingerprint")
                    if not meta.get("extractor_fingerprint"):
                        meta["extractor_fingerprint"] = extractor_fp

                    meta["extracted_numbers"] = list(_prev_nums)
                    meta["numbers_found"] = len(_prev_nums)

                    _pc = (_prev.get("content") if isinstance(_prev, dict) else "") or ""
                    _pt = (_prev.get("clean_text") if isinstance(_prev, dict) else "") or ""
                    meta["content"] = _pc or _pt or ""
                    meta["clean_text"] = _pt or _pc or ""
                    meta["content_len"] = len(meta.get("content") or "")
                    meta["clean_text_len"] = len(meta.get("clean_text") or "")

                    out["scraped_meta"][url] = meta
                    out["scraped_content"][url] = meta.get("clean_text") or meta.get("content") or ""

                    scraped_ok_text += 1
                    if meta.get("numbers_found", 0) > 0:
                        scraped_ok_numbers += 1
                    # recovered; do not count as scraped_failed.
                    try:
                        _record_last_good_fallback(url)
                    except Exception:
                        pass
                    continue
            except Exception:
                pass
            scraped_failed += 1
            out["scraped_meta"][url] = meta
            out["errors"].append(meta["status_detail"])

        if progress:
            try:
                progress.progress((i + 1) / max(1, len(admitted)))
            except Exception:
                pass


    try:
        d = out.get("diag_injected_urls")
        if isinstance(d, dict):
            _inj = set(d.get("intake_norm") or [])
            sm = out.get("scraped_meta") or {}
            attempted = []
            persisted = []
            if isinstance(sm, dict):
                for u in sorted(_inj):
                    meta = sm.get(u) or {}
                    status = (meta.get("status") or "")
                    status_detail = (meta.get("status_detail") or "")
                    content = meta.get("clean_text") or meta.get("content") or ""
                    attempted.append({
                        "url": u,
                        "attempted": bool(u in (admitted or [])),
                        "fetch_status": "success" if (str(status_detail).startswith("success") or status == "fetched") else ("failed" if meta else "skipped"),
                        "fail_reason": (str(status_detail) or str(status) or "")[:80],
                        "content_len": int(len(content) if isinstance(content, str) else 0),
                        "numbers_found": int(meta.get("numbers_found") or 0),
                    })
                    if str(status_detail).startswith("success") or status == "fetched":
                        persisted.append(u)
            d["attempted"] = attempted
            d["persisted"] = persisted
            d.setdefault("set_hashes", {})
            if isinstance(d["set_hashes"], dict):
                d["set_hashes"]["persisted"] = _inj_diag_set_hash(persisted)
    except Exception:
        pass

    out["debug_counts"].update({
        "scraped_attempted": int(scraped_attempted),
        "scraped_ok_text": int(scraped_ok_text),
        "scraped_ok_numbers": int(scraped_ok_numbers),
        "scraped_failed": int(scraped_failed),
    })

    # Dashboard scrape summary
    try:
        st.info(
            f"🧽 Scrape Results: **{out['debug_counts']['scraped_ok_text']} ok-text** | "
            f"**{out['debug_counts']['scraped_ok_numbers']} ok-numbers** | "
            f"**{out['debug_counts']['scraped_failed']} failed**"
        )
    except Exception:
        pass

    # status summarization

    # Why:
    # - When scenario B "extra URLs" are provided, it can be unclear whether they:
    #     (a) were normalized/deduped
    #     (b) were admitted into the scrape list
    #     (c) were successfully scraped
    #     (d) actually entered the snapshot-hash pool used by analysis/evolution
    # - This patch records a deterministic, non-invasive trace in web_context only.
    try:
        if not isinstance(out.get("debug_counts"), dict):
            out["debug_counts"] = {}
        _dbg_counts = out["debug_counts"]

        _extra_trace = {
            "extra_urls_requested": list(extra_urls or []) if isinstance(extra_urls, list) else [],
            "extra_urls_normalized": list(_extras or []) if "_extras" in locals() and isinstance(_extras, list) else [],
            "extra_urls_admitted": [],
            "extra_urls_scraped": [],
            "extra_urls_in_hash_pool": [],
            "notes": [],
        }

        # Which extras actually made it into the final admitted URL list?
        try:
            _admitted_urls = []
            if "admitted" in locals() and isinstance(admitted, list):
                _admitted_urls = [u for u in admitted if isinstance(u, str) and u.strip()]
            _extra_set = set(_extra_trace["extra_urls_normalized"])
            _extra_trace["extra_urls_admitted"] = [u for u in _admitted_urls if u in _extra_set]
        except Exception:
            pass

        # How did each extra URL scrape?
        try:
            sm = out.get("scraped_meta") or {}
            if isinstance(sm, dict):
                for u in _extra_trace["extra_urls_normalized"]:
                    meta = sm.get(u) or {}
                    if isinstance(meta, dict) and meta:
                        content = meta.get("clean_text") or meta.get("content") or ""
                        fp = meta.get("fingerprint")
                        _extra_trace["extra_urls_scraped"].append({
                            "url": u,
                            "status": meta.get("status"),
                            "status_detail": meta.get("status_detail"),
                            "fingerprint": (fp[:16] if isinstance(fp, str) else fp),
                            "numbers_found": meta.get("numbers_found"),
                            "content_len": (len(content) if isinstance(content, str) else 0),
                            "content_type": meta.get("content_type") or "",
                        })
        except Exception:
            pass

        # Approximate "hash pool" membership (non-invasive):
        # we mark extras whose scrape produced a non-empty fingerprint + some text.
        try:
            sm = out.get("scraped_meta") or {}
            if isinstance(sm, dict):
                for row in (_extra_trace.get("extra_urls_scraped") or []):
                    u = row.get("url")
                    meta = sm.get(u) or {}
                    content = meta.get("clean_text") or meta.get("content") or ""
                    fp = meta.get("fingerprint")
                    if isinstance(fp, str) and fp and isinstance(content, str) and len(content) >= 200:
                        _extra_trace["extra_urls_in_hash_pool"].append(u)
        except Exception:
            pass

        out["extra_urls_debug"] = _extra_trace
        _dbg_counts["extra_urls_trace"] = {
            "requested": len(_extra_trace.get("extra_urls_requested") or []),
            "normalized": len(_extra_trace.get("extra_urls_normalized") or []),
            "admitted": len(_extra_trace.get("extra_urls_admitted") or []),
            "scraped": len(_extra_trace.get("extra_urls_scraped") or []),
            "in_hash_pool": len(_extra_trace.get("extra_urls_in_hash_pool") or []),
        }
    except Exception:
        pass
    if scraped_ok_text == 0:
        out["status"] = "failed"
        out["status_detail"] = "no_usable_text"
    elif scraped_ok_numbers == 0:
        out["status"] = "partial"
        out["status_detail"] = "text_ok_numbers_empty"
    else:
        out["status"] = "success"
        out["status_detail"] = "ok"

    return out


def unit_clean_first_letter(unit: str) -> str:
    """Normalize units to first letter (T/B/M/K/%), ignoring $ and spaces."""
    if not unit:
        return ""
    u = unit.replace("$", "").replace(" ", "").strip().upper()
    return u[0] if u else ""

# 7. LLM QUERY FUNCTIONS

def query_perplexity(query: str, web_context: Dict, query_structure: Optional[Dict[str, Any]] = None) -> Optional[str]:
    """
    Query Perplexity and return a validated JSON string (LLMResponse-compatible).
    Removes 'action' and excludes None fields from output JSON.
    """
    if not PERPLEXITY_KEY:
        st.error("❌ PERPLEXITY_KEY not set.")
        return None

    query_structure = query_structure or {}
    structure_txt = ""
    ordering_contract = ""

    try:
        structure_txt, ordering_contract = build_query_structure_prompt(query_structure)
    except Exception:
        pass
        structure_txt = ""
        ordering_contract = ""

    # Web context: show top sources + snippets
    sources = (web_context.get("sources", []) if isinstance(web_context, dict) else []) or []
    search_results = (web_context.get("search_results", []) if isinstance(web_context, dict) else []) or []
    search_count = int(web_context.get("search_count", len(search_results)) if isinstance(web_context, dict) else 0)

    context_section = "WEB CONTEXT:\n"
    for url in sources[:6]:
        content = (web_context.get("scraped_content", {}) or {}).get(url) if isinstance(web_context, dict) else None
        if content:
            context_section += f"\n{url}:\n{str(content)[:800]}...\n"
        else:
            context_section += f"\n{url}\n"

    enhanced_query = (
        f"{context_section}\n"
        f"{SYSTEM_PROMPT}\n\n"
        f"User Question: {query}\n\n"
        f"{structure_txt}\n\n"
        f"{ordering_contract}\n"
        f"Web search returned {search_count} results.\n"
        f"Return ONLY valid JSON matching the template and include all required fields."
    )

    headers = {
        "Authorization": f"Bearer {PERPLEXITY_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "sonar",
        "temperature": 0.0,
        "max_tokens": 2400,
        "top_p": 1.0,
        "messages": [{"role": "user", "content": enhanced_query}]
    }

    try:
        resp = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=45)
        resp.raise_for_status()
        data = resp.json()

        if "choices" not in data:
            raise Exception("No 'choices' in Perplexity response")

        content = data["choices"][0]["message"]["content"]
        if not content or not content.strip():
            raise Exception("Empty Perplexity response")

        parsed = parse_json_safely(content, "Perplexity")
        if not parsed:
            return create_fallback_response(query, search_count, web_context)

        repaired = repair_llm_response(parsed)

        # Ensure action is removed even if present
        repaired.pop("action", None)

        validate_numeric_fields(repaired, "Perplexity")

        try:
            llm_obj = LLMResponse.model_validate(repaired)

            # Ensure action not present (belt + suspenders)
            if hasattr(llm_obj, "action"):
                llm_obj.action = None

            # Merge web sources
            if isinstance(web_context, dict) and web_context.get("sources"):
                existing = llm_obj.sources or []
                merged = list(dict.fromkeys(existing + web_context["sources"]))
                llm_obj.sources = merged[:10]
                llm_obj.freshness = "Current (web-enhanced)"

            result = llm_obj.model_dump_json(exclude_none=True)
            cache_llm_response(query, web_context, result)
            return result

        except ValidationError as e:
            st.warning(f"⚠️ Pydantic validation failed: {e}")
            return create_fallback_response(query, search_count, web_context)

    except Exception as e:
        st.error(f"❌ Perplexity API error: {e}")
        return create_fallback_response(query, search_count, web_context)


def query_perplexity_raw(prompt: str, max_tokens: int = 400, timeout: int = 30) -> str:
    """
    Raw Perplexity call that returns text only.
    IMPORTANT: Does NOT attempt to validate as LLMResponse.
    """
    headers = {
        "Authorization": f"Bearer {PERPLEXITY_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "sonar",
        "temperature": 0.0,
        "top_p": 1.0,
        "max_tokens": max_tokens,
        "messages": [{"role": "user", "content": prompt}],
    }

    resp = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=timeout)
    resp.raise_for_status()
    data = resp.json()
    return (data.get("choices", [{}])[0].get("message", {}) or {}).get("content", "") or ""

def create_fallback_response(query: str, search_count: int, web_context: Dict) -> str:
    """Create fallback response matching schema, excluding None fields and removing action."""
    fallback = LLMResponse(
        executive_summary=f"Analysis of '{query}' completed with {search_count} web sources. Schema validation used fallback structure.",
        primary_metrics={
            "sources": MetricDetail(name="Web Sources", value=search_count, unit="sources"),
            "quality": MetricDetail(name="Data Quality", value=70, unit="%")
        },
        key_findings=[
            f"Web search found {search_count} relevant sources.",
            "Primary model output required fallback due to format issues.",
            "Manual review of raw data recommended for accuracy."
        ],
        top_entities=[
            TopEntityDetail(name="Source 1", share="N/A", growth="N/A")
        ],
        trends_forecast=[
            TrendForecastDetail(trend="Schema validation used fallback", direction="⚠️", timeline="Now")
        ],
        visualization_data=VisualizationData(
            chart_labels=["Attempt"],
            chart_values=[search_count],
            chart_title="Search Results"
        ),
        sources=web_context.get("sources", []),
        confidence=60,
        freshness="Current (fallback)",
        action=None
    )

    return fallback.model_dump_json(exclude_none=True)


# 7B. ANCHORED EVOLUTION QUERY

def _ensure_metric_labels(metric_changes: list) -> list:
    """
    Backward/forward compatible label normalization:
    - guarantees a non-empty display label
    - adds aliases so different UIs render correctly: metric_name, metric, label
    """
    import re

    def _prettify(s: str) -> str:
        s = str(s or "").strip()
        if not s:
            return ""
        s = s.replace("__", " ").replace("_", " ")
        s = re.sub(r"\s+", " ", s).strip()
        return s[:120]

    out = []
    for row in (metric_changes or []):
        if not isinstance(row, dict):
            continue

        name = row.get("name")
        if isinstance(name, str):
            name = name.strip()
        else:
            name = ""

        # try to derive a label if name missing (canonical_key or metric_definition.name)
        if not name:
            md = row.get("metric_definition") if isinstance(row.get("metric_definition"), dict) else {}
            name = (md.get("name") or "").strip() if isinstance(md.get("name"), str) else ""
        if not name:
            ckey = row.get("canonical_key")
            name = _prettify(ckey) if ckey else "Unknown Metric"

        # write canonical label + aliases
        row["name"] = name
        row.setdefault("metric_name", name)
        row.setdefault("metric", name)
        row.setdefault("label", name)

        out.append(row)

    return out


def format_previous_metrics(metrics: Dict) -> str:
    """Format previous metrics for prompt"""
    if not metrics:
        return "No previous metrics available"

    lines = []
    for key, m in metrics.items():
        if isinstance(m, dict):
            lines.append(f"- {m.get('name', key)}: {m.get('value', 'N/A')} {m.get('unit', '')}")
    return "\n".join(lines) if lines else "No metrics"

def format_previous_entities(entities: List) -> str:
    """Format previous entities for prompt"""
    if not entities:
        return "No previous entities available"

    lines = []
    for i, e in enumerate(entities, 1):
        if isinstance(e, dict):
            lines.append(f"{i}. {e.get('name', 'Unknown')}: {e.get('share', 'N/A')} share, {e.get('growth', 'N/A')} growth")
    return "\n".join(lines) if lines else "No entities"

def format_previous_findings(findings: List) -> str:
    """Format previous findings for prompt"""
    if not findings:
        return "No previous findings available"

    lines = [f"- {f}" for f in findings if f]
    return "\n".join(lines) if lines else "No findings"

def calculate_time_ago(timestamp_str: str) -> str:
    """Calculate human-readable time difference"""
    try:
        prev_time = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
        delta = datetime.now() - prev_time.replace(tzinfo=None)

        hours = delta.total_seconds() / 3600
        if hours < 24:
            return f"{hours:.1f} hours ago"
        elif hours < 168:  # 7 days
            return f"{hours/24:.1f} days ago"
        elif hours < 720:  # 30 days
            return f"{hours/168:.1f} weeks ago"
        else:
            return f"{hours/720:.1f} months ago"
    except:
        return "unknown time ago"

def query_perplexity_anchored(query: str, previous_data: Dict, web_context: Dict, temperature: float = 0.1) -> str:
    """
    Query Perplexity with previous analysis as anchor.
    This produces an evolution-aware response that tracks changes.
    """

    prev_response = previous_data.get("primary_response", {})
    prev_timestamp = previous_data.get("timestamp", "")
    prev_question = previous_data.get("question", query)

    time_ago = calculate_time_ago(prev_timestamp)

    # Build the anchored prompt
    anchored_prompt = EVOLUTION_PROMPT_TEMPLATE.format(
        time_ago=time_ago,
        previous_question=prev_question,
        previous_timestamp=prev_timestamp,
        previous_summary=prev_response.get("executive_summary", "No previous summary"),
        previous_metrics=format_previous_metrics(prev_response.get("primary_metrics", {})),
        previous_entities=format_previous_entities(prev_response.get("top_entities", [])),
        previous_findings=format_previous_findings(prev_response.get("key_findings", [])),
        query=query
    )

    # Add web context if available
    if web_context.get("summary"):
        anchored_prompt = f"CURRENT WEB RESEARCH:\n{web_context['summary']}\n\n{anchored_prompt}"

    # API request
    headers = {
        "Authorization": f"Bearer {PERPLEXITY_KEY}",
        "Content-Type": "application/json"
    }


    payload = {
        "model": "sonar",
        "temperature": 0.0,      # DETERMINISTIC
        "max_tokens": 2500,
        "top_p": 1.0,            # DETERMINISTIC
        "messages": [{"role": "user", "content": anchored_prompt}]
    }

    try:
        resp = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=60)
        resp.raise_for_status()
        data = resp.json()

        if "choices" not in data:
            raise Exception("No choices in response")

        content = data["choices"][0]["message"]["content"]
        if not content:
            raise Exception("Empty response")

        # Parse JSON
        parsed = parse_json_safely(content, "Perplexity-Anchored")
        if not parsed:
            return create_anchored_fallback(query, previous_data, web_context)

        # Add sources from web context
        if web_context.get("sources"):
            existing = parsed.get("sources", [])
            parsed["sources"] = list(dict.fromkeys(existing + web_context["sources"]))[:10]

        return json.dumps(parsed)

    except Exception as e:
        st.error(f"❌ Anchored query error: {e}")
        return create_anchored_fallback(query, previous_data, web_context)

def create_anchored_fallback(query: str, previous_data: Dict, web_context: Dict) -> str:
    """Create fallback for anchored evolution query"""
    prev_response = previous_data.get("primary_response", {})

    fallback = {
        "executive_summary": f"Evolution analysis for '{query}' - model returned invalid format. Showing previous data.",
        "analysis_delta": {
            "time_since_previous": calculate_time_ago(previous_data.get("timestamp", "")),
            "overall_trend": "unknown",
            "major_changes": ["Unable to determine changes - API error"],
            "data_freshness": "Unknown"
        },
        "primary_metrics": prev_response.get("primary_metrics", {}),
        "key_findings": ["[UNCHANGED] " + f for f in prev_response.get("key_findings", [])[:3]],
        "top_entities": prev_response.get("top_entities", []),
        "trends_forecast": prev_response.get("trends_forecast", []),
        "sources": web_context.get("sources", []),
        "confidence": 50,
        "freshness": "Fallback",
        "drift_summary": {
            "metrics_changed": 0,
            "metrics_unchanged": len(prev_response.get("primary_metrics", {})),
            "entities_reshuffled": 0,
            "findings_updated": 0,
            "overall_stability_pct": 100
        }
    }
    return json.dumps(fallback)

# 8. VALIDATION & SCORING


def parse_number_with_unit(val_str: str) -> float:
    """
    Parse a numeric string into a comparable base scale.
    Returns a float in "millions" for currency/volume-like values.
    Percentages are returned as their numeric value (e.g., "9.8%" -> 9.8).

    Handles:
      - $58.3B, 58.3B, S$29.8B, 29.8 S$B, USD 21.18 B
      - 58.3 billion, 58.3 bn, 58.3 million, 58.3 mn, 570 thousand
      - 570,000 (interpreted as an absolute count -> converted to millions)
      - 9.8% (kept as 9.8)
    """
    if val_str is None:
        return 0.0

    s = str(val_str).strip()
    if not s:
        return 0.0

    s_low = s.lower()

    # If it's a percentage, return the raw percent number (not millions)
    if "%" in s_low:
        m = re.search(r'(-?\d+(?:\.\d+)?)', s_low)
        if not m:
            return 0.0
        try:
            return float(m.group(1))
        except Exception:
            return 0.0

    # Normalize: remove commas and common currency tokens/symbols
    # (keep letters because we need bn/mn/b/m/k detection)
    s_low = s_low.replace(",", " ")
    for token in ["s$", "usd", "sgd", "us$", "$", "€", "£", "aud", "cad"]:
        s_low = s_low.replace(token, " ")

    # Collapse whitespace
    s_low = re.sub(r"\s+", " ", s_low).strip()

    # Extract the first number
    m = re.search(r'(-?\d+(?:\.\d+)?)', s_low)
    if not m:
        return 0.0

    try:
        num = float(m.group(1))
    except Exception:
        return 0.0

    # Look at the remaining text after the number for unit words/suffix
    tail = s_low[m.end():].strip()

    # Decide multiplier (base = millions)
    # billions -> *1000, millions -> *1, thousands -> *0.001
    multiplier = 1.0

    # Word-based units
    if re.search(r'\b(trillion|tn)\b', tail):
        multiplier = 1_000_000.0  # trillion -> million
    elif re.search(r'\b(billion|bn)\b', tail):
        multiplier = 1000.0
    elif re.search(r'\b(million|mn)\b', tail):
        multiplier = 1.0
    elif re.search(r'\b(thousand|k)\b', tail):
        multiplier = 0.001
    else:
        # Suffix-style units (possibly with spaces), e.g. "29.8 b", "21.18 b", "58.3m"
        # We only look at the very first letter-ish token in tail.
        t0 = tail[:4].strip()  # enough to catch "b", "m", "k"
        if t0.startswith("b"):
            multiplier = 1000.0
        elif t0.startswith("m"):
            multiplier = 1.0
        elif t0.startswith("k"):
            multiplier = 0.001
        else:
            # No unit detected. If it's a big integer like 570000 (jobs, people),
            # interpret as an absolute count and convert to millions.
            # (570000 -> 0.57 million)
            if abs(num) >= 10000 and float(num).is_integer():
                multiplier = 1.0 / 1_000_000.0
            else:
                multiplier = 1.0

    return num * multiplier


def numeric_consistency_with_sources(primary_data: dict, web_context: dict) -> float:
    """Compare primary numbers vs source numbers"""
    primary_metrics = primary_data.get("primary_metrics", {})
    primary_numbers = []

    for metric in primary_metrics.values():
        if isinstance(metric, dict):
            val = metric.get("value")
            num = parse_number_with_unit(str(val))
            if num > 0:
                primary_numbers.append(num)

    if not primary_numbers:
        return 50.0  # Neutral when no metrics to compare

    # Extract source numbers with same parsing
    source_numbers = []
    search_results = web_context.get("search_results", [])

    for result in search_results:
        snippet = str(result.get("snippet", ""))
        # Match patterns like "$58.3B", "123M", "456 billion"
        patterns = [
            r'\$?(\d+(?:\.\d+)?)\s*([BbMmKk])',  # $58.3B
            r'(\d+(?:\.\d+)?)\s*(billion|million|thousand)',  # 58.3 billion
        ]

        for pattern in patterns:
            matches = re.findall(pattern, snippet, re.IGNORECASE)
            for num, unit in matches:
                source_numbers.append(parse_number_with_unit(f"{num}{unit[0].upper()}"))

    if not source_numbers:
        return 50.0  # Neutral when no source numbers found

    # Check agreement (within 25% tolerance)
    agreements = 0
    for p_num in primary_numbers:
        for s_num in source_numbers:
            if abs(p_num - s_num) / max(p_num, s_num, 1) < 0.25:
                agreements += 1
                break

    # Scale: 0 agreements = 30%, all agreements = 95%
    agreement_ratio = agreements / len(primary_numbers)
    agreement_pct = 30.0 + (agreement_ratio * 65.0)
    return min(agreement_pct, 95.0)

def numeric_consistency_with_sources_v2(primary_data: dict, web_context: dict) -> float:
    """
    Stable numeric consistency (0-100):
    - Evidence text: search_results snippets + web_context summary + scraped_content
    - Unit-aware parsing via parse_number_with_unit()
    - Range-aware (supports min/max if metric has a 'range' dict)
    - Downweights proxy metrics (is_proxy=True) so they don't tank the score
    """

    try:
        # Prefer canonical metrics if available (has is_proxy, range, etc.)
        metrics = primary_data.get("primary_metrics_canonical") or primary_data.get("primary_metrics") or {}
        if not isinstance(metrics, dict) or not metrics:
            return 50.0

        # Build evidence text corpus
        texts = []

        # 1) snippets
        sr = (web_context or {}).get("search_results") or []
        if isinstance(sr, list):
            for r in sr:
                if isinstance(r, dict):
                    snip = r.get("snippet", "")
                    if isinstance(snip, str) and snip.strip():
                        texts.append(snip)

        # 2) summary
        summary = (web_context or {}).get("summary") or ""
        if isinstance(summary, str) and summary.strip():
            texts.append(summary)

        # 3) scraped_content
        scraped = (web_context or {}).get("scraped_content") or {}
        if isinstance(scraped, dict):
            for _, content in scraped.items():
                if isinstance(content, str) and content.strip():
                    texts.append(content)

        evidence_text = "\n".join(texts)
        if not evidence_text.strip():
            return 45.0  # no evidence stored

        # Extract numeric candidates from evidence text
        # Keep this broad; parse_number_with_unit will normalize.
        patterns = [
            r'\$?\s?\d{1,3}(?:,\d{3})*(?:\.\d+)?\s*[BbMmKk]\b',                 # 29.8B, 570K, 1.2M
            r'\$?\s?\d+(?:\.\d+)?\s*(?:billion|million|thousand|bn|mn)\b',      # 29.8 billion, 29.8 bn
            r'\b\d{1,3}(?:,\d{3})+(?:\.\d+)?\b',                               # 570,000
            r'\b\d+(?:\.\d+)?\s*%\b',                                          # 9.8%
        ]

        evidence_numbers = []
        lowered = evidence_text.lower()

        for pat in patterns:
            for m in re.findall(pat, lowered, flags=re.IGNORECASE):
                n = parse_number_with_unit(str(m))
                if n and n > 0:
                    evidence_numbers.append(n)

        # If nothing extracted, don’t penalize too hard
        if not evidence_numbers:
            return 50.0

        # Verify each metric against evidence numbers (tolerance match)
        def _metric_candidates(m: dict) -> list:
            """Return list of candidate numeric values for a metric (range-aware)."""
            out = []
            if not isinstance(m, dict):
                return out

            # Range support: check min/max if present
            rng = m.get("range") if isinstance(m.get("range"), dict) else None
            if rng:
                if rng.get("min") is not None:
                    out.append(rng.get("min"))
                if rng.get("max") is not None:
                    out.append(rng.get("max"))

            # Also check main value
            if m.get("value") is not None:
                out.append(m.get("value"))

            return out

        def _parse_metric_num(val, unit_hint: str = "") -> float:
            # build a value+unit string so parse_number_with_unit has a chance
            if val is None:
                return 0.0
            s = str(val)
            if unit_hint and unit_hint.lower() not in s.lower():
                s = f"{s} {unit_hint}"
            return parse_number_with_unit(s)

        def _is_supported(target: float, evidence_nums: list, rel_tol: float = 0.25) -> bool:
            # same tolerance approach as v1 (25%)
            if not target or target <= 0:
                return False
            for e in evidence_nums:
                if e <= 0:
                    continue
                if abs(target - e) / max(target, e, 1) < rel_tol:
                    return True
            return False

        supported_w = 0.0
        total_w = 0.0

        for _, m in metrics.items():
            if not isinstance(m, dict):
                continue

            unit = str(m.get("unit") or "").strip()

            # proxy weighting
            is_proxy = bool(m.get("is_proxy"))
            w = 0.5 if is_proxy else 1.0

            cands = _metric_candidates(m)
            if not cands:
                continue

            # parse candidates into numeric values
            parsed_targets = []
            for c in cands:
                n = _parse_metric_num(c, unit_hint=unit)
                if n and n > 0:
                    parsed_targets.append(n)

            if not parsed_targets:
                continue

            total_w += w

            # supported if ANY candidate matches evidence
            if any(_is_supported(t, evidence_numbers, rel_tol=0.25) for t in parsed_targets):
                supported_w += w

        if total_w <= 0:
            return 50.0

        ratio = supported_w / total_w
        # Map: keep a soft floor so one miss doesn't tank the whole run
        score = 30.0 + (ratio * 65.0)  # same scale as v1 (30..95)
        return min(max(score, 20.0), 95.0)

    except Exception:
        return 45.0


def source_consensus(web_context: dict) -> float:
    """
    Calculate source consensus based on proportion of high-quality sources.
    Returns continuous score 0-100 based on quality distribution.
    """
    reliabilities = web_context.get("source_reliability", [])

    if not reliabilities:
        return 50.0  # Neutral when no sources

    total = len(reliabilities)
    high_count = sum(1 for r in reliabilities if "✅" in str(r))
    medium_count = sum(1 for r in reliabilities if "⚠️" in str(r))
    low_count = sum(1 for r in reliabilities if "❌" in str(r))

    # Weighted score: High=100, Medium=60, Low=30
    weighted_sum = (high_count * 100) + (medium_count * 60) + (low_count * 30)
    consensus_score = weighted_sum / total

    # Bonus for having multiple high-quality sources
    if high_count >= 3:
        consensus_score = min(100, consensus_score + 10)
    elif high_count >= 2:
        consensus_score = min(100, consensus_score + 5)

    return round(consensus_score, 1)

def evidence_based_veracity(primary_data: dict, web_context: dict) -> dict:
    """
    Evidence-driven veracity scoring.
    Returns breakdown of component scores and overall score (0-100).
    """
    breakdown = {}

    # 1. SOURCE QUALITY (35% weight)
    sources = primary_data.get("sources", [])
    src_score = source_quality_score(sources)
    breakdown["source_quality"] = round(src_score, 1)

    # 2. NUMERIC CONSISTENCY (30% weight)
    num_score = numeric_consistency_with_sources_v2(primary_data, web_context)
    breakdown["numeric_consistency"] = round(num_score, 1)

    # 3. CITATION DENSITY (20% weight)
    # FIXED: Higher score when sources support findings, not penalize detail
    sources_count = len(sources)
    findings_count = len(primary_data.get("key_findings", []))
    metrics_count = len(primary_data.get("primary_metrics", {}))

    # Total claims = findings + metrics
    total_claims = findings_count + metrics_count

    if total_claims == 0:
        citations_score = 40.0  # Low score for no claims
    else:
        # Ratio of sources to claims - ideal is ~0.5-1.0 sources per claim
        ratio = sources_count / total_claims
        if ratio >= 1.0:
            citations_score = 90.0  # Well-supported
        elif ratio >= 0.5:
            citations_score = 70.0 + (ratio - 0.5) * 40  # 70-90 range
        elif ratio >= 0.25:
            citations_score = 50.0 + (ratio - 0.25) * 80  # 50-70 range
        else:
            citations_score = ratio * 200  # 0-50 range

    breakdown["citation_density"] = round(min(citations_score, 95.0), 1)

    # 4. SOURCE CONSENSUS (15% weight)
    consensus_score = source_consensus(web_context)
    breakdown["source_consensus"] = round(consensus_score, 1)

    # Calculate weighted total
    total_score = (
        breakdown["source_quality"] * 0.35 +
        breakdown["numeric_consistency"] * 0.30 +
        breakdown["citation_density"] * 0.20 +
        breakdown["source_consensus"] * 0.15
    )

    breakdown["overall"] = round(total_score, 1)

    return breakdown

def calculate_final_confidence(
    base_conf: float,
    evidence_score: float
) -> float:
    """
    Calculate final confidence score.

    Formula balances model confidence with evidence quality:
    - Evidence has higher weight (65%) as it's more objective
    - Model confidence (35%) is adjusted by evidence quality

    This ensures:
    - High model + High evidence → High final (~85-90%)
    - High model + Low evidence → Medium final (~55-65%)
    - Low model + High evidence → Medium-High final (~70-80%)
    - Low model + Low evidence → Low final (~40-50%)
    """

    # Normalize inputs to 0-100 range
    base_conf = max(0, min(100, base_conf))
    evidence_score = max(0, min(100, evidence_score))

    # 1. EVIDENCE COMPONENT (65% weight) - Primary driver
    evidence_component = evidence_score * 0.65

    # 2. MODEL COMPONENT (35% weight) - Adjusted by evidence quality
    # When evidence is weak, model confidence is discounted
    evidence_multiplier = 0.5 + (evidence_score / 200)  # Range: 0.5 to 1.0
    model_component = base_conf * evidence_multiplier * 0.35

    final = evidence_component + model_component

    # Ensure result is in valid range
    return round(max(0, min(100, final)), 1)

# 8A. DETERMINISTIC DIFF ENGINE
# Pure Python computation - no LLM variance

@dataclass
class MetricDiff:
    """Single metric change record"""
    name: str
    old_value: Optional[float]
    new_value: Optional[float]
    old_raw: str  # Original string representation
    new_raw: str
    unit: str
    change_pct: Optional[float]
    change_type: str  # 'increased', 'decreased', 'unchanged', 'added', 'removed'

@dataclass
class EntityDiff:
    """Single entity ranking change record"""
    name: str
    old_rank: Optional[int]
    new_rank: Optional[int]
    old_share: Optional[str]
    new_share: Optional[str]
    rank_change: Optional[int]  # Positive = moved up
    change_type: str  # 'moved_up', 'moved_down', 'unchanged', 'added', 'removed'

@dataclass
class FindingDiff:
    """Single finding change record"""
    old_text: Optional[str]
    new_text: Optional[str]
    similarity: float  # 0-100
    change_type: str  # 'retained', 'modified', 'added', 'removed'

@dataclass
class EvolutionDiff:
    """Complete diff between two analyses"""
    old_timestamp: str
    new_timestamp: str
    time_delta_hours: Optional[float]
    metric_diffs: List[MetricDiff]
    entity_diffs: List[EntityDiff]
    finding_diffs: List[FindingDiff]
    stability_score: float  # 0-100
    summary_stats: Dict[str, int]

# CANONICAL METRIC REGISTRY & SEMANTIC FINDING HASH
# Add this section after the dataclass definitions (around line 1587)

# CANONICAL METRIC REGISTRY
# Removes LLM control over metric identity

# Metric type definitions with aliases
# - Remove standalone "sales" from Revenue aliases (too ambiguous)
# - Add money-explicit revenue phrases instead ("sales revenue", "sales value", etc.)
# - Add a couple of volume-style aliases under units_sold ("sales volume", "volume sales")

METRIC_REGISTRY = {
    # Market Size metrics
    "market_size": {
        "canonical_name": "Market Size",
        "aliases": [
            "market size", "market value", "market cap", "total market",
            "global market", "market valuation", "industry size",
            "total addressable market", "tam", "market worth"
        ],
        "unit_type": "currency",
        "category": "size"
    },
    "market_size_current": {
        "canonical_name": "Current Market Size",
        "aliases": [
            "2024 market size", "2025 market size", "current market",
            "present market size", "today market", "current year market",
            "market size 2024", "market size 2025"
        ],
        "unit_type": "currency",
        "category": "size"
    },
    "market_size_projected": {
        "canonical_name": "Projected Market Size",
        "aliases": [
            "projected market", "forecast market", "future market",
            "2026 market", "2027 market", "2028 market", "2029 market", "2030 market",
            "market projection", "expected market size", "estimated market"
        ],
        "unit_type": "currency",
        "category": "size"
    },

    # Growth metrics
    "cagr": {
        "canonical_name": "CAGR",
        "aliases": [
            "cagr", "compound annual growth", "compound growth rate",
            "annual growth rate", "growth rate", "yearly growth"
        ],
        "unit_type": "percentage",
        "category": "growth"
    },
    "yoy_growth": {
        "canonical_name": "YoY Growth",
        "aliases": [
            "yoy growth", "year over year", "year-over-year",
            "annual growth", "yearly growth rate", "growth percentage"
        ],
        "unit_type": "percentage",
        "category": "growth"
    },

    # Revenue metrics
    "revenue": {
        "canonical_name": "Revenue",
        "aliases": [
            "revenue",
            # "sales",
            "total revenue", "annual revenue",
            "yearly revenue", "gross revenue",

            "sales revenue",
            "revenue from sales",
            "sales value",
            "value of sales",
            "sales (value)",
            "turnover",  # common finance synonym
        ],
        "unit_type": "currency",
        "category": "financial"
    },

    # Market share
    "market_share": {
        "canonical_name": "Market Share",
        "aliases": [
            "market share", "share", "market portion", "market percentage",
            "share of market"
        ],
        "unit_type": "percentage",
        "category": "share"
    },

    # Volume metrics
    "units_sold": {
        "canonical_name": "Units Sold",
        "aliases": [
            "units sold", "unit sales", "volume", "units shipped",
            "shipments", "deliveries", "production volume",

            "sales volume",
            "volume sales",
        ],
        "unit_type": "count",
        "category": "volume"
    },

    # Pricing
    "average_price": {
        "canonical_name": "Average Price",
        "aliases": [
            "average price", "avg price", "mean price", "asp",
            "average selling price", "unit price"
        ],
        "unit_type": "currency",
        "category": "pricing"
    },

    # Country / Macro metrics
    "gdp": {
        "canonical_name": "GDP",
        "aliases": ["gdp", "gross domestic product", "economic output"],
        "unit_type": "currency",
        "category": "macro"
    },
    "gdp_per_capita": {
        "canonical_name": "GDP per Capita",
        "aliases": ["gdp per capita", "gdp/capita", "income per person", "per capita gdp"],
        "unit_type": "currency",
        "category": "macro"
    },
    "gdp_growth": {
        "canonical_name": "GDP Growth",
        "aliases": ["gdp growth", "economic growth", "growth rate of gdp", "real gdp growth"],
        "unit_type": "percentage",
        "category": "macro"
    },
    "population": {
        "canonical_name": "Population",
        "aliases": ["population", "population size", "number of people"],
        "unit_type": "count",
        "category": "macro"
    },
    "exports": {
        "canonical_name": "Exports",
        "aliases": ["exports", "export value", "total exports"],
        "unit_type": "currency",
        "category": "trade"
    },
    "imports": {
        "canonical_name": "Imports",
        "aliases": ["imports", "import value", "total imports"],
        "unit_type": "currency",
        "category": "trade"
    },
    "inflation": {
        "canonical_name": "Inflation",
        "aliases": ["inflation", "cpi", "consumer price index", "inflation rate"],
        "unit_type": "percentage",
        "category": "macro"
    },
    "interest_rate": {
        "canonical_name": "Interest Rate",
        "aliases": ["interest rate", "policy rate", "benchmark rate", "central bank rate"],
        "unit_type": "percentage",
        "category": "macro"
    }
}


# Year extraction pattern
YEAR_PATTERN = re.compile(r'(20\d{2})')

# DETERMINISTIC QUESTION SIGNALS
# Drives metric table templates (no LLM)

QUESTION_CATEGORY_TEMPLATES = {
    "country": [
        "gdp",
        "gdp_per_capita",
        "gdp_growth",
        "population",
        "exports",
        "imports",
        "inflation",
        "interest_rate",
    ],
    "industry": [
        "market_size_current",
        "market_size_projected",
        "cagr",
        "revenue",
        "market_share",
        "units_sold",
        "average_price",
    ],
}

def get_expected_metric_ids_for_category(category: str) -> List[str]:
    """
    Domain-agnostic mapping from a template/category string to expected metric IDs.

    Backward compatible:
      - accepts legacy categories like 'country', 'industry', 'company', 'generic'
      - also accepts template IDs like 'ENTITY_OVERVIEW_MARKET_LIGHT_V1', etc.

    NOTE:
    - This function returns a *default* set for a given template/category.
    - The profiler (classify_question_signals) can override/compose expected_metric_ids dynamically.
    """
    c_raw = (category or "unknown").strip()
    c = c_raw.lower().strip()

    # New generalized templates
    if c in {"entity_overview_country_light_v1", "entity_overview_country_v1"}:
        return [
            "population",
            "gdp_nominal",
            "gdp_per_capita",
            "gdp_growth",
            "inflation",
            "currency",
            "unemployment",
            "exports",
            "imports",
            "top_industries",
        ]

    if c in {"entity_overview_market_light_v1"}:
        return [
            "market_size_current",
            "market_size_projected",
            "cagr",
            "key_trends",
            "top_players",
        ]

    if c in {"entity_overview_market_heavy_v1"}:
        return [
            "market_size_current",
            "market_size_projected",
            "cagr",
            "key_trends",
            "top_players",
            "key_regions",
            "segments",
            "market_share",
            "revenue",
            "units_sold",
            "average_price",
        ]

    if c in {"entity_overview_company_light_v1", "entity_overview_company_v1"}:
        return [
            "revenue",
            "growth",
            "gross_margin",
            "operating_margin",
            "net_income",
            "market_cap",
            "valuation_multiple",
        ]

    if c in {"entity_overview_product_light_v1", "entity_overview_product_v1"}:
        return [
            "average_price",
            "units_sold",
            "market_share",
            "growth",
            "key_trends",
        ]

    if c in {"entity_overview_topic_v1", "generic_v1"}:
        return []

    # Legacy categories (still supported)
    if c == "country":
        return get_expected_metric_ids_for_category("ENTITY_OVERVIEW_COUNTRY_LIGHT_V1")

    if c == "industry":
        # legacy industry defaults to light market
        return get_expected_metric_ids_for_category("ENTITY_OVERVIEW_MARKET_LIGHT_V1")

    if c == "company":
        return get_expected_metric_ids_for_category("ENTITY_OVERVIEW_COMPANY_LIGHT_V1")

    if c == "generic":
        return []

    # fallback
    return []


def classify_question_signals(query: str) -> Dict[str, Any]:
    """
    Deterministically classify query and return:
      - category: high-level bucket used for templates (country | industry | company | generic)
      - expected_metric_ids: list[str]
      - signals: list[str] (debuggable reasons)
      - years: list[int]
      - regions: list[str]
      - intents: list[str] (market_size, growth_forecast, competitive_landscape, pricing, regulation, consumer_demand, supply_chain, investment, macro_outlook)
    """
    q_raw = (query or "").strip()
    q = q_raw.lower().strip()
    signals: List[str] = []

    if not q:
        return {
            "category": "generic",
            "expected_metric_ids": [],
            "signals": ["empty_query"],
            "years": [],
            "regions": [],
            "intents": []
        }

    # 1) Extract years (deterministic)
    years: List[int] = []
    try:
        year_matches = re.findall(r"\b(19|20)\d{2}\b", q_raw)
        # The regex above returns the first group; re-run with a non-capturing group to capture full year strings.
        year_matches_full = re.findall(r"\b(?:19|20)\d{2}\b", q_raw)
        years = sorted({int(y) for y in year_matches_full})
        if years:
            signals.append(f"years:{','.join(map(str, years[:8]))}")
    except Exception:
        pass
        years = []

    # 2) Extract regions/countries (best-effort deterministic; spaCy if available)
    regions: List[str] = []
    try:
        nlp = _try_spacy_nlp()
        if nlp:
            doc = nlp(q_raw)
            gpes = [ent.text.strip() for ent in getattr(doc, "ents", []) if ent.label_ in ("GPE", "LOC")]
            regions = []
            for g in gpes:
                if g and g.lower() not in [x.lower() for x in regions]:
                    regions.append(g)
            if regions:
                signals.append(f"regions_spacy:{','.join(regions[:6])}")
    except Exception:
        pass

    # Fallback: very lightweight region tokens
    if not regions:
        region_tokens = [
            "singapore", "malaysia", "indonesia", "thailand", "vietnam", "philippines",
            "china", "india", "japan", "korea", "australia",
            "usa", "united states", "europe", "uk", "united kingdom",
            "asean", "southeast asia", "sea", "global", "worldwide"
        ]
        hits = [t for t in region_tokens if t in q]
        if hits:
            # Keep original casing loosely (title-case single words)
            regions = [h.title() if " " not in h else h.upper() if h in ("usa", "uk") else h.title() for h in hits[:6]]
            signals.append(f"regions_kw:{','.join(hits[:6])}")

    # 3) Intent detection (domain-agnostic)
    intent_patterns: Dict[str, List[str]] = {
        "market_size": ["market size", "tam", "total addressable market", "how big", "size of the market", "market value"],
        "growth_forecast": ["cagr", "forecast", "projection", "by 20", "growth rate", "expected to", "outlook", "trend"],
        "competitive_landscape": ["key players", "competitors", "market share", "top companies", "leading players", "who are the players"],
        "pricing": ["pricing", "price", "asp", "average selling price", "cost", "margins"],
        "consumer_demand": ["demand", "users", "penetration", "adoption", "consumer", "customer", "behavior"],
        "supply_chain": ["supply", "capacity", "production", "manufacturing", "inventory", "shipment", "lead time"],
        "regulation": ["regulation", "policy", "law", "compliance", "tax", "tariff", "subsidy"],
        "investment": ["investment", "capex", "funding", "valuation", "roi", "profit", "ebitda"],
        "macro_outlook": ["gdp", "inflation", "interest rate", "policy rate", "exports", "imports", "currency", "exchange rate", "per capita"],
    }

    intents: List[str] = []
    for intent, pats in intent_patterns.items():
        if any(p in q for p in pats):
            intents.append(intent)

    # Small disambiguation: "by 2030" etc. strongly suggests forecast if years exist
    if years and "growth_forecast" not in intents and any(yr >= 2025 for yr in years):
        intents.append("growth_forecast")

    if intents:
        signals.append(f"intents:{','.join(intents[:10])}")

    # 4) Category decision (template driver)
    # Keep it coarse: country vs industry vs company vs generic
    country_kw = [
        "gdp", "per capita", "population", "exports", "imports",
        "inflation", "cpi", "interest rate", "policy rate", "central bank",
        "currency", "exchange rate"
    ]
    company_kw = ["revenue", "earnings", "profit", "ebitda", "guidance", "quarter", "fy", "10-k", "10q", "balance sheet"]
    industry_kw = [
        "market", "industry", "sector", "tam", "cagr", "market size", "market share",
        "key players", "competitors", "pricing", "forecast", "outlook"
    ]

    country_hits = [k for k in country_kw if k in q]
    company_hits = [k for k in company_kw if k in q]
    industry_hits = [k for k in industry_kw if k in q]

    # If macro intent is present, strongly bias to country
    if "macro_outlook" in intents and (regions or country_hits):
        category = "country"
        signals.append("category_rule:macro_outlook_bias_country")
    elif company_hits and not industry_hits:
        category = "company"
        signals.append(f"category_rule:company_keywords:{','.join(company_hits[:5])}")
    elif industry_hits and not country_hits:
        category = "industry"
        signals.append(f"category_rule:industry_keywords:{','.join(industry_hits[:5])}")
    elif industry_hits and country_hits:
        # tie-break: if market sizing/competitive signals exist -> industry; if macro_outlook -> country
        if "macro_outlook" in intents:
            category = "country"
            signals.append("category_rule:mixed_signals_macro_wins")
        else:
            category = "industry"
            signals.append("category_rule:mixed_signals_default_to_industry")
    else:
        category = "generic"
        signals.append("category_rule:no_template_keywords")

    # 5) Expected metric IDs (category + intent)
    expected_metric_ids: List[str] = []
    try:
        expected_metric_ids = get_expected_metric_ids_for_category(category) or []
    except Exception:
        pass
        expected_metric_ids = []

    # Add a few intent-driven metric IDs (only if your registry supports them)
    intent_metric_suggestions = {
        "market_size": ["market_size", "market_size_2024", "market_size_2025"],
        "growth_forecast": ["cagr", "forecast_period", "market_size_2030"],
        "competitive_landscape": ["market_share", "top_players"],
        "pricing": ["avg_price", "asp"],
        "consumer_demand": ["users", "penetration", "arpu"],
        "supply_chain": ["capacity", "shipments"],
        "investment": ["capex", "profit", "ebitda"],
        "macro_outlook": ["gdp", "inflation", "interest_rate", "exchange_rate"],
    }

    for intent in intents:
        for mid in intent_metric_suggestions.get(intent, []):
            if mid not in expected_metric_ids:
                expected_metric_ids.append(mid)

    return {
        "category": category,
        "expected_metric_ids": expected_metric_ids,
        "signals": signals,
        "years": years,
        "regions": regions,
        "intents": intents
    }


    def _contains_any(needle_list: List[str]) -> bool:
        return any(k in q for k in needle_list)

    # Determine intents
    intents: List[str] = []
    for intent, kws in intent_triggers.items():
        if _contains_any(kws):
            intents.append(intent)

    if intents:
        signals.append("intents:" + ",".join(sorted(set(intents))))

    # Determine entity_kind (best-effort heuristic)
    is_marketish = _contains_any(market_entity_kw) or any(i in intents for i in ["size", "growth", "forecast", "share", "segments", "players", "regions"])
    is_companyish = _contains_any(company_entity_kw) and not _contains_any(country_entity_kw)
    is_countryish = _contains_any(country_entity_kw) and not is_companyish
    is_productish = _contains_any(product_entity_kw) and not (is_marketish or is_countryish or is_companyish)

    if is_countryish:
        entity_kind = "country"
        signals.append("entity_kind:country")
    elif is_companyish:
        entity_kind = "company"
        signals.append("entity_kind:company")
    elif is_productish:
        entity_kind = "product"
        signals.append("entity_kind:product")
    elif is_marketish:
        entity_kind = "market"
        signals.append("entity_kind:market")
    else:
        entity_kind = "topic_general"
        signals.append("entity_kind:topic_general")

    # Determine scope
    is_comparative = _contains_any(comparative_kw)
    is_forecasty = _contains_any(forecast_kw) or bool(YEAR_PATTERN.findall(q_raw))

    # Broad overview should win when user explicitly asks for general explainer
    # BUT: if they also mention measurable intents (size/growth/forecast/etc.), treat as metrics_light.
    is_broad_phrase = _contains_any(broad_phrases)

    if is_comparative:
        scope = "comparative"
        signals.append("scope:comparative")
    elif is_forecasty and any(i in intents for i in ["forecast", "growth", "size"]):
        scope = "forecast_specific"
        signals.append("scope:forecast_specific")
    elif is_broad_phrase and not intents:
        scope = "broad_overview"
        signals.append("scope:broad_overview")
    else:
        # metrics light vs heavy
        heavy_asks = ["segments", "share", "volume", "regions", "players"]
        heavy_requested = any(i in intents for i in heavy_asks)
        if heavy_requested:
            scope = "metrics_heavy"
            signals.append("scope:metrics_heavy")
        else:
            scope = "metrics_light"
            signals.append("scope:metrics_light")

    # Map entity_kind -> category (backward compatible)
    if entity_kind == "country":
        category = "country"
    elif entity_kind == "company":
        category = "company"
    elif entity_kind in {"market", "product"}:
        category = "industry"
    else:
        category = "generic"

    # Choose generalized template + tiers
    # Tier meanings:
    #  1 = high extractability (size/growth/forecast)
    #  2 = medium (players/regions/basic segments)
    #  3 = low (granular channels, detailed splits) -> only if explicitly asked
    if category == "country":
        metric_template_id = "ENTITY_OVERVIEW_COUNTRY_LIGHT_V1" if scope != "metrics_heavy" else "ENTITY_OVERVIEW_COUNTRY_LIGHT_V1"
        metric_tiers_enabled = [1]
    elif category == "company":
        metric_template_id = "ENTITY_OVERVIEW_COMPANY_LIGHT_V1"
        metric_tiers_enabled = [1]
    elif category == "industry":
        if scope in {"metrics_heavy", "comparative"}:
            metric_template_id = "ENTITY_OVERVIEW_MARKET_HEAVY_V1"
            metric_tiers_enabled = [1, 2]
        else:
            metric_template_id = "ENTITY_OVERVIEW_MARKET_LIGHT_V1"
            metric_tiers_enabled = [1]
    else:
        metric_template_id = "ENTITY_OVERVIEW_TOPIC_V1"
        metric_tiers_enabled = []

    # Build expected_metric_ids dynamically from intents (domain-agnostic)
    # Slot -> metric id mapping (kept generic; avoids tourism specialization)
    # If you later add more canonical IDs, expand these mappings.
    market_slot_to_id = {
        "size_current": "market_size_current",
        "size_projected": "market_size_projected",
        "growth_cagr": "cagr",
        "growth_yoy": "growth",
        "share_key": "market_share",
        "volume_current": "units_sold",
        "price_avg": "average_price",
        "players_top": "top_players",
        "regions_key": "key_regions",
        "segments_basic": "segments",
        "trends": "key_trends",
        "revenue": "revenue",
    }

    company_slot_to_id = {
        "revenue": "revenue",
        "growth": "growth",
        "gross_margin": "gross_margin",
        "operating_margin": "operating_margin",
        "net_income": "net_income",
        "market_cap": "market_cap",
        "valuation_multiple": "valuation_multiple",
        "trends": "key_trends",
    }

    country_slot_to_id = {
        "population": "population",
        "gdp_nominal": "gdp_nominal",
        "gdp_per_capita": "gdp_per_capita",
        "gdp_growth": "gdp_growth",
        "inflation": "inflation",
        "currency": "currency",
        "unemployment": "unemployment",
        "exports": "exports",
        "imports": "imports",
        "top_industries": "top_industries",
        "trends": "key_trends",
    }

    # Determine slots from intents
    slots: List[str] = []
    if entity_kind == "country":
        # For countries: macro defaults if broad, otherwise macro intents
        if scope == "broad_overview":
            slots = ["population", "gdp_nominal", "gdp_per_capita", "gdp_growth", "inflation", "currency", "top_industries"]
        else:
            # If user asks for macro (or didn’t specify), still give a tight macro set
            slots = ["population", "gdp_nominal", "gdp_growth", "inflation", "currency"]
            if "macro" in intents:
                slots += ["unemployment", "exports", "imports"]

        mapper = country_slot_to_id

    elif entity_kind == "company":
        slots = ["revenue", "growth", "gross_margin", "operating_margin", "net_income", "market_cap", "valuation_multiple"]
        mapper = company_slot_to_id

    elif entity_kind in {"market", "product"}:
        # Tier 1 core (always when metrics_* scope)
        if scope == "broad_overview":
            slots = ["trends", "players_top"]
        else:
            slots = ["size_current", "growth_cagr"]
            if "forecast" in intents:
                slots.append("size_projected")
            if "trends" in intents:
                slots.append("trends")
            # Tier 2 (only when explicitly asked or heavy scope)
            if scope in {"metrics_heavy", "comparative"}:
                if "players" in intents:
                    slots.append("players_top")
                if "regions" in intents:
                    slots.append("regions_key")
                if "segments" in intents:
                    slots.append("segments_basic")
                if "share" in intents:
                    slots.append("share_key")
                if "volume" in intents:
                    slots.append("volume_current")
                if "price" in intents:
                    slots.append("price_avg")
            else:
                # metrics_light: include players/trends only if asked
                if "players" in intents:
                    slots.append("players_top")
                if "regions" in intents:
                    slots.append("regions_key")

        mapper = market_slot_to_id

    else:
        # topic_general
        slots = []
        mapper = {}

    expected_metric_ids = []
    for s in slots:
        mid = mapper.get(s)
        if mid:
            expected_metric_ids.append(mid)

    # If still empty but template provides defaults, use template defaults
    if not expected_metric_ids:
        expected_metric_ids = get_expected_metric_ids_for_category(metric_template_id)

    # De-dup while preserving order
    seen = set()
    expected_metric_ids = [x for x in expected_metric_ids if not (x in seen or seen.add(x))]

    # Preferred source classes (generic)
    if category == "country":
        preferred_source_classes = ["official_stats", "government", "reputable_org", "reference"]
    elif category == "company":
        preferred_source_classes = ["official_filings", "investor_relations", "reputable_org", "news"]
    elif category == "industry":
        preferred_source_classes = ["industry_association", "reputable_org", "official_stats", "news", "research_portal"]
    else:
        preferred_source_classes = ["reference", "official_stats", "reputable_org"]

    # Attach year detection signal
    years = sorted(set(YEAR_PATTERN.findall(q_raw))) if YEAR_PATTERN.findall(q_raw) else []
    if years:
        signals.append("years_detected:" + ",".join(years))

    return {
        "category": category,
        "expected_metric_ids": expected_metric_ids,
        "signals": signals,
        "entity_kind": entity_kind,
        "scope": scope,
        "metric_template_id": metric_template_id,
        "metric_tiers_enabled": metric_tiers_enabled,
        "preferred_source_classes": preferred_source_classes,
        "intents": sorted(set(intents)),
    }


def get_canonical_metric_id(metric_name: str) -> Tuple[str, str]:
    """
    Map a metric name to its canonical ID and display name.

    Returns:
        Tuple of (canonical_id, canonical_display_name)

    Example:
        "2024 Market Size" -> ("market_size_2024", "Market Size (2024)")
        "Global Market Value" -> ("market_size", "Market Size")
        "CAGR 2024-2030" -> ("cagr_2024_2030", "CAGR (2024-2030)")
    """
    import re

    if not metric_name:
        return ("unknown", "Unknown Metric")

    name_lower = metric_name.lower().strip()
    name_normalized = re.sub(r"[^\w\s]", " ", name_lower)
    name_normalized = re.sub(r"\s+", " ", name_normalized).strip()

    # Extract years
    years = YEAR_PATTERN.findall(metric_name)
    year_suffix = "_".join(sorted(years)) if years else ""

    name_words = set(name_normalized.split())

    # Explicit money intent (strong)
    money_tokens = {
        "revenue", "turnover", "valuation", "valued", "value", "market", "capex", "opex",
        "profit", "earnings", "ebitda", "income",
        "usd", "sgd", "eur", "gbp", "aud", "cad", "jpy", "cny", "rmb"
    }
    # Currency symbols appear in raw text sometimes
    has_currency_symbol = any(sym in metric_name for sym in ["$", "€", "£", "S$"])

    has_money_intent = bool(name_words & money_tokens) or has_currency_symbol

    # Explicit unit/count intent (strong)
    unit_tokens = {
        "unit", "units", "deliveries", "shipments", "registrations", "vehicles",
        "sold", "salesvolume", "volume", "pcs", "pieces"
    }
    # normalize joined token cases like "sales volume"
    joined = name_normalized.replace(" ", "")
    has_unit_intent = bool(name_words & unit_tokens) or any(t in joined for t in ["salesvolume", "unitsold", "vehiclesold"])

    # Find best matching registry entry
    best_match_id = None
    best_match_score = 0.0

    def _is_revenue_like(metric_id: str, config: dict) -> bool:
        mid = (metric_id or "").lower()
        cname = str((config or {}).get("canonical_name") or "").lower()
        # treat "market value" / "valuation" as currency-like too
        if any(k in cname for k in ["revenue", "market value", "valuation", "market size", "turnover"]):
            return True
        if any(k in mid for k in ["revenue", "market_value", "market_size", "valuation"]):
            return True
        return False

    for metric_id, config in METRIC_REGISTRY.items():
        for alias in config["aliases"]:
            # Remove years from alias for comparison
            alias_no_year = YEAR_PATTERN.sub("", alias).strip().lower()
            alias_no_year = re.sub(r"[^\w\s]", " ", alias_no_year)
            alias_no_year = re.sub(r"\s+", " ", alias_no_year).strip()

            name_no_year = YEAR_PATTERN.sub("", name_normalized).strip()

            score = 0.0

            # Exact match
            if alias_no_year == name_no_year and alias_no_year:
                score = 1.0

            # Containment match
            elif alias_no_year and (alias_no_year in name_no_year or name_no_year in alias_no_year):
                score = len(alias_no_year) / max(len(name_no_year), 1)

            # Word overlap match
            else:
                alias_words = set(alias_no_year.split())
                name_words_local = set(name_no_year.split())
                if alias_words and name_words_local:
                    overlap = len(alias_words & name_words_local) / len(alias_words | name_words_local)
                    score = max(score, overlap)

            # - Block "sales" -> revenue when no money intent
            # - Block unit-intent -> revenue-like
            # - Require explicit money intent for revenue-like (soft guard, not hard stop)
            if score > 0.0:
                revenue_like = _is_revenue_like(metric_id, config)

                # If target is revenue-like but name has strong unit intent, penalize heavily
                if revenue_like and has_unit_intent and not has_money_intent:
                    score *= 0.20  # strong downweight

                # If target is revenue-like but name has NO money intent at all, penalize
                if revenue_like and not has_money_intent:
                    score *= 0.55  # moderate downweight

                # If name includes the word "sales" but no money intent, avoid mapping to revenue-like
                if revenue_like and ("sales" in name_no_year.split()) and not has_money_intent:
                    score *= 0.60

                # Conversely: if target is NOT revenue-like but name has money intent, slight penalty
                if (not revenue_like) and has_money_intent and ("sales" in name_no_year.split()) and not has_unit_intent:
                    score *= 0.85

            if score > best_match_score:
                best_match_id = metric_id
                best_match_score = score

            if best_match_score == 1.0:
                break

        if best_match_score == 1.0:
            break

    # Build canonical ID and display name
    if best_match_id and best_match_score > 0.4:
        config = METRIC_REGISTRY[best_match_id]
        canonical_base = best_match_id
        display_name = config["canonical_name"]

        if year_suffix:
            canonical_id = f"{canonical_base}_{year_suffix}"
            if len(years) == 1:
                display_name = f"{display_name} ({years[0]})"
            else:
                display_name = f"{display_name} ({'-'.join(years)})"
        else:
            canonical_id = canonical_base

        return (canonical_id, display_name)

    # Fallback: create ID from normalized name
    fallback_id = re.sub(r"\s+", "_", name_normalized)
    if year_suffix:
        fallback_id = f"{fallback_id}_{year_suffix}" if year_suffix not in fallback_id else fallback_id

    return (fallback_id, metric_name)

# GEO + PROXY TAGGING (DETERMINISTIC)

import re
from typing import Dict, Any, Tuple, List, Optional

REGION_KEYWORDS = {
    "APAC": ["apac", "asia pacific", "asia-pacific"],
    "SOUTHEAST_ASIA": ["southeast asia", "asean", "sea "],  # note space to reduce false matches
    "ASIA": ["asia"],
    "EUROPE": ["europe", "eu", "emea"],
    "NORTH_AMERICA": ["north america"],
    "LATAM": ["latin america", "latam"],
    "MIDDLE_EAST": ["middle east", "mena"],
    "AFRICA": ["africa"],
    "OCEANIA": ["oceania", "australia", "new zealand"],
}

GLOBAL_KEYWORDS = ["global", "worldwide", "world", "international", "across the world"]

# Minimal country map (expand deterministically over time)
COUNTRY_KEYWORDS = {
    "Singapore": ["singapore", "sg"],
    "United States": ["united states", "usa", "u.s.", "us"],
    "United Kingdom": ["united kingdom", "uk", "u.k.", "britain", "england"],
    "China": ["china", "prc"],
    "Japan": ["japan"],
    "India": ["india"],
    "Indonesia": ["indonesia"],
    "Malaysia": ["malaysia"],
    "Thailand": ["thailand"],
    "Vietnam": ["vietnam"],
    "Philippines": ["philippines"],
}

def infer_geo_scope(*texts: str) -> Dict[str, str]:
    """
    Deterministically infer geography from text.
    Returns {"geo_scope": "local|regional|global|unknown", "geo_name": "<name or ''>"}.
    Priority: country > region > global.
    """
    combined = " ".join([t for t in texts if isinstance(t, str) and t.strip()]).lower()
    if not combined:
        return {"geo_scope": "unknown", "geo_name": ""}

    # 1) Country/local (most specific)
    for country, kws in COUNTRY_KEYWORDS.items():
        for kw in kws:
            if kw in combined:
                return {"geo_scope": "local", "geo_name": country}

    # 2) Region
    for region_name, kws in REGION_KEYWORDS.items():
        for kw in kws:
            if kw in combined:
                pretty = region_name.replace("_", " ").title()
                return {"geo_scope": "regional", "geo_name": pretty}

    # 3) Global
    for kw in GLOBAL_KEYWORDS:
        if kw in combined:
            return {"geo_scope": "global", "geo_name": "Global"}

    return {"geo_scope": "unknown", "geo_name": ""}


# "Proxy" = adjacent metric that can help approximate the target but isn't the target definition.
# You can expand these sets deterministically.

PROXY_PATTERNS = [
    # (pattern, proxy_type, reason_template)
    (r"\bapparel\b|\bfashion\b|\bclothing\b", "adjacent_category", "Uses apparel/fashion as an adjacent proxy for streetwear."),
    (r"\bfootwear\b|\bsneaker\b|\bshoes\b", "subsegment", "Uses footwear/sneakers as a subsegment proxy for the broader market."),
    (r"\bresale\b|\bsecondary market\b", "channel_proxy", "Uses resale/secondary-market measures as a channel proxy."),
    (r"\be-?commerce\b|\bonline sales\b|\bsocial commerce\b", "channel_proxy", "Uses e-commerce indicators as a channel proxy."),
    (r"\btourism\b|\bvisitor\b|\btravel retail\b", "demand_driver", "Uses tourism indicators as a demand-driver proxy."),
    (r"\bsearch interest\b|\bgoogle trends\b|\bweb traffic\b", "interest_proxy", "Uses interest/attention measures as a proxy."),
]

# These are words that signal "core market size" style metrics (usually non-proxy if they match the user topic).
CORE_MARKET_PATTERNS = [
    r"\bmarket size\b",
    r"\bmarket value\b",
    r"\brevenue\b",
    r"\bsales\b",
    r"\bcagr\b",
    r"\bgrowth\b",
    r"\bprojected\b|\bforecast\b",
]

def infer_proxy_label(
    metric_name: str,
    question_text: str = "",
    category_hint: str = "",
    *extra_context: str
) -> Dict[str, Any]:
    """
    Deterministically label a metric as proxy/non-proxy.

    Returns fields:
      is_proxy: bool
      proxy_type: str
      proxy_reason: str
      proxy_confidence: float (0-1)
      proxy_target: str (best-guess target topic)
    """
    name = (metric_name or "").lower().strip()
    q = (question_text or "").lower().strip()
    ctx = " ".join([c for c in extra_context if isinstance(c, str)]).lower()

    combined = " ".join([name, q, ctx]).strip()

    # Default: not proxy
    out = {
        "is_proxy": False,
        "proxy_type": "",
        "proxy_reason": "",
        "proxy_confidence": 0.0,
        "proxy_target": ""
    }

    if not combined:
        return out

    # Best-effort target topic extraction (very light heuristic)
    # If you already have question signals elsewhere, you can pass them in category_hint/question_text.
    # Here we just keep a short phrase if present.
    proxy_target = ""
    if "streetwear" in q:
        proxy_target = "streetwear"
    elif "semiconductor" in q:
        proxy_target = "semiconductors"
    elif "battery" in q:
        proxy_target = "batteries"
    out["proxy_target"] = proxy_target

    # If metric name itself looks like core market patterns AND includes the target keyword, treat as non-proxy.
    # (prevents incorrectly labeling "Singapore streetwear market size" as proxy)
    core_like = any(re.search(p, name) for p in CORE_MARKET_PATTERNS)
    if core_like:
        # If it explicitly contains the topic keyword, strongly non-proxy
        if proxy_target and proxy_target in name:
            return out
        # If it says "streetwear market" in name, non-proxy even if target not detected
        if "streetwear" in name:
            return out

    # Detect proxies using patterns.
    for pat, ptype, reason in PROXY_PATTERNS:
        if re.search(pat, combined):
            out["is_proxy"] = True
            out["proxy_type"] = ptype
            out["proxy_reason"] = reason
            # Confidence: stronger if pattern appears in metric name; weaker if only in context.
            if re.search(pat, name):
                out["proxy_confidence"] = 0.9
            elif re.search(pat, ctx):
                out["proxy_confidence"] = 0.7
            else:
                out["proxy_confidence"] = 0.6
            return out

    return out


def merge_group_geo(group: List[Dict[str, Any]]) -> Tuple[str, str]:
    """
    Choose the most frequent geo tag within a merged group deterministically.
    Returns (geo_scope, geo_name).
    """
    items = []
    for g in group:
        s = g.get("geo_scope", "unknown")
        n = g.get("geo_name", "")
        if s and s != "unknown":
            items.append((s, n))

    if not items:
        return "unknown", ""

    counts: Dict[str, int] = {}
    for s, n in items:
        k = f"{s}|{n}"
        counts[k] = counts.get(k, 0) + 1

    best_k = max(counts.items(), key=lambda kv: kv[1])[0]  # deterministic tie via insertion order after stable sort
    s, n = best_k.split("|", 1)
    return s, n


def merge_group_proxy(group: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Merge proxy labels for duplicates deterministically.
    If ANY member is proxy -> merged metric is proxy.
    Choose the highest-confidence proxy candidate.
    """
    best = None
    best_conf = -1.0

    for g in group:
        is_proxy = bool(g.get("is_proxy", False))
        conf = float(g.get("proxy_confidence", 0.0) or 0.0)
        if is_proxy and conf > best_conf:
            best_conf = conf
            best = g

    if best is None:
        return {
            "is_proxy": False,
            "proxy_type": "",
            "proxy_reason": "",
            "proxy_confidence": 0.0,
            "proxy_target": "",
        }

    return {
        "is_proxy": True,
        "proxy_type": best.get("proxy_type", ""),
        "proxy_reason": best.get("proxy_reason", ""),
        "proxy_confidence": float(best.get("proxy_confidence", 0.0) or 0.0),
        "proxy_target": best.get("proxy_target", ""),
    }

# FIX2D59 — Canonical Identity Resolver v1
#
# Exact identity tuple definition (v1):
#   IdentityTupleV1 := {
#       'metric_token':   str,  # schema concept token / canonical_id (concept-level)
#       'time_scope':     str,  # normalized time token (e.g. '2024', 'ytd_2025') if known
#       'geo_scope':      str,  # normalized geo token (e.g. 'global', 'us') if known
#       'dims':           tuple[str,...], # normalized dimension-value tokens (segment/category/channel) if known
#       'dimension':      str,  # 'currency'|'unit_sales'|'percent'|'count'|'index'|'unknown'
#       'unit_family':    str,  # 'currency'|'percent'|'magnitude'|'energy'|'index'|''
#       'unit_tag':       str,  # 'USD'|'%'|'M'|'GWh' etc (normalized)
#       'statistic':      str,  # e.g. 'level'|'yoy_pct'|'cagr'|'share' (optional)
#       'aggregation':    str,  # e.g. 'total'|'avg' (optional)
#   }
#
# Resolver contract:
#   resolve_canonical_identity_v1(identity, metric_schema) -> {
#       'canonical_key': str,          # schema canonical_key if bound, else provisional key
#       'bound': bool,                 # True iff schema-bound
#       'status': str,                 # 'SCHEMA_BOUND' | 'PROVISIONAL'
#       'matched_schema_key': str|''   # the schema key chosen, if any
#   }
#
# Rules:
#   1) Schema-first: if metric_schema contains a canonical_key that matches the identity tuple, return it.
#   2) No silent canonical minting: if identity is under-specified (dimension unknown, or unit_family missing when unit_tag present), return PROVISIONAL.
#   3) Deterministic: matching and tie-breaks must be stable across runs.
#
# Note:
#   This resolver is intended to be used by BOTH Analysis and Evolution finalizers.

def normalize_metric_token_time_scope_v1(metric_token: str, time_scope: str = '') -> tuple:
    """Split embedded time tokens out of metric_token into time_scope (v1).

    Rules (deterministic):
      - Leading year prefix: '2024_global_ev_sales' -> metric_token='global_ev_sales', time_scope='2024'
      - Trailing year suffix: 'global_ev_sales_2024' -> metric_token='global_ev_sales', time_scope='2024'
      - YTD forms: 'global_ev_sales_ytd_2025' -> metric_token='global_ev_sales', time_scope='ytd_2025'
      - Forecast/projected forms: 'forecast_2035_sales' -> metric_token='sales', time_scope='forecast_2035'

    If time_scope is already provided, it is preserved.
    """
    mt = str(metric_token or '').strip().lower()
    ts = str(time_scope or '').strip().lower()
    if not mt:
        return '', ts
    if ts:
        return re.sub(r'_+', '_', mt).strip('_'), ts

    # ytd patterns
    m = re.search(r'(?:^|_)ytd[_-]?(20\d{2})(?:$|_)', mt)
    if m:
        year = m.group(1)
        ts = f'ytd_{year}'
        mt = re.sub(r'(?:^|_)ytd[_-]?%s(?:$|_)' % re.escape(year), '_', mt)

    # forecast/projected patterns (treat as forecast)
    m = re.search(r'(?:^|_)(forecast|projected|projection|estimate|estimated|target)[_-]?(20\d{2})(?:$|_)', mt)
    if m:
        year = m.group(2)
        ts = f'forecast_{year}'
        mt = re.sub(r'(?:^|_)(forecast|projected|projection|estimate|estimated|target)[_-]?%s(?:$|_)' % re.escape(year), '_', mt)

    # leading year
    m = re.match(r'^(20\d{2})_(.+)$', mt)
    if m and not ts:
        ts = m.group(1)
        mt = m.group(2)

    # trailing year
    m = re.match(r'^(.+?)_(20\d{2})$', mt)
    if m and not ts:
        mt = m.group(1)
        ts = m.group(2)

    mt = re.sub(r'_+', '_', mt).strip('_')
    ts = re.sub(r'_+', '_', ts).strip('_')
    return mt, ts

def build_identity_tuple_v1(*, metric_token: str, time_scope: str = '', geo_scope: str = '', dims=None,
                            dimension: str = '', unit_family: str = '', unit_tag: str = '',
                            statistic: str = '', aggregation: str = '') -> dict:
    'Construct a deterministic identity tuple (v1).'
    if dims is None:
        dims = ()
    if not isinstance(dims, (list, tuple)):
        dims = (str(dims),)
    metric_token, time_scope = normalize_metric_token_time_scope_v1(metric_token, time_scope)
    return {
        'metric_token': str(metric_token or '').strip().lower(),
        'time_scope': str(time_scope or '').strip().lower(),
        'geo_scope': str(geo_scope or '').strip().lower(),
        'dims': tuple([str(x or '').strip().lower() for x in list(dims) if str(x or '').strip()]),
        'dimension': str(dimension or '').strip().lower(),
        'unit_family': str(unit_family or '').strip().lower(),
        'unit_tag': str(unit_tag or '').strip(),
        'statistic': str(statistic or '').strip().lower(),
        'aggregation': str(aggregation or '').strip().lower(),
    }


def canonicalize_metrics(
    metrics: Dict,
    metric_schema: Dict = None,
    merge_duplicates_to_range: bool = True,
    question_text: str = "",
    category_hint: str = ""
) -> Dict:
    """
    Convert metrics to canonical IDs, but NEVER merge across incompatible dimensions.

    Key fix:
      - Adds deterministic 'dimension' classification and incorporates it into canonical keys.
      - Prevents revenue vs unit-sales from merging just because the year matches.
      - Keeps your geo + proxy tagging behavior.

    Output:
      canonicalized[canonical_key] -> metric dict with:
        - canonical_id (base id)
        - canonical_key (dimension-safe id you should use everywhere downstream)
        - dimension (currency | unit_sales | percent | count | index | unknown)
        - name (dimension-corrected display name)
    """
    import re  # ========================= PATCH C0 (ADDITIVE): missing import =========================

    if not isinstance(metrics, dict):
        return {}

    # - Prefer existing normalize_unit_tag/unit_family/canonicalize_numeric_candidate if present.
    # - Never breaks if those helpers are missing.
    def _safe_normalize_unit_tag(u: str) -> str:
        try:
            fn = globals().get("normalize_unit_tag")
            if callable(fn):
                return fn(u or "")
        except Exception:
            pass
        # minimal fallback (kept conservative)
        uu = (u or "").strip()
        ul = uu.lower().replace(" ", "")
        # - Legacy extracted units often arrive as phrases (e.g., 'million units', 'billion USD').
        # - We deterministically map magnitude words even when other tokens are present.
        if 'trillion' in ul or ul.endswith('tn') or ' tn' in (uu.lower()):
            return 'T'
        if 'billion' in ul or ul.endswith('bn') or ' bn' in (uu.lower()):
            return 'B'
        if 'million' in ul or ul.endswith('mn') or ' mn' in (uu.lower()) or 'mio' in ul:
            return 'M'
        if 'thousand' in ul or ul.endswith('k') or ' k' in (uu.lower()):
            return 'K'
        if ul in ("%", "pct", "percent"):
            return "%"
        if ul in ("twh",):
            return "TWh"
        if ul in ("gwh",):
            return "GWh"
        if ul in ("mwh",):
            return "MWh"
        if ul in ("kwh",):
            return "kWh"
        if ul in ("wh",):
            return "Wh"
        if ul in ("t", "trillion", "tn"):
            return "T"
        if ul in ("b", "bn", "billion"):
            return "B"
        if ul in ("m", "mn", "mio", "million"):
            return "M"
        if ul in ("k", "thousand", "000"):
            return "K"
        return uu

    def _safe_unit_family(unit_tag: str) -> str:
        try:
            fn = globals().get("unit_family")
            if callable(fn):
                return fn(unit_tag or "")
        except Exception:
            pass
        ut = (unit_tag or "").strip()
        if ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
            return "energy"
        if ut == "%":
            return "percent"
        if ut in ("T", "B", "M", "K"):
            return "magnitude"
        # currency not reliably derived here (handled elsewhere)
        return ""

    def infer_metric_dimension(metric_name: str, unit_raw: str) -> str:
        n = (metric_name or "").lower()
        u = (unit_raw or "").strip().lower()

        # Percent
        if "%" in u or "percent" in n or "share" in n or "cagr" in n:
            return "percent"

        # Currency signals
        currency_tokens = ["$", "s$", "usd", "sgd", "eur", "€", "gbp", "£", "jpy", "¥", "cny", "rmb", "aud", "cad"]
        if any(t in u for t in currency_tokens) or any(t in n for t in ["revenue", "market value", "valuation", "value (", "usd", "sgd", "eur"]):
            return "currency"

        # Unit sales / shipments
        unit_tokens = ["unit", "units", "sold", "sales", "sales volume", "shipments", "registrations", "deliveries", "vehicles", "pcs", "pieces", "volume"]
        if any(t in n for t in unit_tokens) or any(t in u for t in ["unit", "units", "vehicle", "vehicles", "pcs", "pieces"]):
            return "unit_sales"

        # Handles cases like 'Global EV Sales 2024' with unit 'million units' where name contains 'sales'
        # but unit tokens may not include 'units' in the name itself.
        if ('sales' in n or 'ev sales' in n) and (
            ('million' in u) or ('billion' in u) or ('thousand' in u) or ('mn' in u) or ('bn' in u) or
            ('unit' in u) or ('units' in u) or ('vehicle' in u) or ('vehicles' in u)
        ):
            return 'unit_sales'

        # Pure counts
        if any(t in n for t in ["count", "number of", "install base", "installed base", "users", "subscribers"]) and "revenue" not in n:
            return "count"

        # Index / score
        if any(t in n for t in ["index", "score", "rating"]):
            return "index"

        return "unknown"

    def display_name_for_dimension(original_display: str, dim: str) -> str:
        if not original_display:
            return original_display

        od = original_display.strip()
        od_low = od.lower()

        if dim == "unit_sales":
            if "revenue" in od_low or "market value" in od_low or "valuation" in od_low:
                return re.sub(r"(?i)revenue|market value|valuation", "Unit Sales", od).strip()
            if od_low.startswith("sales"):
                return "Unit Sales" + od[len("Sales"):]
            if "sales" in od_low:
                return re.sub(r"(?i)sales", "Unit Sales", od).strip()
            return od

        if dim == "currency":
            if "unit sales" in od_low:
                return re.sub(r"(?i)unit sales", "Revenue", od).strip()
            return od

        if dim == "percent":
            if "unit sales" in od_low or "revenue" in od_low:
                return od
            return od

        return od

    candidates = []

    for key, metric in metrics.items():
        if not isinstance(metric, dict):
            continue

        original_name = metric.get("name", key)
        canonical_id, canonical_name = get_canonical_metric_id(original_name)

        # - If the canonical base metric is in METRIC_REGISTRY, use its unit_type
        #   as a strong prior for dimension classification.
        # - This reduces mislabel drift like "Revenue" being assigned as unit_sales
        #   (or vice-versa) purely from noisy LLM labels.
        #
        # NOTE (conflict fix, additive):
        # - Your prior code risked UnboundLocalError due to base_id scoping.
        # - We keep your legacy behavior, but guard it and define base_id upfront.

        registry_unit_type = ""

        base_id = ""

        try:
            # - canonical_id may contain underscores inside the base id (e.g., "market_size_2025")
            # - Find the LONGEST registry key that is a prefix of canonical_id.
            try:
                reg = globals().get("METRIC_REGISTRY")
                cid = str(canonical_id or "")
                if isinstance(reg, dict) and cid:
                    # choose the longest matching prefix key
                    for k in reg.keys():
                        ks = str(k)
                        if cid == ks or cid.startswith(ks + "_"):
                            if len(ks) > len(base_id):
                                base_id = ks

                    if base_id and isinstance(reg.get(base_id), dict):
                        registry_unit_type = (reg[base_id].get("unit_type") or "").strip().lower()
            except Exception:
                pass
                # keep safe defaults
                pass

            # - This block is redundant with CM1.B, but we keep it as requested.
            # - Guard prevents:
            #   (1) base_id undefined
            #   (2) overwriting registry_unit_type already computed above
            if not registry_unit_type:
                reg = globals().get("METRIC_REGISTRY")
                if base_id and isinstance(reg, dict) and base_id in reg and isinstance(reg[base_id], dict):
                    registry_unit_type = (reg[base_id].get("unit_type") or "").strip().lower()

        except Exception:
            pass
            registry_unit_type = ""

        # Map registry unit_type -> canonicalize_metrics dimension vocabulary
        # (keep it small + deterministic)
        if registry_unit_type:
            if registry_unit_type in ("currency",):
                registry_dim_hint = "currency"
            elif registry_unit_type in ("percentage", "percent"):
                registry_dim_hint = "percent"
            elif registry_unit_type in ("count",):
                # keep "unit_sales" vs "count" distinction:
                # registry says count; name-based inference decides "unit_sales" if it sees units/shipments/deliveries
                registry_dim_hint = "count"
            else:
                registry_dim_hint = ""
        else:
            registry_dim_hint = ""

        raw_unit = (metric.get("unit") or "").strip()

        # - We keep your existing unit_norm logic for backwards compatibility.
        # - But we ALSO attach unit_tag + unit_family so downstream can gate deterministically.
        unit_tag = metric.get("unit_tag") or _safe_normalize_unit_tag(raw_unit)
        unit_family_tag = metric.get("unit_family") or _safe_unit_family(unit_tag)

        unit_norm = raw_unit.upper()  # keep original behavior (do not change)

        # - Adds an auditable trace showing *how* dimension + canonical_key were minted.
        # - This is intentionally local (no new dependencies) and additive only.
        # - Downstream UI/JSON can surface these fields to diagnose drift (e.g., __unknown).
        dim_inferred = infer_metric_dimension(str(original_name), raw_unit)
        dim = dim_inferred

        # - If registry says currency/percent, force that dimension.
        # - If registry says count, prevent accidental "currency"/"percent".
        _trace_dim_override = ""
        if registry_dim_hint in ("currency", "percent"):
            dim = registry_dim_hint
            _trace_dim_override = "registry_force"
        elif registry_dim_hint == "count":
            # Allow unit_sales if name clearly indicates it; else keep "count"
            if dim in ("currency", "percent"):
                dim = "count"
                _trace_dim_override = "registry_guard"

        _ident = build_identity_tuple_v1(metric_token=canonical_id, dimension=dim, unit_family=unit_family_tag, unit_tag=unit_tag, geo_scope=str(metric.get('geo_scope') or ''), time_scope='')
        _res = resolve_canonical_identity_v1(_ident, metric_schema)
        canonical_key = str(_res.get('canonical_key') or f"{canonical_id}__{dim}")

        # NOTE: metric_enriched is created below; stash trace ingredients now.
        _key_mint_trace = {
            "mint_fn": "canonicalize_metrics",
            "canonical_id": canonical_id,
            "canonical_key": canonical_key,
            "original_name": original_name,
            "canonical_name": canonical_name,
            "raw_unit": raw_unit,
            "unit_norm": unit_norm,
            "unit_tag": unit_tag,
            "unit_family": unit_family_tag,
            "dim_inferred": dim_inferred,
            "dim_final": dim,
            "registry_unit_type": registry_unit_type,
            "registry_dim_hint": registry_dim_hint,
            "dim_override": _trace_dim_override,
            "key_mint_path": ("REGISTRY_OVERRIDE" if _trace_dim_override else "NAME_UNIT_INFER"),
            "identity_tuple_v1": _ident if '_ident' in locals() else {},
            "identity_resolve_v1": _res if '_res' in locals() else {},
        }

        parsed_val = parse_to_float(metric.get("value"))
        value_for_sort = parsed_val if parsed_val is not None else str(metric.get("value", ""))

        stable_sort_key = (
            str(original_name).lower().strip(),
            dim,
            unit_norm,
            str(value_for_sort),
            str(key),
        )

        geo = infer_geo_scope(
            str(original_name),
            str(metric.get("context_snippet", "")),
            str(metric.get("source", "")),
            str(metric.get("source_url", "")),
        )

        proxy = infer_proxy_label(
            str(original_name),
            str(question_text),
            str(category_hint),
            str(metric.get("context_snippet", "")),
            str(metric.get("source", "")),
            str(metric.get("source_url", "")),
        )

        # - If canonicalize_numeric_candidate exists, it will attach:
        #   unit_tag/unit_family/base_unit/multiplier_to_base/value_norm
        # - If not, we attach minimal fields ourselves (still additive).
        metric_enriched = dict(metric)  # never mutate caller's dict
        try:
            fn_can = globals().get("canonicalize_numeric_candidate")
            if callable(fn_can):
                metric_enriched = fn_can(metric_enriched)
        except Exception:
            pass

        # Ensure minimal canonical fields exist (additive)
        metric_enriched.setdefault("unit_tag", unit_tag)
        metric_enriched.setdefault("unit_family", unit_family_tag)

        # Add mint trace (additive). Keep it under debug to avoid polluting top-level.
        try:
            metric_enriched.setdefault("debug", {})
            if isinstance(metric_enriched.get("debug"), dict):
                metric_enriched["debug"]["key_mint_trace"] = _key_mint_trace
        except Exception:
            pass

        candidates.append({
            "canonical_id": canonical_id,
            "canonical_key": canonical_key,
            "canonical_name": display_name_for_dimension(canonical_name, dim),
            "original_name": original_name,

            # NOTE: store enriched metric
            "metric": metric_enriched,

            "unit": unit_norm,
            "parsed_val": parsed_val,
            "dimension": dim,
            "stable_sort_key": stable_sort_key,
            "geo_scope": geo["geo_scope"],
            "geo_name": geo["geo_name"],
            **proxy,
        })

    candidates.sort(key=lambda x: x["stable_sort_key"])

    grouped: Dict[str, List[Dict]] = {}
    for c in candidates:
        grouped.setdefault(c["canonical_key"], []).append(c)

    canonicalized: Dict[str, Dict] = {}

    for ckey, group in grouped.items():
        if len(group) == 1 or not merge_duplicates_to_range:
            g = group[0]
            m = g["metric"]

            # (only adds keys; does not remove/rename existing keys)
            out_row = {
                **m,
                "name": g["canonical_name"],
                "canonical_id": g["canonical_id"],
                "canonical_key": ckey,
                "dimension": g["dimension"],
                "original_name": g["original_name"],
                "geo_scope": g.get("geo_scope", "unknown"),
                "geo_name": g.get("geo_name", ""),
                "is_proxy": bool(g.get("is_proxy", False)),
                "proxy_type": g.get("proxy_type", ""),
                "proxy_reason": g.get("proxy_reason", ""),
                "proxy_confidence": float(g.get("proxy_confidence", 0.0) or 0.0),
                "proxy_target": g.get("proxy_target", ""),
            }
            # Ensure these exist if upstream provided them
            for k in ["anchor_hash", "source_url", "context_snippet", "measure_kind", "measure_assoc",
                      "unit_tag", "unit_family", "base_unit", "multiplier_to_base", "value_norm"]:
                if k in m and k not in out_row:
                    out_row[k] = m.get(k)
            canonicalized[ckey] = out_row
            continue

        # Merge duplicates within SAME dimension-safe canonical_key
        base = group[0]
        base_metric = dict(base["metric"])
        base_metric["name"] = base["canonical_name"]
        base_metric["canonical_id"] = base["canonical_id"]
        base_metric["canonical_key"] = ckey
        base_metric["dimension"] = base["dimension"]

        geo_scope, geo_name = merge_group_geo(group)
        base_metric["geo_scope"] = geo_scope
        base_metric["geo_name"] = geo_name

        merged_proxy = merge_group_proxy(group)
        base_metric.update(merged_proxy)

        vals = [g["parsed_val"] for g in group if g["parsed_val"] is not None]
        raw_vals = [str(g["metric"].get("value", "")) for g in group]
        orig_names = [g["original_name"] for g in group]

        units = [g["unit"] for g in group if g["unit"]]
        unit_base = units[0] if units else (base_metric.get("unit") or "")
        base_metric["unit"] = unit_base

        base_metric["original_names"] = orig_names
        base_metric["raw_values"] = raw_vals

        # - Keeps your existing "range" untouched.
        # - Adds "range_norm" when we can compute it.
        vals_norm = []
        for g in group:
            mm = g.get("metric") if isinstance(g, dict) else {}
            if isinstance(mm, dict) and mm.get("value_norm") is not None:
                try:
                    vals_norm.append(float(mm.get("value_norm")))
                except Exception:
                    pass

        # Why:
        # - Avoid median/aggregate drift between analysis and evolution.
        # - If we already chose a specific evidence candidate (candidate_id/anchor_hash),
        #   that candidate should determine the metric's reported value/value_norm/unit.
        # Determinism:
        # - Select the evidence row with highest confidence if present, else first.
        # - No re-fetching, no new extraction; uses existing evidence payload only.
        _anchored_value_set = False
        try:
            _ev = base_metric.get("evidence")
            if isinstance(_ev, list) and _ev:
                # pick best evidence deterministically
                def _ev_score(e):
                    try:
                        c = e.get("confidence")
                        return float(c) if c is not None else 0.0
                    except Exception:
                        return 0.0
                _ev_sorted = sorted([e for e in _ev if isinstance(e, dict)], key=_ev_score, reverse=True)
                _best = _ev_sorted[0] if _ev_sorted else None

                if isinstance(_best, dict):
                    # Prefer canonical normalized fields if present
                    _bn = _best.get("value_norm")
                    _bu = _best.get("base_unit") or _best.get("unit")
                    _rawv = _best.get("raw") if _best.get("raw") is not None else _best.get("value")

                    if _bn is not None:
                        try:
                            base_metric["value_norm"] = float(_bn)
                        except Exception:
                            pass

                    # Preserve unit/base_unit
                    if _bu:
                        try:
                            base_metric["base_unit"] = str(_bu)
                        except Exception:
                            pass
                    if _best.get("unit"):
                        base_metric["unit"] = _best.get("unit")

                    # Preserve raw/value display from evidence (preferred)
                    if _rawv is not None:
                        base_metric["raw"] = _rawv
                        base_metric["value"] = _rawv

                    # Helpful debug: show that we anchored value from evidence
                    base_metric.setdefault("debug", {})
                    if isinstance(base_metric.get("debug"), dict):
                        base_metric["debug"]["value_origin"] = "evidence_best_candidate"
                        base_metric["debug"]["evidence_candidate_id"] = _best.get("candidate_id") or _best.get("anchor_hash")
                    _anchored_value_set = True
        except Exception:
            pass
        if vals and not _anchored_value_set:

            vals_sorted = sorted(vals)
            vmin, vmax = vals_sorted[0], vals_sorted[-1]
            vmed = vals_sorted[len(vals_sorted) // 2]
            base_metric["value"] = vmed
            base_metric["range"] = {
                "min": vmin,
                "max": vmax,
                "candidates": vals_sorted,
                "n": len(vals_sorted),
            }
        else:
            base_metric["range"] = {"min": None, "max": None, "candidates": [], "n": 0}

        if len(vals_norm) >= 2:
            vn = sorted(vals_norm)
            base_metric["range_norm"] = {
                "min": vn[0],
                "max": vn[-1],
                "candidates": vn,
                "n": len(vn),
                "unit": base_metric.get("base_unit") or base_metric.get("unit") or "",
            }

        canonicalized[ckey] = base_metric

    return canonicalized


# - Do NOT allow dimension=='unknown' or missing unit_family to enter primary_metrics_canonical.
# - Preserve them under primary_metrics_provisional with full debug trace for audit.
def _fix2d58b_split_primary_metrics_canonical(pmc: dict):
    try:
        if not isinstance(pmc, dict):
            return {}, {}
        canonical_ok = {}
        provisional = {}
        for k, v in pmc.items():
            if not isinstance(v, dict):
                provisional[k] = v
                continue
            dim = str(v.get('dimension') or '').strip().lower()
            uf = str(v.get('unit_family') or '').strip().lower()
            ut = str(v.get('unit_tag') or '').strip()
            # If dimension already implies family, fill it deterministically (additive).
            if dim in ('currency', 'percent') and not uf:
                vv = dict(v)
                vv['unit_family'] = dim
                v = vv
                uf = dim
            # Quarantine: unknown dimension OR missing family when we at least have a unit tag.
            if dim == 'unknown' or (not uf and bool(ut)):
                vv = dict(v)
                vv.setdefault('debug', {})
                if isinstance(vv.get('debug'), dict):
                    vv['debug']['quarantined_v1'] = True
                    vv['debug']['quarantine_reason_v1'] = 'unknown_dimension_or_missing_unit_family'
                provisional[k] = vv
            else:
                canonical_ok[k] = v
        return canonical_ok, provisional
    except Exception:
        pass
        # Fail-safe: never drop metrics silently if the splitter errors
        return pmc if isinstance(pmc, dict) else {}, {}


# - After FIX2D59 rekeying, primary_metrics_canonical must contain ONLY schema-bound keys.
# - Any PROVISIONAL/UNSPECIFIED rows are moved into primary_metrics_provisional.


def freeze_metric_schema(canonical_metrics: Dict) -> Dict:
    """
    Lock metric identity + expected schema for future evolution.

    Key fix:
      - Stores canonical_key (dimension-safe)
      - Stores dimension + unit family
      - Keywords include dimension hints to improve later matching
    """
    frozen = {}
    if not isinstance(canonical_metrics, dict):
        return frozen

    # This improves consistency with extractor + attribution gating.
    # Falls back safely to old heuristics.
    def _normalize_unit_safe(u: str) -> str:
        try:
            fn = globals().get("normalize_unit_tag")
            if callable(fn):
                return fn(u or "")
        except Exception:
            return (u or "").strip()

    def _unit_family_safe(unit_raw: str, dim_hint: str = "") -> str:
        # 1) dimension-first (strongest signal)
        d = (dim_hint or "").strip().lower()
        if d in ("percent", "pct"):
            return "percent"
        if d in ("currency",):
            return "currency"
        if d in ("energy",):
            return "energy"
        if d in ("unit_sales", "count"):
            # You’ve been treating M/B/T as “magnitude” for counts; keep aligned.
            return "magnitude"
        if d in ("index", "score"):
            return "index"

        # 2) if you already have a unit-family helper in the codebase, use it
        try:
            fn = globals().get("unit_family")
            if callable(fn):
                uf = fn(_normalize_unit_safe(unit_raw))
                if isinstance(uf, str) and uf.strip():
                    return uf.strip().lower()
        except Exception:
            pass

        # 3) fallback to old heuristic (your original logic)
        u = (unit_raw or "").strip().lower()
        if not u:
            return "unknown"
        if "%" in u:
            return "percent"
        if any(t in u for t in ["$", "s$", "usd", "sgd", "eur", "€", "gbp", "£", "jpy", "¥", "cny", "rmb"]):
            return "currency"
        if any(t in u for t in ["b", "bn", "billion", "m", "mn", "million", "k", "thousand", "t", "trillion"]):
            return "magnitude"
        return "other"

    for ckey, m in canonical_metrics.items():
        if not isinstance(m, dict):
            continue

        dim = (m.get("dimension") or "").strip() or "unknown"
        name = m.get("name")
        unit = (m.get("unit") or "").strip()

        uf = _unit_family_safe(unit, dim_hint=dim)

        # Keywords: name + dimension token to prevent cross-dimension matches later
        kws = extract_context_keywords(name or "") or []
        if dim and dim not in kws:
            kws.append(dim)
        if uf and uf not in kws:
            kws.append(uf)

        # - Keep your existing behavior in 'unit' (backward compatible),
        #   BUT also add 'unit_tag' which is the canonicalized unit used downstream.
        # - This avoids the "SGD -> S" schema corruption that breaks currency gating.
        unit_tag = _normalize_unit_safe(unit)
        # Keep existing 'unit' output to avoid breaking consumers:
        unit_out = unit_clean_first_letter(unit.upper())

        frozen[ckey] = {
            "canonical_key": ckey,
            "canonical_id": m.get("canonical_id") or ckey.split("__", 1)[0],
            "dimension": dim,
            "name": name,

            # Existing field kept exactly (backward compatible)
            "unit": unit_out,

            "unit_tag": unit_tag,          # e.g., "%", "M", "B", "TWh"
            "unit_family": uf,             # e.g., "currency", "percent", "magnitude"

            "keywords": kws[:30],
        }


# Purpose:
#   Extend frozen schema with new canonical keys for EV charging infrastructure
#   (e.g., global EV chargers by 2040) so injected charger content can be
#   canonicalised + diffed deterministically under the shared Analysis selector.
# Notes:
#   - Purely additive: does not modify existing schema entries
#   - Dimension uses "count" so existing unit-family logic treats it as magnitude
#   - unit_tag uses "M" (million-scale counts) but unit_family remains "magnitude"

def _fix2u_extend_metric_schema_ev_chargers(metric_schema_frozen: dict) -> dict:
    """Add EV charger infrastructure canonical keys to frozen schema (additive)."""
    try:
        if not isinstance(metric_schema_frozen, dict):
            metric_schema_frozen = {}
    except Exception:
        pass
        metric_schema_frozen = {}

    def _add_if_missing(ckey: str, entry: dict):
        try:
            if not ckey or not isinstance(entry, dict):
                return
            if ckey in metric_schema_frozen:
                return
            metric_schema_frozen[ckey] = entry
        except Exception:
            pass

    # Minimal v1 keys (Global, high-signal, year-qualified)
    # 1) Global EV chargers by 2040 (count)
    _add_if_missing(
        "global_ev_chargers_2040__unit_count",
        {
            "canonical_key": "global_ev_chargers_2040__unit_count",
            "canonical_id": "global_ev_chargers_2040",
            "dimension": "count",
            "name": "Global EV chargers (2040)",
            # Backward-compatible schema fields:
            "unit": "M",
            # Stable, non-breaking additions used by downstream gates:
            "unit_tag": "M",
            "unit_family": "magnitude",
            "keywords": [
                "ev", "electric", "vehicle",
                "charger", "chargers", "charging", "infrastructure", "network",
                "global", "worldwide",
                "2040", "count", "magnitude",
            ],
        },
    )

    # 2) Global EV charging investment by 2040 (currency) — only used if extracted evidence exists
    _add_if_missing(
        "global_ev_charging_investment_2040__currency",
        {
            "canonical_key": "global_ev_charging_investment_2040__currency",
            "canonical_id": "global_ev_charging_investment_2040",
            "dimension": "currency",
            "name": "Global EV charging investment (2040)",
            "unit": "U",      # backward-compatible first-letter field (unit_tag holds the real tag)
            "unit_tag": "USD",
            "unit_family": "currency",
            "keywords": [
                "ev", "electric", "vehicle",
                "charger", "chargers", "charging", "infrastructure", "network",
                "investment", "spend", "spending", "capex", "expenditure",
                "global", "worldwide",
                "2040", "currency", "usd",
            ],
        },
    )

    return metric_schema_frozen


    return frozen

#   - Adds a canonical slot for charger infrastructure CAGR (2026->2040)
#   - Keeps semantics single-sourced under metric_schema_frozen
def _fix2v_extend_metric_schema_ev_chargers_cagr(metric_schema_frozen: dict) -> dict:
    """Add charger CAGR schema key additively (safe no-op if already present)."""
    try:
        if not isinstance(metric_schema_frozen, dict):
            return metric_schema_frozen
        # Do not override if exists
        if "global_ev_chargers_cagr_2026_2040__percent" in metric_schema_frozen:
            return metric_schema_frozen

        metric_schema_frozen = dict(metric_schema_frozen)

        metric_schema_frozen["global_ev_chargers_cagr_2026_2040__percent"] = dict(
            metric_id="global_ev_chargers_cagr_2026_2040",
            canonical_id="global_ev_chargers_cagr_2026_2040",
            metric_name="Global EV charging infrastructure CAGR (2026–2040)",
            dimension="percent",
            unit_family="percent",
            unit_tag="percent",
            # Deterministic keyword allowlist (no fuzzy matching)
            keywords=[
                "global", "worldwide",
                "ev", "charging", "charging infrastructure",
                "cagr", "compound annual growth rate",
                "2026", "2040",
            ],
        )
        return metric_schema_frozen
    except Exception:
        return metric_schema_frozen

#   - Adds a canonical slot for Global EV sales (YTD 2025) in unit_sales
#   - Intended for deterministic diff testing using Rhomotion-style sources
def _fix2ab_extend_metric_schema_global_ev_sales_ytd_2025(metric_schema_frozen: dict) -> dict:
    """Add Global EV Sales (YTD 2025) schema key additively (safe no-op if already present)."""
    try:
        if not isinstance(metric_schema_frozen, dict):
            return metric_schema_frozen
        if "global_ev_sales_ytd_2025__unit_sales" in metric_schema_frozen:
            return metric_schema_frozen

        metric_schema_frozen = dict(metric_schema_frozen)

        metric_schema_frozen["global_ev_sales_ytd_2025__unit_sales"] = dict(
            metric_id="global_ev_sales_ytd_2025",
            canonical_id="global_ev_sales_ytd_2025",
            metric_name="Global EV sales (YTD 2025)",
            dimension="unit_sales",
            unit_family="magnitude",
            unit_tag="million units",
            unit="M",
            # Deterministic keyword allowlist (no fuzzy matching)
            keywords=[
                "global", "worldwide",
                "ev", "sales",
                "ytd", "year-to-date",
                "2025",
                "million", "units",
            ],
        )
        return metric_schema_frozen
    except Exception:
        return metric_schema_frozen


# RANGE + SOURCE ATTRIBUTION (DETERMINISTIC, NO LLM)

def stable_json_hash(obj: Any) -> str:
    import hashlib, json
    try:
        s = json.dumps(obj, sort_keys=True, ensure_ascii=False, separators=(",", ":"))
    except Exception:
        pass
        s = str(obj)
    return hashlib.sha256(s.encode("utf-8", errors="ignore")).hexdigest()

def make_extracted_number_id(source_url: str, fingerprint: str, n: Dict) -> str:
    payload = {
        "url": source_url or "",
        "fp": fingerprint or "",
        "start": n.get("start_idx"),
        "end": n.get("end_idx"),
        "value": n.get("value"),
        "unit": normalize_unit(n.get("unit") or ""),
        "raw": n.get("raw") or "",
        "ctx": " ".join((n.get("context_snippet") or "").split())[:240],
    }
    return stable_json_hash(payload)

def sort_snapshot_numbers(numbers: List[Dict]) -> List[Dict]:
    """
    Deterministic ordering for extracted_numbers in snapshots.

    Backward compatible + robust:
      - Uses start/end idx when present
      - Avoids hard dependency on normalize_unit() (may not exist)
      - Falls back to normalize_unit_tag() if available
    """

    # - Prefer normalize_unit() if it exists
    # - Else fall back to normalize_unit_tag() if present
    # - Else just return stripped unit
    _norm_unit_fn = globals().get("normalize_unit")
    _norm_tag_fn = globals().get("normalize_unit_tag")

    def _safe_norm_unit(u: str) -> str:
        u = (u or "").strip()
        try:
            if callable(_norm_unit_fn):
                return str(_norm_unit_fn(u) or "")
        except Exception:
            pass
        try:
            if callable(_norm_tag_fn):
                # normalize_unit_tag expects tags / unit-ish strings; still better than raw
                return str(_norm_tag_fn(u) or "")
        except Exception:
            return u

    def k(n: Dict[str, Any]):
        n = n or {}
        return (
            n.get("start_idx") if isinstance(n.get("start_idx"), int) else 10**18,
            n.get("end_idx") if isinstance(n.get("end_idx"), int) else 10**18,

            # stable identity ordering
            str(n.get("anchor_hash") or ""),

            # unit + value
            _safe_norm_unit(str(n.get("unit") or "")),
            str(n.get("unit_tag") or ""),
            str(n.get("value_norm") if n.get("value_norm") is not None else n.get("value")),

            # final tie-breakers
            str(n.get("raw") or ""),
            str(n.get("context_snippet") or n.get("context") or "")[:80],
        )

    return sorted((numbers or []), key=k)

def sort_evidence_records(records: List[Dict]) -> List[Dict]:
    """
    Deterministic ordering for evidence_records.

    Backward compatible:
      - Uses url + fingerprint (as you had)
      - Adds fetched_at as tie-breaker if present (non-breaking)
    """

    def k(r: Dict[str, Any]):
        r = r or {}
        return (
            str(r.get("url") or ""),
            str(r.get("fingerprint") or ""),
            str(r.get("fetched_at") or ""),
        )

    return sorted((records or []), key=k)

def sort_metric_anchors(anchors: List[Dict]) -> List[Dict]:
    # - Prefer canonical_key (new)
    # - Fall back to metric_id/metric_name (legacy)
    return sorted(
        (anchors or []),
        key=lambda a: (
            str((a or {}).get("canonical_key") or ""),
            str((a or {}).get("metric_id") or ""),
            str((a or {}).get("metric_name") or ""),
            str((a or {}).get("source_url") or ""),
        ),
    )


def to_billions(value: float, unit_tag: str) -> Optional[float]:
    """Convert T/B/M tagged values into billions. Leaves % unchanged (returns None for % here)."""
    try:
        v = float(value)
    except Exception:
        return None

    if unit_tag == "T":
        return v * 1000.0
    if unit_tag == "B":
        return v
    if unit_tag == "M":
        return v / 1000.0
    return None


def build_metric_keywords(metric_name: str) -> List[str]:
    """Reuse your existing keyword extractor, but ensure we always have something."""
    kws = extract_context_keywords(metric_name) or []
    # Add simple fallback tokens (deterministic)
    for t in re.findall(r"[a-zA-Z]{4,}", str(metric_name).lower()):
        if t not in kws:
            kws.append(t)
    return kws[:25]


def extract_numbers_from_scraped_sources(
    scraped_content: Dict[str, str],
) -> List[Dict[str, Any]]:
    """
    Deterministically extract numeric candidates from all scraped source texts.
    Returns list of {url, value, unit_tag, raw, context}.
    """
    candidates: List[Dict[str, Any]] = []
    if not isinstance(scraped_content, dict):
        return candidates

    for url, content in scraped_content.items():
        if not content or not isinstance(content, str) or len(content) < 200:
            continue

        nums = extract_numbers_with_context(content, source_url=url)

        for n in nums:
            unit_tag = n.get("unit_tag")
            if not unit_tag:
                unit_tag = normalize_unit_tag(n.get("unit", ""))

            row = {
                "url": url,
                "value": n.get("value"),
                "unit_tag": unit_tag,
                "raw": n.get("raw", ""),
                "context": (n.get("context") or ""),
            }

            if "measure_kind" in n:
                row["measure_kind"] = n.get("measure_kind")
            if "measure_assoc" in n:
                row["measure_assoc"] = n.get("measure_assoc")

            # (backwards compatible: we only add keys, never remove)
            for k in [
                "unit", "is_junk", "junk_reason", "anchor_hash",
                "start_idx", "end_idx", "context_snippet",
                "unit_family", "base_unit", "multiplier_to_base", "value_norm"
            ]:
                if k in n:
                    row[k] = n.get(k)

            # Why:
            #   - Some candidates may not carry unit_family/base_unit/value_norm yet
            #   - We want every candidate (analysis + evolution) to have the same
            #     canonical fields so diff + span logic is stable and drift-free.
            #
            # This is additive and safe to call multiple times.
            try:
                fn_can = globals().get("canonicalize_numeric_candidate")
                if callable(fn_can):
                    row = fn_can(row) or row
                else:
                    row = canonicalize_numeric_candidate(row) or row
            except Exception:
                pass

            row.setdefault("unit_family", unit_family(row.get("unit_tag", "") or ""))
            row.setdefault("base_unit", row.get("unit_tag", "") or "")
            row.setdefault("multiplier_to_base", 1.0)
            if row.get("value") is not None and row.get("value_norm") is None:
                try:
                    row["value_norm"] = float(row.get("value"))
                except Exception:
                    pass

            candidates.append(row)

    return candidates


def attribute_span_to_sources(
    metric_name: str,
    metric_unit: str,
    scraped_content: Dict[str, str],
    rel_tol: float = 0.08,
    # - If provided, we enforce schema-first gating for drift stability.
    # - If not provided, we fall back to existing heuristic behavior.
    canonical_key: str = "",
    metric_schema: Dict[str, Any] = None,
) -> Dict[str, Any]:
    """
    Build a deterministic span (min/mid/max) for a metric, and attribute min/max to sources.
    Uses only scraped content + regex extractions (NO LLM).

    Schema-first behavior (when metric_schema/canonical_key provided):
      - Enforces unit_family and currency/count/percent gating from frozen schema
      - Uses measure_kind tags when available to avoid semantic leakage
      - Keeps deterministic tie-breaking
    """
    import re
    import hashlib

    unit_tag_hint = normalize_unit_tag(metric_unit)
    keywords = build_metric_keywords(metric_name)

    all_candidates = extract_numbers_from_scraped_sources(scraped_content)
    filtered: List[Dict[str, Any]] = []

    metric_l = (metric_name or "").lower()

    schema_entry = None
    if isinstance(metric_schema, dict) and canonical_key and isinstance(metric_schema.get(canonical_key), dict):
        schema_entry = metric_schema.get(canonical_key)

    schema_unit_family = ""
    schema_dimension = ""
    schema_unit = ""
    if isinstance(schema_entry, dict):
        schema_unit_family = (schema_entry.get("unit_family") or "").strip().lower()
        schema_dimension = (schema_entry.get("dimension") or "").strip().lower()
        schema_unit = (schema_entry.get("unit") or "").strip()

    expected_family = ""
    if schema_unit_family in ("percent", "currency", "energy"):
        expected_family = schema_unit_family
    if not expected_family:
        ut = normalize_unit_tag(metric_unit)
        if ut == "%":
            expected_family = "percent"
        elif ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
            expected_family = "energy"
        else:
            expected_family = ""

    currencyish = False
    if schema_unit_family == "currency" or schema_dimension == "currency":
        currencyish = True
    if not currencyish:
        mu = (metric_unit or "").lower()
        if any(x in mu for x in ["usd", "sgd", "eur", "gbp", "$", "s$", "€", "£", "aud", "cad", "jpy", "cny", "rmb"]):
            currencyish = True
    if not currencyish and any(x in metric_l for x in ["revenue", "turnover", "valuation", "market value", "market size",
                                                       "profit", "earnings", "ebitda", "capex", "opex"]):
        currencyish = True

    expected_kind = None

    if expected_family == "percent":
        if any(k in metric_l for k in ["growth", "cagr", "increase", "decrease", "yoy", "qoq", "mom", "rate"]):
            expected_kind = "growth_pct"
        else:
            expected_kind = "share_pct"

    if currencyish:
        expected_kind = "money"

    if expected_kind is None and any(k in metric_l for k in [
        "units", "unit sales", "vehicle sales", "vehicles sold", "sold", "sales volume",
        "deliveries", "shipments", "registrations", "volume"
    ]):
        expected_kind = "count_units"

    metric_is_yearish = any(k in metric_l for k in ["year", "years", "fy", "fiscal", "calendar", "timeline", "target year"])

    def _looks_like_year_value(v) -> bool:
        try:
            iv = int(float(v))
            return 1900 <= iv <= 2099
        except Exception:
            return False

    def _ctx_has_year_range(ctx: str) -> bool:
        return bool(re.search(r"\b(19|20)\d{2}\s*(?:-|–|—|to)\s*(19|20)\d{2}\b", ctx or "", flags=re.I))

    def _has_currency_evidence(raw: str, ctx: str) -> bool:
        r = (raw or "")
        c = (ctx or "").lower()

        if any(s in r for s in ["$", "S$", "€", "£"]):
            return True
        if any(code in c for code in [" usd", "sgd", " eur", " gbp", " aud", " cad", " jpy", " cny", " rmb"]):
            return True

        strong_kw = [
            "revenue", "turnover", "valuation", "valued at", "market value", "market size",
            "sales value", "net profit", "operating profit", "gross profit",
            "ebitda", "earnings", "income", "capex", "opex"
        ]
        if any(k in c for k in strong_kw):
            return True
        return False

    # - Stable across runs, depends only on stable fields
    # - Used ONLY as final tie-breaker (won't change non-tie outcomes)
    def _candidate_id(x: dict) -> str:
        try:
            url = str(x.get("url") or x.get("source_url") or "")
            ah = str(x.get("anchor_hash") or "")
            vn = x.get("value_norm")
            bu = str(x.get("base_unit") or x.get("unit") or x.get("unit_tag") or "")
            mk = str(x.get("measure_kind") or "")
            # normalize numeric string for stability
            vn_s = ""
            if vn is not None:
                try:
                    vn_s = f"{float(vn):.12g}"
                except Exception:
                    pass
                    vn_s = str(vn)
            s = f"{url}|{ah}|{vn_s}|{bu}|{mk}"
            return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()
        except Exception:
            return ""

    for c in all_candidates:
        ctx = c.get("context", "")
        if not ctx:
            continue

        if c.get("is_junk") is True:
            continue

        if not metric_is_yearish:
            if (c.get("unit_tag") in ("", None)) and _looks_like_year_value(c.get("value")):
                continue
            if _looks_like_year_value(c.get("value")) and _ctx_has_year_range(ctx):
                continue

        ctx_score = calculate_context_match(keywords, ctx)
        if ctx_score <= 0.0:
            continue

        cand_ut = c.get("unit_tag") or normalize_unit_tag(c.get("unit") or "")
        cand_fam = (c.get("unit_family") or unit_family(cand_ut) or "").strip().lower()

        if expected_family:
            if expected_family == "percent" and cand_fam != "percent":
                continue
            if expected_family == "currency":
                if cand_fam not in ("currency", "magnitude"):
                    continue
                if not _has_currency_evidence(c.get("raw", ""), ctx):
                    continue
            if expected_family == "energy" and cand_fam != "energy":
                continue

        if expected_kind:
            mk = c.get("measure_kind")
            if mk and mk != expected_kind:
                continue

        val_norm = None
        if expected_family == "percent" or unit_tag_hint == "%":
            if cand_ut != "%":
                continue
            val_norm = c.get("value")

        elif expected_family == "energy":
            val_norm = c.get("value_norm")
            if val_norm is None:
                val_norm = c.get("value")

        elif currencyish or expected_family == "currency":
            if c.get("measure_kind") == "count_units":
                continue
            if cand_ut not in ("T", "B", "M"):
                continue
            val_norm = to_billions(c.get("value"), cand_ut)
            if val_norm is None:
                continue

        else:
            try:
                val_norm = float(c.get("value"))
            except Exception:
                pass
                continue

        row = {
            **c,
            "unit_tag": cand_ut,
            "unit_family": cand_fam,
            "value_norm": val_norm,
            "ctx_score": float(ctx_score),
        }

        row.setdefault("candidate_id", _candidate_id(row))

        filtered.append(row)

    if not filtered:
        return {
            "span": None,
            "source_attribution": None,
            "evidence": []
        }

    # Deterministic selection: value_norm then ctx_score then url then candidate_id
    def min_key(x):
        return (
            float(x["value_norm"]),
            -float(x["ctx_score"]),
            str(x.get("url", "")),
            str(x.get("candidate_id", "")),
        )

    def max_key(x):
        return (
            -float(x["value_norm"]),
            -float(x["ctx_score"]),
            str(x.get("url", "")),
            str(x.get("candidate_id", "")),
        )

    min_item = sorted(filtered, key=min_key)[0]
    max_item = sorted(filtered, key=max_key)[0]

    vmin = float(min_item["value_norm"])
    vmax = float(max_item["value_norm"])
    vmid = (vmin + vmax) / 2.0

    if expected_family == "percent" or unit_tag_hint == "%":
        unit_out = "%"
    elif currencyish or expected_family == "currency":
        unit_out = "billion USD"
    elif expected_family == "energy":
        unit_out = "Wh"
    else:
        unit_out = metric_unit or (schema_unit or "")

    evidence = []
    for it in sorted(filtered, key=lambda x: (-float(x["ctx_score"]), str(x.get("url", "")), str(x.get("candidate_id", ""))))[:12]:
        evidence.append({
            "url": it.get("url"),
            "raw": it.get("raw"),
            "unit_tag": it.get("unit_tag"),
            "unit_family": it.get("unit_family"),
            "measure_kind": it.get("measure_kind"),
            "measure_assoc": it.get("measure_assoc"),
            "value_norm": it.get("value_norm"),
            "candidate_id": it.get("candidate_id"),
            "anchor_hash": it.get("anchor_hash"),
            "start_idx": it.get("start_idx"),
            "end_idx": it.get("end_idx"),
            "value_norm": it.get("value_norm"),
            "base_unit": it.get("base_unit"),
            "multiplier_to_base": it.get("multiplier_to_base"),  # PATCH S11: exposed for transparency
            "context_snippet": (it.get("context") or "")[:220],
            "context_score": round(float(it.get("ctx_score", 0.0)) * 100, 1),
        })

    return {
        "span": {
            "min": round(vmin, 4),
            "mid": round(vmid, 4),
            "max": round(vmax, 4),
            "unit": unit_out
        },
        "source_attribution": {
            "min": {
                "url": min_item.get("url"),
                "raw": min_item.get("raw"),
                "measure_kind": min_item.get("measure_kind"),
                "measure_assoc": min_item.get("measure_assoc"),
                "value_norm": min_item.get("value_norm"),
                "candidate_id": min_item.get("candidate_id"),  # PATCH S11
                "context_snippet": (min_item.get("context") or "")[:220],
                "context_score": round(float(min_item.get("ctx_score", 0.0)) * 100, 1),
            },
            "max": {
                "url": max_item.get("url"),
                "raw": max_item.get("raw"),
                "measure_kind": max_item.get("measure_kind"),
                "measure_assoc": max_item.get("measure_assoc"),
                "value_norm": max_item.get("value_norm"),
                "candidate_id": max_item.get("candidate_id"),  # PATCH S11
                "context_snippet": (max_item.get("context") or "")[:220],
                "context_score": round(float(max_item.get("ctx_score", 0.0)) * 100, 1),
            }
        },
        "evidence": evidence
    }


def add_range_and_source_attribution_to_canonical_metrics(
    canonical_metrics: Dict[str, Any],
    web_context: dict,
    # If provided, attribution uses frozen schema to avoid semantic/unit leakage.
    metric_schema: Dict[str, Any] = None,
) -> Dict[str, Any]:
    """
    Enrich canonical metrics with deterministic range + source attribution.

    IMPORTANT:
    - canonical_metrics is expected to be keyed by canonical_key (dimension-safe),
      i.e. the output of canonicalize_metrics().
    - Schema-first mode (recommended): pass metric_schema=metric_schema_frozen so
      attribute_span_to_sources() can enforce unit_family / measure_kind gates.
    - Backward compatible: if metric_schema not provided, attribution falls back
      to existing heuristic behavior inside attribute_span_to_sources().
    """
    enriched: Dict[str, Any] = {}
    if not isinstance(canonical_metrics, dict):
        return enriched

    scraped = (web_context or {}).get("scraped_content") or {}
    if not isinstance(scraped, dict):
        scraped = {}

    schema = metric_schema if isinstance(metric_schema, dict) else {}

    for ckey, m in canonical_metrics.items():
        if not isinstance(m, dict):
            continue

        metric_name = m.get("name") or m.get("original_name") or str(ckey)
        metric_unit = m.get("unit") or ""

        # - canonical_key is the dict key (ckey)
        # - metric_schema is the frozen schema dict (if provided)
        span_pack = attribute_span_to_sources(
            metric_name=metric_name,
            metric_unit=metric_unit,
            scraped_content=scraped,
            canonical_key=str(ckey),
            metric_schema=schema,
        )

        mm = dict(m)

        # Preserve old behavior: only add keys (don’t remove anything)
        if isinstance(span_pack, dict):
            if span_pack.get("span") is not None:
                mm["source_span"] = span_pack.get("span")
            if span_pack.get("source_attribution") is not None:
                mm["source_attribution"] = span_pack.get("source_attribution")
            if span_pack.get("evidence") is not None:
                mm["evidence"] = span_pack.get("evidence")

        enriched[ckey] = mm

    return enriched


# SEMANTIC FINDING HASH
# Removes wording-based churn from findings comparison

# Semantic components to extract from findings
FINDING_PATTERNS = {
    # Growth/decline patterns
    "growth": [
        r'(?:grow(?:ing|th)?|increas(?:e|ing)|expand(?:ing)?|ris(?:e|ing)|up)\s*(?:by|at|of)?\s*(\d+(?:\.\d+)?)\s*%?',
        r'(\d+(?:\.\d+)?)\s*%?\s*(?:growth|increase|expansion|rise)',
    ],
    "decline": [
        r'(?:declin(?:e|ing)|decreas(?:e|ing)|fall(?:ing)?|drop(?:ping)?|down)\s*(?:by|at|of)?\s*(\d+(?:\.\d+)?)\s*%?',
        r'(\d+(?:\.\d+)?)\s*%?\s*(?:decline|decrease|drop|fall)',
    ],

    # Value patterns
    "value": [
        r'\$\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)?',
        r'(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)',
    ],

    # Ranking patterns
    "rank": [
        r'(?:lead(?:ing|er)?|top|first|largest|biggest|#1|number one)',
        r'(?:second|#2|runner.?up)',
        r'(?:third|#3)',
    ],

    # Trend patterns
    "trend_up": [
        r'(?:bullish|optimistic|positive|strong|robust|accelerat)',
    ],
    "trend_down": [
        r'(?:bearish|pessimistic|negative|weak|slow(?:ing)?|decelerat)',
    ],

    # Entity patterns (will be filled dynamically)
    "entities": []
}

# Common stop words to remove
STOP_WORDS = {
    'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
    'should', 'may', 'might', 'must', 'shall', 'can', 'to', 'of', 'in',
    'for', 'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through',
    'during', 'before', 'after', 'above', 'below', 'between', 'under',
    'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where',
    'why', 'how', 'all', 'each', 'few', 'more', 'most', 'other', 'some',
    'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',
    'too', 'very', 'just', 'also', 'now', 'new'
}


def extract_semantic_components(finding: str) -> Dict[str, Any]:
    """
    Extract semantic components from a finding.

    Example:
        "The market is growing at 15% annually" ->
        {
            "direction": "up",
            "percentage": 15.0,
            "subject": "market",
            "timeframe": "annual",
            "entities": [],
            "keywords": ["market", "growing", "annually"]
        }
    """
    if not finding:
        return {}

    finding_lower = finding.lower()
    components = {
        "direction": None,
        "percentage": None,
        "value": None,
        "value_unit": None,
        "subject": None,
        "timeframe": None,
        "entities": [],
        "keywords": []
    }

    # Extract direction
    for pattern in FINDING_PATTERNS["growth"]:
        match = re.search(pattern, finding_lower)
        if match:
            components["direction"] = "up"
            if match.groups():
                try:
                    components["percentage"] = float(match.group(1))
                except:
                    pass
            break

    if not components["direction"]:
        for pattern in FINDING_PATTERNS["decline"]:
            match = re.search(pattern, finding_lower)
            if match:
                components["direction"] = "down"
                if match.groups():
                    try:
                        components["percentage"] = float(match.group(1))
                    except:
                        pass
                break

    # Extract trend sentiment
    if not components["direction"]:
        for pattern in FINDING_PATTERNS["trend_up"]:
            if re.search(pattern, finding_lower):
                components["direction"] = "up"
                break
        for pattern in FINDING_PATTERNS["trend_down"]:
            if re.search(pattern, finding_lower):
                components["direction"] = "down"
                break

    # Extract value
    for pattern in FINDING_PATTERNS["value"]:
        match = re.search(pattern, finding_lower)
        if match:
            try:
                components["value"] = float(match.group(1))
                if len(match.groups()) > 1 and match.group(2):
                    components["value_unit"] = match.group(2)[0].upper()
            except:
                pass
            break

    # Extract timeframe
    timeframe_patterns = {
        "annual": r'\b(?:annual(?:ly)?|year(?:ly)?|per year|yoy|y-o-y)\b',
        "quarterly": r'\b(?:quarter(?:ly)?|q[1-4])\b',
        "monthly": r'\b(?:month(?:ly)?|per month)\b',
        "2024": r'\b2024\b',
        "2025": r'\b2025\b',
        "2026": r'\b2026\b',
        "2030": r'\b2030\b',
    }
    for tf_name, tf_pattern in timeframe_patterns.items():
        if re.search(tf_pattern, finding_lower):
            components["timeframe"] = tf_name
            break

    # Extract subject keywords
    words = re.findall(r'\b[a-z]{3,}\b', finding_lower)
    keywords = [w for w in words if w not in STOP_WORDS]
    components["keywords"] = keywords[:10]  # Limit to top 10

    # Identify likely subject
    subject_candidates = ["market", "industry", "sector", "segment", "revenue", "sales", "demand", "supply"]
    for word in keywords:
        if word in subject_candidates:
            components["subject"] = word
            break

    return components


def compute_semantic_hash(finding: str) -> str:
    """
    Compute a semantic hash for a finding that's invariant to wording changes.

    Two findings with the same meaning should produce the same hash.

    Example:
        "The market is growing at 15% annually" -> "up_15.0_market_annual"
        "Annual growth rate stands at 15%" -> "up_15.0_market_annual"
    """
    components = extract_semantic_components(finding)

    # Build hash components in consistent order
    hash_parts = []

    # Direction
    if components.get("direction"):
        hash_parts.append(components["direction"])

    # Percentage (rounded to avoid float issues)
    if components.get("percentage") is not None:
        hash_parts.append(f"{components['percentage']:.1f}")

    # Value with unit
    if components.get("value") is not None:
        val_str = f"{components['value']:.1f}"
        if components.get("value_unit"):
            val_str += components["value_unit"]
        hash_parts.append(val_str)

    # Subject
    if components.get("subject"):
        hash_parts.append(components["subject"])

    # Timeframe
    if components.get("timeframe"):
        hash_parts.append(components["timeframe"])

    # If we have enough components, use them for hash
    if len(hash_parts) >= 2:
        return "_".join(hash_parts)

    # Fallback: use sorted keywords
    keywords = sorted(components.get("keywords", []))[:5]
    if keywords:
        return "_".join(keywords)

    # Last resort: normalized text hash
    normalized = re.sub(r'\s+', ' ', finding.lower().strip())
    return hashlib.md5(normalized.encode()).hexdigest()[:12]


def compute_semantic_finding_diffs(old_findings: List[str], new_findings: List[str]) -> List[FindingDiff]:
    """
    Compute finding diffs using semantic hashing instead of text similarity.

    This ensures that findings with the same meaning but different wording
    are recognized as the same finding.
    """
    diffs = []
    matched_new_indices = set()

    # Compute hashes for all findings
    old_hashes = [(f, compute_semantic_hash(f), extract_semantic_components(f)) for f in old_findings if f]
    new_hashes = [(f, compute_semantic_hash(f), extract_semantic_components(f)) for f in new_findings if f]

    # Match by semantic hash
    for old_text, old_hash, old_components in old_hashes:
        best_match_idx = None
        best_match_score = 0

        for i, (new_text, new_hash, new_components) in enumerate(new_hashes):
            if i in matched_new_indices:
                continue

            # Exact hash match = same finding
            if old_hash == new_hash:
                best_match_idx = i
                best_match_score = 100
                break

            # Component-based similarity
            score = compute_component_similarity(old_components, new_components)
            if score > best_match_score:
                best_match_score = score
                best_match_idx = i

        if best_match_idx is not None and best_match_score >= 60:
            matched_new_indices.add(best_match_idx)
            new_text = new_hashes[best_match_idx][0]

            if best_match_score >= 90:
                change_type = 'retained'
            else:
                change_type = 'modified'

            diffs.append(FindingDiff(
                old_text=old_text,
                new_text=new_text,
                similarity=best_match_score,
                change_type=change_type
            ))
        else:
            # Finding removed
            diffs.append(FindingDiff(
                old_text=old_text,
                new_text=None,
                similarity=0,
                change_type='removed'
            ))

    # Find added findings
    for i, (new_text, new_hash, new_components) in enumerate(new_hashes):
        if i not in matched_new_indices:
            diffs.append(FindingDiff(
                old_text=None,
                new_text=new_text,
                similarity=0,
                change_type='added'
            ))

    return diffs


def compute_component_similarity(comp1: Dict, comp2: Dict) -> float:
    """
    Compute similarity between two finding component dictionaries.
    Returns a score from 0-100.
    """
    if not comp1 or not comp2:
        return 0

    score = 0
    weights = {
        "direction": 25,
        "percentage": 25,
        "value": 20,
        "subject": 15,
        "timeframe": 10,
        "keywords": 5
    }

    # Direction match
    if comp1.get("direction") and comp2.get("direction"):
        if comp1["direction"] == comp2["direction"]:
            score += weights["direction"]
    elif not comp1.get("direction") and not comp2.get("direction"):
        score += weights["direction"] * 0.5  # Both neutral

    # Percentage match (within 2% tolerance)
    if comp1.get("percentage") is not None and comp2.get("percentage") is not None:
        diff = abs(comp1["percentage"] - comp2["percentage"])
        if diff <= 2:
            score += weights["percentage"]
        elif diff <= 5:
            score += weights["percentage"] * 0.5

    # Value match (within 10% tolerance)
    if comp1.get("value") is not None and comp2.get("value") is not None:
        v1, v2 = comp1["value"], comp2["value"]
        # Normalize by unit
        if comp1.get("value_unit") == comp2.get("value_unit"):
            if v1 > 0 and v2 > 0:
                ratio = min(v1, v2) / max(v1, v2)
                if ratio >= 0.9:
                    score += weights["value"]
                elif ratio >= 0.8:
                    score += weights["value"] * 0.5

    # Subject match
    if comp1.get("subject") and comp2.get("subject"):
        if comp1["subject"] == comp2["subject"]:
            score += weights["subject"]

    # Timeframe match
    if comp1.get("timeframe") and comp2.get("timeframe"):
        if comp1["timeframe"] == comp2["timeframe"]:
            score += weights["timeframe"]

    # Keyword overlap
    kw1 = set(comp1.get("keywords", []))
    kw2 = set(comp2.get("keywords", []))
    if kw1 and kw2:
        overlap = len(kw1 & kw2) / len(kw1 | kw2)
        score += weights["keywords"] * overlap

    return score


# UPDATED METRIC DIFF COMPUTATION
# Using canonical IDs

def compute_metric_diffs_canonical(old_metrics: Dict, new_metrics: Dict) -> List[MetricDiff]:
    """
    Compute metric diffs using canonical IDs for stable matching.
    Range-aware via get_metric_value_span + spans_overlap.
    """
    diffs: List[MetricDiff] = []

    old_canonical = canonicalize_metrics(old_metrics)
    new_canonical = canonicalize_metrics(new_metrics)

    matched_new_ids = set()

    # Match by canonical ID
    for old_id, old_m in old_canonical.items():
        old_name = old_m.get("name", old_id)

        old_span = get_metric_value_span(old_m)
        old_raw = str(old_m.get("value", ""))
        old_unit = old_span.get("unit") or old_m.get("unit", "")
        old_val = old_span.get("mid")

        # Direct canonical ID match
        if old_id in new_canonical:
            new_m = new_canonical[old_id]
            matched_new_ids.add(old_id)

            new_raw = str(new_m.get("value", ""))
            new_span = get_metric_value_span(new_m)
            new_val = new_span.get("mid")
            new_unit = new_span.get("unit") or new_m.get("unit", old_unit)

            if spans_overlap(old_span, new_span, rel_tol=0.05):
                change_pct = 0.0
                change_type = "unchanged"
            else:
                change_pct = compute_percent_change(old_val, new_val)
                if change_pct is None or abs(change_pct) < 0.5:
                    change_type = "unchanged"
                elif change_pct > 0:
                    change_type = "increased"
                else:
                    change_type = "decreased"

            diffs.append(MetricDiff(
                name=old_name,
                old_value=old_val,
                new_value=new_val,
                old_raw=old_raw,
                new_raw=new_raw,
                unit=new_unit or old_unit,
                change_pct=change_pct,
                change_type=change_type
            ))
            continue  # important: don't fall into base-ID matching

        # Base ID match (strip years)
        base_id = re.sub(r'_\d{4}(?:_\d{4})*$', '', old_id)
        found = False

        for new_id, new_m in new_canonical.items():
            if new_id in matched_new_ids:
                continue

            new_base_id = re.sub(r'_\d{4}(?:_\d{4})*$', '', new_id)
            if base_id != new_base_id:
                continue

            matched_new_ids.add(new_id)
            found = True

            new_raw = str(new_m.get("value", ""))
            new_span = get_metric_value_span(new_m)
            new_val = new_span.get("mid")
            new_unit = new_span.get("unit") or new_m.get("unit", old_unit)

            if spans_overlap(old_span, new_span, rel_tol=0.05):
                change_pct = 0.0
                change_type = "unchanged"
            else:
                change_pct = compute_percent_change(old_val, new_val)
                if change_pct is None or abs(change_pct) < 0.5:
                    change_type = "unchanged"
                elif change_pct > 0:
                    change_type = "increased"
                else:
                    change_type = "decreased"

            diffs.append(MetricDiff(
                name=old_name,
                old_value=old_val,
                new_value=new_val,
                old_raw=old_raw,
                new_raw=new_raw,
                unit=new_unit or old_unit,
                change_pct=change_pct,
                change_type=change_type
            ))
            break

        if not found:
            diffs.append(MetricDiff(
                name=old_name,
                old_value=old_val,
                new_value=None,
                old_raw=old_raw,
                new_raw="",
                unit=old_unit,
                change_pct=None,
                change_type="removed"
            ))

    # Added metrics
    for new_id, new_m in new_canonical.items():
        if new_id in matched_new_ids:
            continue

        new_name = new_m.get("name", new_id)
        new_raw = str(new_m.get("value", ""))
        new_span = get_metric_value_span(new_m)
        new_val = new_span.get("mid")
        new_unit = new_span.get("unit") or new_m.get("unit", "")

        diffs.append(MetricDiff(
            name=new_name,
            old_value=None,
            new_value=new_val,
            old_raw="",
            new_raw=new_raw,
            unit=new_unit,
            change_pct=None,
            change_type="added"
        ))

    return diffs


# NUMERIC PARSING (DETERMINISTIC)

def parse_to_float(value: Any) -> Optional[float]:
    """
    Deterministically parse any value to float.
    Returns None if unparseable.
    """
    if value is None:
        return None
    if isinstance(value, (int, float)):
        return float(value)
    if not isinstance(value, str):
        return None

    # Clean string
    cleaned = value.strip().upper()
    cleaned = re.sub(r'[,$]', '', cleaned)

    # Handle empty/NA
    if cleaned in ['', 'N/A', 'NA', 'NULL', 'NONE', '-', '—']:
        return None

    # Extract multiplier
    multiplier = 1.0
    if 'TRILLION' in cleaned or cleaned.endswith('T'):
        multiplier = 1_000_000
        cleaned = re.sub(r'T(?:RILLION)?', '', cleaned)
    elif 'BILLION' in cleaned or cleaned.endswith('B'):
        multiplier = 1_000
        cleaned = re.sub(r'B(?:ILLION)?', '', cleaned)
    elif 'MILLION' in cleaned or cleaned.endswith('M'):
        multiplier = 1
        cleaned = re.sub(r'M(?:ILLION)?', '', cleaned)
    elif 'THOUSAND' in cleaned or cleaned.endswith('K'):
        multiplier = 0.001
        cleaned = re.sub(r'K(?:THOUSAND)?', '', cleaned)

    # Handle percentages
    if '%' in cleaned:
        cleaned = cleaned.replace('%', '')
        # Don't apply multiplier to percentages
        multiplier = 1.0

    try:
        return float(cleaned.strip()) * multiplier
    except (ValueError, TypeError):
        return None


def _fmt_currency_first(raw: str, unit: str) -> str:
    """
    Display helper: formats value+unit as currency-first when applicable.

    Examples:
      - raw='29.8', unit='S$B'  -> 'S$29.8B'
      - raw='120',  unit='USD M' -> '$120M'
      - raw='29.8', unit='%'     -> '29.8%'
      - raw='90',   unit='M'     -> '90 M'
    """
    raw = (raw or "").strip()
    unit = (unit or "").strip()

    if not raw or raw == "-":
        return "-"

    # If already currency-first, trust it
    if raw.startswith("S$") or raw.startswith("$"):
        return raw

    # Percent case
    if unit == "%":
        return f"{raw}%"

    # Detect currency from unit
    currency = ""
    scale = unit.replace(" ", "")

    if scale.upper().startswith("SGD"):
        currency = "S$"
        scale = scale[3:]
    elif scale.upper().startswith("USD"):
        currency = "$"
        scale = scale[3:]
    elif scale.startswith("S$"):
        currency = "S$"
        scale = scale[2:]
    elif scale.startswith("$"):
        currency = "$"
        scale = scale[1:]

    # Human-readable units
    if unit.lower().endswith("billion"):
        return f"{currency}{raw} billion".strip()
    if unit.lower().endswith("million"):
        return f"{currency}{raw} million".strip()

    # Compact units (B/M/K)
    if scale.upper() in {"B", "M", "K"}:
        return f"{currency}{raw}{scale}".strip()

    # Fallback
    return f"{currency}{raw} {unit}".strip()
def get_metric_value_span(metric: Dict) -> Dict[str, Any]:
    """
    Return a numeric span for a metric to support range-aware canonical metrics.

    Output:
      {
        "min": float|None,
        "max": float|None,
        "mid": float|None,   # representative value (median if range, else parsed value)
        "unit": str,         # normalized (upper/stripped), preserves %/$ units if present
        "is_range": bool
      }
    """
    if not isinstance(metric, dict):
        return {"min": None, "max": None, "mid": None, "unit": "", "is_range": False}

    unit = (metric.get("unit") or "").strip()

    # If metric already has a range object, prefer it
    r = metric.get("range")
    if isinstance(r, dict):
        vmin = r.get("min")
        vmax = r.get("max")
        # ensure numeric
        try:
            vmin = float(vmin) if vmin is not None else None
        except Exception:
            pass
            vmin = None
        try:
            vmax = float(vmax) if vmax is not None else None
        except Exception:
            pass
            vmax = None

        # Representative = median of candidates if provided, else midpoint of min/max
        candidates = r.get("candidates")
        nums = []
        if isinstance(candidates, list):
            for c in candidates:
                try:
                    nums.append(float(c))
                except Exception:
                    pass
        if nums:
            nums_sorted = sorted(nums)
            mid = nums_sorted[len(nums_sorted) // 2]
        else:
            mid = None
            if vmin is not None and vmax is not None:
                mid = (vmin + vmax) / 2.0
            elif vmin is not None:
                mid = vmin
            elif vmax is not None:
                mid = vmax

        return {
            "min": vmin,
            "max": vmax,
            "mid": mid,
            "unit": unit,
            "is_range": True
        }

    # Non-range metric: parse single numeric value
    val = parse_to_float(metric.get("value"))
    return {
        "min": val,
        "max": val,
        "mid": val,
        "unit": unit,
        "is_range": False
    }


def spans_overlap(a: Dict[str, Any], b: Dict[str, Any], rel_tol: float = 0.05) -> bool:
    """
    Decide whether two spans overlap "enough" to be considered stable.
    rel_tol provides a small widening to avoid false drift from rounding.
    """
    if not a or not b:
        return False
    a_min, a_max = a.get("min"), a.get("max")
    b_min, b_max = b.get("min"), b.get("max")

    if a_min is None or a_max is None or b_min is None or b_max is None:
        return False

    # Widen spans slightly
    a_pad = max(abs(a_max), abs(a_min), 1.0) * rel_tol
    b_pad = max(abs(b_max), abs(b_min), 1.0) * rel_tol

    a_min2, a_max2 = a_min - a_pad, a_max + a_pad
    b_min2, b_max2 = b_min - b_pad, b_max + b_pad

    return not (a_max2 < b_min2 or b_max2 < a_min2)


def compute_percent_change(old_val: Optional[float], new_val: Optional[float]) -> Optional[float]:
    """
    Compute percent change. Returns None if either value is None or old is 0.
    """
    if old_val is None or new_val is None:
        return None
    if old_val == 0:
        return None if new_val == 0 else float('inf')
    return round(((new_val - old_val) / abs(old_val)) * 100, 2)

# NAME MATCHING (DETERMINISTIC)

def normalize_name(name: str) -> str:
    """Normalize name for matching"""
    if not name:
        return ""
    n = name.lower().strip()
    n = re.sub(r'[^\w\s]', '', n)
    n = re.sub(r'\s+', ' ', n)
    return n

def name_similarity(name1: str, name2: str) -> float:
    """Compute similarity ratio between two names (0-1)"""
    n1 = normalize_name(name1)
    n2 = normalize_name(name2)
    if not n1 or not n2:
        return 0.0
    if n1 == n2:
        return 1.0
    # Check containment
    if n1 in n2 or n2 in n1:
        return 0.9
    # Sequence matcher
    return difflib.SequenceMatcher(None, n1, n2).ratio()

def find_best_match(name: str, candidates: List[str], threshold: float = 0.7) -> Optional[str]:
    """Find best matching name from candidates"""
    best_match = None
    best_score = threshold
    for candidate in candidates:
        score = name_similarity(name, candidate)
        if score > best_score:
            best_score = score
            best_match = candidate
    return best_match

# DETERMINISTIC QUERY STRUCTURE EXTRACTION
# - Classify query into a known category (country / industry / etc.)
# - Extract main question + "side questions" deterministically
# - Optional: spaCy dependency parse (if installed)
# - Optional: embedding similarity (if sentence-transformers/sklearn installed)

SIDE_CONNECTOR_PATTERNS = [
    r"\bimpact of\b",
    r"\beffect of\b",
    r"\binfluence of\b",
    r"\brole of\b",
    r"\bdriven by\b",
    r"\bcaused by\b",
    r"\bdue to\b",
    r"\bincluding\b",
    r"\bincluding but not limited to\b",
    r"\bwith a focus on\b",
    r"\bespecially\b",
    r"\bnotably\b",
    r"\bplus\b",
    r"\bas well as\b",
    r"\band also\b",
    r"\bvs\b",
    r"\bversus\b",
]

QUESTION_CATEGORIES = {
    "country": {
        "signals": [
            "gdp", "gdp per capita", "population", "inflation", "interest rate",
            "exports", "imports", "trade balance", "currency", "fx", "central bank",
            "unemployment", "fiscal", "budget", "debt", "sovereign", "country"
        ],
        "template_sections": [
            "GDP & GDP per capita", "Growth rates", "Population & demographics",
            "Key industries", "Exports & imports", "Currency & FX trends",
            "Interest rates & inflation", "Risks & outlook"
        ],
    },
    "industry": {
        "signals": [
            "market size", "tam", "cagr", "industry", "sector", "market",
            "key players", "competitive landscape", "drivers", "challenges",
            "regulation", "technology trends", "forecast"
        ],
        "template_sections": [
            "Total Addressable Market (TAM) / Market size", "Growth rates (CAGR/YoY)",
            "Key players", "Key drivers", "Challenges & risks",
            "Technology trends", "Regulatory / environmental factors", "Outlook"
        ],
    },
    "company": {
        "signals": [
            "revenue", "earnings", "profit", "margins", "guidance",
            "business model", "segments", "customers", "competitors",
            "valuation", "multiple", "pe ratio", "cash flow"
        ],
        "template_sections": [
            "Business overview", "Revenue / profitability", "Segments",
            "Competitive position", "Key risks", "Guidance / outlook"
        ],
    },
    "unknown": {
        "signals": [],
        "template_sections": [],
    }
}

def _normalize_q(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip())

def _cleanup_clause(text: str) -> str:
    t = _normalize_q(text)
    t = re.sub(r"^[,;:\-\s]+", "", t)
    t = re.sub(r"[,;:\-\s]+$", "", t)
    return t

def detect_query_category(query: str) -> Dict[str, Any]:
    """
    Deterministically classify query category using keyword signals.
    Returns: {"category": "...", "confidence": 0-1, "matched_signals": [...]}
    """
    q = (query or "").lower()
    best_cat = "unknown"
    best_hits = 0
    best_matched = []

    for cat, cfg in QUESTION_CATEGORIES.items():
        if cat == "unknown":
            continue
        matched = [s for s in cfg["signals"] if s in q]
        hits = len(matched)
        if hits > best_hits:
            best_hits = hits
            best_cat = cat
            best_matched = matched

    # simple confidence: saturate after ~6 hits
    conf = min(best_hits / 6.0, 1.0) if best_hits > 0 else 0.0
    return {"category": best_cat, "confidence": round(conf, 2), "matched_signals": best_matched[:8]}

# 3A+. LAYERED QUERY STRUCTURE PARSER (Deterministic -> NLP -> Embeddings -> LLM fallback)

_QUERY_SPLIT_PATTERNS = [
    r"\bas well as\b",
    r"\balong with\b",
    r"\bin addition to\b",
    r"\band\b",
    r"\bplus\b",
    r"\bvs\.?\b",
    r"\bversus\b",
    r",",
    r";",
]

_COUNTRY_OVERVIEW_SIGNALS = [
    "in general", "overview", "tell me about", "general", "profile", "facts about",
    "economy", "population", "gdp", "currency", "exports", "imports",
]

def _normalize_q(q: str) -> str:
    q = (q or "").strip()
    q = re.sub(r"\s+", " ", q)
    return q

def _split_clauses_deterministic(q: str) -> List[str]:
    """
    Deterministically split a question into ordered clauses.

    Supports:
    - comma/connector splits (",", "and", "as well as", "in addition to", etc.)
    - multi-side enumerations like:
        "in addition to: 1. X 2. Y"
        "including: (1) X (2) Y"
        "as well as: • X • Y"
    """
    if not isinstance(q, str):
        return []

    s = q.strip()
    if not s:
        return []

    # Normalize whitespace early (keep original casing if present; upstream may lowercase already)
    s = re.sub(r"\s+", " ", s).strip()

    # --- Step A: If there's an enumeration intro, split head vs tail ---
    # Examples: "in addition to:", "including:", "plus:", "as well as:"
    enum_intro = re.search(
        r"\b(in addition to|in addition|including|in addition to the following|as well as|plus)\b\s*:?\s*",
        s,
        flags=re.IGNORECASE,
    )

    head = s
    tail = ""

    if enum_intro:
        # Split at the FIRST occurrence of the enum phrase
        idx = enum_intro.start()
        # head is everything before the phrase if it exists, otherwise keep whole string
        # but we usually want "Tell me about X in general" to remain in head.
        head = s[:idx].strip().rstrip(",")
        tail = s[enum_intro.end():].strip()

        # If head is empty (e.g., query begins with "In addition to:"), treat everything as head
        if not head:
            head = s
            tail = ""

    clauses: List[str] = []

    # --- Step B: Split head using your existing connector patterns ---
    if head:
        parts = [head]
        for pat in _QUERY_SPLIT_PATTERNS:
            next_parts = []
            for p in parts:
                next_parts.extend(re.split(pat, p, flags=re.IGNORECASE))
            parts = next_parts

        for p in parts:
            p = p.strip(" ,;:.").strip()
            if p:
                clauses.append(p)

    # --- Step C: If tail exists, split as enumerated items/bullets ---
    if tail:
        # Split on "1.", "1)", "(1)", "•", "-", "*"
        # Keep it robust: find item starts, then slice.
        item_start = re.compile(r"(?:^|\s)(?:\(?\d+\)?[\.\)]|[•\-\*])\s+", flags=re.IGNORECASE)
        starts = [m.start() for m in item_start.finditer(tail)]

        if starts:
            # Build slices using detected starts
            spans = []
            for i, st0 in enumerate(starts):
                st = st0
                # Move start to the start of token (strip leading whitespace)
                while st < len(tail) and tail[st].isspace():
                    st += 1
                en = starts[i + 1] if i + 1 < len(starts) else len(tail)
                spans.append((st, en))

            for st, en in spans:
                item = tail[st:en].strip(" ,;:.").strip()
                # Remove the leading bullet/number token again (safety)
                item = re.sub(r"^\(?\d+\)?[\.\)]\s+", "", item)
                item = re.sub(r"^[•\-\*]\s+", "", item)
                item = item.strip(" ,;:.").strip()
                if item:
                    clauses.append(item)
        else:
            # If tail doesn't look enumerated, fall back to normal splitter on tail
            parts = [tail]
            for pat in _QUERY_SPLIT_PATTERNS:
                next_parts = []
                for p in parts:
                    next_parts.extend(re.split(pat, p, flags=re.IGNORECASE))
                parts = next_parts

            for p in parts:
                p = p.strip(" ,;:.").strip()
                if p:
                    clauses.append(p)

    # Final cleanup + dedupe while preserving order
    out: List[str] = []
    seen = set()
    for c in clauses:
        c2 = c.strip()
        if not c2:
            continue
        if c2.lower() in seen:
            continue
        seen.add(c2.lower())
        out.append(c2)

    return out


def _dedupe_clauses(clauses: List[str]) -> List[str]:
    seen = set()
    out = []
    for c in clauses:
        c2 = c.strip().lower()
        if not c2 or c2 in seen:
            continue
        seen.add(c2)
        out.append(c.strip())
    return out

def _choose_main_and_side(clauses: List[str]) -> Tuple[str, List[str]]:
    """
    Pick 'main' as the first clause; side = remainder.
    Deterministic, stable across runs.
    """
    clauses = _dedupe_clauses(clauses)
    if not clauses:
        return "", []
    main = clauses[0]
    side = clauses[1:]
    return main, side

def _try_spacy_nlp():
    """
    Optional NLP layer. If spaCy is installed, use it; otherwise return None.
    """
    try:
        import spacy  # type: ignore
        # Avoid heavy model loading; prefer blank model with sentencizer if no model available.
        try:
            nlp = spacy.load("en_core_web_sm")  # common if installed
        except Exception:
            pass
            nlp = spacy.blank("en")
            if "sentencizer" not in nlp.pipe_names:
                nlp.add_pipe("sentencizer")
        return nlp
    except Exception:
        return None

def _nlp_refine_clauses(query: str, clauses: List[str]) -> Dict[str, Any]:
    """
    Use dependency/NER cues to:
      - detect country-overview questions
      - improve main-vs-side decision (coordination / 'as well as' patterns)
    Returns partial overrides: {"main":..., "side":[...], "hints":{...}}
    """
    nlp = _try_spacy_nlp()
    if not nlp:
        return {"hints": {"nlp_used": False}}

    doc = nlp(_normalize_q(query))
    # Named entities that look like places
    gpes = [ent.text for ent in getattr(doc, "ents", []) if ent.label_ in ("GPE", "LOC")]
    gpes_norm = [g.strip() for g in gpes if g and len(g.strip()) > 1]

    # Coordination hint: if query has "as well as" or "and", keep deterministic split,
    # but try to pick the more "general" clause as main when overview signals exist.
    overview_hit = any(sig in (query or "").lower() for sig in _COUNTRY_OVERVIEW_SIGNALS)
    hints = {
        "nlp_used": True,
        "gpe_entities": gpes_norm[:5],
        "overview_signal_hit": bool(overview_hit),
    }

    main, side = _choose_main_and_side(clauses)

    # If overview signals + place entity present, bias main to the overview clause
    if overview_hit and gpes_norm:
        # choose clause with strongest overview signal density
        def score_clause(c: str) -> int:
            c = c.lower()
            return sum(1 for sig in _COUNTRY_OVERVIEW_SIGNALS if sig in c)
        scored = sorted([(score_clause(c), c) for c in clauses], key=lambda x: (-x[0], x[1]))
        if scored and scored[0][0] > 0:
            main = scored[0][1]
            side = [c for c in clauses if c != main]

    return {"main": main, "side": side, "hints": hints}

def _embedding_category_vote(query: str) -> Dict[str, Any]:
    """
    Deterministic 'embedding-like' similarity using TF-IDF (no external model downloads).
    Produces a category suggestion + confidence based on similarity to category descriptors.
    """
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore
        from sklearn.metrics.pairwise import cosine_similarity  # type: ignore
    except Exception:
        return {"category": "unknown", "confidence": 0.0, "method": "tfidf_unavailable"}

    q = _normalize_q(query).lower()
    if not q:
        return {"category": "unknown", "confidence": 0.0, "method": "tfidf_empty"}

    # Build deterministic descriptors from your registry
    cat_texts = []
    cat_names = []
    for cat, cfg in (QUESTION_CATEGORIES or {}).items():
        if not isinstance(cfg, dict) or cat == "unknown":
            continue
        signals = " ".join(cfg.get("signals", [])[:50])
        sections = " ".join((cfg.get("template_sections", []) or [])[:50])
        descriptor = f"{cat} {signals} {sections}".strip()
        if descriptor:
            cat_names.append(cat)
            cat_texts.append(descriptor)

    if not cat_texts:
        return {"category": "unknown", "confidence": 0.0, "method": "tfidf_no_registry"}

    vec = TfidfVectorizer(ngram_range=(1, 2), min_df=1, max_features=8000)
    X = vec.fit_transform(cat_texts + [q])
    sims = cosine_similarity(X[-1], X[:-1]).flatten()

    best_idx = int(sims.argmax()) if sims.size else 0
    best_sim = float(sims[best_idx]) if sims.size else 0.0
    best_cat = cat_names[best_idx] if cat_names else "unknown"

    # Map cosine similarity (~0-1) into a conservative confidence
    conf = max(0.0, min(best_sim / 0.35, 1.0))  # 0.35 sim ~= "high"
    return {"category": best_cat, "confidence": round(conf, 2), "method": "tfidf"}

def _llm_fallback_query_structure(query: str, web_context: Optional[Dict] = None) -> Optional[Dict[str, Any]]:
    """
    Last resort: ask LLM to output ONLY a small JSON query-structure object.
    Guardrail: do NOT let the LLM invent extra side questions unless the user explicitly enumerated them.
    This path must NOT validate against LLMResponse.
    """
    try:
        q = str(query or "").strip()
        if not q:
            return None

        # --- Detect explicit enumeration / list structure in the USER query ---
        # If the user wrote a list (1., 2), bullets, etc.), it's reasonable to accept multiple sides.
        enum_patterns = [
            r"(^|\n)\s*\d+\s*[\.\)]\s+",     # 1.  / 2)
            r"(^|\n)\s*[-•*]\s+",           # - item / • item
            r"(^|\n)\s*[a-zA-Z]\s*[\.\)]\s+"  # a) / b. etc.
        ]
        has_explicit_enumeration = any(re.search(p, q, flags=re.MULTILINE) for p in enum_patterns)

        # Deterministic baseline (what the system already extracted)
        # We use this to clamp LLM hallucinations.
        det_clauses = _split_clauses_deterministic(_normalize_q(q))
        det_main, det_side = _choose_main_and_side(det_clauses)
        det_side = _dedupe_clauses([s.strip() for s in (det_side or []) if isinstance(s, str) and s.strip()])

        prompt = (
            "Extract a query structure.\n"
            "Return ONLY valid JSON with keys:\n"
            "  category: one of [country, industry, company, finance, market, unknown]\n"
            "  category_confidence: number 0-1\n"
            "  main: string (the main question/topic)\n"
            "  side: array of strings (side questions)\n"
            "No extra keys, no commentary.\n\n"
            f"Query: {q}"
        )

        raw = query_perplexity_raw(prompt, max_tokens=250, timeout=30)

        # Parse
        if isinstance(raw, dict):
            parsed = raw
        else:
            if raw is None:
                raw = ""
            if not isinstance(raw, str):
                raw = str(raw)
            parsed = parse_json_safely(raw, "LLM Query Structure")

        if not isinstance(parsed, dict) or parsed.get("main") is None:
            return None

        # --- Clean/normalize fields ---
        llm_main = str(parsed.get("main") or "").strip()
        llm_side = parsed.get("side") if isinstance(parsed.get("side"), list) else []
        llm_side = [str(s).strip() for s in llm_side if s is not None and str(s).strip()]

        # Reject "invented" side items that look like generic outline bullets
        # (Only apply this rejection when the user did NOT explicitly enumerate a list.)
        def _looks_like_outline_item(s: str) -> bool:
            s2 = s.lower().strip()
            bad_starts = (
                "overview", "key", "key stats", "statistics", "major statistics",
                "policies", "infrastructure", "recent trends", "post-covid", "covid",
                "challenges", "opportunities", "drivers", "headwinds",
                "background", "introduction"
            )
            return any(s2.startswith(b) for b in bad_starts)

        # --- Guardrail policy ---
        # If user didn't enumerate, do NOT accept LLM expansion of side questions.
        if not has_explicit_enumeration:
            # Keep deterministic sides only. (You can allow 1 LLM side if deterministic found none.)
            final_side = det_side
            if not final_side and llm_side:
                # Allow at most one side item as a fallback, but avoid outline-like additions.
                cand = llm_side[0]
                final_side = [] if _looks_like_outline_item(cand) else [cand]
        else:
            # User enumerated: accept multiple sides, but still de-dupe and keep deterministic items first
            merged = []
            for s in (det_side + llm_side):
                s = str(s).strip()
                if not s:
                    continue
                if s not in merged:
                    merged.append(s)
            final_side = merged

        # If LLM main is empty or fragment-y, keep deterministic main
        bad_prefixes = ("as well as", "as well", "and ", "also ", "plus ", "as for ")
        if not llm_main or any(llm_main.lower().startswith(p) for p in bad_prefixes):
            llm_main = (det_main or "").strip()

        # Return only allowed keys
        out = {
            "category": parsed.get("category", "unknown") or "unknown",
            "category_confidence": parsed.get("category_confidence", 0.0),
            "main": llm_main,
            "side": final_side,
        }
        return out

    except Exception:
        return None


def _split_side_candidates(query: str) -> List[str]:
    """
    Deterministic splitting into clause candidates.
    We keep it conservative to avoid over-splitting.
    """
    q = _normalize_q(query)
    # Pull quoted strings as strong side-topic candidates
    quoted = re.findall(r"['\"]([^'\"]{2,80})['\"]", q)
    q_wo_quotes = re.sub(r"['\"][^'\"]{2,80}['\"]", " ", q)

    # Split on major separators
    parts = re.split(r"[;]|(?:\s+-\s+)|(?:\s+—\s+)", q_wo_quotes)
    parts = [p for p in (_cleanup_clause(x) for x in parts) if p]

    # Further split on side connectors
    connector_re = "(" + "|".join(SIDE_CONNECTOR_PATTERNS) + ")"
    expanded = []
    for p in parts:
        # break into chunks but keep connector words in-place by splitting into sentences first
        sub = re.split(r"\.\s+|\?\s+|\!\s+", p)
        for s in sub:
            s = _cleanup_clause(s)
            if not s:
                continue
            # if contains connector, split into [before, after...] using first connector
            m = re.search(connector_re, s.lower())
            if m:
                idx = m.start()
                before = _cleanup_clause(s[:idx])
                after = _cleanup_clause(s[idx:])
                if before:
                    expanded.append(before)
                if after:
                    expanded.append(after)
            else:
                expanded.append(s)

    # Add quoted items as standalone candidates (often side topics)
    for qitem in quoted:
        cleaned = _cleanup_clause(qitem)
        if cleaned:
            expanded.append(cleaned)

    # De-dupe while preserving order (deterministic)
    seen = set()
    out = []
    for x in expanded:
        k = x.lower()
        if k not in seen:
            seen.add(k)
            out.append(x)
    return out

def _extract_spacy_side_topics(query: str) -> List[str]:
    """
    Optional: use spaCy dependency parse if available.
    Extracts objects of 'impact/effect/role/influence' patterns.
    """
    try:
        import spacy  # type: ignore
        try:
            nlp = spacy.load("en_core_web_sm")  # type: ignore
        except Exception:
            return []
    except Exception:
        return []

    doc = nlp(query)
    side = []

    triggers = {"impact", "effect", "influence", "role"}
    for token in doc:
        if token.lemma_.lower() in triggers:
            # Look for "of X" attached to trigger
            for child in token.children:
                if child.dep_ == "prep" and child.text.lower() == "of":
                    pobj = next((c for c in child.children if c.dep_ in ("pobj", "dobj", "obj")), None)
                    if pobj is not None:
                        # take subtree as phrase
                        phrase = " ".join(t.text for t in pobj.subtree)
                        phrase = _cleanup_clause(phrase)
                        if phrase and phrase.lower() not in (s.lower() for s in side):
                            side.append(phrase)
    return side[:5]

def _embedding_similarity(a: str, b: str) -> Optional[float]:
    """
    Optional: compute cosine similarity using:
      - sentence-transformers (preferred) OR
      - sklearn TF-IDF fallback
    Returns None if unavailable.
    """
    a = _normalize_q(a)
    b = _normalize_q(b)
    if not a or not b:
        return None

    # 1) sentence-transformers (if installed)
    try:
        from sentence_transformers import SentenceTransformer  # type: ignore
        import numpy as np  # type: ignore
        model = SentenceTransformer("all-MiniLM-L6-v2")
        emb = model.encode([a, b], normalize_embeddings=True)
        sim = float(np.dot(emb[0], emb[1]))
        return max(min(sim, 1.0), -1.0)
    except Exception:
        pass

    # 2) sklearn TF-IDF cosine similarity (deterministic)
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore
        from sklearn.metrics.pairwise import cosine_similarity  # type: ignore
        vec = TfidfVectorizer(stop_words="english")
        X = vec.fit_transform([a, b])
        sim = float(cosine_similarity(X[0], X[1])[0, 0])
        return max(min(sim, 1.0), -1.0)
    except Exception:
        return None

def extract_query_structure(query: str) -> Dict[str, Any]:
    """
    Layered query structure extraction:
      1) Deterministic clause split -> main/side
      2) Deterministic category from keyword signals (detect_query_category)
      3) Optional NLP refinement (spaCy if available)
      4) Deterministic similarity vote (TF-IDF)
      5) LLM fallback ONLY if confidence remains low
    """
    q = _normalize_q(query)
    clauses = _split_clauses_deterministic(q)
    main, side = _choose_main_and_side(clauses)

    # --- Layer 1: deterministic keyword category ---
    det_cat = detect_query_category(q)
    category = det_cat.get("category", "unknown")
    cat_conf = float(det_cat.get("confidence", 0.0))

    debug = {
        "deterministic": {
            "clauses": clauses,
            "main": main,
            "side": side,
            "category": category,
            "confidence": cat_conf,
            "matched_signals": det_cat.get("matched_signals", []),
        }
    }

    # --- Layer 2: NLP refinement (optional) ---
    nlp_out = _nlp_refine_clauses(q, clauses)
    if isinstance(nlp_out, dict):
        hints = nlp_out.get("hints", {})
        debug["nlp"] = hints or {"nlp_used": False}

        # Override main/side if NLP produced them (guard against fragment-y mains)
        nlp_main = (nlp_out.get("main") or "").strip()
        if nlp_main:
            bad_prefixes = ("as well as", "as well", "and ", "also ", "plus ", "as for ")
            if not any(nlp_main.lower().startswith(p) for p in bad_prefixes):
                main = nlp_main

        if isinstance(nlp_out.get("side"), list):
            side = nlp_out["side"]

        # If NLP detects a place + overview cue, bias to "country"
        gpes = (hints or {}).get("gpe_entities", []) if isinstance(hints, dict) else []
        overview_hit = (hints or {}).get("overview_signal_hit", False) if isinstance(hints, dict) else False
        if overview_hit and gpes and cat_conf < 0.45:
            category = "country"
            cat_conf = max(cat_conf, 0.55)

    # --- Layer 3: embedding-style category vote ---
    emb_vote = _embedding_category_vote(q)
    debug["similarity_vote"] = emb_vote

    emb_cat = emb_vote.get("category", "unknown")
    emb_conf = float(emb_vote.get("confidence", 0.0))

    if cat_conf < 0.40 and emb_cat != "unknown" and emb_conf >= 0.45:
        category = emb_cat
        cat_conf = max(cat_conf, min(0.75, emb_conf))

    # --- Layer 4: LLM fallback if still ambiguous ---
    if cat_conf < 0.30:
        llm = _llm_fallback_query_structure(q)
        debug["llm_fallback_used"] = bool(llm)

        if isinstance(llm, dict):
            category = llm.get("category", category) or category
            try:
                cat_conf = float(llm.get("category_confidence", cat_conf))
            except Exception:
                pass

            llm_main = (llm.get("main") or "").strip()
            llm_side = llm.get("side") if isinstance(llm.get("side"), list) else []

            det_main = (main or "").strip()
            det_side = side or []

            def _overview_score(s: str) -> int:
                if not s:
                    return 0
                s2 = s.lower()
                signals = [
                    "in general", "overview", "background", "basic facts",
                    "at a glance", "tell me about", "describe", "introduction"
                ]
                return sum(1 for sig in signals if sig in s2)

            def _is_bad_main(s: str) -> bool:
                if not s or len(s) < 8:
                    return True
                return s.lower().startswith(
                    ("as well as", "as well", "and ", "also ", "plus ", "as for ")
                )

            merged_side = []
            for s in det_side + llm_side:
                s = str(s).strip()
                if s and s not in merged_side:
                    merged_side.append(s)

            det_score = _overview_score(det_main)
            llm_score = _overview_score(llm_main)

            if llm_main and not _is_bad_main(llm_main):
                if not det_main or llm_score > det_score:
                    main = llm_main

            side = merged_side

    side = _dedupe_clauses([s.strip() for s in (side or []) if s.strip()])

    return {
        "category": category or "unknown",
        "category_confidence": round(max(0.0, min(cat_conf, 1.0)), 2),
        "main": (main or "").strip(),
        "side": side,
        "debug": debug,
    }


def format_query_structure_for_prompt(qs: Optional[Dict[str, Any]]) -> str:
    if not qs or not isinstance(qs, dict):
        return ""

    parts = []
    parts.append("STRUCTURED QUESTION (DETERMINISTIC):")
    parts.append(f"- Category: {qs.get('category','unknown')} (conf {qs.get('category_confidence','')})")
    parts.append(f"- Main (answer this FIRST): {qs.get('main','')}")
    side = qs.get("side") or []

    if side:
        parts.append("- Side questions (answer AFTER main, in this exact order):")
        for i, s in enumerate(side[:10], 1):
            parts.append(f"  {i}. {s}")

    tmpl = qs.get("template_sections") or []
    if tmpl:
        parts.append("- Recommended response sections (use as headings if helpful):")
        for t in tmpl[:10]:
            parts.append(f"  - {t}")

    # Hard behavioral instruction to the LLM (kept short and explicit)
    parts.append(
        "RESPONSE RULES:\n"
        "1) Start by answering the MAIN request with general context.\n"
        "2) Then answer EACH side question explicitly (label them).\n"
        "3) Metrics/findings can include both main + side, but do not ignore the main.\n"
        "4) If you provide tourism/industry metrics, ALSO provide basic country/overview facts when main is an overview."
    )

    return "\n".join(parts).strip()


# (Removed in REFACTOR95) legacy compute_metric_diffs() (unused; canonical path remains)


def compute_entity_diffs(old_entities: List, new_entities: List) -> List[EntityDiff]:
    """
    Compute deterministic diffs between entity rankings.
    """
    diffs = []

    # Build lookups with ranks
    old_lookup = {}
    for i, e in enumerate(old_entities):
        if isinstance(e, dict):
            name = normalize_name(e.get('name', ''))
            old_lookup[name] = {
                'rank': i + 1,
                'share': e.get('share'),
                'original_name': e.get('name', '')
            }

    new_lookup = {}
    for i, e in enumerate(new_entities):
        if isinstance(e, dict):
            name = normalize_name(e.get('name', ''))
            new_lookup[name] = {
                'rank': i + 1,
                'share': e.get('share'),
                'original_name': e.get('name', '')
            }

    # All unique names
    all_names = set(old_lookup.keys()) | set(new_lookup.keys())

    for norm_name in all_names:
        old_data = old_lookup.get(norm_name)
        new_data = new_lookup.get(norm_name)

        if old_data and new_data:
            # Entity exists in both
            rank_change = old_data['rank'] - new_data['rank']  # Positive = moved up

            if rank_change > 0:
                change_type = 'moved_up'
            elif rank_change < 0:
                change_type = 'moved_down'
            else:
                change_type = 'unchanged'

            diffs.append(EntityDiff(
                name=new_data['original_name'],
                old_rank=old_data['rank'],
                new_rank=new_data['rank'],
                old_share=old_data['share'],
                new_share=new_data['share'],
                rank_change=rank_change,
                change_type=change_type
            ))
        elif old_data:
            # Entity removed
            diffs.append(EntityDiff(
                name=old_data['original_name'],
                old_rank=old_data['rank'],
                new_rank=None,
                old_share=old_data['share'],
                new_share=None,
                rank_change=None,
                change_type='removed'
            ))
        else:
            # Entity added
            diffs.append(EntityDiff(
                name=new_data['original_name'],
                old_rank=None,
                new_rank=new_data['rank'],
                old_share=None,
                new_share=new_data['share'],
                rank_change=None,
                change_type='added'
            ))

    # Sort by new rank (added entities at end)
    diffs.sort(key=lambda x: x.new_rank if x.new_rank else 999)
    return diffs

# FINDING DIFF COMPUTATION

def compute_finding_diffs(old_findings: List[str], new_findings: List[str]) -> List[FindingDiff]:
    """
    Compute deterministic diffs between findings using text similarity.
    """
    diffs = []
    matched_new_indices = set()

    # Match old findings to new
    for old_f in old_findings:
        if not old_f:
            continue

        best_match_idx = None
        best_similarity = 0.5  # Minimum threshold

        for i, new_f in enumerate(new_findings):
            if i in matched_new_indices or not new_f:
                continue

            sim = name_similarity(old_f, new_f)  # Reuse name similarity for text
            if sim > best_similarity:
                best_similarity = sim
                best_match_idx = i

        if best_match_idx is not None:
            matched_new_indices.add(best_match_idx)
            similarity_pct = round(best_similarity * 100, 1)

            if similarity_pct >= 90:
                change_type = 'retained'
            else:
                change_type = 'modified'

            diffs.append(FindingDiff(
                old_text=old_f,
                new_text=new_findings[best_match_idx],
                similarity=similarity_pct,
                change_type=change_type
            ))
        else:
            # Finding removed
            diffs.append(FindingDiff(
                old_text=old_f,
                new_text=None,
                similarity=0,
                change_type='removed'
            ))

    # Find added findings
    for i, new_f in enumerate(new_findings):
        if i in matched_new_indices or not new_f:
            continue

        diffs.append(FindingDiff(
            old_text=None,
            new_text=new_f,
            similarity=0,
            change_type='added'
        ))

    return diffs

# 8C. DETERMINISTIC SOURCE EXTRACTION
# Extract metrics/entities directly from web snippets - NO LLM

def extract_metrics_from_sources(web_context: Dict) -> Dict:
    """
    Extract numeric metrics directly from web search snippets.
    100% deterministic - no LLM involved.
    """
    extracted = {}
    search_results = web_context.get("search_results", [])

    # Patterns to match common metric formats
    patterns = [
        # Market size patterns
        (r'\$\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)\b', 'market_size'),
        (r'market\s+size[:\s]+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'market_size'),
        (r'valued\s+at\s+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'market_size'),
        (r'worth\s+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'market_size'),

        # Growth rate patterns
        (r'CAGR[:\s]+of?\s*(\d+(?:\.\d+)?)\s*%', 'cagr'),
        (r'(\d+(?:\.\d+)?)\s*%\s*CAGR', 'cagr'),
        (r'grow(?:th|ing)?\s+(?:at\s+)?(\d+(?:\.\d+)?)\s*%', 'growth_rate'),

        # Revenue patterns
        (r'revenue[:\s]+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'revenue'),

        # Year-specific values
        (r'(?:in\s+)?20\d{2}[:\s]+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'year_value'),
    ]

    all_matches = []

    for result in search_results:
        snippet = result.get("snippet", "")
        title = result.get("title", "")
        source = result.get("source", "")
        text = f"{title} {snippet}".lower()

        for pattern, metric_type in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    value_str, unit = match[0], match[1] if len(match) > 1 else ''
                else:
                    value_str, unit = match, ''

                try:
                    value = float(value_str)

                    # Normalize unit
                    unit_lower = unit.lower() if unit else ''
                    if unit_lower in ['t', 'trillion']:
                        unit_normalized = 'T'
                        value_in_billions = value * 1000
                    elif unit_lower in ['b', 'billion']:
                        unit_normalized = 'B'
                        value_in_billions = value
                    elif unit_lower in ['m', 'million']:
                        unit_normalized = 'M'
                        value_in_billions = value / 1000
                    elif unit_lower == '%':
                        unit_normalized = '%'
                        value_in_billions = value  # Keep as-is for percentages
                    else:
                        unit_normalized = ''
                        value_in_billions = value

                    all_matches.append({
                        'type': metric_type,
                        'value': value,
                        'unit': unit_normalized,
                        'value_normalized': value_in_billions,
                        'source': source,
                        'raw': f"{value_str} {unit}".strip()
                    })
                except (ValueError, TypeError):
                    continue

    # Deduplicate and select best matches by type
    metrics_by_type = {}
    for match in all_matches:
        mtype = match['type']
        if mtype not in metrics_by_type:
            metrics_by_type[mtype] = []
        metrics_by_type[mtype].append(match)

    # For each type, take the most common value (mode) or median
    metric_counter = 0
    for mtype, matches in metrics_by_type.items():
        if not matches:
            continue

        # Group by similar values (within 10%)
        value_groups = []
        for m in matches:
            added = False
            for group in value_groups:
                if group and abs(m['value_normalized'] - group[0]['value_normalized']) / max(group[0]['value_normalized'], 0.001) < 0.1:
                    group.append(m)
                    added = True
                    break
            if not added:
                value_groups.append([m])

        # Take the largest group (most consensus)
        if value_groups:
            best_group = max(value_groups, key=len)
            representative = best_group[0]

            metric_counter += 1
            metric_key = f"extracted_{mtype}_{metric_counter}"

            # Map type to readable name
            type_names = {
                'market_size': 'Market Size',
                'cagr': 'CAGR',
                'growth_rate': 'Growth Rate',
                'revenue': 'Revenue',
                'year_value': 'Market Value'
            }

            extracted[metric_key] = {
                'name': type_names.get(mtype, mtype.replace('_', ' ').title()),
                'value': representative['value'],
                'unit': f"${representative['unit']}" if representative['unit'] in ['T', 'B', 'M'] else representative['unit'],
                'source_count': len(best_group),
                'sources': list(set(m['source'] for m in best_group))[:3]
            }

    return extracted


def extract_entities_from_sources(web_context: Dict) -> List[Dict]:
    """
    Extract company/entity names from web search snippets.
    100% deterministic - no LLM involved.
    """
    search_results = web_context.get("search_results", [])

    # Common market leaders that appear in financial contexts
    known_entities = [
        # Tech
        'apple', 'microsoft', 'google', 'alphabet', 'amazon', 'meta', 'facebook',
        'nvidia', 'tesla', 'intel', 'amd', 'qualcomm', 'broadcom', 'cisco',
        'ibm', 'oracle', 'salesforce', 'adobe', 'netflix', 'uber', 'airbnb',
        # Finance
        'jpmorgan', 'goldman sachs', 'morgan stanley', 'bank of america',
        'wells fargo', 'citigroup', 'blackrock', 'vanguard', 'fidelity',
        # Auto
        'toyota', 'volkswagen', 'ford', 'gm', 'general motors', 'honda',
        'bmw', 'mercedes', 'byd', 'nio', 'rivian', 'lucid',
        # Pharma
        'pfizer', 'johnson & johnson', 'roche', 'novartis', 'merck',
        'abbvie', 'eli lilly', 'astrazeneca', 'moderna', 'gilead',
        # Energy
        'exxon', 'chevron', 'shell', 'bp', 'totalenergies', 'conocophillips',
        # Consumer
        'walmart', 'costco', 'home depot', 'nike', 'starbucks', 'mcdonalds',
        'coca-cola', 'pepsi', 'procter & gamble', 'unilever',
        # Regions (for market share by region)
        'north america', 'europe', 'asia pacific', 'asia-pacific', 'apac',
        'china', 'united states', 'japan', 'germany', 'india', 'uk',
        'latin america', 'middle east', 'africa'
    ]

    entity_mentions = {}

    for result in search_results:
        snippet = result.get("snippet", "").lower()
        title = result.get("title", "").lower()
        text = f"{title} {snippet}"

        for entity in known_entities:
            if entity in text:
                # Try to extract market share if mentioned
                share_pattern = rf'{re.escape(entity)}[^.]*?(\d+(?:\.\d+)?)\s*%'
                share_match = re.search(share_pattern, text, re.IGNORECASE)

                share = None
                if share_match:
                    share = f"{share_match.group(1)}%"

                if entity not in entity_mentions:
                    entity_mentions[entity] = {'count': 0, 'shares': []}

                entity_mentions[entity]['count'] += 1
                if share:
                    entity_mentions[entity]['shares'].append(share)

    # Sort by mention count and build list
    sorted_entities = sorted(entity_mentions.items(), key=lambda x: x[1]['count'], reverse=True)

    entities = []
    for entity_name, data in sorted_entities[:10]:  # Top 10
        # Use most common share if available
        share = None
        if data['shares']:
            # Take the most common share value
            share_counts = Counter(data['shares'])
            share = share_counts.most_common(1)[0][0]

        entities.append({
            'name': entity_name.title(),
            'share': share,
            'growth': None,  # Can't reliably extract growth from snippets
            'mention_count': data['count']
        })

    return entities

# STABILITY SCORE COMPUTATION

def compute_stability_score(
    metric_diffs: List[MetricDiff],
    entity_diffs: List[EntityDiff],
    finding_diffs: List[FindingDiff]
) -> float:
    """
    Compute overall stability score (0-100).
    Higher = more stable (less change).
    """
    scores = []

    # Metric stability (40% weight)
    if metric_diffs:
        stable_metrics = sum(1 for m in metric_diffs if m.change_type == 'unchanged')
        small_change = sum(1 for m in metric_diffs if m.change_pct and abs(m.change_pct) < 10)
        metric_score = ((stable_metrics + small_change * 0.5) / len(metric_diffs)) * 100
        scores.append(('metrics', metric_score, 0.4))

    # Entity stability (35% weight)
    if entity_diffs:
        stable_entities = sum(1 for e in entity_diffs if e.change_type == 'unchanged')
        entity_score = (stable_entities / len(entity_diffs)) * 100
        scores.append(('entities', entity_score, 0.35))

    # Finding stability (25% weight)
    if finding_diffs:
        retained = sum(1 for f in finding_diffs if f.change_type in ['retained', 'modified'])
        finding_score = (retained / len(finding_diffs)) * 100
        scores.append(('findings', finding_score, 0.25))

    if not scores:
        return 100.0

    # Weighted average
    total_weight = sum(s[2] for s in scores)
    weighted_sum = sum(s[1] * s[2] for s in scores)
    return round(weighted_sum / total_weight, 1)

# MAIN DIFF COMPUTATION

def compute_evolution_diff(old_analysis: Dict, new_analysis: Dict) -> EvolutionDiff:
    """
    Main entry point: compute complete deterministic diff between two analyses.
    """
    old_response = old_analysis.get('primary_response', {})
    new_response = new_analysis.get('primary_response', {})

    # Timestamps
    old_ts = old_analysis.get('timestamp', '')
    new_ts = new_analysis.get('timestamp', '')

    # Calculate time delta
    time_delta = None
    try:
        old_dt = datetime.fromisoformat(old_ts.replace('Z', '+00:00'))
        new_dt = datetime.fromisoformat(new_ts.replace('Z', '+00:00'))
        time_delta = round((new_dt.replace(tzinfo=None) - old_dt.replace(tzinfo=None)).total_seconds() / 3600, 1)
    except:
        pass

    # Compute diffs using CANONICAL metric registry for stable matching
    metric_diffs = compute_metric_diffs_canonical(
        old_response.get('primary_metrics', {}),
        new_response.get('primary_metrics', {})
    )

    entity_diffs = compute_entity_diffs(
        old_response.get('top_entities', []),
        new_response.get('top_entities', [])
    )

    # Use SEMANTIC finding comparison (stable across wording changes)
    finding_diffs = compute_semantic_finding_diffs(
        old_response.get('key_findings', []),
        new_response.get('key_findings', [])
    )

    # Compute stability
    stability = compute_stability_score(metric_diffs, entity_diffs, finding_diffs)

    # Summary stats
    summary_stats = {
        'metrics_increased': sum(1 for m in metric_diffs if m.change_type == 'increased'),
        'metrics_decreased': sum(1 for m in metric_diffs if m.change_type == 'decreased'),
        'metrics_unchanged': sum(1 for m in metric_diffs if m.change_type == 'unchanged'),
        'metrics_added': sum(1 for m in metric_diffs if m.change_type == 'added'),
        'metrics_removed': sum(1 for m in metric_diffs if m.change_type == 'removed'),
        'entities_moved_up': sum(1 for e in entity_diffs if e.change_type == 'moved_up'),
        'entities_moved_down': sum(1 for e in entity_diffs if e.change_type == 'moved_down'),
        'entities_unchanged': sum(1 for e in entity_diffs if e.change_type == 'unchanged'),
        'entities_added': sum(1 for e in entity_diffs if e.change_type == 'added'),
        'entities_removed': sum(1 for e in entity_diffs if e.change_type == 'removed'),
        'findings_retained': sum(1 for f in finding_diffs if f.change_type == 'retained'),
        'findings_modified': sum(1 for f in finding_diffs if f.change_type == 'modified'),
        'findings_added': sum(1 for f in finding_diffs if f.change_type == 'added'),
        'findings_removed': sum(1 for f in finding_diffs if f.change_type == 'removed'),
    }

    return EvolutionDiff(
        old_timestamp=old_ts,
        new_timestamp=new_ts,
        time_delta_hours=time_delta,
        metric_diffs=metric_diffs,
        entity_diffs=entity_diffs,
        finding_diffs=finding_diffs,
        stability_score=stability,
        summary_stats=summary_stats
    )

# LLM EXPLANATION (ONLY INTERPRETS DIFFS)

def generate_diff_explanation_prompt(diff: EvolutionDiff, query: str) -> str:
    """
    Generate prompt for LLM to EXPLAIN computed diffs (not discover them).
    """
    # Build metric changes text
    metric_changes = []
    for m in diff.metric_diffs:
        if m.change_type == 'increased':
            metric_changes.append(f"- {m.name}: {m.old_raw} → {m.new_raw} ({m.change_pct:+.1f}%) INCREASED")
        elif m.change_type == 'decreased':
            metric_changes.append(f"- {m.name}: {m.old_raw} → {m.new_raw} ({m.change_pct:+.1f}%) DECREASED")
        elif m.change_type == 'added':
            metric_changes.append(f"- {m.name}: NEW metric added with value {m.new_raw}")
        elif m.change_type == 'removed':
            metric_changes.append(f"- {m.name}: REMOVED (was {m.old_raw})")

    # Build entity changes text
    entity_changes = []
    for e in diff.entity_diffs:
        if e.change_type == 'moved_up':
            entity_changes.append(f"- {e.name}: Rank {e.old_rank} → {e.new_rank} (moved UP {e.rank_change} positions)")
        elif e.change_type == 'moved_down':
            entity_changes.append(f"- {e.name}: Rank {e.old_rank} → {e.new_rank} (moved DOWN {abs(e.rank_change)} positions)")
        elif e.change_type == 'added':
            entity_changes.append(f"- {e.name}: NEW entrant at rank {e.new_rank}")
        elif e.change_type == 'removed':
            entity_changes.append(f"- {e.name}: DROPPED OUT (was rank {e.old_rank})")

    # Build findings changes text
    finding_changes = []
    for f in diff.finding_diffs:
        if f.change_type == 'added':
            finding_changes.append(f"- NEW: {f.new_text}")
        elif f.change_type == 'removed':
            finding_changes.append(f"- REMOVED: {f.old_text}")
        elif f.change_type == 'modified':
            finding_changes.append(f"- MODIFIED: '{f.old_text[:50]}...' → '{f.new_text[:50]}...'")

    prompt = f"""You are a market analyst explaining changes between two analysis snapshots.

    QUERY: {query}
    TIME ELAPSED: {diff.time_delta_hours:.1f} hours
    STABILITY SCORE: {diff.stability_score:.0f}%

    COMPUTED METRIC CHANGES:
    {chr(10).join(metric_changes) if metric_changes else "No significant metric changes"}

    COMPUTED ENTITY RANKING CHANGES:
    {chr(10).join(entity_changes) if entity_changes else "No ranking changes"}

    COMPUTED FINDING CHANGES:
    {chr(10).join(finding_changes) if finding_changes else "No finding changes"}

    SUMMARY STATS:
    - Metrics: {diff.summary_stats['metrics_increased']} increased, {diff.summary_stats['metrics_decreased']} decreased, {diff.summary_stats['metrics_unchanged']} unchanged
    - Entities: {diff.summary_stats['entities_moved_up']} moved up, {diff.summary_stats['entities_moved_down']} moved down
    - Findings: {diff.summary_stats['findings_added']} new, {diff.summary_stats['findings_removed']} removed

    YOUR TASK: Provide a 3-5 sentence executive interpretation of these changes.
    - What is the overall trend (improving/declining/stable)?
    - What are the most significant changes and why might they have occurred?
    - What should stakeholders pay attention to?

    Return ONLY a JSON object:
    {{
        "trend": "improving/declining/stable",
        "headline": "One sentence summary of key change",
        "interpretation": "3-5 sentence detailed interpretation",
        "watch_items": ["Item 1 to monitor", "Item 2 to monitor"]
    }}
    """
    return prompt

def get_llm_explanation(diff: EvolutionDiff, query: str) -> Dict:
    """
    Ask LLM to explain the computed diffs (not discover them).
    """
    prompt = generate_diff_explanation_prompt(diff, query)

    headers = {
        "Authorization": f"Bearer {PERPLEXITY_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "sonar",
        "temperature": 0.0,  # Deterministic
        "max_tokens": 500,
        "top_p": 1.0,
        "messages": [{"role": "user", "content": prompt}]
    }

    try:
        resp = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        content = data["choices"][0]["message"]["content"]

        parsed = parse_json_safely(content, "Explanation")
        if parsed:
            return parsed
    except Exception as e:
        st.warning(f"LLM explanation failed: {e}")

    # Fallback
    return {
        "trend": "stable" if diff.stability_score >= 70 else "changing",
        "headline": f"Analysis shows {diff.stability_score:.0f}% stability over {diff.time_delta_hours:.0f} hours",
        "interpretation": "Unable to generate detailed interpretation.",
        "watch_items": []
    }


# 8B. EVOLUTION DASHBOARD RENDERING

def render_evolution_results(diff: EvolutionDiff, explanation: Dict, query: str):
    """Render deterministic evolution results"""

    st.header("📈 Evolution Analysis")
    st.markdown(f"**Query:** {query}")

    # Overview metrics
    col1, col2, col3, col4 = st.columns(4)

    if diff.time_delta_hours:
        if diff.time_delta_hours < 24:
            time_str = f"{diff.time_delta_hours:.1f}h"
        else:
            time_str = f"{diff.time_delta_hours/24:.1f}d"
        col1.metric("Time Elapsed", time_str)
    else:
        col1.metric("Time Elapsed", "Unknown")

    col2.metric("Stability", f"{diff.stability_score:.0f}%")

    trend = explanation.get('trend', 'stable')
    trend_icon = {'improving': '📈', 'declining': '📉', 'stable': '➡️'}.get(trend, '➡️')
    col3.metric("Trend", f"{trend_icon} {trend.title()}")

    # Stability indicator
    if diff.stability_score >= 80:
        col4.success("🟢 Highly Stable")
    elif diff.stability_score >= 60:
        col4.warning("🟡 Moderate Changes")
    else:
        col4.error("🔴 Significant Drift")

    # Headline
    st.info(f"**{explanation.get('headline', 'Analysis complete')}**")

    st.markdown("---")

    # Interpretation

    try:
        # best effort: use schema carried on diff (if any) else global latest schema
        schema = getattr(diff, "metric_schema_frozen", None)
        if not isinstance(schema, dict):
            schema = {}
        _fix39_sanitize_evolutiondiff_object(diff, schema)
    except Exception:
        pass

    st.subheader("📋 Interpretation")
    st.markdown(explanation.get('interpretation', 'No interpretation available'))

    # Watch items
    watch_items = explanation.get('watch_items', [])
    if watch_items:
        st.markdown("**🔔 Watch Items:**")
        for item in watch_items:
            st.markdown(f"- {item}")

    st.markdown("---")

    # Metric Changes (legacy) UI removed in REFACTOR95 to reduce clutter.
    st.markdown("---")
    # Entity Changes Table
    st.subheader("🏢 Entity Ranking Changes")
    if diff.entity_diffs:
        entity_rows = []
        for e in diff.entity_diffs:
            icon = {
                'moved_up': '⬆️', 'moved_down': '⬇️', 'unchanged': '➡️',
                'added': '🆕', 'removed': '❌'
            }.get(e.change_type, '•')

            rank_str = f"{e.rank_change:+d}" if e.rank_change else "-"

            entity_rows.append({
                "": icon,
                "Entity": e.name,
                "Old Rank": e.old_rank or "-",
                "New Rank": e.new_rank or "-",
                "Rank Δ": rank_str,
                "Old Share": e.old_share or "-",
                "New Share": e.new_share or "-"
            })
        st.dataframe(pd.DataFrame(entity_rows), hide_index=True, use_container_width=True)
    else:
        st.info("No entities to compare")

    st.markdown("---")

    # Finding Changes
    st.subheader("🔍 Finding Changes")
    if diff.finding_diffs:
        added = [f for f in diff.finding_diffs if f.change_type == 'added']
        removed = [f for f in diff.finding_diffs if f.change_type == 'removed']
        modified = [f for f in diff.finding_diffs if f.change_type == 'modified']

        if added:
            st.markdown("**🆕 New Findings:**")
            for f in added:
                st.success(f"• {f.new_text}")

        if removed:
            st.markdown("**❌ Removed Findings:**")
            for f in removed:
                st.error(f"• ~~{f.old_text}~~")

        if modified:
            st.markdown("**✏️ Modified Findings:**")
            for f in modified:
                st.warning(f"• {f.new_text} *(similarity: {f.similarity:.0f}%)*")
    else:
        st.info("No findings to compare")

    st.markdown("---")

    # Summary Stats
    st.subheader("📊 Change Summary")
    stats = diff.summary_stats

    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("**Metrics:**")
        st.write(f"📈 {stats['metrics_increased']} increased")
        st.write(f"📉 {stats['metrics_decreased']} decreased")
        st.write(f"➡️ {stats['metrics_unchanged']} unchanged")

    with col2:
        st.markdown("**Entities:**")
        st.write(f"⬆️ {stats['entities_moved_up']} moved up")
        st.write(f"⬇️ {stats['entities_moved_down']} moved down")
        st.write(f"🆕 {stats['entities_added']} new")

    with col3:
        st.markdown("**Findings:**")
        st.write(f"✅ {stats['findings_retained']} retained")
        st.write(f"✏️ {stats['findings_modified']} modified")
        st.write(f"🆕 {stats['findings_added']} new")


# 8D. SOURCE-ANCHORED EVOLUTION
# Re-fetch the SAME sources from previous analysis for true stability
# Enhanced fetch_url_content function to use scrapingdog as fallback

def _extract_pdf_text_from_bytes(pdf_bytes: bytes, max_pages: int = 6, max_chars: int = 7000) -> Optional[str]:
    """
    Extract readable text from PDF bytes deterministically.
    Limits pages/chars for speed and consistent output.
    """
    try:
        reader = PdfReader(io.BytesIO(pdf_bytes))
        texts = []
        for i, page in enumerate(reader.pages[:max_pages]):
            t = page.extract_text() or ""
            t = t.replace("\x00", " ").strip()
            if t:
                texts.append(t)
        joined = "\n".join(texts).strip()
        if len(joined) < 200:
            return None
        return joined[:max_chars]
    except Exception:
        return None


def fetch_url_content(url: str) -> Optional[str]:
    """Fetch content from a specific URL with ScrapingDog fallback"""

    def extract_text(html: str) -> Optional[str]:
        """Extract clean text from HTML"""
        soup = BeautifulSoup(html, 'html.parser')
        for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
            tag.decompose()
        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        clean_text = ' '.join(line for line in lines if line)
        return clean_text[:5000] if len(clean_text) > 200 else None

    # Try 1: Direct request
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        }
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()

        if 'captcha' not in resp.text.lower():
            content = extract_text(resp.text)
            if content:
                return content
    except:
        pass

    # Try 2: ScrapingDog API
    if SCRAPINGDOG_KEY:
        try:
            api_url = "https://api.scrapingdog.com/scrape"
            params = {"api_key": SCRAPINGDOG_KEY, "url": url, "dynamic": "false"}
            resp = requests.get(api_url, params=params, timeout=30)
            if resp.status_code == 200:
                content = extract_text(resp.text)
                if content:
                    return content
        except:
            pass

    return None


def fetch_url_content_with_status(url: str, timeout: int = 25, force_pdf: bool = False):
    """
    Fetch URL content and return (text, status_detail).

    status_detail:
      - "success"
      - "success_pdf"
      - "http_<code>"
      - "exception:<TypeName>"
      - "empty"
      - "success_scrapingdog"

    Hardened:
      - Uses browser-like headers for direct fetch
      - Falls back to ScrapingDog when blocked/empty and SCRAPINGDOG_KEY is available
      - Avoids returning binary garbage as "text"
    """
    import re
    import requests

    def _normalize_url(s: str) -> str:
        t = (s or "").strip()
        if not t:
            return ""
        if re.match(r"^https?://", t, flags=re.I):
            return t
        if re.match(r"^[a-z0-9.-]+\.[a-z]{2,}(/.*)?$", t, flags=re.I):
            return "https://" + t
        return ""

    url = _normalize_url(url)
    if not url:
        return None, "empty"

    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
    }

    try:
        resp = requests.get(url, timeout=timeout, headers=headers, allow_redirects=True)

        ct = (resp.headers.get("content-type", "") or "").lower()

        if resp.status_code >= 400:
            # If blocked, try ScrapingDog fallback (optional)
            if resp.status_code in (401, 403, 429) and globals().get("SCRAPINGDOG_KEY"):
                txt = _fetch_via_scrapingdog(url, timeout=timeout)
                if txt and txt.strip():
                    return txt, "success_scrapingdog"
            return None, f"http_{resp.status_code}"

        # PDF handling
        if force_pdf or "application/pdf" in ct or url.lower().endswith(".pdf"):
            try:
                import io
                try:
                    import pdfplumber  # type: ignore
                except (ModuleNotFoundError, ImportError):
                    return None, "skipped:pdf_unsupported_missing_dependency"

                with pdfplumber.open(io.BytesIO(resp.content)) as pdf:
                    out = []
                    total_pages = int(len(pdf.pages) if getattr(pdf, 'pages', None) is not None else 0)

                    # REFACTOR92: sample across long PDFs and extract tables (not only front matter)
                    # - Extract first N pages + an evenly-spaced spread across the remainder.
                    # - Add bounded table extraction to surface values hidden in table layouts (e.g., 'USD bn').
                    first_n = 10
                    target_total = 50

                    idxs = set(range(min(first_n, total_pages)))
                    remaining_budget = max(0, min(target_total - len(idxs), total_pages - len(idxs)))

                    if remaining_budget > 0 and total_pages > first_n:
                        start_idx = first_n
                        end_idx = max(first_n, total_pages - 1)
                        denom = max(1, remaining_budget - 1)
                        for s in range(remaining_budget):
                            try:
                                idx = int(round(start_idx + (end_idx - start_idx) * (s / denom)))
                            except Exception:
                                idx = start_idx
                            if 0 <= idx < total_pages:
                                idxs.add(idx)

                    nonempty_pages = 0
                    tables_rows_total = 0

                    for pi in sorted(idxs):
                        try:
                            page = pdf.pages[pi]

                            # 1) Plain text extraction
                            t = page.extract_text() or ''
                            if isinstance(t, str) and t.strip():
                                out.append(t)
                                nonempty_pages += 1

                            # 2) Table extraction (bounded)
                            try:
                                tables = page.extract_tables() or []
                            except Exception:
                                tables = []
                            if isinstance(tables, list) and tables:
                                for tbl in tables[:2]:
                                    if not isinstance(tbl, list):
                                        continue
                                    for row in (tbl[:25] if len(tbl) > 25 else tbl):
                                        if not isinstance(row, list):
                                            continue
                                        cells = []
                                        for c in row[:10]:
                                            try:
                                                cs = (str(c) if c is not None else '').strip()
                                            except Exception:
                                                cs = ''
                                            if cs:
                                                cells.append(cs)
                                        if cells:
                                            out.append(' | '.join(cells))
                                            tables_rows_total += 1
                        except Exception:
                            continue

                    text = '\n'.join(out).strip()
                    if not text:
                        return None, 'empty'
                    return text, f"success_pdf_pages={len(idxs)}/{total_pages};nonempty={nonempty_pages};tables={tables_rows_total}"
            except Exception as e:
                return None, f"exception:{type(e).__name__}"


        # Text/HTML
        text = resp.text or ""
        # REFACTOR120: normalize HTML to visible text so numeric extraction can work on HTML-heavy pages
        try:
            _rf120_raw = text or ""
            _rf120_head = (_rf120_raw[:2000] or "").lower()
            _rf120_looks_html = bool(
                ("<html" in _rf120_head) or ("<body" in _rf120_head) or
                re.search(r"(?is)<(div|span|p|article|main|section|header|footer|nav|script|style)\b", _rf120_raw[:2000] or "")
            )
            if _rf120_looks_html:
                _rf120_txt = ""
                try:
                    from bs4 import BeautifulSoup  # type: ignore
                    soup = BeautifulSoup(_rf120_raw, "html.parser")
                    for tag in soup(["script", "style", "nav", "footer", "header", "aside", "noscript", "form"]):
                        try:
                            tag.decompose()
                        except Exception:
                            pass
                    _rf120_txt = soup.get_text(separator="\n")
                except Exception:
                    _rf120_txt = re.sub(r"(?is)<(script|style|noscript).*?>.*?</\1>", " ", _rf120_raw or "")
                    _rf120_txt = re.sub(r"(?is)<[^>]+>", " ", _rf120_txt or "")
                try:
                    _rf120_lines = [ln.strip() for ln in (_rf120_txt or "").splitlines() if ln.strip()]
                    _rf120_clean = "\n".join(_rf120_lines)
                    _rf120_clean = re.sub(r"\n{3,}", "\n\n", _rf120_clean).strip()
                except Exception:
                    _rf120_clean = (_rf120_txt or "").strip()
                if isinstance(_rf120_clean, str) and len(_rf120_clean) >= 200:
                    text = _rf120_clean
        except Exception:
            pass

        # If empty or suspiciously short, attempt ScrapingDog (optional)
        if (not text.strip() or len(text.strip()) < 300) and globals().get("SCRAPINGDOG_KEY"):
            txt = _fetch_via_scrapingdog(url, timeout=timeout)
            if txt and txt.strip():
                return txt, "success_scrapingdog"

        if not text.strip():
            return None, "empty"

        return text, "success"

    except Exception as e:
        # ScrapingDog as last resort for network-y issues
        try:
            if globals().get("SCRAPINGDOG_KEY"):
                txt = _fetch_via_scrapingdog(url, timeout=timeout)
                if txt and txt.strip():
                    return txt, "success_scrapingdog"
        except Exception:
            return None, f"exception:{type(e).__name__}"


def _fetch_via_scrapingdog(url: str, timeout: int = 25) -> str:
    """
    Internal helper used by fetch_url_content_with_status.
    Returns raw HTML text from ScrapingDog (or "" on failure).
    """
    import requests

    key = globals().get("SCRAPINGDOG_KEY")
    if not key:
        return ""

    params = {"api_key": key, "url": url, "dynamic": "false"}
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        resp = requests.get("https://api.scrapingdog.com/scrape", params=params, headers=headers, timeout=timeout)
        if resp.status_code >= 400:
            return ""
        return resp.text or ""
    except Exception:
        return ""

def get_extractor_fingerprint() -> str:
    """
    Bump this string whenever you change extraction or normalization behavior.
    Used to decide whether cached extracted_numbers are still valid.
    """
    return "extract_v4_pdf_tables_forcepdf_2026-01-29"


def extract_numbers_from_text(text: str) -> List[Dict]:
    """
    Backward-compatible wrapper.

    v7_34 tightening:
    - Delegate to extract_numbers_with_context() so junk suppression is applied consistently.
    """
    try:
        return extract_numbers_with_context(text or "", source_url="", max_results=600) or []
    except Exception:
        return []


def _parse_iso_dt(ts: Optional[str]) -> Optional[datetime]:
    if not ts:
        return None
    try:
        ts2 = ts.replace("Z", "+00:00")
        dt = datetime.fromisoformat(ts2)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception:
        return None

def now_utc() -> datetime:
    """Timezone-aware UTC now (prevents naive/aware datetime bugs)."""
    return datetime.now(timezone.utc)


def _yureeka_now_iso_utc() -> str:
    """UTC ISO-8601 timestamp with offset (e.g., 2026-01-23T11:13:15.665069+00:00)."""
    try:
        return now_utc().isoformat()
    except Exception:
        try:
            from datetime import datetime, timezone
            return datetime.now(timezone.utc).isoformat()
        except Exception:
            return ""


# REFACTOR26: Centralized source_url attribution helpers (schema-preserving)
# - Goal: ensure row-level injection gating can reliably attribute a current metric
#   to its source URL (production vs injected), even when source_url lives only
#   inside evidence/provenance structures.

def _refactor26_extract_metric_source_url_v1(_m: dict):
    """Best-effort extraction of a metric's source URL without changing schema."""
    if not isinstance(_m, dict):
        return None
    # Direct fields
    for k in ("source_url", "url", "source", "sourceURL", "sourceUrl"):
        try:
            v = _m.get(k)
            if isinstance(v, str) and v.strip():
                return v.strip()
        except Exception:
            pass

    # Evidence list
    try:
        ev = _m.get("evidence")
        if isinstance(ev, list):
            for e in ev:
                if isinstance(e, dict):
                    for k in ("source_url", "url"):
                        v = e.get(k)
                        if isinstance(v, str) and v.strip():
                            return v.strip()
    except Exception:
        pass

    # Winner/debug/provenance structures (common in this codebase)
    try:
        wd = _m.get("winner_candidate_debug")
        if isinstance(wd, dict):
            v = wd.get("source_url") or wd.get("url")
            if isinstance(v, str) and v.strip():
                return v.strip()
    except Exception:
        pass

    try:
        prov = _m.get("provenance") or _m.get("provenance_v1") or _m.get("diag")
        if isinstance(prov, dict):
            # Direct provenance URL
            v = prov.get("source_url") or prov.get("url")
            if isinstance(v, str) and v.strip():
                return v.strip()

            # Common nested winner shape: provenance.best_candidate.source_url
            bc = prov.get("best_candidate") or prov.get("best_candidate_v1") or prov.get("winner") or prov.get("best")
            if isinstance(bc, dict):
                v2 = bc.get("source_url") or bc.get("url")
                if isinstance(v2, str) and v2.strip():
                    return v2.strip()

            # Sometimes stored as list of candidates
            cands = prov.get("candidates") or prov.get("top_candidates") or prov.get("candidates_v1")
            if isinstance(cands, list):
                for c in cands:
                    if isinstance(c, dict):
                        v3 = c.get("source_url") or c.get("url")
                        if isinstance(v3, str) and v3.strip():
                            return v3.strip()
    except Exception:
        pass

    return None


def _refactor26_hydrate_primary_metrics_canonical_source_urls_v1(_pmc: dict) -> dict:
    """In-place: ensure pmc[*].source_url exists when discoverable from evidence."""
    if not isinstance(_pmc, dict):
        return _pmc
    for _ck, _m in list(_pmc.items()):
        if not isinstance(_m, dict):
            continue
        try:
            su = _m.get("source_url")
            if isinstance(su, str) and su.strip():
                continue
            su2 = _refactor26_extract_metric_source_url_v1(_m)
            if isinstance(su2, str) and su2.strip():
                _m["source_url"] = su2.strip()
        except Exception:
            pass
    return _pmc


def _refactor26_extract_row_current_source_url_v1(_row: dict):
    """Prefer row-attributed *current* URL fields before any fallbacks."""
    if not isinstance(_row, dict):
        return None
    for k in ("cur_source_url", "current_source_url", "current_source", "current_source_url_effective", "current_source_effective"):
        try:
            v = _row.get(k)
            if isinstance(v, str) and v.strip():
                return v.strip()
        except Exception:
            pass
    # As a last resort, some rows store the current URL in source_url (ambiguous)
    try:
        v = _row.get("source_url")
        if isinstance(v, str) and v.strip():
            return v.strip()
    except Exception:
        pass
    return None


def _refactor26_norm_url_for_compare_v1(_u: str):
    """Normalize URL for set-membership comparisons using existing normalizer when available."""
    if not isinstance(_u, str):
        return None
    u = _u.strip()
    if not u:
        return None
    try:
        fn = globals().get("_inj_diag_norm_url_list")
        if callable(fn):
            out = fn([u])
            if isinstance(out, list) and out and isinstance(out[0], str) and out[0].strip():
                return out[0].strip()
    except Exception:
        pass
    return u
def _yureeka_humanize_seconds_v1(delta_seconds) -> str:
    """Compact human format for a positive second delta (e.g., '1m 18s')."""
    try:
        if delta_seconds is None:
            return ""
        ds = float(delta_seconds)
        if ds < 0:
            ds = 0.0
        total = int(round(ds))
        mins, secs = divmod(total, 60)
        hrs, mins = divmod(mins, 60)
        days, hrs = divmod(hrs, 24)
        parts = []
        if days:
            parts.append(f"{days}d")
        if hrs:
            parts.append(f"{hrs}h")
        if mins:
            parts.append(f"{mins}m")
        parts.append(f"{secs}s")
        return " ".join(parts)
    except Exception:
        return ""


def _normalize_number_to_parse_base(value: float, unit: str) -> float:
    u = (unit or "").strip().upper()
    if u == "T":
        return value * 1_000_000
    if u == "B":
        return value * 1_000
    if u == "M":
        return value * 1
    if u == "K":
        return value * 0.001
    if u == "%":
        return value
    return value


def _refactor13_get_metric_change_rows_v1(out: dict):
    """
    Prefer V2 rows if available; fallback to legacy metric_changes.
    Returned list is safe (always list).
    """
    try:
        if isinstance(out, dict):
            rows = out.get("metric_changes_v2")
            if isinstance(rows, list) and rows:
                return rows
            rows = out.get("metric_changes")
            if isinstance(rows, list):
                return rows
    except Exception:
        pass
    return []


def _refactor13_recompute_summary_and_stability_v1(out: dict) -> None:
    """
    REFACTOR13: Make results.summary + stability_score reflect canonical-first diff rows.

    - summary.metrics_increased / decreased / unchanged are derived from change_type.
    - stability_score is computed from comparable rows:
        1) discrete score: unchanged + 0.5 * small_change(<10%)/N
        2) fallback when discrete would be 0: max(0, 100 - mean_abs_pct_change)
    """
    if not isinstance(out, dict):
        return

    rows = _refactor13_get_metric_change_rows_v1(out)

    # Count change types
    increased = decreased = unchanged = added = removed = 0
    for r in rows:
        try:
            ct = (r.get("change_type") if isinstance(r, dict) else None) or ""
            if ct == "increased":
                increased += 1
            elif ct == "decreased":
                decreased += 1
            elif ct == "unchanged":
                unchanged += 1
            elif ct in ("added", "missing_baseline", "new_metric"):
                added += 1
            elif ct in ("removed", "missing_current"):
                removed += 1
        except Exception:
            pass

    total = len(rows)

    # Update summary (authoritative for UI)
    try:
        s = out.setdefault("summary", {})
        if isinstance(s, dict):
            s["total_metrics"] = total
            # In our canonical-first world, "found" = row count (includes added/removed)
            s["metrics_found"] = total
            s["metrics_increased"] = increased
            s["metrics_decreased"] = decreased
            s["metrics_unchanged"] = unchanged
            # Preserve backward compatibility: do not remove existing keys
            s.setdefault("metrics_added", added)
            s.setdefault("metrics_removed", removed)
    except Exception:
        pass

    # Compute stability from comparable rows
    # NOTE: We treat "small change" as <10% only for *changed* rows (increased/decreased),
    # so unchanged rows are not double-counted (prevents >100% stability).
    comparable = []
    for r in rows:
        if not isinstance(r, dict):
            continue
        ct = r.get("change_type")
        if ct not in ("increased", "decreased", "unchanged"):
            continue
        # Prefer explicit comparability signal
        if r.get("baseline_is_comparable") is False:
            continue
        if r.get("unit_mismatch") is True:
            continue
        # Require numeric pct (or at least numeric norms)
        cp = r.get("change_pct")
        if isinstance(cp, (int, float)):
            comparable.append((float(cp), ct))
        else:
            # fallback if norms are numeric: compute pct safely
            pv = r.get("prev_value_norm")
            cv = r.get("cur_value_norm")
            try:
                if isinstance(pv, (int, float)) and isinstance(cv, (int, float)) and abs(float(pv)) > 1e-12:
                    comparable.append((((float(cv) - float(pv)) / float(pv)) * 100.0, ct))
            except Exception:
                pass

    stability = 100.0
    method = "no_comparable_rows"
    n = len(comparable)

    # REFACTOR51: track graded stats so stability remains meaningful with extreme deltas
    mean_abs_pct_raw = None
    mean_abs_pct_capped = None
    mean_abs_cap_used = None

    if n > 0:
        # Discrete stability:
        #   stability = (unchanged + 0.5 * small_change_changed_rows) / comparable_n * 100
        stable = 0
        small = 0
        abs_vals = []
        for cp, ct in comparable:
            try:
                a = abs(float(cp))
                abs_vals.append(a)
                if ct == "unchanged":
                    stable += 1
                elif ct in ("increased", "decreased") and a < 10.0:
                    small += 1
            except Exception:
                pass

        discrete = ((stable + (small * 0.5)) / float(max(1, n))) * 100.0
        if discrete > 0.0:
            # Clamp for safety (should already be <=100 with the counting rules above)
            stability = max(0.0, min(100.0, discrete))
            method = "discrete_unchanged_smallchange"
        else:
            # Graded fallback:
            #   Use a per-row cap to avoid a single extreme outlier driving mean_abs>=100 -> 0% stability.
            #   stability = 100 - mean(min(abs_pct, 100))  (clamped [0,100])
            if abs_vals:
                try:
                    mean_abs_pct_raw = sum(abs_vals) / float(len(abs_vals))
                except Exception:
                    mean_abs_pct_raw = None
                try:
                    mean_abs_cap_used = 100.0
                    mean_abs_pct_capped = sum((a if a <= mean_abs_cap_used else mean_abs_cap_used) for a in abs_vals) / float(len(abs_vals))
                except Exception:
                    mean_abs_pct_capped = None
                if isinstance(mean_abs_pct_capped, (int, float)):
                    stability = max(0.0, min(100.0, 100.0 - float(mean_abs_pct_capped)))
                    method = "graded_mean_abs_pct_capped"
                elif isinstance(mean_abs_pct_raw, (int, float)):
                    # last-resort: clamp the mean itself (previous behavior)
                    stability = max(0.0, 100.0 - min(100.0, float(mean_abs_pct_raw)))
                    method = "graded_mean_abs_pct"
                else:
                    stability = 0.0
                    method = "no_pct_values"
            else:
                stability = 0.0
                method = "no_pct_values"

    try:
        out["stability_score"] = round(float(stability), 1)
    except Exception:
        pass

    # Mirror into diff_panel_v2_summary for auditability
    try:
        dbg = out.setdefault("debug", {})
        if isinstance(dbg, dict):
            v2s = dbg.get("diff_panel_v2_summary")
            if isinstance(v2s, dict):
                v2s.setdefault("metrics_increased", increased)
                v2s.setdefault("metrics_decreased", decreased)
                v2s.setdefault("metrics_unchanged", unchanged)
                v2s.setdefault("metrics_added", added)
                v2s.setdefault("metrics_removed", removed)
                v2s.setdefault("stability_score_v1", round(float(stability), 1))
                v2s.setdefault("stability_method_v1", method)
                v2s.setdefault("stability_comparable_n_v1", n)

            dbg["refactor13_summary_stability_v1"] = {
                "rows_total": total,
                "comparable_n": n,
                "metrics_increased": increased,
                "metrics_decreased": decreased,
                "metrics_unchanged": unchanged,
                "metrics_added": added,
                "metrics_removed": removed,
                "stability_score": round(float(stability), 1),
                "stability_method": method,
            }
            _r13 = dbg.get("refactor13_summary_stability_v1")
            if isinstance(_r13, dict) and mean_abs_pct_raw is not None:
                try:
                    _r13["mean_abs_pct_raw"] = round(float(mean_abs_pct_raw), 2)
                except Exception:
                    pass
            if isinstance(_r13, dict) and mean_abs_pct_capped is not None:
                try:
                    _r13["mean_abs_pct_capped"] = round(float(mean_abs_pct_capped), 2)
                    _r13["mean_abs_pct_cap_used"] = mean_abs_cap_used
                except Exception:
                    pass
    except Exception:
        pass

def _truncate_json_safely_for_sheets(json_str: str, max_chars: int = 45000) -> str:
    """
# =====================================================================
# PATCH FIX41G (ADDITIVE): Normalize web_context and capture force_rebuild
# Ensures the UI flag reaches the fastpath gate and is recorded in output.
# =====================================================================
if web_context is None or not isinstance(web_context, dict):
    web_context = {}
_fix41_force_rebuild_seen = False
try:
    _fix41_force_rebuild_seen = bool(web_context.get("force_rebuild"))
except Exception:
    pass
    _fix41_force_rebuild_seen = False
# =====================================================================

    PATCH TS1 (ADDITIVE): JSON-safe truncation wrapper
    - Ensures json.loads always succeeds for any returned value.
    - Stores a preview when oversized.
    """
    import json

    s = "" if json_str is None else str(json_str)
    if len(s) <= max_chars:
        return s

    preview_len = max(0, int(max_chars) - 700)
    wrapper = {
        "_sheets_safe": True,
        "_sheet_write": {
            "truncated": True,
            "mode": "json_wrapper",
            "note": "Payload exceeded cell limit; stored preview only.",
        },
        "preview": s[:preview_len],
    }
    try:
        return json.dumps(wrapper, ensure_ascii=False, default=str)
    except Exception:
        return '{"_sheets_safe":true,"_sheet_write":{"truncated":true,"mode":"json_wrapper","note":"json.dumps failed"}}'


def _truncate_for_sheets(s: str, max_chars: int = 45000) -> str:
    """Hard cap to stay under Google Sheets 50k/cell limit."""
    if s is None:
        return ""
    s = str(s)
    if len(s) <= max_chars:
        return s
    head = s[: int(max_chars * 0.75)]
    tail = s[- int(max_chars * 0.20):]
    return head + "\n...\n[TRUNCATED FOR GOOGLE SHEETS]\n...\n" + tail


def _summarize_heavy_fields_for_sheets(obj: dict) -> dict:
    """
    Summarize fields that commonly exceed the per-cell limit while keeping debug utility.
    Only used for Sheets serialization; does NOT modify your in-memory analysis dict.
    """
    if not isinstance(obj, dict):
        return {"_type": str(type(obj)), "value": str(obj)[:500]}

    out = dict(obj)

    # Common bloat fields
    if "scraped_meta" in out:
        sm = out.get("scraped_meta")
        if isinstance(sm, dict):
            compact = {}
            for url, meta in list(sm.items())[:12]:
                if isinstance(meta, dict):
                    compact[url] = {
                        "status": meta.get("status"),
                        "status_detail": meta.get("status_detail"),
                        "numbers_found": meta.get("numbers_found"),
                        "fingerprint": meta.get("fingerprint"),
                        "clean_text_len": meta.get("clean_text_len"),
                    }
            out["scraped_meta"] = {"_summary": True, "count": len(sm), "sample": compact}
        else:
            out["scraped_meta"] = {"_summary": True, "type": str(type(sm))}

    for big_key in ("source_results", "baseline_sources_cache", "baseline_sources_cache_compact"):
        if big_key in out:
            sr = out.get(big_key)
            if isinstance(sr, list):
                sample = []
                for item in sr[:2]:
                    if isinstance(item, dict):
                        item2 = dict(item)
                        if isinstance(item2.get("extracted_numbers"), list):
                            item2["extracted_numbers"] = {"_summary": True, "count": len(item2["extracted_numbers"])}
                        sample.append(item2)
                out[big_key] = {"_summary": True, "count": len(sr), "sample": sample}
            else:
                out[big_key] = {"_summary": True, "type": str(type(sr))}

    # If you store full scraped_content anywhere, summarize it too
    if "scraped_content" in out:
        sc = out.get("scraped_content")
        if isinstance(sc, dict):
            out["scraped_content"] = {"_summary": True, "count": len(sc), "keys_sample": list(sc.keys())[:10]}
        else:
            out["scraped_content"] = {"_summary": True, "type": str(type(sc))}

    # Why:
    # - Your biggest payload is typically results.baseline_sources_cache (full snapshots)
    # - The previous summarizer only handled top-level keys, so Sheets payload still exceeded limits
    # - This keeps the saved JSON smaller AND keeps json.loads(get_history) working reliably
    try:
        r = out.get("results")
        if isinstance(r, dict):
            r2 = dict(r)

            for big_key in ("baseline_sources_cache", "source_results"):
                if big_key in r2:
                    sr = r2.get(big_key)
                    if isinstance(sr, list):
                        sample = []
                        for item in sr[:2]:
                            if isinstance(item, dict):
                                item2 = dict(item)
                                if isinstance(item2.get("extracted_numbers"), list):
                                    item2["extracted_numbers"] = {
                                        "_summary": True,
                                        "count": len(item2["extracted_numbers"])
                                    }
                                sample.append(item2)
                        r2[big_key] = {"_summary": True, "count": len(sr), "sample": sample}
                    else:
                        r2[big_key] = {"_summary": True, "type": str(type(sr))}

            out["results"] = r2
    except Exception:
        pass

    return out


def make_sheet_safe_json(obj: dict, max_chars: int = 45000) -> str:
    """
    Serialize sheet-safe JSON under the cell limit.

    NOTE / CONFLICT:
      - The prior implementation used _truncate_for_sheets() on the JSON string, which can produce
        invalid JSON (cut mid-string). Invalid JSON rows are skipped by get_history() (json.loads fails),
        so evolution can't pick them up.
      - This patch preserves summarization but replaces raw string truncation with a JSON wrapper
        that is ALWAYS valid JSON.

    Output behavior:
      - If JSON fits: returns full compact JSON string.
      - If too large: returns a valid JSON wrapper with a preview + metadata.
    """
    import json

    # Keep existing behavior: summarize heavy fields
    compact = _summarize_heavy_fields_for_sheets(obj if isinstance(obj, dict) else {"value": obj})
    if isinstance(compact, dict):
        compact["_sheets_safe"] = True

    # Try to serialize
    try:
        s = json.dumps(compact, ensure_ascii=False, default=str)
    except Exception:
        pass
        # ultra-safe fallback (still return valid JSON)
        try:
            s = json.dumps({"_sheets_safe": True, "_sheet_write": {"error": "json.dumps failed"}}, ensure_ascii=False)
        except Exception:
            return '{"_sheets_safe":true,"_sheet_write":{"error":"json.dumps failed"}}'

    # If it fits, return as-is
    if isinstance(s, str) and len(s) <= int(max_chars or 45000):
        return s

    # - Never return mid-string truncations that break json.loads in get_history().
    try:
        preview_len = max(0, int(max_chars or 45000) - 700)  # leave room for wrapper fields
        wrapper = {
            "_sheets_safe": True,
            "_sheet_write": {
                "truncated": True,
                "mode": "sheets_safe_wrapper",
                "note": "Payload exceeded cell limit; stored preview only. Full snapshots must be stored separately if needed.",
            },
            # Keep a preview for UI/debugging
            "preview": s[:preview_len],
        }

        # Optional: carry minimal identity fields for convenience (additive)
        if isinstance(obj, dict):
            wrapper["question"] = (obj.get("question") or "")[:200]
            wrapper["timestamp"] = obj.get("timestamp")
            wrapper["code_version"] = obj.get("code_version") or (obj.get("primary_response") or {}).get("code_version")

            # Carry snapshot pointers even when the payload is wrapped.
            # Without these fields, evolution cannot rehydrate full snapshots
            # from the Snapshots worksheet (or local fallback) and will fail
            # the snapshot gate with "No valid snapshots".
            try:
                _ssh = obj.get("source_snapshot_hash") or (obj.get("results") or {}).get("source_snapshot_hash")
                _ref = obj.get("snapshot_store_ref") or (obj.get("results") or {}).get("snapshot_store_ref")
                if _ssh:
                    wrapper["source_snapshot_hash"] = _ssh
                if _ref:
                    wrapper["snapshot_store_ref"] = _ref
            except Exception:
                return json.dumps(wrapper, ensure_ascii=False, default=str)
    except Exception:
        return '{"_sheets_safe":true,"_sheet_write":{"truncated":true,"mode":"sheets_safe_wrapper","note":"wrapper failed"}}'


# Purpose:
#   - Store full baseline_sources_cache outside Google Sheets when rows
#     are too large (Sheets wrapper / preview mode).
#   - Allow deterministic rehydration for evolution (no refetch).
def _snapshot_store_dir() -> str:
    import os
    d = os.path.join(os.getcwd(), "snapshot_store")
    try:
        os.makedirs(d, exist_ok=True)
    except Exception:
        pass
    return d

def store_full_snapshots_local(baseline_sources_cache: list, source_snapshot_hash: str) -> str:
    """
    Store full snapshots deterministically by hash. Returns a store ref string (path).
    Additive-only helper.
    """
    import os, json
    if not source_snapshot_hash:
        return ""
    if not isinstance(baseline_sources_cache, list) or not baseline_sources_cache:
        return ""

    path = ""
    try:
        _d = _snapshot_store_dir() or os.path.join(os.getcwd(), "snapshot_store")
        path = os.path.join(_d, f"{source_snapshot_hash}.json")
    except Exception:
        return ""
    try:
        # write-once semantics (deterministic)
        if os.path.exists(path) and os.path.getsize(path) > 0:
            return path
    except Exception:
        pass

    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(baseline_sources_cache, f, ensure_ascii=False, default=str)
        return path
    except Exception:
        return ""

def load_full_snapshots_local(snapshot_store_ref: str) -> list:
    """
    Load full snapshots from a store ref string (path). Returns [] if not available.
    """
    import json, os
    try:
        if not snapshot_store_ref or not isinstance(snapshot_store_ref, str):
            return []
        if not os.path.exists(snapshot_store_ref):
            return []
        with open(snapshot_store_ref, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data if isinstance(data, list) else []
    except Exception:
        return []

def compute_source_snapshot_hash(baseline_sources_cache: list) -> str:
    import hashlib
    pairs = []
    for sr in (baseline_sources_cache or []):
        if not isinstance(sr, dict):
            continue
        u = (sr.get("source_url") or sr.get("url") or "").strip()
        fp = (sr.get("fingerprint") or sr.get("content_fingerprint") or "").strip()
        if u:
            pairs.append((u, fp))
    pairs.sort()
    sig = "|".join([f"{u}#{fp}" for (u, fp) in pairs])
    return hashlib.sha256(sig.encode("utf-8")).hexdigest() if sig else ""
# Why:
# - Sheets-safe summarization may replace baseline_sources_cache/extracted_numbers
#   with summary dicts. However, evidence_records often remains available and is
#   already deterministic, snapshot-derived data.
# - This helper reconstructs the minimal snapshot shape needed for
#   source-anchored evolution WITHOUT re-fetching or heuristic matching.

# - Keeps v1 compute_source_snapshot_hash() for backward compatibility.
# - v2 includes url + status + fingerprint + (anchor_hash,value_norm,unit_tag) tuples (bounded) for stronger identity.
def compute_source_snapshot_hash_v2(baseline_sources_cache: list, max_items_per_source: int = 120) -> str:
    import hashlib
    import json

    try:
        sources = baseline_sources_cache if isinstance(baseline_sources_cache, list) else []
        parts = []
        for s in sources:
            if not isinstance(s, dict):
                continue
            url = str(s.get("url") or "")
            status = str(s.get("status") or "")
            status_detail = str(s.get("status_detail") or "")
            fingerprint = str(s.get("fingerprint") or "")

            nums = s.get("extracted_numbers") or s.get("numbers") or []
            # Sometimes stored in summarized form
            if isinstance(nums, dict) and nums.get("_summary") and isinstance(nums.get("count"), int):
                # no details available; just use summary
                num_tuples = [("summary_count", int(nums.get("count")))]
            else:
                num_list = nums if isinstance(nums, list) else []
                num_tuples = []
                for n in num_list[: int(max_items_per_source or 120)]:
                    if not isinstance(n, dict):
                        continue
                    ah = str(n.get("anchor_hash") or "")
                    vn = n.get("value_norm")
                    ut = str(n.get("unit_tag") or n.get("unit") or "")
                    # Use JSON for float stability + None handling
                    num_tuples.append((ah, vn, ut))
                # Deterministic order
                num_tuples = sorted(num_tuples, key=lambda t: (t[0], str(t[1]), t[2]))

            parts.append({
                "url": url,
                "status": status,
                "status_detail": status_detail,
                "fingerprint": fingerprint,
                "nums": num_tuples,
            })

        # Deterministic ordering of sources
        parts = sorted(parts, key=lambda d: (d.get("url",""), d.get("fingerprint",""), d.get("status","")))

        payload = json.dumps(parts, ensure_ascii=False, default=str, separators=(",", ":"))
        return hashlib.sha256(payload.encode("utf-8")).hexdigest()
    except Exception:
        pass
        # Ultra-safe fallback (still deterministic-ish)
        try:
            return hashlib.sha256(str(baseline_sources_cache).encode("utf-8")).hexdigest()
        except Exception:
            return "0"*64

def build_baseline_sources_cache_from_evidence_records(evidence_records):

    """

    Rebuild a minimal baseline_sources_cache from evidence_records deterministically.


    PATCH AI3 (ADDITIVE): anchor integrity

    - Ensures each candidate has anchor_hash + candidate_id (derived if missing)

    - Preserves analysis-aligned numeric normalization fields when present

    - Deterministic ordering by (source_url, fingerprint)

    """

    import hashlib


    if not isinstance(evidence_records, list) or not evidence_records:

        return []


    def _sha1(s: str) -> str:

        try:

            return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()

        except Exception:
            return ""


    def _ensure_anchor_fields(c: dict, source_url: str = "") -> dict:

        c = c if isinstance(c, dict) else {}

        ctx = c.get("context_snippet") or c.get("context") or ""

        if isinstance(ctx, str):

            ctx = ctx.strip()[:240]

        else:

            ctx = ""

        raw = c.get("raw")

        if raw is None:

            v = c.get("value_norm") if c.get("value_norm") is not None else c.get("value")

            u = c.get("base_unit") or c.get("unit") or ""

            raw = f"{v}{u}"

        raw = str(raw)[:120]


        ah = c.get("anchor_hash") or c.get("anchor") or ""

        if not ah:

            ah = _sha1(f"{source_url}|{raw}|{ctx}")

            if ah:

                c["anchor_hash"] = ah

        if not c.get("candidate_id") and ah:

            c["candidate_id"] = str(ah)[:16]

        if source_url and not c.get("source_url"):

            c["source_url"] = source_url

        if ctx and not c.get("context_snippet"):

            c["context_snippet"] = ctx

        return c


    by_url = {}

    for rec in evidence_records:

        if not isinstance(rec, dict):

            continue

        url = rec.get("source_url") or rec.get("url") or ""

        fp = rec.get("fingerprint") or ""

        # candidates may be stored under candidates or extracted_numbers depending on producer

        cand_list = rec.get("candidates")

        if not isinstance(cand_list, list):

            cand_list = rec.get("extracted_numbers")

        if not isinstance(cand_list, list):

            cand_list = []


        out_cands = []

        for c in cand_list:

            if not isinstance(c, dict):

                continue

            cc = _ensure_anchor_fields(dict(c), url)

            out_cands.append(cc)


        if not out_cands:

            continue


        key = (str(url), str(fp))

        by_url.setdefault(key, {"source_url": url, "fingerprint": fp, "extracted_numbers": []})

        by_url[key]["extracted_numbers"].extend(out_cands)


    rebuilt = list(by_url.values())


    # deterministic sort & per-source deterministic candidate order

    try:

        rebuilt.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))

        for s in rebuilt:

            if isinstance(s, dict) and isinstance(s.get("extracted_numbers"), list):

                s["extracted_numbers"] = sorted(

                    s["extracted_numbers"],

                    key=lambda c: (

                        str(c.get("anchor_hash") or ""),

                        str(c.get("candidate_id") or ""),

                        str(c.get("raw") or ""),

                        str(c.get("unit") or ""),

                    )

                )

    except Exception:

        pass


    try:
        if isinstance(rebuilt, dict):
            rebuilt.setdefault("_fix41afc5_debug", {})
            if isinstance(rebuilt.get("_fix41afc5_debug"), dict):
                rebuilt["_fix41afc5_debug"].update(dict(_fix41afc5_dbg2))
    except Exception:
        pass

    return rebuilt
def _sheets_now_ts():
    import time
    return time.time()

def _sheets_cache_get(key: str):
    try:
        item = _SHEETS_READ_CACHE.get(key)
        if not item:
            return None
        ts, val = item
        if (_sheets_now_ts() - ts) > _SHEETS_READ_CACHE_TTL_SEC:
            return None
        return val
    except Exception:
        return None

def _sheets_cache_set(key: str, val):
    try:
        _SHEETS_READ_CACHE[key] = (_sheets_now_ts(), val)
    except Exception:
        pass

def _is_sheets_rate_limit_error(err: Exception) -> bool:
    s = ""
    try:
        s = str(err) or ""
    except Exception:
        pass
        s = ""
    # Common markers seen via gspread/googleapiclient:
    markers = ["RESOURCE_EXHAUSTED", "Quota exceeded", "RATE_LIMIT_EXCEEDED", "429"]
    return any(m in s for m in markers)

def sheets_get_all_values_cached(ws, cache_key: str):
    """
    Cached wrapper for ws.get_all_values() with rate-limit fallback.
    cache_key should be stable for the worksheet (e.g., 'Snapshots', 'HistoryFull', 'History').
    """
    global _SHEETS_LAST_READ_ERROR
    key = f"get_all_values:{cache_key}"
    cached = _sheets_cache_get(key)
    if cached is not None:
        return cached
    try:
        # Previous draft accidentally recursed into itself and referenced an undefined variable.
        # This is a direct execution conflict fix (no behavior change intended beyond correctness).
        values = ws.get_all_values() if ws else []
        _sheets_cache_set(key, values)
        return values
    except Exception as e:
        _SHEETS_LAST_READ_ERROR = str(e)
        # Rate-limit fallback: return last cached value if we have one, else empty list
        if _is_sheets_rate_limit_error(e):
            stale = _SHEETS_READ_CACHE.get(key)
            if stale and isinstance(stale, tuple) and len(stale) == 2:
                return stale[1]
            return []
        raise

# Purpose:
#   - Persist full baseline_sources_cache inside the same Spreadsheet
#     but in a dedicated worksheet (tab), chunked across rows.
#   - Enables deterministic rehydration for evolution without refetch.
# Notes:
#   - Write-once semantics by source_snapshot_hash.
#   - Chunking and reassembly are deterministic (part_index ordering).
def get_google_spreadsheet():
    """Connect to Google Spreadsheet (cached connection if available)."""
    try:
        # If get_google_sheet() exists and already opened the spreadsheet as sheet.sheet1,
        # we re-open to obtain the Spreadsheet handle (additive; avoids refactoring).
        import streamlit as st
        from google.oauth2.service_account import Credentials
        import gspread

        SCOPES = [
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive",
        ]
        creds = Credentials.from_service_account_info(
            dict(st.secrets["gcp_service_account"]),
            scopes=_coerce_google_oauth_scopes(SCOPES)
        )
        client = gspread.authorize(creds)
        spreadsheet_name = st.secrets.get("google_sheets", {}).get("spreadsheet_name", "Yureeka_JSON")
        return client.open(spreadsheet_name)
    except Exception:
        return None

def _ensure_snapshot_worksheet(spreadsheet, title: str = "Snapshots"):
    """Ensure a worksheet tab exists for snapshot storage."""
    try:
        if not spreadsheet:
            return None
        try:
            ws = spreadsheet.worksheet(title)
            return ws
        except Exception:
            pass
            # Create with a reasonable default size; Sheets can expand.
            ws = spreadsheet.add_worksheet(title=title, rows=2000, cols=8)
            try:
                ws.append_row(
                    ["source_snapshot_hash", "part_index", "total_parts", "payload_part", "created_at", "code_version", "fingerprints_sig", "sha256"],
                    value_input_option="RAW",
                )
            except Exception:
                return ws
    except Exception:
        return None

def store_full_snapshots_to_sheet(baseline_sources_cache: list, source_snapshot_hash: str, worksheet_title: str = "Snapshots", chunk_chars: int = 20000) -> str:
    """
    Store full snapshots to a dedicated worksheet tab in chunked rows.
    Returns a ref string like: 'gsheet:Snapshots:<hash>'

    REFACTOR40 (BUGFIX):
    - Previously, the "write-once" gate used ws.findall(hash) and would treat ANY existing rows
      as "already written". If a prior write partially failed (rate-limit / quota / transient),
      we could end up with an incomplete snapshot stored under the hash forever, and subsequent
      runs would never repair it.
    - Now:
      * If rows exist, we first validate that the snapshot is actually loadable.
      * If not loadable, we attempt a repair write (append a fresh batch keyed by created_at).
      * After successful writes, we invalidate the worksheet read cache so recent snapshots
        are immediately retrievable.
    """
    import json, hashlib, zlib, base64, zlib, base64
    if not source_snapshot_hash:
        return ""
    if not isinstance(baseline_sources_cache, list) or not baseline_sources_cache:
        return ""

    try:
        ss = get_google_spreadsheet()
        ws = _ensure_snapshot_worksheet(ss, worksheet_title) if ss else None
        if not ws:
            return ""

        # If any rows exist for this hash, only short-circuit if it is actually loadable.
        try:
            existing = ws.findall(source_snapshot_hash)
            if existing:
                try:
                    _probe = load_full_snapshots_from_sheet(source_snapshot_hash, worksheet_title=worksheet_title)
                    if isinstance(_probe, list) and _probe:
                        return f"gsheet:{worksheet_title}:{source_snapshot_hash}"
                except Exception:
                    pass
        except Exception:
            pass

        payload = json.dumps(baseline_sources_cache, ensure_ascii=False, default=str)
        # reduce write volume / API calls (helps avoid rate limits).
        # Storage format:
        #   payload_part begins with 'zlib64:' then base64(zlib(json_bytes))
        # Backward compatible: loader detects/decompresses when prefix present.
        try:
            if isinstance(payload, str) and len(payload) > 120000:
                _raw = payload.encode("utf-8", errors="strict")
                _comp = zlib.compress(_raw, level=9)
                _b64 = base64.b64encode(_comp).decode("ascii")
                payload = "zlib64:" + _b64
        except Exception:
            pass

        sha = hashlib.sha256(payload.encode("utf-8")).hexdigest()

        # deterministic chunking
        chunk_size = max(1000, int(chunk_chars or 45000))
        parts = [payload[i:i+chunk_size] for i in range(0, len(payload), chunk_size)]
        total = len(parts)

        # Optional fingerprints signature (stable)
        pairs = []
        for sr in baseline_sources_cache:
            if isinstance(sr, dict):
                u = (sr.get("source_url") or sr.get("url") or "").strip()
                fp = (sr.get("fingerprint") or sr.get("content_fingerprint") or "").strip()
                if u:
                    pairs.append((u, fp))
        pairs.sort()
        fingerprints_sig = "|".join([f"{u}#{fp}" for (u, fp) in pairs]) if pairs else ""

        created_at = _yureeka_now_iso_utc()

        # best-effort: use global if exists
        code_version = ""
        try:
            code_version = globals().get("CODE_VERSION") or ""
        except Exception:
            code_version = ""

        rows = []
        for idx, part in enumerate(parts):
            rows.append([source_snapshot_hash, idx, total, part, created_at, code_version, fingerprints_sig, sha])

        wrote_all = False
        try:
            # Append in small batches to reduce API payload size / rate-limit failures.
            batch_size = 10
            import time
            _need_throttle = (len(rows) > 60)
            wrote = 0
            for i in range(0, len(rows), batch_size):
                chunk = rows[i:i+batch_size]
                ws.append_rows(chunk, value_input_option="RAW")
                wrote += len(chunk)
                try:
                    if _need_throttle:
                        time.sleep(0.15)
                except Exception:
                    pass
            wrote_all = (wrote == len(rows))
        except Exception:
            # Fall back to append_row loop; do NOT early-return on the first failure.
            success = 0
            for r in rows:
                try:
                    ws.append_row(r, value_input_option="RAW")
                    success += 1
                except Exception:
                    pass
            wrote_all = (success == len(rows))

        # If we believe we wrote all rows, invalidate snapshot read cache so we can re-load immediately.
        if wrote_all:
            try:
                # This cache key format matches sheets_get_all_values_cached()
                _cache_key = f"get_all_values:{worksheet_title}"
                _cache = globals().get("_SHEETS_READ_CACHE")
                if isinstance(_cache, dict):
                    _cache.pop(_cache_key, None)
            except Exception:
                pass
            return f"gsheet:{worksheet_title}:{source_snapshot_hash}"

        # Partial write: return empty ref to avoid pointing to a broken snapshot.
        return ""
    except Exception:
        return ""

def load_full_snapshots_from_sheet(source_snapshot_hash: str, worksheet_title: str = "Snapshots") -> list:
    """Load and reassemble full snapshots list from a dedicated worksheet.

    REFACTOR40 (BUGFIX):
    - Fix stale cache behavior: if the requested hash is not found in cached values, do a direct read once.
    - Fix partial-write repair behavior: if multiple write batches exist for the same hash, select the
      most recent *complete* batch (grouped by created_at), not a mixed/partial merge.
    """
    import json, hashlib
    if not source_snapshot_hash:
        return []
    try:
        ss = get_google_spreadsheet()
        ws = ss.worksheet(worksheet_title) if ss else None
        if not ws:
            return []

        def _read_cached():
            try:
                return sheets_get_all_values_cached(ws, cache_key=worksheet_title)
            except Exception:
                return []

        def _read_direct():
            try:
                return ws.get_all_values()
            except Exception:
                return []

        values = _read_cached()

        # If empty/too small, do one direct read to bypass stale empty cache.
        if not values or len(values) < 2:
            values = _read_direct()
            if not values or len(values) < 2:
                return []
            # Best-effort cache refresh
            try:
                _sheets_cache_set(f"get_all_values:{worksheet_title}", values)
            except Exception:
                pass

        header = values[0] or []
        # Expect at least: source_snapshot_hash, part_index, total_parts, payload_part
        try:
            col_h = header.index("source_snapshot_hash")
            col_i = header.index("part_index")
            col_t = header.index("total_parts")
            col_p = header.index("payload_part")
            col_ca = header.index("created_at") if "created_at" in header else None
            col_sha = header.index("sha256") if "sha256" in header else None
        except Exception:
            # If headers are missing/misaligned, bail safely
            return []

        def _safe_int(x):
            try:
                return int(x)
            except Exception:
                return 0

        def _parse_iso(s: str):
            try:
                # Lexicographic order works for ISO8601 UTC strings, but parse for safety.
                from datetime import datetime
                return datetime.fromisoformat(str(s).replace("Z", "+00:00"))
            except Exception:
                return None

        def _extract_best(values_table):
            rows = []
            for r in values_table[1:]:
                try:
                    if len(r) > col_h and r[col_h] == source_snapshot_hash:
                        rows.append(r)
                except Exception:
                    continue
            if not rows:
                return []

            # Group by created_at (per-write batch). If missing, fall back to a single group.
            groups = {}
            for r in rows:
                k = ""
                try:
                    if col_ca is not None and len(r) > col_ca:
                        k = str(r[col_ca] or "")
                except Exception:
                    k = ""
                if not k:
                    k = "legacy"
                groups.setdefault(k, []).append(r)

            candidates = []
            for created_at, grows in groups.items():
                try:
                    # Determine expected total parts
                    expected_total = 0
                    try:
                        if grows and len(grows[0]) > col_t:
                            expected_total = _safe_int(grows[0][col_t])
                    except Exception:
                        expected_total = 0
                    if expected_total <= 0:
                        continue

                    # Build part map (dedupe by part_index; keep longest payload_part)
                    part_map = {}
                    for rr in grows:
                        try:
                            pi = _safe_int(rr[col_i] if len(rr) > col_i else 0)
                            pp = rr[col_p] if len(rr) > col_p else ""
                            if pi not in part_map or (isinstance(pp, str) and len(pp) > len(part_map.get(pi, ""))):
                                part_map[pi] = pp or ""
                        except Exception:
                            continue

                    # Completeness check: must have all indices 0..expected_total-1
                    if len(part_map) < expected_total:
                        continue
                    missing = False
                    payload_parts = []
                    for i in range(expected_total):
                        if i not in part_map:
                            missing = True
                            break
                        payload_parts.append(part_map[i] or "")
                    if missing:
                        continue

                    payload = "".join(payload_parts)

                    # Optional integrity check
                    try:
                        if col_sha is not None:
                            exp = ""
                            try:
                                exp = (grows[0][col_sha] if len(grows[0]) > col_sha else "") or ""
                            except Exception:
                                exp = ""
                            if exp:
                                actual = hashlib.sha256(payload.encode("utf-8")).hexdigest()
                                if actual != exp:
                                    continue
                    except Exception:
                        pass
                    # JSON decode (supports REFACTOR42 compressed payloads)
                    try:
                        payload_json = payload
                        try:
                            # REFACTOR43 (BUGFIX): transparently decode 'zlib64:' compressed payloads.
                            if isinstance(payload_json, str) and payload_json.startswith("zlib64:"):
                                import base64, zlib
                                b64 = payload_json.split(":", 1)[1] if ":" in payload_json else ""
                                if not b64:
                                    continue
                                comp = base64.b64decode(b64.encode("ascii"), validate=False)
                                raw = zlib.decompress(comp)
                                payload_json = raw.decode("utf-8", errors="strict")
                        except Exception:
                            # If decoding fails, treat as invalid snapshot batch
                            continue

                        data = json.loads(payload_json)
                        if not isinstance(data, list) or not data:
                            continue
                    except Exception:
                        continue
                    # Candidate score: prefer latest created_at when parseable; else fallback to string.
                    dt = _parse_iso(created_at) if created_at and created_at != "legacy" else None
                    candidates.append((dt, created_at, data))
                except Exception:
                    continue

            if not candidates:
                return []
            # Choose best candidate: latest datetime if available else latest created_at string.
            candidates.sort(key=lambda x: (x[0] is not None, x[0] or x[1]), reverse=True)
            return candidates[0][2]

        best = _extract_best(values)

        # If the hash is missing or incomplete in cached values, do ONE direct refresh and retry.
        if not best:
            direct = _read_direct()
            if direct and len(direct) >= 2:
                try:
                    best = _extract_best(direct)
                except Exception:
                    best = []
                # refresh cache best-effort
                try:
                    _sheets_cache_set(f"get_all_values:{worksheet_title}", direct)
                except Exception:
                    pass

        return best if isinstance(best, list) else []
    except Exception:
        return []

# Why:
# - Evolution may receive a sheets-safe wrapper that omits primary_response,
#   metric_schema_frozen, metric_anchors, etc.
# - When wrapper includes full_store_ref ("gsheet:HistoryFull:<analysis_id>"),
#   we can deterministically load the full analysis payload (no re-fetch).
# Notes:
# - Additive only. Safe no-op if sheet/tab not present.


def write_full_history_payload_to_sheet(analysis_id: str, payload: str, worksheet_title: str = "HistoryFull", chunk_size: int = 20000) -> bool:
    """Write a full analysis payload (string) into HistoryFull as chunked rows keyed by analysis_id.

    REFACTOR112:
      - Ensure HistoryFull headers match the 7-column schema:
          analysis_id, part_index, total_parts, payload_part, created_at, code_version, sha256
      - Backward compatible: if sheet is legacy 5-col, we still write 5-col rows.
      - Stores sha256 for integrity (full stitched payload).
    """
    import hashlib
    import datetime as _dt
    if not analysis_id or not payload:
        return False

    # Derive created_at + code_version from payload when possible (best-effort)
    created_at_iso = ""
    code_version = ""
    try:
        obj = json.loads(payload)
        if isinstance(obj, dict):
            created_at_iso = str(obj.get("timestamp") or "")
            if not created_at_iso:
                r = obj.get("results") if isinstance(obj.get("results"), dict) else {}
                created_at_iso = str(r.get("timestamp") or "")
            code_version = str(obj.get("code_version") or "")
            if not code_version:
                r = obj.get("results") if isinstance(obj.get("results"), dict) else {}
                code_version = str(r.get("code_version") or "")
    except Exception:
        pass

    if not created_at_iso:
        try:
            created_at_iso = _dt.datetime.now(_dt.timezone.utc).isoformat()
        except Exception:
            created_at_iso = ""

    if not code_version:
        try:
            code_version = str(globals().get("CODE_VERSION") or globals().get("_YUREEKA_CODE_VERSION_LOCK") or "")
        except Exception:
            code_version = ""

    try:
        ss = get_google_spreadsheet()
        if not ss:
            return False
        try:
            ws = ss.worksheet(worksheet_title)
        except Exception:
            # Create sheet if missing (best-effort)
            try:
                ws = ss.add_worksheet(title=worksheet_title, rows=2000, cols=10)
            except Exception:
                ws = ss.worksheet(worksheet_title)

        # Ensure headers exist (prefer 7-col schema)
        headers = []
        try:
            headers = ws.row_values(1) or []
        except Exception:
            headers = []

        want_7 = False
        try:
            if headers and headers[0] == "analysis_id":
                if len(headers) >= 7 and ("created_at" in headers or "code_version" in headers):
                    want_7 = True
                elif len(headers) >= 7:
                    # header length already 7+; assume 7-col layout
                    want_7 = True
        except Exception:
            pass

        # Upgrade headers to 7-col schema if missing/legacy
        try:
            if (not headers) or (len(headers) < 7) or (headers[0] != "analysis_id") or ("created_at" not in headers) or ("code_version" not in headers):
                _ = ws.update('A1:G1', [["analysis_id", "part_index", "total_parts", "payload_part", "created_at", "code_version", "sha256"]])
                want_7 = True
        except Exception:
            # If header upgrade fails, fall back to legacy 5-col schema
            want_7 = False
            try:
                if (not headers) or (len(headers) < 5) or (headers and headers[0] != "analysis_id"):
                    _ = ws.update('A1:E1', [["analysis_id", "part_index", "total_parts", "payload_part", "sha256"]])
            except Exception:
                pass

        sha = hashlib.sha256(payload.encode("utf-8", errors="ignore")).hexdigest()
        parts = [payload[i:i + chunk_size] for i in range(0, len(payload), chunk_size)]
        total = len(parts) if parts else 0
        if total == 0:
            return False

        rows = []
        for i, part in enumerate(parts):
            if want_7:
                rows.append([analysis_id, str(i), str(total), part, created_at_iso, code_version, sha])
            else:
                rows.append([analysis_id, str(i), str(total), part, sha])

        # Append in one batch if possible
        try:
            ws.append_rows(rows, value_input_option="RAW")
        except Exception:
            # Fallback to per-row append
            for r in rows:
                try:
                    ws.append_row(r, value_input_option="RAW")
                except Exception:
                    return False

        # Invalidate HistoryFull cache after successful write so immediate rehydration sees new rows.
        try:
            _cache = globals().get("_SHEETS_READ_CACHE")
            if isinstance(_cache, dict):
                _cache.pop(f"get_all_values:{worksheet_title}", None)
        except Exception:
            pass

        # Mark HistoryFull as dirty so loader bypasses cached reads once (same-session baseline freshness).
        try:
            import time as _time
            st.session_state["_historyfull_dirty_v1"] = float(_time.time())
            st.session_state["_historyfull_dirty_reason_v1"] = "write_full_history_payload_to_sheet"
        except Exception:
            pass

        return True
    except Exception:
        return False


def load_full_history_payload_from_sheet(analysis_id: str, worksheet_title: str = "HistoryFull") -> dict:
    """
    Load the full analysis JSON payload from the HistoryFull worksheet.

    PATCH HF_LOAD_V2 (ADDITIVE):
    - Supports payloads split across multiple rows (chunked writes).
    - Supports current HistoryFull headers:
        analysis_id, part_index, total_parts, payload_json, created_at, code_version, sha256
    - Backward compatible with older variants:
        id, part_index, total_parts, payload_part / data, sha256
    - Deterministic stitching (sort by part_index) + safe JSON parse.

    PATCH HF_LOAD_V3 (ADDITIVE):
    - Verifies chunk completeness when total_parts is available (0..total_parts-1).
    - Optionally verifies sha256 when present (stitched string).
    - Does not change failure mode: still returns {} on any failure.
    """
    try:
        ss = get_google_spreadsheet()
        if not ss:
            return {}

        try:
            ws = ss.worksheet(worksheet_title)
        except Exception:
            return {}

        # Read all rows (prefer cached getter if present)
        rows = []
        _r110_used_direct = False

        # REFACTOR110: after a HistoryFull write, bypass cached reads once to avoid stale/empty cache.
        try:
            _dirty = st.session_state.get("_historyfull_dirty_v1")
        except Exception:
            _dirty = None
        try:
            import time as _time
            if isinstance(_dirty, (int, float)) and (_time.time() - float(_dirty) < 120):
                rows = ws.get_all_values() or []
                _r110_used_direct = True
                try:
                    _sheets_cache_set(f"get_all_values:{worksheet_title}", rows)
                except Exception:
                    pass
                try:
                    st.session_state.pop("_historyfull_dirty_v1", None)
                    st.session_state.pop("_historyfull_dirty_reason_v1", None)
                except Exception:
                    pass
        except Exception:
            pass

        if not _r110_used_direct:
            try:
                fn = globals().get("sheets_get_all_values_cached")
                rows = fn(ws, cache_key=worksheet_title) if callable(fn) else (ws.get_all_values() or [])
            except Exception:
                pass
                try:
                    rows = ws.get_all_values() or []
                except Exception:
                    return {}

        # REFACTOR110: if cached read returned empty/short, do ONE direct refresh and retry.
        if (not rows) or (len(rows) < 2):
            try:
                direct = ws.get_all_values() or []
                if direct and len(direct) >= 2:
                    rows = direct
                    try:
                        _sheets_cache_set(f"get_all_values:{worksheet_title}", rows)
                    except Exception:
                        pass
            except Exception:
                return {}

        if not rows or len(rows) < 2:
            return {}

        header = rows[0] or []
        body = rows[1:] or []

        def _col(name: str):
            try:
                return header.index(name)
            except Exception:
                return None

        c_id = _col("analysis_id")
        if c_id is None:
            c_id = _col("id")
        if c_id is None:
            c_id = 0  # last-ditch fallback

        c_part = _col("part_index")
        c_total = _col("total_parts")

        # IMPORTANT: your sheet uses payload_json
        c_payload = _col("payload_json")
        if c_payload is None:
            c_payload = _col("payload_part")
        if c_payload is None:
            c_payload = _col("data")
        if c_payload is None:
            c_payload = len(header) - 1  # last-ditch fallback

        c_sha = _col("sha256")

        target_id = str(analysis_id).strip()
        if not target_id:
            return {}

        # (pidx:int|None, total:int|None, chunk:str, sha:str)
        parts_with_sha = []

        for r in body:
            try:
                if not r:
                    continue

                rid = r[c_id] if c_id < len(r) else ""
                rid = str(rid).strip()
                if rid != target_id:
                    continue

                chunk = r[c_payload] if c_payload < len(r) else ""
                chunk = chunk or ""
                if not isinstance(chunk, str):
                    chunk = str(chunk)

                # part_index (optional)
                pidx = None
                if c_part is not None and c_part < len(r):
                    try:
                        pidx = int(str(r[c_part]).strip())
                    except Exception:
                        pass
                        pidx = None

                # total_parts (optional)
                tparts = None
                if c_total is not None and c_total < len(r):
                    try:
                        tparts = int(str(r[c_total]).strip())
                    except Exception:
                        pass
                        tparts = None

                sha = ""
                if c_sha is not None and c_sha < len(r):
                    sha = str(r[c_sha] or "").strip()

                # keep even tiny chunks; concatenation is deterministic
                if chunk.strip() == "":
                    continue

                parts_with_sha.append((pidx, tparts, chunk, sha))
            except Exception:
                pass
                continue

        if not parts_with_sha:
            return {}

        # Score = (unique part_index count, total payload length)
        parts = []          # list[(pidx, chunk)]
        chosen_sha = ""     # sha bucket selected (if any)
        chosen_total = None # total_parts inferred for chosen bucket (if any)
        try:
            if any(s for _, _, _, s in parts_with_sha):
                buckets = {}
                for pidx, tparts, chunk, sha in parts_with_sha:
                    key = sha or "__no_sha__"
                    buckets.setdefault(key, []).append((pidx, tparts, chunk))

                def _score(items):
                    idxs = [i for i, _, _ in items if i is not None]
                    uniq = len(set(idxs)) if idxs else 0
                    total_len = sum(len(c or "") for _, _, c in items)
                    return (uniq, total_len)

                best_key = sorted(buckets.keys(), key=lambda k: _score(buckets[k]), reverse=True)[0]
                chosen_sha = "" if best_key == "__no_sha__" else best_key
                best_items = buckets[best_key]

                # Infer total_parts for this bucket (mode / max)
                try:
                    totals = [tp for _, tp, _ in best_items if isinstance(tp, int) and tp > 0]
                    chosen_total = max(totals) if totals else None
                except Exception:
                    pass
                    chosen_total = None

                parts = [(pidx, chunk) for (pidx, _tparts, chunk) in best_items]
            else:
                parts = [(pidx, chunk) for pidx, _tparts, chunk, _sha in parts_with_sha]
                # Infer total_parts (mode / max) even without sha
                try:
                    totals = [tp for _, tp, _, _ in parts_with_sha if isinstance(tp, int) and tp > 0]
                    chosen_total = max(totals) if totals else None
                except Exception:
                    pass
                    chosen_total = None
        except Exception:
            pass
            parts = [(pidx, chunk) for pidx, _tparts, chunk, _sha in parts_with_sha]
            try:
                totals = [tp for _, tp, _, _ in parts_with_sha if isinstance(tp, int) and tp > 0]
                chosen_total = max(totals) if totals else None
            except Exception:
                pass
                chosen_total = None

        # Sort parts deterministically by part_index; None last
        def _sort_key(t):
            pidx, _ = t
            return (pidx is None, pidx if pidx is not None else 0)

        parts.sort(key=_sort_key)

        # - If total_parts is known and we have part_index values, require 0..total-1.
        # - If incomplete, return {} (do not attempt parse on partial payload).
        try:
            if isinstance(chosen_total, int) and chosen_total > 0:
                idxs = [p for (p, _c) in parts if isinstance(p, int)]
                if idxs:
                    uniq = sorted(set(idxs))
                    expected = list(range(0, chosen_total))
                    if uniq != expected:
                        return {}
        except Exception:
            pass

        # Stitch chunks
        full_json_str = "".join([chunk for _, chunk in parts]).strip()
        if not full_json_str:
            return {}

        # - If chosen_sha exists, compare against sha256(stitched_bytes).
        # - If mismatch, return {} (treat as corrupted / wrong bucket).
        try:
            if isinstance(chosen_sha, str) and chosen_sha:
                import hashlib
                digest = hashlib.sha256(full_json_str.encode("utf-8", errors="ignore")).hexdigest()
                if str(digest).lower() != str(chosen_sha).lower():
                    return {}
        except Exception:
            pass

        import json
        try:
            obj = json.loads(full_json_str)
            if isinstance(obj, dict):
                # Only attaches when parse succeeded.
                try:
                    obj["_rehydration_debug"] = {
                        "worksheet": str(worksheet_title or ""),
                        "analysis_id": str(target_id),
                        "parts_used": int(len(parts)),
                        "total_parts_expected": int(chosen_total) if isinstance(chosen_total, int) else None,
                        "sha_verified": bool(chosen_sha),
                    }
                except Exception:
                    pass
                return obj
            return {}
        except Exception:
            pass
            try:
                # Try to isolate first "{" and last "}" if accidental prefix/suffix exists
                a = full_json_str.find("{")
                b = full_json_str.rfind("}")
                if a != -1 and b != -1 and b > a:
                    obj2 = json.loads(full_json_str[a:b+1])
                    if isinstance(obj2, dict):
                        # (keep same meta stamp behavior)
                        try:
                            obj2["_rehydration_debug"] = {
                                "worksheet": str(worksheet_title or ""),
                                "analysis_id": str(target_id),
                                "parts_used": int(len(parts)),
                                "total_parts_expected": int(chosen_total) if isinstance(chosen_total, int) else None,
                                "sha_verified": bool(chosen_sha),
                                "salvaged": True,
                            }
                        except Exception:
                            return obj2
            except Exception:
                return {}

    except Exception:
        return {}

def fingerprint_text(text: str) -> str:
    """Stable short fingerprint for fetched content (deterministic)."""
    if not text:
        return ""
    normalized = re.sub(r"\s+", " ", text.strip().lower())
    return hashlib.md5(normalized.encode("utf-8")).hexdigest()[:12]

# =========================
# REFACTOR111: Prev snapshot picker (deterministic)
# =========================

def _refactor111_norm_question_v1(q: str) -> str:
    try:
        import re as _re
        return _re.sub(r"\s+", " ", str(q or "").strip().lower())
    except Exception:
        return str(q or "").strip().lower()

def _refactor111_get_question_from_payload_v1(obj: dict) -> str:
    try:
        if not isinstance(obj, dict):
            return ""
        q = obj.get("question") or ""
        if q:
            return str(q)
        r = obj.get("results") if isinstance(obj.get("results"), dict) else {}
        q2 = r.get("question") or ""
        if q2:
            return str(q2)
        pr = obj.get("primary_response") if isinstance(obj.get("primary_response"), dict) else {}
        q3 = pr.get("question") or ""
        return str(q3 or "")
    except Exception:
        return ""

def _refactor111_get_pmc_v1(obj: dict) -> dict:
    try:
        if not isinstance(obj, dict):
            return {}
        pmc = obj.get("primary_metrics_canonical")
        if isinstance(pmc, dict) and pmc:
            return pmc
        r = obj.get("results") if isinstance(obj.get("results"), dict) else {}
        pmc = r.get("primary_metrics_canonical")
        if isinstance(pmc, dict) and pmc:
            return pmc
        pr = obj.get("primary_response") if isinstance(obj.get("primary_response"), dict) else {}
        pmc = pr.get("primary_metrics_canonical")
        if isinstance(pmc, dict) and pmc:
            return pmc
    except Exception:
        pass
    return {}

def _refactor111_extract_bsc_urls_v1(obj: dict) -> list:
    urls = []
    try:
        if not isinstance(obj, dict):
            return []
        # Prefer baseline_sources_cache
        bsc = obj.get("baseline_sources_cache")
        if not isinstance(bsc, list):
            r = obj.get("results") if isinstance(obj.get("results"), dict) else {}
            bsc = r.get("baseline_sources_cache")
        if isinstance(bsc, list):
            for it in bsc:
                if isinstance(it, dict):
                    u = it.get("source_url") or it.get("url")
                    if u:
                        urls.append(str(u))
        # Also allow 'sources' list
        if not urls:
            s = obj.get("sources")
            if isinstance(s, list):
                urls = [str(u) for u in s if isinstance(u, str) and u.strip()]
    except Exception:
        pass
    # stable dedupe
    seen, out = set(), []
    for u in urls:
        uu = (u or "").strip()
        if not uu or uu in seen:
            continue
        seen.add(uu)
        out.append(uu)
    return out

def _refactor111_parse_ts_v1(ts: str):
    try:
        fn = globals().get("_parse_iso_dt")
        if callable(fn):
            dt = fn(ts)
            return dt
    except Exception:
        pass
    return None

def _refactor111_mask_sheet_id_v1(sid: str) -> str:
    try:
        sid = str(sid or "")
        if len(sid) <= 8:
            return sid
        return f"...{sid[-6:]}"
    except Exception:
        return ""
def _refactor112_parse_ts_from_analysis_id_v1(aid: str):
    """REFACTOR112: fallback timestamp inference from analysis_id like YYYYMMDD_HHMMSS_xxxxxx."""
    try:
        import datetime as _dt
        s = str(aid or "").strip()
        # Expected: 20260203_003809_7aa711
        if len(s) >= 15 and s[8] == "_" and s[15:16] == "_":
            d = s[0:8]
            t = s[9:15]
        elif len(s) >= 15 and s[8] == "_" and len(s) >= 15:
            d = s[0:8]
            t = s[9:15]
        else:
            return None
        if not (d.isdigit() and t.isdigit()):
            return None
        dt = _dt.datetime.strptime(d + t, "%Y%m%d%H%M%S")
        # Treat as UTC (naive)
        return dt
    except Exception:
        return None


def _refactor111_pick_latest_prev_snapshot_v1(previous_data: dict, web_context: dict = None):
    """
    REFACTOR111:
    - Prefer HistoryFull by max parsed timestamp (created_at or payload.timestamp)
    - Filter: exact normalized question match; must have non-empty primary_metrics_canonical
    - Fallback: local history files by mtime; finally snapshot_ref/previous_data
    """
    dbg = {
        "origin_attempted": "",
        "candidates_considered": [],
        "selected_ref": "",
        "selected_timestamp": "",
        "selected_code_version": "",
        "reason": "fallback",
        "selected_baseline_sources_urls": [],
        "sheet_probe": {}
    }

    prev = previous_data if isinstance(previous_data, dict) else {}
    wc = web_context if isinstance(web_context, dict) else {}

    target_q = _refactor111_norm_question_v1(
        wc.get("question") or prev.get("question") or _refactor111_get_question_from_payload_v1(prev)
    )

    # -------- Attempt A: Sheet HistoryFull --------
    try:
        if FORCE_LATEST_PREV_SNAPSHOT_V1:
            ss = None
            try:
                ss = globals().get("get_google_spreadsheet")() if callable(globals().get("get_google_spreadsheet")) else None
            except Exception:
                ss = None

            if ss:
                sid = ""
                try:
                    sid = getattr(ss, "id", "") or ""
                except Exception:
                    sid = ""

                dbg["origin_attempted"] = "sheet_historyfull"
                try:
                    dbg["sheet_probe"]["sheet_id_masked"] = _refactor111_mask_sheet_id_v1(sid)
                except Exception:
                    pass

                ws = None
                try:
                    ws = ss.worksheet("HistoryFull")
                except Exception:
                    ws = None

                rows = []
                if ws:
                    try:
                        rows = ws.get_all_values() or []
                    except Exception:
                        rows = []
                    dbg["sheet_probe"]["historyfull_list_ok"] = bool(rows)
                    dbg["sheet_probe"]["historyfull_row_count"] = int(len(rows) if isinstance(rows, list) else 0)

                if rows and len(rows) >= 2:
                    header = rows[0]
                    try:
                        dbg["sheet_probe"]["historyfull_header"] = header
                        _lens = [len(_r) for _r in (rows[1:] or []) if isinstance(_r, list)]
                        if _lens:
                            dbg["sheet_probe"]["historyfull_row_len_minmax"] = [int(min(_lens)), int(max(_lens))]
                        dbg["sheet_probe"]["historyfull_expected_cols"] = 7
                    except Exception:
                        pass
                    def _col(name):
                        try:
                            return header.index(name)
                        except Exception:
                            return -1

                    c_aid = _col("analysis_id")
                    if c_aid < 0:
                        c_aid = _col("id")
                    c_created = _col("created_at")
                    c_cv = _col("code_version")

                    # Build analysis_id -> best_ts (REFACTOR112: fallback parse from analysis_id when created_at is not ISO)
                    best = {}
                    for r in rows[1:]:
                        if not isinstance(r, list):
                            continue
                        aid = (r[c_aid] if c_aid >= 0 and c_aid < len(r) else "") if r else ""
                        aid = str(aid or "").strip()
                        if not aid:
                            continue

                        created_raw = (r[c_created] if c_created >= 0 and c_created < len(r) else "") if r else ""
                        created_raw = str(created_raw or "")

                        dt = _refactor111_parse_ts_v1(created_raw)
                        inferred = False
                        display_created = created_raw

                        if not dt:
                            try:
                                _dt2 = _refactor112_parse_ts_from_analysis_id_v1(aid)
                            except Exception:
                                _dt2 = None
                            if _dt2 is not None:
                                dt = _dt2
                                inferred = True
                                try:
                                    import datetime as _dt
                                    display_created = _dt2.replace(tzinfo=_dt.timezone.utc).isoformat().replace("+00:00", "Z")
                                except Exception:
                                    pass

                        ts_val = 0.0
                        if dt:
                            try:
                                import datetime as _dt
                                ts_val = dt.replace(tzinfo=_dt.timezone.utc).timestamp()
                            except Exception:
                                try:
                                    ts_val = float(dt.timestamp())
                                except Exception:
                                    ts_val = 0.0

                        if aid not in best or ts_val > float(best[aid].get("ts_val") or 0.0):
                            best[aid] = {
                                "ts_val": ts_val,
                                "created_at": str(display_created or ""),
                                "code_version": (r[c_cv] if c_cv >= 0 and c_cv < len(r) else ""),
                                "inferred_from_analysis_id": bool(inferred),
                            }

                    # Sort candidate aids by ts desc
                    cands = sorted(best.items(), key=lambda kv: float(kv[1]["ts_val"] or 0.0), reverse=True)

                    # record considered (top 50)
                    for aid, meta in cands[:50]:
                        dbg["candidates_considered"].append({
                            "ref": f"gsheet:HistoryFull:{aid}",
                            "timestamp": meta.get("created_at") or "",
                            "code_version": str(meta.get("code_version") or ""),
                            "timestamp_inferred": bool(meta.get("inferred_from_analysis_id")),
                        })

                    # walk candidates in timestamp order until match
                    loader = globals().get("load_full_history_payload_from_sheet")
                    _fallback_missing_q = None

                    def _has_any_value(_pmc: dict) -> bool:
                        try:
                            for _k, _v in (_pmc or {}).items():
                                if isinstance(_v, dict) and _v.get("value") is not None:
                                    return True
                        except Exception:
                            return False
                        return False

                    for aid, meta in cands[:50]:
                        full = loader(aid, worksheet_title="HistoryFull") if callable(loader) else {}
                        if not isinstance(full, dict) or not full:
                            continue
                        qn = _refactor111_norm_question_v1(_refactor111_get_question_from_payload_v1(full))
                        pmc = _refactor111_get_pmc_v1(full)
                        if not (isinstance(pmc, dict) and pmc):
                            continue
                        if target_q and qn != target_q:
                            # REFACTOR124: allow newest snapshot even if question is missing, as long as PMC has values
                            try:
                                if (not qn) and _has_any_value(pmc) and (_fallback_missing_q is None):
                                    _fallback_missing_q = (full, aid, meta)
                            except Exception:
                                pass
                            continue

                        # Selected
                        sel_ts = str(full.get("timestamp") or "")
                        dbg["selected_ref"] = f"gsheet:HistoryFull:{aid}"
                        dbg["selected_timestamp"] = sel_ts
                        dbg["selected_code_version"] = str(full.get("code_version") or "")
                        dbg["reason"] = "max_timestamp"
                        try:
                            dbg.setdefault("sheet_probe", {})
                            if isinstance(dbg.get("sheet_probe"), dict):
                                dbg["sheet_probe"]["match_mode"] = "strict_question"
                        except Exception:
                            pass
                        dbg["selected_baseline_sources_urls"] = _refactor111_extract_bsc_urls_v1(full)

                        # Mark payload for downstream fastpath gate
                        full["_refactor111_prev_snapshot_is_latest_v1"] = True
                        full["_refactor111_prev_snapshot_ref_v1"] = dbg["selected_ref"]
                        full["_refactor111_prev_snapshot_pick_v1"] = dbg
                        return full, dbg


                    # REFACTOR124: fallback selection when newest snapshot lacks a question field
                    if target_q and _fallback_missing_q:
                        try:
                            full, aid, meta = _fallback_missing_q
                            sel_ts = str(full.get("timestamp") or meta.get("created_at") or "")
                            dbg["selected_ref"] = f"gsheet:HistoryFull:{aid}"
                            dbg["selected_timestamp"] = sel_ts
                            dbg["selected_code_version"] = str(full.get("code_version") or meta.get("code_version") or "")
                            dbg["reason"] = "max_timestamp_missing_question_with_values"
                            dbg["selected_baseline_sources_urls"] = _refactor111_extract_bsc_urls_v1(full)
                            try:
                                dbg.setdefault("sheet_probe", {})
                                if isinstance(dbg.get("sheet_probe"), dict):
                                    dbg["sheet_probe"]["match_mode"] = "fallback_missing_question"
                            except Exception:
                                pass

                            full["_refactor111_prev_snapshot_is_latest_v1"] = True
                            full["_refactor111_prev_snapshot_ref_v1"] = dbg["selected_ref"]
                            full["_refactor111_prev_snapshot_pick_v1"] = dbg
                            return full, dbg
                        except Exception:
                            pass


                    # If we saw candidates but did not match question/schema, still emit max timestamp seen
                    try:
                        if cands:
                            top_ts = cands[0][1].get("created_at") or ""
                            dbg["sheet_probe"]["historyfull_max_timestamp_seen"] = str(top_ts)
                    except Exception:
                        pass
    except Exception:
        pass

    # -------- Attempt B: local history by mtime --------
    try:
        dbg["origin_attempted"] = dbg["origin_attempted"] or "local_history"
        search_dirs = []
        try:
            search_dirs.append(os.getcwd())
        except Exception:
            pass
        try:
            search_dirs.append(os.path.dirname(__file__))
        except Exception:
            pass
        for extra in ("history", "runs", "outputs"):
            for base in list(search_dirs):
                try:
                    p = os.path.join(base, extra)
                    if os.path.isdir(p):
                        search_dirs.append(p)
                except Exception:
                    pass

        files = []
        for d in list(dict.fromkeys([x for x in search_dirs if isinstance(x, str) and x])):
            try:
                for fn in os.listdir(d):
                    if not (fn.startswith("yureeka_") and fn.endswith(".json")):
                        continue
                    if "evolution" in fn:
                        continue
                    fp = os.path.join(d, fn)
                    try:
                        mt = os.path.getmtime(fp)
                    except Exception:
                        mt = 0.0
                    files.append((mt, fp))
            except Exception:
                pass

        files.sort(key=lambda t: float(t[0] or 0.0), reverse=True)

        for mt, fp in files[:50]:
            dbg["candidates_considered"].append({"ref": f"local:{fp}", "timestamp": "", "code_version": ""})

            try:
                raw = open(fp, "r", encoding="utf-8").read()
                obj = json.loads(raw)
            except Exception:
                continue
            if not isinstance(obj, dict):
                continue

            qn = _refactor111_norm_question_v1(_refactor111_get_question_from_payload_v1(obj))
            if target_q and qn != target_q:
                continue
            pmc = _refactor111_get_pmc_v1(obj)
            if not (isinstance(pmc, dict) and pmc):
                continue

            dbg["selected_ref"] = f"local:{fp}"
            dbg["selected_timestamp"] = str(obj.get("timestamp") or "")
            dbg["selected_code_version"] = str(obj.get("code_version") or "")
            dbg["reason"] = "fallback"
            dbg["selected_baseline_sources_urls"] = _refactor111_extract_bsc_urls_v1(obj)

            obj["_refactor111_prev_snapshot_is_latest_v1"] = False
            obj["_refactor111_prev_snapshot_ref_v1"] = dbg["selected_ref"]
            obj["_refactor111_prev_snapshot_pick_v1"] = dbg
            return obj, dbg
    except Exception:
        pass

    # -------- Attempt C: snapshot_ref / previous_data --------
    try:
        dbg["origin_attempted"] = dbg["origin_attempted"] or "snapshot_ref"
        dbg["selected_ref"] = str(prev.get("full_store_ref") or prev.get("snapshot_store_ref") or "")
        dbg["selected_timestamp"] = str(prev.get("timestamp") or "")
        dbg["selected_code_version"] = str(prev.get("code_version") or "")
        dbg["selected_baseline_sources_urls"] = _refactor111_extract_bsc_urls_v1(prev)
        prev["_refactor111_prev_snapshot_is_latest_v1"] = False
        prev["_refactor111_prev_snapshot_ref_v1"] = dbg["selected_ref"]
        prev["_refactor111_prev_snapshot_pick_v1"] = dbg
    except Exception:
        pass

    return prev, dbg


def attach_source_snapshots_to_analysis(analysis: dict, web_context: dict) -> dict:
    try:
        _q = str((analysis or {}).get('question') or '')
        _fix2d66_promote_injection_in_web_context(web_context, question=_q)
    except Exception:
        pass

    """
    Attach stable source snapshots (from web_context.scraped_meta) into analysis.

    Enhancements (v7_34 patch):
    - Ensures scraped_meta.extracted_numbers is always list-like
    - Adds RANGE capture per canonical metric using admitted snapshots:
        primary_metrics_canonical[ckey]["value_range"] = {min,max,n,examples}
      This restores earlier "range vs point estimate" behavior in a compatible way.
    """
    import re
    from datetime import datetime, timezone

    try:
        _qtxt = str((analysis or {}).get('question') or (analysis or {}).get('query') or '')
        web_context = _fix2d66_promote_injected_urls(web_context or {}, question_text=_qtxt, stage='analysis_attach')
    except Exception:
        pass

    def _now_iso() -> str:
        return datetime.now(timezone.utc).isoformat()

    def _fingerprint(text: str) -> str:
        try:
            fn = globals().get("fingerprint_text")
            if callable(fn):
                return fn(text)
        except Exception:
            pass
        try:
            import hashlib
            t = re.sub(r"\s+", " ", (text or "").strip().lower())
            return hashlib.md5(t.encode("utf-8", errors="ignore")).hexdigest()[:12]
        except Exception:
            return ""

    # - Does NOT change existing behavior if anchor_hash already present.
    def _sha1(s: str) -> str:
        try:
            import hashlib
            return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()
        except Exception:
            return ""

    # - Ensures unit_tag/unit_family/base_unit/value_norm are present when possible.
    # - No behavior change if helper missing.
    _canon_fn = globals().get("canonicalize_numeric_candidate")
    def _maybe_canonicalize(n: dict) -> dict:
        try:
            if callable(_canon_fn):
                return _canon_fn(dict(n))
        except Exception:
            return dict(n)

    def _parse_num(value, unit_hint=""):
        try:
            fn = globals().get("parse_human_number")
            if callable(fn):
                return fn(str(value), unit_hint)
        except Exception:
            pass
        # fallback
        try:
            s = str(value).strip().replace(",", "")
            if not s:
                return None
            return float(re.findall(r"-?\d+(?:\.\d+)?", s)[0])
        except Exception:
            return None

    def _unit_family_from_metric(mdef: dict) -> str:
        # prefer metric schema
        uf = (mdef or {}).get("unit_family") or ""
        uf = str(uf).lower().strip()
        if uf in ("percent", "pct"):
            return "PCT"
        if uf in ("currency",):
            return "CUR"
        if uf in ("magnitude", "unit_sales", "other"):
            return "MAG"
        return "OTHER"

    def _cand_unit_family(cunit: str, craw: str) -> str:
        u = (cunit or "").strip()
        r = (craw or "")
        uu = u.upper()
        ru = r.upper()

        # Percent
        if uu == "%" or "%" in ru:
            return "PCT"

        # Energy
        if any(x in (u or "").lower() for x in ["twh", "gwh", "mwh", "kwh"]) or any(x in (r or "").lower() for x in ["twh", "gwh", "mwh", "kwh"]):
            return "ENERGY"

        # Currency (symbol/code presence)
        #if any(x in ru for x in ["$", "USD", "SGD", "EUR", "GBP", "S$"]) or uu in ("USD", "SGD", "EUR", "GBP"):
        #    return "CUR"

        if re.search(r"(\$|S\$|€|£)\s*\d", r) or any(x in ru for x in ["USD", "SGD", "EUR", "GBP"]) or uu in ("USD","SGD","EUR","GBP"):
            return "CUR"


        # Magnitude (case-insensitive)
        if uu in ("K", "M", "B", "T") or (u or "").lower() in ("k", "m", "b", "t"):
            return "MAG"

        return "OTHER"

    def _tokenize(s: str):
        return [t for t in re.findall(r"[a-z0-9]+", (s or "").lower()) if len(t) > 2]

    def _safe_norm_unit_tag(x: str) -> str:
        try:
            fn = globals().get("normalize_unit_tag")
            if callable(fn):
                return fn(x or "")
        except Exception:
            return (x or "").strip()


    # Build baseline_sources_cache from scraped_meta (snapshot-friendly)
    baseline_sources_cache = []
    scraped_meta = (web_context or {}).get("scraped_meta") or {}
    if isinstance(scraped_meta, dict):
        for url, meta in scraped_meta.items():
            if not isinstance(meta, dict):
                continue
            nums = meta.get("extracted_numbers") or []
            if nums is None or not isinstance(nums, list):
                nums = []

            content = meta.get("content") or meta.get("clean_text") or (web_context.get("scraped_content", {}) or {}).get(url, "") or ""

            _sd = meta.get("status_detail") or meta.get("status") or ""
            _status = "fetched" if str(_sd).startswith("success") or meta.get("status") == "fetched" else "failed"
            if str(_sd).startswith("skipped:"):
                _status = "skipped"
                try:
                    _sd = str(_sd)[len("skipped:"):] or "pdf_unsupported_missing_dependency"
                except Exception:
                    _sd = "pdf_unsupported_missing_dependency"

            baseline_sources_cache.append({
                "url": url,
                "status": _status,
                "status_detail": _sd,
                "numbers_found": int(meta.get("numbers_found") or (len(nums) if isinstance(nums, list) else 0)),
                "fetched_at": meta.get("fetched_at") or _now_iso(),
                "fingerprint": meta.get("fingerprint") or _fingerprint(content),

                # REFACTOR121: store a lightweight excerpt for year-anchor backstop (avoid huge payloads)
                "snapshot_text_excerpt": (content[:12000] if isinstance(content, str) else ""),

                # - This is critical for:
                #   * range gating (metric-aware)
                #   * schema-first attribution
                #   * evolution rebuild (anchor_hash + value_norm + unit_family)
                # - Backward compatible: only adds keys; existing keys unchanged.
                "extracted_numbers": [
                    (lambda nn: {
                        "value": nn.get("value"),
                        "unit": nn.get("unit"),
                        "raw": nn.get("raw"),
                        "context_snippet": (nn.get("context_snippet") or nn.get("context") or "")[:240],

                        # keep existing anchor_hash if present; else stable fallback
                        "anchor_hash": (
                            nn.get("anchor_hash")
                            or _sha1(
                                f"{url}|{str(nn.get('raw') or '')}|{(nn.get('context_snippet') or nn.get('context') or '')[:240]}"
                            )
                        ),

                        "source_url": nn.get("source_url") or url,

                        "is_junk": nn.get("is_junk"),
                        "junk_reason": nn.get("junk_reason"),
                        "start_idx": nn.get("start_idx"),
                        "end_idx": nn.get("end_idx"),

                        "unit_tag": nn.get("unit_tag"),
                        "unit_family": nn.get("unit_family"),
                        "base_unit": nn.get("base_unit"),
                        "multiplier_to_base": nn.get("multiplier_to_base"),
                        "value_norm": nn.get("value_norm"),

                        "measure_kind": nn.get("measure_kind"),
                        "measure_assoc": nn.get("measure_assoc"),
                    })(_maybe_canonicalize(n))
                    for n in nums
                    if isinstance(n, dict)
                ]
            })

    if baseline_sources_cache:

        for s in (baseline_sources_cache or []):
            if isinstance(s, dict) and isinstance(s.get("extracted_numbers"), list):

                try:
                    if "sort_snapshot_numbers" in globals() and callable(globals()["sort_snapshot_numbers"]):
                        s["extracted_numbers"] = sort_snapshot_numbers(s["extracted_numbers"])
                    else:
                        # safe fallback: anchor_hash then raw
                        s["extracted_numbers"] = sorted(
                            s["extracted_numbers"],
                            key=lambda x: (str((x or {}).get("anchor_hash") or ""), str((x or {}).get("raw") or ""))
                        )
                except Exception:
                    pass

                s["numbers_found"] = len(s["extracted_numbers"])

        baseline_sources_cache = sorted(
            baseline_sources_cache,
            key=lambda x: str((x or {}).get("url") or "")
        )

        # - Adds *synthetic* url-only source records for injected URLs that were
        #   persisted (per diag) but are missing from baseline_sources_cache.
        # - Default OFF; only activates when INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH is enabled.
        # - Does NOT alter fastpath logic or metric selection (synthetic has no numbers).
        _inj_hash_added = []
        _inj_hash_reasons = {}
        try:
            _diag_local = {}
            if isinstance(web_context, dict):
                _diag_local = web_context.get("diag_injected_urls") or web_context.get("extra_urls_debug") or {}
            _persisted_for_hash = []
            if isinstance(_diag_local, dict):
                _persisted_for_hash = _inj_diag_norm_url_list(
                    _diag_local.get("persisted_norm") or _diag_local.get("persisted") or []
                )
            _incl_inj_hash = _inj_hash_policy_should_include(_persisted_for_hash)
            if _incl_inj_hash and _persisted_for_hash:
                _bsc_aug, _inj_hash_added, _inj_hash_reasons = _inj_hash_add_synthetic_sources(
                    baseline_sources_cache,
                    _persisted_for_hash,
                    now_iso=_now_iso(),
                )
                baseline_sources_cache = _bsc_aug
        except Exception:
            pass
            _inj_hash_added = []
            _inj_hash_reasons = {}

        # REFACTOR123: Ensure Analysis baseline has the same schema-seeded source pool as Evolution,
        # so previous_value is non-null and diffing can compute deltas.
        try:
            _is_evolution_payload = bool(isinstance(analysis, dict) and analysis.get("analysis_type"))
            if not _is_evolution_payload:
                _rf115_inj_urls = []
                try:
                    _rf115_inj_urls = _refactor115_collect_injection_urls_v1({}, web_context or {})
                except Exception:
                    _rf115_inj_urls = []
                _rf115_injection_present = bool(_rf115_inj_urls)

                _rf115_added = []
                _rf115_extract_diag = []
                _rf115_year_tokens_union = ["2025", "2026", "2040"]

                if isinstance(baseline_sources_cache, list):
                    # normalize existing URLs
                    _existing = set()
                    for _r in (baseline_sources_cache or []):
                        if isinstance(_r, dict):
                            _u = _r.get("url") or _r.get("source_url") or ""
                            _n = ""
                            try:
                                _n = _inj_diag_norm_url_list([_u])[0] if _u else ""
                            except Exception:
                                _n = str(_u or "").strip()
                            if _n:
                                _existing.add(_n)

                    if (not _rf115_injection_present):
                        for _u in (_REFACTOR115_SCHEMA_SEED_URLS_V1 or []):
                            try:
                                _u_norm = _inj_diag_norm_url_list([_u])[0]
                            except Exception:
                                _u_norm = str(_u or "").strip()
                            if not _u_norm or _u_norm in _existing:
                                continue

                            _txt, _detail = None, ""
                            try:
                                _txt, _detail = fetch_url_content_with_status(_u_norm, timeout=25)
                            except Exception as _e:
                                _txt, _detail = None, f"exception:{type(_e).__name__}"

                            # Normalize status for skipped PDF dependency
                            _status = "fetched" if str(_detail).startswith("success") else "failed"
                            _sd = str(_detail or "")
                            if str(_sd).startswith("skipped:"):
                                _status = "skipped"
                                try:
                                    _sd = _sd[len("skipped:"):] or "pdf_unsupported_missing_dependency"
                                except Exception:
                                    _sd = "pdf_unsupported_missing_dependency"

                            if isinstance(_txt, str) and len(_txt.strip()) >= 200:
                                _rf115_extracted_numbers = []
                                _rf115_numbers_found = 0
                                _rf115_fp = ""
                                _rf115_year_hits = {}
                                _rf115_extract_error = ""
                                try:
                                    _rf115_extracted_numbers = extract_numbers_with_context(_txt, source_url=_u_norm, max_results=600) or []
                                except Exception as _ee:
                                    _rf115_extracted_numbers = []
                                    try:
                                        _rf115_extract_error = f"{type(_ee).__name__}:{_ee}"
                                    except Exception:
                                        _rf115_extract_error = "exception"
                                try:
                                    _rf115_numbers_found = sum(
                                        1 for _n in (_rf115_extracted_numbers or [])
                                        if isinstance(_n, dict) and (not bool(_n.get("is_junk")))
                                    )
                                except Exception:
                                    try:
                                        _rf115_numbers_found = int(len(_rf115_extracted_numbers or [])) if isinstance(_rf115_extracted_numbers, list) else 0
                                    except Exception:
                                        _rf115_numbers_found = 0
                                try:
                                    _rf115_fp = fingerprint_text(_txt)
                                except Exception:
                                    _rf115_fp = ""
                                try:
                                    for _yy in (_rf115_year_tokens_union or []):
                                        if isinstance(_yy, str) and _yy:
                                            _rf115_year_hits[_yy] = int(str(_txt).count(_yy))
                                except Exception:
                                    _rf115_year_hits = {}

                                try:
                                    _rf115_extract_diag.append({
                                        "url": _u_norm,
                                        "numbers_found": int(_rf115_numbers_found or 0),
                                        "status": _status,
                                        "status_detail": _sd,
                                        "year_token_hits": _rf115_year_hits,
                                        "extract_error": _rf115_extract_error,
                                    })
                                except Exception:
                                    pass

                                baseline_sources_cache.append({
                                    "source_url": _u_norm,
                                    "url": _u_norm,
                                    "status": _status,
                                    "status_detail": _sd,
                                    "snapshot_text": _txt,
                                    "snapshot_text_excerpt": (_txt[:12000] if isinstance(_txt, str) else ""),
                                    "fingerprint": _rf115_fp,
                                    "extracted_numbers": _rf115_extracted_numbers,
                                    "numbers_found": _rf115_numbers_found,
                                    "seeded": True,
                                    "seeded_reason": "schema_seeds",
                                    "fetched_at": _now_iso(),
                                })
                                _existing.add(_u_norm)
                                _rf115_added.append(_u_norm)
                            else:
                                try:
                                    _rf115_extract_diag.append({
                                        "url": _u_norm,
                                        "numbers_found": 0,
                                        "status": ("seeded_pending" if (not _status == "skipped") else "skipped"),
                                        "status_detail": _sd or "seeded_placeholder",
                                        "year_token_hits": {},
                                        "extract_error": "missing_text",
                                    })
                                except Exception:
                                    pass
                                baseline_sources_cache.append({
                                    "source_url": _u_norm,
                                    "url": _u_norm,
                                    "status": "seeded_pending" if (not _status == "skipped") else "skipped",
                                    "status_detail": _sd or "seeded_placeholder",
                                    "snapshot_text": "",
                                    "snapshot_text_excerpt": "",
                                    "fingerprint": "",
                                    "extracted_numbers": [],
                                    "numbers_found": 0,
                                    "seeded": True,
                                    "seeded_reason": "schema_seeds_placeholder",
                                    "fetched_at": _now_iso(),
                                })
                                _existing.add(_u_norm)
                                _rf115_added.append(_u_norm)

                # Emit debug beacons on analysis payload (additive)
                try:
                    analysis.setdefault("debug", {})
                    if isinstance(analysis.get("debug"), dict):
                        analysis["debug"]["schema_seed_sources_v1"] = {
                            "stage": "analysis_attach",
                            "seeds_added": int(len(_rf115_added)),
                            "added_urls": list(_rf115_added)[:10],
                            "injection_present": bool(_rf115_injection_present),
                            "reason": "schema_seeds",
                        }

                        _nonzero = 0
                        _errs = 0
                        _sample = []
                        _year_hits = {}
                        if isinstance(_rf115_extract_diag, list):
                            try:
                                _nonzero = sum(1 for _d in _rf115_extract_diag if isinstance(_d, dict) and int(_d.get("numbers_found") or 0) > 0)
                            except Exception:
                                _nonzero = 0
                            try:
                                _errs = sum(1 for _d in _rf115_extract_diag if isinstance(_d, dict) and str(_d.get("extract_error") or "").strip())
                            except Exception:
                                _errs = 0
                            for _d in (_rf115_extract_diag or [])[:12]:
                                if isinstance(_d, dict):
                                    _sample.append({"url": _d.get("url"), "numbers_found": _d.get("numbers_found"), "status": _d.get("status")})
                                    try:
                                        yh = _d.get("year_token_hits") or {}
                                        if isinstance(yh, dict):
                                            for k, v in yh.items():
                                                _year_hits[k] = int(_year_hits.get(k, 0)) + int(v or 0)
                                    except Exception:
                                        pass

                        analysis["debug"]["schema_seed_extract_v1"] = {
                            "stage": "analysis_attach",
                            "seeds_added": int(len(_rf115_added)),
                            "seeds_extracted_nonzero": int(_nonzero),
                            "extract_errors": int(_errs),
                            "per_seed_numbers_found_sample": _sample,
                            "per_seed_year_token_hits": _year_hits,
                        }
                except Exception:
                    pass

                # If baseline schema values are null, run schema-only rebuild using the now-seeded sources cache.
                try:
                    pmc = (analysis or {}).get("primary_metrics_canonical")
                    if (not pmc) and isinstance((analysis or {}).get("primary_response"), dict):
                        try:
                            pmc = ((analysis or {}).get("primary_response") or {}).get("primary_metrics_canonical")
                        except Exception:
                            pmc = pmc
                    _all_null = True
                    if isinstance(pmc, dict) and pmc:
                        for _k, _mv in pmc.items():
                            if isinstance(_mv, dict) and (_mv.get("value") is not None):
                                _all_null = False
                                break
                    if _all_null and callable(globals().get("rebuild_metrics_from_snapshots_schema_only")):
                        _rebuilt = rebuild_metrics_from_snapshots_schema_only(analysis, baseline_sources_cache, web_context=web_context)
                        if isinstance(_rebuilt, dict):
                            # REFACTOR124: preserve wrapper fields when applying schema-only rebuild
                            try:
                                _orig_wrapper = analysis if isinstance(analysis, dict) else {}
                                _rebuilt_pmc = None
                                _rebuilt_schema = None
                                try:
                                    _rebuilt_pmc = (_rebuilt or {}).get("primary_metrics_canonical") or (((_rebuilt or {}).get("primary_response") or {}).get("primary_metrics_canonical") if isinstance((_rebuilt or {}).get("primary_response"), dict) else None)
                                except Exception:
                                    _rebuilt_pmc = None
                                try:
                                    _rebuilt_schema = (_rebuilt or {}).get("metric_schema_frozen") or (((_rebuilt or {}).get("primary_response") or {}).get("metric_schema_frozen") if isinstance((_rebuilt or {}).get("primary_response"), dict) else None)
                                except Exception:
                                    _rebuilt_schema = None

                                if isinstance(_rebuilt_pmc, dict) and _rebuilt_pmc:
                                    _orig_wrapper["primary_metrics_canonical"] = _rebuilt_pmc
                                    _orig_wrapper.setdefault("primary_response", {})
                                    if isinstance(_orig_wrapper.get("primary_response"), dict):
                                        _orig_wrapper["primary_response"]["primary_metrics_canonical"] = _rebuilt_pmc
                                if isinstance(_rebuilt_schema, dict) and _rebuilt_schema:
                                    _orig_wrapper["metric_schema_frozen"] = _rebuilt_schema
                                    _orig_wrapper.setdefault("primary_response", {})
                                    if isinstance(_orig_wrapper.get("primary_response"), dict):
                                        _orig_wrapper["primary_response"]["metric_schema_frozen"] = _rebuilt_schema

                                analysis = _orig_wrapper
                            except Exception:
                                # worst-case fallback: keep rebuilt object
                                analysis = _rebuilt

                            try:
                                analysis.setdefault("debug", {})
                                if isinstance(analysis.get("debug"), dict):
                                    analysis["debug"]["refactor123_analysis_rebuild_v1"] = {
                                        "applied": True,
                                        "reason": "baseline_schema_null__rebuild_from_seeded_sources_cache",
                                        "sources_n": int(len(baseline_sources_cache)) if isinstance(baseline_sources_cache, list) else 0,
                                    }
                            except Exception:
                                pass
                except Exception:
                    pass
        except Exception:
            pass
        analysis["baseline_sources_cache"] = baseline_sources_cache
        analysis.setdefault("results", {})
        if isinstance(analysis["results"], dict):

            # - Captures persisted snapshot URLs + exact hash input URL set (A4-A5)
            # - Does NOT alter any gating/selection logic.
            try:
                _diag = {}
                if isinstance(web_context, dict):
                    _diag = web_context.get("diag_injected_urls") or web_context.get("extra_urls_debug") or {}

                _inj_urls = []
                try:
                    _inj_urls = _fix2d66_collect_injected_urls(web_context or {}, question_text=str((analysis or {}).get('question') or ''))
                except Exception:
                    pass
                    _inj_urls = []

                _snap_urls = _inj_diag_hash_inputs_from_bsc(baseline_sources_cache)
                _hash_inputs = _snap_urls

                _h_v1 = ""
                _h_v2 = ""
                try:
                    _h_v1 = compute_source_snapshot_hash(baseline_sources_cache)
                except Exception:
                    pass
                    _h_v1 = ""
                try:
                    _h_v2 = compute_source_snapshot_hash_v2(baseline_sources_cache)
                except Exception:
                    pass
                    _h_v2 = ""

                analysis.setdefault("results", {})
                if isinstance(analysis.get("results"), dict):
                    analysis["results"].setdefault("debug", {})
                    if isinstance(analysis["results"].get("debug"), dict):
                        analysis["results"]["debug"].setdefault("inj_diag", {})
                        analysis["results"]["debug"]["inj_diag"].update({
                            "run_id": str((web_context or {}).get("diag_run_id") or _diag.get("run_id") or ""),
                            "injected_urls": _inj_urls[:50],
                            "snapshot_pool_urls_count": int(len(_snap_urls)),
                            "snapshot_pool_urls_hash": _inj_diag_set_hash(_snap_urls),
                            "hash_input_urls_count": int(len(_hash_inputs)),
                            "hash_input_urls_hash": _inj_diag_set_hash(_hash_inputs),
                            "injected_in_snapshot_pool": sorted(list(set(_inj_urls) & set(_snap_urls)))[:50],
                            "injected_in_hash_inputs": sorted(list(set(_inj_urls) & set(_hash_inputs)))[:50],
                            "computed_hash_v1": _h_v1,
                            "computed_hash_v2": _h_v2,
                        })


                        # Location: analysis.results.debug.inj_trace_v1
                        try:

                            # Ensure inj_trace_v1 shows attempted/persisted evidence even when
                            # upstream diag_injected_urls is partial (e.g., baseline/no-injection).
                            # Diagnostics only.
                            try:
                                if isinstance(_diag, dict):
                                    _diag = _inj_trace_v1_enrich_diag_from_bsc(_diag, baseline_sources_cache)
                            except Exception:
                                pass

                            _trace = _inj_trace_v1_build(
                                diag_injected_urls=_diag if isinstance(_diag, dict) else {},
                                hash_inputs=_hash_inputs,
                                stage="analysis",
                                path="analysis",
                                rebuild_pool=None,
                                rebuild_selected=None,
                                hash_exclusion_reasons=(_inj_hash_reasons if isinstance(locals().get('_inj_hash_reasons'), dict) else {}),
                            )
                            analysis["results"]["debug"].setdefault("inj_trace_v1", {})
                            # Do not overwrite if already present; only fill/merge
                            if isinstance(analysis["results"]["debug"].get("inj_trace_v1"), dict):
                                analysis["results"]["debug"]["inj_trace_v1"].update(_trace)
                        except Exception:
                            pass

            except Exception:
                pass

        analysis["results"]["baseline_sources_cache"] = baseline_sources_cache


    # RANGE capture for canonical metrics
    pmc = analysis.get("primary_response", {}).get("primary_metrics_canonical") if isinstance(analysis.get("primary_response"), dict) else analysis.get("primary_metrics_canonical")
    schema = analysis.get("primary_response", {}).get("metric_schema_frozen") if isinstance(analysis.get("primary_response"), dict) else analysis.get("metric_schema_frozen")

    # Support both placements (your JSON seems to store these at top-level primary_response)
    if pmc is None and isinstance(analysis.get("primary_response"), dict):
        pmc = analysis["primary_response"].get("primary_metrics_canonical")
    if schema is None and isinstance(analysis.get("primary_response"), dict):
        schema = analysis["primary_response"].get("metric_schema_frozen")

    if isinstance(pmc, dict) and isinstance(schema, dict) and baseline_sources_cache:
        # flatten candidates
        all_cands = []
        for sr in baseline_sources_cache:
            for n in (sr.get("extracted_numbers") or []):
                if isinstance(n, dict):
                    all_cands.append(n)

        for ckey, m in pmc.items():
            if not isinstance(m, dict):
                continue
            mdef = schema.get(ckey) or {}
            uf = _unit_family_from_metric(mdef)
            keywords = mdef.get("keywords") or []

            kw_tokens = []
            for k in (keywords or []):
                kw_tokens.extend(_tokenize(str(k)))

            kw_tokens.extend(_tokenize(m.get("name") or m.get("original_name") or ""))
            kw_tokens = list(dict.fromkeys([t for t in kw_tokens if len(t) > 2]))[:40]

            vals = []
            examples = []

            for cand in all_cands:
                craw = str(cand.get("raw") or "")
                cunit = str(cand.get("unit") or "")
                ctx = str(cand.get("context_snippet") or cand.get("context") or "")

                # family gate
                cf = _cand_unit_family(cunit, craw)
                if uf == "PCT" and cf != "PCT":
                    continue
                if uf == "CUR" and cf != "CUR":
                    continue
                # MAG: allow MAG/OTHER but avoid CUR/PCT
                if uf == "MAG" and cf in ("CUR", "PCT"):
                    continue

                # NEW (additive): metric-aware magnitude gate
                if uf == "MAG":

                    cand_tag = _safe_norm_unit_tag(cunit or craw)
                    exp_tag = _safe_norm_unit_tag((mdef.get("unit") or "") or (m.get("unit") or ""))


                    if exp_tag in ("K", "M", "B", "T"):
                        if cand_tag != exp_tag:
                            continue
                    else:
                        if cand_tag not in ("K", "M", "B", "T"):
                            continue

                # token overlap gate
                c_tokens = set(_tokenize(ctx))
                if kw_tokens:
                    overlap = sum(1 for t in kw_tokens if t in c_tokens)
                    if overlap < max(1, min(3, len(kw_tokens) // 8)):
                        continue

                v = _parse_num(cand.get("value"), cunit) or _parse_num(craw, cunit)
                if v is None:
                    continue

                vals.append(float(v))
                if len(examples) < 5:
                    examples.append({
                        "raw": craw[:32],
                        "source_url": cand.get("source_url"),
                        "context_snippet": ctx[:180]
                    })

            if len(vals) >= 2:
                vmin = min(vals)
                vmax = max(vals)
                if abs(vmax - vmin) > max(1e-9, abs(vmin) * 0.02):
                    m["value_range"] = {
                        "min": vmin,
                        "max": vmax,
                        "n": len(vals),
                        "examples": examples,
                        "method": "snapshot_candidates"
                    }
                    try:
                        unit_disp = m.get("unit") or ""
                        m["value_range_display"] = f"{vmin:g}–{vmax:g} {unit_disp}".strip()
                    except Exception:
                        pass
    try:
        _res = analysis.get("results") if isinstance(analysis, dict) else None
        if isinstance(_res, dict):
            for _ck, _m in _res.items():
                if not isinstance(_m, dict):
                    continue
                _ev = _m.get("evidence")
                if not isinstance(_ev, list) or len(_ev) < 2:
                    continue
                _vals = []
                for _e in _ev:
                    if not isinstance(_e, dict):
                        continue
                    _v = _e.get("value_norm")
                    if _v is None:
                        _v = _e.get("value")
                    if isinstance(_v, (int, float)):
                        try:
                            _vals.append(float(_v))
                        except Exception:
                            pass
                if len(_vals) < 2:
                    continue
                _vmin = min(_vals); _vmax = max(_vals)
                if abs(_vmax - _vmin) <= max(1e-9, abs(_vmin) * 0.02):
                    continue
                _m["value_range"] = {"min": _vmin, "max": _vmax, "n": len(_vals), "method": "ph2b_schema_unit_range_v1|fix2b_range2"}
                try:
                    _unit_disp = _m.get("unit") or _m.get("unit_tag") or ""
                    _m["value_range_display"] = f"{_vmin:g}–{_vmax:g} {_unit_disp}".strip()
                except Exception:
                    pass
    except Exception:
        pass


    #
    # Why:
    # - We observed value_range values scaled as if converted to billions (e.g., 17.8M -> 0.0178)
    #   while still displaying "million units", causing downstream eligibility drift.
    # - FIX2B_RANGE2 only patches analysis["results"] (often empty); the dashboard-facing
    #   canonicals live under analysis["primary_response"]["primary_metrics_canonical"].
    #
    # What:
    # - Rebuild value_range from baseline_sources_cache extracted_numbers, treating candidate.value_norm
    #   as schema units (NO double scaling), constrained to the metric's chosen source_url.
    # - Pure post-processing: NO IO, NO refetch, NO hashing changes.
    try:
        import re
        _pr = analysis.get("primary_response") if isinstance(analysis, dict) else None
        _pmc = _pr.get("primary_metrics_canonical") if isinstance(_pr, dict) else None
        _schema = (
            (analysis.get("metric_schema_frozen") if isinstance(analysis, dict) else None)
            or (_pr.get("metric_schema_frozen") if isinstance(_pr, dict) else None)
            or {}
        )
        _bsc = analysis.get("baseline_sources_cache") if isinstance(analysis, dict) else None
        if isinstance(_pmc, dict) and isinstance(_schema, dict) and isinstance(_bsc, list) and _bsc:
            # Flatten candidate universe from snapshots
            _flat = []
            for _src in _bsc:
                if not isinstance(_src, dict):
                    continue
                _nums = _src.get("extracted_numbers")
                if isinstance(_nums, list):
                    _flat.extend([n for n in _nums if isinstance(n, dict)])
            # Helper: scale evidence presence for common magnitudes
            def _ph2b_scale_token_ok(_spec_unit_tag: str, _cand: dict) -> bool:
                try:
                    sut = str(_spec_unit_tag or "").lower()
                    if not sut:
                        return True
                    # Only enforce for explicit scaled magnitudes
                    if ("million" not in sut) and ("billion" not in sut) and ("thousand" not in sut) and ("trillion" not in sut):
                        return True
                    raw = str(_cand.get("raw") or "").lower()
                    ut = str(_cand.get("unit_tag") or _cand.get("unit") or "").lower()
                    ctx = str(_cand.get("context_snippet") or "").lower()
                    blob = " ".join([raw, ut, ctx])
                    if "million" in sut:
                        return ("million" in blob) or re.search(r"\bm\b", blob) is not None or " mn" in blob
                    if "billion" in sut:
                        return ("billion" in blob) or re.search(r"\bb\b", blob) is not None or " bn" in blob
                    if "thousand" in sut:
                        return ("thousand" in blob) or re.search(r"\bk\b", blob) is not None
                    if "trillion" in sut:
                        return ("trillion" in blob) or re.search(r"\bt\b", blob) is not None
                except Exception:
                    return True

            for _ck, _m in list(_pmc.items()):
                if not isinstance(_m, dict):
                    continue
                _spec = _schema.get(_ck) if isinstance(_schema, dict) else None
                if not isinstance(_spec, dict):
                    continue
                _src_url = _m.get("source_url") or _spec.get("preferred_url") or ""
                if not _src_url:
                    continue
                _src_url_n = _ph2b_norm_url(_src_url)
                # Collect eligible vals from same source_url
                _vals = []
                _examples = []
                for _c in _flat:
                    try:
                        if _ph2b_norm_url(_c.get("source_url") or "") != _src_url_n:
                            continue
                        if _c.get("is_junk") is True:
                            continue
                        # Reuse FIX16 allowlist if present
                        try:
                            if callable(globals().get("_fix16_candidate_allowed")):
                                if not globals().get("_fix16_candidate_allowed")(_c, _spec, canonical_key=_ck):
                                    continue
                        except Exception:
                            pass
                        # Enforce scale token for scaled magnitudes
                        if not _ph2b_scale_token_ok(_spec.get("unit_tag") or _spec.get("unit") or "", _c):
                            continue
                        _v = _c.get("value_norm")
                        if _v is None:
                            _v = _c.get("value")
                        if isinstance(_v, (int, float)):
                            _vals.append(float(_v))
                            if len(_examples) < 4:
                                _examples.append({
                                    "raw": _c.get("raw"),
                                    "source_url": _c.get("source_url"),
                                    "context_snippet": (str(_c.get("context_snippet") or "")[:180])
                                })
                    except Exception:
                        pass
                        continue
                if len(_vals) < 2:
                    continue
                _vmin = min(_vals); _vmax = max(_vals)
                if abs(_vmax - _vmin) <= max(1e-9, abs(_vmin) * 0.02):
                    continue
                _m["value_range"] = {
                    "min": _vmin,
                    "max": _vmax,
                    "n": len(_vals),
                    "examples": _examples,
                    "method": "ph2b_schema_unit_range_v2|fix2b_range3"
                }
                try:
                    _unit_disp = _m.get("unit") or _m.get("unit_tag") or _spec.get("unit_tag") or ""
                    _m["value_range_display"] = f"{_vmin:g}–{_vmax:g} {_unit_disp}".strip()
                except Exception:
                    pass
    except Exception:
        pass

    #
    # Location:
    # - This runs AFTER the legacy "snapshot_candidates" range builder blocks above.
    #
    # Goal:
    # - Ensure value_range is computed in *schema units* (treat candidate.value_norm as schema units),
    #   avoiding any double-scaling/divide behavior.
    # - Constrain range computation to the metric's chosen current source_url when present,
    #   preventing cross-source range pollution.
    #
    # Notes:
    # - Downstream-only post-processing. Does NOT affect selection, only range/min/max display.
    # - Safe even when evidence is missing: it simply no-ops.
    try:
        _pr = analysis.get("primary_response") if isinstance(analysis, dict) else None
        _pmc = _pr.get("primary_metrics_canonical") if isinstance(_pr, dict) else None
        _schema = (
            (analysis.get("metric_schema_frozen") if isinstance(analysis, dict) else None)
            or (_pr.get("metric_schema_frozen") if isinstance(_pr, dict) else None)
            or {}
        )
        _bsc = analysis.get("baseline_sources_cache") if isinstance(analysis, dict) else None

        if isinstance(_pmc, dict) and isinstance(_schema, dict) and isinstance(_bsc, list) and _bsc:
            # Flatten candidates once
            _flat = []
            for _src in _bsc:
                if not isinstance(_src, dict):
                    continue
                _nums = _src.get("extracted_numbers")
                if isinstance(_nums, list):
                    _flat.extend([n for n in _nums if isinstance(n, dict)])

            def _ph2b_unit_tag_norm(x: str) -> str:
                return _safe_norm_unit_tag(str(x or ""))

            for _ckey, _m in _pmc.items():
                if not isinstance(_m, dict):
                    continue

                _mdef = _schema.get(_ckey) if isinstance(_schema, dict) else None
                _mdef = _mdef if isinstance(_mdef, dict) else {}
                _exp_unit = str(_mdef.get("unit") or _m.get("unit") or _m.get("unit_tag") or "")
                _exp_tag = _ph2b_unit_tag_norm(_exp_unit)

                # If the metric already has a source_url, treat that as the range scope
                _scope_url = _m.get("cur_source_url") or _m.get("source_url") or ""
                _scope_url_n = _norm_url(_scope_url) if _scope_url else ""

                _vals = []
                for _cand in _flat:
                    if not isinstance(_cand, dict):
                        continue
                    # source scope (if known)
                    if _scope_url_n:
                        _c_url = _cand.get("source_url") or ""
                        if _norm_url(_c_url) != _scope_url_n:
                            continue
                    # unit_tag scope (only enforce when schema declares a scaled magnitude tag)
                    if _exp_tag:
                        _cand_tag = _ph2b_unit_tag_norm(_cand.get("unit_tag") or _cand.get("unit") or _cand.get("raw") or "")
                        if _exp_tag in ("K", "M", "B", "T") and _cand_tag != _exp_tag:
                            continue
                    _vn = _cand.get("value_norm")
                    if _vn is None:
                        continue
                    if isinstance(_vn, (int, float)):
                        try:
                            _vals.append(float(_vn))
                        except Exception:
                            pass

                if len(_vals) >= 2:
                    _vmin = min(_vals); _vmax = max(_vals)
                    if abs(_vmax - _vmin) > max(1e-9, abs(_vmin) * 0.02):
                        _m["value_range"] = {
                            "min": _vmin,
                            "max": _vmax,
                            "n": len(_vals),
                            "method": "ph2b_schema_unit_range_v2|fix2b_range4",
                            "scope_url": _scope_url_n or "",
                            "scope_unit_tag": _exp_tag or ""
                        }
                        try:
                            _unit_disp = _m.get("unit") or _m.get("unit_tag") or _exp_unit or ""
                            _m["value_range_display"] = f"{_vmin:g}–{_vmax:g} {_unit_disp}".strip()
                        except Exception:
                            pass
    except Exception:
        pass


    # - attach_source_snapshots_to_analysis receives the *top-level* output dict,
    #   where schema/pmc live under primary_response.
    # - Prior FIX2D31/FIX2D38 baseline_schema_metrics_v1 builder reads analysis.*
    #   and therefore never ran, dropping baseline_schema_metrics_v1 from JSON.
    # - This shim mirrors metric_schema_frozen / primary_metrics_canonical / metric_anchors
    #   from primary_response into top-level analysis so the existing builder executes.
    # - Additive only: does not change selector semantics.
    try:
        _pr = analysis.get('primary_response')
        if isinstance(_pr, dict):
            if not isinstance(analysis.get('metric_schema_frozen'), dict) and isinstance(_pr.get('metric_schema_frozen'), dict):
                analysis['metric_schema_frozen'] = _pr.get('metric_schema_frozen')
            if not isinstance(analysis.get('primary_metrics_canonical'), dict) and isinstance(_pr.get('primary_metrics_canonical'), dict):
                analysis['primary_metrics_canonical'] = _pr.get('primary_metrics_canonical')
            if not isinstance(analysis.get('metric_anchors'), dict) and isinstance(_pr.get('metric_anchors'), dict):
                analysis['metric_anchors'] = _pr.get('metric_anchors')
    except Exception:
        pass


    #
    # Problem observed:
    # - Some Analysis runs (especially narrative questions) emit no metric_schema_frozen at all.
    # - Evolution schema-only rebuild depends on prev_response.metric_schema_frozen keyspace;
    #   when missing, rebuild returns empty and Diff Panel V2 shows an empty metric change table.
    #
    # Policy:
    # - Always ensure metric_schema_frozen exists at top-level analysis output.
    # - Seed deterministically using known schema extension functions when missing/empty.
    # - No fetch, no heuristic matching.
    try:
        _schema0 = analysis.get('metric_schema_frozen')
        if not isinstance(_schema0, dict) or not _schema0:
            _schema_seed = {}
            for _fn_name in (
                '_fix2u_extend_metric_schema_ev_chargers',
                '_fix2v_extend_metric_schema_ev_chargers_cagr',
                '_fix2ab_extend_metric_schema_global_ev_sales_ytd_2025',
            ):
                try:
                    _fn = globals().get(_fn_name)
                    if callable(_fn):
                        _schema_seed = _fn(_schema_seed) or _schema_seed
                except Exception:
                    pass
            analysis['metric_schema_frozen'] = _schema_seed
            try:
                if not isinstance(analysis.get('debug'), dict):
                    analysis['debug'] = {}
                analysis['debug']['fix2d65d_seeded_schema'] = True
                analysis['debug']['fix2d65d_schema_keys'] = int(len(_schema_seed)) if isinstance(_schema_seed, dict) else 0
            except Exception:
                pass
    except Exception:
        pass


    #
    # Problem:
    # - Some narrative / market-size queries return no primary_metrics in primary_response.
    # - When injected URLs are present, we still want schema+baseline canonicals to materialise
    #   so Evolution schema-only rebuild and Diff Panel V2 can activate.
    #
    # Policy:
    # - If injected URLs exist and metric_schema_frozen is empty/missing, seed an empty schema and
    #   apply deterministic schema extension patches. This enables FIX2D31 schema-authority rebuild
    #   to run over the baseline candidate universe without changing UI/diff logic.
    #
    # Safety:
    # - No refetch, no heuristic matching. Additive only.
    try:
        # Collect injected URLs from UI/diagnostic fields only (plus explicit internal marker fallback)
        # to avoid misclassifying production source lists as "injected".
        _inj_urls = _yureeka_extract_injected_urls_v1(web_context)

        _has_inj = bool(_inj_urls)
        if _has_inj:
            try:
                analysis.setdefault("debug", {})
                if isinstance(analysis.get("debug"), dict):
                    analysis["debug"]["fix2d65b_forced_canonical_pipeline"] = True
                    analysis["debug"]["fix2d65b_injected_urls"] = _inj_urls[:10]
                    analysis["debug"]["injected_urls_v1"] = _inj_urls[:10]
            except Exception:
                pass

        # Ensure metric_schema_frozen exists even when LLM emitted no primary_metrics.
        # NOTE: We no longer gate this on injected URLs: Evolution schema-only rebuild
        # requires a baseline keyspace, and the deterministic schema extensions are safe.
        _schema0 = analysis.get("metric_schema_frozen")
        if not isinstance(_schema0, dict) or not _schema0:
            _schema_seed = {}

            # Apply known deterministic schema extensions (additive)
            for _fn_name in (
                "_fix2u_extend_metric_schema_ev_chargers",
                "_fix2v_extend_metric_schema_ev_chargers_cagr",
                "_fix2ab_extend_metric_schema_global_ev_sales_ytd_2025",
            ):
                try:
                    _fn = globals().get(_fn_name)
                    if callable(_fn):
                        _schema_seed = _fn(_schema_seed) or _schema_seed
                except Exception:
                    pass

            analysis["metric_schema_frozen"] = _schema_seed
    except Exception:
        pass

    # - When metric_schema_frozen exists in Analysis, rebuild primary_metrics_canonical
    #   by running the authoritative Analysis selector (_analysis_canonical_final_selector_v1)
    #   constrained to the frozen schema keys, over the same baseline candidate universe.
    # - This makes Analysis emit schema-aligned baseline metrics so Evolution injection can overlap
    #   and Diff Panel V2 can activate without weakening semantics.
    try:
        _core = analysis
        schema = _core.get("metric_schema_frozen")
        pmc = _core.get("primary_metrics_canonical")
        sel = globals().get("_analysis_canonical_final_selector_v1")
        if isinstance(schema, dict) and schema and callable(sel):
            # Preserve original unconstrained PMC for diagnostics
            if isinstance(pmc, dict) and pmc and "primary_metrics_canonical_unconstrained_v0" not in analysis:
                _core["primary_metrics_canonical_unconstrained_v0"] = dict(pmc)

            # Flatten candidate universe from baseline_sources_cache + web_context.scraped_meta
            flat = []
            bsc = analysis.get("baseline_sources_cache")
            if isinstance(bsc, list):
                for src in bsc:
                    if isinstance(src, dict) and isinstance(src.get("extracted_numbers"), list):
                        flat.extend([n for n in src.get("extracted_numbers") if isinstance(n, dict)])
            if isinstance(web_context, dict):
                sm = web_context.get("scraped_meta")
                if isinstance(sm, dict) and isinstance(sm.get("extracted_numbers"), list):
                    flat.extend([n for n in sm.get("extracted_numbers") if isinstance(n, dict)])

            # Best-effort canonicalize candidates (if helper exists)
            try:
                fn_can = globals().get("canonicalize_numeric_candidate")
                if callable(fn_can):
                    flat = [fn_can(dict(n)) if isinstance(n, dict) else n for n in flat]
            except Exception:
                pass

            anchors = (_core.get("metric_anchors") or analysis.get("metric_anchors"))
            if not isinstance(anchors, dict):
                anchors = {}

            new_pmc = {}
            sel_trace = {}
            for ckey in sorted(schema.keys()):
                spec = schema.get(ckey) or {}
                if not isinstance(spec, dict):
                    continue
                try:
                    out_m, meta = sel(
                        ckey,
                        spec,
                        flat,
                        anchors=anchors,
                        prev_metric=(pmc or {}).get(ckey) if isinstance(pmc, dict) else None,
                        web_context=web_context,
                    )
                    if isinstance(out_m, dict):
                        # FIX2D33: baseline commitment for schema keys.
                        # If selector chose a winner but value_norm is missing, deterministically parse from value/raw.
                        if out_m.get("value_norm") is None:
                            try:
                                fn_parse = globals().get("_parse_num")
                                if callable(fn_parse):
                                    unit_hint = str(out_m.get("unit") or "")
                                    v_try = fn_parse(out_m.get("value"), unit_hint)
                                    if v_try is None:
                                        # fall back to evidence raw
                                        ev0 = None
                                        if isinstance(out_m.get("evidence"), list) and out_m.get("evidence"):
                                            ev0 = out_m.get("evidence")[0]
                                        if isinstance(ev0, dict):
                                            v_try = fn_parse(ev0.get("raw"), unit_hint)
                                    if v_try is not None:
                                        try:
                                            out_m["value_norm"] = float(v_try)
                                        except Exception:
                                            pass
                                            out_m["value_norm"] = v_try
                            except Exception:
                                pass
                        if out_m.get("value_norm") is not None:
                            # FIX2D39: schema-authoritative dimension/unit_family hard-binding.
                            # - If winner has a concrete (non-unknown) dimension/unit_family that CONFLICTS with schema, reject it.
                            # - Otherwise, set dimension/unit_family to schema spec (authoritative) and stamp audit flags.
                            try:
                                spec_dim = ""
                                spec_uf = ""
                                if isinstance(spec, dict):
                                    spec_dim = str(spec.get("dimension") or spec.get("dim") or "").strip()
                                    spec_uf = str(spec.get("unit_family") or spec.get("unitFamily") or "").strip()
                                prior_dim = out_m.get("dimension")
                                prior_uf = out_m.get("unit_family")
                                prior_dim_s = ("" if prior_dim is None else str(prior_dim).strip())
                                prior_uf_s = ("" if prior_uf is None else str(prior_uf).strip())

                                # Reject conflicts: non-unknown prior value disagrees with schema.
                                if spec_dim and prior_dim_s and prior_dim_s not in ("unknown",) and prior_dim_s != spec_dim:
                                    meta = dict(meta) if isinstance(meta, dict) else {}
                                    meta["fix2d39_schema_dim_conflict_v1"] = True
                                    meta["fix2d39_schema_dim_prior_v1"] = prior_dim_s
                                    meta["fix2d39_schema_dim_spec_v1"] = spec_dim
                                    sel_trace[ckey] = meta
                                    continue
                                if spec_uf and prior_uf_s and prior_uf_s not in ("unknown",) and prior_uf_s != spec_uf:
                                    meta = dict(meta) if isinstance(meta, dict) else {}
                                    meta["fix2d39_schema_uf_conflict_v1"] = True
                                    meta["fix2d39_schema_uf_prior_v1"] = prior_uf_s
                                    meta["fix2d39_schema_uf_spec_v1"] = spec_uf
                                    sel_trace[ckey] = meta
                                    continue

                                # Apply schema authority (overwrite empty/unknown, and also normalize same-values).
                                if spec_dim:
                                    if prior_dim_s != spec_dim:
                                        out_m["dimension_prior_v1"] = prior_dim
                                        out_m["dimension_coerced_from_schema_v1"] = True
                                    out_m["dimension"] = spec_dim
                                if spec_uf:
                                    if prior_uf_s != spec_uf:
                                        out_m["unit_family_prior_v1"] = prior_uf
                                        out_m["unit_family_coerced_from_schema_v1"] = True
                                    out_m["unit_family"] = spec_uf
                            except Exception:
                                pass
                            new_pmc[ckey] = out_m
                    sel_trace[ckey] = meta if isinstance(meta, dict) else {"blocked_reason": "no_meta"}
                except Exception as _e:
                    sel_trace[ckey] = {"blocked_reason": "selector_exception", "error": str(_e)[:200]}

            # FIX2D38: Always build a schema-keyed baseline map for diffing.
            # by filling missing schema keys from flat candidates as proxy baselines. This ensures the prev universe
            # is schema-keyed and numeric where possible, so baseline diffing can activate.
            baseline_schema = {}
            try:
                if isinstance(new_pmc, dict) and new_pmc:
                    baseline_schema.update(new_pmc)
            except Exception:
                pass
            # FIX2D40: Remap baseline metrics from generic canonical keys onto schema canonical keys (one-to-one).
            # Rationale: Analysis may emit strong baseline metrics under generic keys (e.g., "2025_global_ev_sales__unknown")
            # while Evolution/injection emits schema keys (e.g., "global_ev_sales_ytd_2025__unit_sales"). Without remap,
            # prev_value_norm stays null and diff cannot activate.
            try:
                pmc_src = analysis.get('primary_metrics_canonical')
                used_src_keys = set()
                if isinstance(schema, dict) and isinstance(pmc_src, dict):
                    def _fix2d40_score(_ck, _m, _schema_key, _spec):
                        try:
                            txt = " ".join([
                                str(_ck or ""),
                                str(_m.get('name') or ""),
                                str(_m.get('context_snippet') or ""),
                                str(_m.get('source_url') or ""),
                            ]).lower()
                        except Exception:
                            pass
                            txt = str(_ck or "").lower()
                        s = 0
                        # Basic topic cues
                        if 'sales' in _schema_key and 'sales' in txt:
                            s += 3
                        if 'chargers' in _schema_key and ('charger' in txt or 'charging' in txt):
                            s += 3
                        if 'cagr' in _schema_key and ('cagr' in txt or 'growth' in txt or 'rate' in txt):
                            s += 3
                        if 'investment' in _schema_key and ('invest' in txt or 'capex' in txt):
                            s += 3
                        # Year cues
                        if '2025' in _schema_key and '2025' in txt:
                            s += 2
                        if '2040' in _schema_key and '2040' in txt:
                            s += 2
                        if '2026' in _schema_key and '2026' in txt:
                            s += 1
                        if '2030' in _schema_key and '2030' in txt:
                            s += 1
                        # Penalize obvious mismatches
                        if 'china' in txt and 'global' in _schema_key:
                            s -= 5
                        # Unit-family compatibility quick check
                        try:
                            spec_dim = str((_spec or {}).get('dimension') or '').strip().lower()
                            spec_uf = str((_spec or {}).get('unit_family') or (_spec or {}).get('unitFamily') or '').strip().lower()
                            prior_uf = str(_m.get('unit_family') or '').strip().lower()
                            if spec_uf and prior_uf and prior_uf not in ('unknown',) and prior_uf != spec_uf:
                                s -= 4
                            prior_dim = str(_m.get('dimension') or '').strip().lower()
                            if spec_dim and prior_dim and prior_dim not in ('unknown',) and prior_dim != spec_dim:
                                s -= 4
                        except Exception:
                            pass
                        # Prefer numeric
                        if _m.get('value_norm') is not None:
                            s += 2
                        return s

                    # For each schema key missing from baseline_schema, pick best candidate from pmc_src
                    for _skey, _spec in schema.items():
                        if not _skey or _skey in baseline_schema:
                            continue
                        best_k = None
                        best_m = None
                        best_score = None
                        for _ck, _m in pmc_src.items():
                            if _ck in used_src_keys:
                                continue
                            if not isinstance(_m, dict):
                                continue
                            sc = _fix2d40_score(_ck, _m, _skey, _spec)
                            if best_score is None or sc > best_score:
                                best_score = sc
                                best_k = _ck
                                best_m = _m
                        # Require a minimum positive score to avoid accidental remaps
                        if best_m is None or (best_score is None) or (best_score < 4):
                            continue

                        out_m = dict(best_m)
                        # Mark remap audit
                        out_m['schema_remap_v1'] = True
                        out_m['schema_remap_from_ckey_v1'] = best_k
                        out_m['schema_remap_to_ckey_v1'] = _skey
                        out_m['schema_remap_score_v1'] = best_score
                        out_m['is_proxy'] = True
                        out_m['proxy_type'] = 'schema_remap'
                        out_m['proxy_reason'] = 'schema_remap'

                        # Enforce FIX2D39 hard-binding against schema (reject conflicts)
                        try:
                            spec_dim = str((_spec or {}).get('dimension') or (_spec or {}).get('dim') or '').strip()
                            spec_uf = str((_spec or {}).get('unit_family') or (_spec or {}).get('unitFamily') or '').strip()
                            prior_dim = out_m.get('dimension')
                            prior_uf = out_m.get('unit_family')
                            prior_dim_s = ('' if prior_dim is None else str(prior_dim).strip())
                            prior_uf_s = ('' if prior_uf is None else str(prior_uf).strip())
                            if spec_dim and prior_dim_s and prior_dim_s not in ('unknown',) and prior_dim_s != spec_dim:
                                continue
                            if spec_uf and prior_uf_s and prior_uf_s not in ('unknown',) and prior_uf_s != spec_uf:
                                continue
                            if spec_dim:
                                if prior_dim_s != spec_dim:
                                    out_m['dimension_prior_v1'] = prior_dim
                                    out_m['dimension_coerced_from_schema_v1'] = True
                                out_m['dimension'] = spec_dim
                            if spec_uf:
                                if prior_uf_s != spec_uf:
                                    out_m['unit_family_prior_v1'] = prior_uf
                                    out_m['unit_family_coerced_from_schema_v1'] = True
                                out_m['unit_family'] = spec_uf
                        except Exception:
                            pass

                        # Override canonical key fields if present
                        try:
                            out_m['canonical_key'] = _skey
                            out_m['ckey'] = _skey
                        except Exception:
                            pass

                        baseline_schema[_skey] = out_m
                        used_src_keys.add(best_k)
            except Exception:
                pass

            # Fill gaps with schema fallback from flat candidates (proxy baselines)
            try:
                if isinstance(schema, dict) and isinstance(flat, list):
                    for ckey, spec in schema.items():
                        if not ckey or ckey in baseline_schema:
                            continue
                        best = None
                        for cand in flat:
                            try:
                                if not isinstance(cand, dict):
                                    continue
                                ck = cand.get('canonical_key') or cand.get('ckey') or cand.get('canon_key')
                                if ck != ckey:
                                    continue
                                # Prefer numeric candidates
                                if cand.get('value_norm') is not None:
                                    best = cand
                                    break
                                if best is None:
                                    best = cand
                            except Exception:
                                pass
                                continue
                        if best is None:
                            continue
                        out_m = dict(best)
                        # Ensure normalized numeric when possible (reuse FIX2D33 parsing helper if present)
                        try:
                            if out_m.get('value_norm') is None:
                                _vraw = out_m.get('value')
                                if (_vraw is None) and isinstance(out_m.get('evidence'), list) and out_m['evidence']:
                                    _ev0 = out_m['evidence'][0]
                                    if isinstance(_ev0, dict):
                                        _vraw = _ev0.get('raw') or _ev0.get('text')
                                if _vraw is not None:
                                    try:
                                        _parsed = _fix2d33_parse_value_norm_v1(_vraw, out_m) if ' _fix2d33_parse_value_norm_v1'.strip() else None
                                    except Exception:
                                        pass
                                        _parsed = None
                                    if _parsed is not None:
                                        out_m['value_norm'] = _parsed
                            # FIX2D39: schema-authoritative dimension/unit_family hard-binding (fallback path)
                            spec_dim = ''
                            spec_uf = ''
                            if isinstance(spec, dict):
                                spec_dim = str(spec.get('dimension') or spec.get('dim') or '').strip()
                                spec_uf = str(spec.get('unit_family') or spec.get('unitFamily') or '').strip()
                            prior_dim = out_m.get('dimension')
                            prior_uf = out_m.get('unit_family')
                            prior_dim_s = ('' if prior_dim is None else str(prior_dim).strip())
                            prior_uf_s = ('' if prior_uf is None else str(prior_uf).strip())

                            # Reject conflicts: non-unknown prior value disagrees with schema.
                            if spec_dim and prior_dim_s and prior_dim_s not in ('unknown',) and prior_dim_s != spec_dim:
                                continue
                            if spec_uf and prior_uf_s and prior_uf_s not in ('unknown',) and prior_uf_s != spec_uf:
                                continue

                            # Apply schema authority.
                            if spec_dim:
                                if prior_dim_s != spec_dim:
                                    out_m['dimension_coerced_from_schema_v1'] = True
                                    out_m['dimension_prior_v1'] = prior_dim
                                out_m['dimension'] = spec_dim
                            if spec_uf:
                                if prior_uf_s != spec_uf:
                                    out_m['unit_family_coerced_from_schema_v1'] = True
                                    out_m['unit_family_prior_v1'] = prior_uf
                                out_m['unit_family'] = spec_uf
                        except Exception:
                            pass
                        # Mark as proxy baseline fallback
                        out_m['is_proxy'] = True
                        out_m['proxy_type'] = 'schema_fallback'
                        out_m['proxy_reason'] = 'schema_fallback'
                        baseline_schema[ckey] = out_m
            except Exception:
                pass

            # FIX2D71: Always commit a schema-keyed baseline PMC for downstream diffing.
            # - If selector produced schema winners (new_pmc), commit those.
            # - Otherwise, if we have a schema-keyed baseline map (baseline_schema) and PMC is empty,
            #   commit the proxy baseline map so Evolution has prev canonical metrics to diff against.
            # This does NOT reintroduce heuristics: baseline_schema is schema-keyed and fully auditable via
            # is_proxy/proxy_reason flags.
            if new_pmc:
                _core['primary_metrics_canonical'] = new_pmc
                try:
                    if not isinstance(analysis.get('results'), dict):
                        analysis['results'] = {}
                    analysis.setdefault('results', {}).setdefault('debug', {})
                    if isinstance(analysis['results'].get('debug'), dict):
                        analysis['results']['debug']['fix2d71_committed_pmc_mode'] = 'selector_winners'
                except Exception:
                    pass
            else:
                try:
                    _pmc0 = _core.get('primary_metrics_canonical')
                    _pmc0_empty = (not isinstance(_pmc0, dict)) or (not _pmc0)
                    if _pmc0_empty and isinstance(baseline_schema, dict) and baseline_schema:
                        _core['primary_metrics_canonical'] = dict(baseline_schema)
                        # Stamp audit flag at top-level (lightweight)
                        try:
                            if not isinstance(_core.get('debug'), dict):
                                _core['debug'] = {}
                            if isinstance(_core.get('debug'), dict):
                                _core['debug']['fix2d71_committed_proxy_baseline_pmc_v1'] = True
                                _core['debug']['fix2d71_proxy_baseline_pmc_count_v1'] = len(baseline_schema)
                        except Exception:
                            pass
                        try:
                            if not isinstance(analysis.get('results'), dict):
                                analysis['results'] = {}
                            analysis.setdefault('results', {}).setdefault('debug', {})
                            if isinstance(analysis['results'].get('debug'), dict):
                                analysis['results']['debug']['fix2d71_committed_pmc_mode'] = 'proxy_baseline_schema_map'
                                analysis['results']['debug']['fix2d71_proxy_baseline_pmc_count_v1'] = len(baseline_schema)
                        except Exception:
                            pass
                except Exception:
                    pass

            # FIX2D38: emit schema-keyed baseline metrics map for diffing (always, with schema fallback).
            if baseline_schema:
                try:
                    _core['baseline_schema_metrics_v1'] = dict(baseline_schema)
                except Exception:
                    pass
                    _core['baseline_schema_metrics_v1'] = baseline_schema
                try:
                    if not isinstance(analysis.get('results'), dict):
                        analysis['results'] = {}
                    analysis['results']['baseline_schema_metrics_v1'] = _core.get('baseline_schema_metrics_v1')
                    try:
                        if _core is not analysis and isinstance(_core, dict) and 'baseline_schema_metrics_v1' in _core:
                            # keep it visible under primary_response for inspection
                            pass
                    except Exception:
                        pass
                except Exception:
                    pass

            # Debug trace
            try:
                if not isinstance(analysis.get('results'), dict):
                    analysis['results'] = {}
                if not isinstance(analysis['results'].get('debug'), dict):
                    analysis['results']['debug'] = {}
                analysis['results']['debug']['fix2d38_schema_baseline_map'] = {
                    'enabled': True,
                    'schema_key_count': int(len(schema)) if isinstance(schema, dict) else 0,
                    'flat_candidate_count': int(len(flat)) if isinstance(flat, list) else 0,
                    'selected_schema_count': int(len(new_pmc)) if isinstance(new_pmc, dict) else 0,
                    'baseline_schema_count': int(len(baseline_schema)) if isinstance(baseline_schema, dict) else 0,
                }
            except Exception:
                pass
    except Exception:
        pass
    # - Pure metadata, NO logic impact
    # - Allows downstream drift attribution:
    #     * pipeline changes vs source changes
    analysis.setdefault("analysis_pipeline_version", "v7_41_endstate_wip_1")
    analysis.setdefault("metric_identity_version", "canon_v2_dim_safe")
    analysis.setdefault("schema_freeze_version", 1)

    analysis.setdefault("code_version", _yureeka_get_code_version())

    # REFACTOR124 beacon: ensure Analysis wrapper shape is preserved for HistoryFull selection
    try:
        analysis.setdefault("debug", {})
        if isinstance(analysis.get("debug"), dict):
            _pr = (analysis or {}).get("primary_response") if isinstance((analysis or {}).get("primary_response"), dict) else {}
            analysis["debug"]["analysis_wrapper_shape_v1"] = {
                "has_question": bool(str((analysis or {}).get("question") or "").strip()),
                "has_timestamp": bool(str((analysis or {}).get("timestamp") or "").strip()),
                "has_question_profile": bool(isinstance((analysis or {}).get("question_profile"), dict) and bool((analysis or {}).get("question_profile"))),
                "pmc_top_n": int(len((analysis or {}).get("primary_metrics_canonical") or {})) if isinstance((analysis or {}).get("primary_metrics_canonical"), dict) else 0,
                "pmc_primary_response_n": int(len((_pr or {}).get("primary_metrics_canonical") or {})) if isinstance((_pr or {}).get("primary_metrics_canonical"), dict) else 0,
            }
    except Exception:
        pass


    return analysis


def normalize_unit(unit: str) -> str:
    """Normalize unit to one of: T/B/M/%/'' (deterministic)."""
    if not unit:
        return ""
    u = unit.strip().upper().replace("USD", "").replace("$", "").replace(" ", "")
    if u in ["TRILLION", "T"]:
        return "T"
    if u in ["BILLION", "B"]:
        return "B"
    if u in ["MILLION", "M"]:
        return "M"
    if u in ["PERCENT", "%"]:
        return "%"
    if u in ["K", "THOUSAND"]:
        return "K"
    return u

def normalize_currency_prefix(raw: str) -> bool:
    """True if looks like a currency number ($/USD)."""
    if not raw:
        return False
    s = raw.strip().upper()
    return s.startswith("$") or " USD" in s or s.startswith("USD")

def is_likely_junk_context(ctx: str) -> bool:
    """
    Returns True if a context snippet strongly indicates the number is coming from
    HTML/JS/CSS/asset junk (srcset resize params, scripts, svg path data, etc.)
    rather than real narrative/tabular data.
    """
    import re

    c = (ctx or "").strip()
    if not c:
        return True

    cl = c.lower()

    # Too much binary / garbled text (common when PDF bytes leak through)
    non_print = sum(1 for ch in c if ord(ch) < 9 or (13 < ord(ch) < 32))
    if non_print > 0:
        return True

    # Lots of replacement chars / unusual glyphs → decode garbage
    bad_glyphs = c.count("\ufffd")
    if bad_glyphs >= 1:
        return True

    # Very long uninterrupted “code-ish” context
    if len(c) > 260 and ("{" in c and "}" in c) and ("function" in cl or "var " in cl or "const " in cl):
        return True

    # Hard “asset / markup / script” indicators
    hard_hints = [
        "srcset=", "resize=", "quality=", "offsc", "offscreencanvas", "createelement(\"canvas\")",
        "willreadfrequently", "function(", "webpack", "window.", "document.", "var ", "const ",
        "<script", "</script", "<style", "</style", "text/javascript", "application/javascript",
        "og:image", "twitter:image", "meta property=", "content=\"width=device-width",
        "/wp-content/", ".jpg", ".jpeg", ".png", ".svg", ".webp", ".css", ".js", ".woff", ".woff2",
        "data:image", "base64,", "viewbox", "path d=", "d=\"m", "aria-label=", "class=\""
    ]
    if any(h in cl for h in hard_hints):
        return True

    # SVG path command patterns like "h4.16v-2.56"
    if re.search(r"(?:^|[^a-z0-9])[a-z]\d+(?:\.\d+)?[a-z]-?\d", cl):
        return True

    # Image resize query param like "...jpg?resize=770%2C513..."
    if re.search(r"resize=\d+%2c\d+", cl):
        return True

    # Phone / tracking / footer junk often has lots of separators and few letters
    letters = sum(1 for ch in c if ch.isalpha())
    if len(c) >= 120 and letters / max(1, len(c)) < 0.08:
        return True

    return False


def parse_human_number(value_str: str, unit: str) -> Optional[float]:
    """
    Parse number + unit into a comparable float scale.
    - For T/B/M: returns value in billions (B) to compare apples-to-apples.
    - For %: returns numeric percent.
    """
    if value_str is None:
        return None

    s = str(value_str).strip()
    if not s:
        return None

    # remove currency symbols/commas/space
    s = s.replace("$", "").replace(",", "").strip()

    # handle parentheses for negatives e.g. (12.3)
    if s.startswith("(") and s.endswith(")"):
        s = "-" + s[1:-1].strip()

    try:
        v = float(s)
    except Exception:
        return None

    u = normalize_unit(unit)

    # Normalize magnitudes into BILLIONS for currency-like units
    if u == "T":
        return v * 1000.0
    if u == "B":
        return v
    if u == "M":
        return v / 1000.0
    if u == "K":
        return v / 1_000_000.0

    # Percent: keep as percent number
    if u == "%":
        return v

    # Unknown unit: leave as-is (still useful for ratio filtering)
    return v

def build_prev_numbers(prev_metrics: Dict) -> Dict[str, Dict]:
    """
    Build previous metric lookup keyed by metric_name string.
    Stores:
      - parsed numeric value (for matching)
      - normalized unit (for gating)
      - raw display string INCLUDING currency/magnitude (for dashboards + evolution JSON)
      - raw_value/raw_unit for debugging
    """
    def _format_raw_display(value: Any, unit: str) -> str:
        v = "" if value is None else str(value).strip()
        u = (unit or "").strip()

        if not v:
            return ""

        # Currency prefix handling (SGD/USD keywords OR symbol prefixes)
        currency = ""
        u_nospace = u.replace(" ", "")

        if u_nospace.upper().startswith("SGD"):
            currency = "S$"
            u_tail = u_nospace[3:]
        elif u_nospace.upper().startswith("USD"):
            currency = "$"
            u_tail = u_nospace[3:]
        elif u_nospace.startswith("S$"):
            currency = "S$"
            u_tail = u_nospace[2:]
        elif u_nospace.startswith("$"):
            currency = "$"
            u_tail = u_nospace[1:]
        else:
            u_tail = u_nospace

        # Percent special case
        if u_tail == "%":
            return f"{v}%"

        # Word scales
        if "billion" in u.lower():
            return f"{currency}{v} billion".strip()
        if "million" in u.lower():
            return f"{currency}{v} million".strip()

        # Compact suffix (B/M/K/T)
        if u_tail.upper() in {"T", "B", "M", "K"}:
            return f"{currency}{v}{u_tail.upper()}".strip()

        # Fallback
        return f"{currency}{v} {u}".strip()

    prev_numbers: Dict[str, Dict] = {}
    for key, metric in (prev_metrics or {}).items():
        if not isinstance(metric, dict):
            continue

        metric_name = metric.get("name", key)
        raw_value = metric.get("value", "")
        raw_unit = metric.get("unit", "")

        val = parse_human_number(str(raw_value), raw_unit)
        if val is None:
            continue

        prev_numbers[metric_name] = {
            "value": val,
            "unit": normalize_unit(raw_unit),
            "raw": _format_raw_display(raw_value, raw_unit),   # ✅ now includes currency + unit
            "raw_value": raw_value,
            "raw_unit": raw_unit,
            "keywords": extract_context_keywords(metric_name),
        }

    return prev_numbers

def _extract_baseline_cache(previous_data: dict) -> list:
    """
    Pull prior source snapshots from any known places v7.x stores them.
    Returns a list of source_result-like dicts, or [].
    """
    pd = previous_data or {}
    pr = (pd.get("primary_response") or {}) if isinstance(pd.get("primary_response"), dict) else {}

    for obj in [
        pd.get("baseline_sources_cache"),
        (pd.get("results") or {}).get("baseline_sources_cache") if isinstance(pd.get("results"), dict) else None,
        (pd.get("results") or {}).get("source_results") if isinstance(pd.get("results"), dict) else None,
        pd.get("source_results"),
        pr.get("baseline_sources_cache"),
        (pr.get("results") or {}).get("source_results") if isinstance(pr.get("results"), dict) else None,
    ]:
        if isinstance(obj, list) and obj:
            return obj

    return []


def _extract_query_from_previous(previous_data: dict) -> str:
    """
    Try to recover the original user query/topic from the saved analysis object.
    v7.27 commonly uses 'question'.
    """
    pd = previous_data or {}
    if isinstance(pd.get("question"), str) and pd["question"].strip():
        return pd["question"].strip()

    pr = pd.get("primary_response") or {}
    if isinstance(pr, dict):
        if isinstance(pr.get("question"), str) and pr["question"].strip():
            return pr["question"].strip()
        if isinstance(pr.get("query"), str) and pr["query"].strip():
            return pr["query"].strip()

    meta = pd.get("meta") or {}
    if isinstance(meta, dict) and isinstance(meta.get("question"), str) and meta["question"].strip():
        return meta["question"].strip()

    return ""

def _build_source_snapshots_from_web_context(web_context: dict) -> list:
    """
    Convert fetch_web_context() output (scraped_meta) into evolution snapshots.

    Preferred inputs:
      - web_context["scraped_meta"][url]["extracted_numbers"] (analysis-aligned)

    Safety-net hard gates (small set):
      1) homepage-like URLs downweighted + tagged
      2) nav/chrome/junk context downweighted
      3) year-only suppression (e.g., raw == "2024" and no unit/context)
      4) light topic gate (requires minimal overlap with query tokens)
    """
    import hashlib
    from datetime import datetime
    from urllib.parse import urlparse
    import re

    def _sha1(s: str) -> str:
        return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()

    def _now() -> str:
        try:
            return datetime.utcnow().isoformat() + "+00:00"
        except Exception:
            return _yureeka_now_iso_utc()

    def _is_homepage_url(u: str) -> bool:
        try:
            p = urlparse((u or "").strip())
            path = (p.path or "").strip()
            if path in ("", "/"):
                return True
            low = path.lower().rstrip("/")
            if low in ("/index", "/index.html", "/index.htm", "/home", "/default", "/default.aspx"):
                return True
            return False
        except Exception:
            return False


    def _tokenize(s: str) -> list:
        toks = re.findall(r"[a-z0-9]+", (s or "").lower())
        stop = {"the","and","or","of","in","to","for","by","from","with","on","at","as","a","an","is","are","this","that"}
        return [t for t in toks if len(t) >= 4 and t not in stop]

    def _looks_like_year_only(n: dict) -> bool:
        try:
            raw = str(n.get("raw") or "").strip()
            # - If raw is empty or looks like '2024.0', normalize to '2024' for checks.
            try:
                if (not raw) or (raw and raw.replace('.', '', 1).isdigit() and '.' in raw):
                    vraw = n.get('value_norm') if n.get('value_norm') is not None else n.get('value')
                    iv = int(float(str(vraw).strip())) if vraw is not None else None
                    if iv is not None and 1900 <= iv <= 2105:
                        raw = str(iv)
            except Exception:
                pass
            unit = str(n.get("unit") or "").strip()
            ctx = str(n.get("context") or n.get("context_snippet") or "").strip()
            # exactly 4 digits year and nothing else
            if re.fullmatch(r"(19|20)\d{2}", raw) and not unit:
                # if context is empty or super short, treat as junk
                if len(ctx) < 12:
                    return True
            return False
        except Exception:
            return False

    def _is_chrome_ctx(ctx: str) -> bool:
        if not ctx:
            return False
        low = ctx.lower()
        for h in globals().get("NON_DATA_CONTEXT_HINTS", []) or []:
            if h in low:
                return True
        return False

    if not isinstance(web_context, dict):
        return []

    scraped_meta = web_context.get("scraped_meta") or {}
    if not isinstance(scraped_meta, dict) or not scraped_meta:
        return []

    query = (web_context.get("query") or "")
    q_toks = set(_tokenize(query))

    out = []

    for url, meta in scraped_meta.items():
        if not isinstance(meta, dict):
            continue

        url_s = str(url or meta.get("url") or "").strip()
        if not url_s:
            continue

        extracted = meta.get("extracted_numbers") or []
        if not isinstance(extracted, list):
            extracted = []

        fp = meta.get("fingerprint") or meta.get("extract_hash") or meta.get("content_fingerprint")
        if fp and not isinstance(fp, str):
            fp = str(fp)
        if not fp and isinstance(meta.get("clean_text"), str):
            fp = _sha1(meta["clean_text"][:200000])

        status_detail = meta.get("status_detail") or meta.get("status") or ""
        fetched_ok = str(status_detail).startswith("success") or meta.get("status") == "fetched"

        is_homepage = _is_homepage_url(url_s)

        cleaned_numbers = []
        for n in extracted:
            if not isinstance(n, dict):
                continue

            if _looks_like_year_only(n):
                continue

            value = n.get("value")
            raw = n.get("raw")
            unit = n.get("unit")
            ctx = n.get("context") or n.get("context_snippet") or ""

            # normalize context
            ctx_s = ctx if isinstance(ctx, str) else ""
            ctx_s = ctx_s.strip()

            chrome_ctx = _is_chrome_ctx(ctx_s)

            # This is intentionally mild: it *downweights* rather than drops everything.
            ctx_toks = set(_tokenize(ctx_s))
            tok_overlap = len(q_toks.intersection(ctx_toks)) if q_toks and ctx_toks else 0

            # quality scoring (small + interpretable)
            quality = 1.0
            reasons = []

            if is_homepage:
                quality *= 0.25
                reasons.append("homepage_like")

            if chrome_ctx:
                quality *= 0.40
                reasons.append("chrome_context")

            if q_toks and tok_overlap == 0:
                quality *= 0.55
                reasons.append("topic_miss")

            # cap/trim context snippet for JSON size
            ctx_snip = ctx_s[:240]

            cleaned_numbers.append({
                "value": value,
                "unit": unit,
                "raw": raw,
                "source_url": n.get("source_url") or url_s,
                "context_snippet": ctx_snip,
                "anchor_hash": n.get("anchor_hash") or _sha1(f"{url_s}|{ctx_snip}|{raw}|{unit}"),
                # Debug fields for tuning:
                "quality_score": round(float(quality), 3),
                "quality_reasons": reasons,
                "topic_overlap": tok_overlap,
            })

        out.append({
            "url": url_s,
            "status": "fetched_extracted" if cleaned_numbers else ("fetched" if fetched_ok else "failed"),
            "status_detail": status_detail,
            "numbers_found": len(cleaned_numbers),
            "fingerprint": fp or "",
            "fetched_at": meta.get("fetched_at") or _now(),
            "is_homepage_like": bool(is_homepage),
            "extracted_numbers": cleaned_numbers,
        })

    return out


def _build_source_snapshots_from_baseline_cache(baseline_cache: list) -> list:
    """
    Normalize prior cached source_results (from previous run) into a consistent schema.

    Tightening:
      - Detect domain-only/homepage URLs and label them (same as web_context snapshots)
      - Keep backward compatible fields; only add new fields.
    """
    from urllib.parse import urlparse

    def _is_homepage_url(u: str) -> bool:
        try:
            p = urlparse((u or "").strip())
            path = (p.path or "").strip()
            if path in ("", "/"):
                return True
            low = path.lower().rstrip("/")
            if low in ("/index", "/index.html", "/index.htm", "/home", "/default", "/default.aspx"):
                return True
            return False
        except Exception:
            return False

    out = []
    if not isinstance(baseline_cache, list):
        return out

    for sr in baseline_cache:
        if not isinstance(sr, dict):
            continue

        url = sr.get("url") or sr.get("source_url")
        if not url:
            continue
        url_s = str(url).strip()
        if not url_s:
            continue

        extracted = sr.get("extracted_numbers") or []
        if not isinstance(extracted, list):
            extracted = []

        cleaned = []
        for n in extracted:
            if not isinstance(n, dict):
                continue
            cleaned.append({
                "value": n.get("value"),
                "unit": n.get("unit"),
                "raw": n.get("raw"),
                "source_url": n.get("source_url") or url_s,
                "context": (n.get("context") or n.get("context_snippet") or "")[:220]
                if isinstance((n.get("context") or n.get("context_snippet")), str) else "",
            })

        fp = sr.get("fingerprint")
        if fp and not isinstance(fp, str):
            fp = str(fp)

        # --- homepage labeling (tightening #3) ---
        is_homepage = bool(sr.get("is_homepage")) or _is_homepage_url(url_s)
        quality_score = sr.get("quality_score")
        if quality_score is None:
            quality_score = 0.15 if is_homepage else 1.0

        skip_reason = sr.get("skip_reason") or ("homepage_url_low_signal" if is_homepage else "")

        host = sr.get("host") or ""
        path = sr.get("path") or ""
        if not host and not path:
            try:
                p = urlparse(url_s)
                host = p.netloc or ""
                path = p.path or ""
            except Exception:
                pass

        out.append({
            "url": url_s,
            "status": sr.get("status") or "",
            "status_detail": sr.get("status_detail") or "",
            "numbers_found": int(sr.get("numbers_found") or len(cleaned)),
            "fingerprint": fp,
            "fetched_at": sr.get("fetched_at"),
            "extracted_numbers": cleaned,

            # NEW debug fields (safe additions)
            "is_homepage": bool(is_homepage),
            "quality_score": float(quality_score),
            "skip_reason": skip_reason,
            "host": host,
            "path": path,
        })

    return out


def _merge_snapshots_prefer_cached_when_unchanged(current_snaps: list, cached_snaps: list) -> list:
    """
    Policy merge:
      - If current fingerprint matches cached fingerprint for same URL:
        reuse cached snapshot (even if live fetch worked)  ✅ point A
      - Else prefer current (fresh).
      - Add cached snapshots not present in current.
      - Also: if current numbers_found is 0 but cached has >0, reuse cached.
    """
    if not isinstance(current_snaps, list):
        current_snaps = []
    if not isinstance(cached_snaps, list):
        cached_snaps = []

    cached_by_url = {}
    for s in cached_snaps:
        if isinstance(s, dict) and s.get("url"):
            cached_by_url[str(s["url"])] = s

    merged = []
    seen = set()

    for cs in current_snaps:
        if not isinstance(cs, dict) or not cs.get("url"):
            continue
        url = str(cs["url"])
        seen.add(url)

        cached = cached_by_url.get(url)
        if not cached:
            merged.append(cs)
            continue

        cur_fp = cs.get("fingerprint")
        old_fp = cached.get("fingerprint")

        cur_nf = int(cs.get("numbers_found") or 0)
        old_nf = int(cached.get("numbers_found") or 0)

        # If current extraction is empty but cached had numbers, reuse cached.
        if cur_nf == 0 and old_nf > 0:
            merged.append(cached)
            continue

        # Fingerprint unchanged -> reuse cached even if live fetch worked.
        if cur_fp and old_fp and str(cur_fp) == str(old_fp):
            merged.append(cached)
        else:
            merged.append(cs)

    for url, cached in cached_by_url.items():
        if url not in seen:
            merged.append(cached)

    return merged


def _safe_parse_current_analysis(query: str, web_context: dict) -> dict:
    """
    Run the same analysis pipeline used in v7.27 to produce primary_response, but safely.
    Returns dict with at least {primary_response:{primary_metrics:{}}} or {} on failure.
    """
    import json
    qp = globals().get("query_perplexity")
    if not callable(qp):
        return {}

    try:
        raw = qp(query, web_context)
        if not raw or not isinstance(raw, str):
            return {}
        obj = json.loads(raw)
        if not isinstance(obj, dict):
            return {}
        return {"primary_response": obj}
    except Exception:
        return {}

def _yureeka_diff_metrics_by_name_wrap1(prev_response: dict, cur_response: dict):
    """
    Canonical-first diff with:
      - HARD STOP when prev canonical_key is missing in current (no name fallback)
      - Row-level metric_definition sourced from PREVIOUS (original new analysis) schema:
          prev_response['metric_schema_frozen'][canonical_key] (preferred)
          else prev_response['primary_metrics_canonical'][canonical_key]
      - Backward compatible: still returns 'name' (non-empty) and existing fields.

    Returns:
      metric_changes, unchanged, increased, decreased, found
    """
    import re

    # Defaults (used unless schema provides overrides)
    ABS_EPS = 1e-9
    REL_EPS = 0.0005

    def norm_name(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def parse_num(v, unit=""):
        fn = globals().get("parse_human_number")
        if callable(fn):
            try:
                return fn(str(v), unit)
            except Exception:
                return None
        try:
            return float(str(v).replace(",", "").strip())
        except Exception:
            return None

    # - Prefer value_norm/base_unit when present (analysis/evolution alignment)
    # - Fall back to existing parse_num(value, unit) when canonical fields missing
    def get_canonical_value_and_unit(m: dict):
        """
        Returns: (val: float|None, unit: str)
        Priority:
          1) value_norm (float-like) + base_unit (if present)
          2) parse_num(value, unit)
        """
        m = m if isinstance(m, dict) else {}

        # 1) canonical path
        if m.get("value_norm") is not None:
            try:
                v = float(m.get("value_norm"))
                u = str(m.get("base_unit") or m.get("unit") or "").strip()
                try:
                    if str(m.get("unit_family") or "").strip().lower() == "currency":
                        _cc = str(m.get("currency_code") or "").strip().upper()
                        if _cc:
                            u = f"{_cc}:{u}" if u else _cc
                except Exception:
                    pass
                return v, u
            except Exception:
                pass

        # 2) legacy parse path
        # When canonical-for-render is active, do NOT infer/parse numbers from
        # free-form raw strings for CURRENT-side metrics. We only trust
        # schema-canonical value_norm emitted by the rebuild layer.
        #
        # Activation: cur_response['_disable_numeric_inference_v27'] == True
        # Safety: render/diff-layer only. Does not touch fastpath/hashing/etc.
        try:
            if isinstance(cur_response, dict) and cur_response.get("_disable_numeric_inference_v27"):
                u = str(m.get("unit") or "").strip()
                return None, u
        except Exception:
            pass
        u = str(m.get("unit") or "").strip()
        v = parse_num(m.get("value"), u)
        return v, u

    # NOTE (IMPORTANT):
    # - Anchor_hash equality should NOT force "unchanged" if numeric values differ.
    #   It means "we matched the same evidence anchor" (identity/matching), not
    #   that the metric's value necessarily didn't change.
    # - This patch keeps anchor_same, but uses it only for match_confidence +
    #   diagnostics, not as a classification override.
    def _get_anchor_hash_from_metric(m: dict):
        try:
            if isinstance(m, dict):
                ah = m.get("anchor_hash") or m.get("anchor") or m.get("anchorHash")
                return str(ah) if ah else None
        except Exception:
            return None

    def _get_prev_anchor_hash(prev_resp: dict, ckey: str, pm: dict):
        # 1) direct on metric row
        ah = _get_anchor_hash_from_metric(pm)
        if ah:
            return ah

        # 2) prev_response.metric_anchors[ckey].anchor_hash
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            return None

    def _get_cur_anchor_hash(cur_resp: dict, ckey: str, cm: dict):
        # 1) direct on metric row (evolution rebuild puts anchor_hash here)
        ah = _get_anchor_hash_from_metric(cm)
        if ah:
            return ah

        # 2) cur_response.metric_anchors[ckey].anchor_hash (if present)
        try:
            ma = (cur_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            return None

    # - Populate context_snippet/source_url from prev_response.metric_anchors[ckey] when available
    # - Output enrichment only
    def _get_prev_anchor_obj(prev_resp: dict, ckey: str):
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                return a if isinstance(a, dict) else {}
        except Exception:
            return {}

    def _anchor_meta(prev_resp: dict, cur_resp: dict, ckey: str, pm: dict, cm: dict):
        """
        Returns: (source_url, context_snippet, anchor_confidence)
        Priority:
          1) prev_response.metric_anchors[ckey] (baseline anchoring is authoritative)
          2) current metric row fields (if present)
          3) prev metric row fields (if present)
        """
        a = _get_prev_anchor_obj(prev_resp, ckey)

        src = a.get("source_url") or a.get("url")
        ctx = a.get("context_snippet") or a.get("context")
        conf = a.get("anchor_confidence")

        if not src:
            try:
                src = (cm or {}).get("source_url") or (cm or {}).get("url")
            except Exception:
                pass
                src = None
        if not ctx:
            try:
                ctx = (cm or {}).get("context_snippet") or (cm or {}).get("context")
            except Exception:
                pass
                ctx = None

        if not src:
            try:
                src = (pm or {}).get("source_url") or (pm or {}).get("url")
            except Exception:
                pass
                src = None
        if not ctx:
            try:
                ctx = (pm or {}).get("context_snippet") or (pm or {}).get("context")
            except Exception:
                pass
                ctx = None

        try:
            if isinstance(ctx, str):
                ctx = ctx.strip()[:220] or None
            else:
                ctx = None
        except Exception:
            pass
            ctx = None

        try:
            conf = float(conf) if conf is not None else None
        except Exception:
            pass
            conf = None

        return src, ctx, conf

    def prettify_ckey(ckey: str) -> str:
        ckey = str(ckey or "").strip()
        if not ckey:
            return "Unknown Metric"
        parts = ckey.split("__", 1)
        left = parts[0].replace("_", " ").strip()
        right = parts[1].replace("_", " ").strip() if len(parts) > 1 else ""
        left = " ".join(w.capitalize() for w in left.split())
        return f"{left} ({right})" if right else left

    def get_metric_definition(prev_resp: dict, ckey: str) -> dict:
        """
        Pull authoritative definition from the ORIGINAL analysis run (prev_response).
        """
        prev_resp = prev_resp if isinstance(prev_resp, dict) else {}

        schema = prev_resp.get("metric_schema_frozen")
        if isinstance(schema, dict):
            d = schema.get(ckey)
            if isinstance(d, dict) and d:
                out = dict(d)
                out.setdefault("canonical_key", ckey)
                return out

        prev_can = prev_resp.get("primary_metrics_canonical")
        if isinstance(prev_can, dict):
            d = prev_can.get(ckey)
            if isinstance(d, dict) and d:
                out = {
                    "canonical_key": ckey,
                    "canonical_id": d.get("canonical_id"),
                    "dimension": d.get("dimension"),
                    "name": d.get("name") or d.get("original_name"),
                    "unit": d.get("unit"),
                    "geo_scope": d.get("geo_scope"),
                    "geo_name": d.get("geo_name"),
                    "keywords": d.get("keywords"),
                }
                return {k: v for k, v in out.items() if v not in (None, "", [], {})}

        return {"canonical_key": ckey, "name": prettify_ckey(ckey)}

    def get_display_name(prev_resp: dict, prev_can_obj: dict, cur_can_obj: dict, ckey: str) -> str:
        schema = prev_resp.get("metric_schema_frozen")
        if isinstance(schema, dict):
            sm = schema.get(ckey)
            if isinstance(sm, dict):
                v = sm.get("name")
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(prev_can_obj, dict):
            for k in ("name", "original_name"):
                v = prev_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(cur_can_obj, dict):
            for k in ("name", "original_name"):
                v = cur_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        return prettify_ckey(ckey)

    # - If schema provides abs_eps/rel_eps use them, else default.
    def get_eps_for_metric(prev_resp: dict, ckey: str):
        ae = ABS_EPS
        re_ = REL_EPS
        try:
            schema = (prev_resp or {}).get("metric_schema_frozen")
            if isinstance(schema, dict):
                d = schema.get(ckey)
                if isinstance(d, dict):
                    if d.get("abs_eps") is not None:
                        try:
                            ae = float(d.get("abs_eps"))
                        except Exception:
                            pass
                    if d.get("rel_eps") is not None:
                        try:
                            re_ = float(d.get("rel_eps"))
                        except Exception:
                            pass
        except Exception:
            return ae, re_

    prev_response = prev_response if isinstance(prev_response, dict) else {}
    cur_response = cur_response if isinstance(cur_response, dict) else {}

    # Why:
    # - Some pipelines persist metric_anchors as a list of records:
    #     [{"canonical_key": ..., "anchor_hash": ..., ...}, ...]
    # - Diff expects a dict mapping canonical_key -> anchor object.
    # Determinism:
    # - Pure reshaping; no new anchors invented.
    def _coerce_metric_anchors_to_dict(resp: dict):
        try:
            if not isinstance(resp, dict):
                return resp
            ma = resp.get("metric_anchors")
            if isinstance(ma, dict) or ma is None:
                return resp
            if isinstance(ma, list):
                out = {}
                for a in ma:
                    if not isinstance(a, dict):
                        continue
                    ck = a.get("canonical_key") or a.get("ckey") or a.get("metric_key")
                    if not ck:
                        continue
                    if ck not in out:
                        out[str(ck)] = a
                resp["metric_anchors"] = out
            return resp
        except Exception:
            return resp

    prev_response = _coerce_metric_anchors_to_dict(prev_response)
    cur_response = _coerce_metric_anchors_to_dict(cur_response)

    prev_can = prev_response.get("primary_metrics_canonical")
    cur_can = cur_response.get("primary_metrics_canonical")

    # Path A: canonical-first
    if isinstance(prev_can, dict) and isinstance(cur_can, dict) and prev_can:
        metric_changes = []
        unchanged = increased = decreased = found = 0

        for ckey, pm in prev_can.items():
            pm = pm if isinstance(pm, dict) else {}
            cm = cur_can.get(ckey)
            cm = cm if isinstance(cm, dict) else {}

            display_name = get_display_name(prev_response, pm, cm, ckey)
            definition = get_metric_definition(prev_response, ckey)

            prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

            # ✅ HARD STOP: canonical key missing in current => not_found (no name fallback)
            if ckey not in cur_can or not isinstance(cur_can.get(ckey), dict):
                _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, {})

                metric_changes.append({
                    "name": display_name,
                    "previous_value": prev_raw,
                    "current_value": "N/A",
                    "change_pct": None,
                    "change_type": "not_found",
                    "match_confidence": 0.0,
                    "context_snippet": _ctx,
                    "source_url": _src,
                    "anchor_used": False,  # not applicable when current metric missing
                    "canonical_key": ckey,
                    "metric_definition": definition,
                    "anchor_confidence": _aconf,
                })
                continue

            found += 1

            cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

            prev_ah = _get_prev_anchor_hash(prev_response, ckey, pm)
            cur_ah = _get_cur_anchor_hash(cur_response, ckey, cm)
            anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))

            prev_val, prev_unit_cmp = get_canonical_value_and_unit(pm)
            cur_val, cur_unit_cmp = get_canonical_value_and_unit(cm)

            abs_eps, rel_eps = get_eps_for_metric(prev_response, ckey)

            change_type = "unknown"
            change_pct = None

            # Why:
            # - anchor_same means "we matched the same evidence anchor"
            # - It MUST NOT short-circuit classification to "unchanged" when values differ.
            # - This fixes the exact bug you observed: prev_value_norm != cur_value_norm
            #   while change_type incorrectly says "unchanged".
            if prev_val is not None and cur_val is not None:
                if abs(prev_val - cur_val) <= max(abs_eps, abs(prev_val) * rel_eps):
                    change_type = "unchanged"
                    change_pct = 0.0
                    unchanged += 1
                elif cur_val > prev_val:
                    change_type = "increased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    increased += 1
                else:
                    change_type = "decreased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    decreased += 1
            # If we cannot compare numerically, fall back:
            # - If anchors match, treat as unchanged ONLY as a last resort (formatting issue)
            elif anchor_same:
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1

            unit_mismatch = False
            try:
                if prev_unit_cmp and cur_unit_cmp and str(prev_unit_cmp) != str(cur_unit_cmp):
                    unit_mismatch = True
            except Exception:
                pass
                unit_mismatch = False

            _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, cm)

            match_conf = 92.0
            try:
                if anchor_same:
                    match_conf = 98.0
            except Exception:
                pass
                match_conf = 92.0

            metric_changes.append({
                "name": display_name,
                "previous_value": prev_raw,
                "current_value": cur_raw,
                "change_pct": change_pct,
                "change_type": change_type,
                "match_confidence": float(match_conf),

                "context_snippet": _ctx,
                "source_url": _src,

                # anchor identity (matching), not classification
                "anchor_used": bool(anchor_same),
                "prev_anchor_hash": prev_ah,
                "cur_anchor_hash": cur_ah,

                "canonical_key": ckey,
                "metric_definition": definition,

                "anchor_confidence": _aconf,

                # expose canonical comparison basis for debugging/convergence
                "prev_value_norm": prev_val,
                "cur_value_norm": cur_val,
                "prev_unit_cmp": prev_unit_cmp,
                "cur_unit_cmp": cur_unit_cmp,
                "unit_mismatch": bool(unit_mismatch),
                "abs_eps_used": abs_eps,
                "rel_eps_used": rel_eps,
            })

        return metric_changes, unchanged, increased, decreased, found

    # Path B: legacy name fallback
    prev_metrics = prev_response.get("primary_metrics") or {}
    cur_metrics = cur_response.get("primary_metrics") or {}
    if not isinstance(prev_metrics, dict):
        prev_metrics = {}
    if not isinstance(cur_metrics, dict):
        cur_metrics = {}

    prev_index = {}
    for k, m in prev_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            prev_index[norm_name(name)] = (name, m)

    cur_index = {}
    for k, m in cur_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            cur_index[norm_name(name)] = (name, m)

    metric_changes = []
    unchanged = increased = decreased = found = 0

    for nk, (display_name, pm) in prev_index.items():
        prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

        if nk not in cur_index:
            metric_changes.append({
                "name": display_name or "Unknown Metric",
                "previous_value": prev_raw,
                "current_value": "N/A",
                "change_pct": None,
                "change_type": "not_found",
                "match_confidence": 0.0,
                "context_snippet": None,
                "source_url": None,
                "anchor_used": False,
            })
            continue

        found += 1
        _, cm = cur_index[nk]
        cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

        prev_val, _prev_unit_cmp = get_canonical_value_and_unit(pm)
        cur_val, _cur_unit_cmp = get_canonical_value_and_unit(cm)

        prev_ah = _get_anchor_hash_from_metric(pm)
        cur_ah = _get_anchor_hash_from_metric(cm)
        anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))

        change_type = "unknown"
        change_pct = None

        if prev_val is not None and cur_val is not None:
            if abs(prev_val - cur_val) <= max(ABS_EPS, abs(prev_val) * REL_EPS):
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            elif cur_val > prev_val:
                change_type = "increased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                increased += 1
            else:
                change_type = "decreased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                decreased += 1
        elif anchor_same:
            change_type = "unchanged"
            change_pct = 0.0
            unchanged += 1

        metric_changes.append({
            "name": display_name or "Unknown Metric",
            "previous_value": prev_raw,
            "current_value": cur_raw,
            "change_pct": change_pct,
            "change_type": change_type,
            "match_confidence": 90.0 if anchor_same else 80.0,
            "context_snippet": None,
            "source_url": None,

            "anchor_used": bool(anchor_same),
            "prev_anchor_hash": prev_ah,
            "cur_anchor_hash": cur_ah,

            "prev_value_norm": prev_val,
            "cur_value_norm": cur_val,
        })

    return metric_changes, unchanged, increased, decreased, found

def _fallback_match_from_snapshots(prev_numbers: dict, snapshots: list, anchors_by_name: dict):
    """
    When current analysis is missing, fall back to cached extracted_numbers only.
    If there is no snapshot candidate, return not_found ✅.

    Tightening implemented:
      1) Reject obvious year mismatches:
         - If metric name or prev_raw includes a year (e.g., 2024), require candidate context to contain it.
         - Also reject candidates that are a bare year if metric is not a year metric.
      2) Unit-family gating:
         - percent vs currency vs magnitude vs other (GW/TWh/tons/etc)
      3) Domain/homepage handling:
         - Downweight homepage sources heavily unless anchored (or if no non-homepage pool exists)

    Debugging enhancements:
      - Each metric row includes match_debug with:
        method, pool sizes, required years, unit families, best score, reject counts, top alternatives (small).
    """
    import re

    ABS_EPS = 1e-9
    REL_EPS = 0.0005

    def norm_unit(u: str) -> str:
        fn = globals().get("normalize_unit")
        if callable(fn):
            try:
                return fn(u)
            except Exception:
                return (u or "").strip()

    def parse_num(v, unit=""):
        fn = globals().get("parse_human_number")
        if callable(fn):
            try:
                return fn(str(v), unit)
            except Exception:
                return None
        try:
            return float(str(v).replace(",", "").strip())
        except Exception:
            return None

    def metric_tokens(name: str):
        toks = re.findall(r"[a-z0-9]+", (name or "").lower())
        stop = {"the","and","or","of","in","to","for","by","from","with","on","at","as"}
        return [t for t in toks if len(t) > 3 and t not in stop][:24]

    def unit_family(unit: str, raw: str = "", ctx: str = "") -> str:
        u = (norm_unit(unit) or "").strip().upper()
        blob = f"{raw or ''} {ctx or ''}".upper()

        # percent
        if u == "%" or "%" in blob:
            return "percent"

        # currency
        if any(x in blob for x in ["USD", "SGD", "EUR", "GBP", "S$", "$", "€", "£"]):
            return "currency"
        if any(x in u for x in ["USD", "SGD", "EUR", "GBP"]) or u.startswith("$") or u.startswith("S$"):
            return "currency"

        # magnitude suffix
        if u in ("K", "M", "B", "T") or any(x in blob for x in [" BILLION", " MILLION", " TRILLION", " BN", " MN"]):
            return "magnitude"

        # otherwise: other units like GW, TWh, tons, units, etc
        return "other"

    def required_years(metric_name: str, prev_raw: str) -> list:
        years = set()
        for s in [metric_name or "", prev_raw or ""]:
            for y in re.findall(r"\b(19\d{2}|20\d{2})\b", str(s)):
                years.add(y)
        return sorted(years)

    def year_ok(req_years: list, ctx: str) -> bool:
        if not req_years:
            return True
        c = (ctx or "").lower()
        return any(y.lower() in c for y in req_years)

    def is_bare_year(raw: str, unit: str) -> bool:
        r = (raw or "").strip()
        if unit and norm_unit(unit) not in ("", None):
            # If there is a unit, don't treat as bare year
            return False
        return bool(re.match(r"^(19\d{2}|20\d{2})$", r))

    def ctx_score(tokens, ctx: str) -> float:
        c = (ctx or "").lower()
        if not tokens:
            return 0.0
        hit = sum(1 for t in tokens if t in c)
        return hit / max(1, len(tokens))

    # Flatten candidates from snapshots ONLY, keep snapshot metadata
    candidates = []
    for sr in (snapshots or []):
        if not isinstance(sr, dict):
            continue
        url = sr.get("url")
        if not url:
            continue
        is_home = bool(sr.get("is_homepage"))
        qs = sr.get("quality_score", 1.0)
        try:
            qs = float(qs)
        except Exception:
            pass
            qs = 1.0

        for n in (sr.get("extracted_numbers") or []):
            if not isinstance(n, dict):
                continue
            candidates.append({
                "url": url,
                "value": n.get("value"),
                "unit": norm_unit(n.get("unit") or ""),
                "raw": n.get("raw") or "",
                "context": n.get("context") or "",
                "is_homepage": is_home,
                "quality_score": qs,
            })

    # Pre-split pools for tightening #3
    non_home = [c for c in candidates if not c.get("is_homepage")]
    home = [c for c in candidates if c.get("is_homepage")]

    out_changes = []
    for metric_name, prev in (prev_numbers or {}).items():
        prev_raw = prev.get("raw") or prev.get("value") or "N/A"
        prev_unit = norm_unit(prev.get("unit") or "")
        prev_val = prev.get("value")
        toks = prev.get("keywords") or metric_tokens(metric_name)

        req_years = required_years(metric_name, str(prev_raw))
        prev_fam = unit_family(prev_unit, str(prev_raw), "")

        anchor = anchors_by_name.get(metric_name) or {}
        anchor_url = anchor.get("source_url") if isinstance(anchor, dict) else None

        # Pool policy:
        # - anchored: use anchor_url pool if exists
        # - else: use non-homepage pool when available; only fall back to homepage if necessary
        pool_policy = "non_home_preferred"
        pool = non_home if non_home else candidates
        if anchor_url:
            anchored_pool = [c for c in candidates if c.get("url") == anchor_url]
            if anchored_pool:
                pool = anchored_pool
                pool_policy = "anchored_url"
            else:
                pool_policy = "anchored_url_not_present"

        reject_counts = {"year_mismatch": 0, "unit_mismatch": 0, "bare_year_reject": 0}
        best = None
        best_score = -1e9
        top_alts = []  # store a few near-misses for debugging

        for c in pool:
            ctx = c.get("context", "") or ""
            raw = c.get("raw", "") or ""
            unit = c.get("unit", "") or ""

            # (1) year gating: if required years exist, require them in context
            if not year_ok(req_years, ctx):
                reject_counts["year_mismatch"] += 1
                continue

            # reject bare-year candidates unless the metric itself is a year metric
            # (prevents "2024" being selected as a value for percent/currency/etc)
            if is_bare_year(str(raw), unit) and prev_fam != "other":
                reject_counts["bare_year_reject"] += 1
                continue

            # (2) unit-family gating
            cand_fam = unit_family(unit, raw, ctx)
            if prev_fam != cand_fam:
                reject_counts["unit_mismatch"] += 1
                continue

            score = ctx_score(toks, ctx)

            # bonus for numeric closeness
            cv = parse_num(c.get("value"), unit) or parse_num(raw, unit)
            if prev_val is not None and cv is not None:
                if abs(prev_val - cv) <= max(ABS_EPS, abs(prev_val) * REL_EPS):
                    score += 0.25

            # (3) homepage penalty unless anchored
            if c.get("is_homepage") and not anchor_url:
                score -= 0.35

            # quality_score weighting
            try:
                score *= max(0.1, min(1.0, float(c.get("quality_score", 1.0))))
            except Exception:
                pass

            # keep top alternatives for debugging
            if len(top_alts) < 5:
                top_alts.append({
                    "raw": raw[:60],
                    "unit": unit,
                    "url": c.get("url"),
                    "score": float(score),
                    "is_homepage": bool(c.get("is_homepage")),
                    "ctx": (ctx or "")[:120],
                })

            if score > best_score:
                best_score = score
                best = c

        # sort alt candidates by score desc
        try:
            top_alts.sort(key=lambda x: x.get("score", 0.0), reverse=True)
        except Exception:
            pass

        if not best:
            out_changes.append({
                "name": metric_name,
                "previous_value": prev_raw,
                "current_value": "N/A",
                "change_pct": None,
                "change_type": "not_found",
                "match_confidence": 0.0,
                "context_snippet": None,
                "source_url": None,
                "anchor_used": bool(anchor_url),

                # NEW debug payload
                "match_debug": {
                    "method": "snapshots_only",
                    "pool_policy": pool_policy,
                    "pool_size": int(len(pool)),
                    "req_years": req_years,
                    "prev_unit": prev_unit,
                    "prev_unit_family": prev_fam,
                    "reject_counts": reject_counts,
                    "top_alternatives": top_alts[:3],
                }
            })
            continue

        cur_raw = best.get("raw") or best.get("value")
        cv = parse_num(best.get("value"), best.get("unit")) or parse_num(cur_raw, best.get("unit"))

        change_type = "unknown"
        change_pct = None
        if prev_val is not None and cv is not None:
            if abs(prev_val - cv) <= max(ABS_EPS, abs(prev_val) * REL_EPS):
                change_type = "unchanged"
                change_pct = 0.0
            elif cv > prev_val:
                change_type = "increased"
                change_pct = ((cv - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
            else:
                change_type = "decreased"
                change_pct = ((cv - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0

        conf = max(0.0, min(60.0, best_score * 60.0))

        out_changes.append({
            "name": metric_name,
            "previous_value": prev_raw,
            "current_value": cur_raw,
            "change_pct": change_pct,
            "change_type": change_type,
            "match_confidence": float(conf),
            "context_snippet": (best.get("context") or "")[:200] if isinstance(best.get("context"), str) else None,
            "source_url": best.get("url"),
            "anchor_used": bool(anchor_url),

            # NEW debug payload
            "match_debug": {
                "method": "snapshots_only",
                "pool_policy": pool_policy,
                "pool_size": int(len(pool)),
                "req_years": req_years,
                "prev_unit": prev_unit,
                "prev_unit_family": prev_fam,
                "best_unit": best.get("unit"),
                "best_unit_family": unit_family(best.get("unit") or "", best.get("raw") or "", best.get("context") or ""),
                "best_score": float(best_score),
                "best_is_homepage": bool(best.get("is_homepage")),
                "reject_counts": reject_counts,
                "top_alternatives": top_alts[:3],
            }
        })

    return out_changes


# Why:
# - REFACTOR59/60 downsizing removed legacy Diff Panel V2 builder defs.
# - compute_source_anchored_diff expects a callable v2 entrypoint to populate
#   metric_changes_v2 (and optionally override metric_changes via canonical-first join).
# - When the v2 entrypoint is missing, Evolution reports "no metrics to display"
#   despite primary_metrics_canonical being present.
#
# What:
# - Provide a small, deterministic, schema-agnostic canonical-first join row builder:
#   build_diff_metrics_panel_v2__rows_refactor47(prev_response, cur_response) -> (rows, summary)
# - Keep strict unit comparability (no silent nonsense deltas).

def _refactor61__unwrap_pmc(obj):
    """Best-effort unwrap for primary_metrics_canonical across historical payload shapes."""
    try:
        if isinstance(obj, dict) and isinstance(obj.get("primary_metrics_canonical"), dict):
            return obj.get("primary_metrics_canonical") or {}
        if isinstance(obj, dict) and isinstance(obj.get("primary_response"), dict) and isinstance(obj["primary_response"].get("primary_metrics_canonical"), dict):
            return obj["primary_response"].get("primary_metrics_canonical") or {}
        if isinstance(obj, dict) and isinstance(obj.get("results"), dict) and isinstance(obj["results"].get("primary_metrics_canonical"), dict):
            return obj["results"].get("primary_metrics_canonical") or {}
        if isinstance(obj, dict) and isinstance(obj.get("results"), dict) and isinstance(obj["results"].get("primary_response"), dict) and isinstance(obj["results"]["primary_response"].get("primary_metrics_canonical"), dict):
            return obj["results"]["primary_response"].get("primary_metrics_canonical") or {}
    except Exception:
        pass
    return {}

def _refactor72__unwrap_metric_schema_frozen(obj):
    """Best-effort unwrap for metric_schema_frozen across historical payload shapes."""
    try:
        if isinstance(obj, dict) and isinstance(obj.get("metric_schema_frozen"), dict):
            return obj.get("metric_schema_frozen") or {}
        if isinstance(obj, dict) and isinstance(obj.get("primary_response"), dict) and isinstance(obj["primary_response"].get("metric_schema_frozen"), dict):
            return obj["primary_response"].get("metric_schema_frozen") or {}
        if isinstance(obj, dict) and isinstance(obj.get("results"), dict) and isinstance(obj["results"].get("metric_schema_frozen"), dict):
            return obj["results"].get("metric_schema_frozen") or {}
        if isinstance(obj, dict) and isinstance(obj.get("results"), dict) and isinstance(obj["results"].get("primary_response"), dict) and isinstance(obj["results"]["primary_response"].get("metric_schema_frozen"), dict):
            return obj["results"]["primary_response"].get("metric_schema_frozen") or {}
    except Exception:
        pass
    return {}


def _refactor61__pick_val_unit(m):
    """Extract (value_norm, unit_tag) deterministically from a canonical metric dict."""
    try:
        if not isinstance(m, dict):
            return (None, "")
        vn = m.get("value_norm")
        if vn is None:
            vn = m.get("value")
        unit = (m.get("unit_tag") or m.get("unit") or m.get("unit_cmp") or m.get("base_unit") or "").strip()
        return (vn, unit)
    except Exception:
        return (None, "")

def _refactor61__pick_source_url(m):
    """Best-effort provenance URL extraction (diff-row attribution).

    Order (REFACTOR123):
      1) top-level m["source_url"] / m["url"]
      2) first evidence source_url/url (if any)
      3) provenance.best_candidate.source_url/url
    """
    try:
        if not isinstance(m, dict):
            return None

        # 1) top-level
        try:
            u0 = m.get("source_url") or m.get("url")
            if u0:
                return str(u0)
        except Exception:
            pass

        # 2) evidence list
        try:
            ev = m.get("evidence")
            if isinstance(ev, list) and ev:
                e0 = ev[0] if isinstance(ev[0], dict) else None
                if isinstance(e0, dict):
                    u1 = e0.get("source_url") or e0.get("url")
                    if u1:
                        return str(u1)
        except Exception:
            pass

        # 3) provenance best_candidate
        try:
            prov = m.get("provenance") if isinstance(m.get("provenance"), dict) else None
            bc = prov.get("best_candidate") if isinstance(prov, dict) and isinstance(prov.get("best_candidate"), dict) else None
            if isinstance(bc, dict):
                u2 = bc.get("source_url") or bc.get("url")
                return str(u2) if u2 else None
        except Exception:
            pass
    except Exception:
        pass
    return None

def build_diff_metrics_panel_v2__rows_refactor47(prev_response: dict, cur_response: dict):
    """
    Canonical-first strict join (schema-complete in REFACTOR74):

      - Prefer iterating *frozen schema keys* when metric_schema_frozen is available.
      - For keys present in both baseline and current:
            * join on exact canonical_key only (no heuristics)
            * enforce strict unit comparability; only compute deltas when units match
      - Additionally emit explicit completeness rows for schema keys when either side is missing:
            * missing_baseline: key absent in baseline but present in current
            * missing_current: key present in baseline but absent in current
            * missing_both: key absent in both (still emitted when schema is known)

    Returns: (rows, summary)
    """
    prev_can = _refactor61__unwrap_pmc(prev_response)
    cur_can = _refactor61__unwrap_pmc(cur_response)

    # Prefer frozen schema key order for completeness (stable UI + deterministic diff feed)
    schema = _refactor72__unwrap_metric_schema_frozen(prev_response) or _refactor72__unwrap_metric_schema_frozen(cur_response)
    schema_keys = []
    try:
        if isinstance(schema, dict) and schema:
            schema_keys = sorted([str(k) for k in schema.keys()])
    except Exception:
        schema_keys = []

    rows = []
    try:
        if not isinstance(prev_can, dict):
            prev_can = {}
        if not isinstance(cur_can, dict):
            cur_can = {}

        if schema_keys:
            iter_keys = list(schema_keys)
            mode = "schema_complete"
        else:
            # Fallback: union so we still show anything we can (legacy behavior)
            iter_keys = sorted({str(k) for k in list(prev_can.keys()) + list(cur_can.keys())})
            mode = "union_fallback"

        for ckey in iter_keys:
            pm = prev_can.get(ckey) if isinstance(prev_can.get(ckey), dict) else None
            cm = cur_can.get(ckey) if isinstance(cur_can.get(ckey), dict) else None

            pm_ok = isinstance(pm, dict) and bool(pm)
            cm_ok = isinstance(cm, dict) and bool(cm)

            pv, pu = (None, "")
            if pm_ok:
                pv, pu = _refactor61__pick_val_unit(pm)

            cv, cu = (None, "")
            if cm_ok:
                cv, cu = _refactor61__pick_val_unit(cm)

            # For display clarity, if only one side is present, carry the known unit across.
            try:
                if (not pm_ok) and cm_ok and str(cu).strip():
                    pu = cu
                if pm_ok and (not cm_ok) and str(pu).strip():
                    cu = pu
            except Exception:
                pass
            # If schema is known and both units are empty, fill from schema when safe.
            # (Avoid currency placeholder unit 'U' to prevent misleading display.)
            try:
                if (not str(pu).strip()) and (not str(cu).strip()) and isinstance(schema, dict):
                    sch = schema.get(ckey) if isinstance(schema.get(ckey), dict) else None
                    su = (sch.get("unit") if isinstance(sch, dict) else "") or ""
                    su = str(su).strip()
                    if su and su.upper() != "U":
                        pu = su
                        cu = su
            except Exception:
                pass


            # Best-effort metric name
            name = ""
            try:
                name = (pm.get("name") if pm_ok else "") or (cm.get("name") if cm_ok else "") or str(ckey)
            except Exception:
                name = str(ckey)

            baseline_is_comparable = False
            delta_abs = None
            delta_pct = None
            # Completeness-first change_type defaults (REFACTOR123)
            # Treat null values as missing even if the key exists in schema maps.
            pm_has_value = bool(pm_ok and (pv is not None))
            cm_has_value = bool(cm_ok and (cv is not None))

            if pm_has_value and cm_has_value:
                change_type = "found"
            elif (not pm_has_value) and cm_has_value:
                change_type = "missing_baseline"
            elif pm_has_value and (not cm_has_value):
                change_type = "missing_current"
            else:
                change_type = "missing_both"

            try:
                # Strict comparability: both present + units must match and be non-empty
                if pm_has_value and cm_has_value and str(pu).strip() and str(cu).strip() and str(pu).strip() == str(cu).strip():
                    baseline_is_comparable = True
                    pvf = float(pv) if isinstance(pv, (int, float)) else None
                    cvf = float(cv) if isinstance(cv, (int, float)) else None
                    if pvf is not None and cvf is not None:
                        delta_abs = cvf - pvf
                        if abs(delta_abs) < 1e-12:
                            delta_abs = 0.0
                            change_type = "unchanged"
                        elif delta_abs > 0:
                            change_type = "increased"
                        else:
                            change_type = "decreased"
                        if pvf != 0:
                            delta_pct = (delta_abs / pvf) * 100.0
            except Exception:
                pass

            # If both present but units differ: explicitly mark unit mismatch.
            try:
                if pm_has_value and cm_has_value and str(pu).strip() and str(cu).strip() and str(pu).strip() != str(cu).strip():
                    change_type = "unit_mismatch"
                    baseline_is_comparable = False
            except Exception:
                pass

            row = {
                "canonical_key": str(ckey),
                "name": name,
                "previous_value": pv,
                "previous_unit": pu,
                "prev_value_norm": pv,
                "current_value": cv,
                "current_unit": cu,
                "cur_value_norm": cv,
                "delta_abs": delta_abs,
                "delta_pct": delta_pct,
                "change_type": change_type,
                "baseline_is_comparable": bool(baseline_is_comparable),
                "current_method": "strict_fallback_v2",
                "source_url": _refactor61__pick_source_url(cm) if cm_ok else None,
                "schema_frozen_key": bool(ckey in schema_keys) if schema_keys else False,
            }

            rows.append(row)
    except Exception:
        pass

    summary = {
        "rows_total": int(len(rows)),
        "builder": "REFACTOR74_build_diff_metrics_panel_v2__rows_refactor47",
        "mode": (mode if 'mode' in locals() else ""),
        "schema_keys_total": int(len(schema_keys)) if schema_keys else 0,
    }
    return rows, summary

# Compatibility aliases (compute_source_anchored_diff expects one of these)
try:
    build_diff_metrics_panel_v2__rows = build_diff_metrics_panel_v2__rows_refactor47  # type: ignore
except Exception:
    pass

def build_diff_metrics_panel_v2(prev_response: dict, cur_response: dict):
    rows, _summary = build_diff_metrics_panel_v2__rows_refactor47(prev_response, cur_response)
    return rows, _summary

def _fix2dXX_hotfix_percent_year_token_sanitize_pmc_v1(
    pmc: dict,
    metric_schema_frozen: dict,
    debug: dict,
    label: str,
) -> dict:
    """
    Hotfix: for __percent keys, reject bare year tokens (1900-2100) unless the raw token itself
    contains strong percent evidence (%/percent/pct). Do NOT trust unit_tag alone.
    """
    if not isinstance(pmc, dict):
        return pmc

    def _is_percent_key(k: str) -> bool:
        if isinstance(k, str) and k.endswith("__percent"):
            return True
        sch = metric_schema_frozen.get(k) if isinstance(metric_schema_frozen, dict) else None
        if isinstance(sch, dict):
            dim = str(sch.get("dimension") or sch.get("unit_kind") or "").lower()
            if dim == "percent":
                return True
        return False

    def _yearlike_raw_token(raw: str) -> bool:
        if not raw:
            return False
        t = raw.strip()
        if len(t) == 4 and t.isdigit():
            y = int(t)
            return 1900 <= y <= 2100
        return False

    def _strong_percent_evidence_in_token(raw: str) -> bool:
        if not raw:
            return False
        s = raw.lower()
        return ("%" in s) or ("percent" in s) or ("pct" in s)

    rejected = []
    out = {}

    for k, v in pmc.items():
        if not _is_percent_key(k):
            out[k] = v
            continue

        raw = ""
        if isinstance(v, dict):
            ev = v.get("evidence")
            if isinstance(ev, dict):
                raw = str(ev.get("raw") or ev.get("raw_text") or ev.get("token") or "")
            if not raw:
                raw = str(v.get("raw") or "")

        if _yearlike_raw_token(raw) and not _strong_percent_evidence_in_token(raw):
            rej = {"canonical_key": k, "raw": raw}
            if isinstance(v, dict):
                rej["value_norm"] = v.get("value_norm")
                rej["anchor_hash"] = v.get("anchor_hash")
                rej["source_url"] = v.get("source_url")
                rej["method"] = v.get("method")
            rejected.append(rej)
            # DROP the key (so Evolution shows it as 'added' instead of prev=2040)
            continue

        out[k] = v

    dbg = debug.setdefault("fix2dXX_percent_year_token_sanitize_v1", {})
    dbg["label"] = label
    dbg["pmc_in_count"] = len(pmc)
    dbg["pmc_out_count"] = len(out)
    dbg["rejected_count"] = len(rejected)
    dbg["rejected_samples"] = rejected[:5]

    return out


def _fix2d86_sanitize_pmc_percent_year_tokens_v1(pmc: dict, metric_schema_frozen: dict, label: str):
    """
    FIX2D86 HOTFIX:
    For __percent keys, drop bindings where the chosen evidence is a bare year token (1900-2100),
    e.g. raw="2040" and value_norm=2040.0. This prevents prev=2040 for CAGR percent keys.
    """
    dbg = {
        "label": label,
        "pmc_in_count": len(pmc) if isinstance(pmc, dict) else 0,
        "pmc_out_count": 0,
        "dropped_count": 0,
        "dropped_samples": [],
    }
    if not isinstance(pmc, dict):
        return pmc, dbg

    def _is_percent_key(k: str) -> bool:
        if isinstance(k, str) and k.endswith("__percent"):
            return True
        sch = metric_schema_frozen.get(k) if isinstance(metric_schema_frozen, dict) else None
        if isinstance(sch, dict):
            dim = str(sch.get("dimension") or sch.get("unit_kind") or "").lower()
            return dim == "percent"
        return False

    def _yearlike_num(v) -> bool:
        try:
            x = float(v)
        except Exception:
            return False
        if abs(x - round(x)) > 1e-9:
            return False
        y = int(round(x))
        return 1900 <= y <= 2100

    def _extract_raw_token(vdict: dict) -> str:
        if not isinstance(vdict, dict):
            return ""
        ev = vdict.get("evidence")

        # evidence can be dict or list of dicts in your codebase
        if isinstance(ev, dict):
            return str(ev.get("raw") or ev.get("raw_text") or ev.get("token") or "")
        if isinstance(ev, list) and ev:
            for item in ev:
                if isinstance(item, dict):
                    raw = str(item.get("raw") or item.get("raw_text") or item.get("token") or "")
                    if raw:
                        return raw
        return str(vdict.get("raw") or "")

    def _raw_is_bare_year(raw: str) -> bool:
        if not raw:
            return False
        t = raw.strip().lower()
        # Must be "2040" or "2040.0" style, and MUST NOT contain percent markers
        if ("%" in t) or ("percent" in t) or ("pct" in t):
            return False
        # accept 4-digit integer, or integer with .0
        if t.isdigit() and len(t) == 4:
            y = int(t)
            return 1900 <= y <= 2100
        if t.endswith(".0"):
            base = t[:-2]
            if base.isdigit() and len(base) == 4:
                y = int(base)
                return 1900 <= y <= 2100
        return False

    out = {}
    for k, v in pmc.items():
        if not _is_percent_key(k):
            out[k] = v
            continue

        if isinstance(v, dict):
            val_norm = v.get("value_norm")
            raw = _extract_raw_token(v)
            if _yearlike_num(val_norm) and _raw_is_bare_year(raw):
                dbg["dropped_count"] += 1
                if len(dbg["dropped_samples"]) < 5:
                    dbg["dropped_samples"].append({
                        "canonical_key": k,
                        "value_norm": val_norm,
                        "raw": raw,
                        "source_url": v.get("source_url"),
                        "method": v.get("method"),
                        "anchor_hash": v.get("anchor_hash"),
                    })
                continue

        out[k] = v

    dbg["pmc_out_count"] = len(out)
    return out, dbg


def compute_source_anchored_diff(previous_data: dict, web_context: dict = None) -> dict:
    """
    Tight source-anchored evolution:
      - Prefer snapshots from analysis (baseline_sources_cache)
      - Optionally reconstruct snapshots from web_context.scraped_meta
      - If no valid snapshots: return not_found (no heuristic junk)

    Always returns a dict.
    """
    import re
    from datetime import datetime, timezone

    try:
        _qtxt = str((analysis or {}).get('question') or (analysis or {}).get('query') or '')
        web_context = _fix2d66_promote_injected_urls(web_context or {}, question_text=_qtxt, stage='analysis_attach')
    except Exception:
        pass

    def _now():
        return datetime.now(timezone.utc).isoformat()

    def _safe_int(x, default=0):
        try:
            return int(x)
        except Exception:
            return default

    def _fingerprint(text: str):
        try:
            fn = globals().get("fingerprint_text")
            if callable(fn):
                return fn(text or "")
        except Exception:
            pass
        try:
            return fingerprint_text(text or "")
        except Exception:
            return None

    # Why:
    # - Some runs store rebuild essentials under primary_response or results.primary_response
    # - Evolution may look only at top-level keys, causing schema=0 / anchors=0
    def _get_nested(d, path, default=None):
        try:
            x = d
            for k in path:
                if not isinstance(x, dict):
                    return default
                x = x.get(k)
            return x if x is not None else default
        except Exception:
            return default

    def _first_present(d, paths, default=None):
        for p in paths:
            v = _get_nested(d, p, None)
            if v is not None:
                return v
        return default

    # Objective:
    # - Ensure compute_source_anchored_diff can ALWAYS populate web_context.diag_injected_urls
    #   even when the caller only supplies:
    #     * web_context["extra_urls"]
    #     * web_context["diag_extra_urls_ui_raw"]
    #     * web_context["diag_run_id"]
    # - Additionally, if the evolution UI did NOT supply extra_urls, record what the baseline
    #   analysis run had (if any) as "replayed_from_analysis_norm" for diagnostics only.
    # Safety:
    # - Purely additive diagnostics. Does NOT alter fastpath logic or hashing inputs.
    def _fix41u_extract_injected_from_prev(prev: dict) -> dict:
        try:
            if not isinstance(prev, dict):
                return {}
            cand_paths = [
                ["results","debug","inj_trace_v1"],
                ["primary_response","results","debug","inj_trace_v1"],
                ["results","primary_response","results","debug","inj_trace_v1"],
                ["debug","inj_trace_v1"],
            ]
            for p in cand_paths:
                v = _get_nested(prev, p, None)
                if isinstance(v, dict) and v:
                    return v
        except Exception:
            return {}

    try:
        if web_context is None or not isinstance(web_context, dict):
            web_context = {}

        # Pull what the caller provided (Evolution UI should pass these)
        _fix41u_ui_raw = ""
        try:
            _fix41u_ui_raw = str(web_context.get("diag_extra_urls_ui_raw") or web_context.get("extra_urls_ui_raw") or "")
        except Exception:
            pass
            _fix41u_ui_raw = ""

        _fix41u_extra_urls = []
        try:
            _fix41u_extra_urls = _inj_diag_norm_url_list(web_context.get("extra_urls") or [])
        except Exception:
            pass
            _fix41u_extra_urls = []

        # If Evolution UI did not supply extras, capture what baseline analysis had (diagnostic only)
        _fix41u_prev_trace = _fix41u_extract_injected_from_prev(previous_data)
        _fix41u_replayed = []
        try:
            if not _fix41u_extra_urls and isinstance(_fix41u_prev_trace, dict):
                _fix41u_replayed = _inj_diag_norm_url_list(_fix41u_prev_trace.get("ui_norm") or [])
        except Exception:
            pass
            _fix41u_replayed = []

        # Ensure diag container exists
        web_context.setdefault("diag_injected_urls", {})
        if isinstance(web_context.get("diag_injected_urls"), dict):
            _d = web_context["diag_injected_urls"]
            # run_id continuity
            try:
                _d.setdefault("run_id", str(web_context.get("diag_run_id") or ""))
            except Exception:
                pass
            # UI-provided inputs (preferred truth)
            _d.setdefault("ui_raw", _fix41u_ui_raw)
            _d.setdefault("ui_norm", list(_fix41u_extra_urls))
            _d.setdefault("intake_norm", list(_fix41u_extra_urls))
            # Replay visibility (diagnostic only; NOT used for hashing unless caller also provided extras)
            if _fix41u_replayed:
                _d.setdefault("replayed_from_analysis_norm", list(_fix41u_replayed))
    except Exception:
        pass


    # Why:
    # - Diff Panel V2 previously called an undefined helper and silently fell back to {}
    #   which then prevented baseline fallback from ever running ({} is a dict).
    # - Provide one deterministic locator for baseline/current primary_metrics_canonical
    #   across the common payload shapes (top-level, primary_response, results, etc.).
    def _refactor89_locate_pmc_dict(obj) -> dict:
        try:
            if not isinstance(obj, dict):
                return {}
            # Most common: top-level
            pmc = obj.get("primary_metrics_canonical")
            if isinstance(pmc, dict) and pmc:
                return pmc
            # Common wrappers
            candidates = [
                ("primary_response", "primary_metrics_canonical"),
                ("results", "primary_metrics_canonical"),
                ("results", "primary_response", "primary_metrics_canonical"),
                ("primary_response", "results", "primary_metrics_canonical"),
                ("primary_response", "results", "primary_response", "primary_metrics_canonical"),
            ]
            for path in candidates:
                x = obj
                ok = True
                for k in path:
                    if not isinstance(x, dict):
                        ok = False
                        break
                    x = x.get(k)
                if ok and isinstance(x, dict) and x:
                    return x
        except Exception:
            pass
        return {}

    # Diff Panel V2 expects this name; define it so it can never NameError.
    def _diffpanel_v2__unwrap_primary_metrics_canonical(obj) -> dict:
        return _refactor89_locate_pmc_dict(obj)
    # Why:
    # - Some UI/Sheets paths provide a summarized wrapper that lacks primary_response,
    #   metric_schema_frozen, metric_anchors, baseline_sources_cache, etc.
    # - If a full_store_ref pointer exists, load the full payload deterministically.
    #
    # NOTE:
    # - Do NOT write to `output` here (output not built yet). We stash flags
    #   and attach them after `output = {...}` is created.
    _prev_rehydrated = False
    _prev_rehydrated_ref = ""

    _refactor89_prev_keys_sample_pre = []
    _refactor89_prev_pmc_count_pre = 0
    _refactor89_prev_pmc_keys_sample_pre = []
    _refactor89_prev_has_full_store_ref = False
    _refactor89_prev_has_snapshot_store_ref = False
    try:
        if isinstance(previous_data, dict):
            _refactor89_prev_keys_sample_pre = list(previous_data.keys())[:25]
            _refactor89_prev_has_full_store_ref = bool(previous_data.get("full_store_ref") or (previous_data.get("results") or {}).get("full_store_ref") or "")
            _refactor89_prev_has_snapshot_store_ref = bool(previous_data.get("snapshot_store_ref") or (previous_data.get("results") or {}).get("snapshot_store_ref") or "")
            _pmc_pre = _refactor89_locate_pmc_dict(previous_data)
            if isinstance(_pmc_pre, dict):
                _refactor89_prev_pmc_count_pre = int(len(_pmc_pre))
                _refactor89_prev_pmc_keys_sample_pre = list(_pmc_pre.keys())[:12]
    except Exception:
        pass

    try:
        if isinstance(previous_data, dict):
            _pr = previous_data.get("primary_response")

            # Determine if we are missing rebuild essentials
            _pmc_present = False
            try:
                _pmc_present = bool(_refactor89_locate_pmc_dict(previous_data) or (_refactor89_locate_pmc_dict(_pr) if isinstance(_pr, dict) else {}))
            except Exception:
                _pmc_present = False

            # Determine if we are missing rebuild essentials (schema OR baseline PMC)
            _need = (
                (not isinstance(_pr, dict))
                or (not _pr)
                or (not isinstance(_pr.get("metric_schema_frozen"), dict))
                or (not _pmc_present)
            )

            if _need:
                # Explicit line (requested): simplest location first
                ref = previous_data.get("full_store_ref", "")  # <-- requested line

                # Then fall back to other known wrapper locations (more robust)
                _ref = (
                    ref
                    or (previous_data.get("results") or {}).get("full_store_ref")
                    or (isinstance(_pr, dict) and _pr.get("full_store_ref"))
                    or ""
                )

                # Last-ditch deterministic fallback: if wrapper carries _sheet_id
                if (not _ref) and isinstance(previous_data.get("_sheet_id"), str) and previous_data.get("_sheet_id"):
                    _ref = f"gsheet:HistoryFull:{previous_data.get('_sheet_id')}"

                if isinstance(_ref, str) and _ref.startswith("gsheet:"):
                    parts = _ref.split(":")
                    _ws_title = parts[1] if len(parts) > 1 and parts[1] else "HistoryFull"
                    _aid = parts[2] if len(parts) > 2 else ""
                    full = load_full_history_payload_from_sheet(_aid, worksheet_title=_ws_title) if _aid else {}
                    if isinstance(full, dict) and full:
                        previous_data = full
                        _prev_rehydrated = True
                        _prev_rehydrated_ref = _ref
    except Exception:
        pass


    try:
        if isinstance(previous_data, dict):
            _pmc_now = _refactor89_locate_pmc_dict(previous_data)
            if not (isinstance(_pmc_now, dict) and _pmc_now):
                try:
                    _pf = _fix24_get_prev_full_payload(previous_data)
                except Exception:
                    _pf = {}
                if isinstance(_pf, dict) and _pf:
                    previous_data = _pf
                    _prev_rehydrated = True
                    if not _prev_rehydrated_ref:
                        try:
                            _prev_rehydrated_ref = str(previous_data.get("full_store_ref") or previous_data.get("snapshot_store_ref") or "fix24_get_prev_full_payload")
                        except Exception:
                            _prev_rehydrated_ref = "fix24_get_prev_full_payload"
    except Exception:
        pass
    _refactor89_prev_payload_probe_v1 = {}
    try:
        _pmc_post = _refactor89_locate_pmc_dict(previous_data) if isinstance(previous_data, dict) else {}
        _refactor89_prev_payload_probe_v1 = {
            "prev_keys_sample": list(_refactor89_prev_keys_sample_pre or []),
            "prev_keys_sample_post": list(previous_data.keys())[:25] if isinstance(previous_data, dict) else [],
            "has_primary_metrics_canonical": bool(_refactor89_prev_pmc_count_pre),
            "has_full_store_ref": bool(_refactor89_prev_has_full_store_ref),
            "has_snapshot_store_ref": bool(_refactor89_prev_has_snapshot_store_ref),
            "rehydrated_prev_ok": bool(_prev_rehydrated),
            "rehydrated_ref": str(_prev_rehydrated_ref or ""),
            "baseline_pmc_count": int(len(_pmc_post)) if isinstance(_pmc_post, dict) else 0,
            "baseline_pmc_keys_sample": list(_pmc_post.keys())[:12] if isinstance(_pmc_post, dict) else [],
            "baseline_pmc_count_pre": int(_refactor89_prev_pmc_count_pre or 0),
            "baseline_pmc_keys_sample_pre": list(_refactor89_prev_pmc_keys_sample_pre or []),
        }
    except Exception:
        _refactor89_prev_payload_probe_v1 = {
            "prev_keys_sample": list(_refactor89_prev_keys_sample_pre or []),
            "rehydrated_prev_ok": bool(_prev_rehydrated),
            "rehydrated_ref": str(_prev_rehydrated_ref or ""),
        }

    snapshot_origin = "none"
    baseline_sources_cache = []

    try:
        if isinstance(previous_data, dict):
            # 1) results.baseline_sources_cache (preferred)
            r = previous_data.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache"), list):
                baseline_sources_cache = r.get("baseline_sources_cache") or []
                if baseline_sources_cache:
                    snapshot_origin = "analysis_results_cache"

            # 2) top-level baseline_sources_cache
            if not baseline_sources_cache and isinstance(previous_data.get("baseline_sources_cache"), list):
                baseline_sources_cache = previous_data.get("baseline_sources_cache") or []
                if baseline_sources_cache:
                    snapshot_origin = "analysis_top_level_cache"
    except Exception:
        pass
        baseline_sources_cache = []

    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            pr = previous_data.get("primary_response") or {}
            if isinstance(pr, dict):
                # A) primary_response.results.baseline_sources_cache
                r2 = pr.get("results")
                if (not baseline_sources_cache) and isinstance(r2, dict) and isinstance(r2.get("baseline_sources_cache"), list):
                    baseline_sources_cache = r2.get("baseline_sources_cache") or []
                    if baseline_sources_cache:
                        snapshot_origin = "primary_response_results_cache"

                # B) primary_response.baseline_sources_cache
                if (not baseline_sources_cache) and isinstance(pr.get("baseline_sources_cache"), list):
                    baseline_sources_cache = pr.get("baseline_sources_cache") or []
                    if baseline_sources_cache:
                        snapshot_origin = "primary_response_top_level_cache"

                # C) primary_response.results.source_results (reconstruct minimal snapshot shape)
                if (not baseline_sources_cache) and isinstance(r2, dict) and isinstance(r2.get("source_results"), list):
                    rebuilt_sr = []
                    for sr in (r2.get("source_results") or []):
                        if not isinstance(sr, dict):
                            continue
                        u = sr.get("source_url") or sr.get("url")
                        ex = sr.get("extracted_numbers")
                        if u and isinstance(ex, list) and ex:
                            rebuilt_sr.append({
                                "source_url": u,
                                "extracted_numbers": ex,
                                "clean_text": sr.get("clean_text") or sr.get("content") or "",
                                "fingerprint": sr.get("fingerprint"),
                                "fetched_at": sr.get("fetched_at"),
                            })
                    rebuilt_sr.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))
                    if rebuilt_sr:
                        baseline_sources_cache = rebuilt_sr
                        snapshot_origin = "primary_response_source_results_rebuild"

        # D) previous_data.results.source_results fallback
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            r3 = previous_data.get("results")
            if isinstance(r3, dict) and isinstance(r3.get("source_results"), list):
                rebuilt_sr2 = []
                for sr in (r3.get("source_results") or []):
                    if not isinstance(sr, dict):
                        continue
                    u = sr.get("source_url") or sr.get("url")
                    ex = sr.get("extracted_numbers")
                    if u and isinstance(ex, list) and ex:
                        rebuilt_sr2.append({
                            "source_url": u,
                            "extracted_numbers": ex,
                            "clean_text": sr.get("clean_text") or sr.get("content") or "",
                            "fingerprint": sr.get("fingerprint"),
                            "fetched_at": sr.get("fetched_at"),
                        })
                rebuilt_sr2.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))
                if rebuilt_sr2:
                    baseline_sources_cache = rebuilt_sr2
                    snapshot_origin = "analysis_source_results_rebuild"
    except Exception:
        pass

    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            _er = None
            if isinstance(previous_data.get("results"), dict):
                _er = previous_data["results"].get("evidence_records")
            if _er is None:
                _er = previous_data.get("evidence_records")
            _rebuilt = build_baseline_sources_cache_from_evidence_records(_er)
            if isinstance(_rebuilt, list) and _rebuilt:
                baseline_sources_cache = _rebuilt
                snapshot_origin = "evidence_records_rebuild"
    except Exception:
        pass

    _snapshot_debug = None
    try:
        _raw_len = int(len(baseline_sources_cache)) if isinstance(baseline_sources_cache, list) else 0
        _kept = []
        if isinstance(baseline_sources_cache, list):
            for s in baseline_sources_cache:
                if not isinstance(s, dict):
                    continue
                u = s.get("source_url") or s.get("url")
                ex = s.get("extracted_numbers")
                if u and isinstance(ex, list):
                    _kept.append(s)
        _kept.sort(key=lambda d: (str(d.get("source_url") or d.get("url") or ""), str(d.get("fingerprint") or "")))
        baseline_sources_cache = _kept
        _snapshot_debug = {
            "origin": snapshot_origin,
            "raw_count": _raw_len,
            "valid_count": int(len(baseline_sources_cache)),
            "example_urls": [x.get("source_url") or x.get("url") for x in (baseline_sources_cache[:3] if isinstance(baseline_sources_cache, list) else [])],
            "prev_keys": sorted(list(previous_data.keys()))[:40] if isinstance(previous_data, dict) else [],
        }
    except Exception:
        pass

    # 3) reconstruct from web_context.scraped_meta (if provided)
    if (not baseline_sources_cache) and isinstance(web_context, dict):
        try:
            scraped_meta = web_context.get("scraped_meta") or {}
            rebuilt = []
            if isinstance(scraped_meta, dict):
                for url, meta in scraped_meta.items():
                    if not isinstance(meta, dict):
                        continue
                    content = meta.get("clean_text") or meta.get("content") or ""
                    fp = meta.get("fingerprint") or _fingerprint(content)
                    if not fp or len(content or "") < 800:
                        continue
                    nums = meta.get("extracted_numbers") or []
                    if not isinstance(nums, list):
                        nums = []
                    rebuilt.append({
                        "url": url,
                        "status": meta.get("status") or "fetched",
                        "status_detail": meta.get("status_detail") or "",
                        "numbers_found": _safe_int(meta.get("numbers_found"), default=len(nums)),
                        "fetched_at": meta.get("fetched_at") or _now(),
                        "fingerprint": fp,
                        "content_type": meta.get("content_type") or "",
                        "extracted_numbers": [
                            {
                                "value": n.get("value"),
                                "unit": n.get("unit"),
                                "raw": n.get("raw"),
                                "context_snippet": (n.get("context_snippet") or n.get("context") or "")[:200],
                                "anchor_hash": n.get("anchor_hash"),
                                "is_junk": n.get("is_junk"),
                                "junk_reason": n.get("junk_reason"),
                                "unit_tag": n.get("unit_tag"),
                                "unit_family": n.get("unit_family"),
                                "base_unit": n.get("base_unit"),
                                "multiplier_to_base": n.get("multiplier_to_base"),
                                "value_norm": n.get("value_norm"),
                                "start_idx": n.get("start_idx"),
                                "end_idx": n.get("end_idx"),
                                "source_url": n.get("source_url") or url,
                            }
                            for n in nums if isinstance(n, dict)
                        ]
                    })
            if rebuilt:
                baseline_sources_cache = rebuilt
                snapshot_origin = "web_context_scraped_meta"
        except Exception:
            pass

    # Also count invalid snapshots for debug (if present)
    invalid_count = 0
    try:
        if isinstance(previous_data, dict):
            r = previous_data.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache_invalid"), list):
                invalid_count = len(r.get("baseline_sources_cache_invalid") or [])
    except Exception:
        pass
        invalid_count = 0

    output = {
        "status": "success",
        "message": "",
        "sources_checked": 0,
        "sources_fetched": 0,
        "numbers_extracted_total": 0,
        "stability_score": 0.0,
        "summary": {
            "total_metrics": 0,
            "metrics_found": 0,
            "metrics_increased": 0,
            "metrics_decreased": 0,
            "metrics_unchanged": 0,
        },
        "metric_changes": [],
        "source_results": [],
        "interpretation": "",
        "snapshot_origin": snapshot_origin,
        "valid_snapshot_count": len(baseline_sources_cache or []),
        "invalid_snapshot_count": int(invalid_count),
        "generated_at": _now(),
    }

    try:
        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict) and isinstance(locals().get("_refactor89_prev_payload_probe_v1"), dict):
            output["debug"]["prev_payload_probe_v1"] = dict(locals().get("_refactor89_prev_payload_probe_v1") or {})
    except Exception:
        pass


    # - Always stamp CODE_VERSION into output
    # - Create output['debug'] container (non-breaking)
    # - Track fastpath eligibility + reason in a deterministic way
    try:
        output["code_version"] = _yureeka_get_code_version()
    except Exception:
        pass
    try:
        _yureeka_lock_version_globals_v1()
        _yureeka_ensure_final_bindings_v1()
        if not isinstance(output.get("debug"), dict):
            output["debug"] = {}

        # REFACTOR17: Binding manifest should reflect the *actual* Diff Panel V2 entrypoint.
        _fn_v2 = None
        _bf_v2 = ""
        for _cand_name in ["build_diff_metrics_panel_v2__rows", "build_diff_metrics_panel_v2"]:
            try:
                _cand = globals().get(_cand_name)
            except Exception:
                _cand = None
            if callable(_cand):
                _fn_v2 = _cand
                _bf_v2 = _cand_name
                break
        try:
            if _bf_v2:
                globals()["_YUREEKA_DIFF_PANEL_V2_ENTRYPOINT_BOUND_FROM"] = _bf_v2
        except Exception:
            pass

        # Legacy diff entrypoint (may be absent during early Streamlit execution).
        _fn = None
        _bf = ""
        try:
            _cand = globals().get("diff_metrics_by_name")
        except Exception:
            _cand = None
        if callable(_cand):
            _fn = _cand
            _bf = "diff_metrics_by_name"
        if not callable(_fn):
            for _cand_name in [
                "diff_metrics_by_name",
                "_yureeka_diff_metrics_by_name_v24",
                "diff_metrics_by_name_V24_BASE",
                "_refactor09_diff_metrics_by_name",
                "diff_metrics_by_name_FIX41_V34C_UNWRAP",
                "diff_metrics_by_name_FIX41_V34_ANCHOR_JOIN",
                "diff_metrics_by_name_FIX40_V32_PREFER_PMC",
                "diff_metrics_by_name_FIX34_V24_STRICT",
                "diff_metrics_by_name_FIX33_V23_CANONICAL_CLEAR",
                "diff_metrics_by_name_FIX2D34",
            ]:
                try:
                    _cand = globals().get(_cand_name)
                except Exception:
                    _cand = None
                if callable(_cand):
                    _fn = _cand
                    _bf = _cand_name
                    try:
                        globals()["diff_metrics_by_name"] = _cand
                    except Exception:
                        pass
                    break

        # Last resort: scan globals for any callable that looks like a diff_metrics_by_name variant.
        if not callable(_fn):
            try:
                _hits = []
                for _k, _v in list(globals().items()):
                    if "diff_metrics_by_name" in str(_k) and callable(_v):
                        _hits.append((_k, _v))
                if _hits:
                    _hits.sort(key=lambda t: str(t[0]))
                    _bf, _fn = _hits[-1]
            except Exception:
                pass

        try:
            if _bf:
                globals()["_YUREEKA_DIFF_METRICS_BY_NAME_BOUND_FROM"] = _bf
        except Exception:
            pass

        _auth_tag = ""
        _auth_src = ""
        _auth_set_ok = False
        try:
            if callable(_fn):
                _auth_set_ok = bool(_yureeka_set_authoritative_binding_v1(_fn, _yureeka_get_code_version()))
        except Exception:
            _auth_set_ok = False

        try:
            _auth_tag = str(getattr(_fn, "__YUREEKA_AUTHORITATIVE_BINDING__", "") or "")
            if _auth_tag:
                _auth_src = "attr"
        except Exception:
            _auth_tag = ""
        if not _auth_tag:
            try:
                _auth_tag = str(globals().get("_YUREEKA_DIFF_METRICS_BY_NAME_AUTHORITATIVE_TAG") or "")
                if _auth_tag:
                    _auth_src = "globals"
            except Exception:
                _auth_tag = ""

        _man = {
            "code_version": _yureeka_get_code_version(),
            "authority_manifest_v1": _yureeka_authority_manifest_v1(),
            "final_bindings_version": str(globals().get("_YUREEKA_FINAL_BINDINGS_VERSION") or ""),

            # Diff Panel V2 (authoritative for metric_changes_v2)
            "diff_panel_v2_entrypoint_name": str(getattr(_fn_v2, "__name__", "") or ""),
            "diff_panel_v2_entrypoint_qualname": str(getattr(_fn_v2, "__qualname__", "") or ""),
            "diff_panel_v2_entrypoint_module": str(getattr(_fn_v2, "__module__", "") or ""),
            "diff_panel_v2_entrypoint_id": str(id(_fn_v2)) if _fn_v2 is not None else "",
            "diff_panel_v2_entrypoint_bound_from": str(_bf_v2 or ""),

            # Legacy diff (best-effort)
            "diff_metrics_by_name_authoritative": str(_auth_tag or ""),
            "diff_metrics_by_name_authoritative_source": str(_auth_src or ""),
            "diff_metrics_by_name_authoritative_set_ok": bool(_auth_set_ok),
            "diff_metrics_by_name_name": str(getattr(_fn, "__name__", "") or ""),
            "diff_metrics_by_name_qualname": str(getattr(_fn, "__qualname__", "") or ""),
            "diff_metrics_by_name_module": str(getattr(_fn, "__module__", "") or ""),
            "diff_metrics_by_name_id": str(id(_fn)) if _fn is not None else "",
            "diff_metrics_by_name_bound_from": str(_bf or ""),
        }

        if not _man.get("diff_panel_v2_entrypoint_name"):
            _man["warning_v2_missing"] = "Diff Panel V2 entrypoint not resolved at manifest time (possible early Streamlit trigger before later defs)."
        if not _man.get("diff_metrics_by_name_name"):
            _man["note_legacy_missing"] = "Legacy diff_metrics_by_name not resolved at manifest time; Diff Panel V2 is authoritative for metric_changes_v2."

        output["debug"]["binding_manifest_v1"] = _man
    except Exception:
        pass


    try:
        if not isinstance(output.get("debug"), dict):
            output["debug"] = {}
        output["debug"].setdefault("fix35", {})
        output["debug"]["fix35"]["current_metrics_origin"] = "unknown"
        output["debug"]["fix35"]["fastpath_eligible"] = False
        output["debug"]["fix35"]["fastpath_reason"] = ""
    except Exception:
        pass

    # Attach debug flags (rehydration + snapshot_debug)
    try:
        if _prev_rehydrated:
            output["previous_data_rehydrated"] = True
            output["previous_data_full_store_ref"] = _prev_rehydrated_ref
    except Exception:
        pass
    try:
        if isinstance(_snapshot_debug, dict) and _snapshot_debug:
            output["snapshot_debug"] = _snapshot_debug
    except Exception:
        pass

    try:
        if not baseline_sources_cache and isinstance(previous_data, dict):
            _ref = previous_data.get("snapshot_store_ref") or (previous_data.get("results") or {}).get("snapshot_store_ref")
            _hash = previous_data.get("source_snapshot_hash") or (previous_data.get("results") or {}).get("source_snapshot_hash")

            # Why:
            # - Analysis now emits stable/v2 snapshot hashes (source_snapshot_hash_v2 / _stable) and
            #   snapshot_store_ref_v2 pointing at the same Snapshots row key.
            # - Evolution must prefer these fields to keep fastpath alignment intact.
            try:
                _ref_v2 = previous_data.get("snapshot_store_ref_v2") or previous_data.get("snapshot_store_ref_stable")
                if (not _ref_v2) and isinstance(previous_data.get("results"), dict):
                    _ref_v2 = (previous_data.get("results") or {}).get("snapshot_store_ref_v2") or (previous_data.get("results") or {}).get("snapshot_store_ref_stable")
                if isinstance(_ref_v2, str) and _ref_v2:
                    _ref = _ref_v2  # prefer stable ref
            except Exception:
                pass

            try:
                _hash_stable = previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2")
                if (not _hash_stable) and isinstance(previous_data.get("results"), dict):
                    _hash_stable = (previous_data.get("results") or {}).get("source_snapshot_hash_stable") or (previous_data.get("results") or {}).get("source_snapshot_hash_v2")
                if isinstance(_hash_stable, str) and _hash_stable:
                    _hash = _hash_stable  # prefer stable hash
            except Exception:
                pass

            if isinstance(_ref, str) and _ref.startswith("gsheet:"):
                parts = _ref.split(":")
                _ws_title = parts[1] if len(parts) > 1 and parts[1] else "Snapshots"
                _h = parts[2] if len(parts) > 2 else ""
                baseline_sources_cache = load_full_snapshots_from_sheet(_h, worksheet_title=_ws_title) if _h else []
                if baseline_sources_cache:
                    output["snapshot_origin"] = "sheet_snapshot_store_ref"

            if not baseline_sources_cache and isinstance(_hash, str) and _hash:
                baseline_sources_cache = load_full_snapshots_from_sheet(_hash, worksheet_title="Snapshots")
                if baseline_sources_cache:
                    output["snapshot_origin"] = "sheet_source_snapshot_hash"

            if not baseline_sources_cache and isinstance(_ref, str) and _ref and not _ref.startswith("gsheet:"):
                baseline_sources_cache = load_full_snapshots_local(_ref)
                if baseline_sources_cache:
                    output["snapshot_origin"] = "local_snapshot_store_ref"

            if isinstance(baseline_sources_cache, list):
                output["valid_snapshot_count"] = len(baseline_sources_cache)
    except Exception:
        pass


    # Purpose:
    # - During HistoryFull persistence we may omit baseline_sources_cache to avoid Sheets cell limits,
    #   and instead persist snapshots in the Snapshots worksheet / local snapshot store with a ref/hash.
    # - Source-anchored evolution MUST be able to rehydrate baseline snapshots from:
    #     * snapshot_store_ref / snapshot_store_ref_v2 (gsheet:Snapshots:<hash> OR local path)
    #     * source_snapshot_hash_v2 / source_snapshot_hash
    # Behavior:
    # - If baseline_sources_cache is empty after normal discovery, attempt to load snapshots deterministically.
    # - Still strict: if we cannot load snapshots, we remain snapshot-gated (no fabricated matches).
    _snapshot_store_debug = {}
    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            _res = previous_data.get("results") if isinstance(previous_data.get("results"), dict) else {}
            _pr = previous_data.get("primary_response") if isinstance(previous_data.get("primary_response"), dict) else {}
            _pr_res = _pr.get("results") if isinstance(_pr.get("results"), dict) else {}

            # Prefer explicit refs; fall back to hashes
            _store_ref = (
                previous_data.get("snapshot_store_ref")
                or (_res.get("snapshot_store_ref") if isinstance(_res, dict) else "")
                or (_pr.get("snapshot_store_ref") if isinstance(_pr, dict) else "")
                or (_pr_res.get("snapshot_store_ref") if isinstance(_pr_res, dict) else "")
                or ""
            )

            _store_ref_v2 = (
                previous_data.get("snapshot_store_ref_v2")
                or (_res.get("snapshot_store_ref_v2") if isinstance(_res, dict) else "")
                or (_pr.get("snapshot_store_ref_v2") if isinstance(_pr, dict) else "")
                or (_pr_res.get("snapshot_store_ref_v2") if isinstance(_pr_res, dict) else "")
                or ""
            )

            _ssh_v2 = (
                previous_data.get("source_snapshot_hash_v2")
                or (_res.get("source_snapshot_hash_v2") if isinstance(_res, dict) else "")
                or (_pr.get("source_snapshot_hash_v2") if isinstance(_pr, dict) else "")
                or (_pr_res.get("source_snapshot_hash_v2") if isinstance(_pr_res, dict) else "")
                or ""
            )

            _ssh_v1 = (
                previous_data.get("source_snapshot_hash")
                or (_res.get("source_snapshot_hash") if isinstance(_res, dict) else "")
                or (_pr.get("source_snapshot_hash") if isinstance(_pr, dict) else "")
                or (_pr_res.get("source_snapshot_hash") if isinstance(_pr_res, dict) else "")
                or ""
            )

            def _extract_hash(ref: str) -> str:
                try:
                    ref = str(ref or "")
                    if ref.startswith("gsheet:Snapshots:"):
                        return ref.split(":")[-1]
                except Exception:
                    pass
                return ""

            _hash_to_load = _extract_hash(_store_ref_v2) or _extract_hash(_store_ref) or str(_ssh_v2 or "") or str(_ssh_v1 or "")

            _snapshot_store_debug = {
                "store_ref": str(_store_ref or ""),
                "store_ref_v2": str(_store_ref_v2 or ""),
                "ssh_v2_present": bool(_ssh_v2),
                "ssh_v1_present": bool(_ssh_v1),
                "hash_to_load": str(_hash_to_load or ""),
            }

            _loaded = []
            _loaded_origin = ""

            # 1) Load by explicit gsheet ref (v2 then v1)
            if isinstance(_store_ref_v2, str) and _store_ref_v2.startswith("gsheet:Snapshots:"):
                try:
                    _loaded = load_full_snapshots_from_sheet(_store_ref_v2.split(":")[-1], worksheet_title="Snapshots")
                    _loaded_origin = "gsheet_ref_v2"
                except Exception:
                    pass

            if (not _loaded) and isinstance(_store_ref, str) and _store_ref.startswith("gsheet:Snapshots:"):
                try:
                    _loaded = load_full_snapshots_from_sheet(_store_ref.split(":")[-1], worksheet_title="Snapshots")
                    _loaded_origin = "gsheet_ref_v1"
                except Exception:
                    pass

            # 2) Load by local store ref if it looks like a path
            if (not _loaded) and isinstance(_store_ref, str) and _store_ref and (not _store_ref.startswith("gsheet:")):
                try:
                    _loaded = load_full_snapshots_local(_store_ref)
                    _loaded_origin = "local_ref"
                except Exception:
                    pass

            # 3) Load by hash (sheet first, then deterministic local path)
            if (not _loaded) and _hash_to_load:
                try:
                    _loaded = load_full_snapshots_from_sheet(str(_hash_to_load), worksheet_title="Snapshots")
                    _loaded_origin = "gsheet_hash"
                except Exception:
                    pass

            if (not _loaded) and _hash_to_load:
                try:
                    # Deterministic path used by store_full_snapshots_local
                    import os
                    _p = os.path.join(_snapshot_store_dir(), f"{str(_hash_to_load)}.json")
                    _loaded = load_full_snapshots_local(_p)
                    if _loaded:
                        _loaded_origin = "local_hash_path"
                except Exception:
                    pass

            if isinstance(_loaded, list) and _loaded:
                baseline_sources_cache = _loaded
                snapshot_origin = f"snapshot_store_fallback:{_loaded_origin}"
                _snapshot_store_debug["loaded_count"] = int(len(_loaded))
                _snapshot_store_debug["loaded_origin"] = str(_loaded_origin)
            else:
                _snapshot_store_debug["loaded_count"] = 0
                _snapshot_store_debug["loaded_origin"] = str(_loaded_origin or "none")
    except Exception:
        pass

    # Re-validate snapshot shape after fallback load (keeps strict invariants)
    try:
        if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
            _kept2 = []
            for s in baseline_sources_cache:
                if not isinstance(s, dict):
                    continue
                u = s.get("source_url") or s.get("url")
                ex = s.get("extracted_numbers")
                # Some legacy stores use "numbers" instead of "extracted_numbers"
                if ex is None and isinstance(s.get("numbers"), list):
                    try:
                        s["extracted_numbers"] = s.get("numbers") or []
                        ex = s.get("extracted_numbers")
                    except Exception:
                        pass
                if u and isinstance(ex, list):
                    _kept2.append(s)
            _kept2.sort(key=lambda d: (str(d.get("source_url") or d.get("url") or ""), str(d.get("fingerprint") or "")))
            baseline_sources_cache = _kept2
            # Update debug if available
            if isinstance(_snapshot_debug, dict):
                _snapshot_debug["origin"] = snapshot_origin
                _snapshot_debug["valid_count"] = int(len(baseline_sources_cache))
    except Exception:
        pass

    # If no valid snapshots, return "not_found"
    if not baseline_sources_cache:
        try:
            output.setdefault("debug", {})
            if isinstance(output.get("debug"), dict):
                if isinstance(_snapshot_debug, dict):
                    output["debug"]["snapshot_debug_v1"] = _snapshot_debug
                if isinstance(_snapshot_store_debug, dict) and _snapshot_store_debug:
                    output["debug"]["snapshot_store_debug_v1"] = _snapshot_store_debug
        except Exception:
            pass
        output["status"] = "failed"
        output["message"] = "No valid snapshots available for source-anchored evolution. (Snapshot store fallback attempted; no re-fetch / no heuristic matching performed.)"
        output["interpretation"] = "Snapshot-gated: evolution refused to fabricate matches without valid cached source text."
        _fix2d20_trace_year_like_commits(output, stage=str((output or {}).get('debug',{}).get('stage') or 'evolution'), callsite='compute_source_anchored_diff_return')
        return output    # ---------- Use your existing deterministic metric diff helper ----------
    prev_response = (previous_data or {}).get("primary_response", {}) or {}

    try:
        if (not isinstance(prev_response, dict) or not prev_response) and isinstance(previous_data, dict):
            if isinstance(previous_data.get("primary_metrics_canonical"), dict) or isinstance(previous_data.get("metric_schema_frozen"), dict):
                prev_response = previous_data
    except Exception:
        pass

    prev_metrics = prev_response.get("primary_metrics_canonical") or prev_response.get("primary_metrics") or {}

    # (safe alias for prior `prev_analysis` usage)
    prev_analysis = previous_data  # PATCH CSR_INPUTS1_ALIAS (ADDITIVE)
    try:
        prev_schema = _first_present(prev_analysis, [
            ("metric_schema_frozen",),
            ("primary_response", "metric_schema_frozen"),
            ("results", "metric_schema_frozen"),
            ("results", "primary_response", "metric_schema_frozen"),
        ], default=None)

        prev_canon = _first_present(prev_analysis, [
            ("primary_metrics_canonical",),
            ("primary_response", "primary_metrics_canonical"),
            ("results", "primary_metrics_canonical"),
            ("results", "primary_response", "primary_metrics_canonical"),
        ], default=None)

        prev_anchors = _first_present(prev_analysis, [
            ("metric_anchors",),
            ("primary_response", "metric_anchors"),
            ("results", "metric_anchors"),
            ("results", "primary_response", "metric_anchors"),
        ], default=None)

        if (not isinstance(prev_schema, dict) or not prev_schema) and isinstance(prev_canon, dict) and prev_canon:
            try:
                fn = globals().get("freeze_metric_schema")
                if callable(fn):
                    prev_schema = fn(prev_canon)
            except Exception:
                pass
    except Exception:
        pass


    # Why:
    # - Diff Panel V2 consumes prev_response.primary_metrics_canonical.
    # - HistoryFull rehydrate can place canonical metrics under nested containers
    #   (e.g., previous_data.results.primary_metrics_canonical), leaving prev_response empty.
    # What:
    # - If prev_canon exists, copy into prev_response.primary_metrics_canonical when missing.
    # - Also expose at top-level previous_data.primary_metrics_canonical (debug/compat).
    # - Record debug counts for closure verification.
    try:
        if isinstance(prev_response, dict):
            if (not isinstance(prev_response.get("primary_metrics_canonical"), dict)) or (not prev_response.get("primary_metrics_canonical")):
                if isinstance(prev_canon, dict) and prev_canon:
                    prev_response["primary_metrics_canonical"] = prev_canon
        if isinstance(previous_data, dict):
            if (not isinstance(previous_data.get("primary_metrics_canonical"), dict)) or (not previous_data.get("primary_metrics_canonical")):
                if isinstance(prev_canon, dict) and prev_canon:
                    previous_data["primary_metrics_canonical"] = prev_canon
    except Exception:
        pass

    try:
        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            output["debug"].setdefault("fix2d73", {})
            if isinstance(output["debug"].get("fix2d73"), dict):
                output["debug"]["fix2d73"].update({
                    "prev_canon_count": int(len(prev_canon)) if isinstance(prev_canon, dict) else 0,
                    "prev_response_pmc_count": int(len(prev_response.get("primary_metrics_canonical") or {})) if isinstance(prev_response, dict) and isinstance(prev_response.get("primary_metrics_canonical"), dict) else 0,
                    "previous_data_top_pmc_count": int(len(previous_data.get("primary_metrics_canonical") or {})) if isinstance(previous_data, dict) and isinstance(previous_data.get("primary_metrics_canonical"), dict) else 0,
                })
    except Exception:
        pass
    # Ensure schema/anchors are available inside prev_response (additive copies)
    try:
        if isinstance(prev_response, dict) and not isinstance(prev_response.get("metric_schema_frozen"), dict):
            if isinstance(previous_data.get("metric_schema_frozen"), dict):
                prev_response["metric_schema_frozen"] = previous_data.get("metric_schema_frozen")
    except Exception:
        pass
    try:
        if isinstance(prev_response, dict) and not isinstance(prev_response.get("metric_anchors"), dict):
            if isinstance(previous_data.get("metric_anchors"), dict):
                prev_response["metric_anchors"] = previous_data.get("metric_anchors")
    except Exception:
        pass

    #
    # Principle:
    #   If the source snapshot inputs are proven unchanged, do NOT perform any
    #   anchor-based selection or rebuild "gymnastics". Reuse the already
    #   processed + schema-gated metrics from the previous analysis payload and
    #   publish directly.
    #
    # Implementation notes:
    #   - We compute a stable hash from baseline_sources_cache[*].extracted_numbers
    #     using a reduced, order-independent projection.
    #   - If it matches previous_data/source_snapshot_hash AND a prior processed
    #     canonical metrics dict exists, we set current_metrics to prev_metrics
    #     and force anchors to be ignored by short-circuiting _get_metric_anchors().
    #   - This is purely additive and does not remove legacy paths.
    _fix31_authoritative_reuse = False
    try:
        import json as _fix31_json
        import hashlib as _fix31_hashlib

        def _fix31_stable_dumps(obj):
            try:
                return _fix31_json.dumps(obj, sort_keys=True, ensure_ascii=False, separators=(",", ":"))
            except Exception:
                pass
                # last resort
                return str(obj)

        def _fix31_snapshot_fingerprint(bsc):
            # Reduced projection: stable across benign field additions/ordering
            rows = []
            for sr in (bsc or []):
                if not isinstance(sr, dict):
                    continue
                u = sr.get("source_url") or sr.get("url") or ""
                nums = []
                for n in (sr.get("extracted_numbers") or []):
                    if not isinstance(n, dict):
                        continue
                    nums.append({
                        "anchor_hash": n.get("anchor_hash") or "",
                        "value_norm": n.get("value_norm"),
                        "unit_tag": n.get("unit_tag") or "",
                        "unit": n.get("unit") or n.get("unit_norm") or "",
                        "currency": n.get("currency") or n.get("currency_symbol") or "",
                        "is_percent": bool(n.get("is_percent") or n.get("has_percent")),
                        "is_junk": bool(n.get("is_junk")),
                    })
                # order-independent for candidates
                nums = sorted(nums, key=lambda x: (_fix31_stable_dumps(x)))
                rows.append({"source_url": u, "extracted_numbers": nums})
            rows = sorted(rows, key=lambda r: r.get("source_url") or "")
            payload = _fix31_stable_dumps(rows).encode("utf-8", errors="ignore")
            return _fix31_hashlib.sha256(payload).hexdigest()

        # - Use the SAME hash function as analysis (compute_source_snapshot_hash_v2) whenever possible.
        # - Falls back to legacy compute_source_snapshot_hash, then to the reduced fingerprint.
        def _fix37_snapshot_hash_stable(bsc):
            try:
                if isinstance(bsc, list) and bsc:
                    try:
                        _h2 = compute_source_snapshot_hash_v2(bsc)
                        if _h2:
                            return str(_h2)
                    except Exception:
                        pass
                    try:
                        _h1 = compute_source_snapshot_hash(bsc)
                        if _h1:
                            return str(_h1)
                    except Exception:
                        pass
            except Exception:
                return _fix31_snapshot_fingerprint(bsc)

        _prev_hash = None
        _prev_hash_stable = None
        if isinstance(previous_data, dict):
            _prev_hash_stable = previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2")
            try:
                if not _prev_hash_stable and isinstance(previous_data.get("results"), dict):
                    _prev_hash_stable = (previous_data.get("results") or {}).get("source_snapshot_hash_stable") or (previous_data.get("results") or {}).get("source_snapshot_hash_v2")
            except Exception:
                pass
            _prev_hash = _prev_hash_stable or previous_data.get("source_snapshot_hash")
            _prev_hash_pref = _prev_hash_stable or previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2") or _prev_hash

            try:
                if not _prev_hash and isinstance(previous_data.get("results"), dict):
                    _prev_hash = (previous_data.get("results") or {}).get("source_snapshot_hash")
            except Exception:
                pass

        # - Record current/previous hashes even on mismatch
        # - Explain which prerequisite failed (no_prev_hash / no_prev_metrics / no_snapshots / hash_mismatch)
        _fix36_cur_hash = None
        _fix36_reason = ""

        #
        # Why:
        # - In your latest evolution JSON, fastpath was correctly bypassed due to injected delta,
        #   but the current snapshot universe (baseline_sources_cache) still did NOT include the
        #   injected URL, so the stable hash still matched and downstream logic treated the run
        #   as "no delta" (no fetch, no rebuild, no injected lifecycle).
        #
        # Goal:
        # - BEFORE computing the current stable hash / fastpath eligibility, deterministically
        #   append placeholder snapshot rows for any injected URLs missing from the current
        #   baseline_sources_cache universe. This makes the hash differ (as it should when the
        #   source universe changes), forcing the normal rebuild/fetch pathways without changing
        #   the hashing algorithm itself.
        #
        # Safety:
        # - Purely additive.
        # - No effect when no injected URLs are present OR all injected URLs already exist in
        #   baseline_sources_cache.
        try:
            _fx15_wc = web_context if isinstance(web_context, dict) else {}
            _fx15_extra = []
            # Prefer already-wired list fields
            if isinstance(_fx15_wc.get("extra_urls"), (list, tuple)):
                _fx15_extra = list(_fx15_wc.get("extra_urls") or [])
            elif isinstance(_fx15_wc.get("diag_extra_urls_ui"), (list, tuple)) and _fx15_wc.get("diag_extra_urls_ui"):
                _fx15_extra = list(_fx15_wc.get("diag_extra_urls_ui") or [])
            elif isinstance(_fx15_wc.get("diag_extra_urls_ui_raw"), str) and (_fx15_wc.get("diag_extra_urls_ui_raw") or "").strip():
                _raw = str(_fx15_wc.get("diag_extra_urls_ui_raw") or "")
                _parts = []
                for _line in _raw.splitlines():
                    _line = (_line or "").strip()
                    if not _line:
                        continue
                    for _p in _line.split(","):
                        _p = (_p or "").strip()
                        if _p:
                            _parts.append(_p)
                _fx15_extra = _parts

            _fx15_inj_norm = _inj_diag_norm_url_list(_fx15_extra) if _fx15_extra else []
            if _fx15_inj_norm and isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                _fx15_base_urls = []
                for _r in (baseline_sources_cache or []):
                    if not isinstance(_r, dict):
                        continue
                    _u = _r.get("source_url") or _r.get("url") or ""
                    if isinstance(_u, str) and _u:
                        _fx15_base_urls.append(_u)
                _fx15_base_set = set(_inj_diag_norm_url_list(_fx15_base_urls)) if _fx15_base_urls else set()
                _fx15_delta = [u for u in _fx15_inj_norm if u and u not in _fx15_base_set]
                if _fx15_delta:
                    # Append stable placeholders so the snapshot hash changes deterministically.
                    for _u in _fx15_delta:
                        baseline_sources_cache.append({
                            "source_url": _u,
                            "url": _u,
                            "status": "injected_pending",
                            "status_detail": "injected_url_placeholder_pre_hash",
                            "snapshot_text": "",
                            "extracted_numbers": [],
                            "numbers_found": 0,
                            "injected": True,
                            "injected_reason": "prehash_placeholder",
                        })
                    # Also ensure downstream sees a consistent universe via web_context["extra_urls"].
                    try:
                        if isinstance(_fx15_wc, dict):
                            _fx15_wc.setdefault("extra_urls", [])
                            if isinstance(_fx15_wc.get("extra_urls"), list):
                                # keep original order; append unique normalized
                                _seen = set(_inj_diag_norm_url_list(_fx15_wc.get("extra_urls") or []))
                                for _u in _fx15_delta:
                                    if _u not in _seen:
                                        _fx15_wc["extra_urls"].append(_u)
                                        _seen.add(_u)
                    except Exception:
                        pass
                    # Debug
                    try:
                        output.setdefault("debug", {})
                        if isinstance(output.get("debug"), dict):
                            output["debug"].setdefault("fix41afc15", {})
                            if isinstance(output["debug"].get("fix41afc15"), dict):
                                output["debug"]["fix41afc15"].update({
                                    "inj_norm_count": int(len(_fx15_inj_norm)),
                                    "inj_norm": list(_fx15_inj_norm),
                                    "inj_delta_count": int(len(_fx15_delta)),
                                    "inj_delta": list(_fx15_delta),
                                    "baseline_sources_cache_count_after_placeholder": int(len(baseline_sources_cache or [])),
                                })
                    except Exception:
                        pass
        except Exception:
            pass

        #
        # Observed gap (from evolution JSON):
        #   - Injected URLs were present in ui/intake/hash_inputs, but remained:
        #       status = "injected_pending" / status_detail = "injected_url_placeholder_pre_hash"
        #   - So they never produced snapshot_text / extracted_numbers, and thus could not
        #     influence metric rebuild beyond a "hash universe" delta.
        #
        # Goal:
        #   - When injection is present, attempt to fetch+extract the injected URLs (delta-only),
        #     and update their baseline_sources_cache rows in-place so downstream rebuild sees them
        #     like normal fetched sources (or explicit failed reasons).
        #
        # Safety:
        #   - Purely additive.
        #   - No effect when no injected URLs are present.
        #   - Only touches rows that are injected placeholders (status == injected_pending) OR
        #     URLs that are injected_delta (not already in baseline).
        try:
            _fx16_wc = web_context if isinstance(web_context, dict) else {}
            _fx16_extra = []
            if isinstance(_fx16_wc.get("extra_urls"), (list, tuple)) and _fx16_wc.get("extra_urls"):
                _fx16_extra = list(_fx16_wc.get("extra_urls") or [])
            elif isinstance(_fx16_wc.get("diag_extra_urls_ui"), (list, tuple)) and _fx16_wc.get("diag_extra_urls_ui"):
                _fx16_extra = list(_fx16_wc.get("diag_extra_urls_ui") or [])
            elif isinstance(_fx16_wc.get("diag_extra_urls_ui_raw"), str) and (_fx16_wc.get("diag_extra_urls_ui_raw") or "").strip():
                _raw = str(_fx16_wc.get("diag_extra_urls_ui_raw") or "")
                _parts = []
                for _line in _raw.splitlines():
                    _line = (_line or "").strip()
                    if not _line:
                        continue
                    for _p in _line.split(","):
                        _p = (_p or "").strip()
                        if _p:
                            _parts.append(_p)
                _fx16_extra = _parts

            _fx16_inj_norm = _inj_diag_norm_url_list(_fx16_extra) if _fx16_extra else []
            _fx16_base_urls = []
            if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                for _r in (baseline_sources_cache or []):
                    if not isinstance(_r, dict):
                        continue
                    _u = _r.get("source_url") or _r.get("url") or ""
                    if isinstance(_u, str) and _u:
                        _fx16_base_urls.append(_u)
            _fx16_base_set = set(_inj_diag_norm_url_list(_fx16_base_urls)) if _fx16_base_urls else set()
            _fx16_delta = [u for u in _fx16_inj_norm if u and u not in _fx16_base_set]

            # Identify placeholder rows that should be fetched
            _fx16_targets = []
            if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                for _r in (baseline_sources_cache or []):
                    if not isinstance(_r, dict):
                        continue
                    _u = _r.get("source_url") or _r.get("url") or ""
                    _u_norm = _inj_diag_norm_url_list([_u])[0] if isinstance(_u, str) and _u else ""
                    if not _u_norm:
                        continue
                    if _r.get("status") == "injected_pending":
                        _fx16_targets.append((_u_norm, _r, "placeholder_row"))
                    elif _u_norm in _fx16_delta:
                        _fx16_targets.append((_u_norm, _r, "delta_row"))

            # Also cover the case where placeholders were not appended (defensive)
            for _u in (_fx16_delta or []):
                if not isinstance(baseline_sources_cache, list):
                    continue
                if any((_inj_diag_norm_url_list([(_r.get("source_url") or _r.get("url") or "")])[0] if isinstance(_r, dict) else "") == _u for _r in (baseline_sources_cache or [])):
                    continue
                baseline_sources_cache.append({
                    "source_url": _u,
                    "url": _u,
                    "status": "injected_pending",
                    "status_detail": "injected_url_placeholder_pre_hash",
                    "snapshot_text": "",
                    "extracted_numbers": [],
                    "numbers_found": 0,
                    "injected": True,
                    "injected_reason": "fx16_defensive_placeholder",
                })
                _fx16_targets.append((_u, baseline_sources_cache[-1], "defensive_placeholder"))

            # Fetch+extract for targets (best-effort)
            _fx16_fetched = []
            _fx16_failed = []
            if _fx16_targets:
                for (_u_norm, _row, _why) in _fx16_targets:
                    # Skip if row already has text/numbers (idempotent)
                    try:
                        if isinstance(_row.get("snapshot_text"), str) and _row.get("snapshot_text").strip():
                            continue
                        if isinstance(_row.get("extracted_numbers"), list) and len(_row.get("extracted_numbers") or []) > 0:
                            continue
                    except Exception:
                        pass

                    _txt = None
                    _detail = ""
                    try:
                        _txt, _detail = fetch_url_content_with_status(_u_norm, timeout=25)
                    except Exception as _e:
                        _txt, _detail = None, f"exception:{type(_e).__name__}"

                    if _txt and isinstance(_txt, str) and len(_txt.strip()) >= 200:
                        # FIX2D69A: ensure numeric extraction runs on snapshot_text, with HTML->text fallback and auditable debug
                        _extract_text = _txt
                        try:
                            _t0 = str(_txt or '')
                            _looks_html = ('<' in _t0 and '>' in _t0 and ('</' in _t0 or '<html' in _t0.lower() or '<body' in _t0.lower()))
                            if _looks_html:
                                try:
                                    from bs4 import BeautifulSoup  # type: ignore
                                    _extract_text = BeautifulSoup(_t0, 'html.parser').get_text(' ')
                                except Exception:
                                    _extract_text = re.sub(r'<[^>]+>', ' ', _t0)
                            if not isinstance(_extract_text, str):
                                _extract_text = str(_extract_text or '')
                            _extract_text = re.sub(r'\s+', ' ', _extract_text).strip()
                            if not _extract_text:
                                _extract_text = _t0
                        except Exception:
                            _extract_text = _txt

                        _nums = []
                        _fx69_errors = []
                        _callable = bool(callable(extract_numbers_with_context))
                        try:
                            _row['fix2d68_extract_attempted'] = bool(_callable)
                            _row['fix2d68_extract_input_len'] = int(len(_extract_text) if isinstance(_extract_text, str) else 0)
                            _row['fix2d68_extract_input_head'] = (_extract_text[:200] if isinstance(_extract_text, str) else '')
                        except Exception:
                            pass
                        if _callable:
                            for _mode in ('source_url', 'url', 'plain'):
                                try:
                                    _tmp = None
                                    if _mode == 'source_url':
                                        _tmp = extract_numbers_with_context(_extract_text, source_url=_u_norm)
                                    elif _mode == 'url':
                                        _tmp = extract_numbers_with_context(_extract_text, url=_u_norm)
                                    else:
                                        _tmp = extract_numbers_with_context(_extract_text)
                                    # normalize extractor return
                                    if _tmp is None:
                                        _nums = []
                                    elif isinstance(_tmp, list):
                                        _nums = _tmp
                                    elif isinstance(_tmp, tuple) and len(_tmp) >= 1 and isinstance(_tmp[0], list):
                                        _nums = _tmp[0]
                                    elif isinstance(_tmp, dict) and isinstance(_tmp.get('extracted_numbers'), list):
                                        _nums = _tmp.get('extracted_numbers') or []
                                    else:
                                        _nums = []
                                    _row['fix2d68_extract_call_mode'] = _mode
                                    break
                                except Exception as _e:
                                    _fx69_errors.append({'mode': _mode, 'error': repr(_e)})
                                    _nums = []
                        if _fx69_errors:
                            _row['fix2d68_extract_errors'] = _fx69_errors

                        _row.update({
                            'status': 'fetched',
                            'status_detail': (_detail or 'success'),
                            'snapshot_text': _txt[:7000],
                            'extracted_numbers': _nums,
                            'numbers_found': int(len(_nums or [])),
                            'injected': True,
                            'injected_reason': _row.get('injected_reason') or 'fx16_fetch_and_extract',
                        })
                        _fx16_fetched.append({
                            'url': _u_norm, 'why': _why, 'numbers_found': int(len(_nums or [])), 'status_detail': (_detail or 'success')
                        })
                    else:
                        _row.update({
                            'status': 'failed',
                            'status_detail': (_detail or 'failed:no_text'),
                            'snapshot_text': '',
                            'extracted_numbers': [],
                            'numbers_found': 0,
                            'injected': True,
                            'injected_reason': _row.get('injected_reason') or 'fx16_fetch_failed',
                        })
                        _fx16_failed.append({
                            'url': _u_norm, 'why': _why, 'status_detail': (_detail or 'failed:no_text')
                        })

            # Emit debug
            try:
                output.setdefault("debug", {})
                if isinstance(output.get("debug"), dict):
                    output["debug"].setdefault("fix41afc16", {})
                    if isinstance(output["debug"].get("fix41afc16"), dict):
                        output["debug"]["fix41afc16"].update({
                            "inj_norm_count": int(len(_fx16_inj_norm or [])),
                            "inj_delta_count": int(len(_fx16_delta or [])),
                            "fetch_target_count": int(len(_fx16_targets or [])),
                            "fetched_count": int(len(_fx16_fetched or [])),
                            "failed_count": int(len(_fx16_failed or [])),
                            "fetched": list(_fx16_fetched or []),
                            "failed": list(_fx16_failed or []),
                        })
            except Exception:
                pass
        except Exception:
            pass


        #
        # Observed gap (from evolution JSON after FIX41AFC16):
        #   - Injected URL reaches intake/admitted/attempted/hash_inputs, but snapshot_debug remains empty
        #     (origin none / raw_count 0), and downstream consumers appear to miss the fetched snapshot_text.
        #
        # Goal:
        #   - Ensure the same snapshot-carrier fields used by New Analysis are populated for Evolution,
        #     so that attach_source_snapshots_to_analysis() (and any downstream rebuild plumbing) can
        #     “see” the injected (and other) snapshots deterministically.
        #
        # Safety:
        #   - Purely additive wiring.
        #   - No effect if baseline_sources_cache is missing.
        #   - Does not alter hashing logic; only ensures snapshots are attached consistently.
        try:
            if isinstance(baseline_sources_cache, list) and baseline_sources_cache and isinstance(web_context, dict):
                # Provide canonical aliases for current pool (additive; downstream may read any of these)
                web_context.setdefault("current_baseline_sources_cache", baseline_sources_cache)
                web_context.setdefault("baseline_sources_cache_current", baseline_sources_cache)
                web_context.setdefault("current_source_results", baseline_sources_cache)

                # Mirror into output for downstream consumers/debug (additive)
                try:
                    output.setdefault("baseline_sources_cache_current", baseline_sources_cache)
                except Exception:
                    pass
                try:
                    output.setdefault("baseline_sources_cache", baseline_sources_cache)
                except Exception:
                    pass
                try:
                    output.setdefault("results", {})
                    if isinstance(output.get("results"), dict):
                        output["results"].setdefault("baseline_sources_cache_current", baseline_sources_cache)
                        output["results"].setdefault("baseline_sources_cache", baseline_sources_cache)
                except Exception:
                    pass

                # Call the same snapshot attach helper used by analysis if present (best-effort)
                try:
                    _att_fn = globals().get("attach_source_snapshots_to_analysis")
                    if callable(_att_fn):
                        _att_fn(output, web_context)
                except Exception:
                    pass

                # Small debug breadcrumb
                try:
                    output.setdefault("debug", {})
                    if isinstance(output.get("debug"), dict):
                        output["debug"].setdefault("fix41afc17", {})
                        if isinstance(output["debug"].get("fix41afc17"), dict):
                            output["debug"]["fix41afc17"].update({
                                "attached_pool_count": int(len(baseline_sources_cache)),
                                "attach_called": bool(callable(globals().get("attach_source_snapshots_to_analysis"))),
                            })
                except Exception:
                    pass
        except Exception:
            pass

        try:
            if not (isinstance(baseline_sources_cache, list) and baseline_sources_cache):
                _fix36_reason = "no_snapshots"
            elif not (isinstance(prev_metrics, dict) and prev_metrics):
                _fix36_reason = "no_prev_metrics"
            else:
                _fix36_cur_hash = _fix37_snapshot_hash_stable(baseline_sources_cache)
                if not (isinstance(_prev_hash, str) and _prev_hash):
                    _fix36_reason = "no_prev_hash"
                elif _fix36_cur_hash != _prev_hash:
                    _fix36_reason = "hash_mismatch"
                else:
                    _fix36_reason = "hash_match_and_prev_metrics_present"

            # Intent:
            #   If the Evolution UI supplies injected URLs that are NOT already part of the
            #   baseline source universe, bypass fastpath eligibility even when hashes match.
            #   This does NOT weaken fastpath for the locked/no-injection case; it only prevents
            #   "observed but inert" injections from being ignored when they materially change
            #   the intended source universe.
            #
            #   Key rule:
            #     - If injected_delta (normalized_injected_urls - normalized_baseline_urls) is non-empty
            #       and fastpath would otherwise be eligible, force _fix36_reason to a bypass reason so
            #       fastpath_eligible becomes False and rebuild path can run.
            try:
                _evo_wc = web_context if isinstance(web_context, dict) else {}
                _evo_diag = _evo_wc.get("diag_injected_urls") if isinstance(_evo_wc.get("diag_injected_urls"), dict) else {}
                _evo_extra_urls = []
                # Prefer already-normalized intake/ui lists if present
                for _k in ("intake", "ui_norm", "ui", "extra_urls"):
                    _v = _evo_diag.get(_k)
                    if isinstance(_v, (list, tuple)) and _v:
                        _evo_extra_urls = list(_v)
                        break
                # Fall back to raw web_context extras if diag not populated
                if not _evo_extra_urls:
                    _v2 = _evo_wc.get("extra_urls")
                    if isinstance(_v2, (list, tuple)) and _v2:
                        _evo_extra_urls = list(_v2)


                #   Robustly recover injected/extra URLs for bypass detection even when
                #   diag_injected_urls is not populated yet (common on replay/fastpath).
                #   Sources (in order):
                #     - web_context["extra_urls"] if list
                #     - web_context["diag_extra_urls_ui"] if list
                #     - web_context["diag_extra_urls_ui_raw"] if str (newline/space separated)
                #   This is diagnostic-only: we ONLY use this to decide whether to bypass
                #   fastpath when hashes otherwise match, so injected URLs can be admitted
                #   via the rebuild path and become first-class inputs.
                try:
                    if not _evo_extra_urls:
                        _v3 = _evo_wc.get("diag_extra_urls_ui")
                        if isinstance(_v3, (list, tuple)) and _v3:
                            _evo_extra_urls = list(_v3)
                    if not _evo_extra_urls:
                        _raw = _evo_wc.get("diag_extra_urls_ui_raw")
                        if isinstance(_raw, str) and _raw.strip():
                            # Split on newlines first; also allow commas/spaces as separators
                            _parts = []
                            for _line in _raw.splitlines():
                                _line = (_line or "").strip()
                                if not _line:
                                    continue
                                # allow comma-separated within a line
                                for _p in _line.split(","):
                                    _p = (_p or "").strip()
                                    if _p:
                                        _parts.append(_p)

                            # REFACTOR76: surface schema-coverage / change_type integrity issues (counts only)
                            try:
                                if _schema_keys and isinstance(_inv, dict):
                                    _mrk = _inv.get("metric_changes_schema_missing_keys") or []
                                    _drk = _inv.get("metric_changes_schema_duplicate_keys") or []
                                    _ctm = _inv.get("metric_changes_change_type_mismatches") or []
                                    if isinstance(_mrk, list) and _mrk:
                                        _parts.append(f"row_missing_keys={len(_mrk)}")
                                    if isinstance(_drk, list) and _drk:
                                        _parts.append(f"row_duplicate_keys={len(_drk)}")
                                    if isinstance(_ctm, list) and _ctm:
                                        _parts.append(f"row_change_type_mismatch={len(_ctm)}")
                            except Exception:
                                pass

                            if _parts:
                                _evo_extra_urls = _parts
                except Exception:
                    pass

                #   If we recovered injected URLs from Streamlit diagnostic fields (e.g.,
                #   diag_extra_urls_ui_raw) and web_context["extra_urls"] is empty, wire the
                #   recovered list into web_context["extra_urls"] so downstream admission/
                #   fetch/persist logic can see the same universe deterministically.
                #   No effect on no-injection runs.
                try:
                    if isinstance(_evo_wc, dict):
                        _wc_extra = _evo_wc.get("extra_urls")
                        if (not isinstance(_wc_extra, (list, tuple)) or not _wc_extra) and isinstance(_evo_extra_urls, list) and _evo_extra_urls:
                            _evo_wc["extra_urls"] = list(_evo_extra_urls)
                            try:
                                _evo_wc["__yureeka_extra_urls_are_injection_v1"] = True
                                _evo_wc["__yureeka_injected_urls_v1"] = list(_evo_extra_urls)
                            except Exception:
                                pass

                except Exception:
                    pass

                _evo_inj_set = set(_inj_diag_norm_url_list(_evo_extra_urls)) if _evo_extra_urls else set()

                # Baseline universe = urls present in baseline_sources_cache (the same object used for hashing)
                _evo_base_urls = []
                if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                    for _row in baseline_sources_cache:
                        if isinstance(_row, dict) and isinstance(_row.get("source_url"), str) and _row.get("source_url"):
                            _evo_base_urls.append(_row.get("source_url"))
                _evo_base_set = set(_inj_diag_norm_url_list(_evo_base_urls)) if _evo_base_urls else set()

                _evo_inj_delta = sorted(list(_evo_inj_set - _evo_base_set)) if _evo_inj_set else []
                #   We persist a simple boolean flag in locals so the downstream FIX31
                #   authoritative reuse check can be disabled without refactoring.
                _fix41af_inj_delta_present = bool(_evo_inj_delta)


                # Only bypass when hashes match and we would otherwise take fastpath
                if _evo_inj_delta and _fix36_reason == "hash_match_and_prev_metrics_present":
                    _fix36_reason = "hash_match_but_injected_urls_present_bypass_fastpath"
                    try:
                        if isinstance(output.get("debug"), dict) and isinstance(output.get("debug", {}).get("fix35"), dict):
                            output["debug"]["fix35"]["fastpath_bypass_injected_delta"] = _evo_inj_delta
                            output["debug"]["fix35"]["fastpath_bypass_injected_delta_count"] = len(_evo_inj_delta)
                    except Exception:
                        pass
            except Exception:
                pass
                # Never break evolution on diagnostics / bypass checks
                pass
            if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                # Preserve any earlier reason, but fill if empty
                if not output["debug"]["fix35"].get("fastpath_reason"):
                    output["debug"]["fix35"]["fastpath_reason"] = _fix36_reason
                output["debug"]["fix35"]["fastpath_eligible"] = bool(_fix36_reason == "hash_match_and_prev_metrics_present")
                if _fix36_cur_hash:
                    output["debug"]["fix35"]["source_snapshot_hash_current"] = _fix36_cur_hash
                    output["debug"]["fix35"]["source_snapshot_hash_current_alg"] = "fix37_stable_v2_preferred"
                if isinstance(_prev_hash, str) and _prev_hash:
                    output["debug"]["fix35"]["source_snapshot_hash_previous"] = (_prev_hash_pref if isinstance(locals().get("_prev_hash_pref"), str) and locals().get("_prev_hash_pref") else _prev_hash)
                try:
                    if isinstance(_prev_hash_stable, str) and _prev_hash_stable:
                        output["debug"]["fix35"]["source_snapshot_hash_previous_stable"] = _prev_hash_stable
                except Exception:
                    pass
        except Exception:
            pass


        # REFACTOR115: schema-seeded production sources (minimal, deterministic; skipped when injection active)
        try:
            _rf115_inj_urls = _refactor115_collect_injection_urls_v1({}, web_context or {})
            _rf115_injection_present = bool(_rf115_inj_urls)
            _rf115_added = []
            _rf115_extract_diag = []  # REFACTOR119
            _rf115_year_tokens_union = ["2025", "2026", "2040"]  # REFACTOR119
            if isinstance(baseline_sources_cache, list):
                # normalize existing URLs
                _existing = set()
                for _r in (baseline_sources_cache or []):
                    if isinstance(_r, dict):
                        _u = _r.get("url") or _r.get("source_url") or ""
                        _n = None
                        try:
                            _n = _inj_diag_norm_url_list([_u])[0] if _u else ""
                        except Exception:
                            _n = str(_u or "").strip()
                        if _n:
                            _existing.add(_n)
                if (not _rf115_injection_present):
                    for _u in (_REFACTOR115_SCHEMA_SEED_URLS_V1 or []):
                        try:
                            _u_norm = _inj_diag_norm_url_list([_u])[0]
                        except Exception:
                            _u_norm = str(_u or "").strip()
                        if not _u_norm or _u_norm in _existing:
                            continue
                        _txt, _detail = None, ""
                        try:
                            _txt, _detail = fetch_url_content_with_status(_u_norm, timeout=25)
                        except Exception as _e:
                            _txt, _detail = None, f"exception:{type(_e).__name__}"
                        # Normalize status for skipped PDF dependency
                        _status = "fetched" if str(_detail).startswith("success") else "failed"
                        _sd = str(_detail or "")
                        if str(_sd).startswith("skipped:"):
                            _status = "skipped"
                            try:
                                _sd = _sd[len("skipped:"):] or "pdf_unsupported_missing_dependency"
                            except Exception:
                                _sd = "pdf_unsupported_missing_dependency"
                        if isinstance(_txt, str) and len(_txt.strip()) >= 200:
                            # REFACTOR119: actually extract numeric candidates from schema-seeded snapshot_text
                            _rf115_extracted_numbers = []
                            _rf115_numbers_found = 0
                            _rf115_fp = ""
                            _rf115_year_hits = {}
                            _rf115_extract_error = ""
                            try:
                                _rf115_extracted_numbers = extract_numbers_with_context(_txt, source_url=_u_norm, max_results=600) or []
                            except Exception as _ee:
                                _rf115_extracted_numbers = []
                                try:
                                    _rf115_extract_error = f"{type(_ee).__name__}:{_ee}"
                                except Exception:
                                    _rf115_extract_error = "exception"
                            try:
                                _rf115_numbers_found = sum(1 for _n in (_rf115_extracted_numbers or []) if isinstance(_n, dict) and (not bool(_n.get("is_junk"))))
                            except Exception:
                                try:
                                    _rf115_numbers_found = int(len(_rf115_extracted_numbers or [])) if isinstance(_rf115_extracted_numbers, list) else 0
                                except Exception:
                                    _rf115_numbers_found = 0
                            try:
                                _rf115_fp = fingerprint_text(_txt)
                            except Exception:
                                _rf115_fp = ""
                            try:
                                for _yy in (_rf115_year_tokens_union or []):
                                    if isinstance(_yy, str) and _yy:
                                        _rf115_year_hits[_yy] = int(str(_txt).count(_yy))
                            except Exception:
                                _rf115_year_hits = {}
                            try:
                                _rf115_extract_diag.append({
                                    "url": _u_norm,
                                    "numbers_found": int(_rf115_numbers_found or 0),
                                    "status": _status,
                                    "status_detail": _sd,
                                    "year_token_hits": _rf115_year_hits,
                                    "extract_error": _rf115_extract_error,
                                })
                            except Exception:
                                pass

                            baseline_sources_cache.append({
                                "source_url": _u_norm,
                                "url": _u_norm,
                                "status": _status,
                                "status_detail": _sd,
                                "snapshot_text": _txt,
                                "snapshot_text_excerpt": (_txt[:12000] if isinstance(_txt, str) else ""),
                                "fingerprint": _rf115_fp,
                                "extracted_numbers": _rf115_extracted_numbers,
                                "numbers_found": _rf115_numbers_found,
                                "seeded": True,
                                "seeded_reason": "schema_seeds",
                                "fetched_at": _now(),
                            })
                            _existing.add(_u_norm)
                            _rf115_added.append(_u_norm)
                        else:
                            # Still append a placeholder so hash changes deterministically and future rebuild can fetch.
                            try:
                                _rf115_extract_diag.append({
                                    "url": _u_norm,
                                    "numbers_found": 0,
                                    "status": ("seeded_pending" if (not _status == "skipped") else "skipped"),
                                    "status_detail": _sd or "seeded_placeholder",
                                    "year_token_hits": {},
                                    "extract_error": "missing_text",
                                })
                            except Exception:
                                pass
                            baseline_sources_cache.append({
                                "source_url": _u_norm,
                                "url": _u_norm,
                                "status": "seeded_pending" if (not _status == "skipped") else "skipped",
                                "status_detail": _sd or "seeded_placeholder",
                                "snapshot_text": "",
                                "fingerprint": "",
                                "extracted_numbers": [],
                                "numbers_found": 0,
                                "seeded": True,
                                "seeded_reason": "schema_seeds_placeholder",
                                "fetched_at": _now(),
                            })
                            _existing.add(_u_norm)
                            _rf115_added.append(_u_norm)
            # Emit debug beacon (additive)
            try:
                output.setdefault("debug", {})
                if isinstance(output.get("debug"), dict):
                    output["debug"]["schema_seed_sources_v1"] = {
                        "seeds_added": int(len(_rf115_added)),
                        "added_urls": list(_rf115_added)[:10],
                        "final_sources_urls": [ (r.get("url") or r.get("source_url")) for r in (baseline_sources_cache or []) if isinstance(r, dict) and (r.get("url") or r.get("source_url")) ][:50],
                        "reason": "schema_seeds",
                        "injection_present": bool(_rf115_injection_present),
                    }

                    # REFACTOR119: seed extraction beacon (did seeds produce numeric candidates?)
                    try:
                        _nonzero = 0
                        _errs = 0
                        _sample = []
                        _year_hits = {}
                        if isinstance(_rf115_extract_diag, list):
                            try:
                                _nonzero = sum(1 for _d in _rf115_extract_diag if isinstance(_d, dict) and int(_d.get("numbers_found") or 0) > 0)
                            except Exception:
                                _nonzero = 0
                            try:
                                _errs = sum(1 for _d in _rf115_extract_diag if isinstance(_d, dict) and str(_d.get("extract_error") or "").strip())
                            except Exception:
                                _errs = 0
                            for _d in (_rf115_extract_diag or [])[:12]:
                                if not isinstance(_d, dict):
                                    continue
                                _sample.append({
                                    "url": _d.get("url"),
                                    "numbers_found": _d.get("numbers_found"),
                                    "status": _d.get("status"),
                                })
                                _yh = _d.get("year_token_hits")
                                if isinstance(_yh, dict) and _yh and isinstance(_d.get("url"), str):
                                    _year_hits[str(_d.get("url"))] = dict(_yh)
                        output["debug"]["schema_seed_extract_v1"] = {
                            "seeds_added": int(len(_rf115_added)),
                            "seeds_extracted_nonzero": int(_nonzero or 0),
                            "per_seed_numbers_found_sample": list(_sample or []),
                            "per_seed_year_token_hits": dict(_year_hits or {}),
                            "extract_errors": int(_errs or 0),
                        }
                    except Exception:
                        pass
            except Exception:
                pass
        except Exception:
            pass

        # REFACTOR121: snapshot excerpt coverage beacon (year-anchor backstop)
        try:
            _tot = int(len(baseline_sources_cache) if isinstance(baseline_sources_cache, list) else 0)
            _with = 0
            _seeded_with = 0
            _lens = []
            if isinstance(baseline_sources_cache, list):
                for _s in (baseline_sources_cache or []):
                    if not isinstance(_s, dict):
                        continue
                    _ex = _s.get("snapshot_text_excerpt")
                    if isinstance(_ex, str) and _ex.strip():
                        _with += 1
                        _lens.append(len(_ex))
                        if bool(_s.get("seeded")):
                            _seeded_with += 1
            output.setdefault("debug", {})["snapshot_excerpt_coverage_v1"] = {
                "sources_total": int(_tot),
                "sources_with_excerpt": int(_with),
                "seeded_sources_with_excerpt": int(_seeded_with),
                "excerpt_len_avg": (float(sum(_lens) / max(1, len(_lens))) if _lens else 0.0),
                "excerpt_len_max": (int(max(_lens)) if _lens else 0),
            }
            output["debug"]["year_anchor_page_text_v1"] = {
                "indexed_text_sources": int(_with),
                "sources_total": int(_tot),
                "index_key_preference": "snapshot_text_excerpt>snapshot_text>clean_text>text",
            }
        except Exception:
            pass

        # REFACTOR115: FIX31 fast-path gating to avoid masking progress during validation/injection/null-schema scenarios.
        # Only attempt fast-path if we have snapshots AND prior canonical metrics to reuse
        if isinstance(baseline_sources_cache, list) and baseline_sources_cache and isinstance(prev_metrics, dict) and prev_metrics:
            # - Previously FIX31 compared a v1 fingerprint against prev source_snapshot_hash,
            #   which could mismatch even when data was unchanged.
            # - We now prefer the same stable/v2 hash used by analysis & FIX37 debug.
            _cur_hash_v1 = _fix31_snapshot_fingerprint(baseline_sources_cache)
            try:
                _cur_hash = _fix37_snapshot_hash_stable(baseline_sources_cache)
            except Exception:
                pass
                _cur_hash = _cur_hash_v1

            # Prefer stable/v2 previous hash if present
            _prev_hash_pref = previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2") or _prev_hash

            #
            # Goal:
            #   When hashes are unequal, rebuild should run on the SAME snapshot pool
            #   that "new analysis" just produced, not on the stale baseline cache
            #   embedded in the previous analysis payload.
            #
            # Where it comes from:
            #   - web_context["current_baseline_sources_cache"]  (preferred)
            #   - web_context["baseline_sources_cache_current"]
            #   - web_context["current_source_results"]         (fallback alias)
            #
            # Policy (additive, fastpath-safe):
            #   - If force_rebuild is asserted by UI/web_context, always use current pool.
            #   - Else, only switch to current pool if its stable hash != previous hash.
            #   - If hashes match, we keep existing behavior (but either pool is equivalent).
            _fix42_used_current_pool = False
            _fix42_reason = ""
            _fix42_cur_pool_hash = None
            try:
                if isinstance(web_context, dict):
                    _cur_pool = (
                        web_context.get("current_baseline_sources_cache")
                        or web_context.get("baseline_sources_cache_current")
                        or web_context.get("current_source_results")
                        or web_context.get("current_source_results_cache")
                        or None
                    )
                    if isinstance(_cur_pool, list) and _cur_pool:
                        _force = bool(
                            web_context.get("force_rebuild")
                            or web_context.get("forced_rebuild")
                            or web_context.get("force_full_rebuild")
                            or web_context.get("force_metric_rebuild")
                        )
                        try:
                            _fix42_cur_pool_hash = _fix37_snapshot_hash_stable(_cur_pool)
                        except Exception:
                            pass
                            _fix42_cur_pool_hash = None

                        _prev_hash_for_compare = _prev_hash_pref if (isinstance(_prev_hash_pref, str) and _prev_hash_pref) else _prev_hash
                        _hash_mismatch = bool(
                            (isinstance(_prev_hash_for_compare, str) and _prev_hash_for_compare and isinstance(_fix42_cur_pool_hash, str) and _fix42_cur_pool_hash)
                            and (_fix42_cur_pool_hash != _prev_hash_for_compare)
                        )

                        if _force or _hash_mismatch:
                            baseline_sources_cache = _cur_pool
                            snapshot_origin = (snapshot_origin or "analysis_cache") + "|fix42_current_pool"
                            _fix42_used_current_pool = True
                            _fix42_reason = "forced_rebuild" if _force else "hash_mismatch_use_current_pool"
            except Exception:
                pass

            # Attach FIX42 diagnostics (non-breaking)
            try:
                if _fix42_used_current_pool:
                    output["snapshot_origin"] = snapshot_origin
                if isinstance(output.get("debug"), dict):
                    output["debug"].setdefault("fix42", {})
                    output["debug"]["fix42"]["used_current_pool"] = bool(_fix42_used_current_pool)
                    output["debug"]["fix42"]["reason"] = _fix42_reason
                    if isinstance(_fix42_cur_pool_hash, str) and _fix42_cur_pool_hash:
                        output["debug"]["fix42"]["current_pool_hash_stable"] = _fix42_cur_pool_hash
            except Exception:
                pass


            #   If an injected URL delta exists, we MUST NOT take FIX31 authoritative
            #   reuse (fastpath replay), even if hashes match. We do this additively by
            #   temporarily blanking _prev_hash_pref so the existing hash-match check
            #   remains unchanged for normal runs.
            _fix41af_prev_hash_pref_saved = None
            try:
                if bool(locals().get("_fix41af_inj_delta_present")):
                    _fix41af_prev_hash_pref_saved = _prev_hash_pref
                    _prev_hash_pref = ""
                    try:
                        if isinstance(output.get("debug"), dict) and isinstance(output.get("debug", {}).get("fix35"), dict):
                            output["debug"]["fix35"]["fastpath_reason"] = "hash_match_but_injected_urls_present_bypass_fastpath"
                            output["debug"]["fix35"]["fastpath_eligible"] = False
                    except Exception:
                        pass
            except Exception:
                pass

            # REFACTOR99 safety: do NOT take FIX31 authoritative reuse when previous canonical metrics are too sparse.
            # This prevents being "stuck" with a thin prior run (e.g., 1/4 keys) even though sources are stable.
            try:
                _fix99_prev_pmc = _refactor89_locate_pmc_dict(prev_response) if isinstance(prev_response, dict) else {}
                _fix99_schema = (
                    (prev_response or {}).get("metric_schema_frozen")
                    or ((prev_response or {}).get("primary_response") or {}).get("metric_schema_frozen")
                    or ((prev_response or {}).get("results") or {}).get("metric_schema_frozen")
                    or {}
                )
                _fix99_prev_pmc_count = int(len(_fix99_prev_pmc) if isinstance(_fix99_prev_pmc, dict) else 0)
                _fix99_schema_count = int(len(_fix99_schema) if isinstance(_fix99_schema, dict) else 0)
                _fix99_min_needed = 0
                try:
                    if _fix99_schema_count >= 2:
                        _fix99_min_needed = max(2, int((_fix99_schema_count + 1) // 2))
                except Exception:
                    _fix99_min_needed = 2 if _fix99_schema_count >= 2 else 0
                if _fix99_schema_count and _fix99_min_needed and (_fix99_prev_pmc_count < _fix99_min_needed):
                    _prev_hash_pref = ""  # bypass hash-match fastpath
                    try:
                        if isinstance(output.get("debug"), dict) and isinstance(output.get("debug", {}).get("fix35"), dict):
                            output["debug"]["fix35"]["fastpath_reason"] = f"hash_match_but_prev_pmc_sparse_bypass_fastpath:{_fix99_prev_pmc_count}/{_fix99_schema_count}"
                            output["debug"]["fix35"]["fastpath_eligible"] = False
                            output["debug"]["fix35"]["prev_pmc_count"] = _fix99_prev_pmc_count
                            output["debug"]["fix35"]["schema_key_count"] = _fix99_schema_count
                    except Exception:
                        pass
            except Exception:
                pass

            _fix31_prev_code_version = ""
            try:
                _fix31_prev_code_version = str((previous_data or {}).get("code_version") or "")
            except Exception:
                _fix31_prev_code_version = ""

            # REFACTOR115: compute fast-path eligibility gate
            _rf115_injection_present = False
            _rf115_triad_mode = False
            _rf115_schema_all_null = False
            _rf115_disabled_reason = ""
            try:
                _rf115_injection_present = bool(_refactor115_collect_injection_urls_v1({}, web_context or {}))
            except Exception:
                _rf115_injection_present = False
            try:
                _rf115_triad_mode = bool((web_context or {}).get("triad_validation_mode"))
            except Exception:
                _rf115_triad_mode = False
            try:
                _rf115_schema_all_null = bool(_refactor115_all_schema_values_null_in_payload_v1(previous_data))
            except Exception:
                _rf115_schema_all_null = False
            if _rf115_injection_present:
                _rf115_disabled_reason = "injection_present"
            elif _rf115_triad_mode:
                _rf115_disabled_reason = "triad_validation_mode"
            elif _rf115_schema_all_null:
                _rf115_disabled_reason = "all_schema_values_null"
            # Emit beacon
            # REFACTOR117: honor global DISABLE_FASTPATH_FOR_NOW as a hard gate during refactor triad validation.
            try:
                if (not _rf115_disabled_reason) and bool(globals().get("DISABLE_FASTPATH_FOR_NOW")):
                    _rf115_disabled_reason = "disable_fastpath_for_now"
            except Exception:
                pass

            try:
                output.setdefault("debug", {})
                if isinstance(output.get("debug"), dict):
                    output["debug"]["fastpath_gate_v2"] = {
                        "eligible": bool(_rf115_disabled_reason == ""),
                        "disabled_reason": _rf115_disabled_reason,
                        "cur_hash": _cur_hash if isinstance(locals().get("_cur_hash"), str) else None,
                        "prev_hash": _prev_hash_pref if isinstance(locals().get("_prev_hash_pref"), str) else None,
                    }
            except Exception:
                pass

            if (not _rf115_disabled_reason) and isinstance(_prev_hash_pref, str) and _prev_hash_pref and _cur_hash == _prev_hash_pref and _fix31_prev_code_version == str(globals().get("CODE_VERSION") or ""):
                _fix31_authoritative_reuse = True
                try:
                    if _fix41af_prev_hash_pref_saved is not None:
                        _prev_hash_pref = _fix41af_prev_hash_pref_saved
                except Exception:
                    pass

                try:
                    output["rebuild_skipped"] = True
                    output["rebuild_skipped_reason"] = "fix31_sources_unchanged_reuse_prev_metrics"
                    output["source_snapshot_hash_current"] = _cur_hash
                    output["source_snapshot_hash_previous"] = (_prev_hash_cmp if " _prev_hash_cmp" in locals() else _prev_hash)
                    try:
                        if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                            output["debug"]["fix35"]["fastpath_eligible"] = True
                            output["debug"]["fix35"]["fastpath_reason"] = "hash_match_and_prev_metrics_present"
                            output["debug"]["fix35"]["source_snapshot_hash_current"] = _cur_hash
                            output["debug"]["fix35"]["source_snapshot_hash_previous"] = (_prev_hash_pref if isinstance(locals().get("_prev_hash_pref"), str) and locals().get("_prev_hash_pref") else _prev_hash)
                            output["debug"]["fix35"]["current_metrics_origin"] = "reuse_processed_metrics_fastpath"
                    except Exception:
                        pass
                except Exception:
                    pass
    except Exception:
        pass
        _fix31_authoritative_reuse = False
        try:
            if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                if not output["debug"]["fix35"].get("fastpath_reason"):
                    output["debug"]["fix35"]["fastpath_reason"] = "fastpath_not_taken_or_exception"
        except Exception:
            pass

    # Build a minimal current metrics dict from snapshots:
    current_metrics = {}
    try:

        #   (covers the case where hash-match condition was false and the inline restore
        #   inside the if-body did not execute).
        try:
            if locals().get("_fix41af_prev_hash_pref_saved") is not None and not _prev_hash_pref:
                _prev_hash_pref = locals().get("_fix41af_prev_hash_pref_saved")
        except Exception:
            pass

        if _fix31_authoritative_reuse and isinstance(prev_metrics, dict) and prev_metrics:
            current_metrics = dict(prev_metrics)
            try:
                output["snapshot_origin"] = (output.get("snapshot_origin") or "") + "|fix31_reuse_prev_metrics"
            except Exception:
                pass
    except Exception:
        pass


    except Exception:
        _fix31_authoritative_reuse = False
        try:
            if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                if not output["debug"]["fix35"].get("fastpath_reason"):
                    output["debug"]["fix35"]["fastpath_reason"] = "fastpath_not_taken_or_exception"
        except Exception:
            pass

    # Prefer metric_anchors to rebuild current_metrics (snapshot-gated)
    def _get_metric_anchors(prev: dict) -> dict:
        # so the reused, schema-gated metrics remain untouched.
        try:
            if _fix31_authoritative_reuse:
                return {}
        except Exception:
            pass

        if not isinstance(prev, dict):
            return {}
        a = prev.get("metric_anchors")
        if isinstance(a, dict) and a:
            return a
        pr = prev.get("primary_response")
        if isinstance(pr, dict):
            a2 = pr.get("metric_anchors")
            if isinstance(a2, dict) and a2:
                return a2
        res = prev.get("results")
        if isinstance(res, dict):
            a3 = res.get("metric_anchors")
            if isinstance(a3, dict) and a3:
                return a3
        return {}

    def _canonicalize_candidate(n: dict) -> dict:
        try:
            fn = globals().get("canonicalize_numeric_candidate")
            if callable(fn):
                return fn(dict(n))
        except Exception:
            return dict(n)

    def _build_anchor_to_candidate_map(snapshots: list) -> dict:
        m = {}
        for sr in snapshots or []:
            if not isinstance(sr, dict):
                continue
            for n in (sr.get("extracted_numbers") or []):
                if not isinstance(n, dict):
                    continue
                nn = _canonicalize_candidate(n)
                ah = nn.get("anchor_hash")
                if not ah:
                    continue
                if ah not in m:
                    m[ah] = nn
        return m

    try:
        metric_anchors = _get_metric_anchors(previous_data)
        anchor_to_candidate = _build_anchor_to_candidate_map(baseline_sources_cache)

        if isinstance(metric_anchors, dict) and metric_anchors:
            for ckey, a in metric_anchors.items():
                if not isinstance(a, dict):
                    continue
                ah = a.get("anchor_hash") or a.get("anchor")
                if not ah:
                    continue
                cand = anchor_to_candidate.get(ah)
                if not isinstance(cand, dict):
                    continue

                base = prev_metrics.get(ckey) if isinstance(prev_metrics, dict) else None
                out_row = dict(base) if isinstance(base, dict) else {}
                out_row.update({
                    "canonical_key": ckey,
                    "anchor_hash": ah,
                    "anchor_used": True,
                    "anchor_confidence": a.get("anchor_confidence"),
                    "source_url": cand.get("source_url") or a.get("source_url"),
                    "raw": cand.get("raw"),
                    "value": cand.get("value"),
                    "unit": cand.get("unit"),
                    "value_norm": cand.get("value_norm"),
                    "context_snippet": cand.get("context_snippet") or cand.get("context") or "",
                    "candidate_id": cand.get("candidate_id") or a.get("candidate_id"),
                })
                current_metrics[ckey] = out_row
    except Exception:
        pass

    # Rebuild fallback only if anchors didn't produce metrics
    if not isinstance(current_metrics, dict) or not current_metrics:
        try:
            fn_rebuild = globals().get("rebuild_metrics_from_snapshots_schema_only") or globals().get("rebuild_metrics_from_snapshots")
            if callable(fn_rebuild):
                current_metrics = fn_rebuild(prev_response, baseline_sources_cache, web_context=web_context)
        except Exception:
            pass
            current_metrics = {}
    if not isinstance(current_metrics, dict) or not current_metrics:
        # FIX2D65A: do not fail hard here; emit a warning and continue so Evolution can still output JSON.
        try:
            output.setdefault("warnings", [])
            output["warnings"].append({
                "code": "FIX2D65_REBUILD_EMPTY_WITH_SNAPSHOTS",
                "message": "Valid snapshots exist, but metric rebuild returned empty. Continuing (no refetch, no heuristic).",
                "sources_checked": int(len(baseline_sources_cache or [])),
                "sources_fetched": int(len(baseline_sources_cache or [])),
            })
        except Exception:
            pass
        output["status"] = output.get("status") or "ok_with_warnings"
        output["message"] = output.get("message") or "Valid snapshots exist, but metric rebuild returned empty. Continuing (no refetch, no heuristic)."
        # Keep current_metrics as empty dict; downstream code should handle it.
        current_metrics = {}
        _fix2d20_trace_year_like_commits(output, stage=str((output or {}).get('debug',{}).get('stage') or 'evolution'), callsite='compute_source_anchored_diff_return')
    #
    # Why:
    # - Latest evo JSON shows current metrics can be selected from non-matching units
    #   (e.g., unit_sales metric receiving a unitless/negative number; percent metric
    #   receiving a magnitude unit like 'B'). This leads the dashboard "Current" column
    #   to display the wrong metric values even though injection plumbing is progressing.
    # - The new analysis pipeline already relies on FIX16-style hard eligibility gates +
    #   anchor_hash deterministic rebuild. Evolution must use the same selection rules
    #   when fastpath is NOT taken (hash mismatch or injection-triggered rebuild).
    #
    # What:
    # - Right before diffing, attempt an anchor-first rebuild using:
    #     rebuild_metrics_from_snapshots_schema_only_fix16(prev_response, pool, web_context)
    #   when available.
    # - If it returns a non-empty dict, it *overrides* the previously computed
    #   current_metrics (additive override only when rebuild succeeded).
    # - Emits explicit debug fields for traceability.
    #
    # Non-negotiables:
    # - Does NOT alter fastpath logic.
    # - Only activates when fastpath is not taken (i.e., not authoritative reuse).
    try:
        _fix41afc19_applied = False
        _fix41afc19_reason = ""
        _fix41afc19_fn_name = ""
        _fix41afc19_rebuilt_count = 0

        # Only consider override when fastpath is not active
        if not bool(locals().get("_fix31_authoritative_reuse")):
            # Attempt to locate the "current" snapshot pool (post-injection merge/attach)
            _fix41afc19_pool = (
                locals().get("baseline_sources_cache_current")
                or (output.get("baseline_sources_cache_current") if isinstance(output, dict) else None)
                or (output.get("results", {}).get("baseline_sources_cache_current") if isinstance(output, dict) else None)
                or locals().get("baseline_sources_cache")
                or locals().get("baseline_sources_cache_prefetched")
                or None
            )

            # - Some pipelines store the post-attach merged pool under different locals()
            #   names (or only inside nested objects). If the pool is missed, FIX41AFC19
            #   appears "not applied" even on rebuild runs.
            # - We search locals() for any list-like baseline_sources_cache* variants and
            #   choose the largest plausible pool as a safe fallback.
            if _fix41afc19_pool is None:
                try:
                    _cand_pools = []
                    for _k, _v in (locals() or {}).items():
                        if not isinstance(_k, str):
                            continue
                        if "baseline_sources_cache" in _k and isinstance(_v, list) and _v:
                            _cand_pools.append((_k, _v))
                    # Choose the largest pool (most likely post-attach merged universe)
                    if _cand_pools:
                        _cand_pools.sort(key=lambda kv: len(kv[1] or []), reverse=True)
                        _fix41afc19_pool = _cand_pools[0][1]
                        _fix41afc19_reason = (_fix41afc19_reason or "") + "|ph2b_s2_pool_fallback:" + str(_cand_pools[0][0])
                except Exception:
                    pass

            # Prefer Analysis-canonical rebuild (Phase 2B hard-wire) when present; else fall back to FIX16 schema-only rebuild
            _fix41afc19_fn = globals().get("rebuild_metrics_from_snapshots_analysis_canonical_v1")
            if callable(_fix41afc19_fn):
                _fix41afc19_fn_name = "rebuild_metrics_from_snapshots_analysis_canonical_v1"
            else:
                _fix41afc19_fn = globals().get("rebuild_metrics_from_snapshots_schema_only_fix16")
                if callable(_fix41afc19_fn):
                    _fix41afc19_fn_name = "rebuild_metrics_from_snapshots_schema_only_fix16"
                else:
                    _fix41afc19_fn = None

            if callable(_fix41afc19_fn) and _fix41afc19_pool is not None:
                try:
                    _fix41afc19_rebuilt = _fix41afc19_fn(prev_response, _fix41afc19_pool, web_context=web_context)
                except TypeError:
                    # Backward-compat: older signature without web_context
                    _fix41afc19_rebuilt = _fix41afc19_fn(prev_response, _fix41afc19_pool)

                if isinstance(_fix41afc19_rebuilt, dict) and _fix41afc19_rebuilt:
                    current_metrics = dict(_fix41afc19_rebuilt)
                    _fix41afc19_applied = True
                    _fix41afc19_rebuilt_count = len(current_metrics)
                    _fix41afc19_reason = "override_current_metrics_with_fix16_anchor_rebuild"
                else:
                    _fix41afc19_reason = (_fix41afc19_reason or "") + "|rebuilt_empty_or_non_dict"
    except Exception:
        pass

    # Emit debug for FIX41AFC19 (non-breaking)
    try:
        if isinstance(output.get("debug"), dict):
            output["debug"].setdefault("fix41afc19", {})
            output["debug"]["fix41afc19"]["applied"] = bool(locals().get("_fix41afc19_applied"))
            output["debug"]["fix41afc19"]["reason"] = locals().get("_fix41afc19_reason") or ""
            output["debug"]["fix41afc19"]["fn"] = locals().get("_fix41afc19_fn_name") or ""
            output["debug"]["fix41afc19"]["rebuilt_count"] = int(locals().get("_fix41afc19_rebuilt_count") or 0)
    except Exception:
        pass

    # Objective:
    # - Ensure Evolution diff "current" side is built from the same canonical semantics as Analysis
    #   by forcing a best-effort canonical rebuild for DISPLAY/DIFF, even when earlier logic
    #   skipped due to authoritative reuse or pool-resolution drift.
    # - Adds two minimal diagnostics:
    #   (2) debug.fix41afc19 truth table (attempted/applied/skip_reason + keys_sample + pool_count)
    #   (3) debug.evo_winner_trace_v1 for key EV metrics (winner provenance + top3 candidate glimpse)
    # Safety:
    # - Additive-only. Does not modify hashing inputs or snapshot attach. Affects only what diff renders.
    _fix41afc19_attempted_v19 = False
    _fix41afc19_skip_reason_v19 = ""
    _fix41afc19_pool_count_v19 = 0
    _fix41afc19_keys_sample_v19 = []
    _fix41afc19_winner_trace_v19 = {}

    try:
        # Start with whatever earlier stage produced
        current_metrics_for_display = locals().get("current_metrics") if isinstance(locals().get("current_metrics"), dict) else {}

        # Force a display rebuild if earlier FIX41AFC19 did not apply or rebuilt_count==0
        _already_applied = bool(locals().get("_fix41afc19_applied"))
        _already_count = int(locals().get("_fix41afc19_rebuilt_count") or 0)

        if (not _already_applied) or (_already_count <= 0):
            _fix41afc19_attempted_v19 = True

            # Resolve the best available snapshot pool (post-attach merged universe)
            # NOTE: Different callers store the "current" snapshot pool under different names.
            # We intentionally scan a wide set of candidate keys and pick the largest non-empty list.
            _pool = None
            _pool_key_used = None
            try:
                _cand = []

                # Common/expected keys (current run)
                _cand.append(("baseline_sources_cache_current", locals().get("baseline_sources_cache_current")))
                _cand.append(("baseline_sources_cache", locals().get("baseline_sources_cache")))
                _cand.append(("baseline_sources_cache_prefetched", locals().get("baseline_sources_cache_prefetched")))

                # Attachment / merge outputs (varies by branch)
                _cand.append(("baseline_sources_cache_attached", locals().get("baseline_sources_cache_attached")))
                _cand.append(("baseline_sources_cache_merged", locals().get("baseline_sources_cache_merged")))
                _cand.append(("attached_pool", locals().get("attached_pool")))
                _cand.append(("current_pool", locals().get("current_pool")))
                _cand.append(("source_snapshots_current", locals().get("source_snapshots_current")))
                _cand.append(("snapshots_current", locals().get("snapshots_current")))

                # Sometimes carried on output dict
                _out = (locals().get("output") if isinstance(locals().get("output"), dict) else None)
                if isinstance(_out, dict):
                    _cand.append(("output.baseline_sources_cache_current", _out.get("baseline_sources_cache_current")))
                    _cand.append(("output.baseline_sources_cache", _out.get("baseline_sources_cache")))
                    _res = _out.get("results") if isinstance(_out.get("results"), dict) else None
                    if isinstance(_res, dict):
                        _cand.append(("output.results.baseline_sources_cache_current", _res.get("baseline_sources_cache_current")))
                        _cand.append(("output.results.baseline_sources_cache", _res.get("baseline_sources_cache")))
                        _cand.append(("output.results.attached_pool", _res.get("attached_pool")))
                        _cand.append(("output.results.current_pool", _res.get("current_pool")))

                # Final sweep: anything in locals containing baseline_sources_cache*
                for _k, _v in (locals() or {}).items():
                    if not isinstance(_k, str):
                        continue
                    if "baseline_sources_cache" in _k and isinstance(_v, list) and _v:
                        _cand.append((_k, _v))

                # Choose best candidate (largest list wins)
                _best = None
                for _k, _v in _cand:
                    if isinstance(_v, list) and _v:
                        if _best is None or len(_v) > len(_best[1] or []):
                            _best = (_k, _v)
                if _best is not None:
                    _pool_key_used, _pool = _best[0], _best[1]
            except Exception:
                pass
                _pool = None
                _pool_key_used = None


            if _pool is None:
                try:
                    _cand_pools = []
                    for _k, _v in (locals() or {}).items():
                        if not isinstance(_k, str):
                            continue
                        if "baseline_sources_cache" in _k and isinstance(_v, list) and _v:
                            _cand_pools.append((_k, _v))
                    if _cand_pools:
                        _cand_pools.sort(key=lambda kv: len(kv[1] or []), reverse=True)
                        _pool = _cand_pools[0][1]
                except Exception:
                    pass
                    _pool = None

            if isinstance(_pool, list):
                _fix41afc19_pool_count_v19 = len(_pool or [])
                _fix41afc19_pool_key_used_v19 = _pool_key_used

            # Pick best available rebuild fn
            def _fix2d3_resolve_callable(_name: str):
                try:
                    fn = globals().get(_name)
                    if callable(fn):
                        return fn
                except Exception:
                    pass
                try:
                    fn = locals().get(_name)
                    if callable(fn):
                        return fn
                except Exception:
                    pass
                try:
                    import sys as _sys
                    _mod = _sys.modules.get(__name__)
                    fn = getattr(_mod, _name, None) if _mod else None
                    if callable(fn):
                        return fn
                except Exception:
                    return None


            _fn = _fix2d3_resolve_callable("rebuild_metrics_from_snapshots_analysis_canonical_v1")
            _fn_name = "rebuild_metrics_from_snapshots_analysis_canonical_v1"
            if not callable(_fn):
                _fn = _fix2d3_resolve_callable("rebuild_metrics_from_snapshots_schema_only_fix16")
                _fn_name = "rebuild_metrics_from_snapshots_schema_only_fix16"


            # Some branches expose only the legacy names. Accept them as safe fallbacks
            # so 'fn_missing' doesn't mask a usable rebuild implementation.
            if not callable(_fn):
                _fn = globals().get("rebuild_metrics_from_snapshots_schema_only")
                _fn_name = "rebuild_metrics_from_snapshots_schema_only"
            if not callable(_fn):
                _fn = globals().get("rebuild_metrics_from_snapshots_with_anchors")
                _fn_name = "rebuild_metrics_from_snapshots_with_anchors"
            if not callable(_fn):
                _fn = globals().get("rebuild_metrics_from_snapshots")
                _fn_name = "rebuild_metrics_from_snapshots"

            if not callable(_fn):
                _fix41afc19_skip_reason_v19 = "fn_missing"
            elif _pool is None:
                _fix41afc19_skip_reason_v19 = "pool_missing"
            elif not isinstance(_pool, list) or not _pool:
                _fix41afc19_skip_reason_v19 = "pool_empty"
            else:
                try:
                    try:
                        _rebuilt = _fn(prev_response, _pool, web_context=web_context)
                    except TypeError:
                        _rebuilt = _fn(prev_response, _pool)
                except Exception as _e:
                    _rebuilt = None
                    _fix41afc19_skip_reason_v19 = "rebuild_exception:" + str(type(_e).__name__)
                    try:
                        _m = str(_e) if _e is not None else ""
                        if _m:
                            _fix41afc19_skip_reason_v19 = _fix41afc19_skip_reason_v19 + ":" + _m[:160]
                    except Exception:
                        pass

                # REFACTOR35: guard against schema-only rebuilds leaking debug keys into PMC
                # Keep only keys that exist in the frozen schema.
                try:
                    _schema_keys = set((analysis.get('metric_schema_frozen') or {}).keys()) if isinstance(analysis.get('metric_schema_frozen'), dict) else set()
                    if isinstance(_rebuilt, dict) and _schema_keys:
                        _rebuilt = {k: v for (k, v) in _rebuilt.items() if isinstance(k, str) and k in _schema_keys and isinstance(v, dict)}
                except Exception:
                    pass

                if isinstance(_rebuilt, dict) and _rebuilt:
                    current_metrics_for_display = dict(_rebuilt)
                    _fix41afc19_skip_reason_v19 = "applied_v19_display_rebuild:" + str(_fn_name)
                    try:
                        _fix41afc19_keys_sample_v19 = list(current_metrics_for_display.keys())[:10]
                    except Exception:
                        pass
                        _fix41afc19_keys_sample_v19 = []
                else:
                    _fix41afc19_skip_reason_v19 = _fix41afc19_skip_reason_v19 or "rebuilt_empty_or_non_dict"

        # Apply display override (used by diff below)
        locals()["current_metrics"] = current_metrics_for_display  # keep variable name used by downstream diff
        # Purpose: Override current_metrics_for_display with schema-anchored rebuild
        try:
            _fix2d9_over, _fix2d9_diag = _fix2d9_schema_anchored_rebuild_current_metrics_v1(
                prev_response=prev_response,
                pool=_pool,
                web_context=web_context,
            )
            try:
                output.setdefault('results', {}).setdefault('debug', {})
                output['results']['debug']['fix2d9_schema_anchored_rebuild_v1'] = _fix2d9_diag
            except Exception:
                pass
            if isinstance(_fix2d9_over, dict) and _fix2d9_over:
                current_metrics_for_display = dict(_fix2d9_over)
                locals()['current_metrics'] = current_metrics_for_display
                try:
                    output.setdefault('results', {})['primary_metrics_canonical'] = current_metrics_for_display
                except Exception:
                    pass
        except Exception:
            pass
        # Purpose:
        #   - Stamp exec code version + join mode into results.debug
        #   - Propagate current_metrics_for_display into results.primary_metrics_canonical
        #     so downstream render/diff can populate the 'Current' column.
        try:
            output.setdefault('results', {}).setdefault('debug', {})
            output['results']['debug']['__exec_code_version'] = globals().get('CODE_VERSION')
            try:
                output['results']['debug']['__exec_join_mode'] = _fix2d6_get_diff_join_mode_v1()
            except Exception:
                pass
        except Exception:
            pass
        try:
            if isinstance(current_metrics_for_display, dict) and current_metrics_for_display:
                output.setdefault('results', {})
                output['results']['primary_metrics_canonical'] = dict(current_metrics_for_display)
                try:
                    output['results'].setdefault('primary_response', {})
                    if isinstance(output['results']['primary_response'], dict):
                        output['results']['primary_response']['primary_metrics_canonical'] = dict(current_metrics_for_display)
                except Exception:
                    pass
                output.setdefault('debug', {})
                if isinstance(output['debug'], dict):
                    output['debug']['fix2d7_propagate_current_canon_v1'] = {
                        'applied': True,
                        'count': int(len(current_metrics_for_display)),
                        'keys_sample': list(current_metrics_for_display.keys())[:10],
                    }
        except Exception:
            pass

        try:
            _fix41afc19_keys_sample_v19 = _fix41afc19_keys_sample_v19 or (list(current_metrics_for_display.keys())[:10] if isinstance(current_metrics_for_display, dict) else [])
        except Exception:
            pass

        # Heuristic: pick the known canonical keys if present, else infer from schema/keys.
        _key_candidates = [
            "units_sold_2024__unit_sales",
            "market_share_2024__percent",
            "projected_market_share_2026__percent",
            "projected_market_share_2030__percent",
            "yoy_growth_rate_2024__percent",
            "cagr_2024__percent",
        ]

        try:
            prev_canon = (prev_response or {}).get("primary_metrics_canonical") if isinstance(prev_response, dict) else {}
            if not isinstance(prev_canon, dict):
                prev_canon = {}

            cur_canon = current_metrics_for_display if isinstance(current_metrics_for_display, dict) else {}

            try:
                if isinstance(output.get("debug"), dict):
                    output["debug"]["key_overlap_v1"] = _emit_key_overlap_debug_v1(
                        prev_metrics=prev_canon,
                        cur_metrics=cur_canon,
                        target_key="global_ev_sales_ytd_2025__unit_sales"
                    )
            except Exception:
                pass

            # Infer projected-share keys if the exact ones are not present
            if isinstance(prev_canon, dict) and isinstance(cur_canon, dict):
                all_keys = set(list(prev_canon.keys()) + list(cur_canon.keys()))
                for k in sorted(all_keys):
                    lk = k.lower()
                    if ("project" in lk or "proj" in lk) and ("share" in lk or "market_share" in lk) and ("2026" in lk or "2030" in lk) and k not in _key_candidates:
                        _key_candidates.append(k)

            # Flatten snapshot candidates once (for top3 glimpse)
            flat = []
            try:
                _pool_for_flat = locals().get("baseline_sources_cache") if isinstance(locals().get("baseline_sources_cache"), list) else []
                for sr in _pool_for_flat or []:
                    if isinstance(sr, dict):
                        for c in (sr.get("extracted_numbers") or []):
                            if isinstance(c, dict):
                                flat.append(c)
            except Exception:
                pass
                flat = []

            def _cand_has_unit_evidence(c: dict) -> bool:
                try:
                    if (c.get("unit_tag") or c.get("unit") or c.get("unit_norm") or c.get("unit_raw") or "").strip():
                        return True
                    if (c.get("currency") or c.get("currency_symbol") or "").strip():
                        return True
                    if c.get("is_percent") or c.get("has_percent"):
                        return True
                    if (c.get("base_unit") or "").strip():
                        return True
                    if (c.get("unit_family") or "").strip():
                        return True
                    if isinstance(c.get("unit_tokens"), list) and c.get("unit_tokens"):
                        return True
                except Exception:
                    return False
                return False

            # Schema lookup (if available)
            schema = {}
            try:
                if isinstance(previous_data, dict):
                    pr = previous_data.get("primary_response") if isinstance(previous_data.get("primary_response"), dict) else previous_data
                    schema = (pr.get("metric_schema_frozen") or {}) if isinstance(pr, dict) else {}
            except Exception:
                pass
                schema = {}

            def _score_candidate_for_key(c: dict, ckey: str) -> int:
                try:
                    md = schema.get(ckey) if isinstance(schema, dict) else {}
                    kws = []
                    if isinstance(md, dict):
                        kws = md.get("keywords") or md.get("keyword_hints") or []
                    kws = [str(k).lower() for k in kws if str(k).strip()]
                    ctx = (c.get("context") or c.get("window") or c.get("context_window") or "").lower()
                    s = 0
                    for k in kws[:25]:
                        if k and k in ctx:
                            s += 1
                    if _cand_has_unit_evidence(c):
                        s += 5
                    # Prefer anchor-matching if both present
                    try:
                        if (c.get("anchor_hash") or "") and isinstance(md, dict) and (md.get("anchor_hash") or ""):
                            if str(c.get("anchor_hash")) == str(md.get("anchor_hash")):
                                s += 10
                    except Exception:
                        return int(s)
                except Exception:
                    return 0

            # Build traces for up to 5 keys that exist in either prev/cur
            _keys_for_trace = []
            for k in _key_candidates:
                if k in (prev_canon or {}) or k in (cur_canon or {}):
                    _keys_for_trace.append(k)
                if len(_keys_for_trace) >= 5:
                    break
            if not _keys_for_trace:
                # fallback: first 5 keys from prev canonical
                _keys_for_trace = list(prev_canon.keys())[:5]

            for k in _keys_for_trace:
                pv = (prev_canon.get(k) or {}) if isinstance(prev_canon, dict) else {}
                cv = (cur_canon.get(k) or {}) if isinstance(cur_canon, dict) else {}

                winner_src = "unknown"
                if _fix41afc19_attempted_v19 and "applied_v19_display_rebuild" in (_fix41afc19_skip_reason_v19 or ""):
                    winner_src = "analysis_canonical_rebuild_v19"
                elif bool(locals().get("_fix41afc19_applied")):
                    winner_src = "analysis_canonical_rebuild"
                else:
                    winner_src = "fallback_snapshot_selector"

                # Top3 glimpse from flat pool
                top3 = []
                try:
                    scored = sorted(flat, key=lambda c: _score_candidate_for_key(c, k), reverse=True)[:3]
                    for t in scored:
                        top3.append({
                            "raw": t.get("raw"),
                            "value_norm": t.get("value_norm"),
                            "unit_tag": t.get("unit_tag") or t.get("unit") or "",
                            "has_unit_evidence": bool(_cand_has_unit_evidence(t)),
                            "anchor_hash": t.get("anchor_hash"),
                        })
                except Exception:
                    pass
                    top3 = []

                _fix41afc19_winner_trace_v19[k] = {
                    "winner_source": winner_src,
                    "prev_value_norm": pv.get("value_norm"),
                    "prev_unit": pv.get("unit") or pv.get("unit_tag") or "",
                    "cur_value_norm": cv.get("value_norm"),
                    "cur_unit": cv.get("unit") or cv.get("unit_tag") or "",
                    "cur_has_unit_evidence": bool((cv.get("unit") or cv.get("unit_tag") or "").strip()),
                    "top3_candidates_glimpse": top3,
                }
        except Exception:
            pass

    except Exception:
        pass

    try:
        if isinstance(output.get("debug"), dict):
            output["debug"].setdefault("fix41afc19", {})
            output["debug"]["fix41afc19"]["attempted"] = bool(_fix41afc19_attempted_v19)
            # Keep the pre-existing 'applied' field as-is, but add applied_v19_display_override
            output["debug"]["fix41afc19"]["applied_v19_display_override"] = bool(_fix41afc19_attempted_v19 and ("applied_v19_display_rebuild" in (_fix41afc19_skip_reason_v19 or "")))
            # Never allow blank reason: prefer v19 skip_reason when earlier reason is empty
            _prev_reason = str(output["debug"]["fix41afc19"].get("reason") or "")
            if (not _prev_reason.strip()) and (_fix41afc19_skip_reason_v19 or "").strip():
                output["debug"]["fix41afc19"]["reason"] = str(_fix41afc19_skip_reason_v19)
            output["debug"]["fix41afc19"]["skip_reason_v19"] = str(_fix41afc19_skip_reason_v19 or "")
            output["debug"]["fix41afc19"]["pool_count_v19"] = int(_fix41afc19_pool_count_v19 or 0)
            output["debug"]["fix41afc19"]["pool_key_used_v19"] = str(locals().get("_fix41afc19_pool_key_used_v19") or locals().get("_pool_key_used") or "")
            output["debug"]["fix41afc19"]["rebuilt_keys_sample_v19"] = list(_fix41afc19_keys_sample_v19 or [])
            if _fix41afc19_winner_trace_v19:
                output["debug"]["evo_winner_trace_v1"] = _fix41afc19_winner_trace_v19
    except Exception:
        pass
    # "Current" from a canonical-for-render payload (analysis-aligned) WITHOUT
    # touching fastpath/hashing/snapshot-attach.
    #
    # Why:
    # - Evolution UI renders from diff rows (metric_changes), not analysis key-metrics.
    # - If diff rows source "Current" from raw extracted pools, unitless survivors
    #   (e.g., 170, 2) can win.
    # - We compute a late, render-only canonical dict from the frozen snapshot pool
    #   using the same rebuild semantics as analysis (best effort), then force the
    #   diff + row hydration to use it.
    #
    # Safety:
    # - Purely post-snapshot, post-hash: affects ONLY dashboard/diff rendering.
    # - Does NOT alter source selection, hashing inputs, injection lifecycle, fastpath.
    #
    # Diagnostics:
    # - output.debug.canonical_for_render_v1
    # - output.debug.canonical_for_render_row_audit_v1
    _canonical_for_render_applied = False
    _canonical_for_render_reason = ""
    _canonical_for_render_fn = ""
    _canonical_for_render_count = 0
    _canonical_for_render_keys_sample = []
    _canonical_for_render_replaced_current_metrics = False

    canonical_for_render = {}
    try:
        # Default: use whatever current_metrics we already have
        canonical_for_render = current_metrics if isinstance(current_metrics, dict) else {}
        # REFACTOR15: if refactors nested the current PMC under output['results'], recover it so diffing can't go empty.
        if (not isinstance(canonical_for_render, dict)) or (isinstance(canonical_for_render, dict) and not canonical_for_render):
            try:
                if isinstance(output.get("primary_metrics_canonical"), dict) and output.get("primary_metrics_canonical"):
                    canonical_for_render = output.get("primary_metrics_canonical")
            except Exception:
                pass
        if (not isinstance(canonical_for_render, dict)) or (isinstance(canonical_for_render, dict) and not canonical_for_render):
            try:
                _pr = output.get("primary_response") if isinstance(output.get("primary_response"), dict) else None
                if isinstance(_pr, dict) and isinstance(_pr.get("primary_metrics_canonical"), dict) and _pr.get("primary_metrics_canonical"):
                    canonical_for_render = _pr.get("primary_metrics_canonical")
            except Exception:
                pass
        if (not isinstance(canonical_for_render, dict)) or (isinstance(canonical_for_render, dict) and not canonical_for_render):
            try:
                _r = output.get("results") if isinstance(output.get("results"), dict) else None
                if isinstance(_r, dict) and isinstance(_r.get("primary_metrics_canonical"), dict) and _r.get("primary_metrics_canonical"):
                    canonical_for_render = _r.get("primary_metrics_canonical")
            except Exception:
                pass
        # Also publish for downstream consumers (UI + v2 diff builder)
        try:
            if isinstance(canonical_for_render, dict):
                output["primary_metrics_canonical"] = canonical_for_render
                if isinstance(output.get("primary_response"), dict):
                    output["primary_response"]["primary_metrics_canonical"] = canonical_for_render
        except Exception:
            pass

        # REFACTOR26: hydrate source_url for canonical metrics (in-place, schema-preserving)
        try:
            _pmc_tmp = output.get("primary_metrics_canonical") if isinstance(output, dict) else None
            if isinstance(_pmc_tmp, dict):
                _refactor26_hydrate_primary_metrics_canonical_source_urls_v1(_pmc_tmp)
        except Exception:
            pass


        # Goal:
        # - Stop seeding canonical_for_render from current_metrics because current_metrics may already
        #   contain year-like / unitless / junk winners (e.g., "2.0 B", "-6441").
        # - Force the downstream rebuild path (which is intended to be analysis-aligned) to run,
        #   while keeping fastpath/hashing/injection/snapshot attach untouched.
        #
        # Mechanism:
        # - If prev_response carries a frozen schema, disable the seed by default.
        # - Allow opt-out via env var EVO_CANONICAL_FOR_RENDER_ALLOW_SEED=1.
        # - Emit a small trace later via _canonical_for_render_reason tag.
        try:
            _allow_seed = str(os.getenv("EVO_CANONICAL_FOR_RENDER_ALLOW_SEED", "") or "").strip() in ("1", "true", "True", "yes", "YES")
            _has_schema = isinstance(prev_response, dict) and isinstance(prev_response.get("metric_schema_frozen") or {}, dict) and bool(prev_response.get("metric_schema_frozen"))
            if _has_schema and not _allow_seed:
                canonical_for_render = {}
                _canonical_for_render_reason = "v30_seed_disabled_force_rebuild"
        except Exception:
            pass

        # Even when current_metrics has "enough" keys, it can still be junk (year-like/unitless winners).
        # Detect suspicious existing canonical dict and force a render-only rebuild in that case.
        def _v21_yearlike(x):
            try:
                if x is None:
                    return False
                fx = float(x)
                if abs(fx - round(fx)) < 1e-9:
                    ix = int(round(fx))
                    return 1900 <= ix <= 2105
                return False
            except Exception:
                return False

        def _v21_metric_suspicious(m):
            try:
                if not isinstance(m, dict):
                    return True
                u = (m.get("unit") or m.get("unit_tag") or "").strip()
                vn = m.get("value_norm")
                if (not u) and (_v21_yearlike(vn) or vn is None):
                    return True
                return False
            except Exception:
                return True

        _suspicious_existing = False
        try:
            if isinstance(canonical_for_render, dict) and canonical_for_render:
                _keys_sample_chk = list(sorted(list(canonical_for_render.keys())))[:25]
                _sus = 0
                _tot = 0
                for _k in _keys_sample_chk:
                    _tot += 1
                    if _v21_metric_suspicious(canonical_for_render.get(_k)):
                        _sus += 1
                if _tot > 0:
                    _suspicious_existing = (_sus / float(_tot)) >= 0.30
        except Exception:
            pass
            _suspicious_existing = False

        # Best-effort: rebuild canonical-for-render from frozen snapshots using analysis-aligned builder.
        # Apply when current canonical is missing/suspiciously small.
        _need_render_rebuild = (not isinstance(canonical_for_render, dict)) or (len(canonical_for_render) < 3) or bool(_suspicious_existing)
        if _need_render_rebuild and isinstance(baseline_sources_cache, list) and baseline_sources_cache:
            # Resolve schema/anchors/canon from prev_response (analysis baseline)
            _schema = {}
            _anchors = {}
            _prev_canon = {}
            try:
                if isinstance(prev_response, dict):
                    _schema = prev_response.get("metric_schema_frozen") or {}
                    _anchors = prev_response.get("metric_anchors") or {}
                    _prev_canon = prev_response.get("primary_metrics_canonical") or {}
            except Exception:
                pass
                _schema, _anchors, _prev_canon = {}, {}, {}

            # Choose the best available analysis-aligned rebuild function
            _fn = globals().get("rebuild_metrics_from_snapshots_analysis_canonical_v1")
            if callable(_fn):
                try:
                    canonical_for_render = _fn(baseline_sources_cache, _schema, _anchors, _prev_canon)
                    _canonical_for_render_fn = "rebuild_metrics_from_snapshots_analysis_canonical_v1"
                except Exception:
                    pass
                    canonical_for_render = {}
            if (not canonical_for_render) and callable(globals().get("rebuild_metrics_from_snapshots_with_anchors_fix16")):
                try:
                    _fn2 = globals().get("rebuild_metrics_from_snapshots_with_anchors_fix16")
                    canonical_for_render = _fn2(baseline_sources_cache, _schema, _anchors, _prev_canon)
                    _canonical_for_render_fn = "rebuild_metrics_from_snapshots_with_anchors_fix16"
                except Exception:
                    pass
                    canonical_for_render = {}
            if (not canonical_for_render) and callable(globals().get("rebuild_metrics_from_snapshots_schema_only")):
                try:
                    _fn3 = globals().get("rebuild_metrics_from_snapshots_schema_only")
                    canonical_for_render = _fn3(baseline_sources_cache, _schema)
                    _canonical_for_render_fn = "rebuild_metrics_from_snapshots_schema_only"
                except Exception:
                    pass
                    canonical_for_render = {}

            if isinstance(canonical_for_render, dict) and canonical_for_render:
                _canonical_for_render_applied = True
                _canonical_for_render_reason = "applied_render_only_rebuild" if not bool(_suspicious_existing) else "applied_render_only_rebuild_forced_suspicious"
                _canonical_for_render_count = int(len(canonical_for_render))
                _canonical_for_render_keys_sample = list(sorted(list(canonical_for_render.keys())))[:12]
                _canonical_for_render_replaced_current_metrics = True
                try:
                    if not str(_canonical_for_render_fn or "").strip():
                        _canonical_for_render_fn = "unknown_rebuild_fn"
                except Exception:
                    pass
            else:
                _canonical_for_render_reason = "render_rebuild_failed_or_empty"
        else:
            _canonical_for_render_reason = "used_existing_current_metrics" if not bool(_suspicious_existing) else "forced_render_rebuild_due_to_suspicious_existing_failed"
            _canonical_for_render_count = int(len(canonical_for_render)) if isinstance(canonical_for_render, dict) else 0
            _canonical_for_render_keys_sample = list(sorted(list(canonical_for_render.keys())))[:12] if isinstance(canonical_for_render, dict) else []
    except Exception:
        pass
        canonical_for_render = current_metrics if isinstance(current_metrics, dict) else {}
        _canonical_for_render_reason = "exception_fallback_existing"


    # Problem observed:
    # - canonical_for_render rebuild may select junk numbers from the frozen pool
    #   (e.g., GlobeNewswire footer "2B" or email fragments "-6441") when anchors
    #   are not strictly enforced.
    #
    # Fix:
    # - If prev_response provides metric_anchors for a canonical_key, forcibly
    #   resolve the exact anchored candidate from baseline_sources_cache by matching
    #   anchor_hash and use that to populate canonical_for_render for that key.
    #
    # Scope / Safety:
    # - Render-only: affects ONLY canonical_for_render (dashboard "Current")
    # - Does NOT touch hashing, fastpath, injection lifecycle, or snapshot attach.
    #
    # Diagnostics:
    # - output.debug.canonical_for_render_anchor_enforce_v28 (summary)
    # - per-metric cm["diag"]["v28_anchor_enforced"] (when applied)
    _v28_anchor_enforce = {
        "attempted": False,
        "schema_keys": 0,
        "anchors_keys": 0,
        "hits": 0,
        "misses": 0,
        "hit_keys_sample": [],
        "miss_keys_sample": [],
        "note": "render-only anchor enforcement by anchor_hash against frozen extracted_numbers pool",
    }

    def _v28_iter_numbers_from_sources_cache(_sources_cache):
        try:
            for _src in (_sources_cache or []):
                if not isinstance(_src, dict):
                    continue
                _url = _src.get("url") or _src.get("source_url") or ""
                nums = _src.get("extracted_numbers") or []
                if isinstance(nums, list):
                    for _n in nums:
                        if isinstance(_n, dict):
                            yield _url, _n
        except Exception:
            return

    def _v28_pick_by_anchor_hash(_sources_cache, _anchor_hash: str):
        try:
            ah = str(_anchor_hash or "").strip()
            if not ah or ah == "None":
                return None
            for _url, _n in _v28_iter_numbers_from_sources_cache(_sources_cache):
                try:
                    if str(_n.get("anchor_hash") or "").strip() == ah:
                        out = dict(_n)
                        if _url and (not out.get("source_url")):
                            out["source_url"] = _url
                        return out
                except Exception:
                    pass
                    continue
            return None
        except Exception:
            return None

    def _v28_schema_unit_label(_schema_row: dict) -> str:
        try:
            if not isinstance(_schema_row, dict):
                return ""
            # Prefer schema unit_tag (human-friendly) then unit
            u = (_schema_row.get("unit_tag") or _schema_row.get("unit") or "").strip()
            # Small convenience mapping
            if u == "M":
                return "million units"
            if u == "B":
                return "billion"
            return u
        except Exception:
            return ""

    try:
        if isinstance(canonical_for_render, dict) and isinstance(baseline_sources_cache, list) and baseline_sources_cache:
            _v28_anchor_enforce["attempted"] = True
            _schema = {}
            _anchors = {}
            try:
                if isinstance(prev_response, dict):
                    _schema = prev_response.get("metric_schema_frozen") or {}
                    _anchors = prev_response.get("metric_anchors") or {}
            except Exception:
                pass
                _schema, _anchors = {}, {}
            _v28_anchor_enforce["schema_keys"] = int(len(_schema)) if isinstance(_schema, dict) else 0
            _v28_anchor_enforce["anchors_keys"] = int(len(_anchors)) if isinstance(_anchors, dict) else 0

            if isinstance(_anchors, dict) and _anchors:
                for _ckey, _ainfo in list(_anchors.items()):
                    try:
                        if not _ckey:
                            continue
                        if not isinstance(_ainfo, dict):
                            continue
                        _ah = _ainfo.get("anchor_hash") or _ainfo.get("anchor") or ""
                        if not str(_ah or "").strip() or str(_ah) == "None":
                            continue

                        cand = _v28_pick_by_anchor_hash(baseline_sources_cache, _ah)
                        if not isinstance(cand, dict):
                            _v28_anchor_enforce["misses"] += 1
                            if len(_v28_anchor_enforce["miss_keys_sample"]) < 12:
                                _v28_anchor_enforce["miss_keys_sample"].append(str(_ckey))
                            continue

                        # Build minimal schema-aligned canonical metric
                        srow = _schema.get(_ckey) if isinstance(_schema, dict) else None
                        unit_lbl = _v28_schema_unit_label(srow if isinstance(srow, dict) else {})
                        vnorm = cand.get("value_norm")
                        if vnorm is None:
                            vnorm = cand.get("value")
                        raw = (cand.get("raw") or "").strip()
                        if not raw:
                            try:
                                if vnorm is not None and unit_lbl:
                                    raw = f"{vnorm} {unit_lbl}".strip()
                                elif vnorm is not None:
                                    raw = str(vnorm)
                            except Exception:
                                pass
                                raw = ""

                        cm = canonical_for_render.get(_ckey) if isinstance(canonical_for_render, dict) else None
                        if not isinstance(cm, dict):
                            cm = {}
                        cm["value_norm"] = vnorm
                        cm["unit"] = unit_lbl
                        cm["unit_tag"] = unit_lbl
                        if raw:
                            cm["raw"] = raw
                        # Provide evidence and source hint
                        cm["source_url"] = cand.get("source_url") or cand.get("url") or _ainfo.get("source_url") or ""
                        cm["context_snippet"] = cand.get("context_snippet") or cand.get("context") or _ainfo.get("context_snippet") or ""
                        cm["evidence"] = [cand]
                        cm.setdefault("diag", {})
                        if isinstance(cm.get("diag"), dict):
                            cm["diag"]["v28_anchor_enforced"] = True
                            cm["diag"]["v28_anchor_hash"] = str(_ah)
                            cm["diag"]["v28_anchor_candidate_raw"] = cand.get("raw")
                            cm["diag"]["v28_anchor_candidate_unit"] = cand.get("unit") or cand.get("unit_tag") or ""
                            cm["diag"]["v28_anchor_candidate_value_norm"] = cand.get("value_norm")

                        canonical_for_render[_ckey] = cm
                        _v28_anchor_enforce["hits"] += 1
                        if len(_v28_anchor_enforce["hit_keys_sample"]) < 12:
                            _v28_anchor_enforce["hit_keys_sample"].append(str(_ckey))
                    except Exception:
                        pass
                        continue


    except Exception:
        pass


    # Purpose:
    #   Render-gate fallback (UNION / demo mode only).
    #   If V28 anchor enforcement was attempted but produced zero hits,
    #   populate canonical_for_render from current primary_metrics_canonical,
    #   and label metrics as unanchored-for-render.
    #
    # Safety:
    #   - Render-only (does not affect canonicalisation, hashing, snapshots, fastpath)
    #   - UNION mode only
    #   - No new try/except blocks inside compute_source_anchored_diff

    _fix2d11_join_mode = None
    if "_fix2d6_get_diff_join_mode_v1" in globals():
        _fix2d11_join_mode = _fix2d6_get_diff_join_mode_v1()

    if (
        isinstance(locals().get("_v28_anchor_enforce"), dict)
        and _v28_anchor_enforce.get("attempted")
        and int(_v28_anchor_enforce.get("hits") or 0) == 0
        and str(_fix2d11_join_mode or "").lower() == "union"
    ):
        _pmc = None
        if isinstance(output.get("results"), dict):
            _pmc = output["results"].get("primary_metrics_canonical")

        if isinstance(_pmc, dict) and _pmc:
            canonical_for_render = dict(_pmc)
            for _k, _cm in canonical_for_render.items():
                if isinstance(_cm, dict):
                    _cm.setdefault("diag", {})
                    if isinstance(_cm.get("diag"), dict):
                        _cm["diag"]["render_origin"] = "current_unanchored"

            _v28_anchor_enforce["fix2d11_fallback_applied"] = True
            _v28_anchor_enforce["fix2d11_fallback_count"] = len(canonical_for_render)
        else:
            _v28_anchor_enforce["fix2d11_fallback_applied"] = False
            _v28_anchor_enforce["fix2d11_fallback_reason"] = "no_current_primary_metrics_canonical"


    try:
        if isinstance(output.get("debug"), dict):
            output["debug"]["canonical_for_render_anchor_enforce_v28"] = _v28_anchor_enforce
    except Exception:
        pass

    # Problem:
    # - canonical_for_render can still select "junk" numerics (e.g., footer phone
    #   fragments like -6441 or marketing magnitudes like 2B) as Current, because
    #   the frozen extracted_numbers pool is noisy and some late selection paths
    #   lack strict schema gating.
    #
    # Fix (render-only):
    # - Apply a strict schema-compatibility gate for canonical_for_render values.
    # - Hard-reject phone/contact/email/footer-like contexts.
    # - If the existing canonical_for_render metric is suspicious (unitless for
    #   unit-required dimensions, or unit-incompatible like "B" for percent),
    #   attempt to replace it with the best compatible candidate from the frozen
    #   extracted_numbers pool.
    #
    # Safety:
    # - Render-only: affects ONLY canonical_for_render (dashboard Current).
    # - Does NOT touch fastpath replay, hashing universe, injection lifecycle,
    #   snapshot attach, or extraction.
    #
    # Diagnostics:
    # - output.debug.canonical_for_render_schema_gate_v29 (summary)
    # - per-metric cm["diag"]["v29_schema_gate_*"] flags (when applied)
    _v29_schema_gate = {
        "attempted": False,
        "canonical_keys": 0,
        "suspicious": 0,
        "replaced": 0,
        "kept": 0,
        "candidates_checked": 0,
        "replaced_keys_sample": [],
        "suspicious_keys_sample": [],
        "note": "render-only schema gate + junk reject on canonical_for_render",
    }

    def _v29_s(_x):
        try:
            return str(_x or "")
        except Exception:
            return ""

    def _v29_lower(_x):
        try:
            return _v29_s(_x).lower()
        except Exception:
            return ""

    def _v29_get_text_blob(*parts):
        try:
            out = []
            for p in parts:
                if not p:
                    continue
                if isinstance(p, (list, tuple)):
                    out.extend([_v29_s(z) for z in p if z])
                else:
                    out.append(_v29_s(p))
            return " ".join([z for z in out if z]).strip()
        except Exception:
            return ""

    def _v29_phoneish(text):
        # Catch common phone patterns including "+1-888-600-6441" and fragments.
        try:
            import re
            t = _v29_s(text)
            if not t:
                return False
            if re.search(r"\+\d[\d\-\s]{7,}\d", t):
                return True
            if re.search(r"\b\d{3}[-\s]\d{3}[-\s]\d{4}\b", t):
                return True
            if re.search(r"\bext\.?\s*\d+\b", t, re.I):
                return True
            return False
        except Exception:
            return False

    def _v29_junk_context(text):
        try:
            t = _v29_lower(text)
            if not t:
                return False
            junk_terms = [
                "contact", "email", "phone", "tel", "telephone", "fax", "call us",
                "press release", "copyright", "all rights reserved", "subscribe",
                "unsubscribe", "privacy policy", "terms of use", "cookie", "newsletter",
                "about us", "follow us", "for media", "media contact"
            ]
            if any(w in t for w in junk_terms):
                return True
            if _v29_phoneish(text):
                return True
            # Many PR footers include an email address
            if "@" in t and "." in t:
                return True
            return False
        except Exception:
            return False

    def _v29_unit_str(obj):
        try:
            if isinstance(obj, dict):
                return _v29_s(obj.get("unit") or obj.get("unit_tag") or obj.get("unit_cmp") or obj.get("cur_unit_cmp") or "")
            return ""
        except Exception:
            return ""

    def _v29_value_norm(obj):
        try:
            if isinstance(obj, dict):
                v = obj.get("value_norm")
                if v is None:
                    v = obj.get("cur_value_norm")
                if v is None:
                    v = obj.get("value")
                return v
            return None
        except Exception:
            return None

    def _v29_has_unit_evidence(obj):
        try:
            # Conservative: unit evidence if unit string non-empty OR raw contains %/$/€ etc.
            if not isinstance(obj, dict):
                return False
            u = _v29_unit_str(obj).strip()
            if u:
                return True
            raw = _v29_s(obj.get("raw") or "")
            if "%" in raw or "$" in raw or "€" in raw or "£" in raw:
                return True
            return False
        except Exception:
            return False

    def _v29_expected_dimension(schema_row):
        try:
            if isinstance(schema_row, dict):
                # Prefer FIX16 helper if present
                if "_fix16_expected_dimension" in globals():
                    return _fix16_expected_dimension(schema_row)
                return schema_row.get("dimension") or schema_row.get("unit_family") or ""
            return ""
        except Exception:
            return ""

    def _v29_schema_requires_unit(schema_row):
        try:
            dim = _v29_lower(_v29_expected_dimension(schema_row))
            if dim in ("currency", "percent", "rate", "ratio"):
                return True
            uf = _v29_lower(_v29_s(schema_row.get("unit_family") if isinstance(schema_row, dict) else ""))
            if uf in ("currency", "percent", "rate", "ratio"):
                return True
            # unit_sales / unit counts should have some "unit-ness"
            if "unit" in dim or "unit" in uf:
                return True
            return False
        except Exception:
            return False

    def _v29_unit_compatible(schema_row, cand_obj):
        try:
            if isinstance(schema_row, dict) and isinstance(cand_obj, dict):
                if "_fix16_unit_compatible" in globals():
                    return bool(_fix16_unit_compatible(schema_row, cand_obj))
            # fallback heuristic
            dim = _v29_lower(_v29_expected_dimension(schema_row))
            u = _v29_lower(_v29_unit_str(cand_obj))
            raw = _v29_lower(_v29_s(cand_obj.get("raw") or ""))
            if dim == "percent":
                return ("%"
                        in u) or ("percent" in u) or ("%" in raw)
            if dim == "currency":
                return any(sym in raw for sym in ["$", "€", "£"]) or ("usd" in u) or ("eur" in u) or ("sgd" in u) or ("currency" in u)
            if "unit" in dim:
                # must not be percent/currency-like
                if "%" in raw or "%" in u:
                    return False
                if any(sym in raw for sym in ["$", "€", "£"]):
                    return False
                # prefer explicit unit words
                if "unit" in u or "vehicle" in u or "car" in u or "sales" in u:
                    return True
                # allow million/billion with implied units only if raw/context says units/sales
                ctx = _v29_lower(_v29_get_text_blob(cand_obj.get("context_snippet"), cand_obj.get("context")))
                if ("unit" in ctx) or ("sales" in ctx) or ("vehicle" in ctx):
                    return True
                return False
            return True
        except Exception:
            return False

    def _v29_is_suspicious_current(schema_row, cm):
        try:
            if not isinstance(cm, dict):
                return True
            dim = _v29_lower(_v29_expected_dimension(schema_row))
            requires_unit = _v29_schema_requires_unit(schema_row)
            u = _v29_lower(_v29_unit_str(cm))
            v = _v29_value_norm(cm)
            blob = _v29_get_text_blob(cm.get("raw"), cm.get("context_snippet"), cm.get("source_url"))
            if _v29_junk_context(blob):
                return True
            if requires_unit and not _v29_has_unit_evidence(cm):
                return True
            # Percent metrics must not carry magnitude units like B/M
            if dim == "percent" and ("b" in u or "m" in u) and "%" not in u:
                return True
            if dim == "percent" and isinstance(v, (int, float)) and abs(float(v)) > 1000:
                return True
            # Unit sales must not be negative (phone fragments, ids)
            if "unit" in dim and isinstance(v, (int, float)) and float(v) < 0:
                return True
            return not _v29_unit_compatible(schema_row, cm)
        except Exception:
            return True

    def _v29_keywords(schema_row):
        try:
            import re
            if not isinstance(schema_row, dict):
                return []
            nm = _v29_lower(schema_row.get("name") or schema_row.get("label") or schema_row.get("display_name") or "")
            toks = [t for t in re.split(r"[^a-z0-9]+", nm) if t and len(t) >= 4]
            # prune common filler
            bad = set(["global", "projected", "market", "share", "sales", "volume", "units", "unit", "year"])
            toks = [t for t in toks if t not in bad]
            return toks[:10]
        except Exception:
            return []

    def _v29_score_candidate(schema_row, cand_obj):
        try:
            score = 0
            u = _v29_lower(_v29_unit_str(cand_obj))
            raw = _v29_lower(_v29_s(cand_obj.get("raw") or ""))
            ctx = _v29_lower(_v29_get_text_blob(cand_obj.get("context_snippet"), cand_obj.get("context")))
            dim = _v29_lower(_v29_expected_dimension(schema_row))
            if dim == "percent":
                if "%" in raw or "%" in u or "percent" in u:
                    score += 5
                if "b" in u or "m" in u:
                    score -= 4
            if "unit" in dim:
                if "unit" in ctx or "sales" in ctx or "vehicle" in ctx:
                    score += 3
                if "%" in raw or "$" in raw:
                    score -= 3
            if not _v29_junk_context(ctx + " " + raw):
                score += 1
            for kw in _v29_keywords(schema_row):
                if kw and kw in ctx:
                    score += 1
            return score
        except Exception:
            return 0

    try:
        if isinstance(canonical_for_render, dict) and isinstance(prev_response, dict) and isinstance(baseline_sources_cache, list) and baseline_sources_cache:
            _v29_schema_gate["attempted"] = True
            _schema = prev_response.get("metric_schema_frozen") or {}
            _v29_schema_gate["canonical_keys"] = int(len(canonical_for_render)) if isinstance(canonical_for_render, dict) else 0

            for _ckey, _cm in list(canonical_for_render.items()):
                try:
                    if not _ckey:
                        continue
                    srow = _schema.get(_ckey) if isinstance(_schema, dict) else None
                    if not isinstance(srow, dict):
                        # without schema, we cannot safely gate; keep
                        _v29_schema_gate["kept"] += 1
                        continue

                    if _v29_is_suspicious_current(srow, _cm):
                        _v29_schema_gate["suspicious"] += 1
                        if len(_v29_schema_gate["suspicious_keys_sample"]) < 12:
                            _v29_schema_gate["suspicious_keys_sample"].append(str(_ckey))

                        best = None
                        best_score = -10**9
                        # search frozen pool for compatible candidates
                        for cand in _v28_iter_numbers_from_sources_cache(baseline_sources_cache):
                            _v29_schema_gate["candidates_checked"] += 1
                            if not isinstance(cand, dict):
                                continue
                            blob = _v29_get_text_blob(cand.get("raw"), cand.get("context_snippet"), cand.get("context"), cand.get("source_url") or cand.get("url"))
                            if _v29_junk_context(blob):
                                continue
                            # normalize candidate
                            cobj = dict(cand)
                            # ensure value_norm present if possible
                            if cobj.get("value_norm") is None and cobj.get("value") is not None:
                                cobj["value_norm"] = cobj.get("value")
                            if not _v29_unit_compatible(srow, cobj):
                                continue
                            if _v29_schema_requires_unit(srow) and not _v29_has_unit_evidence(cobj):
                                continue
                            sc = _v29_score_candidate(srow, cobj)
                            if sc > best_score:
                                best_score = sc
                                best = cobj
                        if isinstance(best, dict):
                            prior_v = _v29_value_norm(_cm if isinstance(_cm, dict) else {})
                            prior_u = _v29_unit_str(_cm if isinstance(_cm, dict) else {})
                            # overwrite with best candidate
                            if not isinstance(_cm, dict):
                                _cm = {}
                            _cm["value_norm"] = best.get("value_norm")
                            _cm["unit"] = _v28_schema_unit_label(srow) or (_v29_unit_str(best) or _v28_schema_unit_label(srow))
                            _cm["unit_tag"] = _cm.get("unit")
                            if best.get("raw"):
                                _cm["raw"] = best.get("raw")
                            _cm["source_url"] = best.get("source_url") or best.get("url") or _cm.get("source_url") or ""
                            _cm["context_snippet"] = best.get("context_snippet") or best.get("context") or _cm.get("context_snippet") or ""
                            _cm["evidence"] = [best]
                            _cm.setdefault("diag", {})
                            if isinstance(_cm.get("diag"), dict):
                                _cm["diag"]["v29_schema_gate_replaced"] = True
                                _cm["diag"]["v29_schema_gate_prior_value_norm"] = prior_v
                                _cm["diag"]["v29_schema_gate_prior_unit"] = prior_u
                                _cm["diag"]["v29_schema_gate_best_score"] = best_score
                                _cm["diag"]["v29_schema_gate_best_raw"] = best.get("raw")
                                _cm["diag"]["v29_schema_gate_best_unit"] = best.get("unit") or best.get("unit_tag") or ""
                            canonical_for_render[_ckey] = _cm
                            _v29_schema_gate["replaced"] += 1
                            if len(_v29_schema_gate["replaced_keys_sample"]) < 12:
                                _v29_schema_gate["replaced_keys_sample"].append(str(_ckey))
                        else:
                            _v29_schema_gate["kept"] += 1
                    else:
                        _v29_schema_gate["kept"] += 1
                except Exception:
                    pass
                    continue
    except Exception:
        pass

    try:
        if isinstance(output.get("debug"), dict):
            output["debug"]["canonical_for_render_schema_gate_v29"] = _v29_schema_gate
    except Exception:
        pass


    # downstream row hydration does not overwrite Current with blanks when the rebuilt dict uses alternate fields.
    # - Derives value_norm/unit/raw from common alternate keys and evidence entries.
    # - Purely render-layer enrichment; does NOT alter selection/hashing.
    def _v22_extract_numeric(v):
        try:
            if v is None:
                return None
            if isinstance(v, (int, float)):
                return float(v)
            s = str(v).strip()
            if not s:
                return None
            # strip commas
            s2 = s.replace(",", "")
            return float(s2)
        except Exception:
            return None

    def _v22_norm_metric(cm: dict) -> dict:
        try:
            if not isinstance(cm, dict):
                return cm
            # value_norm
            vn = cm.get("value_norm")
            if vn is None:
                for k in ("value", "value_num", "value_float", "norm_value", "canonical_value_norm"):
                    if k in cm and cm.get(k) is not None:
                        vn = cm.get(k)
                        break
            # evidence-derived
            if vn is None and isinstance(cm.get("evidence"), list) and cm.get("evidence"):
                try:
                    ev0 = cm.get("evidence")[0]
                    if isinstance(ev0, dict):
                        vn = ev0.get("value_norm") if ev0.get("value_norm") is not None else ev0.get("value")
                except Exception:
                    pass
            vn_f = _v22_extract_numeric(vn)
            if vn_f is not None:
                cm["value_norm"] = vn_f

            # unit
            unit = (cm.get("unit") or cm.get("unit_tag") or cm.get("unit_label") or "").strip()
            if (not unit) and isinstance(cm.get("evidence"), list) and cm.get("evidence"):
                try:
                    ev0 = cm.get("evidence")[0]
                    if isinstance(ev0, dict):
                        unit = (ev0.get("unit") or ev0.get("unit_tag") or "").strip()
                except Exception:
                    pass
            if unit:
                cm["unit"] = unit

            # raw/display
            raw = (cm.get("raw") or cm.get("value_raw") or cm.get("raw_value") or cm.get("display") or "").strip()
            if not raw:
                try:
                    if cm.get("value_norm") is not None and (cm.get("unit") or ""):
                        raw = f"{cm.get('value_norm')} {cm.get('unit')}".strip()
                    elif cm.get("value_norm") is not None:
                        raw = str(cm.get("value_norm"))
                except Exception:
                    pass
                    raw = ""
            if raw:
                cm["raw"] = raw
            cm.setdefault("diag", {})
            if isinstance(cm.get("diag"), dict):
                cm["diag"].setdefault("v22_norm", True)
            return cm
        except Exception:
            return cm

    try:
        if isinstance(canonical_for_render, dict) and canonical_for_render:
            for _k, _m in list(canonical_for_render.items()):
                if isinstance(_m, dict):
                    canonical_for_render[_k] = _v22_norm_metric(_m)
    except Exception:
        pass


    # Goal:
    # - Apply an analysis-like schema/unit compatibility gate at render-time.
    # - Explicitly reject obviously incompatible unit evidence (e.g. "2.0 B" for a % metric,
    #   or unitless negatives like "-6441" for a unit_sales metric).
    #
    # Notes:
    # - Render-only: does not affect extraction, snapshot attach, hashing, or fastpath replay.
    # - Best-effort: only runs if FIX16 helpers are present.
    _v30_strict_gate = {"attempted": False, "dropped": 0, "dropped_keys_sample": []}
    try:
        _v30_strict_gate["attempted"] = True
        _schema = {}
        try:
            if isinstance(prev_response, dict):
                _schema = prev_response.get("metric_schema_frozen") or {}
        except Exception:
            pass
            _schema = {}

        _fix16_exp = globals().get("_fix16_expected_dimension")
        _fix16_comp = globals().get("_fix16_unit_compatible")
        _fix16_has_unit = globals().get("_fix16_candidate_has_any_unit")
        if callable(_fix16_exp) and callable(_fix16_comp) and isinstance(_schema, dict) and _schema:
            _dropped = []
            for _ck, _m in list((canonical_for_render or {}).items()):
                if not isinstance(_m, dict):
                    continue
                _defn = _schema.get(_ck) or {}
                try:
                    _expected = _fix16_exp(_defn)
                except Exception:
                    pass
                    _expected = None

                # pull unit evidence in the same style analysis expects
                _unit_tag = str(_m.get("unit_tag") or _m.get("unit") or "").strip()
                _raw = str(_m.get("raw") or _m.get("raw_value") or "").strip()
                _has_any = False
                try:
                    if callable(_fix16_has_unit):
                        _has_any = bool(_fix16_has_unit(_m))
                except Exception:
                    pass
                    _has_any = bool(_unit_tag) or ("% " in (_raw + " ") or "$" in _raw)

                _ok = True
                try:
                    _ok = bool(_fix16_comp(_expected, _unit_tag, _has_any))
                except Exception:
                    pass
                    _ok = True

                if not _ok:
                    _dropped.append(_ck)
                    try:
                        canonical_for_render.pop(_ck, None)
                    except Exception:
                        pass

            _v30_strict_gate["dropped"] = len(_dropped)
            _v30_strict_gate["dropped_keys_sample"] = list(_dropped[:12])
    except Exception:
        pass

    # Diff using existing diff helper if present, but FORCE cur_response to canonical-for-render.
    metric_changes = []
    cur_resp_for_diff = None
    try:
        fn_diff = globals().get("diff_metrics_by_name")
        if callable(fn_diff):
            cur_resp_for_diff = {"primary_metrics_canonical": canonical_for_render}
            # Ensure the diff layer receives metric_anchors for CURRENT so that
            # v34 anchor-hash secondary join can resolve drifting canonical_keys.
            # - Deterministic, inference-free: only uses anchor_hash already present
            #   on canonical_for_render entries (if any).
            try:
                _cur_metric_anchors = {}
                if isinstance(canonical_for_render, dict):
                    for _ckey, _m in canonical_for_render.items():
                        if not isinstance(_ckey, str):
                            continue
                        if not isinstance(_m, dict):
                            continue
                        _ah = _m.get("anchor_hash")
                        if not _ah and isinstance(_m.get("anchor"), dict):
                            _ah = _m.get("anchor", {}).get("anchor_hash")
                        if not _ah and isinstance(_m.get("anchor_meta"), dict):
                            _ah = _m.get("anchor_meta", {}).get("anchor_hash")
                        # Only emit when present and non-null-ish
                        if _ah and str(_ah).lower() not in ("none", "null", ""):
                            _cur_metric_anchors[_ckey] = {"anchor_hash": str(_ah)}
                if _cur_metric_anchors:
                    cur_resp_for_diff["metric_anchors"] = _cur_metric_anchors
            except Exception:
                pass


            # Signal to diff layer: when canonical-for-render is active, do NOT
            # infer/parse numeric values for CURRENT from free-form strings.
            try:
                cur_resp_for_diff["_disable_numeric_inference_v27"] = True
            except Exception:
                pass
            # When using canonical-for-render, force strict canonical_key identity matching in diff layer.
            # This prevents cross-metric substitution (e.g., 2.0 B / 170.0 / year values) from fallback matchers.
            try:
                cur_resp_for_diff["_ph2b_strict_ckey_v24"] = True
            except Exception:
                pass
            metric_changes, unchanged, increased, decreased, found = fn_diff(prev_response, cur_resp_for_diff)
            # Surface v34 join summary (if produced by diff layer) into top-level debug
            # so it appears in the Evolution JSON for audit.
            try:
                _dj = None
                if isinstance(cur_resp_for_diff, dict):
                    _dj = (cur_resp_for_diff.get("debug") or {}).get("diff_join_anchor_v34")
                if isinstance(_dj, dict):
                    try:
                        if "debug" not in output or not isinstance(output.get("debug"), dict):
                            output["debug"] = {}
                    except Exception:
                        pass
                        output["debug"] = {}
                    try:
                        output["debug"]["diff_join_anchor_v34"] = _dj
                    except Exception:
                        pass
            except Exception:
                pass


        else:
            metric_changes, unchanged, increased, decreased, found = ([], 0, 0, 0, 0)
    except Exception:
        pass
        metric_changes, unchanged, increased, decreased, found = ([], 0, 0, 0, 0)

    # Post-process diff rows: ensure "Current" fields are derived from canonical-for-render.
    _row_audit = {
        "rows_total": int(len(metric_changes or [])),
        "rows_hydrated": 0,
        "rows_missing_canonical": 0,
        "rows_skipped_missing_fields": 0,
        "rows_with_prior_current_overridden": 0,
    }
    try:
        if isinstance(metric_changes, list) and isinstance(canonical_for_render, dict):
            for row in metric_changes:
                if not isinstance(row, dict):
                    continue
                ckey = row.get("canonical_key") or row.get("canonical") or row.get("canonical_id") or ""
                if not ckey:
                    continue
                cm = canonical_for_render.get(ckey)
                if not isinstance(cm, dict):
                    _row_audit["rows_missing_canonical"] += 1
                    continue

                # Prevents overwriting a previously non-empty current with blanks when canon metric is sparse.
                _cm_vn = cm.get("value_norm")
                _cm_unit = (cm.get("unit") or cm.get("unit_tag") or "").strip()
                _cm_raw = (cm.get("raw") or cm.get("value_raw") or cm.get("raw_value") or "").strip()
                if _cm_vn is None and (not _cm_raw):
                    # no usable canonical payload to hydrate from
                    _row_audit["rows_skipped_missing_fields"] += 1
                    row.setdefault("diag", {})
                    if isinstance(row.get("diag"), dict):
                        row["diag"].setdefault("canonical_for_render_v1", {})
                        row["diag"]["canonical_for_render_v1"]["applied"] = False
                        row["diag"]["canonical_for_render_v1"]["reason"] = "skipped_missing_canonical_fields"
                        row["diag"]["canonical_for_render_v1"]["fn"] = _canonical_for_render_fn
                    continue

                # Capture prior values for audit if we are overriding
                prior = {
                    "current_value": row.get("current_value"),
                    "current_value_norm": row.get("current_value_norm"),
                    "cur_value_norm": row.get("cur_value_norm"),
                    "cur_unit_cmp": row.get("cur_unit_cmp"),
                    "current_unit": row.get("current_unit"),
                }

                # Hydrate from canonical metric
                vnorm = cm.get("value_norm")
                unit = (cm.get("unit") or cm.get("unit_tag") or "").strip()
                raw = (cm.get("raw") or cm.get("value_raw") or cm.get("raw_value") or "").strip()
                if not raw:
                    # Build a lightweight display string when raw isn't available
                    try:
                        if vnorm is not None and unit:
                            raw = f"{vnorm} {unit}".strip()
                        elif vnorm is not None:
                            raw = str(vnorm)
                    except Exception:
                        pass
                        raw = row.get("current_value") or ""

                # Apply canonical-for-render to diff row
                row["current_value_norm"] = vnorm
                row["cur_value_norm"] = vnorm
                row["current_unit"] = unit
                row["cur_unit_cmp"] = unit
                row["current_value"] = raw

                # a schema-aligned unit+value, clear any prior unit_mismatch that came from raw/fallback.
                try:
                    if (vnorm is not None) and str(unit or "").strip():
                        if row.get("unit_mismatch") is True:
                            row["unit_mismatch"] = False
                        if row.get("change_type") in ("unit_mismatch", "invalid_current"):
                            pv = row.get("previous_value")
                            pvf = _v22_extract_numeric(pv)
                            cvf = _v22_extract_numeric(vnorm)
                            if pvf is not None and cvf is not None:
                                if abs(cvf - pvf) < 1e-9:
                                    row["change_type"] = "unchanged"
                                elif cvf > pvf:
                                    row["change_type"] = "increased"
                                else:
                                    row["change_type"] = "decreased"
                except Exception:
                    pass

                # Range fields (if present in canonical)
                if isinstance(cm.get("value_range"), dict):
                    row["current_value_range"] = cm.get("value_range")
                if (cm.get("value_range_display") or "").strip():
                    row["current_value_range_display"] = cm.get("value_range_display")

                # Audit stamp
                row.setdefault("diag", {})
                if isinstance(row.get("diag"), dict):
                    row["diag"].setdefault("canonical_for_render_v1", {})
                    row["diag"]["canonical_for_render_v1"]["applied"] = True
                    row["diag"]["canonical_for_render_v1"]["fn"] = _canonical_for_render_fn
                    row["diag"]["canonical_for_render_v1"]["reason"] = _canonical_for_render_reason
                    row["diag"]["canonical_for_render_v1"]["prior_current"] = prior

                # Goal: Once a row is hydrated from canonical-for-render, lock the "Current" fields
                #       so later post-processing (heuristics/sanitizers) cannot overwrite them.
                # This is render-only and does NOT affect hashing/fastpath/snapshot attach.
                try:
                    row["_lock_current_v26"] = True
                    # ensure nested dict exists
                    row.setdefault("diag", {})
                    if isinstance(row.get("diag"), dict):
                        row["diag"].setdefault("canonical_for_render_v1", {})
                        if isinstance(row["diag"].get("canonical_for_render_v1"), dict):
                            row["diag"]["canonical_for_render_v1"]["locked_current_v26"] = {
                                "current_value": row.get("current_value"),
                                "current_value_norm": row.get("current_value_norm"),
                                "cur_value_norm": row.get("cur_value_norm"),
                                "cur_unit_cmp": row.get("cur_unit_cmp"),
                                "current_unit": row.get("current_unit"),
                                "current_value_range": row.get("current_value_range"),
                                "current_value_range_display": row.get("current_value_range_display"),
                            }
                except Exception:
                    pass


                # Determine if we actually changed the row
                if prior.get("cur_value_norm") != vnorm or str(prior.get("cur_unit_cmp") or "") != unit or str(prior.get("current_value") or "") != raw:
                    _row_audit["rows_with_prior_current_overridden"] += 1
                _row_audit["rows_hydrated"] += 1
    except Exception:
        pass

    # Attach diagnostics for auditability
    try:
        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            output["debug"]["canonical_for_render_v1"] = {
                "applied": bool(_canonical_for_render_applied),
                "reason": str(_canonical_for_render_reason or ""),
                "fn": str(_canonical_for_render_fn or ""),
                "rebuilt_count": int(_canonical_for_render_count or 0),
                "keys_sample": list(_canonical_for_render_keys_sample or []),
                "diag_ext": (lambda: {
                    "current_metrics_count": int(len(current_metrics)) if isinstance(current_metrics, dict) else 0,
                    "baseline_sources_cache_current_rows": int(len(baseline_sources_cache_current)) if isinstance(baseline_sources_cache_current, list) else 0,
                    "baseline_sources_cache_prev_rows": int(len((locals().get("baseline_sources_cache_prev") or baseline_sources_cache or []))) if isinstance((locals().get("baseline_sources_cache_prev") or baseline_sources_cache), list) else 0,
                    "has_web_context": bool(isinstance(web_context, dict)),
                    "web_context_extra_urls_count": int(len(web_context.get("extra_urls") or [])) if isinstance(web_context, dict) and isinstance(web_context.get("extra_urls"), list) else 0,
                    "web_context_fix2v_binding": (web_context.get("fix2v_candidate_binding_v1") or {}) if isinstance(web_context, dict) else {},
                    "reason_is_empty_rebuild": bool(str(_canonical_for_render_reason or "") in ("render_rebuild_failed_or_empty", "forced_render_rebuild_due_to_suspicious_existing_failed")),
                })(),
                "replaced_current_metrics_for_render": bool(_canonical_for_render_replaced_current_metrics),
            }
            # Purpose:
            #   Diff/dashboard diagnostics consume results.debug.*, but canonical_for_render_v1
            #   was attached to output.debug only. Mirror into output.results.debug to prevent
            #   false "missing_output_debug.canonical_for_render_v1" diagnoses.
            #   (Additive; no behavior change)
            try:
                if isinstance(output.get("results"), dict):
                    output["results"].setdefault("debug", {})
                    if isinstance(output["results"].get("debug"), dict):
                        output["results"]["debug"]["canonical_for_render_v1"] = dict(output["debug"].get("canonical_for_render_v1") or {})
            except Exception:
                pass


            output["debug"]["canonical_for_render_row_audit_v1"] = _row_audit
    except Exception:
        pass


    # REFACTOR56: Diff Panel V2 lastmile (downsizing)
    # Objective:
    # - Keep Diff Panel V2 as the authoritative producer of the metric changes table feed.
    # - Do not emit metric_changes_legacy (removed).
    # Safety:
    # - Render-only. No inference. No changes to hashing/fastpath/snapshots/extraction.
    _diff_v2_rows = []
    _diff_v2_summary = None
    # REFACTOR45: Diff Panel V2 hardening (RecursionError-safe)
    # - Always pass minimal, acyclic wrappers into the V2 builder.
    # - Capture traceback for any V2 builder failure.
    # - If V2 fails, DO NOT overwrite previously computed rows; otherwise
    #   synthesize strict canonical-join rows so the panel never goes empty
    #   when prev/cur canonical maps are present.
    try:
        import traceback as _tb
    except Exception:
        _tb = None

    _prev_can_v2 = {}
    _cur_can_v2 = {}

    try:
        _fn_v2 = (globals().get("build_diff_metrics_panel_v2__rows_refactor47")
                 or globals().get("build_diff_metrics_panel_v2__rows")
                 or globals().get("build_diff_metrics_panel_v2"))
        if callable(_fn_v2):
            # Previous (baseline) canonical map
            try:
                _prev_can_v2 = _diffpanel_v2__unwrap_primary_metrics_canonical(prev_response)
            except Exception:
                _prev_can_v2 = {}
            # If the unwrap returned an empty dict (a past silent failure mode), fall back.
            if (not isinstance(_prev_can_v2, dict)) or (not _prev_can_v2):
                try:
                    _prev_can_v2 = (
                        _refactor89_locate_pmc_dict(prev_response)
                        or _refactor89_locate_pmc_dict(previous_data)
                        or (prev_response.get("primary_metrics_canonical") if isinstance(prev_response, dict) else {})
                    )
                except Exception:
                    _prev_can_v2 = {}
            if not isinstance(_prev_can_v2, dict):
                _prev_can_v2 = {}

            # Make baseline failure mode explicit (diagnostic only; do not fabricate deltas)
            try:
                if isinstance(output.get("debug"), dict) and not _prev_can_v2:
                    _reason = "baseline_pmc_not_found"
                    if bool(locals().get("_prev_rehydrated")):
                        _reason += "_after_rehydrate"
                    output["debug"]["baseline_missing_reason_v1"] = str(_reason)
            except Exception:
                pass

            # Current canonical map (prefer already rebuilt current_metrics)
            try:
                if isinstance(current_metrics, dict) and current_metrics:
                    _cur_can_v2 = current_metrics
                else:
                    _cur_can_v2 = _diffpanel_v2__unwrap_primary_metrics_canonical(cur_resp_for_diff) if isinstance(cur_resp_for_diff, dict) else {}
            except Exception:
                _cur_can_v2 = {}
            if not isinstance(_cur_can_v2, dict):
                _cur_can_v2 = {}

            # Minimal wrappers (avoid passing the full evolution output dict)
            _prev_for_v2 = {"primary_metrics_canonical": dict(_prev_can_v2)}
            _cur_for_v2 = {"primary_metrics_canonical": dict(_cur_can_v2)}

            # Carry schema freeze map if present (helps unit-family expectations)
            try:
                _msf = None
                if isinstance(prev_response, dict):
                    _msf = prev_response.get("metric_schema_frozen") or (prev_response.get("results") or {}).get("metric_schema_frozen")
                if _msf is None and isinstance(output, dict):
                    _msf = output.get("metric_schema_frozen") or (output.get("results") or {}).get("metric_schema_frozen")
                if isinstance(_msf, dict) and _msf:
                    _prev_for_v2["metric_schema_frozen"] = _msf
                    _cur_for_v2["metric_schema_frozen"] = _msf
            except Exception:
                pass

            # Attach observed-number pools (source_results) for V2 "observed" promotion
            try:
                _sr = None
                if isinstance(baseline_sources_cache_current, list) and baseline_sources_cache_current:
                    _sr = baseline_sources_cache_current
                elif isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                    _sr = baseline_sources_cache
                elif isinstance(output, dict):
                    _sr = output.get("source_results") or ((output.get("results") or {}).get("source_results"))
                if isinstance(_sr, list) and _sr:
                    _cur_for_v2.setdefault("results", {})
                    if isinstance(_cur_for_v2.get("results"), dict) and (not isinstance(_cur_for_v2["results"].get("source_results"), list) or not _cur_for_v2["results"].get("source_results")):
                        _cur_for_v2["results"]["source_results"] = _sr
            except Exception:
                pass

            _diff_v2_rows, _diff_v2_summary = _fn_v2(_prev_for_v2, _cur_for_v2 or {})
    except Exception as _e:
        try:
            _dbg = output.setdefault("debug", {})
            if isinstance(_dbg, dict):
                # REFACTOR58: suppress diff_panel_v2_error emission; fallback builder handles rows.
                try:
                    if _tb is not None:
                        _dbg["diff_panel_v2_traceback"] = _tb.format_exc()
                except Exception:
                    pass
        except Exception:
            pass

        # If rows were already computed earlier in this function, preserve them.
        try:
            _existing = output.get("metric_changes_v2")
            if isinstance(_existing, list) and _existing:
                _diff_v2_rows = _existing
                _diff_v2_summary = None
            else:
                # Strict schema-complete fallback (no inference, no heuristics)
                _diff_v2_rows = []
                _prev_map = _prev_can_v2 if isinstance(_prev_can_v2, dict) else {}
                _cur_map = _cur_can_v2 if isinstance(_cur_can_v2, dict) else {}

                def _vn(_m):
                    try:
                        if isinstance(_m, dict):
                            v = _m.get("value_norm")
                            if v is None:
                                v = _m.get("value")
                            return float(v) if v is not None else None
                    except Exception:
                        return None
                    return None

                def _unit(_m):
                    try:
                        if isinstance(_m, dict):
                            return (_m.get("unit_tag") or _m.get("unit") or None)
                    except Exception:
                        return None
                    return None

                # Prefer frozen schema keys if available (guarantees schema-complete rows)
                _schema_keys = []
                try:
                    _msf = None
                    if isinstance(prev_response, dict):
                        _msf = prev_response.get("metric_schema_frozen") or (prev_response.get("results") or {}).get("metric_schema_frozen")
                    if _msf is None and isinstance(output, dict):
                        _msf = output.get("metric_schema_frozen") or (output.get("results") or {}).get("metric_schema_frozen")
                    if _msf is None and isinstance(cur_resp_for_diff, dict):
                        _msf = cur_resp_for_diff.get("metric_schema_frozen") or (cur_resp_for_diff.get("results") or {}).get("metric_schema_frozen")
                    if isinstance(_msf, dict) and _msf:
                        _schema_keys = [str(k) for k in _msf.keys()]
                    elif isinstance(_msf, list) and _msf:
                        _schema_keys = [str(x) for x in _msf if x is not None]
                    _schema_keys = sorted([k for k in _schema_keys if isinstance(k, str) and k])
                except Exception:
                    _schema_keys = []

                try:
                    if _schema_keys:
                        _iter_keys = list(_schema_keys)
                        _mode = "schema_complete_fallback"
                    else:
                        _iter_keys = sorted({str(k) for k in list(_prev_map.keys()) + list(_cur_map.keys()) if k})
                        _mode = "union_fallback"
                except Exception:
                    _iter_keys = []
                    _mode = "empty"

                for _ck in _iter_keys:
                    _pm = _prev_map.get(_ck) if isinstance(_prev_map, dict) else None
                    _cm = _cur_map.get(_ck) if isinstance(_cur_map, dict) else None
                    _pm_ok = isinstance(_pm, dict) and bool(_pm)
                    _cm_ok = isinstance(_cm, dict) and bool(_cm)

                    _pv = _vn(_pm) if _pm_ok else None
                    _cv = _vn(_cm) if _cm_ok else None
                    _pu = _unit(_pm) if _pm_ok else None
                    _cu = _unit(_cm) if _cm_ok else None

                    # For display clarity, carry known unit across when only one side is present
                    try:
                        if (not _pm_ok) and _cm_ok and str(_cu or "").strip():
                            _pu = _cu
                        if _pm_ok and (not _cm_ok) and str(_pu or "").strip():
                            _cu = _pu
                    except Exception:
                        pass

                    _d = None
                    _pct = None
                    _is_comp = False

                    if _pm_ok and _cm_ok:
                        if (_pv is not None) and (_cv is not None) and str(_pu or "").strip() and str(_cu or "").strip() and str(_pu).strip() == str(_cu).strip():
                            _is_comp = True
                            _d = float(_cv) - float(_pv)
                            if abs(float(_pv)) > 1e-12:
                                _pct = (_d / float(_pv)) * 100.0
                            if abs(_d) < 1e-9:
                                _ctype = "unchanged"
                                _d = 0.0
                            elif _d > 0:
                                _ctype = "increased"
                            else:
                                _ctype = "decreased"
                        else:
                            # Both present but not comparable (usually unit mismatch)
                            if str(_pu or "").strip() and str(_cu or "").strip() and str(_pu).strip() != str(_cu).strip():
                                _ctype = "unit_mismatch"
                            else:
                                _ctype = "found"
                    elif (not _pm_ok) and _cm_ok:
                        _ctype = "missing_baseline"
                    elif _pm_ok and (not _cm_ok):
                        _ctype = "missing_current"
                    else:
                        _ctype = "missing_both"

                    _name = None
                    try:
                        _name = ((_pm or {}).get("name") if _pm_ok else None) or ((_cm or {}).get("name") if _cm_ok else None) or _ck
                    except Exception:
                        _name = _ck

                    _src = None
                    try:
                        if _cm_ok:
                            _src = _cm.get("source_url") or _cm.get("url")
                    except Exception:
                        _src = None

                    _diff_v2_rows.append({
                        "canonical_key": _ck,
                        "name": _name,
                        "previous_value": _pv,
                        "current_value": (_cv if _cm_ok else "N/A"),
                        "previous_unit": _pu,
                        "current_unit": (_cu if _cm_ok else _cu),
                        "prev_value_norm": _pv,
                        "cur_value_norm": (_cv if _cm_ok else None),
                        "delta_abs": _d,
                        "delta_pct": _pct,
                        "change_type": _ctype,
                        "baseline_is_comparable": bool(_is_comp),
                        "current_method": "strict_schema_fallback_v2",
                        "source_url": _src,
                        "schema_frozen_key": bool(_ck in _schema_keys) if _schema_keys else False,
                    })

                _diff_v2_summary = {
                    "rows_total": int(len(_diff_v2_rows)),
                    "builder_id": "REFACTOR74_STRICT_SCHEMA_FALLBACK",
                    "mode": _mode,
                    "schema_keys_total": int(len(_schema_keys)) if _schema_keys else 0,
                }

        except Exception:
            _diff_v2_rows, _diff_v2_summary = ([], None)

    # Persist V2 artifacts for auditability
    try:
        output["metric_changes_v2"] = _diff_v2_rows or []
    except Exception:
        pass
    try:
        if isinstance(_diff_v2_summary, dict):
            _dbg = output.setdefault("debug", {})
            if isinstance(_dbg, dict):
                _dbg["diff_panel_v2_summary"] = _diff_v2_summary
    except Exception:
        pass

    # Option B: override UI feed if V2 emitted any rows
    try:
        if isinstance(_diff_v2_rows, list) and _diff_v2_rows:
            metric_changes = _diff_v2_rows
            try:
                found = int(_diff_v2_summary.get("rows_total", found)) if isinstance(_diff_v2_summary, dict) else found
            except Exception:
                pass
    except Exception:
        pass

    output["metric_changes"] = metric_changes or []
    # REFACTOR68: recompute summary + stability from the final canonical-first diff rows.
    # This prevents stale 0 counters (and stability=0) when we swap the UI feed to metric_changes_v2.
    try:
        _refactor13_recompute_summary_and_stability_v1(output)
    except Exception:
        pass
    output["source_results"] = baseline_sources_cache[:50]
    output["sources_checked"] = len(baseline_sources_cache)
    output["sources_fetched"] = len(baseline_sources_cache)

    try:
        total_nums = 0
        for sr in baseline_sources_cache or []:
            if isinstance(sr, dict) and isinstance(sr.get("extracted_numbers"), list):
                total_nums += len(sr.get("extracted_numbers") or [])
        output["numbers_extracted_total"] = int(total_nums)
    except Exception:
        pass

    output["message"] = "Source-anchored evolution completed (snapshot-gated, analysis-aligned)."
    output["interpretation"] = "Evolution used cached source snapshots only; no brute-force candidate harvesting."

    # - If a diff row shows a year-like integer as current for a unit-required metric,
    #   emit a compact trace: origin, schema unit_family, current fields, and top candidates.
    try:
        if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
            bad_traces = {}
            # Build a flattened candidate pool once (from snapshots only)
            flat = []
            for sr in baseline_sources_cache or []:
                if isinstance(sr, dict):
                    for c in (sr.get("extracted_numbers") or []):
                        if isinstance(c, dict):
                            flat.append(c)

            def _is_yearlike(v):
                try:
                    iv = int(float(v))
                    return 1900 <= iv <= 2100
                except Exception:
                    return False

            def _schema_unit_required(md: dict, ckey: str = "") -> bool:
                uf = ((md or {}).get("unit_family") or (md or {}).get("unit") or "").strip().lower()
                if uf in {"currency", "percent", "rate", "ratio"}:
                    return True
                ck = (ckey or "").lower().strip()
                return ck.endswith("__currency") or ck.endswith("__percent") or ck.endswith("__rate") or ck.endswith("__ratio")

            def _cand_unit_evidence(c: dict) -> bool:
                if not isinstance(c, dict):
                    return False
                if (c.get("unit_tag") or c.get("unit") or c.get("unit_norm") or c.get("unit_raw") or "").strip():
                    return True
                if (c.get("currency") or c.get("currency_symbol") or "").strip():
                    return True
                if c.get("is_percent") or c.get("has_percent"):
                    return True
                if (c.get("base_unit") or "").strip():
                    return True
                if (c.get("unit_family") or "").strip():
                    return True
                if isinstance(c.get("unit_tokens"), list) and c.get("unit_tokens"):
                    return True
                return False

            schema = {}
            try:
                if isinstance(previous_data, dict):
                    pr = previous_data.get("primary_response") if isinstance(previous_data.get("primary_response"), dict) else previous_data
                    schema = (pr.get("metric_schema_frozen") or {}) if isinstance(pr, dict) else {}
            except Exception:
                pass
                schema = {}

            for row in output.get("metric_changes") or []:
                try:
                    ckey = row.get("canonical_key") or row.get("canonical") or ""
                    md = schema.get(ckey) if isinstance(schema, dict) else None
                    if not _schema_unit_required(md or {}, ckey):
                        continue

                    cur_val = row.get("current_value_norm")
                    cur_unit = (row.get("cur_unit_cmp") or row.get("current_unit") or "").strip()
                    if cur_val is None:
                        continue
                    if not _is_yearlike(cur_val):
                        continue
                    if cur_unit:
                        continue

                    kws = []
                    if isinstance(md, dict):
                        kws = md.get("keywords") or md.get("keyword_hints") or []
                    kws = [str(k).lower() for k in kws if str(k).strip()]

                    def _hit_score(c):
                        ctx = (c.get("context") or c.get("window") or c.get("context_window") or "").lower()
                        score = 0
                        for k in kws[:25]:
                            if k and k in ctx:
                                score += 1
                        if _cand_unit_evidence(c):
                            score += 5
                        if _is_yearlike(c.get("value_norm")) and not _cand_unit_evidence(c):
                            score -= 5
                        return score

                    top = sorted(flat, key=_hit_score, reverse=True)[:10]
                    bad_traces[ckey or row.get("name") or "unknown_metric"] = {
                        "current_value_norm": cur_val,
                        "cur_unit_cmp": cur_unit,
                        "schema_unit_family": (md or {}).get("unit_family") if isinstance(md, dict) else "",
                        "origin": output["debug"]["fix35"].get("current_metrics_origin", "unknown"),
                        "top_candidates": [
                            {
                                "raw": t.get("raw"),
                                "value_norm": t.get("value_norm"),
                                "unit_tag": t.get("unit_tag"),
                                "unit_family": t.get("unit_family"),
                                "base_unit": t.get("base_unit"),
                                "has_unit_evidence": bool(_cand_unit_evidence(t)),
                                "anchor_hash": t.get("anchor_hash"),
                            }
                            for t in top
                        ],
                    }
                except Exception:
                    pass
                    continue

            if bad_traces:
                output["debug"]["fix35"]["bad_current_traces"] = bad_traces
                output["debug"]["fix35"]["bad_current_trace_count"] = len(bad_traces)
    except Exception:
        pass

    # - Mirrors to output.results.debug.inj_trace_v1 for a fixed location across modes
    # - Does NOT affect fastpath decisioning
    try:
        _wc_diag = {}
        if isinstance(web_context, dict):
            _wc_diag = web_context.get("diag_injected_urls") or {}
        _hash_inputs = _inj_diag_hash_inputs_from_bsc(baseline_sources_cache)

        # Determine path from existing fix35 origin stamp
        _path = ""
        try:
            origin = ""
            if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                origin = str(output["debug"]["fix35"].get("current_metrics_origin") or "")
            if "fastpath" in origin:
                _path = "fastpath"
            elif "rebuild" in origin:
                _path = "rebuild"
            else:
                _path = "unknown"
        except Exception:
            pass
            _path = "unknown"

        # Rebuild "selected" URLs: unique source_url from current metrics (if present)
        _selected = []
        try:
            cm = output.get("current_metrics")
            if isinstance(cm, dict):
                for v in cm.values():
                    if isinstance(v, dict):
                        u = (v.get("source_url") or "").strip()
                        if u:
                            _selected.append(u)
            _selected = sorted(set(_selected))
        except Exception:
            pass
            _selected = []

        # For evolution, rebuild_pool is effectively the hash input URL universe available via snapshots
        # - If injected URLs exist but are not in hash_inputs, we record the most likely reason:
        #     * excluded_by_flag_default_off  (when inclusion switch is OFF)
        #     * missing_from_hash_inputs      (when switch ON but still absent)
        _evo_hash_reasons = {}
        try:
            _evo_persisted = []
            if isinstance(_wc_diag, dict):
                _evo_persisted = _inj_diag_norm_url_list(_wc_diag.get("persisted_norm") or _wc_diag.get("persisted") or [])
            _evo_inj = _inj_diag_norm_url_list(
                (_wc_diag.get("intake_norm") if isinstance(_wc_diag, dict) else []) or
                (_wc_diag.get("ui_norm") if isinstance(_wc_diag, dict) else []) or
                []
            )
            _evo_targets = _evo_persisted or _evo_inj
            _hashset = set(_inj_diag_norm_url_list(_hash_inputs or []))
            _incl = _inj_hash_policy_should_include(_evo_targets)
            for u in _evo_targets:
                if u in _hashset:
                    _evo_hash_reasons[u] = "present_in_hash_inputs"
                else:
                    _evo_hash_reasons[u] = ("excluded_by_policy_disable" if _inj_hash_policy_explicit_disable() else ("excluded_by_legacy_switch_default_off" if not _incl else "missing_from_hash_inputs"))
        except Exception:
            pass
            _evo_hash_reasons = {}

        # Goal:
        # - Evolution often bypasses fetch_web_context(), so "admitted" may be unset even
        #   when URLs are actually in the current scrape/hash universe.
        # - This patch makes inj_trace_v1 "admitted_norm" reflect the same practical
        #   universe used for scraping/hashing (without changing any control flow).
        #
        # Policy (diagnostics only):
        # - If diag.admitted is empty but hash_inputs are present, treat hash_inputs as
        #   admitted for trace purposes.
        # - Prefer any explicit FIX24 evo merge set if present (urls_after_merge_norm).
        try:
            if isinstance(_wc_diag, dict):
                _ad = _inj_diag_norm_url_list(_wc_diag.get("admitted") or _wc_diag.get("extra_urls_admitted") or [])
                if not _ad:
                    _pref = []
                    try:
                        _pref = _inj_diag_norm_url_list(_wc_diag.get("urls_after_merge_norm") or [])
                    except Exception:
                        pass
                        _pref = []
                    if not _pref:
                        _pref = _inj_diag_norm_url_list(_hash_inputs or [])
                    if _pref:
                        _wc_diag["admitted"] = list(_pref)
                        _wc_diag.setdefault("admission_reason", "trace_fallback_to_hash_inputs_or_urls_after_merge")
        except Exception:
            pass

        # Goal:
        # - In fastpath/replay or when current_metrics lacks source_url fields,
        #   rebuild_selected_norm can be empty, creating misleading pool_minus_selected.
        #
        # Diagnostics-only fallback:
        # - If rebuild_selected is empty but rebuild_pool/hash_inputs exists, treat
        #   selected as the full pool for trace purposes.
        try:
            if (not _selected) and _hash_inputs:
                _selected = list(_inj_diag_norm_url_list(_hash_inputs))
                if isinstance(_wc_diag, dict):
                    _wc_diag.setdefault("rebuild_selected_reason", "trace_fallback_to_hash_inputs_no_current_metric_sources")
        except Exception:
            pass


        #
        # Why:
        # - inj_trace_v1 shows injected URLs at intake but missing from admitted (unknown_rejected_pre_admission).
        # - We must "pin" injection at the admission boundary for evolution (delta-only), and emit a post-fetch trace
        #   because inj_trace_v1 may be emitted before the fetch/persist stage completes.
        #
        # What:
        # - If injected URLs are present (from web_context.extra_urls OR diag ui fields) we force-add them into
        #   _wc_diag["admitted"] so the trace reflects admission override deterministically (delta-only).
        # - Additionally, emit inj_trace_v2_postfetch using best-effort enrichment from scraped_meta / cur_bsc if available.
        #
        # Safety:
        # - Purely additive; never raises; does not modify fastpath rules or hashing.
        try:
            _fx12_wc = web_context if isinstance(web_context, dict) else {}
            _fx12_diag = _wc_diag if isinstance(locals().get("_wc_diag"), dict) else (_fx12_wc.get("diag_injected_urls") if isinstance(_fx12_wc.get("diag_injected_urls"), dict) else {})
            _fx12_inj_raw = []
            try:
                _fx12_inj_raw = list(_fx12_wc.get("extra_urls") or [])
            except Exception:
                pass
                _fx12_inj_raw = []
            if not _fx12_inj_raw and isinstance(_fx12_diag, dict):
                try:
                    _fx12_inj_raw = list(_fx12_diag.get("ui_norm") or _fx12_diag.get("intake_norm") or [])
                except Exception:
                    pass
                    _fx12_inj_raw = []
            _fx12_inj = _inj_diag_norm_url_list(_fx12_inj_raw or [])
            if _fx12_inj and isinstance(_wc_diag, dict):
                _fx12_prev_ad = _inj_diag_norm_url_list(_wc_diag.get("admitted") or [])
                _fx12_forced = sorted(list(set(_fx12_inj) - set(_fx12_prev_ad)))
                if _fx12_forced:
                    _wc_diag["admitted"] = list(_inj_diag_stable_dedupe_order((_fx12_prev_ad or []) + _fx12_forced))
                    _wc_diag.setdefault("forced_admit_reasons", {})
                    if isinstance(_wc_diag.get("forced_admit_reasons"), dict):
                        for _u in _fx12_forced:
                            _wc_diag["forced_admit_reasons"][_u] = "forced_admit_injected_url_override"
                    try:
                        output.setdefault("debug", {})
                        if isinstance(output.get("debug"), dict):
                            output["debug"].setdefault("fix41afc12", {})
                            if isinstance(output["debug"].get("fix41afc12"), dict):
                                output["debug"]["fix41afc12"].update({
                                    "forced_admit_injected_count": int(len(_fx12_forced)),
                                    "forced_admit_injected_urls": list(_fx12_forced),
                                })
                    except Exception:
                        pass
        except Exception:
            pass
        _trace = _inj_trace_v1_build(
            diag_injected_urls=_wc_diag if isinstance(_wc_diag, dict) else {},
            hash_inputs=_hash_inputs,
            stage="evolution",
            path=_path,
            rebuild_pool=_hash_inputs,
            rebuild_selected=_selected,
            hash_exclusion_reasons=_evo_hash_reasons,
        )

        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            output["debug"]["inj_trace_v1"] = _trace

        # Fixed location mirror: results.debug.inj_trace_v1
        output.setdefault("results", {})
        if isinstance(output.get("results"), dict):
            output["results"].setdefault("debug", {})
            if isinstance(output["results"].get("debug"), dict):
                output["results"]["debug"]["inj_trace_v1"] = _trace

                #
                # Emit a second trace after best-effort enrichment from scraped_meta / baseline cache so that
                # attempted/persisted deltas reflect the true post-fetch state (inj_trace_v1 may be earlier).
                try:
                    _fx12_diag2 = dict(_wc_diag) if isinstance(_wc_diag, dict) else {}
                    try:
                        _sm = locals().get("scraped_meta")
                        if isinstance(_sm, dict):
                            _fx12_diag2 = _inj_trace_v1_enrich_diag_from_scraped_meta(_fx12_diag2, _sm, (_inj_extra_urls or []))
                    except Exception:
                        pass
                    try:
                        _cb = locals().get("cur_bsc") or locals().get("baseline_sources_cache") or locals().get("baseline_sources_cache_current")
                        if isinstance(_cb, list):
                            _fx12_diag2 = _inj_trace_v1_enrich_diag_from_bsc(_fx12_diag2, _cb)
                    except Exception:
                        pass
                    _trace2 = _inj_trace_v1_build(
                        diag_injected_urls=_fx12_diag2 if isinstance(_fx12_diag2, dict) else {},
                        hash_inputs=_hash_inputs,
                        stage="evolution",
                        path=str(_path or "evolution") + "_postfetch",
                        rebuild_pool=_hash_inputs,
                        rebuild_selected=_selected,
                        hash_exclusion_reasons=_evo_hash_reasons,
                    )
                    output["debug"]["inj_trace_v2_postfetch"] = _trace2
                    if isinstance(output.get("results"), dict) and isinstance(output["results"].get("debug"), dict):
                        output["results"]["debug"]["inj_trace_v2_postfetch"] = _trace2
                except Exception:
                    pass


    except Exception:
        pass

    #
    # Why:
    # - Conclusively identify which structure the Evolution dashboard reads for
    #   the "Current" column, and whether upstream patches are modifying that
    #   exact structure.
    #
    # What:
    # - Emit a compact debug payload that:
    #     * states the dashboard read-path ("results.metric_changes[].current_value")
    #     * samples the first N metric_changes rows (canonical_key, current_value, unit hints, diag keys)
    # - Purely additive: no selection, hashing, or fastpath behavior changes.
    try:
        if isinstance(output, dict):
            _dbg = output.get("debug") if isinstance(output.get("debug"), dict) else {}
            if not isinstance(_dbg, dict):
                _dbg = {}
            rows = output.get("metric_changes") or []
            #
            # Why:
            # - Even after schema-only rebuild hardening, dashboard "Current" may still
            #   be hydrated from metric_changes rows that pull values from multiple
            #   paths (canonical, observed, legacy). We must prevent unitless bare
            #   years (e.g., 2024, 2030, including 2030.0) from appearing as metric
            #   values for non-year metrics.
            #
            # What:
            # - Post-process metric_changes in-place before the dashboard consumes it.
            # - If current value is yearlike AND unit is empty AND metric is not a
            #   year-as-value metric, blank it (N/A) and attach diagnostics.
            # - Keep a compact trace in output.debug.fix2d24_yearlike_current_trace_v1.
            def _fix2d24_is_yearlike(v, raw=None):
                try:
                    if v is None:
                        return False
                    fv = float(v)
                    if fv < 1900.0 or fv > 2100.0:
                        return False
                    if abs(fv - round(fv)) > 1e-9:
                        return False
                    rs = str(raw if raw is not None else v).strip()
                    rs = rs.replace(",", "")
                    if re.match(r"^\d{4}(?:\.0+)?$", rs):
                        return True
                    # allow e.g. '2030.0' in string form
                    if re.match(r"^\d{4}\.0+$", rs):
                        return True
                    return True
                except Exception:
                    return False

            def _fix2d24_metric_expects_year(ckey, name=None):
                try:
                    ck = str(ckey or "").lower()
                    nm = str(name or "").lower()
                    if ck.endswith("__year"):
                        return True
                    if "year" in ck and ("__" in ck):
                        # conservative: treat explicit year unit tags as year metrics
                        if "__year" in ck:
                            return True
                    if nm.strip() == "year":
                        return True
                    return False
                except Exception:
                    return False

            _fix2d24_filtered = 0
            _fix2d24_samples = []
            if isinstance(rows, list):
                for _r in rows:
                    if not isinstance(_r, dict):
                        continue
                    _ckey = _r.get("canonical_key")
                    _nm = _r.get("metric") or _r.get("name")
                    _unit = _r.get("cur_unit_cmp")
                    if _unit is None:
                        _unit = _r.get("current_unit")
                    if _unit is None:
                        _unit = _r.get("unit")
                    _unit = str(_unit or "").strip()
                    _cvn = _r.get("current_value_norm")
                    if _cvn is None:
                        _cvn = _r.get("cur_value_norm")
                    _raw = _r.get("current_value")
                    if _raw is None:
                        _raw = _r.get("cur_raw")

                    if _fix2d24_is_yearlike(_cvn, _raw) and (not _unit) and (not _fix2d24_metric_expects_year(_ckey, _nm)):
                        _fix2d24_filtered += 1
                        if len(_fix2d24_samples) < 25:
                            _fix2d24_samples.append({
                                "canonical_key": _ckey,
                                "name": _nm,
                                "blocked_value_norm": _cvn,
                                "blocked_raw": _raw,
                                "source_url": _r.get("source_url") or _r.get("cur_source_url"),
                            })
                        _r.setdefault("diag", {})
                        if isinstance(_r.get("diag"), dict):
                            _r["diag"]["fix2d24_yearlike_current_blocked"] = {
                                "blocked": True,
                                "value_norm": _cvn,
                                "raw": _raw,
                                "unit": _unit,
                            }
                        # blank current for dashboard consumption
                        _r["current_value"] = "N/A"
                        _r["current_value_norm"] = None
                        _r["cur_value_norm"] = None
                        # Why:
                        # - FIX2D24 correctly blocks unitless yearlike current values (e.g. 2024/2030)
                        #   but previously left Current as N/A. This patch immediately falls back to the
                        #   guarded inference pool and commits a binding current value into metric_changes.
                        # What:
                        # - Build a shared extracted-number pool from baseline_sources_cache(_current)
                        # - Score candidates with unit-family + keyword/context hints
                        # - Commit into current_value/current_value_norm/current_source/current_method
                        # - Attach explicit trace yearlike_current_blocked_then_inferred_v1
                        try:
                            _r_diag = _r.get('diag') if isinstance(_r.get('diag'), dict) else {}
                            _blocked_vn = _cvn
                            _blocked_raw = _raw
                            # lazily build pool once
                            if '_fix2d2l_pool_v1' not in locals():
                                _fix2d2l_pool_v1 = []
                                def _fix2d2l_add_from_bsc(_bsc_list):
                                    try:
                                        if not isinstance(_bsc_list, list):
                                            return
                                        for _src in _bsc_list:
                                            if not isinstance(_src, dict):
                                                continue
                                            _surl = _src.get('source_url') or _src.get('url')
                                            _nums = _src.get('extracted_numbers')
                                            if not isinstance(_nums, list):
                                                continue
                                            for _n in _nums:
                                                if not isinstance(_n, dict):
                                                    continue
                                                _vn = _n.get('value_norm')
                                                _rw = _n.get('raw')
                                                _ut = str(_n.get('unit_tag') or '').strip()
                                                _uf = str(_n.get('unit_family') or '').strip()
                                                _ctx = _n.get('context_snippet') or _n.get('context') or ''
                                                # derive minimal unit_family if missing
                                                if not _uf:
                                                    if _ut == '%':
                                                        _uf = 'percent'
                                                    elif _ut in {'USD','US$','$','EUR','€','GBP','£','SGD','S$','JPY','¥','CNY','RMB','CN¥'}:
                                                        _uf = 'currency'
                                                    elif _ut in {'M','B','T','K'}:
                                                        _uf = 'magnitude'
                                                _fix2d2l_pool_v1.append({
                                                    'value_norm': _vn,
                                                    'raw': _rw,
                                                    'unit_tag': _ut,
                                                    'unit_family': _uf,
                                                    'source_url': (_n.get('source_url') or _surl),
                                                    'context_snippet': _ctx,
                                                })
                                    except Exception:
                                        return
                                # collect from common locations
                                _fix2d2l_add_from_bsc(output.get('baseline_sources_cache_current'))
                                _fix2d2l_add_from_bsc(output.get('baseline_sources_cache'))
                                if isinstance(output.get('results'), dict):
                                    _fix2d2l_add_from_bsc(output['results'].get('baseline_sources_cache_current'))
                                    _fix2d2l_add_from_bsc(output['results'].get('baseline_sources_cache'))
                            _pool = locals().get('_fix2d2l_pool_v1') or []
                            # determine desired unit_family from row hints
                            _desired_uf = ''
                            try:
                                _hint_u = str(_r.get('prev_unit_cmp') or _r.get('unit') or _r.get('cur_unit_cmp') or '').strip().lower()
                                _hint_name = str(_nm or '').lower()
                                if '%' in _hint_u or 'percent' in _hint_u or 'share' in _hint_name:
                                    _desired_uf = 'percent'
                                elif any(x in _hint_u for x in ['usd','us$','$','eur','€','gbp','£','sgd','s$','jpy','¥','cny','rmb']):
                                    _desired_uf = 'currency'
                                elif any(x in _hint_name for x in ['sales','units','deliver','ship','vehicle']):
                                    _desired_uf = 'magnitude'
                            except Exception:
                                pass
                                _desired_uf = ''
                            # keyword set for context scoring
                            _kw = set()
                            try:
                                for tok in re.split(r"[^a-z0-9]+", str(_nm or _ckey or '').lower()):
                                    if len(tok) >= 4 and tok not in {'global','total','market','share','sales','units','value','year'}:
                                        _kw.add(tok)
                            except Exception:
                                pass
                                _kw = set()
                            # injected-first preference (FIX2D2M)
                            _inj_urls = set()
                            try:
                                # Prefer explicit injected sources when present in caches
                                def _fix2d2m_scan_injected(bsc):
                                    if not isinstance(bsc, list):
                                        return
                                    for it in bsc:
                                        try:
                                            if not isinstance(it, dict):
                                                continue
                                            if it.get('injected') is True:
                                                su = str(it.get('source_url') or '').strip()
                                                if su:
                                                    _inj_urls.add(su)
                                        except Exception:
                                            pass
                                            continue
                                _fix2d2m_scan_injected(output.get('baseline_sources_cache_current'))
                                _fix2d2m_scan_injected(output.get('baseline_sources_cache'))
                                if isinstance(output.get('results'), dict):
                                    _fix2d2m_scan_injected(output['results'].get('baseline_sources_cache_current'))
                                    _fix2d2m_scan_injected(output['results'].get('baseline_sources_cache'))
                            except Exception:
                                pass
                                _inj_urls = set()

                            def _fix2d2m_score_pool(_pool_in):
                                _sc = []
                                for _cand in _pool_in:
                                    try:
                                        if not isinstance(_cand, dict):
                                            continue
                                        _vn = _cand.get('value_norm')
                                        _rw = _cand.get('raw')
                                        if _fix2d24_is_yearlike(_vn, _rw):
                                            continue
                                        _uf = str(_cand.get('unit_family') or '').strip()
                                        _ut = str(_cand.get('unit_tag') or '').strip()
                                        _ctx = str(_cand.get('context_snippet') or '').lower()
                                        _score = 0.0
                                        if _desired_uf and _uf == _desired_uf:
                                            _score += 10.0
                                        if _desired_uf == 'percent' and ('%' in _ctx or _ut == '%'):
                                            _score += 3.0
                                        if _desired_uf == 'magnitude' and ('million' in _ctx or 'units' in _ctx):
                                            _score += 2.0
                                        if _desired_uf == 'currency' and any(x in _ctx for x in ['$', 'us$', 'usd', 'eur', '€', 'gbp', '£']):
                                            _score += 2.0
                                        _row_surl = str(_r.get('source_url') or _r.get('cur_source_url') or '').strip()
                                        if _row_surl and str(_cand.get('source_url') or '').strip() == _row_surl:
                                            _score += 2.0
                                        if _kw:
                                            _hits = sum(1 for k in _kw if k in _ctx)
                                            _score += min(6.0, 1.5 * _hits)
                                        _sc.append((_score, _cand))
                                    except Exception:
                                        pass
                                        continue
                                _sc.sort(key=lambda t: (t[0] if t and len(t)>0 else 0), reverse=True)
                                return _sc

                            # two-pass selection: injected-only then global
                            _pool_injected = []
                            if _inj_urls:
                                _pool_injected = [c for c in _pool if isinstance(c, dict) and str(c.get('source_url') or '').strip() in _inj_urls]
                            _scored_inj = _fix2d2m_score_pool(_pool_injected) if _pool_injected else []
                            _sel = _scored_inj[0][1] if _scored_inj and _scored_inj[0][0] >= 3.0 else None
                            _used_injected_pass = bool(_sel is not None)
                            _scored = _scored_inj
                            if _sel is None:
                                _scored = _fix2d2m_score_pool(_pool)
                                _sel = _scored[0][1] if _scored and _scored[0][0] >= 3.0 else None
                            _committed = False
                            _reason = 'no_eligible_candidates' if not _scored else ('score_below_threshold' if _sel is None else 'selected')
                            if isinstance(_sel, dict):
                                # commit into metric_changes row (UI-read fields)
                                _r['current_value_norm'] = _sel.get('value_norm')
                                _r['cur_value_norm'] = _sel.get('value_norm')
                                _r['current_value'] = (_sel.get('raw') if _sel.get('raw') not in (None, '') else str(_sel.get('value_norm')))
                                _r['current_source'] = _sel.get('source_url')
                                _r['current_method'] = 'yearlike_blocked_then_inferred'
                                # comparable only if prev exists
                                _pv = _r.get('previous_value_norm')
                                if _pv is None:
                                    _pv = _r.get('prev_value_norm')
                                _r['baseline_is_comparable'] = bool(_pv is not None and _r.get('current_value_norm') is not None)
                                _committed = True
                            # trace
                            _r.setdefault('diag', {})
                            if isinstance(_r.get('diag'), dict):
                                _r['diag']['yearlike_current_blocked_then_inferred_v1'] = {
                                    'blocked_value_norm': _blocked_vn,
                                    'blocked_raw': _blocked_raw,
                                    'desired_unit_family': _desired_uf or None,
                                    'pool_size': int(len(_pool)) if isinstance(_pool, list) else None,
                                    'eligible_scored': int(len(_scored)),
                                    'selected': bool(_sel is not None),
                                    'selected_value_norm': (_sel.get('value_norm') if isinstance(_sel, dict) else None),
                                    'selected_raw': (_sel.get('raw') if isinstance(_sel, dict) else None),
                                    'selected_unit_family': (_sel.get('unit_family') if isinstance(_sel, dict) else None),
                                    'selected_unit_tag': (_sel.get('unit_tag') if isinstance(_sel, dict) else None),
                                    'selected_source_url': (_sel.get('source_url') if isinstance(_sel, dict) else None),
                                    'committed': bool(_committed),
                                    'reason': _reason,
                                    'top3': [
                                        {
                                            'score': float(sc),
                                            'value_norm': c.get('value_norm') if isinstance(c, dict) else None,
                                            'raw': c.get('raw') if isinstance(c, dict) else None,
                                            'unit_family': c.get('unit_family') if isinstance(c, dict) else None,
                                            'unit_tag': c.get('unit_tag') if isinstance(c, dict) else None,
                                            'source_url': c.get('source_url') if isinstance(c, dict) else None,
                                        }
                                        for sc, c in (_scored[:3] if isinstance(_scored, list) else [])
                                    ],
                                }
                            # injected preference trace
                            try:
                                _r['diag'].setdefault('injected_source_preference_trace_v1', {})
                                _r['diag']['injected_source_preference_trace_v1'] = {
                                    'injected_present': bool(_inj_urls),
                                    'injected_urls': sorted(list(_inj_urls))[:3] if _inj_urls else [],
                                    'pass1_injected_pool_size': int(len(_pool_injected)) if isinstance(_pool_injected, list) else 0,
                                    'pass1_selected': bool(_used_injected_pass),
                                    'fallback_used': bool((not _used_injected_pass) and (_sel is not None)),
                                    'selected_source_url': (_sel.get('source_url') if isinstance(_sel, dict) else None),
                                }
                            except Exception:
                                pass

                            # aggregate
                            try:
                                _dbg.setdefault('fix2d2l_yearlike_block_fallback_trace_v1', {})
                                _agg = _dbg.get('fix2d2l_yearlike_block_fallback_trace_v1')
                                if not isinstance(_agg, dict):
                                    _agg = {}
                                _agg['fallback_attempts'] = int(_agg.get('fallback_attempts', 0)) + 1
                                if _committed:
                                    _agg['fallback_commits'] = int(_agg.get('fallback_commits', 0)) + 1
                                _agg.setdefault('samples', [])
                                if isinstance(_agg.get('samples'), list) and len(_agg['samples']) < 10:
                                    _agg['samples'].append({
                                        'canonical_key': _ckey,
                                        'blocked_value_norm': _blocked_vn,
                                        'committed': bool(_committed),
                                        'selected_value_norm': (_sel.get('value_norm') if isinstance(_sel, dict) else None),
                                        'selected_source_url': (_sel.get('source_url') if isinstance(_sel, dict) else None),
                                        'reason': _reason,
                                    })
                                _dbg['fix2d2l_yearlike_block_fallback_trace_v1'] = _agg
                                output['debug'] = _dbg
                            except Exception:
                                pass
                        except Exception:
                            pass

            # attach trace
            try:
                _dbg = output.get("debug") if isinstance(output.get("debug"), dict) else {}
                if not isinstance(_dbg, dict):
                    _dbg = {}
                _dbg.setdefault("fix2d24_yearlike_current_trace_v1", {})
                _dbg["fix2d24_yearlike_current_trace_v1"] = {
                    "filtered_count": int(_fix2d24_filtered),
                    "samples": _fix2d24_samples,
                    "note": "Blocks unitless yearlike current values at metric_changes hydration",
                }
                output["debug"] = _dbg
            except Exception:
                pass

            _sample = []
            if isinstance(rows, list):
                for _r in rows[:25]:
                    if not isinstance(_r, dict):
                        continue
                    _diag = _r.get("diag") if isinstance(_r.get("diag"), dict) else {}
                    _sample.append({
                        "canonical_key": _r.get("canonical_key"),
                        "name": _r.get("metric") or _r.get("name"),
                        "current_value": _r.get("current_value"),
                        "current_value_norm": (_r.get("current_value_norm") if _r.get("current_value_norm") is not None else _r.get("cur_value_norm")),
                        "cur_unit_cmp": (_r.get("cur_unit_cmp") if _r.get("cur_unit_cmp") is not None else _r.get("current_unit")),
                        "anchor_used": _r.get("anchor_used"),
                        "unit_mismatch": _r.get("unit_mismatch"),
                        "diag_keys": (list(_diag.keys()) if isinstance(_diag, dict) else []),
                    })
            _dbg["dashboard_current_source_v25"] = {
                "dashboard_reads": "results.metric_changes[].current_value",
                "rows_sample_n": len(_sample),
                "rows_sample": _sample,
            }
            # REFACTOR12: canonical_for_render debug block must be present (avoid stale "missing" diagnostics)
            try:
                _cfr = _dbg.get("canonical_for_render_v1")
                if (not isinstance(_cfr, dict)) or (not _cfr):
                    _dbg["canonical_for_render_v1"] = {
                        "present": True,
                        "code_version": _yureeka_get_code_version(),
            "authority_manifest_v1": _yureeka_authority_manifest_v1(),
                        "note": "REFACTOR12 standardized debug block (was intermittently missing)",
                    }
            except Exception:
                pass
            _dbg["canonical_for_render_present_v25"] = bool(_dbg.get("canonical_for_render_v1"))

            # Why:
            # - We sometimes observe canonical_for_render_present_v25 == False and/or
            #   missing bound canonical entities in the dashboard, even when injection
            #   extracted numbers exist.
            #
            # What:
            # - Emit a single compact debug object that explains, deterministically,
            #   why canonical_for_render did not run / did not apply / did not produce
            #   candidates / did not change any rows.
            # - Purely additive; does not change selection, hashing, or rendering.
            try:
                _cfr_dbg = _dbg.get("canonical_for_render_v1") if isinstance(_dbg.get("canonical_for_render_v1"), dict) else None

                # Row-level diag aggregation (if present)
                _rows = None
                try:
                    _rows = output.get("metric_changes") if isinstance(output, dict) else None
                except Exception:
                    pass
                    _rows = None

                _row_diag_present = 0
                _row_diag_applied_true = 0
                _row_diag_applied_false = 0
                _row_diag_reasons = {}
                _row_diag_reason_samples = []

                if isinstance(_rows, list):
                    for _r in _rows[:250]:
                        if not isinstance(_r, dict):
                            continue
                        _d = _r.get("diag") if isinstance(_r.get("diag"), dict) else {}
                        _c = _d.get("canonical_for_render_v1") if isinstance(_d.get("canonical_for_render_v1"), dict) else None
                        if not isinstance(_c, dict):
                            continue
                        _row_diag_present += 1
                        _ap = _c.get("applied")
                        if _ap is True:
                            _row_diag_applied_true += 1
                        elif _ap is False:
                            _row_diag_applied_false += 1
                        _rsn = str(_c.get("reason") or "")
                        if _rsn:
                            _row_diag_reasons[_rsn] = int(_row_diag_reasons.get(_rsn, 0)) + 1
                            if len(_row_diag_reason_samples) < 12:
                                _row_diag_reason_samples.append({
                                    "canonical_key": _r.get("canonical_key"),
                                    "reason": _rsn,
                                    "fn": str(_c.get("fn") or ""),
                                })

                # Determine the most precise reason we can provide
                _reason = ""
                _reason_detail = {}

                if not isinstance(_cfr_dbg, dict):
                    _reason = "missing_output_debug.canonical_for_render_v1"
                    _reason_detail = {
                        "has_output_debug": bool(isinstance(_dbg, dict)),
                        "row_diag_present": int(_row_diag_present),
                        "row_diag_applied_true": int(_row_diag_applied_true),
                        "row_diag_applied_false": int(_row_diag_applied_false),
                    }
                else:
                    _applied = bool(_cfr_dbg.get("applied"))
                    _rb = int(_cfr_dbg.get("rebuilt_count") or 0)
                    _rsn = str(_cfr_dbg.get("reason") or "")
                    if not _applied:
                        _reason = "canonical_for_render_not_applied"
                        _reason_detail = {
                            "canonical_for_render_reason": _rsn,
                            "rebuilt_count": _rb,
                        }
                    elif _applied and _rb <= 0:
                        _reason = "canonical_for_render_applied_but_empty"
                        _reason_detail = {
                            "canonical_for_render_reason": _rsn,
                            "rebuilt_count": _rb,
                        }
                    elif _applied and _rb > 0 and _row_diag_applied_true == 0:
                        _reason = "canonical_for_render_rebuilt_but_no_rows_hydrated"
                        _reason_detail = {
                            "canonical_for_render_reason": _rsn,
                            "rebuilt_count": _rb,
                        }
                    else:
                        _reason = "canonical_for_render_present"
                        _reason_detail = {
                            "canonical_for_render_reason": _rsn,
                            "rebuilt_count": _rb,
                        }

                _dbg["canonical_for_render_diagnosis_fix2ac_v1"] = {
                    "reason": _reason,
                    "reason_detail": _reason_detail,
                    "canonical_for_render_present_v25": bool(_dbg.get("canonical_for_render_v25") or _dbg.get("canonical_for_render_present_v25")),
                    "row_diag_present": int(_row_diag_present),
                    "row_diag_applied_true": int(_row_diag_applied_true),
                    "row_diag_applied_false": int(_row_diag_applied_false),
                    "row_diag_reason_counts": _row_diag_reasons,
                    "row_diag_reason_samples": _row_diag_reason_samples,
                    "has_canonical_for_render_v1": bool(isinstance(_cfr_dbg, dict)),
                    "canonical_for_render_v1_summary": (
                        {
                            "applied": bool(_cfr_dbg.get("applied")),
                            "reason": str(_cfr_dbg.get("reason") or ""),
                            "fn": str(_cfr_dbg.get("fn") or ""),
                            "rebuilt_count": int(_cfr_dbg.get("rebuilt_count") or 0),
                            "keys_sample": list(_cfr_dbg.get("keys_sample") or [])[:10],
                        }
                        if isinstance(_cfr_dbg, dict) else {}
                    ),
                }
            except Exception:
                pass

            # Surface which response-shape path diff used to hydrate "current".
            try:
                _paths = None
                try:
                    _paths = cur_response.get("_diff_panel_canonical_paths_v33") if isinstance(cur_response, dict) else None
                except Exception:
                    pass
                    _paths = None
                if isinstance(_paths, dict) and _paths:
                    _dbg["diff_panel_canonical_paths_v33"] = _paths
            except Exception:
                pass

            output["debug"] = _dbg
    except Exception:
        pass


    # If any later code overwrote current_* fields, restore the locked canonical-for-render
    # fields right before returning the evolution output.
    try:
        _lock_dbg = {"rows_total": 0, "rows_locked": 0, "rows_restored": 0, "rows_missing_lock": 0, "restored_keys_sample": []}
        for _r in (output.get("metric_changes") or []):
            if not isinstance(_r, dict):
                continue
            _lock_dbg["rows_total"] += 1
            if not _r.get("_lock_current_v26"):
                continue
            _lock_dbg["rows_locked"] += 1
            _d = _r.get("diag") if isinstance(_r.get("diag"), dict) else {}
            _lc = None
            try:
                if isinstance(_d, dict):
                    _cfr = _d.get("canonical_for_render_v1") if isinstance(_d.get("canonical_for_render_v1"), dict) else {}
                    _lc = _cfr.get("locked_current_v26") if isinstance(_cfr, dict) else None
            except Exception:
                pass
                _lc = None
            if not isinstance(_lc, dict):
                _lock_dbg["rows_missing_lock"] += 1
                continue
            # If current fields differ from locked, restore
            _changed = False
            for _k in ("current_value", "current_value_norm", "cur_value_norm", "cur_unit_cmp", "current_unit",
                       "current_value_range", "current_value_range_display"):
                if _k in _lc:
                    if _r.get(_k) != _lc.get(_k):
                        _r[_k] = _lc.get(_k)
                        _changed = True
            if _changed:
                _lock_dbg["rows_restored"] += 1
                ck = _r.get("canonical_key") or _r.get("canonical") or ""
                if ck and len(_lock_dbg["restored_keys_sample"]) < 20:
                    _lock_dbg["restored_keys_sample"].append(str(ck))
        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            output["debug"]["lock_current_v26"] = _lock_dbg
    except Exception:
        pass


    # REFACTOR56: enforce removal of legacy metric_changes output (safety rail)
    try:
        output.pop("metric_changes_legacy", None)
    except Exception:
        pass

    _fix2d20_trace_year_like_commits(output, stage=str((output or {}).get('debug',{}).get('stage') or 'evolution'), callsite='compute_source_anchored_diff_return')


    # REFACTOR66: De-duplicate nested output['results'] mirror (footprint control)
    #
    # Problem:
    # - Some legacy flows attach a nested 'results' dict that re-includes heavy fields
    #   such as baseline_sources_cache / baseline_sources_cache_current / primary_response.
    # - This duplicates large payload segments in Evolution JSON and can aggravate
    #   Sheets cell limits / local snapshot store bloat.
    #
    # Behavior:
    # - Preserve backward compatibility by keeping output['results'] as a lightweight
    #   stub containing only small, commonly expected fields.
    # - Never remove top-level authoritative fields.
    #
    # Safety:
    # - Purely a payload-shape cleanup; does not change diffing, schema, or key grammar.
    try:
        _nested = output.get("results")
        if isinstance(_nested, dict) and _nested:
            # Promote a few lightweight fields if they exist only under nested results
            for _k in ("run_delta_seconds", "run_delta_human", "code_version"):
                try:
                    if _k in _nested and _k not in output:
                        output[_k] = _nested.get(_k)
                except Exception:
                    pass

            # Build a lightweight compatibility mirror (avoid duplicating heavy caches)
            _light = {}
            try:
                _light["code_version"] = str(output.get("code_version") or globals().get("_YUREEKA_CODE_VERSION_LOCK") or "")
            except Exception:
                pass
            for _k in ("run_delta_seconds", "run_delta_human"):
                try:
                    if _k in output:
                        _light[_k] = output.get(_k)
                except Exception:
                    pass

            # Optionally keep a small primary_metrics_canonical mirror (usually tiny: 4 keys)
            try:
                _pmc = output.get("primary_metrics_canonical")
                if isinstance(_pmc, dict) and _pmc:
                    _light["primary_metrics_canonical"] = _pmc
            except Exception:
                pass

            # Keep a minimal debug marker for older consumers
            try:
                _d = output.get("debug") if isinstance(output.get("debug"), dict) else {}
                if isinstance(_d, dict) and _d:
                    _light["debug"] = {"__exec_code_version": _d.get("__exec_code_version")}
            except Exception:
                pass

            output["results"] = _light
    except Exception:
        pass
    # END REFACTOR66


    # REFACTOR70: Metric-changes + stability output bridge (safety rail)
    #
    # Why:
    # - During controlled downsizing, minor nesting changes can cause the UI/export
    #   path to miss the authoritative metric_changes list.
    #
    # What:
    # - Enforce a single authoritative metric_changes list at output["metric_changes"].
    # - Mirror to output["metric_changes_v2"] for compatibility.
    # - Mirror stability_score to top-level when present.
    # - Never emit metric_changes_legacy.
    try:
        if isinstance(output, dict):
            _mc = None
            if isinstance(output.get("metric_changes"), list):
                _mc = output.get("metric_changes")
            elif isinstance(output.get("metric_changes_v2"), list):
                _mc = output.get("metric_changes_v2")
            else:
                _r = output.get("results")
                if isinstance(_r, dict):
                    if isinstance(_r.get("metric_changes"), list):
                        _mc = _r.get("metric_changes")
                    elif isinstance(_r.get("metric_changes_v2"), list):
                        _mc = _r.get("metric_changes_v2")
            if _mc is None:
                _mc = []
            # Ensure lists, and make metric_changes authoritative
            if not isinstance(_mc, list):
                try:
                    _mc = list(_mc)
                except Exception:
                    _mc = []
            output["metric_changes"] = _mc
            output["metric_changes_v2"] = _mc

            # Hard-remove legacy feed
            output.pop("metric_changes_legacy", None)
            try:
                _r = output.get("results")
                if isinstance(_r, dict):
                    _r.pop("metric_changes_legacy", None)
            except Exception:
                pass

            # Stability score mirror (clamped to [0, 100])
            _ss = None
            if isinstance(output.get("stability_score"), (int, float)):
                _ss = float(output.get("stability_score"))
            else:
                _r = output.get("results")
                if isinstance(_r, dict) and isinstance(_r.get("stability_score"), (int, float)):
                    _ss = float(_r.get("stability_score"))
            if _ss is not None:
                if _ss < 0:
                    _ss = 0.0
                if _ss > 100:
                    _ss = 100.0
                output["stability_score"] = round(_ss, 1)
    except Exception:
        pass


    # Why:
    # - External source flakiness (e.g., failed:no_text) can silently reduce
    #   primary_metrics_canonical_count below the frozen schema size.
    # - This patch stamps a deterministic debug warning with missing keys
    #   and source failure summaries so the harness can't "silently degrade".
    try:
        _schema_frozen = None
        try:
            _schema_frozen = _first_present(previous_data or {}, [
                ["metric_schema_frozen"],
                ["results", "metric_schema_frozen"],
                ["primary_response", "metric_schema_frozen"],
                ["results", "primary_response", "metric_schema_frozen"],
            ], default=None)
        except Exception:
            _schema_frozen = None
        if _schema_frozen is None:
            _schema_frozen = output.get("metric_schema_frozen")

        _schema_keys = []
        if isinstance(_schema_frozen, dict):
            _schema_keys = list(_schema_frozen.keys())
        elif isinstance(_schema_frozen, list):
            _schema_keys = [str(x) for x in _schema_frozen if x is not None]

        # baseline (prev) canonical metrics keys
        _prev_pmc = _first_present(previous_data or {}, [
            ["primary_metrics_canonical"],
            ["results", "primary_metrics_canonical"],
            ["primary_response", "primary_metrics_canonical"],
            ["results", "primary_response", "primary_metrics_canonical"],
        ], default={}) or {}
        _prev_keys = set(_prev_pmc.keys()) if isinstance(_prev_pmc, dict) else set()

        # current (cur) canonical metrics keys
        _cur_pmc = output.get("primary_metrics_canonical") or _get_nested(output, ["results", "primary_metrics_canonical"], default={}) or {}
        _cur_keys = set(_cur_pmc.keys()) if isinstance(_cur_pmc, dict) else set()

        _missing_prev = []
        _missing_cur = []
        if _schema_keys:
            _missing_prev = [k for k in _schema_keys if k not in _prev_keys]
            _missing_cur = [k for k in _schema_keys if k not in _cur_keys]

        def _summ_failures(_lst):
            _out = []
            if not isinstance(_lst, list):
                return _out
            for _e in _lst:
                if not isinstance(_e, dict):
                    continue
                _url = _e.get("url") or _e.get("source_url") or _e.get("source") or ""
                _st = str(_e.get("status") or "")
                _sd = str(_e.get("status_detail") or "")
                # mark anything explicitly failed, or any status_detail that starts with "failed"
                _is_fail = False
                if _st and _st.lower() in ("failed", "error", "timeout", "blocked"):
                    _is_fail = True
                if _sd and _sd.lower().startswith("failed"):
                    _is_fail = True
                if _is_fail:
                    _out.append({"url": _url, "status": _st, "status_detail": _sd, "numbers_found": _e.get("numbers_found")})
            return _out


        def _summ_fallbacks(_lst):
            _out = []
            if not isinstance(_lst, list):
                return _out
            for _e in _lst:
                if not isinstance(_e, dict):
                    continue
                _url = _e.get("url") or _e.get("source_url") or _e.get("source") or ""
                _st = str(_e.get("status") or "")
                _sd = str(_e.get("status_detail") or "")
                _fb = bool(_e.get("fallback_used") or _e.get("reused_snapshot"))
                if (_sd and _sd.lower().startswith("fallback")) or _fb:
                    _out.append({"url": _url, "status": _st, "status_detail": _sd, "numbers_found": _e.get("numbers_found")})
            return _out

        _prev_cache = _first_present(previous_data or {}, [
            ["baseline_sources_cache"],
            ["results", "baseline_sources_cache"],
            ["primary_response", "baseline_sources_cache"],
            ["results", "primary_response", "baseline_sources_cache"],
        ], default=None)
        _cur_sources = output.get("source_results") or output.get("baseline_sources_cache_current") or []

        _prev_failures = _summ_failures(_prev_cache)
        _cur_failures = _summ_failures(_cur_sources)
        _prev_fallbacks = _summ_fallbacks(_prev_cache)
        _cur_fallbacks = _summ_fallbacks(_cur_sources)


        _inv = {
            "schema_frozen_key_count": len(_schema_keys),
            "baseline_key_count": len(_prev_keys),
            "current_key_count": len(_cur_keys),
            "missing_baseline_keys": _missing_prev,
            "missing_current_keys": _missing_cur,
            "baseline_source_failures": _prev_failures,
            "current_source_failures": _cur_failures,
            "baseline_source_fallbacks": _prev_fallbacks,
            "current_source_fallbacks": _cur_fallbacks,
        }
        # REFACTOR74: Diff row count vs schema size (completeness-first invariant)
        _rows_for_cnt = output.get("metric_changes")
        if not isinstance(_rows_for_cnt, list) or not _rows_for_cnt:
            _rows_for_cnt = output.get("metric_changes_v2")
        _row_count = int(len(_rows_for_cnt)) if isinstance(_rows_for_cnt, list) else 0
        _expected_rows = int(len(_schema_keys)) if _schema_keys else 0
        try:
            _inv["metric_changes_row_count"] = _row_count
            _inv["metric_changes_expected_row_count"] = _expected_rows
            _inv["metric_changes_row_count_matches_schema"] = (bool(_expected_rows and (_row_count == _expected_rows)) if _expected_rows else None)
        except Exception:
            pass

        # REFACTOR76: Schema-key coverage + change_type integrity (warning-only invariant)
        try:
            _row_keys = []
            if isinstance(_rows_for_cnt, list):
                for _r in _rows_for_cnt:
                    if not isinstance(_r, dict):
                        continue
                    _ck = _r.get("canonical_key") or _r.get("canonical_metric_key") or _r.get("key")
                    if isinstance(_ck, str) and _ck:
                        _row_keys.append(_ck)
            _row_key_set = set(_row_keys)
            _dups = []
            try:
                from collections import Counter as _Counter
                _c = _Counter(_row_keys)
                _dups = [k for k, v in _c.items() if int(v) > 1]
            except Exception:
                _dups = []
            _schema_key_set = set(_schema_keys or [])
            _missing_rows = [k for k in (_schema_keys or []) if k not in _row_key_set] if _schema_keys else []
            _extra_rows = [k for k in _row_key_set if (_schema_keys and (k not in _schema_key_set))] if _schema_keys else []
            _inv["metric_changes_schema_missing_keys"] = _missing_rows
            _inv["metric_changes_schema_extra_keys"] = sorted(list(_extra_rows))[:100] if isinstance(_extra_rows, list) else []
            _inv["metric_changes_schema_duplicate_keys"] = _dups
            _inv["metric_changes_schema_coverage_ok"] = (bool(_schema_keys) and (not _missing_rows) and (not _extra_rows) and (not _dups)) if _schema_keys else None

            _row_by_key = {}
            if isinstance(_rows_for_cnt, list):
                for _r in _rows_for_cnt:
                    if not isinstance(_r, dict):
                        continue
                    _ck = _r.get("canonical_key") or _r.get("canonical_metric_key") or _r.get("key")
                    if isinstance(_ck, str) and _ck and (_ck not in _row_by_key):
                        _row_by_key[_ck] = _r

            _ct_mismatches = []
            if _schema_keys:
                for _k in (_missing_prev or []):
                    if _k in _cur_keys:
                        _row = _row_by_key.get(_k) or {}
                        _ct = str(_row.get("change_type") or "")
                        if _ct not in ("missing_baseline", "new_metric"):
                            _ct_mismatches.append({"key": _k, "expected": "missing_baseline", "got": _ct})
                for _k in (_missing_cur or []):
                    if _k in _prev_keys:
                        _row = _row_by_key.get(_k) or {}
                        _ct = str(_row.get("change_type") or "")
                        if _ct not in ("missing_current",):
                            _ct_mismatches.append({"key": _k, "expected": "missing_current", "got": _ct})

            _inv["metric_changes_change_type_mismatches"] = _ct_mismatches[:50]
            _inv["metric_changes_change_type_ok"] = (len(_ct_mismatches) == 0) if _schema_keys else None
        except Exception:
            pass


        # REFACTOR78: Version-stamp self-check vs patch tracker (warning-only)
        try:
            _cv = str(output.get("code_version") or _yureeka_get_code_version() or "")
            _latest = None
            _maxn = None
            _pt = globals().get("PATCH_TRACKER_V1")
            if isinstance(_pt, list):
                import re as _re
                for _e in _pt:
                    if not isinstance(_e, dict):
                        continue
                    _pid = str(_e.get("patch_id") or "")
                    _m = _re.match(r"REFACTOR(\d+)$", _pid)
                    if _m:
                        try:
                            _n = int(_m.group(1))
                        except Exception:
                            continue
                        if (_maxn is None) or (_n > _maxn):
                            _maxn = _n
                            _latest = _pid
            _inv["code_version"] = _cv
            _inv["patch_tracker_latest_refactor"] = _latest
            _inv["code_version_matches_patch_tracker_latest"] = ((_cv == _latest) if (_cv and _latest) else None)
        except Exception:
            pass


        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            output["debug"]["harness_invariants_v1"] = _inv

        # Add a short banner string for UI visibility (non-breaking additive field)
        _parts = []
        if (_missing_prev or _missing_cur or _prev_failures or _cur_failures or _prev_fallbacks or _cur_fallbacks) and _schema_keys:
            if _missing_prev:
                _parts.append(f"baseline_missing={len(_missing_prev)}/{len(_schema_keys)}")
            if _missing_cur:
                _parts.append(f"current_missing={len(_missing_cur)}/{len(_schema_keys)}")
            if _prev_failures:
                _parts.append(f"baseline_failures={len(_prev_failures)}")
            if _cur_failures:
                _parts.append(f"current_failures={len(_cur_failures)}")
            if _prev_fallbacks:
                _parts.append(f"baseline_fallbacks={len(_prev_fallbacks)}")
            if _cur_fallbacks:
                _parts.append(f"current_fallbacks={len(_cur_fallbacks)}")

        # REFACTOR78: Surface version mismatch banner (if any)
        try:
            _cv = str(_inv.get("code_version") or "")
            _latest = str(_inv.get("patch_tracker_latest_refactor") or "")
            if _cv and _latest and (_cv != _latest):
                _parts.append(f"version_mismatch={_cv}!={_latest}")
        except Exception:
            pass


        # Always surface a diff-row count mismatch against frozen schema keys (if any).
        try:
            _rc = int(_inv.get("metric_changes_row_count") or 0) if isinstance(_inv, dict) else 0
            _er = int(_inv.get("metric_changes_expected_row_count") or 0) if isinstance(_inv, dict) else 0
            if _schema_keys and _er and (_rc != _er):
                _parts.append(f"row_count_mismatch={_rc}/{_er}")
        except Exception:
            pass

        if _parts:
            output["harness_warning_v1"] = " | ".join(_parts)
    except Exception:
        pass

    # REFACTOR119: summarize year-anchor gating + seeded URL coverage (diagnostic only; additive)
    try:
        _seeded_set = set()
        try:
            for _sr in (baseline_sources_cache or []):
                if not isinstance(_sr, dict):
                    continue
                if not bool(_sr.get("seeded")):
                    continue
                _su = _sr.get("source_url") or _sr.get("url")
                if isinstance(_su, str) and _su.strip():
                    _seeded_set.add(_refactor26_norm_url_for_compare_v1(_su.strip()) or _su.strip())
        except Exception:
            _seeded_set = set()

        _pmc_for_cov = None
        try:
            _pmc_for_cov = _refactor89_locate_pmc_dict(output)
        except Exception:
            _pmc_for_cov = None
        if not (isinstance(_pmc_for_cov, dict) and _pmc_for_cov):
            try:
                _pmc_for_cov = output.get("primary_metrics_canonical")
            except Exception:
                _pmc_for_cov = None

        _cov = {}
        if isinstance(_pmc_for_cov, dict):
            for _k, _m in (_pmc_for_cov or {}).items():
                if not isinstance(_k, str) or not _k:
                    continue
                if not isinstance(_m, dict):
                    continue
                _prov = _m.get("provenance") if isinstance(_m.get("provenance"), dict) else {}
                _ya = _prov.get("selection_year_anchor_v1") if isinstance(_prov.get("selection_year_anchor_v1"), dict) else {}
                _top3 = _ya.get("top3") if isinstance(_ya.get("top3"), list) else []
                _candidates = int(len(_top3))
                _year_ok = 0
                _seeded_in_top3 = 0
                _top_url = ""
                for _i, _t in enumerate(_top3):
                    if not isinstance(_t, dict):
                        continue
                    if _i == 0:
                        try:
                            _top_url = str(_t.get("url") or "")
                        except Exception:
                            _top_url = ""
                    if bool(_t.get("year_ok")):
                        _year_ok += 1
                    _tu = _t.get("url")
                    if isinstance(_tu, str) and _tu.strip():
                        _nu = _refactor26_norm_url_for_compare_v1(_tu.strip()) or _tu.strip()
                        if _nu in _seeded_set:
                            _seeded_in_top3 += 1
                _cov[_k] = {
                    "candidates": _candidates,
                    "year_ok_count": _year_ok,
                    "top_url": _top_url,
                    "seeded_in_top3": _seeded_in_top3,
                }
                try:
                    if "required_year" in _ya:
                        _cov[_k]["required_year"] = _ya.get("required_year")
                except Exception:
                    pass
                try:
                    if "required_year_tokens" in _ya:
                        _cov[_k]["required_year_tokens"] = _ya.get("required_year_tokens")
                except Exception:
                    pass

        if _cov:
            output.setdefault("debug", {})
            if isinstance(output.get("debug"), dict):
                output["debug"]["year_anchor_seed_coverage_v1"] = _cov
    except Exception:
        pass

    return output


def extract_context_keywords(metric_name: str) -> List[str]:
    """
    General-purpose keyword extraction for matching metric names to page contexts.

    Goals:
    - Work for ANY topic (not tourism-specific)
    - Keep deterministic behavior
    - Extract years/quarters, key financial/stat terms, and meaningful tokens
    """
    if not metric_name:
        return []

    name = str(metric_name)
    n = name.lower()

    keywords: List[str] = []

    # Years (e.g., 2019, 2024)
    years = re.findall(r"\b(19\d{2}|20\d{2})\b", name)
    keywords.extend(years)

    # Quarters / time buckets
    q = re.findall(r"\bq[1-4]\b", n)
    keywords.extend([x.upper() for x in q])

    # Common metric concepts (broad, cross-industry)
    concept_phrases = [
        "market size", "revenue", "sales", "turnover", "profit", "operating profit",
        "ebit", "ebitda", "net income", "gross margin", "margin",
        "growth", "yoy", "cagr", "share", "penetration",
        "forecast", "projected", "projection", "estimate", "expected",
        "actual", "baseline", "target",
        "volume", "units", "shipments", "users", "subscribers", "visitors",
        "price", "asp", "arpu", "aov",
        "inflation", "gdp", "unemployment", "interest rate"
    ]
    for p in concept_phrases:
        if p in n:
            keywords.append(p)

    # Units / scales that help matching
    unit_hints = ["trillion", "billion", "million", "thousand", "%", "percent"]
    for u in unit_hints:
        if u in n:
            keywords.append(u)

    # Tokenize remaining meaningful words
    tokens = re.findall(r"[a-z0-9]+", n)
    stop = {
        "the","and","or","of","in","to","for","by","from","with","on","at","as",
        "total","overall","average","avg","number","rate","value","amount",
        "annual","year","years","monthly","month","daily","day","quarter","quarters"
    }
    for t in tokens:
        if t in stop:
            continue
        if len(t) <= 2:
            continue
        keywords.append(t)

    # De-dup, keep stable ordering
    seen = set()
    out = []
    for k in keywords:
        if k and k not in seen:
            seen.add(k)
            out.append(k)

    return out[:30]

def extract_numbers_with_context(text, source_url: str = "", max_results: int = 350):
    """
    Extract numeric candidates with context windows (analysis-aligned, hardened).

    Fixes / tightening:
    - ALWAYS returns a list (never None)  ✅ critical for snapshots & evolution
    - Strips HTML tags/scripts/styles if HTML-like
    - Nav/chrome/junk rejection (analytics, cookie banners, menus, footers, etc.)
    - Suppress year-only candidates (e.g., "2024") unless clearly a metric
    - Suppress ID-like long integers, phone-like patterns, DOI/ISBN-like contexts
    - Captures currency + scale + percent + common magnitude suffixes
    - Adds anchor_hash for stable matching
    """
    import re
    import hashlib

    if not text or not str(text).strip():
        return []

    raw = str(text)

    def _sha1(s: str) -> str:
        return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()

    def _normalize_unit(u: str) -> str:
        u = (u or "").strip()
        if not u:
            return ""
        ul = u.lower().replace(" ", "")

        # Energy units (must come before magnitude)
        if "twh" in ul:
            return "TWh"
        if "gwh" in ul:
            return "GWh"
        if "mwh" in ul:
            return "MWh"
        if "kwh" in ul:
            return "kWh"
        if ul == "wh":
            return "Wh"

        # Magnitudes (case-insensitive; fix: accept single-letter suffixes)
        if ul in ("bn", "billion", "b"):
            return "B"
        if ul in ("mn", "mio", "million", "m"):
            return "M"
        if ul in ("k", "thousand", "000"):
            return "K"
        if ul in ("trillion", "tn", "t"):
            return "T"

        if ul in ("pct", "percent", "%"):
            return "%"

        return u

    def _looks_html(s: str) -> bool:
        sl = s.lower()
        return ("<html" in sl) or ("<div" in sl) or ("<p" in sl) or ("<script" in sl) or ("</" in sl)

    def _html_to_text(s: str) -> str:
        # Prefer BeautifulSoup if available
        try:
            from bs4 import BeautifulSoup  # type: ignore
            soup = BeautifulSoup(s, "html.parser")
            for tag in soup(["script", "style", "noscript", "svg", "canvas", "iframe", "header", "footer", "nav", "form"]):
                try:
                    tag.decompose()
                except Exception:
                    pass
            txt = soup.get_text(separator=" ", strip=True)
            txt = re.sub(r"\s+", " ", txt).strip()
            return txt
        except Exception:
            pass
            # fallback: cheap strip
            s2 = re.sub(r"(?is)<(script|style|noscript).*?>.*?</\1>", " ", s)
            s2 = re.sub(r"(?is)<[^>]+>", " ", s2)
            s2 = re.sub(r"\s+", " ", s2).strip()
            return s2

    def _is_phone_like(ctx: str, rawnum: str) -> bool:
        # strict phone pattern or phone keywords nearby
        if re.search(r"\b\d{3}-\d{3}-\d{4}\b", rawnum):
            return True
        c = (ctx or "").lower()
        if any(k in c for k in ["call", "phone", "tel:", "telephone", "contact us", "whatsapp"]):
            if re.search(r"\b\d{7,}\b", rawnum):
                return True
        return False

    def _is_id_like(val_str: str, ctx: str) -> bool:
        # very long digit strings typically IDs, unless explicitly monetary with symbols
        digits = re.sub(r"\D", "", val_str or "")
        if len(digits) >= 13:
            c = (ctx or "").lower()
            if any(k in c for k in ["isbn", "doi", "issn", "arxiv", "repec", "id:", "order", "invoice", "reference"]):
                return True
            # generic ID-like (too many digits)
            return True
        return False

    def _chrome_junk(ctx: str) -> bool:
        c = (ctx or "").lower()
        # common site chrome / analytics / cookie / nav junk
        bad = [
            "googleanalyticsobject", "gtag(", "googletagmanager", "analytics", "doubleclick",
            "cookie", "consent", "privacy", "terms", "copyright", "all rights reserved",
            "subscribe", "newsletter", "sign in", "login", "menu", "search", "breadcrumb",
            "share this", "follow us", "social media", "footer", "header", "nav", "sitemap"
        ]
        if any(b in c for b in bad):
            return True
        # css/js-like
        if any(b in c for b in ["function(", "var ", "const ", "let ", "webpack", "sourcemappingurl", ".css", "{", "};"]):
            return True
        # low alpha ratio
        if len(c) > 80:
            letters = sum(ch.isalpha() for ch in c)
            if letters / max(1, len(c)) < 0.18:
                return True
        return False

    def _year_only_suppression(num: float, unit: str, rawnum: str, ctx: str) -> bool:
        # suppress standalone 4-digit years like 2024 with no unit/currency
        if unit:
            return False
        s = (rawnum or "").strip()
        if re.fullmatch(r"\d{4}", s):
            year = int(s)
            if 1900 <= year <= 2099:
                c = (ctx or "").lower()
                allow_kw = ["cagr", "growth", "inflation", "gdp", "revenue", "market", "sales", "shipments", "capacity"]
                if not any(k in c for k in allow_kw):
                    return True
        return False

    # Do this AFTER HTML->text and BEFORE regex extraction.

    if _looks_html(raw):
        raw = _html_to_text(raw)

    # cap huge pages
    raw = raw[:250_000]

    raw = re.sub(r"\b((?:19|20)\d)\s+(\d)\b", r"\1\2", raw)

    def _is_year_range_context(ctx: str) -> bool:
        return bool(re.search(r"\b(19|20)\d{2}\s*(?:-|–|—|to)\s*(19|20)\d{2}\b", ctx or "", flags=re.I))

    # - We DO NOT filter here; we tag and downstream excludes by default.
    def _junk_tag(value: float, unit: str, raw_disp: str, ctx: str):
        """
        Non-destructive junk classifier.
        Returns (is_junk: bool, reason: str).
        """
        c = (ctx or "").lower()
        u = (unit or "").strip()

        try:
            iv = int(float(value))
            if u == "" and 1900 <= iv <= 2099 and _is_year_range_context(ctx):
                return True, "year_range"
        except Exception:
            pass

        nav_hits = [
            "skip to content", "menu", "search", "login", "sign in", "sign up",
            "subscribe", "newsletter", "cookie", "privacy", "terms", "copyright",
            "all rights reserved", "back to top", "next", "previous", "page ",
            "home", "about", "contact", "sitemap", "breadcrumb"
        ]
        if any(h in c for h in nav_hits):
            try:
                if u == "" and abs(float(value)) <= 20:
                    return True, "nav_small_int"
            except Exception:
                pass

        if u == "":
            try:
                if abs(float(value)) <= 12:
                    if any(h in c for h in ["•", "–", "step", "chapter", "section", "item", "no."]):
                        return True, "enumeration_small_int"
            except Exception:
                pass

        if u == "":
            try:
                iv = int(abs(float(value)))
                if 190 <= iv <= 209:
                    if any(x in (raw_disp or "") for x in ["202", "203", "204", "205", "206", "207", "208", "209"]):
                        return True, "year_fragment_3digit"
            except Exception:
                return False, ""

    # NOTE: moved OUTSIDE the loop for determinism + speed (no behavioral change).
    # Also emits a "measure_assoc" label that downstream can display easily.
    def _classify_measure(unit_tag: str, ctx: str):
        """
        Returns (measure_kind, measure_assoc):
          - measure_kind: stable internal tag (share_pct / growth_pct / count_units / money / etc.)
          - measure_assoc: human-meaning label ("share", "growth", "units", "money", "energy", etc.)
        """
        c = (ctx or "").lower()
        ut = (unit_tag or "").strip()

        if ut == "%":
            if any(k in c for k in ["market share", "share of", "share", "penetration", "portion", "contribution"]):
                return "share_pct", "share"
            if any(k in c for k in ["growth", "cagr", "increase", "decrease", "yoy", "mom", "qoq", "rate"]):
                return "growth_pct", "growth"
            return "percent_other", "percent"

        if ut in ("K", "M", "B", "T", ""):
            if any(k in c for k in ["units", "unit", "vehicles", "cars", "sold", "sales volume", "shipments", "deliveries", "registrations"]):
                return "count_units", "units"
            if any(k in c for k in ["revenue", "sales ($", "usd", "$", "market size", "valuation", "turnover"]):
                return "money", "money"
            return "magnitude_other", "magnitude"

        if ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
            return "energy", "energy"

        return "other", "other"

    # - Fix US$ being parsed as S$ by matching US\$ first.
    # - Also accept "US$" as a single token (case-insensitive).
    pat = re.compile(
        r"(US\$|US\$(?!\w)|S\$|\$|USD|SGD|EUR|€|GBP|£)?\s*"
        # - We'll still allow negatives generally, but we'll tag the special "2025-2030" case below.
        # (No behavior change for real negatives like -1.2% etc.)
        r"(-?\d{1,3}(?:[,\s]\d{3})*(?:\.\d+)?|-?\d+(?:\.\d+)?)(?!\d)\s*"
        # - Keep your A5 safeguard: single-letter magnitudes only match if NOT followed by a letter.
        r"(TWh|GWh|MWh|kWh|Wh|tn|(?:T|B|M|K)(?![A-Za-z])|trillion|billion|million|bn|mn|%|percent)?",
        flags=re.I
    )

    out = []
    for m in pat.finditer(raw):
        cur = (m.group(1) or "").strip()
        num_s = (m.group(2) or "").strip()
        unit_s = (m.group(3) or "").strip()

        if not num_s:
            continue

        start = max(0, m.start() - 160)
        end = min(len(raw), m.end() + 160)
        ctx = raw[start:end].replace("\n", " ")
        ctx = re.sub(r"\s+", " ", ctx).strip()
        ctx_store = ctx[:240]

        # numeric parse
        try:
            val = float(num_s.replace(",", "").replace(" ", ""))
        except Exception:
            pass
            continue

        # normalize unit
        unit = _normalize_unit(unit_s)

        raw_disp = f"{cur} {num_s}{unit_s}".strip()
        raw_num_only = (cur + num_s).strip()

        if _chrome_junk(ctx_store):
            continue
        if _is_phone_like(ctx_store, raw_disp):
            continue
        if _is_id_like(raw_disp, ctx_store):
            continue
        if _year_only_suppression(val, unit, num_s, ctx_store):
            continue

        # Example: "CAGR 2025-2030" producing "-2030"
        # - Do NOT drop here (keep non-destructive policy); just tag.
        neg_from_hyphen_range = False
        neg_year_from_range = False
        try:
            if num_s.startswith("-") and m.start() > 0 and raw[m.start() - 1].isdigit():
                neg_from_hyphen_range = True
                iv = int(abs(float(val)))
                if 1900 <= iv <= 2099:
                    neg_year_from_range = True
        except Exception:
            pass
            neg_from_hyphen_range = False
            neg_year_from_range = False

        anchor_hash = _sha1(f"{source_url}|{raw_disp}|{ctx_store}")
        # FIX2D69B: defensive tuple normalization (prevent unpack None)
        try:
            _jt = _junk_tag(val, unit, raw_disp, ctx_store)
            if isinstance(_jt, tuple) and len(_jt) == 2:
                is_junk, junk_reason = _jt
            else:
                is_junk, junk_reason = (False, "")
        except Exception:
            is_junk, junk_reason = (False, "")

        if neg_year_from_range:
            is_junk = True
            junk_reason = "year_range_negative_endpoint"
        elif neg_from_hyphen_range:
            is_junk = True
            junk_reason = "hyphen_range_negative_endpoint"


            # Why:
            # - Years (e.g., 2025) frequently appear in headings/ranges and should not
            #   compete with real metric values (currency, %, volumes) in evolution.
            # - We keep years only if there is strong metric context nearby.
            # Rules:
            # - If value is an integer-like 4-digit year in [1900..2100],
            #   unit is empty, and context lacks currency/%/magnitude cues => mark junk.
            try:
                if (not is_junk) and (not str(unit or "").strip()):
                    _v_int = None
                    try:
                        _v_int = int(float(val)) if val is not None else None
                    except Exception:
                        pass
                        _v_int = None

                    if _v_int is not None and 1900 <= _v_int <= 2100:
                        # Treat 4-digit years as non-metric tokens; keep but mark as junk.
                        is_junk = True
                        if not junk_reason:
                            junk_reason = "year_token"
            except Exception:
                pass
# semantic association tags
        # FIX2D69B: defensive tuple normalization (prevent unpack None)
        try:
            _cm = _classify_measure(unit, ctx_store)
            if isinstance(_cm, tuple) and len(_cm) == 2:
                measure_kind, measure_assoc = _cm
            else:
                measure_kind, measure_assoc = ("other", "other")
        except Exception:
            measure_kind, measure_assoc = ("other", "other")

        out.append({
            "value": val,
            "unit": unit,
            "raw": raw_disp,
            "source_url": source_url,
            "context": ctx_store,
            "context_snippet": ctx_store,
            "anchor_hash": anchor_hash,

            "is_junk": bool(is_junk),
            "junk_reason": junk_reason,
            "start_idx": int(m.start()),
            "end_idx": int(m.end()),

            "measure_kind": measure_kind,
            "measure_assoc": measure_assoc,
        })

        if len(out) >= int(max_results or 350):
            break

    # - Ensures out entries include unit_tag, unit_family, and corrected currency measure_kind/assoc
    # - Adds compact unit_measure_classifier_trace_v1 for audit
    try:
        fn_can = globals().get("canonicalize_numeric_candidate")
        if not callable(fn_can):
            fn_can = canonicalize_numeric_candidate
        out2 = []
        for _c in out:
            if isinstance(_c, dict):
                try:
                    _c2 = fn_can(dict(_c)) or dict(_c)
                except Exception:
                    pass
                    _c2 = dict(_c)
                out2.append(_c2)
            else:
                out2.append(_c)
        out = out2
    except Exception:
        pass

    return out


def extract_numbers_with_context_pdf(text):
    """
    PDF-specialized extractor wrapper.

    Tightening changes (v7.29+):
    - Inherit the year-only rejection from extract_numbers_with_context().
    - Keep boilerplate filters; prefer metric/table-like contexts.
    """
    import re

    if not text:
        return []

    base = extract_numbers_with_context(text) or []

    def _bad_pdf_context(ctx):
        c = (ctx or "").lower()
        bad = [
            "issn", "isbn", "doi", "catalogue", "legal notice",
            "all rights reserved", "reproduction is authorised",
            "printed by", "manuscript completed", "©", "copyright",
            "table of contents"
        ]
        return any(b in c for b in bad)

    def _good_pdf_context(ctx):
        c = (ctx or "").lower()
        # Lightweight heuristic: "table-ish" or "metric-ish"
        good = [
            "market", "revenue", "sales", "capacity", "generation", "growth",
            "cagr", "forecast", "projection", "increase", "decrease",
            "percent", "%", "billion", "million", "trillion", "usd", "eur", "gbp", "sgd"
        ]
        return any(g in c for g in good)

    filtered = []
    for n in base:
        if not isinstance(n, dict):
            continue
        ctx = n.get("context") or ""
        if _bad_pdf_context(ctx):
            continue
        filtered.append(n)

    # Prefer contexts that look "metric-like"
    preferred = [n for n in filtered if _good_pdf_context(n.get("context") or "")]

    # If we filtered too aggressively, fall back safely
    if preferred:
        return preferred
    if filtered:
        return filtered
    return base


def calculate_context_match(keywords: List[str], context: str) -> float:
    """Calculate how well keywords match the context (deterministic)."""
    if not context:
        return 0.0

    context_lower = context.lower()

    # If no keywords, give a small baseline (we'll rely more on value_score)
    if not keywords:
        return 0.25

    # Year keywords MUST match if present
    year_keywords = [kw for kw in keywords if re.fullmatch(r"20\d{2}", kw)]
    if year_keywords:
        if not any(y in context_lower for y in year_keywords):
            return 0.0

    matches = sum(1 for kw in keywords if kw.lower() in context_lower)

    # Instead of hard "matches < 2 = reject", scale smoothly:
    match_ratio = matches / max(len(keywords), 1)

    # If nothing matches, reject
    if matches == 0:
        return 0.0

    # Score between 0.35 and 1.0 depending on ratio
    return 0.35 + (match_ratio * 0.65)


def render_source_anchored_results(results, query: str):
    """Render source-anchored evolution results (guarded + backward compatible + tuned debug UI)."""
    import math
    import re
    from collections import Counter, defaultdict

    st.header("📈 Source-Anchored Evolution Analysis")
    st.markdown(f"**Query:** {query}")

    if not isinstance(results, dict):
        st.error("❌ Evolution returned an invalid result payload (not a dict).")
        st.write(results)
        return

    status = (results.get("status") or "").strip().lower()
    message = results.get("message") or ""

    def _safe_int(x, default=0) -> int:
        try:
            if x is None:
                return default
            return int(x)
        except Exception:
            return default

    def _safe_float(x, default=0.0) -> float:
        try:
            if x is None:
                return default
            return float(x)
        except Exception:
            return default

    def _fmt_pct(x, default="—") -> str:
        try:
            if x is None:
                return default
            v = float(x)
            if math.isnan(v):
                return default
            return f"{v:.0f}%"
        except Exception:
            return default

    def _fmt_change_pct(x) -> str:
        try:
            if x is None:
                return "-"
            v = float(x)
            if math.isnan(v):
                return "-"
            return f"{v:+.1f}%"
        except Exception:
            return "-"

    def _short(u: str, n: int = 95) -> str:
        if not u:
            return ""
        return (u[:n] + "…") if len(u) > n else u

    if status != "success":
        st.error(f"❌ {message or 'Evolution failed'}")
        sr = results.get("source_results") or []
        if isinstance(sr, list) and sr:
            st.subheader("🔗 Source Verification")
            for src in sr:
                if not isinstance(src, dict):
                    continue
                u = _short((src.get("url") or ""), 90)
                st.error(f"❌ {u} - {src.get('status_detail', 'Unknown error')}")
        return

    sources_checked = _safe_int(results.get("sources_checked"), 0)
    sources_fetched = _safe_int(results.get("sources_fetched"), 0)
    stability = _safe_float(results.get("stability_score"), 0.0)
    summary = results.get("summary") or {}
    if not isinstance(summary, dict):
        summary = {}

    metrics_inc = _safe_int(summary.get("metrics_increased"), 0)
    metrics_dec = _safe_int(summary.get("metrics_decreased"), 0)
    metrics_unch = _safe_int(summary.get("metrics_unchanged"), 0)

    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Sources Checked", sources_checked)
    col2.metric("Sources Fetched", sources_fetched)
    col3.metric("Stability", _fmt_pct(stability))
    if metrics_inc > metrics_dec:
        col4.success("📈 Trending Up")
    elif metrics_dec > metrics_inc:
        col4.error("📉 Trending Down")
    else:
        col4.info("➡️ Stable")

    if message:
        st.caption(message)

    st.markdown("---")

    # Source status
    st.subheader("🔗 Source Verification")
    src_results = results.get("source_results") or []
    if not isinstance(src_results, list):
        src_results = []

    # If everything failed, show breakdown
    if sources_checked > 0 and sources_fetched == 0 and src_results:
        reasons = []
        for s in src_results:
            if isinstance(s, dict):
                reasons.append((s.get("status_detail") or "unknown").split(":")[0])
        top = Counter(reasons).most_common(6)
        if top:
            st.warning("No sources were fetched successfully. Top failure types:")
            st.write({k: v for k, v in top})

    for src in src_results:
        if not isinstance(src, dict):
            continue
        url = src.get("url") or ""
        sstatus = src.get("status") or ""
        detail = src.get("status_detail") or ""
        ctype = src.get("content_type") or ""
        nfound = _safe_int(src.get("numbers_found"), 0)

        short = _short(url, 95)

        # show extra debug flags if present
        flags = []
        if src.get("snapshot_origin"):
            flags.append(f"origin={src.get('snapshot_origin')}")
        if src.get("is_homepage"):
            flags.append("homepage")
        if src.get("skip_reason"):
            flags.append(f"skip={src.get('skip_reason')}")
        if src.get("quality_score") is not None:
            try:
                flags.append(f"q={float(src.get('quality_score')):.2f}")
            except Exception:
                pass
                flags.append(f"q={src.get('quality_score')}")

        flag_txt = f" • {' • '.join(flags)}" if flags else ""

        if str(sstatus).startswith("fetched"):
            extra = f" ({nfound} nums)"
            if ctype:
                extra += f" • {ctype}"
            st.success(f"✅ {short}{extra}{flag_txt}")
        else:
            extra = f" - {detail}" if detail else ""
            if ctype:
                extra += f" • {ctype}"
            st.error(f"❌ {short}{extra}{flag_txt}")

    st.markdown("---")

    # Metric changes table
    st.subheader("💰 Metric Changes")

    # Prefer the V2 schema if present; fall back to legacy key for older snapshots.
    rows = results.get("metric_changes") or results.get("metric_changes_v2") or []
    if not isinstance(rows, list) or not rows:
        st.info("No metric changes to display.")
        return

    # REFACTOR91: hide "missing_both" rows by default (coverage gaps), with an opt-in toggle.
    def _refactor91__row_change_type(_r: dict) -> str:
        try:
            return str(_r.get("change_type") or _r.get("status") or "").strip().lower()
        except Exception:
            return ""

    _refactor91_missing_both_count = 0
    try:
        _refactor91_missing_both_count = sum(
            1 for _r in rows
            if isinstance(_r, dict) and _refactor91__row_change_type(_r) == "missing_both"
        )
    except Exception:
        _refactor91_missing_both_count = 0

    if _refactor91_missing_both_count:
        st.caption(f"Coverage gaps (missing_both): {_refactor91_missing_both_count}")
        _refactor91_show_missing_both = st.checkbox(
            "Show missing-both rows (coverage gaps)",
            value=False,
            key="yureeka_show_missing_both_rows_v1",
        )
        if not _refactor91_show_missing_both:
            _refactor91_filtered = [
                _r for _r in rows
                if not (isinstance(_r, dict) and _refactor91__row_change_type(_r) == "missing_both")
            ]
            if _refactor91_filtered:
                rows = _refactor91_filtered
            else:
                st.info("All rows are coverage gaps (missing_both). Enable the toggle to view them.")
                return


    def _is_v2_row(_r: dict) -> bool:
        try:
            return isinstance(_r, dict) and (
                ("delta_pct" in _r) or ("prev_value_norm" in _r) or ("cur_value_norm" in _r)
            )
        except Exception:
            return False

    is_v2 = any(_is_v2_row(r) for r in rows)

    # Only show Δt column if at least one row has a non-empty value.
    show_delta = any(
        isinstance(r, dict) and str(r.get("analysis_evolution_delta_human") or "").strip()
        for r in rows
    )

    def _fmt_vu(v, u: str) -> str:
        """Format value + unit compactly for tables."""
        u = (u or "").strip()
        if v is None or v == "":
            return ""
        try:
            vv = float(v)
            # Use general format; keep it compact.
            s = f"{vv:g}"
        except Exception:
            s = str(v)
        return f"{s} {u}".strip()

    def _fmt_delta_pct_v2(v) -> str:
        if v is None or v == "":
            return ""
        try:
            return f"{_safe_float(v, 0.0):.2f}%"
        except Exception:
            return str(v)

    table_rows = []
    for r in rows:
        if not isinstance(r, dict):
            continue

        if is_v2:
            prev_u = r.get("previous_unit") or ""
            cur_u = r.get("current_unit") or prev_u or ""

            out_row = {
                "Metric": (r.get("name") or r.get("metric") or ""),
                "Canonical Key": (r.get("canonical_key") or ""),
                "Previous": _fmt_vu(r.get("previous_value"), prev_u),
                "Current": _fmt_vu(r.get("current_value"), cur_u),
                "Δ": ("" if r.get("delta_abs") is None else f"{_safe_float(r.get('delta_abs'), 0.0):g}"),
                "Δ%": _fmt_delta_pct_v2(r.get("delta_pct")),
                "Status": (r.get("change_type") or r.get("status") or ""),
                "Comparable": ("✅" if r.get("baseline_is_comparable") else "⚠"),
                "Method": (r.get("current_method") or ""),
            }
            if show_delta:
                out_row["Δt (A→E)"] = r.get("analysis_evolution_delta_human") or ""
            table_rows.append(out_row)
        else:
            metric_label = r.get("metric") or r.get("name") or ""
            status_label = r.get("status") or r.get("change_type") or ""

            out_row = {
                "Metric": metric_label,
                "Canonical Key": r.get("canonical_key", "") or "",
                "Match Stage": r.get("match_stage", "") or "",
                "Previous": r.get("previous_value", "") or "",
                "Current": r.get("current_value", "") or "",
                "Δ%": _fmt_change_pct(r.get("change_pct") if r.get("change_pct") is not None else r.get("delta_pct")),
                "Status": status_label,
                "Match": _fmt_pct(r.get("match_confidence")),
                "Score": ("" if r.get("match_score") is None else f"{_safe_float(r.get('match_score'), 0.0):.2f}"),
                "Anchor": "✅" if r.get("anchor_used") else "",
            }
            if show_delta:
                out_row["Δt (A→E)"] = r.get("analysis_evolution_delta_human") or ""
            table_rows.append(out_row)

    st.dataframe(table_rows, use_container_width=True)


    # Debug / tuning views
    # Aggregate rejection reasons across all metrics (quick tuning signal)
    agg_rej = Counter()
    for r in rows:
        if isinstance(r, dict) and isinstance(r.get("rejected_reason_counts"), dict):
            for k, v in r["rejected_reason_counts"].items():
                try:
                    agg_rej[k] += int(v or 0)
                except Exception:
                    pass

    if agg_rej:
        with st.expander("🧰 Tuning Summary (aggregate rejects across all metrics)"):
            st.write(dict(agg_rej.most_common(20)))

    # Full per-metric debug
    with st.expander("🧾 Per-metric match details (debug)"):
        for i, r in enumerate(rows, 1):
            if not isinstance(r, dict):
                continue

            metric_label = r.get("metric") or r.get("name") or f"metric_{i}"
            status_label = r.get("status") or r.get("change_type") or "unknown"

            canonical_key = r.get("canonical_key", "") or ""
            stage = r.get("match_stage", "") or ""
            conf = r.get("match_confidence", None)
            score = r.get("match_score", None)

            header = f"{i}. {metric_label} — {status_label}"
            meta_bits = []
            if canonical_key:
                meta_bits.append(f"ck={canonical_key}")
            if stage:
                meta_bits.append(f"stage={stage}")
            if conf is not None:
                meta_bits.append(f"conf={_fmt_pct(conf)}")
            if score is not None:
                try:
                    meta_bits.append(f"score={float(score):.2f}")
                except Exception:
                    pass
                    meta_bits.append(f"score={score}")

            if meta_bits:
                header += f"  ({' • '.join(meta_bits)})"

            with st.expander(header):
                # Values
                st.write({
                    "previous_value": r.get("previous_value"),
                    "current_value": r.get("current_value"),
                    "change_pct": r.get("change_pct"),
                })

                # Candidate considered / rejects
                st.write("Candidates considered:", _safe_int(r.get("candidates_considered_count"), 0))

                rej = r.get("rejected_reason_counts")
                if isinstance(rej, dict) and rej:
                    # sort largest first
                    try:
                        rej_sorted = dict(sorted(((k, int(v or 0)) for k, v in rej.items()), key=lambda x: x[1], reverse=True))
                    except Exception:
                        pass
                        rej_sorted = rej
                    st.write("Rejected reason counts:", rej_sorted)

                # Score breakdown (if present)
                sb = r.get("score_breakdown")
                if isinstance(sb, dict) and sb:
                    st.write("Score breakdown:", sb)

                # Matched candidate (new)
                mc = r.get("matched_candidate")
                if isinstance(mc, dict) and mc:
                    st.markdown("**Matched candidate**")
                    st.write({
                        "raw": mc.get("raw"),
                        "value": mc.get("value"),
                        "unit": mc.get("unit"),
                        "source_url": mc.get("source_url"),
                        "anchor_hash": mc.get("anchor_hash"),
                        "is_homepage": mc.get("is_homepage"),
                        "skip_reason": mc.get("skip_reason"),
                        "quality_score": mc.get("quality_score"),
                    })
                    ctx = mc.get("context_snippet")
                    if ctx:
                        st.write("Context:")
                        st.code(str(ctx))
                else:
                    # Backward-compatible fields
                    src = r.get("matched_source") or r.get("source_url")
                    ctx = r.get("matched_context") or r.get("context_snippet")
                    if src:
                        st.write("Source:", src)
                    if ctx:
                        st.write("Context:")
                        st.code(str(ctx))

                # Additional anchor hash compatibility
                if r.get("matched_anchor_hash"):
                    st.write("Matched Anchor Hash:", r.get("matched_anchor_hash"))

    st.markdown("---")


# 9. DASHBOARD RENDERING

def detect_x_label_dynamic(labels: list) -> str:
    """Enhanced X-axis detection with better region matching"""
    if not labels:
        return "Category"

    # Convert to lowercase for comparison
    label_texts = [str(l).lower().strip() for l in labels]
    all_text = ' '.join(label_texts)

    # 1. GEOGRAPHIC REGIONS (PRIORITY 1)
    region_keywords = [
        'north america', 'asia pacific', 'asia-pacific', 'apac', 'europe', 'emea',
        'latin america', 'latam', 'middle east', 'africa', 'oceania',
        'rest of world', 'row', 'china', 'usa', 'india', 'japan', 'germany'
    ]

    # Count how many labels contain region keywords
    region_matches = sum(
        1 for label in label_texts
        if any(keyword in label for keyword in region_keywords)
    )

    # If 40%+ of labels are regions → "Regions"
    if region_matches / len(labels) >= 0.4:
        return "Regions"

    # 2. YEARS (e.g., 2023, 2024, 2025)
    year_pattern = r'\b(19|20)\d{2}\b'
    year_count = sum(1 for label in label_texts if re.search(year_pattern, label))
    if year_count / len(labels) > 0.5:
        return "Years"

    # 3. QUARTERS (Q1, Q2, Q3, Q4)
    quarter_pattern = r'\bq[1-4]\b'
    quarter_count = sum(1 for label in label_texts if re.search(quarter_pattern, label, re.IGNORECASE))
    if quarter_count >= 2:
        return "Quarters"

    # 4. MONTHS
    months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']
    month_count = sum(1 for label in label_texts if any(month in label for month in months))
    if month_count >= 3:
        return "Months"

    # 5. COMPANIES (common suffixes)
    company_keywords = ['inc', 'corp', 'ltd', 'llc', 'gmbh', 'ag', 'sa', 'plc']
    company_count = sum(1 for label in label_texts if any(kw in label for kw in company_keywords))
    if company_count >= 2:
        return "Companies"

    # 6. PRODUCTS/SEGMENTS (if contains "segment", "product", "category")
    if any(word in all_text for word in ['segment', 'product line', 'category', 'type']):
        return "Segments"

    # Default
    return "Categories"

def detect_y_label_dynamic(values: list) -> str:
    """Fully dynamic Y-axis label based on magnitude + context"""
    if not values:
        return "Value"

    numeric_values = []
    for v in values:
        try:
            numeric_values.append(abs(float(v)))
        except (ValueError, TypeError):
            continue

    if not numeric_values:
        return "Value"

    avg_mag = np.mean(numeric_values)
    max_mag = max(numeric_values)

    # Non-overlapping ranges with clear boundaries
    # 1. BILLIONS (large market sizes)
    if max_mag > 100 or avg_mag > 50:
        return "USD B"

    # 2. MILLIONS (medium values)
    elif max_mag > 10 or avg_mag > 5:
        return "USD M"

    # 3. PERCENTAGES (typical 0-100 range, but also small decimals)
    elif max_mag <= 100 and avg_mag <= 50:
        # Check if values look like percentages (mostly 0-100)
        if all(0 <= v <= 100 for v in numeric_values):
            return "Percent %"
        else:
            return "USD K"

    # 4. Default
    else:
        return "Units"

# 3A. QUESTION CATEGORIZATION + SIGNALS (DETERMINISTIC)

def categorize_question_signals(query: str, qs: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Build a question_profile used for structured reporting.

    IMPORTANT:
      - category must follow query_structure if provided (single source of truth).
      - signals can be rich, but must not contradict the chosen category.
    """
    qs = qs or {}
    q = (query or "").strip()

    # Prefer category/main/side from query_structure when available
    category = (qs.get("category") or "").strip() or "unknown"
    main_q = (qs.get("main") or "").strip() or q
    side_qs = qs.get("side") if isinstance(qs.get("side"), list) else []

    # Deterministic signals (richer classifier)
    base = classify_question_signals(q) or {}

    # Force category + expected_metric_ids to match query_structure category
    # (but preserve other extracted info like years/regions/intents)
    signals: Dict[str, Any] = {}
    signals["category"] = category

    # Carry over extracted fields
    signals["years"] = base.get("years", []) or []
    signals["regions"] = base.get("regions", []) or []
    signals["intents"] = base.get("intents", []) or []

    # Keep raw signals for debugging
    raw_hits = list(base.get("signals") or [])
    signals["raw_signals"] = raw_hits

    def _signal_consistent_with_category(sig: str, cat: str) -> bool:
        s = (sig or "").lower()
        c = (cat or "").lower()
        if not s:
            return False

        # If final category is country, drop industry/company category-rule strings
        if c == "country":
            if "industry_keywords" in s or "mixed_signals_default_to_industry" in s or "company_keywords" in s:
                return False

        # If final category is industry, drop explicit country-rule strings
        if c == "industry":
            if "macro_outlook_bias_country" in s or "country_keywords" in s:
                return False

        return True

    signals["signals"] = [s for s in raw_hits if _signal_consistent_with_category(s, category)]

    # Expected metric IDs: always determined by the final category, then lightly enriched by intents (optional)
    expected_metric_ids: List[str] = []
    try:
        expected_metric_ids = get_expected_metric_ids_for_category(category) or []
    except Exception:
        pass
        expected_metric_ids = []

    # Optional: enrich with intent-based suggestions (won't remove anything)
    intent_metric_suggestions = {
        "market_size": ["market_size", "market_size_2024", "market_size_2025"],
        "growth_forecast": ["cagr", "market_size_2030"],
        "competitive_landscape": ["market_share", "top_players"],
        "pricing": ["avg_price", "asp"],
        "consumer_demand": ["users", "penetration", "arpu"],
        "supply_chain": ["capacity", "shipments"],
        "investment": ["capex", "profit", "ebitda"],
        "macro_outlook": ["gdp", "inflation", "interest_rate", "exchange_rate"],
    }

    intents = signals.get("intents") or []
    for intent in intents:
        for mid in intent_metric_suggestions.get(intent, []):
            if mid not in expected_metric_ids:
                expected_metric_ids.append(mid)

    signals["expected_metric_ids"] = expected_metric_ids

    profile: Dict[str, Any] = {
        "category": category,
        "signals": signals,
        "main_question": main_q,
        "side_questions": side_qs,
    }

    # Keep debug for traceability
    if qs.get("debug") is not None:
        profile["debug_query_structure"] = qs.get("debug")

    return profile


def render_dashboard(
    primary_json: str,
    final_conf: float,
    web_context: Dict,
    base_conf: float,
    user_question: str,
    veracity_scores: Optional[Dict] = None,
    source_reliability: Optional[List[str]] = None,
):
    """Render the analysis dashboard"""

    # Parse primary response

    # - Prevents slice errors when primary_json is dict/list/etc.
    # - Keeps original behavior for strings
    def _preview(x, limit: int = 1000) -> str:
        try:
            if isinstance(x, (dict, list)):
                s = json.dumps(x, ensure_ascii=False, indent=2, default=str)
            else:
                s = str(x)
        except Exception:
            pass
            s = repr(x)
        return s[:limit]

    try:
        # - If caller passes dict (primary_data), just use it
        # - If caller passes list, wrap it (keeps downstream dict access safe)
        # - Else try json.loads on string
        if isinstance(primary_json, dict):
            data = primary_json
        elif isinstance(primary_json, list):
            data = {"_list": primary_json}
        else:
            data = json.loads(primary_json)

    except Exception as e:
        st.error(f"❌ Cannot render dashboard: {e}")
        st.code(_preview(primary_json))
        return

    # Helper: metric value formatting (currency + compact units) + RANGE SUPPORT
    def _format_metric_value(m: Any) -> str:
        """
        Format metric values cleanly, with RANGE SUPPORT:
        - If value_range exists (min/max), show min–max using the same currency/unit rules
        - Otherwise show the point value as before
        """
        if not isinstance(m, dict):
            if m is None:
                return "N/A"
            return str(m)

        # Helper: format a single numeric endpoint (val+unit)
        def _format_point(val: Any, unit: str) -> str:
            if val is None or val == "":
                return "N/A"

            unit = (unit or "").strip()
            raw_val = str(val).strip()

            # Try parse numeric
            try:
                num = float(raw_val.replace(",", ""))
            except Exception:
                pass
                # If we can't parse as float, just glue value+unit neatly
                return f"{raw_val}{unit}".strip() if unit else raw_val

            # Normalize unit spacing
            unit = unit.replace(" ", "")
            currency_prefix = ""
            u_upper = unit.upper()

            # Common patterns: "S$B", "SGDB", "USD B", "$B"
            if u_upper.startswith("S$"):
                currency_prefix = "S$"
                unit = unit[2:]
            elif u_upper.startswith("SGD"):
                currency_prefix = "S$"
                unit = unit[3:]
            elif u_upper.startswith("USD"):
                currency_prefix = "$"
                unit = unit[3:]
            elif u_upper.startswith("$"):
                currency_prefix = "$"
                unit = unit[1:]

            unit = unit.strip()

            # Percent
            if unit == "%":
                return f"{num:.1f}%"

            # Compact units
            unit_upper = unit.upper()
            if unit_upper in ("B", "BILLION"):
                formatted = f"{num:.2f}".rstrip("0").rstrip(".") + "B"
                return f"{currency_prefix}{formatted}".strip()
            if unit_upper in ("M", "MILLION"):
                formatted = f"{num:.2f}".rstrip("0").rstrip(".") + "M"
                return f"{currency_prefix}{formatted}".strip()
            if unit_upper in ("K", "THOUSAND"):
                formatted = f"{num:.2f}".rstrip("0").rstrip(".") + "K"
                return f"{currency_prefix}{formatted}".strip()

            # Plain number formatting
            if abs(num) >= 1000:
                if float(num).is_integer():
                    formatted = f"{int(num):,}"
                else:
                    formatted = f"{num:,.2f}".rstrip("0").rstrip(".")
            else:
                formatted = f"{num:g}"

            # Unit glue
            if unit:
                formatted = f"{formatted} {unit}".strip()

            return f"{currency_prefix}{formatted}".strip()

        # RANGE: prefer value_range if present and meaningful
        unit = (m.get("unit") or "").strip()
        vr = m.get("value_range")

        if isinstance(vr, dict):
            vmin = vr.get("min")
            vmax = vr.get("max")
            if vmin is not None and vmax is not None:
                left = _format_point(vmin, unit)
                right = _format_point(vmax, unit)
                if left != "N/A" and right != "N/A" and left != right:
                    return f"{left}–{right}"

        # Precomputed range display (optional)
        vr_disp = m.get("value_range_display")
        if isinstance(vr_disp, str) and vr_disp.strip():
            return vr_disp.strip()

        # POINT VALUE fallback
        val = m.get("value")
        if val is None or val == "":
            return "N/A"

        return _format_point(val, unit)

    # Header + confidence row
    st.header("📊 Yureeka Market Report")
    st.markdown(f"**Question:** {user_question}")

    col1, col2, col3 = st.columns(3)
    col1.metric("Final Confidence", f"{float(final_conf):.1f}%")
    col2.metric("Base Model", f"{float(base_conf):.1f}%")
    if isinstance(veracity_scores, dict):
        col3.metric("Evidence", f"{float(veracity_scores.get('overall', 0) or 0):.1f}%")
    else:
        col3.metric("Evidence", "N/A")

    st.markdown("---")

    # Executive Summary
    st.subheader("📋 Executive Summary")
    st.markdown(f"**{data.get('executive_summary', 'No summary available')}**")

    # Optional: expand summary if side-questions exist
    side_questions = data.get("side_questions") or (data.get("question_profile", {}) or {}).get("side_questions", [])
    if side_questions:
        st.markdown("")
        st.markdown("**Also addressed:**")
        for sq in side_questions[:6]:
            if sq:
                st.markdown(f"- {sq}")

    st.markdown("---")

    # Key Metrics
    st.subheader("💰 Key Metrics")
    metrics = data.get("primary_metrics", {}) or {}

    question_category = data.get("question_category") or (data.get("question_profile", {}) or {}).get("category")
    question_signals = data.get("question_signals") or (data.get("question_profile", {}) or {}).get("signals", {})
    expected_ids = data.get("expected_metric_ids") or ((data.get("question_signals") or {}).get("expected_metric_ids") or [])

    metric_rows: List[Dict[str, str]] = []

    if question_category:
        metric_rows.append({"Metric": "Question Category", "Value": str(question_category)})
    if isinstance(question_signals, dict) and question_signals:
        metric_rows.append({"Metric": "Signals", "Value": ", ".join([str(x) for x in (question_signals.get("signals") or [])][:10])})
    if expected_ids:
        metric_rows.append({"Metric": "Expected Metrics", "Value": ", ".join([str(x) for x in expected_ids][:10])})

    # Render primary metrics
    if isinstance(metrics, dict) and metrics:
        for _, m in metrics.items():
            if isinstance(m, dict):
                name = m.get("name") or "Metric"
                metric_rows.append({"Metric": str(name), "Value": _format_metric_value(m)})

    # Display metrics table
    if metric_rows:
        try:
            import pandas as pd  # optional dependency in your environment
            df_metrics = pd.DataFrame(metric_rows)
            st.dataframe(df_metrics, use_container_width=True, hide_index=True)
        except Exception:
            pass
            for r in metric_rows:
                st.write(f"**{r.get('Metric','')}**: {r.get('Value','')}")

    st.markdown("---")

    # Key Findings
    st.subheader("🧠 Key Findings")
    kf = data.get("key_findings") or []
    if isinstance(kf, list) and kf:
        for item in kf[:12]:
            if item:
                st.markdown(f"- {item}")
    else:
        st.info("No key findings available.")

    st.markdown("---")

    # Trends / Forecast
    st.subheader("📈 Trends & Forecast")
    tf = data.get("trends_forecast") or []
    if isinstance(tf, list) and tf:
        for t in tf[:12]:
            if isinstance(t, dict):
                trend = t.get("trend") or ""
                direction = t.get("direction") or ""
                timeline = t.get("timeline") or ""
                st.markdown(f"- **{trend}** {direction} ({timeline})")
            elif t:
                st.markdown(f"- {t}")
    else:
        st.info("No trends forecast available.")

    st.markdown("---")

    # Sources / Web Context summary
    st.subheader("🔎 Sources & Evidence")
    sources = data.get("sources") or data.get("web_sources") or []
    if isinstance(sources, list) and sources:
        with st.expander(f"Show sources ({len(sources)})"):
            for s in sources[:50]:
                if s:
                    st.markdown(f"- {s}")
            if len(sources) > 50:
                st.markdown(f"... (+{len(sources)-50} more)")

    # Web context debug counters if present
    if isinstance(web_context, dict):
        dbg = web_context.get("debug_counts") or {}
        if isinstance(dbg, dict) and dbg:
            with st.expander("Collector debug counts"):
                st.json(dbg)

    try:
        exdbg = {}
        if isinstance(web_context, dict):
            exdbg = web_context.get("extra_urls_debug") or {}
            # Back-compat: allow nested placement under debug_counts
            if (not exdbg) and isinstance(web_context.get("debug_counts"), dict):
                exdbg = (web_context.get("debug_counts") or {}).get("extra_urls_debug") or {}
        if isinstance(exdbg, dict) and exdbg:
            with st.expander("Extra URLs trace (injected sources)"):
                st.json(exdbg)
    except Exception:
        pass

    # Source reliability badges (if provided)
    if isinstance(source_reliability, list) and source_reliability:
        with st.expander("Source reliability"):
            for line in source_reliability[:80]:
                st.write(line)


def render_native_comparison(baseline: Dict, compare: Dict):
    """Render a clean comparison between two analyses"""

    st.header("📊 Analysis Comparison")

    # Time info
    baseline_time = baseline.get('timestamp', '')
    compare_time = compare.get('timestamp', '')

    try:
        baseline_dt = datetime.fromisoformat(baseline_time.replace('Z', '+00:00'))
        compare_dt = datetime.fromisoformat(compare_time.replace('Z', '+00:00'))
        delta = compare_dt - baseline_dt
        if delta.days > 0:
            delta_str = f"{delta.days}d {delta.seconds // 3600}h"
        else:
            delta_str = f"{delta.seconds // 3600}h {(delta.seconds % 3600) // 60}m"
    except:
        delta_str = "Unknown"

    # Overview row
    col1, col2, col3 = st.columns(3)
    col1.metric("Baseline", baseline_time[:16] if baseline_time else "N/A")
    col2.metric("Current", compare_time[:16] if compare_time else "N/A")
    col3.metric("Time Delta", delta_str)

    st.markdown("---")

    # Extract metrics
    baseline_metrics = baseline.get('primary_response', {}).get('primary_metrics', {})
    compare_metrics = compare.get('primary_response', {}).get('primary_metrics', {})

    # Build metric diff table
    st.subheader("💰 Metric Changes")

    diff_rows = []
    stability_count = 0
    total_count = 0

    # Canonicalize metrics for stable matching
    baseline_canonical = canonicalize_metrics(baseline_metrics)
    compare_canonical = canonicalize_metrics(compare_metrics)

    # Build lookup by canonical ID
    baseline_by_id = {}
    compare_by_id = {}

    for cid, m in baseline_canonical.items():
        baseline_by_id[cid] = m

    for cid, m in compare_canonical.items():
        compare_by_id[cid] = m

    all_ids = set(baseline_by_id.keys()).intersection(compare_by_id.keys())

    for cid in sorted(all_ids):
        baseline_m = baseline_by_id.get(cid)
        compare_m = compare_by_id.get(cid)

        # Use canonical name for display, fallback to original
        display_name = cid
        if baseline_m and baseline_m.get('name'):
            display_name = baseline_m['name']


        if baseline_m and compare_m:
            old_val = baseline_m.get('value', 'N/A')
            new_val = compare_m.get('value', 'N/A')
            unit = compare_m.get('unit', baseline_m.get('unit', ''))

            old_num = parse_to_float(old_val)
            new_num = parse_to_float(new_val)

            if old_num is not None and new_num is not None and old_num != 0:
                change_pct = ((new_num - old_num) / abs(old_num)) * 100

                if abs(change_pct) < 1:
                    icon, reason = "➡️", "No change"
                    stability_count += 1
                elif abs(change_pct) < 5:
                    icon, reason = "➡️", "Minor change"
                    stability_count += 1
                elif change_pct > 0:
                    icon, reason = "📈", "Increased"
                else:
                    icon, reason = "📉", "Decreased"

                delta_str = f"{change_pct:+.1f}%"
            else:
                icon, delta_str, reason = "➡️", "-", "Non-numeric"
                stability_count += 1

            diff_rows.append({
                '': icon,
                'Metric': display_name,
                'Old': _fmt_currency_first(str(old_val), str(unit)),
                'New': _fmt_currency_first(str(new_val), str(unit)),
                'Δ': delta_str,
                'Reason': reason
            })
            total_count += 1

        elif baseline_m:
            old_val = baseline_m.get('value', 'N/A')
            unit = baseline_m.get('unit', '')
            diff_rows.append({
                '': '❌',
                'Metric': display_name,
                'Old': f"{old_val} {unit}".strip(),
                'New': '-',
                'Δ': '-',
                'Reason': 'Removed'
            })
            total_count += 1
        else:
            new_val = compare_m.get('value', 'N/A')
            unit = compare_m.get('unit', '')
            diff_rows.append({
                '': '🆕',
                'Metric': display_name,
                'Old': '-',
                'New': f"{new_val} {unit}".strip(),
                'Δ': '-',
                'Reason': 'New'
            })
            total_count += 1

    if diff_rows:
        st.dataframe(pd.DataFrame(diff_rows), hide_index=True, use_container_width=True)

        # Show canonical ID mapping for debugging
        with st.expander("🔧 Canonical ID Mapping (Debug)"):
            st.write("**How metrics were matched:**")

            baseline_canonical = canonicalize_metrics(baseline_metrics)
            compare_canonical = canonicalize_metrics(compare_metrics)

            col1, col2 = st.columns(2)

            with col1:
                st.write("**Baseline Metrics:**")
                for cid, m in baseline_canonical.items():
                    original = m.get('original_name', 'N/A')
                    canonical = m.get('name', 'N/A')
                    st.caption(f"`{cid}`")
                    st.write(f"  {original} → {canonical}")

            with col2:
                st.write("**Current Metrics:**")
                for cid, m in compare_canonical.items():
                    original = m.get('original_name', 'N/A')
                    canonical = m.get('name', 'N/A')
                    st.caption(f"`{cid}`")
                    st.write(f"  {original} → {canonical}")
    else:
        st.info("No metrics to compare")

    # Stability score
    stability_pct = (stability_count / total_count * 100) if total_count > 0 else 100

    st.markdown("---")
    st.subheader("📊 Stability Score")

    col1, col2, col3 = st.columns(3)
    col1.metric("Stable Metrics", f"{stability_count}/{total_count}")
    col2.metric("Stability", f"{stability_pct:.0f}%")

    if stability_pct >= 80:
        col3.success("🟢 Highly Stable")
    elif stability_pct >= 60:
        col3.warning("🟡 Moderate Changes")
    else:
        col3.error("🔴 Significant Drift")

    # Confidence comparison
    st.markdown("---")
    st.subheader("🎯 Confidence Change")

    col1, col2, col3 = st.columns(3)
    baseline_conf = baseline.get('final_confidence', 0)
    compare_conf = compare.get('final_confidence', 0)
    conf_change = compare_conf - baseline_conf if isinstance(baseline_conf, (int, float)) and isinstance(compare_conf, (int, float)) else 0

    col1.metric("Baseline", f"{baseline_conf:.1f}%" if isinstance(baseline_conf, (int, float)) else "N/A")
    col2.metric("Current", f"{compare_conf:.1f}%" if isinstance(compare_conf, (int, float)) else "N/A")
    col3.metric("Change", f"{conf_change:+.1f}%")

    # Download comparison
    st.markdown("---")
    comparison_output = {
        "comparison_timestamp": _yureeka_now_iso_utc(),
        "baseline": baseline,
        "current": compare,
        "stability_score": stability_pct,
        "metrics_compared": total_count,
        "metrics_stable": stability_count
    }

    st.download_button(
        label="💾 Download Comparison Report",
        data=json.dumps(comparison_output, indent=2, ensure_ascii=False).encode('utf-8'),
        file_name=f"yureeka_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        mime="application/json"
    )

# 10. MAIN APPLICATION

#
# Why:
# - Even if upstream selection is tightened, some paths (UI render, sheet publish,
#   legacy mappings) can still surface unit-less year-like integers (e.g., 2024/2025)
#   in the "Current" column for unit-required metrics (currency/percent/rate/ratio).
# - FIX39 enforces the invariant at the last mile: right before rendering/publishing.
#
# Behavior:
# - For each metric change row (dict-form) and each EvolutionDiff metric entry (object-form),
#   if schema indicates unit required (via unit_family or canonical_key suffix) AND
#   current value lacks token-level unit evidence, then:
#     * blank out Current/new_raw
#     * set unit_mismatch flag / change_type to "unit_mismatch" where possible
# - Purely additive; does not refactor upstream pipelines.

def _fix39_schema_unit_required(metric_def: dict, canonical_key: str = "") -> bool:
    try:
        uf = str((metric_def or {}).get("unit_family") or (metric_def or {}).get("unit") or "").strip().lower()
        if uf in {"currency", "percent", "rate", "ratio"}:
            return True
    except Exception:
        pass
    ck = (canonical_key or "").strip().lower()
    if ck.endswith("__currency") or ck.endswith("__percent") or ck.endswith("__rate") or ck.endswith("__ratio"):
        return True
    # unit_tag explicit
    try:
        ut = str((metric_def or {}).get("unit_tag") or "").strip()
        if ut:
            # if schema explicitly wants a unit token, treat as required
            return True
    except Exception:
        return False

def _fix39_has_unit_evidence(metric_like: dict) -> bool:
    """Token-level unit evidence check (tolerant across shapes)."""
    try:
        m = metric_like if isinstance(metric_like, dict) else {}
        for k in ("unit", "unit_tag", "base_unit", "unit_family", "currency", "currency_symbol"):
            if str(m.get(k) or "").strip():
                return True
        if bool(m.get("is_percent") or m.get("has_percent")):
            return True
        # Some rows store comparator field
        if str(m.get("cur_unit_cmp") or "").strip():
            return True
        raw = str(m.get("raw") or m.get("value") or m.get("new_raw") or "")
        if raw and any(sym in raw for sym in ("$", "€", "£", "¥", "%")):
            return True
    except Exception:
        return False

def _fix39_sanitize_metric_change_rows(results_dict: dict) -> None:
    """Sanitize dict-based evolution results before publishing/rendering."""
    if not isinstance(results_dict, dict):
        return
    try:
        schema = results_dict.get("metric_schema_frozen") or results_dict.get("schema") or {}
        metric_changes = None
        # common nesting patterns
        if isinstance(results_dict.get("results"), dict) and isinstance(results_dict["results"].get("metric_changes"), list):
            metric_changes = results_dict["results"]["metric_changes"]
        elif isinstance(results_dict.get("metric_changes"), list):
            metric_changes = results_dict.get("metric_changes")

        if not isinstance(metric_changes, list):
            return

        bad = []
        for row in metric_changes:
            if not isinstance(row, dict):
                continue
            ck = row.get("canonical_key") or row.get("canonical") or row.get("key") or ""
            md = {}
            try:
                if isinstance(schema, dict) and ck in schema:
                    md = schema.get(ck) or {}
            except Exception:
                pass
                md = {}
            if _fix39_schema_unit_required(md, ck):
                # current fields may be in different keys
                cur_like = {
                    "unit": row.get("current_unit") or row.get("cur_unit") or row.get("unit") or "",
                    "unit_tag": row.get("current_unit_tag") or row.get("unit_tag") or "",
                    "unit_family": row.get("schema_unit_family") or "",
                    "cur_unit_cmp": row.get("cur_unit_cmp") or "",
                    "raw": row.get("current_value") or row.get("current_raw") or row.get("Current") or "",
                    "new_raw": row.get("new_raw") or "",
                    "currency_symbol": row.get("currency_symbol") or "",
                    "is_percent": row.get("is_percent") or False,
                }
                if not _fix39_has_unit_evidence(cur_like):
                    # FIX2D76: v2 rows often keep unit evidence under diag.diff_current_source_trace_v1
                    try:
                        diag = row.get("diag") if isinstance(row.get("diag"), dict) else {}
                        dcs = diag.get("diff_current_source_trace_v1") if isinstance(diag.get("diff_current_source_trace_v1"), dict) else {}
                        ut = str(dcs.get("current_unit_tag") or dcs.get("current_unit_tag_norm") or "").strip()
                        if ut:
                            cur_like["unit"] = cur_like.get("unit") or ut
                            cur_like["unit_tag"] = cur_like.get("unit_tag") or ut
                            cur_like["cur_unit_cmp"] = cur_like.get("cur_unit_cmp") or ut
                    except Exception:
                        pass
                if not _fix39_has_unit_evidence(cur_like):
                    # invalidate
                    row["unit_mismatch"] = True
                    # prefer explicit fields if present
                    for k in ("current_value", "current_raw", "new_raw", "Current"):
                        if k in row:
                            row[k] = ""
                    if "current_value_norm" in row:
                        row["current_value_norm"] = None
                    if "cur_value_norm" in row:
                        row["cur_value_norm"] = None
                    # normalize change_type
                    if row.get("change_type") not in ("unit_mismatch", "invalid_current"):
                        row["change_type"] = "unit_mismatch"
                    bad.append(str(ck))
        # small debug marker
        dbg = results_dict.setdefault("debug", {})
        f39 = dbg.setdefault("fix39", {})
        f39["invalidated_count"] = len(bad)
        if bad:
            f39["invalidated_keys_sample"] = bad[:20]
    except Exception:
        return

def _fix39_sanitize_evolutiondiff_object(diff_obj, metric_schema_frozen: dict = None):
    """Sanitize object-based EvolutionDiff (used by Streamlit renderer)."""
    try:
        schema = metric_schema_frozen or {}
        mdiffs = getattr(diff_obj, "metric_diffs", None)
        if not mdiffs:
            return diff_obj
        bad = []
        for m in mdiffs:
            try:
                ck = getattr(m, "canonical_key", "") or getattr(m, "canonical", "") or ""
                md = schema.get(ck) if isinstance(schema, dict) else {}
                if _fix39_schema_unit_required(md or {}, ck):
                    unit = getattr(m, "unit", "") or ""
                    new_raw = getattr(m, "new_raw", None)
                    # basic evidence check: unit or symbol in new_raw
                    has_e = bool(str(unit).strip())
                    if not has_e:
                        s = str(new_raw or "")
                        if any(sym in s for sym in ("$", "€", "£", "¥", "%")):
                            has_e = True
                    if not has_e:
                        # invalidate
                        try: setattr(m, "new_raw", "")
                        except Exception: pass
                        try: setattr(m, "new_value", None)
                        except Exception: pass
                        try: setattr(m, "change_type", "unit_mismatch")
                        except Exception: pass
                        bad.append(str(ck))
            except Exception:
                pass
                continue
        try:
            dbg = getattr(diff_obj, "debug", None)
            if isinstance(dbg, dict):
                dbg.setdefault("fix39", {})["invalidated_count"] = len(bad)
        except Exception:
            return diff_obj
    except Exception:
        return diff_obj

def main():
    st.set_page_config(
        page_title="Yureeka Market Report",
        page_icon="💹",
        layout="wide"
    )

    st.title("💹 Yureeka Market Intelligence")
    _yureeka_show_debug_playbook_in_streamlit_v1()

    # Info section
    col_info, col_status = st.columns([3, 1])
    with col_info:
        st.markdown("""
        **Yureeka** provides AI-powered market research and analysis for finance,
        economics, and business questions.
        Powered by evidence-based verification and real-time web search.

        *Currently in prototype stage.*
        """)

    # Create tabs
    tab1, tab2 = st.tabs(["🔍 New Analysis", "📈 Evolution Analysis"])

    with tab1:
        query = st.text_input(
            "Enter your question about markets, industries, finance, or economics:",
            placeholder="e.g., What is the size of the global EV battery market?"
        )

        col_opt1, col_opt2 = st.columns(2)
        with col_opt1:
            use_web = st.checkbox(
                "Enable web search (recommended)",
                value=bool(SERPAPI_KEY),
                disabled=not SERPAPI_KEY
            )


            # - Add extra URL injection UI directly to TAB 1 (New Analysis)

            # - Does NOT alter behavior unless user supplies URLs


            extra_sources_text_tab1 = st.text_area(

                "Extra source URLs (optional, one per line)",

                placeholder="https://example.com/report\nhttps://another-source.com/page",

                help="Add these URLs to the admitted source list for this analysis run (useful for hash-mismatch tests).",

                height=90,

                key="ui_extra_sources_tab1",

            )


        if st.button("🔍 Analyze", type="primary") and query:
            if len(query.strip()) < 5:
                st.error("❌ Please enter a question with at least 5 characters")
                return

            query = query.strip()[:500]

            query_structure = extract_query_structure(query) or {}
            question_profile = categorize_question_signals(query, qs=query_structure)
            question_signals = question_profile.get("signals", {}) or {}

            web_context = {}
            if use_web:
                with st.spinner("🌐 Searching the web..."):

                    existing_snapshots = None

                    # If you have an analysis dict already in scope, reuse its cache
                    try:
                        if isinstance(locals().get("analysis"), dict):
                            existing_snapshots = (
                                analysis.get("baseline_sources_cache")
                                or (analysis.get("results", {}) or {}).get("baseline_sources_cache")
                                or (analysis.get("results", {}) or {}).get("source_results")
                            )
                    except Exception:
                        pass
                        existing_snapshots = None

                    # Optional: if you keep a prior analysis in session_state, reuse it
                    try:
                        prev = st.session_state.get("last_analysis")
                        if existing_snapshots is None and isinstance(prev, dict):
                            existing_snapshots = (
                                prev.get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("source_results")
                            )
                    except Exception:
                        pass

                    extra_urls = []
                    try:
                        for _l in str(extra_sources_text_tab1 or "").splitlines():
                            _u = _l.strip()
                            if not _u:
                                continue
                            if _u.startswith("http://") or _u.startswith("https://"):
                                extra_urls.append(_u)
                    except Exception:
                        pass
                        extra_urls = []


                    _analysis_run_id = _inj_diag_make_run_id("analysis")

                    web_context = fetch_web_context(
                        query,
                        num_sources=3,
                        existing_snapshots=existing_snapshots,
                        extra_urls=extra_urls,
                        diag_run_id=_analysis_run_id,
                        diag_extra_urls_ui_raw=(extra_sources_text_tab1 or ""),
                    )

            if not web_context or not web_context.get("search_results"):
                st.info("💡 Using AI knowledge without web search")
                web_context = {
                    "search_results": [],
                    "scraped_content": {},
                    "summary": "",
                    "sources": [],
                    "source_reliability": []
                }

            with st.spinner("🤖 Analyzing query..."):
                primary_response = query_perplexity(query, web_context, query_structure=query_structure)

            if not primary_response:
                st.error("❌ Primary model failed to respond")
                return

            try:
                primary_data = json.loads(primary_response)
            except Exception as e:
                st.error(f"❌ Failed to parse primary response: {e}")
                st.code(primary_response[:1000])
                return

            with st.spinner("✅ Verifying evidence quality..."):
                veracity_scores = evidence_based_veracity(primary_data, web_context)

            base_conf = float(primary_data.get("confidence", 75))
            final_conf = calculate_final_confidence(base_conf, veracity_scores.get("overall", 0))

            # Optional: canonicalize + attribution + schema freeze (only if your codebase defines these)
            try:
                # 1) canonicalize (unchanged)
                if primary_data.get("primary_metrics"):
                    _pmc_raw = canonicalize_metrics(
                        primary_data.get("primary_metrics", {}),
                        merge_duplicates_to_range=True,
                        question_text=query,
                        category_hint=str(primary_data.get("question_category", ""))
                    )
                    _pmc_ok, _pmc_prov = _fix2d58b_split_primary_metrics_canonical(_pmc_raw)
                    primary_data["primary_metrics_canonical"] = _pmc_ok
                    if _pmc_prov:
                        primary_data["primary_metrics_provisional"] = _pmc_prov

                # 2) freeze schema FIRST ✅ (so attribution can be schema-first)
                if primary_data.get("primary_metrics_canonical"):
                    primary_data["metric_schema_frozen"] = freeze_metric_schema(
                        primary_data["primary_metrics_canonical"]
                    )


                try:
                    fn_fix2u = globals().get("_fix2u_extend_metric_schema_ev_chargers")
                    if callable(fn_fix2u):
                        primary_data["metric_schema_frozen"] = fn_fix2u(primary_data.get("metric_schema_frozen") or {})
                except Exception:
                    pass

                try:
                    fn_fix2v = globals().get("_fix2v_extend_metric_schema_ev_chargers_cagr")
                    if callable(fn_fix2v):
                        primary_data["metric_schema_frozen"] = fn_fix2v(primary_data.get("metric_schema_frozen") or {})
                except Exception:
                    pass

                try:
                    fn_fix2ab = globals().get("_fix2ab_extend_metric_schema_global_ev_sales_ytd_2025")
                    if callable(fn_fix2ab):
                        primary_data["metric_schema_frozen"] = fn_fix2ab(primary_data.get("metric_schema_frozen") or {})
                except Exception:
                    pass

                # - Build schema proposals from primary_metrics_provisional using freeze_metric_schema.
                # - Auto-promote into metric_schema_frozen (governance can later restrict via allowlist).
                # - Record proposals/promotions for audit.
                try:
                    _prov = primary_data.get("primary_metrics_provisional")
                    _schema = primary_data.get("metric_schema_frozen")
                    if isinstance(_prov, dict) and _prov and isinstance(_schema, dict):
                        _prov_schema = freeze_metric_schema(_prov)
                        if isinstance(_prov_schema, dict) and _prov_schema:
                            primary_data["schema_promotion_proposals_v1"] = sorted([str(k) for k in _prov_schema.keys()])
                            # Auto-promote: merge proposals into frozen schema
                            for _k, _spec in _prov_schema.items():
                                if _k not in _schema:
                                    _schema[_k] = _spec
                            primary_data["metric_schema_frozen"] = _schema
                            primary_data["schema_promoted_v1"] = sorted([str(k) for k in _prov_schema.keys() if str(k) in _schema])
                except Exception:
                    pass


                # 2.B) FIX2D59: schema-first canonical identity rekey (Analysis)
                # - Routes existing pmc keys through the resolver using the frozen schema (after schema extension patches).
                try:
                    if isinstance(primary_data.get('primary_metrics_canonical'), dict) and isinstance(primary_data.get('metric_schema_frozen'), dict):
                        primary_data['primary_metrics_canonical'] = rekey_metrics_via_identity_resolver_v1(
                            primary_data.get('primary_metrics_canonical') or {},
                            primary_data.get('metric_schema_frozen') or {},
                        )
                except Exception:
                    pass
                try:
                    if isinstance(primary_data.get('primary_metrics_provisional'), dict) and isinstance(primary_data.get('metric_schema_frozen'), dict):
                        primary_data['primary_metrics_provisional'] = rekey_metrics_via_identity_resolver_v1(
                            primary_data.get('primary_metrics_provisional') or {},
                            primary_data.get('metric_schema_frozen') or {},
                        )
                except Exception:
                    pass


                # - After schema promotion + rekey, merge provisional into canonical so bound rows can be retained.
                try:
                    _prov = primary_data.get("primary_metrics_provisional")
                    if isinstance(_prov, dict) and _prov:
                        _can = primary_data.get("primary_metrics_canonical")
                        if not isinstance(_can, dict):
                            _can = {}
                        for _k, _v in _prov.items():
                            _can[_k] = _v
                        primary_data["primary_metrics_canonical"] = _can
                        primary_data["primary_metrics_provisional"] = {}
                except Exception:
                    pass

                # - After rekeying, keep ONLY schema-bound keys in primary_metrics_canonical.
                # - Move everything else into primary_metrics_provisional (quarantined for audit).
                try:
                    _pmc_bound, _pmc_not_bound = _fix2d60_split_schema_bound_only(primary_data.get('primary_metrics_canonical') or {})
                    if isinstance(_pmc_bound, dict):
                        primary_data['primary_metrics_canonical'] = _pmc_bound
                    if isinstance(_pmc_not_bound, dict) and _pmc_not_bound:
                        _prov = primary_data.get('primary_metrics_provisional')
                        if not isinstance(_prov, dict):
                            _prov = {}
                        # merge (schema-bound rule is stronger than any earlier provisional split)
                        for _k, _v in _pmc_not_bound.items():
                            _prov[_k] = _v
                        primary_data['primary_metrics_provisional'] = _prov
                except Exception:
                    pass

                # 3) attribution using frozen schema ✅
                if primary_data.get("primary_metrics_canonical"):
                    primary_data["primary_metrics_canonical"] = add_range_and_source_attribution_to_canonical_metrics(
                        primary_data.get("primary_metrics_canonical", {}),
                        web_context,
                        metric_schema=(primary_data.get("metric_schema_frozen") or {}),
                    )

                try:
                    fn = globals().get("apply_schema_validation_and_evidence_gating")
                    if callable(fn):
                        primary_data = fn(primary_data)
                except Exception:
                    pass

            except Exception:
                pass

            # Hash key findings (optional)
            try:
                if primary_data.get("key_findings"):
                    findings_with_hash = []
                    for finding in primary_data.get("key_findings", []):
                        if finding:
                            findings_with_hash.append({
                                "text": finding,
                                "semantic_hash": compute_semantic_hash(finding)
                            })
                    primary_data["key_findings_hashed"] = findings_with_hash
            except Exception:
                pass


            # Save baseline numeric cache if available (existing behavior)

            # Build output
            output = {
                "question": query,
                "question_profile": question_profile,
                "question_category": question_profile.get("category"),
                "question_signals": question_signals,
                "side_questions": question_profile.get("side_questions", []),
                "timestamp": _yureeka_now_iso_utc(),
                "primary_response": primary_data,
                "final_confidence": final_conf,
                "veracity_scores": veracity_scores,
                "web_sources": web_context.get("sources", []),
                "code_version": _yureeka_get_code_version(),
            "authority_manifest_v1": _yureeka_authority_manifest_v1(),
                }


            try:
                output.setdefault("debug", {})
                if isinstance(output.get("debug"), dict):
                    output["debug"].setdefault("runtime_identity_v1", _yureeka_runtime_identity_v1())
            except Exception:
                pass


            try:
                if isinstance(output.get("primary_response"), dict):
                    output["primary_response"]["code_version"] = _yureeka_get_code_version()
            except Exception:
                pass


            # ✅ NEW: attach analysis-aligned snapshots (from scraped_meta)
            # This is the stable cache evolution should reuse.
            try:
                output = attach_source_snapshots_to_analysis(output, web_context)
            except Exception:
                pass

            # Why:
            # - Evolution diffing requires previous_data.primary_metrics_canonical to exist and be schema-keyed.
            # - Some analysis paths seed metric_schema_frozen but do not emit primary_metrics_canonical into the
            #   persisted payload. This causes Evolution to report no_prev_metrics even when keys are stable.
            # What:
            # - If primary_metrics_canonical is missing/empty, rebuild it deterministically from the frozen schema
            #   using the same authoritative rebuild helper used by Evolution.
            # - Write into BOTH top-level and results for maximum persistence compatibility.
            try:
                _pmc0 = output.get('primary_metrics_canonical')
                _pmc0_empty = (not isinstance(_pmc0, dict)) or (not _pmc0)
                if _pmc0_empty:
                    fn_rebuild = globals().get('rebuild_metrics_from_snapshots_analysis_canonical_v1')
                    if callable(fn_rebuild):
                        _rebuilt = fn_rebuild(output.get('primary_response') or {}, output.get('baseline_sources_cache') or [], web_context=web_context)
                        if isinstance(_rebuilt, dict) and _rebuilt:
                            output['primary_metrics_canonical'] = _rebuilt
                            output.setdefault('results', {})
                            if isinstance(output.get('results'), dict):
                                output['results']['primary_metrics_canonical'] = _rebuilt
                            # lightweight debug stamp
                            try:
                                output.setdefault('debug', {})
                                if isinstance(output.get('debug'), dict):
                                    output['debug']['fix2d72_analysis_pmc_written'] = True
                                    output['debug']['fix2d72_pmc_key_count'] = len(_rebuilt)
                            except Exception:
                                pass
            except Exception:
                pass

            with st.spinner("💾 Saving to history..."):
                if add_to_history(output):
                    st.success("✅ Analysis saved to Google Sheets")
                else:
                    st.warning("⚠️ Saved to session only (Google Sheets unavailable)")

            json_bytes = json.dumps(output, indent=2, ensure_ascii=False).encode("utf-8")
            filename = f"yureeka_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

            st.download_button(
                label="💾 Download Analysis JSON",
                data=json_bytes,
                file_name=filename,
                mime="application/json"
            )

            render_dashboard(
            primary_data,
            final_conf,
            web_context,
            base_conf,
            query,
            veracity_scores,
            web_context.get("source_reliability", [])
            )


            with st.expander("🔧 Debug Information"):
                st.write("**Confidence Breakdown:**")
                st.json({
                    "base_confidence": base_conf,
                    "evidence_score": veracity_scores.get("overall", 0),
                    "final_confidence": final_conf,
                    "veracity_breakdown": veracity_scores
                })
                st.write("**Primary Model Response:**")
                st.json(primary_data)

    with tab2:
        st.markdown("""
        ### 📈 Track the evolution of key metrics over time using **deterministic source-anchored analysis**.

        **How it works:**
        - Select a baseline from your history (stored in Google Sheets)
        - Re-fetches the **exact same sources** from that analysis
        - Extracts current numbers using regex (no LLM variance)
        - Computes deterministic diffs with context-aware matching
        """)

        with st.sidebar:
            st.subheader("📚 History")

            if st.button("🔄 Refresh"):
                st.cache_resource.clear()
                st.rerun()

            sheet = get_google_sheet()
            if sheet:
                st.success("✅ Google Sheets connected")
            else:
                st.warning("⚠️ Using session storage")

            # - Streamlit Cloud UI has no free-text question editing (dropdown-only).
            # - This toggle lets you intentionally bypass the unchanged fastpath so you
            #   can validate the rebuild path + FIX39 publish invariants.
            # - Pure UI flag; no logic changes unless explicitly enabled.
            force_rebuild = st.checkbox(
                "🧪 Force rebuild (ignore snapshot fastpath)",
                value=False,
                key="fix41_force_rebuild_toggle",
                help="Debug only: forces evolution to rebuild even if sources+data are unchanged."
            )

        # ✅ REFACTOR99: Evolution baseline MUST be an Analysis payload (exclude evolution reports) and default to latest.
        history_all = get_history() or []

        def _r99_is_analysis_payload(h: dict) -> bool:
            try:
                if not isinstance(h, dict):
                    return False
                # Evolution reports use analysis_type='source_anchored' at top-level
                if str(h.get("analysis_type") or "").strip().lower() in ("source_anchored", "evolution"):
                    return False
                # If it looks like an evolution report shape (minimal wrapper with results+interpretation), exclude
                if ("interpretation" in h) and ("primary_response" not in h) and (h.get("analysis_type") == "source_anchored"):
                    return False

                ms = (
                    h.get("metric_schema_frozen")
                    or (h.get("primary_response") or {}).get("metric_schema_frozen")
                    or (h.get("results") or {}).get("metric_schema_frozen")
                )
                if isinstance(ms, dict) and ms:
                    return True

                # REFACTOR103:
                # The latest Analysis row in Sheet1 may be a sheets-safe / truncated wrapper (no schema),
                # but it is still a valid baseline if it can be deterministically rehydrated from HistoryFull
                # via full_store_ref and/or _sheet_id.
                ref = (
                    h.get("full_store_ref")
                    or (h.get("results") or {}).get("full_store_ref")
                    or (h.get("primary_response") or {}).get("full_store_ref")
                )
                if isinstance(ref, str) and ref:
                    return True
                if h.get("_sheet_id"):
                    return True

                # Snapshot pointers also allow deterministic rehydration paths.
                if (
                    h.get("snapshot_store_ref")
                    or (h.get("results") or {}).get("snapshot_store_ref")
                    or h.get("source_snapshot_hash")
                    or (h.get("results") or {}).get("source_snapshot_hash")
                ):
                    return True

                return False
            except Exception:
                return False

        history = [h for h in history_all if _r99_is_analysis_payload(h)]
        # Sort newest-first so the default selection is the most recent baseline.
        def _r99_ts_key(h: dict):
            try:
                # Prefer top-level timestamp, but fall back to nested results/primary_response timestamps.
                _t = (
                    (h.get("timestamp") if isinstance(h, dict) else "")
                    or ((h.get("results") or {}).get("timestamp") if isinstance(h.get("results"), dict) else "")
                    or ((h.get("primary_response") or {}).get("timestamp") if isinstance(h.get("primary_response"), dict) else "")
                    or ""
                )
                _dt = _parse_iso_dt(_t) if _t else None
                return _dt.timestamp() if _dt else 0.0
            except Exception:
                return 0.0

        history.sort(key=_r99_ts_key, reverse=True)

        # REFACTOR104: Prefer in-session last_analysis as newest baseline when Sheets History is stale/cached.
        _r104_session_last = None
        try:
            _r104_session_last = st.session_state.get("last_analysis")
        except Exception:
            _r104_session_last = None

        def _r104_parse_ts(_obj: dict):
            try:
                if isinstance(_obj, dict):
                    _t = _obj.get("timestamp") or (_obj.get("results") or {}).get("timestamp") or ""
                    _dt = _parse_iso_dt(_t) if _t else None
                    return (_t, _dt.timestamp() if _dt else 0.0)
            except Exception:
                pass
            return ("", 0.0)

        try:
            if isinstance(_r104_session_last, dict):
                _sess_t, _sess_ts = _r104_parse_ts(_r104_session_last)
                _hist_t, _hist_ts = _r104_parse_ts(history[0]) if history else ("", 0.0)
                _sess_is_newer = bool(_sess_ts) and (_sess_ts > (_hist_ts or 0.0) + 0.5)
                if (not history) or _sess_is_newer:
                    # De-dup by timestamp to avoid doubles when Sheets has already refreshed.
                    _dedup = []
                    for _h in (history or []):
                        try:
                            _t, _ = _r104_parse_ts(_h)
                            if _t and _sess_t and _t == _sess_t:
                                continue
                        except Exception:
                            pass
                        _dedup.append(_h)
                    history = [_r104_session_last] + _dedup
                    try:
                        history.sort(key=_r99_ts_key, reverse=True)
                    except Exception:
                        pass
        except Exception:
            pass

        if not history:
            st.info("📭 No previous analyses found. Run an analysis in the 'New Analysis' tab first.")
            return

        baseline_options = [
            f"{i+1}. {h.get('question', 'N/A')}  ({h.get('timestamp', '')})"
            for i, h in enumerate(history)
        ]
        _r104_baseline_select_key = "baseline_select"
        try:
            _r104_baseline_select_key = f"baseline_select_{(history[0].get('timestamp') or '')}"
        except Exception:
            pass
        # Default to the most recent baseline (index=0 because list is newest-first).
        baseline_choice = st.selectbox("Select baseline analysis:", baseline_options, index=0, key=_r104_baseline_select_key)
        baseline_idx = int(baseline_choice.split(".")[0]) - 1
        baseline_data = history[baseline_idx]
        # REFACTOR106: Final guard against stale baseline dropdown state.
        # If user leaves the selector on default (index 0) but we have a newer Analysis payload
        # for the SAME question (in-session last_analysis or freshly-read Sheet1), use it.
        _r106_autobump_v1 = {"attempted": False, "did_autobump": False}
        try:
            _r106_autobump_v1["attempted"] = True
            def _r106_norm_q(_q):
                try:
                    import re as _re
                    return _re.sub(r"\s+", " ", str(_q or "").strip().lower())
                except Exception:
                    return str(_q or "").strip().lower()
            def _r106_is_analysis_like(_obj):
                """Loose-but-safe predicate for 'can be used as an Analysis baseline'.

                Why:
                - Some Analysis payloads store primary_metrics_canonical under primary_response.
                - Some rows are sheets-safe/truncated wrappers (no schema/pmc) but are still deterministically rehydratable
                  via full_store_ref/_sheet_id/snapshot refs (HistoryFull).
                - Evolution baseline selection/autobump must treat these as valid so we don't fall back to older snapshots.
                """
                try:
                    if not isinstance(_obj, dict):
                        return False

                    # Exclude Evolution-shaped payloads.
                    if str(_obj.get("analysis_type") or "").strip().lower() in ("source_anchored", "evolution"):
                        return False

                    _q = _obj.get("question") or ((_obj.get("results") or {}).get("question") if isinstance(_obj.get("results"), dict) else "") or ""
                    _t = _obj.get("timestamp") or ((_obj.get("results") or {}).get("timestamp") if isinstance(_obj.get("results"), dict) else "") or ""
                    if not (str(_q).strip() and str(_t).strip()):
                        return False

                    _schema = (
                        _obj.get("metric_schema_frozen")
                        or ((_obj.get("primary_response") or {}).get("metric_schema_frozen") if isinstance(_obj.get("primary_response"), dict) else None)
                        or ((_obj.get("results") or {}).get("metric_schema_frozen") if isinstance(_obj.get("results"), dict) else None)
                    )
                    if isinstance(_schema, dict) and _schema:
                        return True

                    _pmc = (
                        _obj.get("primary_metrics_canonical")
                        or ((_obj.get("primary_response") or {}).get("primary_metrics_canonical") if isinstance(_obj.get("primary_response"), dict) else None)
                        or ((_obj.get("results") or {}).get("primary_metrics_canonical") if isinstance(_obj.get("results"), dict) else None)
                    )
                    if isinstance(_pmc, dict) and _pmc:
                        return True

                    # Rehydratable wrappers: treat as valid baselines (HistoryFull).
                    _ref = (
                        _obj.get("full_store_ref")
                        or ((_obj.get("results") or {}).get("full_store_ref") if isinstance(_obj.get("results"), dict) else "")
                        or ((_obj.get("primary_response") or {}).get("full_store_ref") if isinstance(_obj.get("primary_response"), dict) else "")
                        or ""
                    )
                    if isinstance(_ref, str) and _ref.strip():
                        return True
                    if _obj.get("_sheet_id"):
                        return True
                    if (
                        _obj.get("snapshot_store_ref")
                        or ((_obj.get("results") or {}).get("snapshot_store_ref") if isinstance(_obj.get("results"), dict) else "")
                        or _obj.get("source_snapshot_hash")
                        or ((_obj.get("results") or {}).get("source_snapshot_hash") if isinstance(_obj.get("results"), dict) else "")
                    ):
                        return True

                    return False
                except Exception:
                    return False
            if int(baseline_idx or 0) == 0:
                _sel_q = _r106_norm_q((baseline_data or {}).get("question") or (((baseline_data or {}).get("results") or {}).get("question")))
                _sel_ts = _parse_iso_dt((baseline_data or {}).get("timestamp") or (((baseline_data or {}).get("results") or {}).get("timestamp")))
                # 1) Prefer in-session last_analysis if it's analysis-like and newer for same question
                _sess_last = None
                try:
                    _sess_last = st.session_state.get("last_analysis")
                except Exception:
                    _sess_last = None
                if _r106_is_analysis_like(_sess_last):
                    _sess_q = _r106_norm_q(_sess_last.get("question") or ((_sess_last.get("results") or {}).get("question")))
                    _sess_ts = _parse_iso_dt(_sess_last.get("timestamp") or ((_sess_last.get("results") or {}).get("timestamp")))
                    _r106_autobump_v1.update({"selected_timestamp": str((baseline_data or {}).get("timestamp") or ""), "session_last_timestamp": str(_sess_last.get("timestamp") or "")})
                    if _sel_q and _sess_q and _sel_q == _sess_q and _sess_ts and (not _sel_ts or _sess_ts > _sel_ts):
                        baseline_data = _sess_last
                        _r106_autobump_v1["did_autobump"] = True
                        _r106_autobump_v1["reason"] = "session_last_analysis_newer_same_question_default_selection"
                # 2) If still stale, do a direct (non-cached) Sheet1 scan for the newest matching Analysis
                if not _r106_autobump_v1.get("did_autobump") and _sel_q:
                    try:
                        _sheet_direct = get_google_sheet()
                    except Exception:
                        _sheet_direct = None
                    if _sheet_direct:
                        try:
                            _rows = _sheet_direct.get_all_values() or []
                        except Exception:
                            _rows = []
                        _best = None
                        _best_ts = None
                        try:
                            for _r in (_rows[1:] if len(_rows) > 1 else []):
                                if not isinstance(_r, list) or len(_r) < 5:
                                    continue
                                _q = _r106_norm_q(_r[2] if len(_r) > 2 else "")
                                if not _q or _q != _sel_q:
                                    continue
                                _payload_raw = _r[4]
                                if not isinstance(_payload_raw, str) or not _payload_raw.strip():
                                    continue
                                try:
                                    _obj = json.loads(_payload_raw)
                                except Exception:
                                    continue
                                if not _r106_is_analysis_like(_obj):
                                    continue
                                _ts = _parse_iso_dt(_obj.get("timestamp") or ((_obj.get("results") or {}).get("timestamp")))
                                if not _ts:
                                    continue
                                if _best_ts is None or _ts > _best_ts:
                                    _best_ts = _ts
                                    _best = _obj
                            if _best and _best_ts and (not _sel_ts or _best_ts > _sel_ts):
                                baseline_data = _best
                                _r106_autobump_v1["did_autobump"] = True
                                _r106_autobump_v1["reason"] = "direct_sheet_scan_newer_same_question_default_selection"
                                _r106_autobump_v1["sheet_scan_newest_timestamp"] = str(_best.get("timestamp") or "")
                        except Exception:
                            pass
        except Exception as _e:
            try:
                _r106_autobump_v1["error"] = str(_e)
            except Exception:
                pass

        compare_method = st.selectbox(
            "Comparison method:",
            [
                "source-anchored evolution (re-fetch same sources)",
                "another saved analysis (deterministic)",
                "fresh analysis (volatile)"
            ]
        )

        extra_sources_text = st.text_area(
            "Extra source URLs (optional, one per line)",
            placeholder="https://example.com/report\nhttps://another-source.com/page",
            help="Adds these URLs to the admitted source list for this run. Useful to test hash-mismatch rebuilds.",
            height=110,
        )

        compare_data = None
        if "another saved analysis" in compare_method:
            compare_options = [
                f"{i+1}. {h.get('question', 'N/A')}  ({h.get('timestamp', '')})"
                for i, h in enumerate(history) if i != baseline_idx
            ]
            if compare_options:
                compare_choice = st.selectbox("Select comparison analysis:", compare_options)
                compare_idx = int(compare_choice.split(".")[0]) - 1
                compare_data = history[compare_idx]
            else:
                st.warning("No other saved analyses to compare with.")

        st.markdown("---")

        if st.button("🧬 Run Evolution Analysis", type="primary"):

            if "source-anchored evolution" in compare_method:
                evolution_query = baseline_data.get("question", "")
                if not evolution_query:
                    st.error("❌ No question found in baseline.")
                    return

                with st.spinner("🧬 Running source-anchored evolution..."):

                    try:


                        _evo_run_id = _inj_diag_make_run_id("evo")

                        _extra_urls_evo = []

                        try:

                            for _l in str(extra_sources_text or "").splitlines():

                                _u = _l.strip()

                                if not _u:

                                    continue

                                if _u.startswith("http://") or _u.startswith("https://"):

                                    _extra_urls_evo.append(_u)

                        except Exception:
                            pass

                            _extra_urls_evo = []



                        _diag_baseline_freshness_v1 = {}
                        try:
                            _b_ts = (baseline_data or {}).get("timestamp") or ((baseline_data or {}).get("results") or {}).get("timestamp") or ""
                            _s_last = None
                            try:
                                _s_last = st.session_state.get("last_analysis")
                            except Exception:
                                _s_last = None
                            _s_ts = ""
                            if isinstance(_s_last, dict):
                                _s_ts = _s_last.get("timestamp") or (_s_last.get("results") or {}).get("timestamp") or ""
                            _diag_baseline_freshness_v1 = {
                                "baseline_selected_timestamp": str(_b_ts or ""),
                                "session_last_analysis_timestamp": str(_s_ts or ""),
                                "baseline_is_session_last": bool(_b_ts and _s_ts and str(_b_ts) == str(_s_ts)),
                                "history_newest_timestamp": str((history[0].get("timestamp") if isinstance(history, list) and history and isinstance(history[0], dict) else "") or ""),
                                "baseline_autobump_v1": (_r106_autobump_v1 if isinstance(_r106_autobump_v1, dict) else {}),
                            }
                        except Exception:
                            _diag_baseline_freshness_v1 = {}

                        results = run_source_anchored_evolution(

                            baseline_data,

                            web_context={

                                "force_rebuild": bool(force_rebuild),

                                "extra_urls": _extra_urls_evo,

                                "diag_run_id": _evo_run_id,

                                "diag_extra_urls_ui_raw": (extra_sources_text or ""),

                                "diag_baseline_freshness_v1": _diag_baseline_freshness_v1,

                            },

                        )


                    except Exception as e:

                        st.error(f"❌ Evolution failed: {e}")

                        return


                interpretation = ""
                try:
                    if results and isinstance(results, dict):
                        interpretation = results.get("interpretation", "") or ""
                except Exception:
                    pass
                    interpretation = ""


                # REFACTOR25: Analysis→Evolution timing delta (production only)
                # - Standardize timestamps to UTC with offset (+00:00)
                # - Compute/stamp run_timing_v1 in Evolution results
                # REFACTOR99: record which baseline payload was selected (helps detect accidental evolution-as-baseline)
                try:
                    if isinstance(results, dict):
                        _dbg = results.get("debug") if isinstance(results.get("debug"), dict) else None
                        if _dbg is None:
                            results["debug"] = {}
                            _dbg = results["debug"]
                        _pmc = _refactor89_locate_pmc_dict(baseline_data) if isinstance(baseline_data, dict) else {}
                        _ms = (baseline_data or {}).get("metric_schema_frozen") or ((baseline_data or {}).get("primary_response") or {}).get("metric_schema_frozen") or ((baseline_data or {}).get("results") or {}).get("metric_schema_frozen") or {}
                        _dbg["baseline_selector_v1"] = {
                            "selected_timestamp": (baseline_data or {}).get("timestamp"),
                            "selected_code_version": (baseline_data or {}).get("code_version"),
                            "selected_analysis_type": (baseline_data or {}).get("analysis_type"),
                            "baseline_pmc_count": int(len(_pmc) if isinstance(_pmc, dict) else 0),
                            "schema_key_count": int(len(_ms) if isinstance(_ms, dict) else 0),
                            "autobump_v1": (_r106_autobump_v1 if isinstance(_r106_autobump_v1, dict) else {}),
                        }
                except Exception:
                    pass

                # - Attach per-row delta fields; blank when current metric is injected-sourced
                _analysis_ts_raw = None
                _analysis_ts_norm = None
                _evo_ts = _yureeka_now_iso_utc()
                try:
                    # REFACTOR115: Use the effective baseline timestamp from the snapshot actually selected.
                    # Prefer Evolution output/selector fields over stale embedded baseline_data.timestamp.
                    _prev_ts_field = None
                    try:
                        _prev_ts_field = (results or {}).get("previous_timestamp")
                    except Exception:
                        _prev_ts_field = None
                    if not _prev_ts_field:
                        try:
                            _prev_ts_field = _first_present(results or {}, [
                                ["debug", "prev_snapshot_pick_v1", "selected_timestamp"],
                                ["results", "debug", "prev_snapshot_pick_v1", "selected_timestamp"],
                                ["debug", "baseline_selector_v1", "selected_timestamp"],
                                ["results", "debug", "baseline_selector_v1", "selected_timestamp"],
                            ], default=None)
                        except Exception:
                            _prev_ts_field = None

                    _raw_previous_data_timestamp = None
                    try:
                        _raw_previous_data_timestamp = (previous_data or {}).get("timestamp")
                    except Exception:
                        _raw_previous_data_timestamp = None

                    _analysis_ts_raw = _prev_ts_field or _raw_previous_data_timestamp or (baseline_data or {}).get("timestamp")
                    _dt_a = _parse_iso_dt(_analysis_ts_raw) if _analysis_ts_raw else None
                    _analysis_ts_norm = _dt_a.isoformat() if _dt_a else (_analysis_ts_raw or None)
                except Exception:
                    _analysis_ts_norm = _analysis_ts_raw or None

                _delta_seconds = None
                _delta_human = ""
                _delta_warnings = []
                try:
                    _dt_a2 = _parse_iso_dt(_analysis_ts_norm) if _analysis_ts_norm else None
                    _dt_e2 = _parse_iso_dt(_evo_ts) if _evo_ts else None
                    if _dt_a2 and _dt_e2:
                        _ds = (_dt_e2 - _dt_a2).total_seconds()
                        if _ds < 0:
                            _delta_warnings.append("delta_negative_clamped_to_zero")
                            _ds = 0.0
                        _delta_seconds = float(_ds)
                        _delta_human = _yureeka_humanize_seconds_v1(_delta_seconds)
                    else:
                        _delta_warnings.append("delta_uncomputed_missing_timestamp")
                except Exception:
                    _delta_warnings.append("delta_uncomputed_exception")
                # REFACTOR115: Do not suppress run-level Δt in injection runs.
                # Row-level suppression is handled during metric_changes hydration when a row is sourced from injected URLs.
                try:
                    _inj_probe = _refactor115_collect_injection_urls_v1(results or {}, web_context or {})
                    if isinstance(_inj_probe, list) and _inj_probe:
                        _delta_warnings.append("injection_present_row_delta_suppression_only")
                except Exception:
                    pass

                # REFACTOR115 beacon: effective timing computed from selected baseline snapshot
                try:
                    if isinstance(results, dict):
                        _dbg_eff = results.get("debug")
                        if not isinstance(_dbg_eff, dict):
                            _dbg_eff = {}
                            results["debug"] = _dbg_eff
                        _dbg_eff["run_timing_effective_v2"] = {
                            "raw_previous_data_timestamp": (previous_data or {}).get("timestamp") if isinstance(previous_data, dict) else None,
                            "previous_timestamp_field": _prev_ts_field if "_prev_ts_field" in locals() else None,
                            "analysis_timestamp_effective": _analysis_ts_norm,
                            "evolution_timestamp": _evo_ts,
                            "delta_seconds_effective": _delta_seconds,
                            "delta_human_effective": _delta_human,
                        }
                except Exception:
                    pass


# Attach run timing to Evolution results (debug + non-debug copy)
                try:
                    if isinstance(results, dict):
                        _dbg = results.get("debug")
                        if not isinstance(_dbg, dict):
                            _dbg = {}
                            results["debug"] = _dbg
                        _dbg["run_timing_v1"] = {
                            "analysis_timestamp": _analysis_ts_norm,
                            "evolution_timestamp": _evo_ts,
                            "delta_seconds": _delta_seconds,
                            "delta_human": _delta_human,
                            "warnings": list(_delta_warnings),
                        }
                        results["run_delta_seconds"] = _delta_seconds
                        results["run_delta_human"] = _delta_human
                        if isinstance(results.get("results"), dict):
                            results["results"]["run_delta_seconds"] = _delta_seconds
                            results["results"]["run_delta_human"] = _delta_human
                            _dbg2 = results["results"].get("debug")
                            if not isinstance(_dbg2, dict):
                                _dbg2 = {}
                                results["results"]["debug"] = _dbg2
                            _dbg2["run_timing_v1"] = dict(_dbg["run_timing_v1"])
                except Exception:
                    pass


                try:
                    def _refactor80_is_injection_active(_res: dict) -> bool:
                        """Return True only when the user provided injection URLs (ui_norm/intake_norm).
                        Note: do NOT treat admitted production sources as injection."""
                        try:
                            if not isinstance(_res, dict):
                                return False
                            _dbg = _res.get("debug") if isinstance(_res.get("debug"), dict) else {}
                            _it = _dbg.get("inj_trace_v1") if isinstance(_dbg.get("inj_trace_v1"), dict) else {}
                            _counts = _it.get("counts") if isinstance(_it.get("counts"), dict) else {}
                            for _k in ("ui_norm", "intake_norm"):
                                try:
                                    if int(_counts.get(_k) or 0) > 0:
                                        return True
                                except Exception:
                                    pass
                            for _k in ("ui_norm", "intake_norm"):
                                _lst = _it.get(_k)
                                if isinstance(_lst, list) and any(isinstance(x, str) and x.strip() for x in _lst):
                                    return True
                        except Exception:
                            return False
                        return False

                    _is_inj = _refactor80_is_injection_active(results)

                    # Always record the flag truthfully (prevents false-positive harness banners)
                    try:
                        _dbg_rt = results.get("debug") if isinstance(results.get("debug"), dict) else {}
                        _rt = _dbg_rt.get("run_timing_v1") if isinstance(_dbg_rt.get("run_timing_v1"), dict) else None
                        if isinstance(_rt, dict):
                            _rt["suppressed_by_injection"] = False
                            _rt["injection_present_v2"] = bool(_refactor115_collect_injection_urls_v1(results or {}, web_context or {}))
                    except Exception:
                        pass
                except Exception:
                    pass

# Add per-row delta fields (production only; blank if injected)
                try:
                    # REFACTOR115: robust injection URL set (supports legacy debug fields)
                    _inj_urls = _refactor115_collect_injection_urls_v1(results or {}, web_context or {})
                    _inj_norm = None
                    try:
                        _inj_norm = _inj_diag_norm_url_list(_inj_urls)
                    except Exception:
                        _inj_norm = None
                    _inj_set = set([str(u).strip() for u in (_inj_norm or _inj_urls) if isinstance(u, str) and u.strip()])

                    def _refactor25_extract_metric_source_url(_m: dict):
                        try:
                            return _refactor26_extract_metric_source_url_v1(_m)
                        except Exception:
                            return None

                    _pmc = {}
                    if isinstance(results, dict):
                        _pmc = results.get("primary_metrics_canonical") or {}
                        if (not isinstance(_pmc, dict)) and isinstance(results.get("results"), dict):
                            _pmc = results["results"].get("primary_metrics_canonical") or {}
                    if not isinstance(_pmc, dict):
                        _pmc = {}

                                        # REFACTOR29: collect per-row gating stats for injection/production delta display
                    _row_delta_gating = {
                        "inj_set_size": len(_inj_set),
                        "inj_urls_sample": list(list(_inj_set)[:3]),
                        "rows_total": 0,
                        "injected_rows_total": 0,
                        "injected_rows_blank_delta": 0,
                        "production_rows_total": 0,
                        "production_rows_with_delta": 0,
                        "rows_with_source_url": 0,
                        "rows_missing_source_url": 0,
                        "rows_suppressed_by_injection": 0,
                        "unattributed_rows": 0,
                        "duplicate_rows_skipped": 0,
                    }

                    _seen_ck = set()

                    def _apply_delta_to_rows(_rows: list):
                        if not isinstance(_rows, list):
                            return
                        for _r in _rows:
                            if not isinstance(_r, dict):
                                continue

                            _ckey_for_count = None
                            try:
                                _ckey_for_count = _r.get("canonical_key")
                            except Exception:
                                _ckey_for_count = None

                            _uniq_key = _ckey_for_count if (isinstance(_ckey_for_count, str) and _ckey_for_count) else ("__row_%s" % (id(_r),))
                            _count_row = True
                            try:
                                if _uniq_key in _seen_ck:
                                    _count_row = False
                                    try:
                                        _row_delta_gating["duplicate_rows_skipped"] += 1
                                    except Exception:
                                        pass
                                else:
                                    _seen_ck.add(_uniq_key)
                            except Exception:
                                pass

                            if _count_row:
                                try:
                                    _row_delta_gating["rows_total"] += 1
                                except Exception:
                                    pass

                            is_injected = False
                            _ckey = _r.get("canonical_key")

                            # REFACTOR80: always count source-attribution (even when injection set is empty)
                            _su = None
                            try:
                                _su = _refactor26_extract_row_current_source_url_v1(_r)
                            except Exception:
                                _su = None
                            if _su is None:
                                _cm = _pmc.get(_ckey) if isinstance(_ckey, str) else None
                                _su = _refactor25_extract_metric_source_url(_cm) if isinstance(_cm, dict) else None

                            if _su is None:
                                # Cannot attribute -> treat as production (do not suppress)
                                if _count_row:
                                    try:
                                        _row_delta_gating["unattributed_rows"] += 1
                                        _row_delta_gating["rows_missing_source_url"] += 1
                                    except Exception:
                                        pass
                                is_injected = False
                            else:
                                if _count_row:
                                    try:
                                        _row_delta_gating["rows_with_source_url"] += 1
                                    except Exception:
                                        pass
                                if _inj_set:
                                    _su_norm = _su
                                    try:
                                        _tmp = _inj_diag_norm_url_list([_su])
                                        if isinstance(_tmp, list) and _tmp:
                                            _su_norm = str(_tmp[0] or _su).strip()
                                    except Exception:
                                        _su_norm = _su
                                    is_injected = (_su_norm in _inj_set)

                            if _count_row:
                                try:
                                    if is_injected:
                                        _row_delta_gating["injected_rows_total"] += 1
                                        _row_delta_gating["rows_suppressed_by_injection"] += 1
                                    else:
                                        _row_delta_gating["production_rows_total"] += 1
                                except Exception:
                                    pass

                            if (not is_injected) and (_delta_human or _delta_seconds is not None):
                                _r["analysis_evolution_delta_human"] = _delta_human
                                _r["analysis_evolution_delta_seconds"] = _delta_seconds
                                try:
                                    _row_delta_gating["production_rows_with_delta"] += 1
                                except Exception:
                                    pass
                            else:
                                _r["analysis_evolution_delta_human"] = ""
                                _r["analysis_evolution_delta_seconds"] = None
                                if is_injected and _count_row:
                                    try:
                                        _row_delta_gating["injected_rows_blank_delta"] += 1
                                    except Exception:
                                        pass

                    # REFACTOR30: avoid double-counting by applying delta stamping once, then propagating
                    _rows_a = results.get("metric_changes")
                    _rows_b = results.get("metric_changes_v2")
                    _primary_rows = _rows_a if isinstance(_rows_a, list) and _rows_a else (_rows_b if isinstance(_rows_b, list) else None)

                    # If metric_changes is missing/empty but v2 exists, alias metric_changes to v2 (UI reads metric_changes).
                    try:
                        if isinstance(results, dict) and isinstance(_primary_rows, list):
                            if (not isinstance(_rows_a, list)) or (len(_rows_a) == 0):
                                results["metric_changes"] = _primary_rows
                            if (not isinstance(_rows_b, list)) or (len(_rows_b) == 0):
                                results["metric_changes_v2"] = _primary_rows
                    except Exception:
                        pass

                    if isinstance(_primary_rows, list):
                        _apply_delta_to_rows(_primary_rows)

                    # Propagate delta fields to the other list (best-effort), keyed by canonical_key
                    try:
                        _rows_a2 = results.get("metric_changes")
                        _rows_b2 = results.get("metric_changes_v2")
                        if isinstance(_rows_a2, list) and isinstance(_rows_b2, list) and (_rows_a2 is not _rows_b2) and isinstance(_primary_rows, list):
                            _map = {}
                            for _pr in _primary_rows:
                                if isinstance(_pr, dict):
                                    _ck = _pr.get("canonical_key")
                                    if isinstance(_ck, str) and _ck and _ck not in _map:
                                        _map[_ck] = _pr
                            _secondary = _rows_b2 if _primary_rows is _rows_a2 else _rows_a2
                            for _sr in _secondary:
                                if not isinstance(_sr, dict):
                                    continue
                                _ck2 = _sr.get("canonical_key")
                                _src = _map.get(_ck2) if isinstance(_ck2, str) else None
                                if isinstance(_src, dict):
                                    _sr["analysis_evolution_delta_human"] = _src.get("analysis_evolution_delta_human") or ""
                                    _sr["analysis_evolution_delta_seconds"] = _src.get("analysis_evolution_delta_seconds")
                                else:
                                    _sr.setdefault("analysis_evolution_delta_human", "")
                                    _sr.setdefault("analysis_evolution_delta_seconds", None)
                    except Exception:
                        pass


                    # Harness / invariants (soft assertions + diagnostics)
                    try:
                        if isinstance(results, dict) and isinstance(results.get("debug"), dict):
                            rt = results["debug"].get("run_timing_v1")
                            if isinstance(rt, dict):
                                rt.setdefault("assertions", {})
                                rt["row_delta_gating_v2"] = dict(_row_delta_gating)
                                try:
                                    results["debug"]["row_delta_gating_v2"] = dict(_row_delta_gating)
                                except Exception:
                                    pass

                                if isinstance(rt.get("assertions"), dict):
                                    if (not _inj_set) and _analysis_ts_norm:
                                        rt["assertions"]["delta_computed_non_negative"] = bool((_delta_seconds is not None) and (float(_delta_seconds) >= 0))
                                    if _inj_set:
                                        rt["assertions"]["injected_rows_have_blank_delta"] = bool(
                                            _row_delta_gating.get("injected_rows_blank_delta", 0) == _row_delta_gating.get("injected_rows_total", 0)
                                        )
                                    if (_delta_seconds is not None) and (_row_delta_gating.get("production_rows_total", 0) > 0):
                                        rt["assertions"]["production_rows_have_delta"] = bool(
                                            _row_delta_gating.get("production_rows_with_delta", 0) == _row_delta_gating.get("production_rows_total", 0)
                                        )

                        # Keep nested results copy aligned (best-effort)
                        if isinstance(results, dict) and isinstance(results.get("results"), dict):
                            _dbg_nested = results["results"].get("debug")
                            if not isinstance(_dbg_nested, dict):
                                _dbg_nested = {}
                                results["results"]["debug"] = _dbg_nested
                            if isinstance(results.get("debug"), dict) and isinstance(results["debug"].get("run_timing_v1"), dict):
                                _dbg_nested["run_timing_v1"] = dict(results["debug"]["run_timing_v1"])
                    except Exception:
                        pass
                except Exception:
                    pass

                # REFACTOR116: Post-hoc Δt + per-row Δt stamping using the *selected* baseline snapshot timestamp.
                # This corrects stale previous_data.timestamp usage when evolution rehydrates a newer baseline snapshot.
                try:
                    _rf117_wrapper = {
                        "timestamp": _evo_ts,
                        "previous_timestamp": (
                            ((results or {}).get("debug") or {}).get("prev_snapshot_pick_v1", {}).get("selected_timestamp")
                            or _analysis_ts_norm
                        ),
                        "results": results,
                    }
                    _refactor116_apply_effective_timing_and_row_deltas_v1(
                        _rf117_wrapper,
                        (previous_data if "previous_data" in locals() else baseline_data),
                        web_context if "web_context" in locals() else {},
                    )
                except Exception:
                    pass

                evolution_output = {
                    "question": evolution_query,
                    "timestamp": _evo_ts,
                    "code_version": _yureeka_get_code_version(),
                    "analysis_type": "source_anchored",
                    "previous_timestamp": (
                        ((results or {}).get("debug") or {}).get("prev_snapshot_pick_v1", {}).get("selected_timestamp")
                        or _analysis_ts_norm
                    ),
                    "results": results,
                    "interpretation": {
                        "text": interpretation,
                        "authoritative": False,
                        "source": "llm_optional"
                    }
                }

                st.download_button(
                    label="💾 Download Evolution Report",
                    data=json.dumps(evolution_output, indent=2, ensure_ascii=False).encode("utf-8"),
                    file_name=f"yureeka_evolution_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json"
                )

                # ✅ FIX: guarded renderer to avoid stability_score=None formatting crashes
                render_source_anchored_results(results, evolution_query)

            elif "another saved analysis" in compare_method:
                if compare_data:
                    st.success("✅ Comparing two saved analyses (deterministic)")
                    render_native_comparison(baseline_data, compare_data)
                else:
                    st.error("❌ Please select a comparison analysis")

            else:
                st.warning("⚠️ Running fresh analysis - results may vary")

                query = baseline_data.get("question", "")
                if not query:
                    st.error("❌ No query found")
                    return

                with st.spinner("🌐 Fetching current data..."):
                    existing_snapshots = None

                    try:
                        prev = st.session_state.get("last_analysis")
                        if isinstance(prev, dict):
                            existing_snapshots = (
                                prev.get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("source_results")
                            )
                    except Exception:
                        pass
                        existing_snapshots = None

                    web_context = fetch_web_context(
                        query,
                        num_sources=3,
                        existing_snapshots=existing_snapshots,
                    )


                if not web_context:
                    web_context = {
                        "search_results": [],
                        "scraped_content": {},
                        "summary": "",
                        "sources": [],
                        "source_reliability": []
                    }

                with st.spinner("🤖 Running analysis..."):
                    new_response = query_perplexity(query, web_context)

                if new_response:
                    try:
                        new_parsed = json.loads(new_response)
                        veracity = evidence_based_veracity(new_parsed, web_context)
                        base_conf = float(new_parsed.get("confidence", 75))
                        final_conf = calculate_final_confidence(base_conf, veracity.get("overall", 0))

                        compare_data = {
                            "question": query,
                            "timestamp": _yureeka_now_iso_utc(),
                            "primary_response": new_parsed,
                            "final_confidence": final_conf,
                            "veracity_scores": veracity,
                            "web_sources": web_context.get("sources", [])
                        }

                        add_to_history(compare_data)
                        st.success("✅ Saved to history")

                        render_native_comparison(baseline_data, compare_data)
                    except Exception as e:
                        st.error(f"❌ Failed: {e}")
                else:
                    st.error("❌ Analysis failed")


# - Additive only: does not remove or refactor existing code.
# - Only applied in TAB 1 (New Analysis) via a small post-pass hook.
# - Does NOT alter evolution behavior (no changes to evolution functions).

def validate_metric_schema_frozen(metric_schema_frozen: dict) -> dict:
    """
    Validate frozen metric schema for internal consistency.
    Returns: {"ok": bool, "errors": [...], "warnings": [...], "by_key": {...}}
    """
    issues = {"ok": True, "errors": [], "warnings": [], "by_key": {}}

    def _add(kind: str, canonical_key: str, msg: str):
        issues["ok"] = issues["ok"] and (kind != "errors")
        issues[kind].append({"canonical_key": canonical_key, "message": msg})
        issues["by_key"].setdefault(canonical_key, {"errors": [], "warnings": []})
        issues["by_key"][canonical_key][kind].append(msg)

    if not isinstance(metric_schema_frozen, dict):
        _add("errors", "__schema__", "metric_schema_frozen missing or not a dict")
        return issues

    for canonical_key, spec in metric_schema_frozen.items():
        if not isinstance(spec, dict):
            _add("errors", canonical_key, "schema entry not a dict")
            continue

        dim = (spec.get("dimension") or spec.get("measure_kind") or "").lower().strip()
        unit = (spec.get("unit") or spec.get("unit_tag") or "").strip()
        unit_family = (spec.get("unit_family") or spec.get("unit_family_tag") or "").lower().strip()
        name = (spec.get("name") or canonical_key or "").lower()

        # Hard conflict: currency + percent
        if dim in ("currency", "revenue", "market_value", "value") and unit in ("%", "percent", "percentage"):
            _add("errors", canonical_key, "dimension=currency but unit is percent (%)")

        # Soft checks for percent metrics without percent unit
        if ("cagr" in name or dim in ("percent", "percentage", "growth_rate")) and unit and unit not in ("%", "percent", "percentage"):
            _add("warnings", canonical_key, f"percent-like metric but unit='{unit}' (expected '%')")

        # Common drift hazard: CAGR schema includes 'share'
        kw = " ".join([str(x) for x in (spec.get("keywords") or [])]).lower()
        if "cagr" in name and "share" in kw:
            _add("warnings", canonical_key, "CAGR schema keywords include 'share' (risk of mapping share% to CAGR)")

        # Unit family conflicts
        if dim == "currency" and unit_family and unit_family not in ("currency", "money"):
            _add("warnings", canonical_key, f"dimension=currency but unit_family='{unit_family}'")

    return issues


def _metric_evidence_list(metric: dict):
    ev = metric.get("evidence")
    if isinstance(ev, list):
        return ev
    return []


def _synthesize_evidence_from_examples(metric: dict, max_items: int = 5) -> list:
    """
    If metric has value_range.examples (from attribution pass), synthesize evidence records.
    This keeps JSON stable and makes evolution rebuild auditing possible.
    """
    examples = None
    vr = metric.get("value_range")
    if isinstance(vr, dict):
        examples = vr.get("examples")
    if not isinstance(examples, list) or not examples:
        return []

    # Try to use an existing anchor hash function if present
    anchor_fn = globals().get("compute_anchor_hash")
    out = []
    for ex in examples[:max_items]:
        if not isinstance(ex, dict):
            continue
        url = ex.get("source_url") or ex.get("url") or ""
        raw = ex.get("raw") or ""
        ctx = ex.get("context") or ex.get("context_window") or ex.get("snippet") or ""
        ah = ex.get("anchor_hash") or ""
        if not ah and callable(anchor_fn):
            try:
                ah = anchor_fn(url, ctx)
            except Exception:
                pass
                ah = ""
        out.append({
            "source_url": url,
            "raw": raw,
            "context_snippet": ctx[:500] if isinstance(ctx, str) else "",
            "anchor_hash": ah,
            "method": "value_range_examples",
        })
    return out


def ensure_metric_has_evidence(metric: dict) -> dict:
    """
    Evidence gating for a single metric:
    - If evidence already exists -> no change
    - Else synthesize from value_range.examples if available
    - Else mark as proxy (do not delete or zero the metric)
    """
    if not isinstance(metric, dict):
        return metric

    ev = _metric_evidence_list(metric)
    if ev:
        return metric

    synth = _synthesize_evidence_from_examples(metric)
    if synth:
        metric["evidence"] = synth
        return metric

    # No evidence at all: mark proxy (do not alter numeric payload)
    metric.setdefault("evidence", [])
    metric["is_proxy"] = True
    metric["proxy_type"] = "evidence_missing"
    metric["proxy_reason"] = "no_evidence_anchors_available"
    metric["proxy_confidence"] = float(metric.get("proxy_confidence") or 0.2)
    return metric


def enforce_evidence_gating(primary_metrics_canonical: dict) -> dict:
    """
    Apply evidence gating across canonical metrics.
    Returns the (mutated) dict for compatibility.
    """
    if not isinstance(primary_metrics_canonical, dict):
        return primary_metrics_canonical

    for k, m in list(primary_metrics_canonical.items()):
        if isinstance(m, dict):
            primary_metrics_canonical[k] = ensure_metric_has_evidence(m)

    return primary_metrics_canonical


def apply_schema_validation_and_evidence_gating(primary_data: dict) -> dict:
    """
    New Analysis post-pass hook:
    - validates metric_schema_frozen
    - evidence-gates primary_metrics_canonical
    - marks schema-conflict metrics as proxy (does not remove anything)
    """
    if not isinstance(primary_data, dict):
        return primary_data

    # Where schema is stored
    schema = (
        primary_data.get("metric_schema_frozen")
        or (primary_data.get("primary_response") or {}).get("metric_schema_frozen")
        or (primary_data.get("results") or {}).get("metric_schema_frozen")
        or {}
    )

    validation = validate_metric_schema_frozen(schema)
    primary_response = primary_data.setdefault("primary_response", {})
    primary_response["schema_validation"] = validation

    # Mark schema-conflict metrics as proxy (additive)
    pmc = primary_data.get("primary_metrics_canonical")
    if isinstance(pmc, dict) and validation.get("by_key"):
        for ck, iss in validation["by_key"].items():
            if ck in pmc and isinstance(pmc[ck], dict) and iss.get("errors"):
                pmc[ck]["is_proxy"] = True
                pmc[ck]["proxy_type"] = "schema_conflict"
                pmc[ck]["proxy_reason"] = "schema_validation_error"
                pmc[ck]["proxy_confidence"] = float(pmc[ck].get("proxy_confidence") or 0.15)
                pmc[ck]["schema_issues"] = {"errors": iss.get("errors", []), "warnings": iss.get("warnings", [])}

    # Evidence gating
    pmc2 = primary_data.get("primary_metrics_canonical")
    if isinstance(pmc2, dict):
        before = sum(1 for v in pmc2.values() if isinstance(v, dict) and _metric_evidence_list(v))
        enforce_evidence_gating(pmc2)
        after = sum(1 for v in pmc2.values() if isinstance(v, dict) and _metric_evidence_list(v))
        prox = sum(1 for v in pmc2.values() if isinstance(v, dict) and v.get("is_proxy"))
        primary_response["evidence_gating_summary"] = {
            "total_metrics": len(pmc2),
            "metrics_with_evidence_before": before,
            "metrics_with_evidence_after": after,
            "metrics_marked_proxy": prox,
        }

    return primary_data


def _normalize_prev_response_for_rebuild(previous_data):
    """Best-effort normalization of the loaded baseline object for rebuild dispatch.
    - If previous_data is a JSON string, parse it.
    - If it contains nested 'primary_response' as JSON string, parse it.
    - If it contains a top-level wrapper with 'data' or 'results', keep as dict.
    This is additive and only affects rebuild dispatch input normalization.
    """
    import json
    try:
        pd = previous_data
        if isinstance(pd, str) and pd.strip().startswith(("{","[")):
            try:
                pd = json.loads(pd)
            except Exception:
                pass
                pd = previous_data
        if isinstance(pd, dict):
            pr = pd.get("primary_response")
            if isinstance(pr, str) and pr.strip().startswith(("{","[")):
                try:
                    pd["primary_response"] = json.loads(pr)
                except Exception:
                    pass
            # Some callers store the main payload under 'data'
            d = pd.get("data")
            if isinstance(d, str) and d.strip().startswith(("{","[")):
                try:
                    pd["data"] = json.loads(d)
                except Exception:
                    return pd
    except Exception:
        return previous_data


if __name__ == "__main__":
    # REFACTOR35: main() invocation moved to end-of-file so all late refactor defs/overrides are loaded before any runs.
    pass


def _get_metric_anchors_any(prev_response: dict) -> dict:
    """Best-effort retrieval of metric_anchors from any plausible location (additive helper)."""
    try:
        if not isinstance(prev_response, dict):
            return {}
        for path in (
            ("metric_anchors",),
            ("results", "metric_anchors"),
            ("primary_response", "metric_anchors"),
            ("primary_response", "results", "metric_anchors"),
        ):
            cur = prev_response
            ok = True
            for k in path:
                if isinstance(cur, dict) and k in cur:
                    cur = cur[k]
                else:
                    ok = False
                    break
            if ok and isinstance(cur, dict) and cur:
                return cur
        return {}
    except Exception:
        return {}

def _coerce_prev_response_any(previous_data):
    """Normalize previous_data into a dict-shaped 'prev_response' for rebuild dispatch (additive helper)."""
    try:
        return previous_data if isinstance(previous_data, dict) else {}
    except Exception:
        return {}

# Goals (deterministic, no re-architecture):
#   1) De-year schema keyword scoring for non-year metrics
#   2) Hard unit expectation gating (unitless years can't win currency/percent)
#   3) Absolute anchor priority when anchors exist
# Notes:
#   - Additive only: we define FIX16 rebuild functions and re-wire dispatch
#   - No refetch, no heuristics beyond schema/unit/anchors

def _fix16_is_year_token(s: str) -> bool:
    try:
        s2 = str(s or "").strip()
        return bool(re.fullmatch(r"(19\d{2}|20\d{2})", s2))
    except Exception:
        return False


def _fix16_metric_is_year_like(metric_spec: dict, canonical_key: str = "") -> bool:
    """Deterministic allow-list for metrics whose value is genuinely a year."""
    try:
        spec = metric_spec or {}
        blob = " ".join([
            str(canonical_key or ""),
            str(spec.get("name") or ""),
            str(spec.get("canonical_key") or spec.get("canonical_id") or ""),
            " ".join([str(x) for x in (spec.get("keywords") or spec.get("keyword_hints") or []) if x]),
            str(spec.get("dimension") or ""),
        ]).lower()
        # year-ish intents
        return any(k in blob for k in (" year", "year_", "founded", "since", "established", "launch year", "model year"))
    except Exception:
        return False


def _fix16_prune_year_keywords(keywords: list, metric_is_year_like: bool) -> list:
    """Remove YYYY tokens from keyword scoring unless the metric is year-like."""
    try:
        if metric_is_year_like:
            return list(keywords or [])
        out = []
        for k in (keywords or []):
            if _fix16_is_year_token(k):
                continue
            out.append(k)
        return out
    except Exception:
        return list(keywords or [])


def _fix16_expected_dimension(metric_spec: dict) -> str:
    try:
        spec = metric_spec or {}
        dim = (spec.get("dimension") or spec.get("unit_family") or spec.get("expected_unit_family") or "").strip().lower()
        return dim
    except Exception:
        return ""


def _fix16_infer_dimension_from_canonical_key(canonical_key: str) -> str:
    """Infer an expected dimension when schema row is missing dimension/unit_family.
    Keeps REFACTOR02 behavior deterministic and blocks cross-dimension leakage.
    """
    try:
        ck = str(canonical_key or "").strip().lower()
        if not ck:
            return ""
        if "__percent" in ck or ck.endswith("_percent"):
            return "percent"
        if "__currency" in ck or ck.endswith("_currency"):
            return "currency"
        if "__unit_" in ck:
            return "magnitude"
        return ""
    except Exception:
        return ""


def _fix16_candidate_has_any_unit(c: dict) -> bool:
    try:
        if not isinstance(c, dict):
            return False
        for k in ("base_unit", "unit", "unit_tag", "unit_family"):
            if str(c.get(k) or "").strip():
                return True
        # raw sometimes carries $ or % even if unit field blank
        raw = str(c.get("raw") or "")
        if "$" in raw or "%" in raw:
            return True
        return False
    except Exception:
        return False


def _fix16_unit_compatible(c: dict, expected_dim: str) -> bool:
    """Hard gate: if schema expects a unit family/dimension, candidate must be compatible.

    Backward-compatible:
      - Some legacy call sites pass (metric_spec_dict, candidate_dict). In that case we swap.
    """
    try:
        if isinstance(expected_dim, dict) and isinstance(c, dict):
            spec_like = any(k in c for k in ("dimension", "unit_family", "expected_unit_family", "canonical_key", "name"))
            cand_like = any(k in expected_dim for k in ("raw", "value", "value_norm", "unit", "unit_tag", "unit_family", "base_unit"))
            if spec_like and cand_like:
                _candidate = expected_dim
                expected_dim = _fix16_expected_dimension(c)
                c = _candidate

        if not expected_dim:
            return True
        if not isinstance(c, dict):
            return False

        dim = str(expected_dim).strip().lower()

        # Normalize common synonyms
        if dim in ("magnitude", "count", "quantity", "units", "unit_count", "unit_sales", "number", "numbers", "volume"):
            dim = "magnitude"
        if dim in ("pct", "percentage"):
            dim = "percent"
        if dim in ("money",):
            dim = "currency"

        raw = str(c.get("raw") or "").lower()
        u = (c.get("base_unit") or c.get("unit") or c.get("unit_tag") or "").strip().lower()
        cand_fam = (c.get("unit_family") or "").strip().lower()

        def _has_percent_marker() -> bool:
            try:
                return ("%" in raw) or ("percent" in raw) or ("%" in u) or ("percent" in u)
            except Exception:
                return False

        def _has_currency_marker() -> bool:
            try:
                return (
                    ("$" in raw) or ("us$" in raw) or ("usd" in raw) or ("sgd" in raw) or ("eur" in raw) or ("gbp" in raw)
                    or ("aud" in raw) or ("cny" in raw) or ("jpy" in raw) or ("€" in raw) or ("£" in raw) or ("¥" in raw)
                    or ("$" in u) or ("usd" in u) or ("sgd" in u) or ("eur" in u) or ("gbp" in u) or ("€" in u) or ("£" in u) or ("¥" in u)
                )
            except Exception:
                return False

        if dim == "percent":
            if not _has_percent_marker():
                return False
            try:
                v = c.get("value") if c.get("value") is not None else c.get("value_norm")
                if isinstance(v, (int, float)):
                    iv = int(v)
                    if 1900 <= iv <= 2100 and abs(float(v) - float(iv)) < 1e-9:
                        # if it really is a percent, raw should contain an explicit '%'
                        if "%" not in raw:
                            return False
            except Exception:
                pass
            if cand_fam:
                return cand_fam == "percent"
            return True

        if dim == "currency":
            if not _has_currency_marker():
                return False
            if cand_fam:
                return cand_fam == "currency"
            return True

        if dim == "magnitude":
            if cand_fam in ("currency", "percent", "rate", "ratio"):
                return False
            if _has_currency_marker() or _has_percent_marker():
                return False
            return True

        requires_unit = dim in ("rate", "ratio")
        if requires_unit and not _fix16_candidate_has_any_unit(c):
            return False
        if cand_fam and dim in ("rate", "ratio"):
            return cand_fam == dim

        return True
    except Exception:
        return True


def _fix16_candidate_allowed(c: dict, metric_spec: dict, canonical_key: str = "") -> bool:
    """Compose fix15 exclusion + fix16 hard unit gate + year-token guard."""
    try:
        if not isinstance(c, dict):
            return False

        # Respect fix15 junk/year-only exclusion if present
        fn = globals().get("_candidate_disallowed_for_metric")
        if callable(fn):
            if fn(c, dict(metric_spec or {}, canonical_key=canonical_key)):
                return False
        expected_dim = _fix16_expected_dimension(metric_spec)
        if not expected_dim:
            expected_dim = _fix16_infer_dimension_from_canonical_key(canonical_key)
        if not _fix16_unit_compatible(c, expected_dim):
            return False

        # Extra deterministic guard: unitless year-like numbers should never compete
        # for non-year metrics even if upstream tagging missed them.
        if not _fix16_metric_is_year_like(metric_spec, canonical_key=canonical_key):
            v = c.get("value") if c.get("value") is not None else c.get("value_norm")
            u = (c.get("base_unit") or c.get("unit") or "").strip()
            if u == "" and isinstance(v, (int, float)):
                iv = int(v)
                if 1900 <= iv <= 2100:
                    return False

        return True
    except Exception:
        return True


def rebuild_metrics_from_snapshots_with_anchors_fix16(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    FIX16 anchor-aware rebuild:
      - Absolute anchor priority when anchor_hash exists in prev_response.metric_anchors
      - Hard disallow junk/year-like unitless candidates for non-year metrics
      - Hard unit expectation gating for currency/percent/rate/ratio dimensions
    """
    import re

    if not isinstance(prev_response, dict):
        return {}

    metric_anchors = (
        prev_response.get("metric_anchors")
        or (prev_response.get("primary_response") or {}).get("metric_anchors")
        or (prev_response.get("results") or {}).get("metric_anchors")
        or {}
    )
    if not isinstance(metric_anchors, dict) or not metric_anchors:
        return {}

    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )

    # Build deterministic candidate index (anchor_hash -> best candidate)
    fn_idx = globals().get("_es_build_candidate_index_deterministic")
    cand_index = fn_idx(baseline_sources_cache) if callable(fn_idx) else {}

    rebuilt = {}

    for canonical_key, a in (metric_anchors or {}).items():
        if not isinstance(a, dict):
            continue
        ah = a.get("anchor_hash") or a.get("anchor") or ""
        if not ah:
            continue

        spec = (metric_schema.get(canonical_key) if isinstance(metric_schema, dict) else None) or {}
        spec = dict(spec)
        spec.setdefault("name", a.get("name") or canonical_key)
        spec.setdefault("canonical_key", canonical_key)

        c = cand_index.get(ah)
        if not isinstance(c, dict):
            continue

        # FIX16 eligibility hard-gates
        if not _fix16_candidate_allowed(c, spec, canonical_key=canonical_key):
            continue

            try:
                _ok_u, _why_u = _fix2d2u_semantic_eligible_global(c, spec, str(canonical_key))
                if not _ok_u:
                    continue
            except Exception:
                pass

        rebuilt[canonical_key] = {
            "canonical_key": canonical_key,
            "name": spec.get("name") or canonical_key,
            "value": c.get("value"),
            "unit": c.get("unit") or "",
            "value_norm": c.get("value_norm"),
            "source_url": c.get("source_url") or "",
            "anchor_hash": c.get("anchor_hash") or ah,
            "evidence": [{
                "source_url": c.get("source_url") or "",
                "raw": c.get("raw") or "",
                "context_snippet": (c.get("context_snippet") or c.get("context") or c.get("context_window") or "")[:400],
                "anchor_hash": c.get("anchor_hash") or ah,
                "method": "anchor_hash_rebuild_fix16",
            }],
            "anchor_used": True,
        }

    return rebuilt


# Objective:
# - Deterministically map a selected set of injected/observed extractions into
#   existing Analysis canonical_keys so they participate in diffing.
# - Rules are explicit and auditable: domain + year + exact context substrings,
#   with deterministic unit_tag/measure_kind tagging.
# - Canonical authority remains single-sourced via _analysis_canonical_final_selector_v1
#   (this mapping only influences candidate eligibility for a target canonical_key).

def _fix2s_extract_year_from_candidate(c: dict) -> int:
    """Deterministically extract a 4-digit year from candidate fields or text."""
    try:
        if not isinstance(c, dict):
            return 0
        # Preferred explicit fields
        for k in ("year", "year_int", "target_year"):
            v = c.get(k)
            try:
                iv = int(v)
                if 1900 <= iv <= 2100:
                    return iv
            except Exception:
                pass
        # Fallback: strict regex scan of available text fields
        blob = " ".join([
            str(c.get("context_snippet") or ""),
            str(c.get("label") or ""),
            str(c.get("raw") or ""),
            str(c.get("text") or ""),
        ])
        m = re.search(r"\b(19\d{2}|20\d{2})\b", blob)
        if m:
            iv = int(m.group(1))
            if 1900 <= iv <= 2100:
                return iv
    except Exception:
        return 0


def _fix2s_apply_observed_to_canonical_rules_v1(candidates: list, metric_schema: dict, web_context=None) -> dict:
    """
    Apply deterministic mapping rules to candidate dicts.

    Returns a diagnostics dict:
      {
        "mapping_rules_version": str,
        "mapping_hits": [ {anchor_hash, target_key, year, source_url} ... ],
        "observed_rows_canonicalized_by_mapping": int,
        "mapping_misses": [ {anchor_hash, reason, source_url} ... ]   # optional
      }
    """
    diag = {
        "mapping_rules_version": "fix2s_rules_v1",
        "mapping_hits": [],
        "observed_rows_canonicalized_by_mapping": 0,
        "mapping_misses": [],
    }

    if not isinstance(candidates, list) or not candidates:
        return diag
    if not isinstance(metric_schema, dict) or not metric_schema:
        return diag

    # NOTE: target_key must already exist in Analysis' canonical namespace (schema).
    # We only promote if target_key exists in metric_schema.
    rule_table = [
        # Market share (%)
        {
            "anchor_hash": "f6cb33abc9aff96c8280223ee5f62cfe7be064f5",
            "target_key": "global_2025_market_share",
            "year": 2025,
            "kind": "percent",
            "domain_substr": "ev-volumes.com",
            "context_substr_any": ["market share", "ev share", "%", "share of"],
        },
        {
            "anchor_hash": "eedc7b779f3d87c41943da9d82d04b3f2b9a02e5",
            "target_key": "global_2026_market_share",
            "year": 2026,
            "kind": "percent",
            "domain_substr": "ev-volumes.com",
            "context_substr_any": ["market share", "ev share", "%", "share of"],
        },
        {
            "anchor_hash": "2fa90308784a5827b2e8c63b0a3992c5846e8e22",
            "target_key": "global_2030_market_share",
            "year": 2030,
            "kind": "percent",
            "domain_substr": "ev-volumes.com",
            "context_substr_any": ["market share", "ev share", "%", "share of"],
        },
        # Units sold (unit_sales)
        {
            "anchor_hash": "47da6bcf38afcc47c8b36819b69d88dffc772618",
            "target_key": "global_2024_units_sold",
            "year": 2024,
            "kind": "unit_sales",
            "domain_substr": "ev-volumes.com",
            "context_substr_any": ["ev sales", "sales", "sold", "registrations", "million"],
        },
        {
            "anchor_hash": "5679e35e4abefef7f6a3842414eceed7c3020508",
            "target_key": "global_2025_units_sold",
            "year": 2025,
            "kind": "unit_sales",
            "domain_substr": "ev-volumes.com",
            "context_substr_any": ["ev sales", "sales", "sold", "registrations", "million"],
        },
        {
            "anchor_hash": "37d6d391458630e5bbd4d34564c7490792dfbaf0",
            "target_key": "global_2040_units_sold",
            "year": 2040,
            "kind": "unit_sales",
            "domain_substr": "ev-volumes.com",
            "context_substr_any": ["ev sales", "sales", "sold", "registrations", "million"],
        },
    ]

    rules_by_anchor = {r.get("anchor_hash"): r for r in rule_table if isinstance(r, dict) and r.get("anchor_hash")}

    def _norm(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    for c in candidates:
        if not isinstance(c, dict):
            continue
        ah = str(c.get("anchor_hash") or "")
        if not ah or ah not in rules_by_anchor:
            continue

        r = rules_by_anchor.get(ah) or {}
        tgt = r.get("target_key") or ""
        if not tgt:
            continue

        # Guard: only map into existing schema keys
        if tgt not in metric_schema:
            diag["mapping_misses"].append({"anchor_hash": ah, "reason": "target_not_in_schema", "source_url": c.get("source_url")})
            continue

        url = str(c.get("source_url") or "")
        if r.get("domain_substr") and r["domain_substr"] not in url:
            diag["mapping_misses"].append({"anchor_hash": ah, "reason": "domain_mismatch", "source_url": url})
            continue

        yr = _fix2s_extract_year_from_candidate(c)
        if r.get("year") and yr != int(r["year"]):
            diag["mapping_misses"].append({"anchor_hash": ah, "reason": "year_mismatch", "source_url": url, "year": yr})
            continue

        # Exact substring allowlist (deterministic, no fuzzy matching)
        blob = " ".join([
            str(c.get("context_snippet") or ""),
            str(c.get("label") or ""),
            str(c.get("raw") or ""),
            str(c.get("text") or ""),
        ])
        blob_n = _norm(blob)
        allow = r.get("context_substr_any") or []
        if allow:
            ok = False
            for lit in allow:
                if _norm(str(lit)) and _norm(str(lit)) in blob_n:
                    ok = True
                    break
            if not ok:
                diag["mapping_misses"].append({"anchor_hash": ah, "reason": "context_mismatch", "source_url": url})
                continue

        # Deterministic unit tagging to satisfy schema/unit gates
        kind = r.get("kind")
        if kind == "percent":
            c.setdefault("unit_tag", "percent")
            if not c.get("unit"):
                c["unit"] = "%"
            c.setdefault("measure_kind", "percent")
        elif kind == "unit_sales":
            c.setdefault("unit_tag", "unit_sales")
            if not c.get("unit"):
                c["unit"] = "unit_sales"
            c.setdefault("measure_kind", "unit_sales")

        # Force competition only for this target_key (prevents cross-metric leakage)
        c["fix2s_force_canonical_key"] = tgt

        # Provide canonical key hint (selector remains authority)
        c["canonical_key"] = tgt

        diag["mapping_hits"].append({"anchor_hash": ah, "target_key": tgt, "year": yr, "source_url": url})
        diag["observed_rows_canonicalized_by_mapping"] += 1

    # Attach to web_context for downstream summary merge (additive)
    try:
        if isinstance(web_context, dict):
            web_context["_fix2s_mapping_diag"] = diag
    except Exception:
        return diag


def rebuild_metrics_from_snapshots_schema_only_fix16(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    FIX16 schema-only rebuild:
      - Removes YYYY tokens from keyword scoring for non-year metrics
      - Hard unit expectation gating
      - Applies fix15 junk/year exclusion + fix16 extra year-token disallow
      - Deterministic selection/tie-breaks
    """
    import re

    if not isinstance(prev_response, dict):
        return {}

    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )
    if not isinstance(metric_schema, dict) or not metric_schema:
        return {}

    # Flatten snapshot candidates (no re-fetch)
    if isinstance(baseline_sources_cache, dict) and isinstance(baseline_sources_cache.get("snapshots"), list):
        sources = baseline_sources_cache.get("snapshots", [])
    elif isinstance(baseline_sources_cache, list):
        sources = baseline_sources_cache
    else:
        sources = []

    # Purpose:
    #   The Analysis-parity rebuild consumes a "snapshot pool" shaped like:
    #     [{source_url, extracted_numbers, ...}, ...]
    #   In several evolution runs, injected URLs were fetched + extracted (source_results),
    #   but their extracted_numbers were NOT present in baseline_sources_cache["snapshots"],
    #   so rebuild never saw them.
    #
    # This patch additively admits injected fetched sources into the snapshot pool by:
    #   - reading baseline_sources_cache["source_results"] when present
    #   - filtering strictly to injected URLs (normalized membership)
    #   - appending a snapshot-shaped dict into `sources`
    # Non-negotiables:
    #   - additive only; no behavior change for non-injected sources
    #   - no domain hardcoding; uses diag_injected_urls admitted/ui list
    _fix2aa_diag = {
        "enabled": True,
        "source_results_seen": 0,
        "injected_norm_set_size": 0,
        "admitted_to_snapshot_pool": 0,
        "admitted_urls": [],
        "skipped_already_present": 0,
        "skipped_not_injected": 0,
        "skipped_no_extracted_numbers": 0,
        "skipped_bad_shape": 0,
    }
    try:
        if isinstance(web_context, dict) and isinstance(baseline_sources_cache, dict):
            # Build injected normalized set
            _d = web_context.get("diag_injected_urls") or web_context.get("extra_urls_debug") or {}
            _inj_norm = set()
            if isinstance(_d, dict):
                _ad = _inj_diag_norm_url_list(_d.get("admitted") or _d.get("extra_urls_admitted") or [])
                _ui = _inj_diag_norm_url_list(_d.get("ui_norm") or _d.get("extra_urls_ui_norm") or _d.get("extra_urls_normalized") or [])
                _inj_norm = set(_ad or _ui or [])
            _fix2aa_diag["injected_norm_set_size"] = int(len(_inj_norm))

            # Build set of already-present source URLs in snapshot pool (normalized)
            _present = set()
            try:
                for _s in (sources or []):
                    if not isinstance(_s, dict):
                        continue
                    _u0 = _s.get("source_url") or _s.get("url") or ""
                    _present.add((_canonicalize_injected_url(_u0) if callable(globals().get("_canonicalize_injected_url")) else str(_u0 or "")).strip())
            except Exception:
                pass
                _present = set()

            _sr_list = baseline_sources_cache.get("source_results")
            if isinstance(_sr_list, list) and _sr_list:
                _fix2aa_diag["source_results_seen"] = int(len(_sr_list))
                for _sr in _sr_list:
                    if not isinstance(_sr, dict):
                        _fix2aa_diag["skipped_bad_shape"] += 1
                        continue
                    _u = _sr.get("source_url") or _sr.get("url") or ""
                    _un = (_canonicalize_injected_url(_u) if callable(globals().get("_canonicalize_injected_url")) else str(_u or "")).strip()
                    if not _un or (_un not in _inj_norm):
                        _fix2aa_diag["skipped_not_injected"] += 1
                        continue
                    if _un in _present:
                        _fix2aa_diag["skipped_already_present"] += 1
                        continue
                    _xs = _sr.get("extracted_numbers")
                    if not isinstance(_xs, list) or not _xs:
                        _fix2aa_diag["skipped_no_extracted_numbers"] += 1
                        continue

                    # Admit as snapshot-shaped dict
                    sources.append({
                        "source_url": _u,
                        "extracted_numbers": _xs,
                        "fix2aa_admitted_from": "source_results",
                    })
                    _present.add(_un)
                    _fix2aa_diag["admitted_to_snapshot_pool"] += 1
                    if len(_fix2aa_diag["admitted_urls"]) < 50:
                        _fix2aa_diag["admitted_urls"].append(_u)
    except Exception:
        pass
        # diagnostics only; never block rebuild
        pass
    try:
        if isinstance(web_context, dict):
            web_context["fix2aa_injected_snapshot_admission_v1"] = _fix2aa_diag
    except Exception:
        pass

    candidates = []
    for s in sources:
        if not isinstance(s, dict):
            continue
        url = s.get("source_url") or s.get("url") or ""
        xs = s.get("extracted_numbers")
        if isinstance(xs, list) and xs:
            for c in xs:
                if not isinstance(c, dict):
                    continue
                c2 = dict(c)
                c2.setdefault("source_url", url)
                candidates.append(c2)

    #   - Only binds candidates originating from injected URLs (admitted/ui list)
    #   - Uses exact substring + explicit year guards (no fuzzy matching)
    #   - Adds a per-candidate force key so only the intended schema slot competes
    _fix2v_injected_norm_set = set()
    try:
        if isinstance(web_context, dict):
            _d = web_context.get("diag_injected_urls") or web_context.get("extra_urls_debug") or {}
            if isinstance(_d, dict):
                _ad = _inj_diag_norm_url_list(_d.get("admitted") or _d.get("extra_urls_admitted") or [])
                _ui = _inj_diag_norm_url_list(_d.get("ui_norm") or _d.get("extra_urls_ui_norm") or _d.get("extra_urls_normalized") or [])
                _fix2v_injected_norm_set = set(_ad or _ui or [])
    except Exception:
        pass
        _fix2v_injected_norm_set = set()

    def _fix2v_norm_url(u: str) -> str:
        try:
            cu = _canonicalize_injected_url(u) if callable(globals().get("_canonicalize_injected_url")) else None
            return (cu or str(u or "")).strip()
        except Exception:
            return str(u or "").strip()

    def _fix2v_extract_years_from_blob(blob: str) -> set:
        try:
            ys = set()
            for mm in re.findall(r"\b(19\d{2}|20\d{2})\b", str(blob or "")):
                try:
                    ys.add(int(mm))
                except Exception:
                    return ys
        except Exception:
            return set()

    _fix2v_bind_hits = []
    for _c in candidates:
        try:
            if not isinstance(_c, dict):
                continue
            _u = _fix2v_norm_url(_c.get("source_url") or "")
            _c["_fix2v_source_is_injected"] = bool(_u and (_u in _fix2v_injected_norm_set))
            if not _c.get("_fix2v_source_is_injected"):
                continue

            _blob = " ".join([
                str(_c.get("context_snippet") or ""),
                str(_c.get("label") or ""),
                str(_c.get("raw") or ""),
            ])
            _blob_n = _norm(_blob)
            _years = _fix2v_extract_years_from_blob(_blob)

            # --- Bind: 2040 charger infrastructure count (e.g., 206.6 million worldwide) ---
            if (2040 in _years) and (("charging infrastructure" in _blob_n) or ("ev charging" in _blob_n) or ("chargers" in _blob_n) or ("charger" in _blob_n)):
                if ("worldwide" in _blob_n) or ("global" in _blob_n):
                    # If upstream left unit empty but raw contains 'million', use unit_tag M for magnitude
                    if (not str(_c.get("unit_tag") or _c.get("unit") or "").strip()) and ("million" in str(_c.get("raw") or "").lower()):
                        _c["unit_tag"] = "M"
                    _c["fix2v_force_canonical_key"] = "global_ev_chargers_2040__unit_count"
                    _fix2v_bind_hits.append(dict(kind="count_2040", anchor_hash=_c.get("anchor_hash"), value=_c.get("value_norm") or _c.get("value"), source_url=_c.get("source_url")))
                    continue

            # --- Bind: CAGR 12.3% from 2026 to 2040 ---
            if (2026 in _years) and (2040 in _years):
                if ("cagr" in _blob_n) or ("compound annual growth rate" in _blob_n):
                    raw_l = str(_c.get("raw") or "").lower()
                    u_l = str(_c.get("unit") or _c.get("unit_tag") or "").lower()
                    if ("%" in raw_l) or ("%" in u_l) or ("percent" in u_l) or ("percent" in raw_l):
                        _c["fix2v_force_canonical_key"] = "global_ev_chargers_cagr_2026_2040__percent"
                        _fix2v_bind_hits.append(dict(kind="cagr_2026_2040", anchor_hash=_c.get("anchor_hash"), value=_c.get("value_norm") or _c.get("value"), source_url=_c.get("source_url")))
                        continue
        except Exception:
            pass
            continue

    try:
        if isinstance(web_context, dict):
            web_context.setdefault("fix2v_candidate_binding_v1", {})
            web_context["fix2v_candidate_binding_v1"]["injected_norm_set_size"] = int(len(_fix2v_injected_norm_set))
            web_context["fix2v_candidate_binding_v1"]["binding_hits"] = _fix2v_bind_hits[:200]
            web_context["fix2v_candidate_binding_v1"]["binding_hit_count"] = int(len(_fix2v_bind_hits))
    except Exception:
        pass

    # Purpose:
    #   Provide a deterministic "why not canonical?" autopsy for targeted keys.
    #   Records:
    #     - candidate pool size
    #     - eligible vs rejected counts + first-N reject reasons
    #     - top-scoring candidates (pre-selection) + winner summary
    # Non-negotiables:
    #   - additive only; no behavior changes to selection
    #   - uses existing FIX16 gates; does not bypass selector
    # Scope:
    #   Only for the new EV-charger keys introduced in FIX2U/FIX2V.
    _fix2y_targets = set([
        "global_ev_chargers_2040__unit_count",
        "global_ev_chargers_cagr_2026_2040__percent",
    ])
    _fix2y_autopsy = {}
    try:
        # Make FIX2W eval sampler safe: some branches refer to extracted_candidates.
        extracted_candidates = candidates  # noqa: F841
    except Exception:
        pass

    def _fix2y_gate_reason(_c: dict, _spec: dict, _ck: str) -> str:
        try:
            if not isinstance(_c, dict):
                return "non_dict_candidate"
            # fix15 junk/year-only exclusion (if present)
            _fn = globals().get("_candidate_disallowed_for_metric")
            if callable(_fn):
                try:
                    if _fn(_c, dict(_spec or {}, canonical_key=_ck)):
                        return "fix15_disallowed"
                except Exception:
                    return "fix15_disallowed_err"
            try:
                _expected_dim = _fix16_expected_dimension(_spec)
            except Exception:
                pass
                _expected_dim = ""
            try:
                if not _fix16_unit_compatible(_c, _expected_dim):
                    return "unit_incompatible"
            except Exception:
                pass
                # if unit compatibility check fails, treat as incompatible for diagnosis only
                return "unit_incompatible_err"
            # year-token guard (unitless year-like numerics)
            try:
                if not _fix16_metric_is_year_like(_spec, canonical_key=_ck):
                    _v = _c.get("value") if _c.get("value") is not None else _c.get("value_norm")
                    _u = (str(_c.get("base_unit") or _c.get("unit") or "")).strip()
                    if _u == "" and isinstance(_v, (int, float)):
                        _iv = int(_v)
                        if 1900 <= _iv <= 2100:
                            return "unitless_year_guard"
            except Exception:
                return "ok"
        except Exception:
            return "gate_reason_err"

    def _fix2y_score_hits(_c: dict, _kw_norm: list) -> int:
        try:
            ctx = _norm(_c.get("context_snippet") or _c.get("context") or _c.get("context_window") or "")
            raw = _norm(_c.get("raw") or "")
            _hits = 0
            for _k in (_kw_norm or []):
                if _k and (_k in ctx or _k in raw):
                    _hits += 1
            return int(_hits)
        except Exception:
            return 0

    # We'll populate _fix2y_autopsy inside the schema loop when the key matches.

    try:
        _eval_samples = []
        _eligible_but_unbound = 0
        if isinstance(extracted_candidates, list):
            for _c in extracted_candidates:
                if not isinstance(_c, dict):
                    continue
                _is_inj = False
                try:
                    _is_inj = bool(_ph2b_norm_url(_c.get("source_url") or "") in _fix2v_injected_norm_set)
                except Exception:
                    pass
                    _is_inj = False
                if not _is_inj:
                    continue

                _raw = str(_c.get("context_snippet") or _c.get("snippet") or _c.get("raw") or "")
                _raw_l = _raw.lower()
                if ("charg" not in _raw_l) and ("cagr" not in _raw_l) and ("compound annual" not in _raw_l) and ("2040" not in _raw_l):
                    continue

                has_charging = (("charging infrastructure" in _raw_l) or ("ev charging" in _raw_l) or ("charger" in _raw_l) or ("chargers" in _raw_l))
                has_global = (("global" in _raw_l) or ("worldwide" in _raw_l))
                has_2040 = ("2040" in _raw_l) or (str(_c.get("year") or "") == "2040")
                has_2026 = ("2026" in _raw_l) or (str(_c.get("year") or "") == "2026")
                has_cagr = (("cagr" in _raw_l) or ("compound annual growth rate" in _raw_l))
                u_l = str(_c.get("unit") or _c.get("unit_tag") or _c.get("unit_norm") or "").lower()
                has_percent = ("%" in _raw_l) or ("%" in u_l) or ("percent" in u_l) or ("percent" in _raw_l)
                has_magnitude_m = (" million" in _raw_l) or (u_l.strip() in ("m", "mn", "million", "mio")) or (str(_c.get("unit") or "").strip() == "M")

                predicted = "none"
                if has_charging and has_global and has_2040 and (not has_percent) and has_magnitude_m:
                    predicted = "count_2040"
                if has_cagr and has_2026 and has_2040 and has_percent:
                    predicted = "cagr_2026_2040"

                bound_key = str(_c.get("fix2v_force_canonical_key") or "")
                if predicted != "none" and not bound_key:
                    _eligible_but_unbound += 1

                _eval_samples.append({
                    "source_url": _c.get("source_url"),
                    "source_url_norm": _ph2b_norm_url(_c.get("source_url") or ""),
                    "value_norm": _c.get("value_norm") if isinstance(_c.get("value_norm"), (int, float)) else _c.get("value"),
                    "unit": _c.get("unit") or _c.get("unit_tag"),
                    "has_charging": bool(has_charging),
                    "has_global": bool(has_global),
                    "has_2040": bool(has_2040),
                    "has_2026": bool(has_2026),
                    "has_cagr": bool(has_cagr),
                    "has_percent": bool(has_percent),
                    "has_magnitude_m": bool(has_magnitude_m),
                    "predicted_rule": predicted,
                    "bound_key": bound_key or None,
                })
                if len(_eval_samples) >= 200:
                    break

        if isinstance(web_context, dict):
            web_context.setdefault("fix2v_candidate_binding_v1", {})
            web_context["fix2v_candidate_binding_v1"]["rule_eval_samples"] = _eval_samples
            web_context["fix2v_candidate_binding_v1"]["eligible_but_unbound_count"] = int(_eligible_but_unbound)
    except Exception:
        pass

    #   - Deterministically synthesize schema-bound candidates from extracted_numbers
    #   - Injected-only (domain-agnostic): only candidates proven from injected URL set
    #   - No fuzzy matching: exact substring keyword hits + unit-family compatibility
    #   - Does NOT bypass the Analysis canonical selector; it only increases eligible pool
    try:
        _fix2z_hits = []
        _fix2z_added = 0
        _fix2z_seen = 0

        # Build list of schema keys grouped by unit_family for quick scan
        _fix2z_schema_percent = []
        _fix2z_schema_magnitude = []
        for _k, _spec in (metric_schema or {}).items():
            if not isinstance(_spec, dict):
                continue
            _uf = str(_spec.get("unit_family") or "").lower().strip()
            if _uf == "percent":
                _fix2z_schema_percent.append((_k, _spec))
            elif _uf == "magnitude":
                _fix2z_schema_magnitude.append((_k, _spec))

        def _fix2z_blob(c: dict) -> str:
            return " ".join([
                str(c.get("context_snippet") or ""),
                str(c.get("label") or ""),
                str(c.get("raw") or ""),
                str(c.get("context") or ""),
            ]).strip()

        def _fix2z_is_injected(c: dict) -> bool:
            if bool(c.get("_fix2v_source_is_injected")):
                return True
            try:
                u = str(c.get("source_url") or "")
                u_norm = (_inj_diag_norm_url_list([u])[0] if u else "")
                return (u_norm in _fix2v_injected_norm_set)
            except Exception:
                return False

        def _fix2z_has_percent(c: dict, blob_l: str) -> bool:
            u = str(c.get("unit_tag") or c.get("unit") or "").lower()
            if ("%" in u) or ("percent" in u):
                return True
            if "%" in (str(c.get("raw") or "")):
                return True
            if ("%" in blob_l) or (" percent" in blob_l):
                return True
            mk = str(c.get("measure_kind") or "").lower()
            if "pct" in mk or "percent" in mk:
                return True
            return False

        def _fix2z_has_magnitude_m(c: dict, blob_l: str) -> bool:
            u = str(c.get("unit_tag") or c.get("unit") or "").lower()
            if u == "m" or "million" in u:
                return True
            if " million" in blob_l:
                return True
            if str(c.get("unit_tag") or "") == "M":
                return True
            return False

        def _fix2z_money_context(blob_l: str) -> bool:
            # Avoid contaminating count metrics with spend/currency context
            money_terms = ["$", " usd", "usd ", "billion", "bn", "spend", "investment", "capex", "revenue", "worth"]
            return any(t in blob_l for t in money_terms)

        def _fix2z_keyword_hits(blob_l: str, keywords) -> int:
            hits = 0
            if not isinstance(keywords, list):
                return 0
            for kw in keywords:
                kw_s = str(kw or "").strip().lower()
                if not kw_s:
                    continue
                if kw_s.isdigit():
                    # year tokens must be present literally
                    if kw_s in blob_l:
                        hits += 1
                    continue
                if kw_s in blob_l:
                    hits += 1
            return hits

        # Scan unbound injected candidates and synthesize schema-bound copies
        for _c in candidates:
            if not isinstance(_c, dict):
                continue
            if _c.get("fix2v_force_canonical_key"):
                continue  # already bound by FIX2V
            if not _fix2z_is_injected(_c):
                continue  # injected-only admission
            _fix2z_seen += 1

            _blob = _fix2z_blob(_c)
            _blob_l = _blob.lower()

            # Quick classify candidate type
            _is_pct = _fix2z_has_percent(_c, _blob_l)
            _is_mag = _fix2z_has_percent(_c, _blob_l) is False and _fix2z_has_magnitude_m(_c, _blob_l)

            _best = None
            _best_hits = -1
            _best_spec = None

            if _is_pct:
                for _k, _spec in _fix2z_schema_percent:
                    _hits = _fix2z_keyword_hits(_blob_l, _spec.get("keywords"))
                    # Require both boundary years if schema implies a window
                    if ("2026" in [str(x) for x in (_spec.get("keywords") or [])]) and ("2026" not in _blob_l):
                        continue
                    if ("2040" in [str(x) for x in (_spec.get("keywords") or [])]) and ("2040" not in _blob_l):
                        continue
                    if _hits > _best_hits:
                        _best_hits = _hits
                        _best = _k
                        _best_spec = _spec
            elif _is_mag:
                for _k, _spec in _fix2z_schema_magnitude:
                    # Do not bind magnitude count keys if money context detected
                    if str(_spec.get("dimension") or "").lower().strip() == "count":
                        if _fix2z_money_context(_blob_l):
                            continue
                    _hits = _fix2z_keyword_hits(_blob_l, _spec.get("keywords"))
                    # Ensure 2040 is present if schema expects it
                    if ("2040" in [str(x) for x in (_spec.get("keywords") or [])]) and ("2040" not in _blob_l):
                        continue
                    if _hits > _best_hits:
                        _best_hits = _hits
                        _best = _k
                        _best_spec = _spec

            # Deterministic threshold: require a minimum keyword-hit support
            if _best and _best_spec and _best_hits >= 4:
                _c2 = dict(_c)
                _c2["fix2v_force_canonical_key"] = _best
                _c2["fix2z_bound_by"] = "schema_keywords"
                _c2["fix2z_keyword_hits"] = int(_best_hits)
                # Encourage unit tagging consistency (do not overwrite existing unit tags)
                try:
                    if not str(_c2.get("unit_tag") or _c2.get("unit") or "").strip():
                        if str(_best_spec.get("unit_tag") or "").strip():
                            _c2["unit_tag"] = str(_best_spec.get("unit_tag"))
                    if str(_best_spec.get("dimension") or "").strip() and not str(_c2.get("dimension") or "").strip():
                        _c2["dimension"] = str(_best_spec.get("dimension"))
                    if str(_best_spec.get("unit_family") or "").strip() and not str(_c2.get("unit_family") or "").strip():
                        _c2["unit_family"] = str(_best_spec.get("unit_family"))
                except Exception:
                    pass
                candidates.append(_c2)
                _fix2z_added += 1
                if len(_fix2z_hits) < 200:
                    _fix2z_hits.append(dict(
                        bound_key=_best,
                        keyword_hits=int(_best_hits),
                        value=_c.get("value_norm") or _c.get("value"),
                        unit=_c.get("unit_tag") or _c.get("unit"),
                        source_url=_c.get("source_url"),
                        anchor_hash=_c.get("anchor_hash"),
                    ))

        if isinstance(web_context, dict):
            web_context.setdefault("fix2z_schema_binding_admission_v1", {})
            web_context["fix2z_schema_binding_admission_v1"]["seen_injected_unbound"] = int(_fix2z_seen)
            web_context["fix2z_schema_binding_admission_v1"]["synth_candidates_added"] = int(_fix2z_added)
            web_context["fix2z_schema_binding_admission_v1"]["hits"] = _fix2z_hits
    except Exception:
        pass

    def _norm(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def _cand_sort_key(c: dict):
        try:
            return (
                str(c.get("anchor_hash") or ""),
                str(c.get("source_url") or ""),
                int(c.get("start_idx") or 0),
                str(c.get("raw") or ""),
                str(c.get("unit") or ""),
                float(c.get("value_norm") or 0.0),
            )
        except Exception:
            return ("", "", 0, "", "", 0.0)

    # Deterministic global ordering of candidates
    candidates.sort(key=_cand_sort_key)

    rebuilt = {}

    for canonical_key, sch in metric_schema.items():
        if not isinstance(sch, dict):
            continue

        spec = dict(sch)
        spec.setdefault("canonical_key", canonical_key)
        spec.setdefault("name", sch.get("name") or canonical_key)

        metric_is_year_like = _fix16_metric_is_year_like(spec, canonical_key=canonical_key)

        keywords = sch.get("keywords") or sch.get("keyword_hints") or []
        if isinstance(keywords, str):
            keywords = [keywords]
        keywords = _fix16_prune_year_keywords(list(keywords), metric_is_year_like)
        kw_norm = [_norm(k) for k in keywords if k]

        expected_dim = _fix16_expected_dimension(spec)


        if canonical_key in _fix2y_targets:
            try:
                _a = {
                    "canonical_key": canonical_key,
                    "schema_expected_dim": expected_dim,
                    "keywords": list(keywords) if isinstance(keywords, list) else [],
                    "kw_norm_count": int(len(kw_norm)) if isinstance(kw_norm, list) else 0,
                    "pool_total": int(len(candidates)) if isinstance(candidates, list) else 0,
                    "pool_force_filtered": 0,
                    "eligible_count": 0,
                    "rejected_count": 0,
                    "reject_reasons": {},
                    "reject_samples": [],
                    "top_candidates": [],
                    "winner": {},
                }

                _force_pool = []
                for _c0 in (candidates or []):
                    if not isinstance(_c0, dict):
                        continue
                    _fk0 = _c0.get("fix2v_force_canonical_key")
                    if _fk0 and _fk0 != canonical_key:
                        continue
                    _force_pool.append(_c0)
                _a["pool_force_filtered"] = int(len(_force_pool))

                _eligible = []
                for _c1 in _force_pool:
                    _gr = _fix2y_gate_reason(_c1, spec, canonical_key)
                    if _gr == "ok" and _fix16_candidate_allowed(_c1, spec, canonical_key=canonical_key):
                        _eligible.append(_c1)
                    else:
                        _a["rejected_count"] += 1
                        _a["reject_reasons"][_gr] = int(_a["reject_reasons"].get(_gr, 0)) + 1
                        if len(_a["reject_samples"]) < 25:
                            _a["reject_samples"].append({
                                "reason": _gr,
                                "value_norm": _c1.get("value_norm") if isinstance(_c1.get("value_norm"), (int, float)) else _c1.get("value"),
                                "unit": _c1.get("unit") or _c1.get("unit_tag") or "",
                                "source_url": _c1.get("source_url") or "",
                                "anchor_hash": _c1.get("anchor_hash") or "",
                                "raw_head": (str(_c1.get("raw") or "")[:120]),
                            })

                _a["eligible_count"] = int(len(_eligible))

                # Rank preview: top candidates by keyword hits then FIX16 sort key
                _ranked = []
                for _c2 in _eligible:
                    _hits2 = _fix2y_score_hits(_c2, kw_norm)
                    _ranked.append((_hits2, _cand_sort_key(_c2), _c2))
                _ranked.sort(key=lambda t: (-int(t[0]), t[1]))
                for _hits2, _sk2, _c2 in _ranked[:10]:
                    _a["top_candidates"].append({
                        "hits": int(_hits2),
                        "value_norm": _c2.get("value_norm") if isinstance(_c2.get("value_norm"), (int, float)) else _c2.get("value"),
                        "unit": _c2.get("unit") or _c2.get("unit_tag") or "",
                        "source_url": _c2.get("source_url") or "",
                        "anchor_hash": _c2.get("anchor_hash") or "",
                        "raw_head": (str(_c2.get("raw") or "")[:160]),
                    })

                _fix2y_autopsy[canonical_key] = _a
            except Exception:
                pass

        best = None
        best_tie = None

        for c in candidates:
            # FIX2V: if candidate is force-bound to a specific canonical slot, only allow it to compete there
            _fk = c.get("fix2v_force_canonical_key")
            if _fk and _fk != canonical_key:
                continue

            # FIX16 hard eligibility gates
            if not _fix16_candidate_allowed(c, spec, canonical_key=canonical_key):
                continue

            try:
                _ok_u, _why_u = _fix2d2u_semantic_eligible(c, spec, canonical_key)
                if not _ok_u:
                    continue
            except Exception:
                pass

            try:
                _ok_u, _why_u = _fix2d2u_semantic_eligible(c, spec, canonical_key)
                if not _ok_u:
                    continue
            except Exception:
                pass

            # FIX2D2R: forbid bare-year tokens when a better sibling exists nearby (parity guard)
            try:
                _ek = 'other'
                _ed = (expected_dim or '').lower().strip()
                if canonical_key.endswith('__percent') or _ed == 'percent':
                    _ek = 'percent'
                elif canonical_key.endswith('__currency') or _ed == 'currency':
                    _ek = 'currency'
                elif canonical_key.endswith('__unit_sales') or 'unit' in canonical_key.lower() or 'sales' in canonical_key.lower():
                    _ek = 'unit'
                elif canonical_key.endswith('__year') or 'year' in _ed:
                    _ek = 'year'

                if _ek != 'year' and _fix2d2r_is_bare_year_cand(c) and not str(c.get('unit') or c.get('unit_tag') or '').strip():
                    if _fix2d2r_has_better_sibling(c, candidates, _ek):
                        continue
            except Exception:
                pass

            # keyword relevance
            ctx = _norm(c.get("context_snippet") or c.get("context") or c.get("context_window") or "")
            raw = _norm(c.get("raw") or "")

            hits = 0
            for k in kw_norm:
                if k and (k in ctx or k in raw):
                    hits += 1

            # If there are no keyword hits at all, keep as weak fallback only if unit family matches strongly
            # but do not select zero-hit candidates over hit candidates.
            tie = (-hits,) + _cand_sort_key(c)
            if best is None or tie < best_tie:
                best = c
                best_tie = tie

        if not isinstance(best, dict):
            continue

        # Require at least one keyword hit unless the schema has no keywords
        if kw_norm:
            if best_tie is not None and isinstance(best_tie, tuple):
                try:
                    if (-best_tie[0]) <= 0:
                        continue
                except Exception:
                    pass

        rebuilt[canonical_key] = {
            "canonical_key": canonical_key,
            "name": spec.get("name") or canonical_key,
            "value": best.get("value"),
            "unit": best.get("unit") or "",
            "value_norm": best.get("value_norm"),
            "source_url": best.get("source_url") or "",
            "anchor_hash": best.get("anchor_hash") or "",
            "evidence": [{
                "source_url": best.get("source_url") or "",
                "raw": best.get("raw") or "",
                "context_snippet": (best.get("context_snippet") or best.get("context") or best.get("context_window") or "")[:400],
                "anchor_hash": best.get("anchor_hash") or "",
                "method": "schema_only_rebuild_fix16",
            }],
            "anchor_used": False,
        }

    return rebuilt


# - Keep names identical so evolution uses these as the LAST definitions
# - We expose both functions while preserving older ones for reference


# Goal:
#   - Provide exactly ONE authoritative selector for dashboard-facing "Current"
#   - Treat candidate.value_norm as schema units (no base-unit assumption)
#   - Deterministic tie-breaks; NO IO; NO re-fetch; NO hashing changes
#
# Notes:
#   - This is a single-metric selector. Batch rebuild helpers may call it.
#   - Reuses FIX16 hard eligibility gates (_fix16_candidate_allowed) + FIX16 scoring.
#   - Adds optional preferred/anchor lock when anchors are present (stays in preferred source).

def _ph2b_norm_url(url: str) -> str:
    try:
        fn = globals().get("_normalize_url")
        if callable(fn):
            return str(fn(url or ""))
    except Exception:
        return str((url or "").strip())


# - Uses LOCAL context_snippet (tight window) instead of page-wide context_window.
# - Prevents cross-metric pollution in schema-only rebuild paths.
# - Applied in BOTH Analysis selector and Evolution schema-only rebuild(s).

def _fix2d2u_norm_text(s: str) -> str:
    try:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()
    except Exception:
        return ""


def _fix2d2u_local_text(cand: dict) -> str:
    """Prefer the tightest snippet around the number."""
    try:
        if not isinstance(cand, dict):
            return ""
        for k in ("context_snippet", "context", "context_window", "context_window_raw", "context_window_text"):
            v = cand.get(k)
            if isinstance(v, str) and v.strip():
                return v
        return str(cand.get("raw") or cand.get("value") or "")
    except Exception:
        return ""


def _fix2d2u_required_token_groups(canonical_key: str, spec: dict) -> list:
    """Return OR-groups; every group must have at least one hit in local snippet."""
    try:
        ck = str(canonical_key or "").lower()
        nm = str((spec or {}).get("name") or (spec or {}).get("metric_name") or "").lower()
        base = (ck + " " + nm).strip()

        groups = []

        # Geo cues
        if "china" in base:
            groups.append(["china", "chinese"])

        # Time cues: require explicit 4-digit years present in key/name
        years = re.findall(r"\b(19\d{2}|20\d{2})\b", base)
        for y in years[:3]:
            groups.append([y])

        # Metric family cues
        if ("charger" in base) or ("charging" in base):
            groups.append(["charger", "chargers", "charging", "station", "stations", "infrastructure"])
        if ("investment" in base) or ("capex" in base) or ("spend" in base) or (str((spec or {}).get("unit_family") or "").lower() == "currency"):
            groups.append(["investment", "invest", "capex", "spend", "spending", "cost", "expenditure"])
        if ("share" in base) or (str((spec or {}).get("unit_family") or "").lower() == "percent"):
            groups.append(["market share", "share", "%", "percent"])
        if ("sale" in base) or ("sales" in base) or ("unit_sales" in base):
            groups.append(["sales", "sold", "deliveries", "registrations", "units"])
        if "ytd" in base:
            groups.append(["ytd", "year to date"])

        # De-dupe
        out=[]
        for g in groups:
            gg=[]
            for t in g:
                tt=str(t or "").strip().lower()
                if tt and tt not in gg:
                    gg.append(tt)
            if gg:
                out.append(gg)
        return out
    except Exception:
        return []


def _fix2d2u_semantic_eligible_global(cand: dict, spec: dict, canonical_key: str) -> tuple:
    """(ok, reason) gate; no-op when no required groups."""
    try:
        groups = _fix2d2u_required_token_groups(canonical_key, spec or {})
        if not groups:
            return True, ""
        txt = _fix2d2u_local_text(cand)
        blob = " ".join([_fix2d2u_norm_text(txt), _fix2d2u_norm_text(str(cand.get("raw") or ""))]).strip()
        for g in groups:
            hit=False
            for tok in g:
                tn=_fix2d2u_norm_text(tok)
                if tn and tn in blob:
                    hit=True
                    break
            if not hit:
                return False, "missing_required_tokens:" + "|".join(g[:3])
        return True, ""
    except Exception:
        return True, ""


# commit point using a tight local window around the numeric token.
# - Prevents cross-metric pollution where a China sales snippet populates
#   chargers/investment/CAGR schema keys.
# - Requires canonical-key year tokens to appear locally (if present).
# - Logs reject counts into web_context.debug.fix2d2v_schema_commit_rejects.

def _fix2d2v_tight_window(cand: dict, width: int = 120) -> str:
    try:
        txt = str(cand.get('context_snippet') or cand.get('context') or cand.get('context_window') or '')
        if not txt:
            return ''
        raw = str(cand.get('raw') or cand.get('value') or '')
        if raw:
            i = txt.find(raw)
            if i >= 0:
                a = max(0, i - width)
                b = min(len(txt), i + len(raw) + width)
                return txt[a:b]
        # fallback: just trim
        return txt[: (2*width)]
    except Exception:
        return ''


def _fix2d2v_years_from_key(canonical_key: str):
    try:
        import re
        return re.findall(r"\b(19\d{2}|20\d{2})\b", str(canonical_key or ''))
    except Exception:
        return []


def _fix2d2v_semantic_eligible_commit(cand: dict, spec: dict, canonical_key: str, web_context=None) -> tuple:
    """Stricter eligibility used immediately before committing schema_only_rebuild outputs."""
    try:
        if not bool(globals().get("_FIX2D2U_ENABLE", True)):
            return True, ''
        # 1) Start with FIX2D2U required-token groups, but only check within a tight window
        groups = _fix2d2u_required_token_groups(canonical_key, spec)
        win = _fix2d2u_norm(_fix2d2v_tight_window(cand) + ' ' + str(cand.get('raw') or ''))
        for g in groups or []:
            hit = False
            for tok in g:
                tn = _fix2d2u_norm(tok)
                if tn and tn in win:
                    hit = True
                    break
            if not hit:
                return False, 'missing_required_tokens_tight'
        # 2) If the canonical key contains explicit years, require them locally as well
        years = _fix2d2v_years_from_key(canonical_key)
        if years:
            for y in years:
                if y and (_fix2d2u_norm(y) not in win):
                    return False, 'missing_required_year_tight'
        return True, ''
    finally:
        pass


# against injected-year pollution for unit/count metrics.
#
# Motivation:
#   Even with downstream yearlike blocking, schema_only_rebuild_fix17 can
#   still select a nearby year token (e.g., 2024/2025) as the VALUE for a
#   unit_sales metric when the context includes headings like "YTD 2025".
#   This patch moves the rejection upstream, at candidate-eligibility time,
#   and fixes a variable typo that could silently disable FIX2D2U gating.
#
# Policy (for unit/count-like schema keys):
#   - Reject yearlike numeric candidates unless they carry unit evidence.
#   - Prefer candidates with unit evidence (unit_tag/unit/unit_family/context).
#   - Keep existing bare-year token guards as a last-mile safety net.

def _fix2d63_is_yearlike_value(cand: dict) -> bool:
    try:
        v = cand.get('value_norm')
        if v is None:
            try:
                v = float(cand.get('value') or 0.0)
            except Exception:
                pass
                v = None
        if v is None:
            return False
        iv = int(float(v))
        # Conservative year window
        return (1900 <= iv <= 2100) and abs(float(v) - float(iv)) < 1e-9
    except Exception:
        return False


def _fix2d63_has_unit_evidence(cand: dict) -> bool:
    try:
        ut = str(cand.get('unit_tag') or cand.get('unit') or '').strip()
        uf = str(cand.get('unit_family') or '').strip().lower()
        mk = str(cand.get('measure_kind') or '').strip().lower()
        ma = str(cand.get('measure_assoc') or '').strip().lower()
        if ut:
            return True
        if uf in ('magnitude', 'percent', 'currency', 'energy', 'index'):
            return True
        if mk in ('count_units', 'count', 'quantity'):
            return True
        if ma in ('units', 'unit_sales', 'sales'):
            return True
        ctx = (str(cand.get('context_snippet') or cand.get('context') or cand.get('context_window') or '') + ' ' + str(cand.get('raw') or '')).lower()
        # Contextual unit hints
        if any(w in ctx for w in ['million', 'billion', 'thousand', 'trillion', 'units', 'unit', 'vehicles', '%', 'percent', 'usd', 'sgd', 'eur', '$', '€', '£', '¥']):
            return True
        return False
    except Exception:
        return False


def _fix2d63_schema_expects_unit_or_count(canonical_key: str, spec: dict) -> bool:
    try:
        ck = str(canonical_key or '')
        dim = str((spec or {}).get('dimension') or '').strip().lower()
        uf = str((spec or {}).get('unit_family') or '').strip().lower()
        # Suffix-based (most reliable given existing schema patterns)
        if ck.endswith('__unit_sales') or ck.endswith('__units') or ck.endswith('__unit'):
            return True
        # Schema hints
        if dim in ('unit_sales', 'count', 'quantity'):
            return True
        if uf == 'magnitude' and dim:
            # magnitude metrics are generally non-year values unless explicitly year metrics
            return True
        return False
    except Exception:
        return False


def _analysis_canonical_final_selector_v1(
    canonical_key: str,
    schema_frozen: dict,
    candidates: list,
    anchors: dict = None,
    prev_metric: dict = None,
    web_context: dict = None,
) -> tuple:
    """Pure selector: returns (best_metric_or_None, meta_dict)."""
    import re

    meta = {
        "selector_used": "analysis_canonical_v1",
        "canonical_key": canonical_key or "",
        "anchor_used": False,
        "blocked_reason": "",
        "preferred_url": "",
        "chosen_source_url": "",
        "tie_break": "",
        "eligible_count": 0,
        "range_method": "",
    }

    spec = schema_frozen or {}
    if not isinstance(spec, dict) or not spec:
        meta["blocked_reason"] = "missing_schema"
        return None, meta

    # Determine preferred URL from anchors (strongest) or schema if present
    preferred_url = ""
    anchor = None
    if isinstance(anchors, dict) and canonical_key in anchors and isinstance(anchors.get(canonical_key), dict):
        anchor = anchors.get(canonical_key) or {}
        preferred_url = anchor.get("source_url") or ""
    preferred_url = preferred_url or (spec.get("preferred_url") or spec.get("source_url") or "")
    if preferred_url:
        meta["preferred_url"] = _ph2b_norm_url(preferred_url)

    # Prepare FIX16 keyword scoring (same as rebuild_metrics_from_snapshots_schema_only_fix16)
    def _norm(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def _cand_sort_key(c: dict):
        try:
            return (
                str(c.get("anchor_hash") or ""),
                str(c.get("source_url") or ""),
                int(c.get("start_idx") or 0),
                str(c.get("raw") or ""),
                str(c.get("unit") or ""),
                float(c.get("value_norm") or 0.0),
            )
        except Exception:
            return ("", "", 0, "", "", 0.0)

    # FIX16 keyword pruning helper if present; otherwise keep keywords as-is
    metric_is_year_like = False
    try:
        fn_year = globals().get("_fix16_metric_is_year_like")
        if callable(fn_year):
            metric_is_year_like = bool(fn_year(spec, canonical_key=canonical_key))
    except Exception:
        pass
        metric_is_year_like = False

    keywords = spec.get("keywords") or spec.get("keyword_hints") or []
    if isinstance(keywords, str):
        keywords = [keywords]
    try:
        fn_prune = globals().get("_fix16_prune_year_keywords")
        if callable(fn_prune):
            keywords = fn_prune(list(keywords), metric_is_year_like)
    except Exception:
        pass
        keywords = list(keywords) if isinstance(keywords, list) else []


    # REFACTOR117: For charging investment-by-2040 schema key, accept common synonyms used in sources
    # (e.g., 'spend', 'spending', 'market', 'total investments') without changing the frozen schema.
    try:
        if str(canonical_key or "") == "global_ev_charging_investment_2040__currency":
            _extra_kw = [
                "investment", "investments", "total investments",
                "spend", "spending", "annual spend", "annual spending",
                "infrastructure spend", "infrastructure spending",
                "market", "market size", "capex",
            ]
            if not isinstance(keywords, list):
                keywords = list(keywords) if keywords is not None else []
            for _k in _extra_kw:
                if _k and _k not in keywords:
                    keywords.append(_k)
    except Exception:
        pass

    kw_norm = [_norm(k) for k in (keywords or []) if k]

    # Candidate filtering
    cands = [c for c in (candidates or []) if isinstance(c, dict)]
    try:
        meta["candidate_count_in"] = int(len(cands))
    except Exception:
        pass
    # Enforce preferred source lock when available (prevents cross-source hijack)
    if meta["preferred_url"]:
        pref = meta["preferred_url"]
        cands_pref = []
        for c in cands:
            cu = _ph2b_norm_url(c.get("source_url") or "")
            if cu and cu == pref:
                cands_pref.append(c)
        # If preferred exists but yields zero candidates, we keep empty (hard lock).
        cands = cands_pref
        try:
            meta["candidate_count_pref"] = int(len(cands))
        except Exception:
            pass

    eligible = []
    for c in cands:
        try:
            # Many snapshot candidates omit unit_family even when unit_tag/raw clearly indicates
            # magnitude/percent/currency. The analysis selector treats unit_family as authoritative
            # for schema gating; leaving it blank causes false ineligibility (empty Current).
            try:
                if isinstance(c, dict) and not str(c.get("unit_family") or "").strip():
                    _raw = str(c.get("raw") or "")
                    _ut = str(c.get("unit_tag") or c.get("unit") or "")
                    _ctx = str(c.get("context_snippet") or "")
                    _blob = (" ".join([_raw, _ut, _ctx])).lower()
                    uf = ""
                    if "%" in _blob or "percent" in _blob or "percentage" in _blob:
                        uf = "percent"
                    elif any(tok in _blob for tok in ["usd", "sgd", "eur", "gbp", "$", "€", "£", "¥", "aud", "cad", "inr", "cny", "rmb"]):
                        uf = "currency"
                    else:
                        # Magnitude / counts (incl. unit sales)
                        if any(w in _blob for w in ["million", "billion", "thousand", "trillion"]) or re.search(r"[mbkt]", _blob):
                            uf = "magnitude"
                        elif str(c.get("measure_kind") or "").lower() in ("count_units", "count", "quantity"):
                            uf = "magnitude"
                        elif str(c.get("measure_assoc") or "").lower() in ("units", "unit_sales", "sales"):
                            uf = "magnitude"
                    if uf:
                        c["unit_family"] = uf
                        # unit_cmp hint (best-effort; used only for display/debug)
                        if not str(c.get("unit_cmp") or "").strip():
                            if uf == "percent":
                                c["unit_cmp"] = "%"
                            elif uf == "currency":
                                c["unit_cmp"] = "currency"
                            else:
                                c["unit_cmp"] = (_ut or "").strip()
            except Exception:
                pass

            if not _fix16_candidate_allowed(c, spec, canonical_key=canonical_key):
                continue

            # FIX2D2R: forbid bare-year tokens when a better sibling exists nearby (parity guard)
            try:
                _ek = 'other'
                _ed = (expected_dim or '').lower().strip()
                if canonical_key.endswith('__percent') or _ed == 'percent':
                    _ek = 'percent'
                elif canonical_key.endswith('__currency') or _ed == 'currency':
                    _ek = 'currency'
                elif canonical_key.endswith('__unit_sales') or 'unit' in canonical_key.lower() or 'sales' in canonical_key.lower():
                    _ek = 'unit'
                elif canonical_key.endswith('__year') or 'year' in _ed:
                    _ek = 'year'

                if _ek != 'year' and _fix2d2r_is_bare_year_cand(c) and not str(c.get('unit') or c.get('unit_tag') or '').strip():
                    if _fix2d2r_has_better_sibling(c, candidates, _ek):
                        continue
            except Exception:
                pass

            # - Rejects percent/currency evidence when schema expects magnitude (prevents % hijacks).
            # - Suppresses unitless bare years (e.g., 2030) as candidates.
            # - Enforces unit evidence for scaled magnitude schemas (million/billion/etc.).
            try:
                _raw0 = str(c.get("raw") or "").strip()
                _raw0_l = _raw0.lower()
                _v0 = c.get("value_norm", None)
                if _v0 is None:
                    _v0 = c.get("value", None)

                _cand_family = str(c.get("unit_family") or "").strip().lower()
                _cand_ucmp = str(c.get("unit_cmp") or c.get("unit_tag") or "").strip().lower()
                _spec_family = str(spec.get("unit_family") or "").strip().lower()
                # If schema.unit_family is missing/blank, infer expected family deterministically
                # from schema.dimension / canonical_key suffixes using existing FIX17 helper.
                # This prevents silent bypass of percent/currency hard-gates.
                try:
                    if not _spec_family:
                        _fn_exp = globals().get("_fix17_expected_dimension")
                        if callable(_fn_exp):
                            _exp = str(_fn_exp(spec, canonical_key=canonical_key) or "").strip().lower()
                            if _exp in ("percent", "currency", "magnitude", "rate", "ratio"):
                                _spec_family = _exp
                except Exception:
                    pass
                _spec_ut = str(spec.get("unit_tag") or spec.get("unit") or "").strip().lower()

                # evidence flags
                _has_pct = ("%" in _raw0) or ("percent" in _raw0_l) or ("%" in _cand_ucmp) or (_cand_family == "percent")
                _has_ccy = any(sym in _raw0 for sym in ("$", "€", "£", "¥")) or any(tok in _raw0_l for tok in ("usd", "eur", "sgd", "gbp", "jpy")) or (_cand_family == "currency")

                _has_unit_ev0 = False
                try:
                    for _k in ("base_unit", "unit", "unit_tag", "unit_family"):
                        if str(c.get(_k) or "").strip():
                            _has_unit_ev0 = True
                            break
                    if not _has_unit_ev0:
                        if any(tok in _raw0_l for tok in ("million", "billion", "trillion", "mn", "bn", "thousand")):
                            _has_unit_ev0 = True
                        if _has_pct or _has_ccy:
                            _has_unit_ev0 = True
                except Exception:
                    pass

                # unit-family hard gating (schema-driven)
                try:
                    if _spec_family == "percent":
                        if not _has_pct:
                            meta["blocked_reason"] = "percent_evidence_missing_hard_block"
                            continue
                    elif _spec_family == "currency":
                        if not _has_ccy:
                            meta["blocked_reason"] = "currency_evidence_missing_hard_block"
                            continue
                    elif _spec_family == "magnitude":
                        # reject percent/currency candidates outright
                        if _has_pct or _has_ccy:
                            meta["blocked_reason"] = "unit_mismatch_hard_block"
                            continue
                except Exception:
                    pass

                # year-only candidate suppression (strict): only when unit evidence is missing
                try:
                    if (not _has_unit_ev0) and isinstance(_v0, (int, float)):
                        _iv0 = int(float(_v0))
                        if 1900 <= _iv0 <= 2100:
                            import re as _re
                            if _re.fullmatch(r"(19\d{2}|20\d{2})", _raw0 or str(_iv0)):
                                continue
                except Exception:
                    pass

                # scaled magnitude requires unit evidence (schema implies million/billion/etc.)
                try:
                    _spec_nm = str(spec.get("name") or "").lower()
                    _scaled = any(t in _spec_ut for t in ("million", "billion", "trillion", "thousand")) or (_spec_ut in ("m", "b", "t", "k"))
                    _countish = any(t in _spec_nm for t in ("unit", "units", "sales", "deliveries", "shipments", "registrations", "volume"))
                    if _spec_family == "magnitude" and _scaled and _countish and (not _has_unit_ev0):
                        meta["blocked_reason"] = "unit_evidence_missing_hard_block"
                        continue
                except Exception:
                    pass

                # Purpose: Fix broken scale/countish gating where earlier patch strings were truncated
                #          (e.g., "milli..." / "uni...") and therefore never matched.
                #          Enforce: if schema implies scaled magnitude (million/billion/etc.), unit evidence
                #          must exist in the candidate (unit_tag/unit_cmp/raw/context), else hard-block.
                # Safety: additive-only; does not change fastpath/hashing/injection/snapshot attach.
                try:
                    _spec_nm2 = str(spec.get("name") or spec.get("label") or "").lower()
                    _spec_ut2 = str(_spec_ut or "").lower()

                    def _ph2b_has_scale_token(_s: str) -> bool:
                        try:
                            _s = str(_s or "").lower()
                        except Exception:
                            pass
                            _s = ""
                        if not _s:
                            return False
                        toks = [
                            "million", "mn", "m", "millions",
                            "billion", "bn", "b", "billions",
                            "trillion", "tn", "t", "trillions",
                            "thousand", "k", "000",
                        ]
                        # require word-boundary-ish for single-letter tokens
                        if "million" in _s or "millions" in _s or "billion" in _s or "billions" in _s or "trillion" in _s or "trillions" in _s or "thousand" in _s:
                            return True
                        import re as _re2
                        if _re2.search(r"\b(mn|bn|tn|k)\b", _s):
                            return True
                        # "m" / "b" / "t" are too ambiguous; only accept when adjacent to "usd/units/sales" etc.
                        if _re2.search(r"\b(m|b|t)\b", _s) and _re2.search(r"(units?|sales|deliveries|shipments|vehicles|usd|eur|sgd|gbp|jpy|cny|aud|cad)", _s):
                            return True
                        return False

                    def _ph2b_has_unit_evidence_candidate(_c: dict) -> bool:
                        if not isinstance(_c, dict):
                            return False
                        if str(_c.get("unit_tag") or "").strip():
                            return True
                        if str(_c.get("unit_cmp") or "").strip():
                            return True
                        if str(_c.get("unit_family") or "").strip():
                            return True
                        # raw/context tokens
                        if _ph2b_has_scale_token(_c.get("raw")):
                            return True
                        if _ph2b_has_scale_token(_c.get("context_snippet") or _c.get("context")):
                            return True
                        return False

                    _schema_scaled = _ph2b_has_scale_token(_spec_ut2)
                    # If schema is scaled, we hard-require candidate unit evidence regardless of name.
                    if _spec_family == "magnitude" and _schema_scaled and (not _ph2b_has_unit_evidence_candidate(c0)):
                        meta["blocked_reason"] = "unit_evidence_missing_hard_block"
                        continue

                    # Extra guard: suppress obvious document-structure numbers (pages/figures) when schema is scaled.
                    try:
                        if _spec_family == "magnitude" and _schema_scaled:
                            _ctx = str((c0 or {}).get("context_snippet") or (c0 or {}).get("context") or "").lower()
                            if ("pages" in _ctx) or ("figures" in _ctx) or ("page " in _ctx):
                                # treat as ineligible rather than junking globally
                                continue
                    except Exception:
                        pass
                except Exception:
                    pass

                # Why:
                # - Earlier gating treated inferred unit_family='magnitude' as "unit evidence", allowing unitless
                #   integers (e.g., 170) to pass for schemas like "million units".
                # - For scaled schemas, we require explicit scale evidence in raw/unit_tag/unit_cmp (million/billion/etc.).
                # Safety:
                # - Only applies when schema implies a scale (million/billion/thousand/trillion or M/B/K/T).
                # - Does NOT change fastpath/hashing/injection/snapshot attach.
                try:
                    _scaled2 = False
                    try:
                        _scaled2 = any(t in _spec_ut for t in ("million", "billion", "trillion", "thousand")) or (_spec_ut in ("m", "b", "t", "k"))
                    except Exception:
                        pass
                        _scaled2 = False
                    if _spec_family == "magnitude" and _scaled2:
                        _blob = (" ".join([
                            str(c.get("raw") or ""),
                            str(c.get("unit_cmp") or ""),
                            str(c.get("unit_tag") or c.get("unit") or ""),
                            str(c.get("context_snippet") or c.get("context") or ""),
                        ])).lower()
                        _has_scale_ev = any(tok in _blob for tok in ("million", "billion", "trillion", "thousand", "mn", "bn")) or bool(re.search(r"\b[mbkt]\b", _blob))
                        # For scaled schemas, require scale to *match* the schema (e.g., million vs billion).
                        # Prevents wrong-scale candidates from surviving for 'million units' schemas.
                        try:
                            _schema_scale = ""
                            if "million" in _spec_ut or _spec_ut == "m":
                                _schema_scale = "m"
                            elif "billion" in _spec_ut or _spec_ut == "b":
                                _schema_scale = "b"
                            elif "thousand" in _spec_ut or _spec_ut == "k":
                                _schema_scale = "k"
                            elif "trillion" in _spec_ut or _spec_ut == "t":
                                _schema_scale = "t"

                            _cand_scale = ""
                            if any(t in _blob for t in ("million", "mn")) or bool(re.search(r"m", _blob)):
                                _cand_scale = "m"
                            elif any(t in _blob for t in ("billion", "bn")) or bool(re.search(r"b", _blob)):
                                _cand_scale = "b"
                            elif "thousand" in _blob or bool(re.search(r"k", _blob)):
                                _cand_scale = "k"
                            elif "trillion" in _blob or bool(re.search(r"t", _blob)):
                                _cand_scale = "t"

                            if _schema_scale and _cand_scale and _schema_scale != _cand_scale:
                                meta["blocked_reason"] = "scale_mismatch_hard_block"
                                continue
                        except Exception:
                            pass
                        if not _has_scale_ev:
                            meta["blocked_reason"] = "scale_evidence_missing_hard_block"
                            continue
                except Exception:
                    pass
            except Exception:
                pass
        except Exception:
            pass
            continue
        # Prevents year tokens like 2030 from being committed as metric values for non-year metrics,
        # and prevents unrelated snippets (e.g., "By 2030 ... sales ...") from satisfying chargers/investment metrics.
        try:
            _fix2d17_spec = spec if isinstance(spec, dict) else {}
            _fix2d17_ckey = str(canonical_key or "")
            _fix2d17_dim = str(_fix2d17_spec.get("dimension") or "")
            _fix2d17_unitfam = str(_fix2d17_spec.get("unit_family") or "")
            _fix2d17_ut = str(_fix2d17_spec.get("unit_tag") or _fix2d17_spec.get("unit") or "")
            _fix2d17_year_metric = (
                ("year" in _fix2d17_dim.lower())
                or ("year" in _fix2d17_unitfam.lower())
                or (_fix2d17_ut.lower() in ["year", "yr", "years"])
                or (re.search(r"(?:^|_)(?:19|20)\d{2}(?:_|$)", _fix2d17_ckey) is not None)
            )

            _raw = str(c.get("raw") or "")
            try:
                _vf = float(c.get("value_norm") or 0.0)
            except Exception:
                pass
                _vf = 0.0
            _raw_digits = re.sub(r"[^0-9]", "", _raw or "")
            _looks_year = (1900.0 <= _vf <= 2100.0) and (len(_raw_digits) == 4 and _raw_digits == str(int(_vf)))

            _cand_unit = str(c.get("unit") or c.get("unit_tag") or "").strip()
            _cand_uf = str(c.get("unit_family") or "").strip().lower()

            if _looks_year and not _fix2d17_year_metric:
                # Allow only if this "year" token is clearly attached to a real unit (rare); otherwise reject.
                if (not _cand_unit) and (_cand_uf in ["", "unknown", "none"]):
                    meta["fix2d17_reject_bare_year"] = int(meta.get("fix2d17_reject_bare_year") or 0) + 1
                    continue

            # Domain keyword overlap enforcement for certain metrics to prevent cross-metric pollution.
            ctx_blob = (" " + str(c.get("context_snippet") or c.get("context") or "") + " " + _raw + " ").lower()
            # Normalize to token-ish spaces
            ctx_tok = " " + re.sub(r"[^a-z0-9]+", " ", ctx_blob) + " "

            domain_tokens = {
                "charger", "chargers", "charging", "station", "stations", "infrastructure",
                "investment", "invest", "capex", "spend", "spending",
                "sales", "sold", "market", "share", "revenue", "turnover",
                "units", "deliveries", "registrations",
            }

            kw = _fix2d17_spec.get("keywords") or []
            if isinstance(kw, (list, tuple)):
                kw_l = [str(x).lower().strip() for x in kw if str(x).strip()]
            else:
                kw_l = []
            required = [k for k in kw_l if k in domain_tokens]

            # If schema explicitly includes domain tokens, require at least one to appear in candidate context/raw.
            if required:
                if not any((" " + t + " ") in ctx_tok for t in required):
                    meta["fix2d17_reject_domain_mismatch"] = int(meta.get("fix2d17_reject_domain_mismatch") or 0) + 1
                    continue
        except Exception:
            pass

        eligible.append(c)

    meta["eligible_count"] = int(len(eligible) or 0)
    try:
        meta["candidate_count_eligible"] = int(len(eligible))
    except Exception:
        pass

    if not eligible:
        meta["blocked_reason"] = "no_eligible_candidates_in_preferred_source" if meta["preferred_url"] else "no_eligible_candidates"
        return None, meta

    # Deterministic scoring: keyword hits + stable tie-break
    best = None
    best_tie = None
    for c in sorted(eligible, key=_cand_sort_key):
        ctx = _norm(c.get("context_snippet") or c.get("context") or c.get("context_window") or "")
        raw = _norm(c.get("raw") or "")
        hits = 0
        for k in kw_norm:
            if not k:
                continue
            if k in ctx:
                hits += 2
            if k in raw:
                hits += 1

        # Prefer anchored candidate_id/anchor_hash when anchors exist
        anchor_bonus = 0
        if isinstance(anchor, dict) and anchor:
            ah = str(anchor.get("anchor_hash") or "")
            cid = str(anchor.get("candidate_id") or "")
            if ah and str(c.get("anchor_hash") or "") == ah:
                anchor_bonus += 10
            if cid and str(c.get("candidate_id") or "") == cid:
                anchor_bonus += 10

        score = hits + anchor_bonus

        # Tie-break: higher score, then earlier occurrence, then stable sort key
        tie = (int(score), -int(c.get("start_idx") or 0), _cand_sort_key(c))
        if best is None or tie > best_tie:
            best = c
            best_tie = tie

    if best is None:
        meta["blocked_reason"] = "no_winner_after_scoring"
        return None, meta

    try:
        _raw = str(best.get("raw") or "")
        try:
            _vf = float(best.get("value_norm") or 0.0)
        except Exception:
            pass
            _vf = 0.0
        _raw_digits = re.sub(r"[^0-9]", "", _raw or "")
        _looks_year = (1900.0 <= _vf <= 2100.0) and (len(_raw_digits) == 4 and _raw_digits == str(int(_vf)))
        _dim = str(spec.get("dimension") or "")
        _uf = str(spec.get("unit_family") or "")
        _ut = str(spec.get("unit_tag") or spec.get("unit") or "")
        _year_metric = (
            ("year" in _dim.lower()) or ("year" in _uf.lower()) or (_ut.lower() in ["year", "yr", "years"])
            or (re.search(r"(?:^|_)(?:19|20)\\d{2}(?:_|$)", str(canonical_key or "")) is not None)
        )
        _cand_unit = str(best.get("unit") or best.get("unit_tag") or "").strip()
        _cand_uf = str(best.get("unit_family") or "").strip().lower()
        if _looks_year and (not _year_metric) and (not _cand_unit) and (_cand_uf in ["", "unknown", "none"]):
            meta["blocked_reason"] = "fix2d17_reject_bare_year_best"
            meta["fix2d17_bare_year_best"] = True
            return None, meta
    except Exception:
        pass

    # Build value_range in schema units (NO double divide)
    try:
        vals = []
        for c in eligible:
            v = c.get("value_norm")
            if v is None:
                # fallback to parsing 'value'/'raw' in the unit of the candidate/spec
                try:
                    fn_parse = globals().get("_parse_num")
                    if callable(fn_parse):
                        v = fn_parse(c.get("value"), c.get("unit") or "") or fn_parse(c.get("raw"), c.get("unit") or "")
                except Exception:
                    pass
                    v = None
            if v is None:
                continue
            try:
                vals.append(float(v))
            except Exception:
                pass
        if len(vals) >= 2:
            vmin = min(vals); vmax = max(vals)
            meta["value_range"] = {"min": vmin, "max": vmax, "n": len(vals), "method": "ph2b_schema_unit_range_v2|fix2b_range4"}
            meta["range_method"] = "ph2b_schema_unit_range_v2|fix2b_range4"
    except Exception:
        pass

    out = {
        "name": spec.get("name") or spec.get("label") or canonical_key,
        "canonical_key": canonical_key,
        "value": best.get("value"),
        "unit": best.get("unit") or spec.get("unit_tag") or "",
        "value_norm": best.get("value_norm"),
        "source_url": best.get("source_url") or "",
        "anchor_hash": best.get("anchor_hash") or "",
        "candidate_id": best.get("candidate_id") or "",
        "context_snippet": best.get("context_snippet") or best.get("context") or "",
        "anchor_used": bool(isinstance(anchor, dict) and anchor and (
            (anchor.get("anchor_hash") and str(best.get("anchor_hash") or "") == str(anchor.get("anchor_hash")))
            or (anchor.get("candidate_id") and str(best.get("candidate_id") or "") == str(anchor.get("candidate_id")))
        )),
        "evidence": [{
            "source_url": best.get("source_url") or "",
            "raw": best.get("raw") or "",
            "context_snippet": (best.get("context_snippet") or best.get("context") or best.get("context_window") or "")[:400],
            "anchor_hash": best.get("anchor_hash") or "",
            "method": "analysis_canonical_selector_v1",
        }],
    }

    meta["anchor_used"] = bool(out.get("anchor_used"))
    meta["chosen_source_url"] = _ph2b_norm_url(out.get("source_url") or "")

    # - Adds winner_candidate_debug + would_block_reason for scaled schemas.
    try:
        _winner = out if isinstance(out, dict) else {}
        meta["winner_candidate_debug"] = {
        "canonical_key": str(canonical_key),
        "source_url": str(_winner.get("source_url") or ""),
        "source_url_norm": _ph2b_norm_url(_winner.get("source_url") or ""),
        "candidate_id": str(_winner.get("candidate_id") or _winner.get("id") or _winner.get("anchor_hash") or ""),
        "value_norm": _winner.get("value_norm"),
        "raw": str(_winner.get("raw") or _winner.get("value") or ""),
        "unit_cmp": str(_winner.get("unit_cmp") or ""),
        "unit_family": str(_winner.get("unit_family") or ""),
        "unit_tag": str(_winner.get("unit_tag") or ""),
        "context_snippet": str(_winner.get("context_snippet") or _winner.get("context") or ""),
        }

    # "Would block" diagnostic: scaled magnitude schema but chosen candidate lacks unit evidence.
        _spec_unit = str((schema_frozen or {}).get("unit_tag") or (schema_frozen or {}).get("unit") or "")
        _spec_unit_l = _spec_unit.lower()
        _is_scaled = any(tok in _spec_unit_l for tok in ["million", "billion", "trillion", "thousand", "mn", "bn", "m ", "b ", "k "]) or (_spec_unit.strip() in ["M", "B", "K", "T"])
        _winner_unit_cmp = str(_winner.get("unit_cmp") or "")
        _winner_unit_tag = str(_winner.get("unit_tag") or "")
        _winner_raw = str(_winner.get("raw") or _winner.get("value") or "")
        _winner_has_scale = any(tok in (_winner_raw.lower()) for tok in ["million", "billion", "trillion", "thousand"]) or (_winner_unit_tag.strip().upper() in ["M", "B", "K", "T"]) or (_winner_unit_cmp.strip().upper() in ["M", "B", "K", "T", "%"])
        if _is_scaled and not _winner_has_scale and not _winner_unit_cmp:
            meta["would_block_reason"] = "unit_evidence_missing_for_scaled_schema"
        else:
            meta["would_block_reason"] = ""
    except Exception:
        pass

    # - Does NOT change selection; purely diagnostic.
    try:
        meta["analysis_selector_trace_v1"] = {
            "selector_used": meta.get("selector_used"),
            "preferred_url": meta.get("preferred_url"),
            "chosen_source_url": meta.get("chosen_source_url"),
            "n_candidates_in": int(meta.get("candidate_count_in") or 0),
            "n_candidates_pref": int(meta.get("candidate_count_pref") or 0),
            "n_candidates_eligible": int(meta.get("candidate_count_eligible") or 0),
            "blocked_reason": meta.get("blocked_reason") or "",
            "anchor_used": bool(meta.get("anchor_used")),
            "would_block_reason": meta.get("would_block_reason") or "",
            "winner_candidate_debug": dict(meta.get("winner_candidate_debug") or {}) if isinstance(meta.get("winner_candidate_debug"), dict) else {},
        }
    except Exception:
        return out, meta


def rebuild_metrics_from_snapshots_analysis_canonical_v1(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """Batch rebuild using the extracted canonical selector (pure, deterministic)."""
    if not isinstance(prev_response, dict):
        return {}
    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )
    if not isinstance(metric_schema, dict) or not metric_schema:
        return {}

    metric_anchors = (
        prev_response.get("metric_anchors")
        or (prev_response.get("results") or {}).get("metric_anchors")
        or {}
    )
    if not isinstance(metric_anchors, dict):
        metric_anchors = {}

    # Flatten candidates from snapshots
    if isinstance(baseline_sources_cache, dict) and isinstance(baseline_sources_cache.get("snapshots"), list):
        sources = baseline_sources_cache.get("snapshots", [])
    elif isinstance(baseline_sources_cache, list):
        sources = baseline_sources_cache
    else:
        sources = []

    candidates = []
    for s in sources:
        if not isinstance(s, dict):
            continue
        url = s.get("source_url") or s.get("url") or ""
        xs = s.get("extracted_numbers")
        if isinstance(xs, list) and xs:
            for c in xs:
                if not isinstance(c, dict):
                    continue
                c2 = dict(c)
                if url and not c2.get("source_url"):
                    c2["source_url"] = url
                candidates.append(c2)

    try:
        _fix2s_apply_observed_to_canonical_rules_v1(candidates, metric_schema, web_context=web_context)
    except Exception:
        pass
    def _norm(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def _cand_sort_key(c: dict):
        try:
            return (
                str(c.get("anchor_hash") or ""),
                str(c.get("source_url") or ""),
                int(c.get("start_idx") or 0),
                str(c.get("raw") or ""),
                str(c.get("unit") or ""),
                float(c.get("value_norm") or 0.0),
            )
        except Exception:
            return ("", "", 0, "", "", 0.0)

    _fix2d16_disable_fix2d15 = False
    _fix2d17_disable_fix2d16 = True

    # (Legacy block retained for context but disabled)
    # Goals:
    #   1) Prevent bare-year tokens (e.g., 2030) from being selected as metric values
    #      when the metric does NOT explicitly expect a year-as-value.
    #   2) Prevent cross-metric pollution where a sales/market snippet satisfies
    #      unrelated schema metrics (e.g., chargers, charging investment).
    #   3) Enforce light unit-family expectations (percent / currency).
    # Notes:
    #   - This patch intentionally supersedes (and replaces) FIX2D15's earlier guard.
    #   - Keep changes local to schema_only_rebuild_fix17 selection.

    def _fix2d15_expects_year_value(spec: dict, canonical_key: str) -> bool:
        try:
            unit_hint = str(spec.get("unit_tag") or spec.get("unit") or "").lower()
            dim_hint = str(spec.get("dimension") or spec.get("value_type") or "").lower()
            if dim_hint == "year":
                return True
            if "year" in unit_hint:
                return True
            ck = str(canonical_key or "")
            if ck.endswith("__year") or ("_year_" in ck):
                return True
            return False
        except Exception:
            return False

    def _fix2d15_is_bare_year_token(cand: dict) -> bool:
        try:
            raw = str(cand.get("raw") or cand.get("value") or "").strip()
            v = cand.get("value_norm")
            try:
                vf = float(v)
            except Exception:
                pass
                try:
                    vf = float(re.sub(r"[^0-9\.\-]+", "", raw or ""))
                except Exception:
                    return False

            if vf < 1900 or vf > 2100:
                return False

            # Accept raw like "2030" and "2030.0" (and "2030.00") as year tokens
            looks_year = False
            try:
                if re.fullmatch(r"\d{4}", raw or ""):
                    looks_year = True
                elif re.fullmatch(r"\d{4}\.0+", raw or ""):
                    looks_year = True
            except Exception:
                pass
                looks_year = False

            if not looks_year:
                try:
                    iv = int(vf)
                    looks_year = (abs(vf - iv) < 1e-6) and (1900 <= iv <= 2100)
                except Exception:
                    pass
                    looks_year = False

            if not looks_year:
                return False

            unit = str(cand.get("unit") or cand.get("unit_tag") or "").strip()
            if unit:
                return False
            if "%" in raw or "$" in raw or "€" in raw or "£" in raw:
                return False

            return True
        except Exception:
            return False


    def _fix2d15_metric_domain_tokens(canonical_key: str, spec: dict) -> list:
        """Derive a small set of domain tokens from canonical_key/name."""
        try:
            ck = _norm(str(canonical_key or ""))
            nm = _norm(str(spec.get("name") or spec.get("metric_name") or ""))
            base = " ".join([ck, nm]).strip()
            toks = [w for w in base.split() if w and not re.fullmatch(r"\d{4}", w)]
            stop = set(["global","world","worldwide","total","overall","ev","electric","vehicle","vehicles","metric","market"])
            out = []
            for w in toks:
                if w in stop:
                    continue
                if len(w) < 4:
                    continue
                if w not in out:
                    out.append(w)
            return out[:6]
        except Exception:
            return []

    # - Prevent generic keyword hits (e.g., 'global', 'market') from allowing
    #   unrelated candidates (e.g., year tokens) to satisfy domain-specific
    #   metrics like chargers/investment.
    # - For certain metrics, require at least one strong domain token to
    #   appear in candidate context/raw.
    def _fix2d19_required_domain_tokens(canonical_key: str, spec: dict) -> list:
        try:
            if bool(globals().get("_FIX2D20_DISABLE_FIX2D19", False)):
                return []
            ck = str(canonical_key or '').lower()
            dim = str(spec.get('dimension') or spec.get('value_type') or '').lower()
            uf = str(spec.get('unit_family') or '').lower()
            req = []
            if 'charger' in ck or 'charging' in ck:
                req += ['charger', 'charging', 'station', 'infrastructure']
            if 'investment' in ck or 'capex' in ck or dim == 'currency' or uf == 'currency':
                # still allow currency cues to satisfy, but require at least one semantic token
                req += ['invest', 'investment', 'capex', 'spend', 'spending', 'cost']
            if 'share' in ck or dim == 'percent' or uf == 'percent':
                req += ['share', 'market share', 'ev share']
            if 'sale' in ck or 'sales' in ck or dim in ('unit_sales','sales','units') or uf in ('unit_sales','sales','units'):
                req += ['sales', 'sold', 'deliveries', 'deliver']
            # de-dupe while keeping order
            out=[]
            for r in req:
                r=str(r).strip().lower()
                if r and r not in out:
                    out.append(r)
            return out[:8]
        except Exception:
            return []


    # - This is the decisive fix: reject invalid candidates *before* ranking.
    # - Prevents bare year tokens (e.g., 2024/2030/2030.0) from ever being
    #   eligible evidence for non-year metrics.
    # - Enforces unit-family requirements and required domain-token binding.
    def _fix2d22_candidate_eligible(cand: dict, spec: dict, canonical_key: str, kw_norm: list) -> (bool, str):
        try:
            # 1) Hard bare-year rejection unless metric explicitly expects year-as-value
            if _fix2d15_is_bare_year_token(cand) and not _fix2d15_expects_year_value(spec, canonical_key):
                return False, 'bare_year_token'

            # 2) Unit-family enforcement (percent/currency/unit_sales)
            if not _fix2d15_unit_family_ok(cand, spec):
                return False, 'unit_family_mismatch'

            # 3) Required domain token binding (strong)
            ctx = _norm(cand.get('context_snippet') or cand.get('context') or cand.get('context_window') or '')
            rawn = _norm(cand.get('raw') or '')

            try:
                _yrs = re.findall(r"\b(19\d{2}|20\d{2})\b", str(canonical_key or ""))
                if _yrs:
                    _hit_all = True
                    for _y in _yrs:
                        if _y and (_y not in ctx) and (_y not in rawn):
                            _hit_all = False
                            break
                    if not _hit_all:
                        return False, 'missing_required_year_token'
            except Exception:
                pass


            req_dom = _fix2d19_required_domain_tokens(canonical_key, spec)
            if req_dom:
                hit = 0
                for r in req_dom:
                    rr = _norm(r)
                    if rr and (rr in ctx or rr in rawn):
                        hit += 1
                        break
                if hit <= 0:
                    return False, 'missing_required_domain_token'

            # 4) If schema provides keywords/domain hints, require at least one hit
            dom = _fix2d15_metric_domain_tokens(canonical_key, spec)
            if (kw_norm or dom):
                hit_kw = 0
                for k in (kw_norm or []):
                    if k and (k in ctx or k in rawn):
                        hit_kw += 1
                        break
                hit_dom = 0
                for d in (dom or []):
                    if d and (d in ctx or d in rawn):
                        hit_dom += 1
                        break
                if hit_kw <= 0 and hit_dom <= 0:
                    return False, 'no_keyword_or_domain_hits'

            return True, ''
        except Exception:
            return True, ''

    def _fix2d15_unit_family_ok(cand: dict, spec: dict) -> bool:
        try:
            dim = str(spec.get("dimension") or "").lower()
            uf = str(spec.get("unit_family") or "").lower()
            raw = str(cand.get("raw") or "")
            unit = str(cand.get("unit") or cand.get("unit_tag") or "").lower()

            s = (raw + " " + unit).lower()
            has_currency = ("$" in raw) or ("us$" in s) or ("usd" in s) or ("eur" in s) or ("gbp" in s) or ("€" in raw) or ("£" in raw)
            has_percent = ("%" in raw) or ("percent" in s) or ("pct" in s)

            if dim == "percent" or uf == "percent":
                return has_percent
            if dim == "currency" or uf == "currency":
                return has_currency

            # FIX2D18: unit-sales expectations (prevents bare years like 2030 from winning)
            if not bool(globals().get("_FIX2D20_DISABLE_FIX2D18", False)):
                if dim in ("unit_sales","units","sales") or uf in ("unit_sales","units","sales"):
                    # require some unit cue
                    has_units = ("unit" in s) or ("units" in s) or ("million" in s) or ("mn" in s) or ("m " in s) or (" m" in s)
                    if not has_units:
                        return False

            return True
        except Exception:
            return True

    def _fix2d15_candidate_ok(cand: dict, spec: dict, canonical_key: str, kw_norm: list) -> (bool, str):
        try:
            if _fix2d15_is_bare_year_token(cand) and not _fix2d15_expects_year_value(spec, canonical_key):
                return False, "bare_year_token"

            if not _fix2d15_unit_family_ok(cand, spec):
                return False, "unit_family_mismatch"

            ctx = _norm(cand.get("context_snippet") or cand.get("context") or cand.get("context_window") or "")
            rawn = _norm(cand.get("raw") or "")
            dom = _fix2d15_metric_domain_tokens(canonical_key, spec)

            # FIX2D19: strong required domain-token binding
            req_dom = _fix2d19_required_domain_tokens(canonical_key, spec)
            if req_dom:
                _req_hit = 0
                for r in req_dom:
                    if not r:
                        continue
                    rr = _norm(r)
                    if rr and (rr in ctx or rr in rawn):
                        _req_hit += 1
                if _req_hit <= 0:
                    return False, 'missing_required_domain_token'


            hit_kw = 0
            for k in (kw_norm or []):
                if k and (k in ctx or k in rawn):
                    hit_kw += 1

            hit_dom = 0
            for d in (dom or []):
                if d and (d in ctx or d in rawn):
                    hit_dom += 1

            if (kw_norm or dom) and (hit_kw <= 0 and hit_dom <= 0):
                return False, "no_keyword_or_domain_hits"

            return True, ""
        except Exception:
            return True, ""

    candidates.sort(key=_cand_sort_key)

    # Debug sink
    dbg = prev_response.setdefault("_evolution_rebuild_debug", {})
    dbg.setdefault("schema_only_zero_hit_metrics_fix17", [])
    dbg.setdefault("fix2d15_reject_reasons", {})
    dbg.setdefault("fix2d22_reject_reasons", {})
    dbg.setdefault("fix2d22_year_reject_samples", [])
    dbg.setdefault("fix2d15_year_reject_samples", [])

    rebuilt = {}

    for canonical_key, sch in metric_schema.items():
        if not isinstance(sch, dict):
            continue

        spec = dict(sch)
        spec.setdefault("canonical_key", canonical_key)
        spec.setdefault("name", sch.get("name") or canonical_key)

        # Use fix16 year-like & keyword pruning if available
        fn_metric_is_year_like = globals().get("_fix17_metric_is_year_like")
        if callable(fn_metric_is_year_like):
            try:
                metric_is_year_like = bool(fn_metric_is_year_like(spec, canonical_key=canonical_key))
            except Exception:
                metric_is_year_like = False
        else:
            metric_is_year_like = False

        keywords = sch.get("keywords") or sch.get("keyword_hints") or []
        if isinstance(keywords, str):
            keywords = [keywords]

        fn_prune = globals().get("_fix16_prune_year_keywords")
        if callable(fn_prune):
            keywords2 = fn_prune(list(keywords), metric_is_year_like)
        else:
            keywords2 = list(keywords)

        kw_norm = [_norm(k) for k in (keywords2 or []) if k]

        best = None
        best_tie = None
        best_hits = 0

        # Motivation:
        #   schema_only_rebuild can still end up selecting a bare year token (e.g. 2024/2026)
        #   when that token appears in the same snippet as the real metric value.
        # Policy:
        #   If the schema does NOT expect a year-as-value and there exists at least one eligible
        #   non-year candidate in the pool for this canonical_key, then exclude bare-year tokens
        #   from competition for this canonical_key.
        # Notes:
        #   - This is a local, deterministic filter applied BEFORE keyword scoring.
        #   - It does not weaken year-blocking; it strengthens selection parity.
        _fix2d2s_expect_year = False
        try:
            _fix2d2s_expect_year = bool(_fix2d15_expects_year_value(spec, str(canonical_key)))
        except Exception:
            pass
            _fix2d2s_expect_year = False

        _fix2d2s_eligible = []
        _fix2d2s_has_non_year = False
        for _c in (candidates or []):
            ok, _reason = _fix17_candidate_allowed_with_reason(_c, spec, canonical_key=canonical_key)
            if not ok:
                continue
            try:
                # FIX2D63: bugfix - use the correct candidate variable (_c), not the outer loop's c.
                _ok_u, _why_u = _fix2d2u_semantic_eligible(_c, spec, canonical_key=str(canonical_key))
                if not _ok_u:
                    continue
            except Exception:
                pass

            # FIX2D63: upstream reject yearlike numeric tokens for unit/count metrics
            # unless the candidate carries unit evidence (prevents YTD 2025 headings from hijacking values).
            try:
                if _fix2d63_schema_expects_unit_or_count(str(canonical_key), spec):
                    if _fix2d63_is_yearlike_value(_c) and (not _fix2d63_has_unit_evidence(_c)):
                        dbg.setdefault('fix2d63_reject_yearlike_no_unit_evidence', 0)
                        dbg['fix2d63_reject_yearlike_no_unit_evidence'] = int(dbg.get('fix2d63_reject_yearlike_no_unit_evidence') or 0) + 1
                        continue
            except Exception:
                pass
            try:
                _ok2, _why2 = _fix2d22_candidate_eligible(_c, spec, canonical_key=str(canonical_key), kw_norm=kw_norm)
                if not _ok2:
                    dbg.setdefault('fix2d22_reject_reasons', {})
                    dbg['fix2d22_reject_reasons'][_why2] = int(dbg['fix2d22_reject_reasons'].get(_why2) or 0) + 1
                    if _why2 == 'bare_year_token':
                        dbg.setdefault('fix2d22_year_reject_samples', [])
                        if len(dbg.get('fix2d22_year_reject_samples') or []) < 20:
                            dbg['fix2d22_year_reject_samples'].append({
                                'canonical_key': str(canonical_key),
                                'raw': str(_c.get('raw') or _c.get('value') or '')[:80],
                                'value_norm': _c.get('value_norm'),
                                'unit': str(_c.get('unit') or ''),
                                'source_url': str(_c.get('source_url') or '')[:120],
                            })
                    continue
            except Exception:
                pass

            _fix2d2s_eligible.append(_c)
            try:
                if (not _fix2d2s_expect_year) and (not _fix2d15_is_bare_year_token(_c)):
                    _fix2d2s_has_non_year = True
            except Exception:
                pass

        if _fix2d2s_eligible and _fix2d2s_has_non_year and (not _fix2d2s_expect_year):
            try:
                dbg.setdefault('fix2d2s_filtered_bare_year_tokens', 0)
                _before = len(_fix2d2s_eligible)
                _fix2d2s_eligible = [x for x in _fix2d2s_eligible if not _fix2d15_is_bare_year_token(x)]
                dbg['fix2d2s_filtered_bare_year_tokens'] = int(dbg.get('fix2d2s_filtered_bare_year_tokens') or 0) + int(max(0, _before - len(_fix2d2s_eligible)))
            except Exception:
                pass

        for c in (_fix2d2s_eligible or []):
            ok, _reason = _fix17_candidate_allowed_with_reason(c, spec, canonical_key=canonical_key)
            if not ok:
                continue

            try:
                _ok_u, _why_u = _fix2d2u_semantic_eligible(c, spec, str(canonical_key))
                if not _ok_u:
                    dbg.setdefault('fix2d2u_reject_reasons_fix17', {})
                    dbg['fix2d2u_reject_reasons_fix17'][_why_u] = int(dbg['fix2d2u_reject_reasons_fix17'].get(_why_u) or 0) + 1
                    continue
            except Exception:
                pass            # PATCH FIX2D22 (ADD): eligibility-before-scoring gate
            try:
                _ok2, _why2 = _fix2d22_candidate_eligible(c, spec, canonical_key=str(canonical_key), kw_norm=kw_norm)
                if not _ok2:
                    dbg.setdefault("fix2d22_reject_reasons", {})
                    dbg["fix2d22_reject_reasons"][_why2] = int(dbg["fix2d22_reject_reasons"].get(_why2) or 0) + 1
                    if _why2 == 'bare_year_token':
                        dbg.setdefault("fix2d22_year_reject_samples", [])
                        if len(dbg.get("fix2d22_year_reject_samples") or []) < 20:
                            dbg["fix2d22_year_reject_samples"].append({
                                'canonical_key': str(canonical_key),
                                'raw': str(c.get('raw') or c.get('value') or '')[:80],
                                'value_norm': c.get('value_norm'),
                                'unit': str(c.get('unit') or ''),
                                'source_url': str(c.get('source_url') or '')[:120],
                            })
                    continue
            except Exception:
                pass

            ctx = _norm(c.get("context_snippet") or c.get("context") or c.get("context_window") or "")
            raw = _norm(c.get("raw") or "")

            hits = 0
            for k in kw_norm:
                if k and (k in ctx or k in raw):
                    hits += 1

            tie = (-hits,) + _cand_sort_key(c)
            if best is None or tie < best_tie:
                best = c
                best_tie = tie
                best_hits = hits

        if not isinstance(best, dict):
            continue

        # If schema has keywords, require at least one hit.
        if kw_norm and best_hits <= 0:
            dbg["schema_only_zero_hit_metrics_fix17"].append({"canonical_key": canonical_key, "reason": "no_keyword_hits"})
            continue

        # Even if earlier eligibility gates miss it, prevent committing a pure year
        # (e.g., 2030) as the metric value for non-year metrics.
        try:
            if _fix2d15_is_bare_year_token(best) and not _fix2d15_expects_year_value(spec, str(canonical_key)):
                dbg.setdefault("fix2d16_rejected_bare_year_commit", 0)
                dbg["fix2d16_rejected_bare_year_commit"] = int(dbg.get("fix2d16_rejected_bare_year_commit") or 0) + 1
                dbg.setdefault("fix2d16_rejected_bare_year_samples", [])
                if len(dbg.get("fix2d16_rejected_bare_year_samples") or []) < 20:
                    dbg["fix2d16_rejected_bare_year_samples"].append({
                        "canonical_key": str(canonical_key),
                        "raw": str(best.get("raw") or best.get("value") or "")[:80],
                        "value_norm": best.get("value_norm"),
                        "source_url": str(best.get("source_url") or "")[:120],
                    })
                continue
        except Exception:
            pass


        rebuilt[canonical_key] = {
            "canonical_key": canonical_key,
            "name": spec.get("name") or canonical_key,
            "value": best.get("value"),
            "unit": best.get("unit") or "",
            "value_norm": best.get("value_norm"),
            "source_url": best.get("source_url") or "",
            "anchor_hash": best.get("anchor_hash") or "",
            "evidence": [{
                "source_url": best.get("source_url") or "",
                "raw": best.get("raw") or "",
                "context_snippet": (best.get("context_snippet") or best.get("context") or best.get("context_window") or "")[:400],
                "anchor_hash": best.get("anchor_hash") or "",
                "method": "schema_only_rebuild_fix17",
            }],
            "anchor_used": False,
        }

    return rebuilt


# - Uses LOCAL context_snippet (not page-wide context_window)
# - Prevents cross-metric pollution in schema-only rebuild and other projections
# - Called by BOTH Analysis selector and Evolution rebuild paths

_FIX2D2U_ENABLE = True

def _fix2d2u_norm(s: str) -> str:
    try:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()
    except Exception:
        return ""

def _fix2d2u_local_text(cand: dict) -> str:
    try:
        if not isinstance(cand, dict):
            return ""
        for k in ("context_snippet", "context", "context_window", "context_window_raw", "context_window_text"):
            v=cand.get(k)
            if isinstance(v, str) and v.strip():
                return v
        return str(cand.get("raw") or cand.get("value") or "")
    except Exception:
        return ""

def _fix2d2u_required_token_groups(canonical_key: str, spec: dict) -> list:
    """Return a list of OR-groups; each group must have at least one hit in local text."""
    try:
        ck = str(canonical_key or "").lower()
        nm = str((spec or {}).get("name") or (spec or {}).get("metric_name") or "").lower()
        uf = str((spec or {}).get("unit_family") or "").lower()
        dim = str((spec or {}).get("dimension") or (spec or {}).get("value_type") or "").lower()
        base = ck + " " + nm

        groups = []

        # Geo cues
        if "china" in base or re.search(r"\bcn\b", base):
            groups.append(["china", "chinese"])

        # Time cues: if canonical key/name includes a year, require it locally
        years = re.findall(r"\b(19\d{2}|20\d{2})\b", base)
        for y in years[:2]:
            groups.append([y])

        # Metric family cues
        if ("charger" in base) or ("charging" in base):
            groups.append(["charger", "chargers", "charging", "station", "stations", "infrastructure"])
        if ("investment" in base) or ("capex" in base) or ("spend" in base) or (uf == "currency") or (dim == "currency"):
            groups.append(["investment", "invest", "capex", "spend", "spending", "cost", "expenditure"])
        if ("share" in base) or (uf == "percent") or (dim == "percent"):
            groups.append(["market share", "share", "%", "percent"])
        if ("sale" in base) or ("sales" in base) or (uf in ("unit_sales", "sales", "units")) or (dim in ("unit_sales", "sales", "units")):
            groups.append(["sales", "sold", "deliveries", "registrations", "units"])
        if "ytd" in base:
            groups.append(["ytd", "year to date"])

        # De-dupe groups
        out=[]
        for g in groups:
            gg=[]
            for t in g:
                tt=str(t or "").strip().lower()
                if tt and tt not in gg:
                    gg.append(tt)
            if gg:
                out.append(gg)
        return out
    except Exception:
        return []

def _fix2d2u_semantic_eligible(cand: dict, spec: dict, canonical_key: str) -> tuple:
    try:
        if not bool(globals().get("_FIX2D2U_ENABLE", True)):
            return True, ""
        groups = _fix2d2u_required_token_groups(canonical_key, spec)
        if not groups:
            return True, ""
        txt = _fix2d2u_local_text(cand)
        blob = _fix2d2u_norm(txt + " " + str(cand.get('raw') or ''))
        for g in groups:
            hit=False
            for tok in g:
                tn=_fix2d2u_norm(tok)
                if tn and tn in blob:
                    hit=True
                    break
            if not hit:
                return False, "missing_required_tokens"
        return True, ""
    except Exception:
        return True, ""


# - Keep names identical so evolution uses these as the LAST definitions

def rebuild_metrics_from_snapshots_with_anchors(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:  # noqa: F811
    return rebuild_metrics_from_snapshots_with_anchors_fix17(prev_response, baseline_sources_cache, web_context=web_context)


# Goal:
#   - If analysis emitted an anchor for a canonical metric, evolution MUST NOT
#     fall back to schema-only selection when the anchor cannot be used.
#   - This closes the final loophole where unitless year tokens win schema-only
#     scoring after an anchor rejection.
# Implementation:
#   - Provide a FIX18 schema-only rebuild that:
#       (1) rebuilds anchored metrics via rebuild_metrics_from_snapshots_with_anchors_fix17
#       (2) rebuilds ONLY unanchored metrics via rebuild_metrics_from_snapshots_schema_only_fix17
#       (3) merges results deterministically (anchored first)
#   - Re-wire the public rebuild_metrics_from_snapshots_schema_only to FIX18.
# Notes:
#   - Fully deterministic; no refetch; no LLM.
#   - Additive only: leaves FIX17 implementations intact.

def rebuild_metrics_from_snapshots_schema_only_fix18(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    FIX18 rebuild:
      - Anchored metrics: ONLY from anchor-aware rebuild (fix17), or skipped if rejected.
      - Unanchored metrics: schema-only rebuild (fix17) as before.
      - Never allows schema-only fallback to fill an anchored canonical_key.

    FIX2D21 add:
      - In Evolution baseline-compare mode, derive the schema keyspace from the Analysis baseline
        (previous_data.results.primary_metrics_canonical) so schema-only rebuild targets baseline keys.
    """
    if not isinstance(prev_response, dict):
        return {}

    # - This makes Evolution naturally produce current values for the baseline keyspace, enabling
    #   Analysis -> Evolution deltas without requiring injected URLs during Analysis.
    try:
        _fix2d21_prev_pmc = None
        if isinstance(prev_response.get("results"), dict):
            _fix2d21_prev_pmc = prev_response.get("results", {}).get("primary_metrics_canonical")
        if not isinstance(_fix2d21_prev_pmc, dict):
            _fix2d21_prev_pmc = prev_response.get("primary_metrics_canonical")

        if isinstance(_fix2d21_prev_pmc, dict) and _fix2d21_prev_pmc:
            ms_from_baseline = {}
            for ck, mo in _fix2d21_prev_pmc.items():
                if not isinstance(ck, str) or not ck:
                    continue
                spec = {}
                if isinstance(mo, dict):
                    # carry minimal hints to help selection
                    ut = mo.get("unit_tag") or mo.get("unit") or ""
                    uf = mo.get("unit_family") or ""
                    dim = mo.get("dimension") or mo.get("value_type") or ""
                    if not uf and isinstance(ut, str) and "%" in ut:
                        uf = "percent"
                    if not dim and uf:
                        dim = uf
                    spec = {
                        "name": mo.get("name") or mo.get("metric_name") or ck,
                        "unit_tag": ut,
                        "unit_family": uf,
                        "dimension": dim,
                    }
                ms_from_baseline[ck] = spec

            # only override if we got something non-trivial
            if ms_from_baseline:
                prev_response = dict(prev_response)
                prev_response["metric_schema_frozen"] = ms_from_baseline
    except Exception:
        pass
# Anchored part (authoritative when present)
    fn_anchor = globals().get("rebuild_metrics_from_snapshots_with_anchors_fix17")
    anchored = fn_anchor(prev_response, baseline_sources_cache, web_context=web_context) if callable(fn_anchor) else {}

    # Identify anchored keys (only those with a concrete anchor_hash)
    metric_anchors_any = globals().get("_get_metric_anchors_any")
    metric_anchors = metric_anchors_any(prev_response) if callable(metric_anchors_any) else (
        prev_response.get("metric_anchors") or {}
    )
    anchored_keys = set()
    try:
        for k, a in (metric_anchors or {}).items():
            if isinstance(a, dict) and (a.get("anchor_hash") or a.get("anchor")):
                anchored_keys.add(k)
    except Exception:
        pass
        anchored_keys = set()

    # Build a shallow prev_response copy where schema-only sees ONLY unanchored metrics
    pr2 = dict(prev_response)
    ms = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )
    if isinstance(ms, dict) and anchored_keys:
        ms2 = {k: v for k, v in ms.items() if k not in anchored_keys}
        pr2["metric_schema_frozen"] = ms2

    # Unanchored part
    fn_schema = globals().get("rebuild_metrics_from_snapshots_schema_only_fix17")
    unanchored = fn_schema(pr2, baseline_sources_cache, web_context=web_context) if callable(fn_schema) else {}

    # Deterministic merge: anchored first, then unanchored
    rebuilt = {}
    if isinstance(anchored, dict):
        rebuilt.update(anchored)
    if isinstance(unanchored, dict):
        for k, v in unanchored.items():
            if k not in rebuilt:
                rebuilt[k] = v
    return rebuilt


# Re-wire schema-only entrypoint to FIX18 (keep names identical for evolution dispatch)
def rebuild_metrics_from_snapshots_schema_only(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:  # noqa: F811
    return rebuild_metrics_from_snapshots_schema_only_fix18(prev_response, baseline_sources_cache, web_context=web_context)


# scrape+hash gate for evolution to prevent any rebuild/picking when unchanged.
#
# Goals:
#   1) Evolution ALWAYS performs a current scrape/fetch pass to compute a "current"
#      snapshot hash (v2 preferred).
#   2) If current hash == prior analysis hash (v2 preferred), evolution stops and
#      replays the prior FULL analysis payload rehydrated from Google Sheets,
#      publishing metrics to the dashboard WITHOUT any metric rebuild/selection.
#   3) If hashes differ, evolution proceeds via the existing deterministic path
#      (same rebuild/anchors logic as used elsewhere in this codebase).
#
# This patch is purely additive:
#   - REFACTOR67: legacy base evolution runner removed; changed-case recompute calls compute_source_anchored_diff directly
#   - Adds helper functions prefixed _fix24_*
#   - Overrides run_source_anchored_evolution by re-defining it below


def _fix24_get_prev_full_payload(previous_data: dict) -> dict:
    """
    Load the FULL prior analysis payload from Google Sheets if possible.
    Falls back to previous_data if already full.
    """
    try:
        if not isinstance(previous_data, dict):
            return {}
        # If it already looks like a full payload (contains canonical metrics), return as-is
        if isinstance(previous_data.get("primary_metrics_canonical"), dict) and previous_data["primary_metrics_canonical"]:
            try:
                previous_data = _fix2d73_promote_rehydrated_prevdata_v1(previous_data)
            except Exception:
                pass
            return previous_data

        # Preferred: explicit snapshot_store_ref / full_store_ref
        ref = previous_data.get("full_store_ref") or previous_data.get("snapshot_store_ref") or ""
        # Fallback: sheet id
        if (not ref) and isinstance(previous_data.get("_sheet_id"), str) and previous_data.get("_sheet_id"):
            # Assume HistoryFull
            ref = f"gsheet:HistoryFull:{previous_data.get('_sheet_id')}"

        if isinstance(ref, str) and ref.startswith("gsheet:"):
            parts = ref.split(":")
            ws_title = parts[1] if len(parts) > 1 and parts[1] else "HistoryFull"
            aid = parts[2] if len(parts) > 2 else ""
            if aid:
                fn = globals().get("load_full_history_payload_from_sheet")
                if callable(fn):
                    full = fn(aid, worksheet_title=ws_title)
                    if isinstance(full, dict) and full:
                        try:
                            full = _fix2d73_promote_rehydrated_prevdata_v1(full)
                        except Exception:
                            pass
                        return full
    except Exception:
        return previous_data if isinstance(previous_data, dict) else {}

    return previous_data if isinstance(previous_data, dict) else {}


def _fix24_extract_source_urls(prev_full: dict) -> list:
    """
    Determine the URL list to fetch for current-hash computation.
    Uses analysis 'sources' if available, else URLs from baseline_sources_cache.
    """
    urls = []
    try:
        if isinstance(prev_full, dict):
            s = prev_full.get("sources")
            if isinstance(s, list) and s:
                urls = [str(u) for u in s if isinstance(u, str) and u.strip()]
            if not urls:
                # Try results.source_results urls
                r = prev_full.get("results") if isinstance(prev_full.get("results"), dict) else {}
                sr = r.get("source_results") if isinstance(r, dict) else None
                if isinstance(sr, list):
                    for item in sr:
                        if isinstance(item, dict):
                            u = item.get("url") or item.get("source_url")
                            if u:
                                urls.append(str(u))
            if not urls:
                # Try baseline_sources_cache urls
                r = prev_full.get("results") if isinstance(prev_full.get("results"), dict) else {}
                bsc = None
                if isinstance(r, dict):
                    bsc = r.get("baseline_sources_cache")
                if not isinstance(bsc, list):
                    bsc = prev_full.get("baseline_sources_cache")
                if isinstance(bsc, list):
                    for item in bsc:
                        if isinstance(item, dict):
                            u = item.get("source_url") or item.get("url")
                            if u:
                                urls.append(str(u))
    except Exception:
        pass

    # Stable de-dupe order
    seen = set()
    out = []
    for u in urls:
        uu = (u or "").strip()
        if not uu or uu in seen:
            continue
        seen.add(uu)
        out.append(uu)
    return out[:25]


def _fix24_build_scraped_meta(urls: list, max_chars_per_source: int = 180000) -> dict:
    """
    Fetch each URL (deterministically) and return scraped_meta in the same shape
    attach_source_snapshots_to_analysis expects: {url: {"status":..., "text":..., "extracted_numbers":[...]}}
    """
    _fix2af_ledger = globals().get("_fix2af_last_scrape_ledger")
    try:
        _fix2af_norm_urls, _fix2af_norm_diag = _fix2af_normalize_url_items(urls)
        urls = _fix2af_norm_urls
        if isinstance(_fix2af_ledger, dict):
            _fix2af_ledger["__fix2af_url_normalize_diag__"] = _fix2af_norm_diag
    except Exception:
        pass

    scraped_meta = {}
    fetch_fn = globals().get("fetch_url_content_with_status") or globals().get("fetch_url_content")
    extract_fn = globals().get("extract_numbers_with_context")

    for u in urls or []:
        url = str(u or "").strip()
        try:
            _fix2af_ledger_put(_fix2af_ledger, url, stage="attempted", reason="entered_loop")
        except Exception:
            pass
        if not url:
            continue
        try:
            if callable(fetch_fn) and fetch_fn.__name__.endswith("_with_status"):
                try:
                    if str(url).lower().endswith(".pdf"):
                        text, status = fetch_fn(url, force_pdf=True)
                    else:
                        text, status = fetch_fn(url)
                except TypeError:
                    text, status = fetch_fn(url)
            elif callable(fetch_fn):
                text = fetch_fn(url)
                status = "success_direct" if (text and str(text).strip()) else "empty"
            else:
                text, status = (None, "no_fetch_fn")

            txt = _fix2af_scraped_text_accessor(text)
            if max_chars_per_source and len(txt) > int(max_chars_per_source):
                txt = txt[: int(max_chars_per_source)]

            try:
                _fix2af_fail_class = _fix2af_classify_fetch_failure(status, txt)
                _fix2af_ledger_put(_fix2af_ledger, url, stage="fetched", reason=_fix2af_fail_class, extra={"status": status, "text_len": len(txt or "")})
            except Exception:
                pass

            nums = []
            if callable(extract_fn) and txt.strip():
                try:
                    nums = extract_fn(txt, source_url=url)
                    if nums is None:
                        nums = []
                except Exception:
                    pass
                    nums = []

            try:
                _fix2af_ledger_put(_fix2af_ledger, url, stage="extracted", reason="ok" if (isinstance(nums, list) and len(nums)>0) else "no_numbers", extra={"numbers_count": (len(nums) if isinstance(nums, list) else -1)})
            except Exception:
                pass

            scraped_meta[url] = {
                "status": status,
                "text": txt,
                "extracted_numbers": nums if isinstance(nums, list) else [],
                "fix2af_fetch_diag": {
                    "url_norm": _fix2af_norm_url(url),
                    "status": status,
                    "text_len": len(txt or ""),
                    "failure_class": _fix2af_classify_fetch_failure(status, txt),
                    "numbers_count": (len(nums) if isinstance(nums, list) else -1),
                },
            }
        except Exception as e:
            try:
                _fix2af_ledger_put(_fix2af_ledger, url, stage="exception", reason=type(e).__name__, extra={"msg": str(e)[:300]})
            except Exception:
                pass
            scraped_meta[url] = {"status": f"exception:{type(e).__name__}", "text": "", "extracted_numbers": [], "fix2af_fetch_diag": {"url_norm": _fix2af_norm_url(url), "status": f"exception:{type(e).__name__}", "text_len": 0, "failure_class": type(e).__name__, "numbers_count": 0}}

    return scraped_meta


def _fix24_baseline_sources_cache_from_scraped_meta(scraped_meta: dict) -> list:
    """
    Use attach_source_snapshots_to_analysis (existing deterministic normalizer) to produce
    baseline_sources_cache from scraped_meta, ensuring value_norm/unit_tag fields are present.
    """
    try:
        fn = globals().get("attach_source_snapshots_to_analysis")
        if not callable(fn):
            return []
        dummy = {"results": {}}
        web_context = {"scraped_meta": scraped_meta or {}}
        fn(dummy, web_context)
        r = dummy.get("results") if isinstance(dummy.get("results"), dict) else {}
        bsc = r.get("baseline_sources_cache") if isinstance(r, dict) else None
        return bsc if isinstance(bsc, list) else []
    except Exception:
        return []


def _fix24_get_prev_hashes(prev_full: dict) -> dict:
    """
    Extract prior snapshot hashes (v2 preferred) from a full analysis payload.
    """
    out = {"v2": "", "v1": ""}
    try:
        if not isinstance(prev_full, dict):
            return out
        out["v2"] = str(prev_full.get("source_snapshot_hash_v2") or "")
        out["v1"] = str(prev_full.get("source_snapshot_hash") or "")
        r = prev_full.get("results") if isinstance(prev_full.get("results"), dict) else {}
        if isinstance(r, dict):
            out["v2"] = out["v2"] or str(r.get("source_snapshot_hash_v2") or "")
            out["v1"] = out["v1"] or str(r.get("source_snapshot_hash") or "")
    except Exception:
        return out

    return out


def _fix24_compute_current_hashes(baseline_sources_cache: list) -> dict:
    """
    Compute current snapshot hashes (v2 preferred).
    """
    out = {"v2": "", "v1": ""}
    try:
        fn1 = globals().get("compute_source_snapshot_hash")
        fn2 = globals().get("compute_source_snapshot_hash_v2")
        if callable(fn2):
            out["v2"] = str(fn2(baseline_sources_cache) or "")
        if callable(fn1):
            out["v1"] = str(fn1(baseline_sources_cache) or "")
    except Exception:
        return out

    return out


def _fix24_make_replay_output(prev_full: dict, hashes: dict) -> dict:
    """
    Build a minimal evolution payload for the dashboard that reflects the prior analysis
    payload verbatim (no rebuild). This avoids the diff panel showing years by ensuring
    the "evolution column" is sourced from stored canonical metrics.
    """
    pmc = prev_full.get("primary_metrics_canonical") if isinstance(prev_full, dict) else {}
    pmc = pmc if isinstance(pmc, dict) else {}

    # Build a deterministic "no-change" metric_changes list WITHOUT re-selecting metrics.
    metric_changes = []
    try:
        for ckey in sorted(pmc.keys()):
            m = pmc.get(ckey) if isinstance(pmc.get(ckey), dict) else {}
            name = str(m.get("name") or m.get("metric_name") or ckey)
            v = m.get("value_norm", m.get("value"))
            unit = m.get("base_unit") or m.get("unit_tag") or m.get("unit") or ""
            metric_changes.append({
                "canonical_key": ckey,
                "name": name,
                "previous_value": v,
                "current_value": v,
                "previous_unit": unit,
                "current_unit": unit,
                "change_type": "unchanged",
                "confidence": 1.0,
            })
    except Exception:
        pass
        metric_changes = []

    return {
        "status": "ok",
        "mode": "replay_unchanged_fix24",
        "message": "Sources + data unchanged (hash match). Replaying prior analysis snapshot from Sheets.",
        "sources_checked": int(len(prev_full.get("sources") or [])) if isinstance(prev_full, dict) else 0,
        "sources_fetched": 0,
        "sources_failed": 0,
        "sources_skipped": 0,
        "source_results": [],
        "metric_changes": metric_changes,
        "change_stats": {
            "metrics_increased": 0,
            "metrics_decreased": 0,
            "metrics_unchanged": len(metric_changes),
            "metrics_total": len(metric_changes),
        },
        "debug": {
            "fix24": True,
            "prev_source_snapshot_hash_v2": hashes.get("prev_v2",""),
            "cur_source_snapshot_hash_v2": hashes.get("cur_v2",""),
            "prev_source_snapshot_hash": hashes.get("prev_v1",""),
            "cur_source_snapshot_hash": hashes.get("cur_v1",""),
            "hash_equal_v2": bool(hashes.get("prev_v2") and hashes.get("cur_v2") and hashes.get("prev_v2")==hashes.get("cur_v2")),
            "hash_equal_v1": bool(hashes.get("prev_v1") and hashes.get("cur_v1") and hashes.get("prev_v1")==hashes.get("cur_v1")),
        },
        # Provide the replay payload so the dashboard can render canonical metrics directly if desired
        "replay_analysis_payload": prev_full,
    }


# REFACTOR88 (HOTFIX): Restore/guard FIX2D55 prev-lift helper used by FIX24 recompute
# Why:
# - REFACTOR87 pruned a legacy ladder block which previously defined _fix2d55_apply_prev_lift.
# - The FIX24 changed-case evolution recompute path calls this helper before compute_source_anchored_diff.
# - Without it, Evolution shows: "FIX24: Evolution recompute failed (compute_source_anchored_diff path)."
# What:
# - Provide a small, deterministic "lift" that copies canonical maps/schema/anchors from any nested
# - Idempotent and safe: if nothing is found, it becomes a no-op.

def _fix2d55_apply_prev_lift(prev_full: dict, web_context: dict = None) -> None:
    try:
        if not isinstance(prev_full, dict):
            return

        # Find a primary_response container if present
        pr = None
        try:
            pr = prev_full.get("primary_response") if isinstance(prev_full.get("primary_response"), dict) else None
            if pr is None and isinstance(prev_full.get("results"), dict):
                pr = prev_full["results"].get("primary_response") if isinstance(prev_full["results"].get("primary_response"), dict) else None
        except Exception:
            pr = None

        # Locate canonical metrics map from common storage shapes
        pmc = None
        try:
            for cand in [
                prev_full.get("primary_metrics_canonical"),
                (prev_full.get("results") or {}).get("primary_metrics_canonical") if isinstance(prev_full.get("results"), dict) else None,
                (pr or {}).get("primary_metrics_canonical") if isinstance(pr, dict) else None,
                ((pr or {}).get("results") or {}).get("primary_metrics_canonical") if isinstance(pr, dict) and isinstance(pr.get("results"), dict) else None,
            ]:
                if isinstance(cand, dict) and cand:
                    pmc = cand
                    break
        except Exception:
            pmc = None

        # Locate schema + anchors similarly
        schema = None
        anchors = None
        try:
            for cand in [
                prev_full.get("metric_schema_frozen"),
                (prev_full.get("results") or {}).get("metric_schema_frozen") if isinstance(prev_full.get("results"), dict) else None,
                (pr or {}).get("metric_schema_frozen") if isinstance(pr, dict) else None,
                ((pr or {}).get("results") or {}).get("metric_schema_frozen") if isinstance(pr, dict) and isinstance(pr.get("results"), dict) else None,
            ]:
                if isinstance(cand, dict) and cand:
                    schema = cand
                    break
        except Exception:
            schema = None

        try:
            for cand in [
                prev_full.get("metric_anchors"),
                (prev_full.get("results") or {}).get("metric_anchors") if isinstance(prev_full.get("results"), dict) else None,
                (pr or {}).get("metric_anchors") if isinstance(pr, dict) else None,
                ((pr or {}).get("results") or {}).get("metric_anchors") if isinstance(pr, dict) and isinstance(pr.get("results"), dict) else None,
            ]:
                if isinstance(cand, dict) and cand:
                    anchors = cand
                    break
        except Exception:
            anchors = None

        # Apply lifts (additive only; do not overwrite non-empty)
        try:
            if isinstance(pmc, dict) and pmc:
                if not isinstance(prev_full.get("primary_metrics_canonical"), dict) or not prev_full.get("primary_metrics_canonical"):
                    prev_full["primary_metrics_canonical"] = pmc
                if isinstance(pr, dict):
                    if not isinstance(pr.get("primary_metrics_canonical"), dict) or not pr.get("primary_metrics_canonical"):
                        pr["primary_metrics_canonical"] = pmc
        except Exception:
            pass

        try:
            if isinstance(schema, dict) and schema:
                if not isinstance(prev_full.get("metric_schema_frozen"), dict) or not prev_full.get("metric_schema_frozen"):
                    prev_full["metric_schema_frozen"] = schema
                if isinstance(pr, dict):
                    if not isinstance(pr.get("metric_schema_frozen"), dict) or not pr.get("metric_schema_frozen"):
                        pr["metric_schema_frozen"] = schema
        except Exception:
            pass

        try:
            if isinstance(anchors, dict) and anchors:
                if not isinstance(prev_full.get("metric_anchors"), dict) or not prev_full.get("metric_anchors"):
                    prev_full["metric_anchors"] = anchors
                if isinstance(pr, dict):
                    if not isinstance(pr.get("metric_anchors"), dict) or not pr.get("metric_anchors"):
                        pr["metric_anchors"] = anchors
        except Exception:
            pass

        # Ensure primary_response container is present if we found one
        try:
            if isinstance(pr, dict):
                prev_full.setdefault("primary_response", pr)
        except Exception:
            pass

        # Debug stamp (safe)
        try:
            prev_full.setdefault("debug", {})
            if isinstance(prev_full.get("debug"), dict):
                prev_full["debug"].setdefault("fix2d55_prev_lift_v1", {})
                if isinstance(prev_full["debug"].get("fix2d55_prev_lift_v1"), dict):
                    prev_full["debug"]["fix2d55_prev_lift_v1"].update({
                        "lifted_pmc": bool(isinstance(pmc, dict) and pmc),
                        "lifted_schema": bool(isinstance(schema, dict) and schema),
                        "lifted_anchors": bool(isinstance(anchors, dict) and anchors),
                    })
        except Exception:
            pass
    except Exception:
        # Never block evolution recompute due to lift helper
        return


def run_source_anchored_evolution(previous_data: dict, web_context: dict = None) -> dict:
    """
    PATCH FIX24 (ADDITIVE): Evolution flow is:
      1) Rehydrate prior full analysis payload from Sheets (HistoryFull)
      2) Scrape/fetch current sources to build scraped_meta + baseline_sources_cache_current
      3) Compute current snapshot hash (v2 preferred)
      4) If hash matches prior analysis: STOP and replay from Sheets (no rebuild/selection)
      5) If changed: proceed with the existing deterministic evolution path, but ensure
         it routes through the same snapshot/anchor deterministic plumbing used elsewhere.

    Note: This does NOT refactor existing evolution code; it wraps it.
    """
    # Step 1: Rehydrate prior payload
    prev_full = _fix24_get_prev_full_payload(previous_data or {})
    prev_hashes = _fix24_get_prev_hashes(prev_full)

    # Step 2: Build current scraped_meta by fetching the same URLs used previously
    urls = _fix24_extract_source_urls(prev_full)
    #
    # Purpose:
    # - Streamlit may provide injected URLs only via diagnostic fields
    #   (diag_extra_urls_ui / diag_extra_urls_ui_raw).
    # - Downstream evolution admission & fetch logic keys off web_context['extra_urls'].
    #
    # Behavior:
    # - If web_context['extra_urls'] is empty/missing, recover from (in order):
    #     1) web_context['diag_extra_urls_ui']     (list)
    #     2) web_context['diag_extra_urls_ui_raw'] (str; newline/comma separated)
    # - Normalize/canonicalize via _inj_diag_norm_url_list (tracking params stripped).
    #
    # Safety:
    # - Purely additive wiring; no effect when no injection is present.
    # - Never raises; falls back silently.
    try:
        if isinstance(web_context, dict):
            _wc_extra0 = web_context.get('extra_urls')
            _needs = (not isinstance(_wc_extra0, (list, tuple)) or not _wc_extra0)
            if _needs:
                _recovered = []
                # FIX2D66: also recover from extra_urls_ui_raw and question text
                try:
                    _qtxt = str((prev_full or {}).get('question') or (previous_data or {}).get('question') or '')
                    _more = _fix2d66_collect_injected_urls(web_context or {}, question_text=_qtxt)
                    if _more:
                        _recovered.extend(_more)
                except Exception:
                    pass
                _v_list = web_context.get('diag_extra_urls_ui')
                if isinstance(_v_list, (list, tuple)) and _v_list:
                    _recovered = list(_v_list)
                if not _recovered:
                    _raw = web_context.get('diag_extra_urls_ui_raw') or web_context.get('extra_urls_ui_raw')
                    if isinstance(_raw, str) and _raw.strip():
                        _parts = []
                        for _line in _raw.splitlines():
                            _line = (_line or '').strip()
                            if not _line:
                                continue
                            for _p in _line.split(','):
                                _p = (_p or '').strip()
                                if _p:
                                    _parts.append(_p)
                        if _parts:
                            _recovered = _parts
                if _recovered:
                    _recovered_norm = _inj_diag_norm_url_list(_recovered)
                    if _recovered_norm:
                        web_context['extra_urls'] = list(_recovered_norm)
                        # Also consider URLs embedded in the question text (last resort)

                        web_context.setdefault('debug', {})
                        if isinstance(web_context.get('debug'), dict):
                            web_context['debug'].setdefault('fix41afc3', {})
                            if isinstance(web_context['debug'].get('fix41afc3'), dict):
                                web_context['debug']['fix41afc3'].update({
                                    'extra_urls_recovered': True,
                                    'extra_urls_recovered_count': int(len(_recovered_norm)),
                                })
    except Exception:
        pass


    #
    # Goal:
    # - When Evolution UI provides injected URLs, route them through the SAME
    #   admission/normalization/dedupe logic used by analysis (fetch_web_context),
    #   but in IDENTITY-ONLY mode (no scraping).
    #
    # Why:
    # - Previously, injected URLs could appear in ui_norm/intake_norm but never
    #   reach the admission gate, yielding empty admission_decisions.
    #
    # Behavior:
    # - Only active when web_context['extra_urls'] is non-empty.
    # - Does NOT change fastpath logic directly; it only defines the "current URL
    #   universe" inputs (urls) used for hashing/scrape_meta building.
    #
    # Safety:
    # - identity_only=True prevents any network scrape inside fetch_web_context.
    # - Purely additive; if anything fails, it falls back to existing urls list.
    try:
        _evo_extra_urls_raw = (web_context or {}).get("extra_urls") or []
        _evo_extra_urls_norm = _inj_diag_norm_url_list(_evo_extra_urls_raw)
        if _evo_extra_urls_norm:
            _baseline_urls_for_fwc = _fix24_extract_source_urls(prev_full) or []
            _baseline_urls_for_fwc_norm = _inj_diag_norm_url_list(_baseline_urls_for_fwc)
            _q_for_fwc = str((prev_full or {}).get("question") or (previous_data or {}).get("question") or "").strip()
            _fwc = fetch_web_context(
                _q_for_fwc or "evolution_identity_only",
                num_sources=int(min(12, max(1, len(_baseline_urls_for_fwc_norm) + len(_evo_extra_urls_norm)))),
                fallback_mode=True,
                fallback_urls=_baseline_urls_for_fwc_norm,
                existing_snapshots=(prev_full or {}).get("baseline_sources_cache") or (prev_full or {}).get("baseline_sources_cache_v2") or None,
                extra_urls=_evo_extra_urls_norm,
                diag_run_id=str((web_context or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                diag_extra_urls_ui_raw=(web_context or {}).get("diag_extra_urls_ui_raw"),
                identity_only=True,
            ) or {}
            _fwc_admitted = _fwc.get("web_sources") or _fwc.get("sources") or []
            if isinstance(_fwc_admitted, list) and _fwc_admitted:
                urls = list(_fwc_admitted)
            # Attach the admission decisions to web_context for unified inj_trace reporting
            if isinstance(web_context, dict):
                if isinstance(_fwc.get("diag_injected_urls"), dict):
                    web_context.setdefault("diag_injected_urls", {})
                    if isinstance(web_context.get("diag_injected_urls"), dict):
                        # do not clobber if already present
                        for _k, _v in _fwc.get("diag_injected_urls").items():
                            web_context["diag_injected_urls"].setdefault(_k, _v)
                web_context.setdefault("debug", {})
                if isinstance(web_context.get("debug"), dict):
                    web_context["debug"].setdefault("evo_fwc_identity_only", {})
                    if isinstance(web_context["debug"].get("evo_fwc_identity_only"), dict):
                        web_context["debug"]["evo_fwc_identity_only"].update({
                            "called": True,
                            "baseline_urls_count": int(len(_baseline_urls_for_fwc_norm)),
                            "extra_urls_count": int(len(_evo_extra_urls_norm)),
                            "admitted_count": int(len(urls or [])),
                            "admitted_set_hash": _inj_diag_set_hash(_inj_diag_norm_url_list(urls or [])),
                        })
    except Exception:
        pass


    #
    # Problem observed (inj_trace_v1):
    # - Injected URL shows up in ui_norm/intake_norm, but can still vanish from admitted_norm/hash_inputs_norm.
    # - Root cause: the identity-only fetch_web_context() step may replace `urls` with an admitted list
    #   that excludes injected URLs, and later injected logic may read from a different variable/path.
    #
    # Goal:
    # - If injected URLs are present in web_context['extra_urls'], ensure they are ALWAYS merged into the
    #   local `urls` list used by _fix24_build_scraped_meta(), so the injected URLs are at least
    #   attempted (scraped_meta populated) and can become part of current hash identity when successful.
    #
    # Safety:
    # - Only active when injection is present.
    # - Purely additive: does not change hashing algorithm or fastpath rules; it only ensures the URL
    #   universe includes the injected URLs when the user provided them.
    # - Never raises.
    try:
        _fx9_wc = web_context if isinstance(web_context, dict) else {}
        _fx9_inj = _inj_diag_norm_url_list((_fx9_wc or {}).get("extra_urls") or [])
        if _fx9_inj and isinstance(urls, list):
            _fx9_seen = set(_inj_diag_norm_url_list([(_d.get("url") if isinstance(_d, dict) else _d) for _d in urls]))
            _fx9_added = []
            for _u in _fx9_inj:
                if _u in _fx9_seen:
                    continue
                _fx9_seen.add(_u)
                urls.append(_u)
                _fx9_added.append(_u)

            if isinstance(_fx9_wc, dict):
                _fx9_wc.setdefault("debug", {})
                if isinstance(_fx9_wc.get("debug"), dict):
                    _fx9_wc["debug"].setdefault("fix41afc9", {})
                    if isinstance(_fx9_wc["debug"].get("fix41afc9"), dict):
                        _fx9_wc["debug"]["fix41afc9"].update({
                            "merged_into_urls_universe": True,
                            "injected_urls_count": int(len(_fx9_inj)),
                            "added_to_urls_count": int(len(_fx9_added)),
                            "added_to_urls": list(_fx9_added),
                            "urls_count_after_merge": int(len(urls)),
                        })
    except Exception:
        pass

    #
    # Observation (from inj_trace_v1 in evolution JSON):
    # - Injected URL appears in ui_norm/intake_norm, but attempted/persisted remain empty,
    #   and the injected URL never reaches hash_inputs because evolution never performs a
    #   fetch cycle for the injected delta (it only replays cached snapshots).
    #
    # Goal:
    # - ONLY when injected URLs introduce a true delta vs the baseline source universe,
    #   run fetch_web_context() in normal mode (identity_only=False) so the injected URL
    #   is actually attempted/persisted and can become a first-class current source
    #   (and thus can affect downstream identity/hash inputs).
    #
    # Safety:
    # - No effect on no-injection runs.
    # - No effect when injection is empty or introduces no delta.
    # - Uses existing_snapshots to avoid re-fetching baseline sources.
    # - Never raises; falls back to existing behavior.
    try:
        _fix41afc6_wc = web_context if isinstance(web_context, dict) else {}
        _fix41afc6_inj = _inj_diag_norm_url_list((_fix41afc6_wc or {}).get("extra_urls") or [])
        _fix41afc6_base = _inj_diag_norm_url_list(_fix24_extract_source_urls(prev_full) or [])
        _fix41afc6_delta = sorted(list(set(_fix41afc6_inj) - set(_fix41afc6_base))) if _fix41afc6_inj else []
        if _fix41afc6_inj:
            _fix41afc6_q = str((prev_full or {}).get("question") or (previous_data or {}).get("question") or "").strip()
            _fix41afc6_prev_snap = (prev_full or {}).get("baseline_sources_cache") or (prev_full or {}).get("baseline_sources_cache_v2") or None

            _fix41afc6_fwc = fetch_web_context(
                _fix41afc6_q or "evolution_injection_fetch",
                num_sources=int(min(12, max(1, len(_fix41afc6_base) + len(_fix41afc6_inj)))),
                fallback_mode=True,
                fallback_urls=_fix41afc6_base,
                existing_snapshots=_fix41afc6_prev_snap,
                extra_urls=_fix41afc6_inj,
                diag_run_id=str((_fix41afc6_wc or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                diag_extra_urls_ui_raw=(_fix41afc6_wc or {}).get("diag_extra_urls_ui_raw"),
                force_scrape_extra_urls=True,
                force_admit_extra_urls=True,
                identity_only=False,
            ) or {}

            # Prefer the admitted list from fetch_web_context (it includes injected URLs that pass admission)
            _fix41afc6_admitted = _fix41afc6_fwc.get("web_sources") or _fix41afc6_fwc.get("sources") or []
            if isinstance(_fix41afc6_admitted, list) and _fix41afc6_admitted:
                urls = list(_fix41afc6_admitted)

            # Bubble up a small marker so inj_trace_v1 can report whether evolution actually called FWC
            if isinstance(web_context, dict):
                web_context["evolution_calls_fetch_web_context"] = True
                try:
                    # FIX2D56: record that injection fetch is allowed/enabled
                    web_context.setdefault("debug", {})
                    if isinstance(web_context.get("debug"), dict):
                        web_context["debug"].setdefault("fix2d56", {})
                        if isinstance(web_context["debug"].get("fix2d56"), dict):
                            web_context["debug"]["fix2d56"].update({
                                "force_fetch_injection": True,
                                "injected_urls_count": int(len(_fix41afc6_inj or [])),
                            })
                except Exception:
                    pass
                web_context.setdefault("debug", {})
                if isinstance(web_context.get("debug"), dict):
                    web_context["debug"].setdefault("fix41afc6", {})
                    if isinstance(web_context["debug"].get("fix41afc6"), dict):
                        web_context["debug"]["fix41afc6"].update({
                            "called_fetch_web_context": True,
                            "injected_delta_count": int(len(_fix41afc6_delta)),
                            "injected_delta": list(_fix41afc6_delta),
                            "admitted_count": int(len(_fix41afc6_admitted or [])) if isinstance(_fix41afc6_admitted, list) else 0,
                        })

                    try:
                        web_context.setdefault("debug", {})
                        if isinstance(web_context.get("debug"), dict):
                            web_context["debug"].setdefault("fix41afc8", {})
                            if isinstance(web_context["debug"].get("fix41afc8"), dict):
                                # delta URLs are what we intend to force-attempt
                                _fx8_delta_urls = list(_fix41afc6_delta or [])
                                # attempted/persist outcomes can be inferred from fetch_web_context scraped_meta
                                _fx8_results = {}
                                try:
                                    _fx8_sm = _fix41afc6_fwc.get("scraped_meta") or {}
                                    if isinstance(_fx8_sm, dict):
                                        for _u in _fx8_delta_urls:
                                            meta = _fx8_sm.get(_u) or {}
                                            if isinstance(meta, dict) and meta:
                                                _fx8_results[_u] = meta.get("status") or meta.get("fetch_status") or meta.get("reason") or "attempted"
                                            else:
                                                _fx8_results[_u] = "not_in_scraped_meta"
                                except Exception:
                                    pass
                                web_context["debug"]["fix41afc8"].update({
                                    "forced_fetch_reason": "injected_delta_present_force_fetch_even_if_not_admitted",
                                    "forced_fetch_urls": _fx8_delta_urls,
                                    "forced_fetch_count": int(len(_fx8_delta_urls)),
                                    "forced_fetch_results": _fx8_results,
                                })
                    except Exception:
                        pass

    except Exception:
        pass

    #
    # Problem observed in evolution JSON:
    # - ui_norm/intake_norm contains the injected URL (from Streamlit textarea),
    #   but web_context["extra_urls"] can still be empty at evolution core, which
    #   causes downstream "fetch injected delta" and "forced admit" patches to see
    #   an empty injected set and skip.
    #
    # Goal:
    # - If web_context["extra_urls"] is empty, recover injected URLs from the same
    #   Streamlit diagnostic fields used at intake:
    #     1) web_context["diag_extra_urls_ui"] (list)
    #     2) web_context["diag_extra_urls_ui_raw"] (string, newline/comma separated)
    # - Normalize/canonicalize deterministically via _inj_diag_norm_url_list().
    # - Latch the recovered list back into web_context["extra_urls"] so ALL later
    #   injected URL logic (fetch + forced admit + hash identity) sees the same set.
    #
    # Safety:
    # - Purely additive. No effect when extra_urls already present.
    # - Never raises; falls back silently.
    try:
        if isinstance(web_context, dict):
            _fix41afc7_norm = []
            _existing = web_context.get("extra_urls")
            _need = (not isinstance(_existing, (list, tuple)) or not list(_existing))
            if _need:
                _raw = []
                _v_list = web_context.get("diag_extra_urls_ui")
                if isinstance(_v_list, (list, tuple)) and _v_list:
                    _raw = list(_v_list)
                if not _raw:
                    _v_raw = web_context.get("diag_extra_urls_ui_raw")
                    if isinstance(_v_raw, str) and _v_raw.strip():
                        _parts = []
                        for _line in _v_raw.splitlines():
                            _line = (_line or "").strip()
                            if not _line:
                                continue
                            for _p in _line.split(","):
                                _p = (_p or "").strip()
                                if _p:
                                    _parts.append(_p)
                        if _parts:
                            _raw = _parts

                _fix41afc7_norm = _inj_diag_norm_url_list(_raw)
                if _fix41afc7_norm:
                    web_context["extra_urls"] = list(_fix41afc7_norm)

            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].setdefault("fix41afc7", {})
                if isinstance(web_context["debug"].get("fix41afc7"), dict):
                    web_context["debug"]["fix41afc7"].update({
                        "recovery_needed": bool(_need),
                        "recovered_extra_urls_count": int(len(_fix41afc7_norm or [])),
                        "recovered_extra_urls": list(_fix41afc7_norm or [])[:20],
                        "extra_urls_present_after_recovery": bool(isinstance(web_context.get("extra_urls"), (list, tuple)) and list(web_context.get("extra_urls") or [])),
                    })
    except Exception:
        pass

    # - Only active if caller provides web_context['extra_urls']
    # - Does NOT affect default fastpath behavior.
    _inj_diag_run_id = ""
    _inj_extra_urls = []
    try:
        _inj_diag_run_id = str((web_context or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo")
        _inj_extra_urls = _inj_diag_norm_url_list((web_context or {}).get("extra_urls") or [])
    except Exception:
        pass
        _inj_diag_run_id = _inj_diag_make_run_id("evo")
        _inj_extra_urls = []

    try:
        if _inj_extra_urls:
            _u_seen = set([str(u or "").strip() for u in (urls or []) if str(u or "").strip()])
            for _u in _inj_extra_urls:
                if _u not in _u_seen:
                    _u_seen.add(_u)
                    urls.append(_u)
    except Exception:
        pass


    #
    # Problem (observed in evolution JSON):
    # - Injected URL appears in ui_norm/intake_norm but is missing from admitted_norm,
    #   so it never reaches attempted/persisted/hash_inputs.
    # - This typically happens when admission/allowlist logic rejects injected URLs
    #   before the fetch loop, leaving attempted empty.
    #
    # Goal:
    # - ONLY when injection is present AND it introduces a true delta vs the baseline
    #   source universe, ensure the injected URLs are included in `urls` (the universe
    #   FIX24 uses for scrape_meta building).
    #
    # Safety:
    # - Purely additive; no effect when no injection or no delta.
    # - Does not modify fastpath logic/hashing; it only ensures injected URLs are
    #   present in the post-intake universe when delta exists.
    # - Never raises; falls back silently.
    try:
        _fix41afc4_inj_norm = _inj_diag_norm_url_list(_inj_extra_urls or [])
        _fix41afc4_base_norm = _inj_diag_norm_url_list(_fix24_extract_source_urls(prev_full) or [])
        _fix41afc4_delta = sorted(list(set(_fix41afc4_inj_norm) - set(_fix41afc4_base_norm))) if _fix41afc4_inj_norm else []
        _fix41afc4_applied = False

        if _fix41afc4_delta:
            # Determine expected URL container shape (strings vs dicts)
            _urls_list = urls if isinstance(urls, list) else []
            _urls_are_dicts = bool(_urls_list) and isinstance(_urls_list[0], dict)

            # Build a normalized "seen" set from existing urls
            if _urls_are_dicts:
                _seen_norm = set(_inj_diag_norm_url_list([(_d.get("url") if isinstance(_d, dict) else "") for _d in _urls_list]))
            else:
                _seen_norm = set(_inj_diag_norm_url_list(_urls_list))

            for _u in _fix41afc4_delta:
                if _u in _seen_norm:
                    continue
                _seen_norm.add(_u)
                if _urls_are_dicts:
                    _urls_list.append({"url": _u})
                else:
                    _urls_list.append(_u)
                _fix41afc4_applied = True

            urls = _urls_list  # rebind defensively

        if isinstance(web_context, dict):
            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].setdefault("fix41afc4", {})
                if isinstance(web_context["debug"].get("fix41afc4"), dict):
                    web_context["debug"]["fix41afc4"].update({
                        "forced_admit_applied": bool(_fix41afc4_applied),
                        "forced_admit_injected_urls_count": int(len(_fix41afc4_delta)),
                        "forced_admit_injected_urls": list(_fix41afc4_delta),
                        "baseline_urls_count": int(len(_fix41afc4_base_norm)),
                        "urls_after_forced_admit_count": int(len(_inj_diag_norm_url_list([(_d.get("url") if isinstance(_d, dict) else _d) for _d in (urls or [])] if isinstance(urls, list) else []))),
                    })
    except Exception:
        pass


    #
    # Why:
    # - When a URL appears in ui_norm/intake_norm but not in admitted_norm, we need
    #   an explicit reason before we change any behavior.
    #
    # What this records (debug only):
    # - Whether evolution is using fetch_web_context (it is not in FIX24 path)
    # - The pre- and post-injection URL universe
    # - Per-injected-URL admission decision + reason codes
    #
    # Safety:
    # - Does NOT alter control flow, fastpath eligibility, scraping, hashing, or selection.
    try:
        _urls_prev_full = _fix24_extract_source_urls(prev_full)
        _urls_prev_full_norm = _inj_diag_norm_url_list(_urls_prev_full or [])
        _urls_after_merge_norm = _inj_diag_norm_url_list(urls or [])

        _admission_decisions = {}
        for _u in (_inj_extra_urls or []):
            if _u in set(_urls_after_merge_norm):
                _admission_decisions[_u] = {
                    "decision": "admitted",
                    "reason_code": "merged_into_urls_for_scrape",
                }
            else:
                _admission_decisions[_u] = {
                    "decision": "rejected",
                    "reason_code": "not_present_in_urls_after_merge",
                }

        if isinstance(web_context, dict):
            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].setdefault("evo_injection_trace", {})
                if isinstance(web_context["debug"].get("evo_injection_trace"), dict):
                    web_context["debug"]["evo_injection_trace"].update({
                        "uses_fetch_web_context": False,
                        "urls_prev_full_count": int(len(_urls_prev_full_norm)),
                        "urls_prev_full_set_hash": _inj_diag_set_hash(_urls_prev_full_norm),
                        "urls_after_merge_count": int(len(_urls_after_merge_norm)),
                        "urls_after_merge_set_hash": _inj_diag_set_hash(_urls_after_merge_norm),
                        "inj_extra_urls_norm": list(_inj_extra_urls or []),
                        "inj_merge_applied": bool(_inj_extra_urls),
                        "inj_admission_decisions": _admission_decisions,
                    })

            # Also attach to diag_injected_urls for unified downstream reporting
            web_context.setdefault("diag_injected_urls", {})
            if isinstance(web_context.get("diag_injected_urls"), dict):
                web_context["diag_injected_urls"].setdefault("admission_decisions", {})
                if isinstance(web_context["diag_injected_urls"].get("admission_decisions"), dict):
                    web_context["diag_injected_urls"]["admission_decisions"].update(_admission_decisions)
                web_context["diag_injected_urls"].setdefault("urls_prev_full_norm", _urls_prev_full_norm)
                web_context["diag_injected_urls"].setdefault("urls_after_merge_norm", _urls_after_merge_norm)
    except Exception:
        pass

    #
    # Problem:
    # - Injected URLs can appear in UI intake but get dropped pre-admission, resulting in:
    #     attempted=0, persisted_norm=0, hash_inputs_norm unchanged.
    #
    # Goal:
    # - When injected URL DELTA exists (vs current urls baseline), deterministically:
    #     1) Force-admit injected delta into local `urls` universe (so downstream meta sees it)
    #     2) Force-fetch injected delta via fetch_web_context(force_scrape_extra_urls=True),
    #        so we get attempted/persisted entries or an explicit failure reason.
    #
    # Safety:
    # - No effect when no injection / no delta.
    # - Does not weaken normal fastpath logic (already bypassed upstream when delta exists).
    try:
        _fix41afc11_wc = web_context if isinstance(web_context, dict) else {}
        # Robust recovery (order required)
        _fix41afc11_extra = []
        if isinstance(_fix41afc11_wc.get("extra_urls"), (list, tuple)) and _fix41afc11_wc.get("extra_urls"):
            _fix41afc11_extra = list(_fix41afc11_wc.get("extra_urls") or [])
        elif isinstance(_fix41afc11_wc.get("diag_extra_urls_ui"), (list, tuple)) and _fix41afc11_wc.get("diag_extra_urls_ui"):
            _fix41afc11_extra = list(_fix41afc11_wc.get("diag_extra_urls_ui") or [])
        elif isinstance(_fix41afc11_wc.get("diag_extra_urls_ui_raw"), str) and (_fix41afc11_wc.get("diag_extra_urls_ui_raw") or "").strip():
            _raw = str(_fix41afc11_wc.get("diag_extra_urls_ui_raw") or "")
            _parts = []
            for _line in _raw.splitlines():
                _line = (_line or "").strip()
                if not _line:
                    continue
                for _p in _line.split(","):
                    _p = (_p or "").strip()
                    if _p:
                        _parts.append(_p)
            _fix41afc11_extra = _parts

        _fix41afc11_inj_norm = _inj_diag_norm_url_list(_fix41afc11_extra) if _fix41afc11_extra else []
        _fix41afc11_urls_norm = _inj_diag_norm_url_list(urls) if urls else []
        _fix41afc11_inj_set = set(_fix41afc11_inj_norm or [])
        _fix41afc11_base_set = set(_fix41afc11_urls_norm or [])
        _fix41afc11_delta = sorted(list(_fix41afc11_inj_set - _fix41afc11_base_set)) if _fix41afc11_inj_set else []

        if _fix41afc11_delta:
            # (1) Force-admit delta into local urls universe deterministically
            _added = []
            if urls and isinstance(urls[0], dict):
                _seen = set(_fix41afc11_urls_norm or [])
                for _u in _fix41afc11_delta:
                    if _u in _seen:
                        continue
                    urls.append({"url": _u, "source": "injected_force_admit", "is_injected": True})
                    _seen.add(_u)
                    _added.append(_u)
            else:
                _seen = set(_fix41afc11_urls_norm or [])
                for _u in _fix41afc11_delta:
                    if _u in _seen:
                        continue
                    urls.append(_u)
                    _seen.add(_u)
                    _added.append(_u)

            # (2) Must-fetch lane: force scrape extra urls even if not admitted by normal filter
            _q = str((prev_full or {}).get("question") or (previous_data or {}).get("question") or "evolution_injection_force_fetch").strip()
            _prev_snap = (prev_full or {}).get("baseline_sources_cache") or (prev_full or {}).get("baseline_sources_cache_v2") or None
            try:
                _fwc = fetch_web_context(
                    _q or "evolution_injection_force_fetch",
                    num_sources=int(min(12, max(1, len(_fix41afc11_urls_norm or []) + len(_fix41afc11_inj_norm or [])))),
                    fallback_mode=True,
                    fallback_urls=list(_fix41afc11_urls_norm or []),
                    existing_snapshots=_prev_snap,
                    extra_urls=list(_fix41afc11_inj_norm or []),
                    diag_run_id=str((_fix41afc11_wc or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                    diag_extra_urls_ui_raw=(_fix41afc11_wc or {}).get("diag_extra_urls_ui_raw"),
                    identity_only=False,
                    force_scrape_extra_urls=True,
                force_admit_extra_urls=True,
                ) or {}
            except TypeError:
                # Backward-compat: older fetch_web_context without force_scrape_extra_urls
                _fwc = fetch_web_context(
                    _q or "evolution_injection_force_fetch",
                    num_sources=int(min(12, max(1, len(_fix41afc11_urls_norm or []) + len(_fix41afc11_inj_norm or [])))),
                    fallback_mode=True,
                    fallback_urls=list(_fix41afc11_urls_norm or []),
                    existing_snapshots=_prev_snap,
                    extra_urls=list(_fix41afc11_inj_norm or []),
                    diag_run_id=str((_fix41afc11_wc or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                    diag_extra_urls_ui_raw=(_fix41afc11_wc or {}).get("diag_extra_urls_ui_raw"),
                    identity_only=False,
                ) or {}

            # If fetch_web_context returns a concrete web_sources list, prefer it for downstream scraped_meta
            _fwc_sources = _fwc.get("web_sources") or _fwc.get("sources") or None
            if isinstance(_fwc_sources, list) and _fwc_sources:
                urls = list(_fwc_sources)

            # Emit explicit debug fields
            if isinstance(web_context, dict):
                web_context.setdefault("debug", {})
                if isinstance(web_context.get("debug"), dict):
                    web_context["debug"].setdefault("fix41afc11", {})
                    if isinstance(web_context["debug"].get("fix41afc11"), dict):
                        web_context["debug"]["fix41afc11"].update({
                            "inj_force_admit_applied": True,
                            "inj_force_admit_count": int(len(_added)),
                            "inj_force_admit_urls": list(_added),
                            "inj_delta_count": int(len(_fix41afc11_delta)),
                            "inj_delta": list(_fix41afc11_delta),
                            "inj_must_fetch_called": True,
                            "inj_must_fetch_sources_count": int(len(_fwc_sources or [])) if isinstance(_fwc_sources, list) else 0,
                        })
    except Exception:
        pass
    # Goal:
    #   Ensure injected URLs are merged into the scrape/fetch URL universe in a shape-aware way.
    #   If urls is a list of dicts, append {"url": u}; otherwise append the string u.
    # Diagnostics:
    #   web_context.debug.fix2ae reports counts and added URLs.
    # Safety:
    #   Additive only. Never removes or reorders existing entries.
    try:
        _fx2ae_inj_raw = list(_inj_extra_urls or [])
        _fx2ae_inj_norm = _inj_diag_norm_url_list(_fx2ae_inj_raw) if _fx2ae_inj_raw else []
        _fx2ae_added = []
        _fx2ae_shape = "unknown"
        _fx2ae_urls_before = []
        _fx2ae_urls_after = []
        if isinstance(urls, list):
            _fx2ae_urls_before = [(_d.get("url") if isinstance(_d, dict) else _d) for _d in (urls or [])]
            _urls_are_dicts = bool(urls) and isinstance(urls[0], dict)
            _fx2ae_shape = "dicts" if _urls_are_dicts else "strings"
            _seen_norm = set(_inj_diag_norm_url_list(_fx2ae_urls_before))
            for _u in (_fx2ae_inj_norm or []):
                if not _u:
                    continue
                if _u in _seen_norm:
                    continue
                _seen_norm.add(_u)
                if _urls_are_dicts:
                    urls.append({"url": _u})
                else:
                    urls.append(_u)
                _fx2ae_added.append(_u)
            _fx2ae_urls_after = [(_d.get("url") if isinstance(_d, dict) else _d) for _d in (urls or [])]
        try:
            if isinstance(web_context, dict):
                web_context.setdefault("debug", {})
                if isinstance(web_context.get("debug"), dict):
                    web_context["debug"].setdefault("fix2ae", {})
                    if isinstance(web_context["debug"].get("fix2ae"), dict):
                        web_context["debug"]["fix2ae"].update({
                            "inj_count_raw": int(len(_fx2ae_inj_raw or [])),
                            "inj_count_norm": int(len(_fx2ae_inj_norm or [])),
                            "urls_shape": _fx2ae_shape,
                            "urls_before_count": int(len(_inj_diag_norm_url_list(_fx2ae_urls_before) or [])),
                            "urls_after_count": int(len(_inj_diag_norm_url_list(_fx2ae_urls_after) or [])),
                            "added_count": int(len(_fx2ae_added or [])),
                            "added_urls": list(_fx2ae_added or [])[:50],
                        })
        except Exception:
            pass
    except Exception:
        pass


    scraped_meta = _fix24_build_scraped_meta(urls)

    try:
        _fix2af_led = globals().get("_fix2af_last_scrape_ledger")
        if isinstance(web_context, dict) and isinstance(_fix2af_led, dict):
            web_context["fix2af_scrape_ledger_v1"] = _fix2af_led
    except Exception:
        pass

    # Step 3: Normalize into baseline_sources_cache and hash
    cur_bsc = _fix24_baseline_sources_cache_from_scraped_meta(scraped_meta)

    # Policy + wiring alignment for Evolution UI injected URLs
    #
    # Goal:
    # - Treat Evolution-tab injected URLs as part of the *current source universe*
    #   for hash identity *when they fetch successfully*, consistent with baseline
    #   sources (successful snapshots contribute to identity).
    #
    # Behavior (safe):
    # - If an injected URL was provided (web_context['extra_urls']) and its scrape
    #   status is success, it must appear in cur_bsc so that hash inputs can include it.
    # - If success-but-missing occurs (unexpected), we add a synthetic url-only entry
    #   tagged for hash identity (debug only; no numbers).
    # - If fetch failed, we do NOT force hash mismatch (consistent with policy),
    #   but we record explicit attempted status + reason into diagnostics.
    #
    # Safety:
    # - Does NOT alter fastpath eligibility logic directly; it only ensures that
    #   the identity inputs reflect the actual successfully fetched current sources.
    # - Purely additive; never removes or refactors existing logic.
    try:
        _inj_sm = scraped_meta if isinstance(scraped_meta, dict) else {}
        _inj_attempted_rows = []
        _inj_success_urls = set()
        for _u in (_inj_extra_urls or []):
            _m = _inj_sm.get(_u) if isinstance(_inj_sm.get(_u), dict) else {}
            _st = str(_m.get('status') or _m.get('fetch_status') or '')
            _reason = str(_m.get('status_detail') or _m.get('fail_reason') or '')
            _clen = _m.get('clean_text_len') or _m.get('content_len') or 0
            _inj_attempted_rows.append({
                'url': _u,
                'status': _st or 'unknown',
                'reason': _reason,
                'content_len': int(_clen) if str(_clen).isdigit() else 0,
            })
            if (_st or '').lower() in ('success','ok','fetched'):
                _inj_success_urls.add(_u)

        # Ensure success injected urls are represented in cur_bsc (hash identity)
        if _inj_success_urls and isinstance(cur_bsc, list):
            _bsc_urls = set()
            for _row in cur_bsc:
                if isinstance(_row, dict):
                    _bu = str(_row.get('url') or _row.get('source_url') or '').strip()
                    if _bu:
                        _bsc_urls.add(_bu)
            _missing_success = sorted(list(_inj_success_urls - _bsc_urls))
            for _u in _missing_success:
                cur_bsc.append({
                    'url': _u,
                    'status': 'success',
                    'status_detail': 'synthetic_success_missing_in_bsc',
                    'clean_text': '',
                    'clean_text_len': 0,
                    'extracted_numbers': [],
                    'numbers_found': 0,
                    'fingerprint': 'synthetic_url_only_for_hash',
                    'is_synthetic_for_hash': True,
                })

        # Write attempted status into web_context diagnostics for transparency
        if isinstance(web_context, dict):
            web_context.setdefault('diag_injected_urls', {})
            if isinstance(web_context.get('diag_injected_urls'), dict):
                web_context['diag_injected_urls'].setdefault('attempted', [])
                # Only overwrite if empty to avoid clobbering richer traces
                if not web_context['diag_injected_urls'].get('attempted'):
                    web_context['diag_injected_urls']['attempted'] = _inj_attempted_rows
                web_context['diag_injected_urls']['success_urls'] = sorted(list(_inj_success_urls))
    except Exception:
        pass

    cur_hashes = _fix24_compute_current_hashes(cur_bsc)


    try:
        _hash_inputs = _inj_diag_hash_inputs_from_bsc(cur_bsc)
        if isinstance(web_context, dict):
            web_context.setdefault("diag_injected_urls", {})
            if isinstance(web_context.get("diag_injected_urls"), dict):
                web_context["diag_injected_urls"].update({
                    "run_id": _inj_diag_run_id,
                    "ui_raw": (web_context or {}).get("diag_extra_urls_ui_raw") or "",
                    "ui_norm": _inj_extra_urls,
                    "intake_norm": _inj_extra_urls,
                    "admitted": list(urls or []),
                    "hash_inputs": _hash_inputs,
                    "injected_in_hash_inputs": sorted(list(set(_inj_extra_urls) & set(_hash_inputs))),
                    "set_hashes": {
                        "hash_inputs": _inj_diag_set_hash(_hash_inputs),
                        "admitted": _inj_diag_set_hash(list(urls or [])),
                    }
                })
    except Exception:
        pass


    # Step 4: Compare (v2 preferred)
    equal_v2 = bool(prev_hashes.get("v2") and cur_hashes.get("v2") and prev_hashes["v2"] == cur_hashes["v2"])
    equal_v1 = bool(prev_hashes.get("v1") and cur_hashes.get("v1") and prev_hashes["v1"] == cur_hashes["v1"])
    unchanged = equal_v2 or (not prev_hashes.get("v2") and equal_v1)

    # If the UI (or caller) requests force_rebuild, we intentionally bypass
    # the unchanged fastpath even if hashes match, to exercise rebuild logic.
    _force_rebuild = False
    try:
        _force_rebuild = bool((web_context or {}).get("force_rebuild"))
    except Exception:
        pass
        _force_rebuild = False
    if _force_rebuild:
        unchanged = False
        _fix41_force_rebuild_honored = True
    else:
        _fix41_force_rebuild_honored = False

    if unchanged:
        hashes = {
            "prev_v2": prev_hashes.get("v2",""),
            "cur_v2": cur_hashes.get("v2",""),
            "prev_v1": prev_hashes.get("v1",""),
            "cur_v1": cur_hashes.get("v1",""),
        }
        out_replay = _fix24_make_replay_output(prev_full, hashes)
        try:
            if isinstance(out_replay, dict):
                out_replay.setdefault("code_version", _yureeka_get_code_version())
                out_replay.setdefault("debug", {}).setdefault("fix41", {})
                out_replay["debug"]["fix41"].update({
                    "force_rebuild_seen": bool(_fix41_force_rebuild_seen),
                    "force_rebuild_honored": bool(locals().get("_fix41_force_rebuild_honored", False)),
                    "path": "replay_unchanged",
                })
        except Exception:
            pass

        # Why:
        # - The FIX24 replay path returns early (skipping compute_source_anchored_diff),
        #   which previously meant results.debug.inj_trace_v1 might be missing.
        # - We need injected-URL lifecycle visibility even when hashes match (fastpath/replay)
        #   to validate UI wiring and to explain why a mismatch did/did not occur.
        # Safety:
        # - Pure debug emission only; does NOT affect hash logic, scraping, or fastpath decisions.
        try:
            _wc = web_context if isinstance(web_context, dict) else {}
            _diag = _wc.get("diag_injected_urls") if isinstance(_wc.get("diag_injected_urls"), dict) else {}

            # Populate attempted/persisted for injected URLs from scraped_meta/cur_bsc
            # even when replay fastpath returns early.
            # Also attach an explicit reason when injected URLs are present but not
            # admitted/hashed due to replay semantics.
            try:
                if isinstance(_diag, dict):
                    # Enrich from scraped_meta (injected only) and from BSC (all)
                    _diag = _inj_trace_v1_enrich_diag_from_scraped_meta(_diag, scraped_meta, (_inj_extra_urls or []))
                    _diag = _inj_trace_v1_enrich_diag_from_bsc(_diag, cur_bsc if isinstance(cur_bsc, list) else [])
                    # Explain replay semantics when UI extras exist
                    _ui_norm = _inj_diag_norm_url_list(_diag.get("ui_norm") or [])
                    if _ui_norm:
                        _diag.setdefault("admission_reason", "fastpath_replay_no_rebuild_no_admission")
                        _diag.setdefault("injection_effective", False)
            except Exception:
                pass

            _hash_inputs_replay = _inj_diag_hash_inputs_from_bsc(cur_bsc)
            _trace_replay = _inj_trace_v1_build(
                diag_injected_urls=_diag,
                hash_inputs=_hash_inputs_replay,
                stage="evolution",
                path="fastpath_replay",
                rebuild_pool=None,
                rebuild_selected=None,
            )
            out_replay.setdefault("results", {})
            if isinstance(out_replay.get("results"), dict):
                out_replay["results"].setdefault("debug", {})
                if isinstance(out_replay["results"].get("debug"), dict):
                    out_replay["results"]["debug"]["inj_trace_v1"] = _trace_replay
        except Exception:
            pass

        _fix2d20_trace_year_like_commits(out_replay, stage='evolution', callsite='run_source_anchored_evolution_replay')

        return out_replay
    # Step 5: Changed -> run deterministic evolution diff using existing machinery.
    # Provide web_context with scraped_meta so compute_source_anchored_diff can reconstruct snapshots deterministically.
    wc = {"scraped_meta": scraped_meta}
    # so downstream diff/rebuild logic can record provenance if needed.
    try:
        if isinstance(web_context, dict):
            wc.update({k: v for k, v in web_context.items() if k != "scraped_meta"})
    except Exception:
        pass


    # REFACTOR109: Ensure compute_source_anchored_diff can access the rebuilt *current* pool
    # (Otherwise FIX42 can silently fall back to baseline_sources_cache as 'current'.)
    try:
        wc["current_baseline_sources_cache"] = cur_bsc
        wc["current_sources_cache"] = cur_bsc
        wc["diag_current_sources_cache"] = cur_bsc
        wc.setdefault("debug", {})
        if isinstance(wc.get("debug"), dict):
            wc["debug"]["refactor109_current_pool_wired"] = True
            wc["debug"]["refactor109_current_pool_size"] = len(cur_bsc) if isinstance(cur_bsc, dict) else None
    except Exception:
        pass


    # REFACTOR67: changed-case recompute routes directly through compute_source_anchored_diff

    fn = globals().get("compute_source_anchored_diff")
    if callable(fn):
        try:
            # FIX2D55: lift prev_full onto schema keys BEFORE diff computation

            try:
                _fix2d55_apply_prev_lift(prev_full, wc)
            except Exception:
                pass

            out_changed = fn(prev_full, web_context=wc)
            try:
                if isinstance(out_changed, dict):
                    out_changed.setdefault("code_version", _yureeka_get_code_version())
                    out_changed.setdefault("debug", {}).setdefault("fix41", {})
                    out_changed["debug"]["fix41"].update({
                        "force_rebuild_seen": bool(_fix41_force_rebuild_seen),
                        "force_rebuild_honored": bool(locals().get("_fix41_force_rebuild_honored", False)) or bool(_fix41_force_rebuild_seen),
                        "path": "changed_compute_source_anchored_diff",
                    })
            except Exception:
                pass

            _fix2d20_trace_year_like_commits(out_changed, stage='evolution', callsite='run_source_anchored_evolution_changed')

            # REFACTOR67: always mark changed recompute path with FIX24 debug (no base runner branch)
            try:
                if isinstance(out_changed, dict):
                    out_changed.setdefault("debug", {})
                    if isinstance(out_changed.get("debug"), dict):
                        out_changed["debug"]["fix24"] = True
                        out_changed["debug"]["fix24_mode"] = "recompute_changed"
                        out_changed["debug"]["prev_source_snapshot_hash_v2"] = prev_hashes.get("v2", "")
                        out_changed["debug"]["cur_source_snapshot_hash_v2"] = cur_hashes.get("v2", "")
                        out_changed["debug"]["prev_source_snapshot_hash"] = prev_hashes.get("v1", "")
                        out_changed["debug"]["cur_source_snapshot_hash"] = cur_hashes.get("v1", "")
            except Exception:
                pass

            return out_changed
        except Exception:
            return {
        "status": "failed",
        "message": "FIX24: Evolution recompute failed (compute_source_anchored_diff path).",
        "sources_checked": len(urls),
        "sources_fetched": len(urls),
        "metric_changes": [],
        "debug": {"fix24": True, "fix24_mode": "recompute_failed"},
    }


# [REFACTOR87] legacy diff ladder pruned (FIX32→FIX2D77) — see patch tracker.

#                           force-apply at schema_only rebuild + prev_data
# Why FIX2D80 can still leak 2040->__percent:
#   - The schema_only rebuild may select the year token as the "best" match.
#   - Percent evidence checks can be fooled by unrelated % signs elsewhere.
#   - In some Streamlit exec() reload patterns, earlier wrappers may not
#     (re-)install as expected, so sanitation doesn't run.
#
# FIX2D82 makes the guardrail unconditional:
#   1) Override percent sanitizer: if value_norm is yearlike AND the token
#      itself is that year (raw=="YYYY"), drop regardless of context.
#   2) Wrap rebuild_metrics_from_snapshots_schema_only_fix16 directly so that
#      any caller (incl. FIX2D75 materialize) gets the sanitized output.
#   3) Wrap compute_source_anchored_diff directly to sanitize previous_data
#      before diff join (cleans old HistoryFull snapshots too).

try:

    def _fix2d82__extract_raw_token_v1(rec: dict) -> str:
        """Extract the best *token* string for this record (not context)."""
        try:
            if not isinstance(rec, dict):
                return ''
            ev = rec.get('evidence')
            if isinstance(ev, list) and ev:
                e0 = ev[0]
                if isinstance(e0, dict):
                    r = e0.get('raw')
                    if isinstance(r, str) and r.strip():
                        return r.strip()
            r2 = rec.get('raw')
            if isinstance(r2, str) and r2.strip():
                return r2.strip()
            v = rec.get('value')
            if v is None:
                v = rec.get('value_norm')
            return str(v).strip() if v is not None else ''
        except Exception:
            return ''


    def _fix2d82__is_year_token_v1(vnorm, raw_token: str) -> bool:
        """True when (vnorm is yearlike 1900-2100 int) AND raw_token is that YYYY."""
        try:
            if not callable(globals().get('_fix2d80__is_yearlike_number_v1')):
                # fallback
                try:
                    fv = float(vnorm)
                    if not fv.is_integer():
                        return False
                    iv = int(fv)
                    yearlike = 1900 <= iv <= 2100
                except Exception:
                    return False
            else:
                yearlike = bool(_fix2d80__is_yearlike_number_v1(vnorm))
            if not yearlike:
                return False
            tok = str(raw_token or '').strip()
            if len(tok) != 4 or (not tok.isdigit()):
                return False
            return int(tok) == int(float(vnorm))
        except Exception:
            return False


    def _fix2d82__is_percent_key_v1(k: str, rec: dict, schema: dict) -> bool:
        try:
            if isinstance(k, str) and k.endswith('__percent'):
                return True
            if isinstance(schema, dict) and isinstance(k, str):
                spec = schema.get(k)
                if isinstance(spec, dict):
                    dim = str(spec.get('dimension') or '').strip().lower()
                    uf = str(spec.get('unit_family') or '').strip().lower()
                    ut = str(spec.get('unit_tag') or '').strip().lower()
                    blob = ' '.join([dim, uf, ut])
                    return ('percent' in blob) or ('%' in blob)
            if isinstance(rec, dict):
                dim = str(rec.get('dimension') or '').strip().lower()
                uf = str(rec.get('unit_family') or '').strip().lower()
                ut = str(rec.get('unit_tag') or '').strip().lower()
                blob = ' '.join([dim, uf, ut])
                return ('percent' in blob) or ('%' in blob)
        except Exception:
            return False
        return False


    def _fix2d82__sanitize_pmc_percent_keys_v1(pmc: dict, metric_schema_frozen: dict = None) -> tuple:
        dbg = {
            "applied": False,
            "input_count": int(len(pmc) if isinstance(pmc, dict) else 0),
            "output_count": 0,
            "checked_percent_keys": 0,
            "dropped_count": 0,
            "dropped_keys_sample": [],
            "reasons_sample": [],
            "fix": "FIX2D82",
        }
        try:
            if not isinstance(pmc, dict) or not pmc:
                dbg["applied"] = True
                dbg["output_count"] = 0
                return pmc if isinstance(pmc, dict) else {}, dbg

            schema = metric_schema_frozen if isinstance(metric_schema_frozen, dict) else {}
            out = dict(pmc)

            for k in list(out.keys()):
                rec = out.get(k)
                if not _fix2d82__is_percent_key_v1(k, rec if isinstance(rec, dict) else {}, schema):
                    continue

                dbg["checked_percent_keys"] += 1
                if not isinstance(rec, dict):
                    out.pop(k, None)
                    dbg["dropped_count"] += 1
                    if len(dbg["dropped_keys_sample"]) < 8:
                        dbg["dropped_keys_sample"].append(str(k))
                        dbg["reasons_sample"].append({"key": str(k), "reason": "non_dict_record"})
                    continue

                vnorm = rec.get('value_norm')
                raw_token = _fix2d82__extract_raw_token_v1(rec)

                # FIX2D82: hard drop year-token values for percent keys.
                if _fix2d82__is_year_token_v1(vnorm, raw_token):
                    out.pop(k, None)
                    dbg["dropped_count"] += 1
                    if len(dbg["dropped_keys_sample"]) < 8:
                        dbg["dropped_keys_sample"].append(str(k))
                        dbg["reasons_sample"].append({
                            "key": str(k),
                            "reason": "year_token_for_percent",
                            "raw_token": str(raw_token)[:60],
                            "value_norm": vnorm,
                        })
                    continue

                # Strong percent evidence must be tied to the token/unit, not context.
                unit = str(rec.get('unit') or rec.get('unit_tag') or '')
                rt = str(raw_token or '').lower()
                token_has_pct = ('%' in rt) or ('percent' in rt) or ('pct' in rt)
                unit_has_pct = ('%' in unit) or ('percent' in unit.lower())

                if (not token_has_pct) and (not unit_has_pct):
                    out.pop(k, None)
                    dbg["dropped_count"] += 1
                    if len(dbg["dropped_keys_sample"]) < 8:
                        dbg["dropped_keys_sample"].append(str(k))
                        dbg["reasons_sample"].append({
                            "key": str(k),
                            "reason": "no_strong_percent_evidence_token",
                            "unit": unit,
                            "raw_token": str(raw_token)[:80],
                            "value_norm": vnorm,
                        })
                    continue

            dbg["output_count"] = int(len(out))
            dbg["applied"] = True
            return out, dbg
        except Exception as e:
            dbg["applied"] = False
            dbg["error"] = str(type(e).__name__)
            return pmc, dbg


    def _fix2d82__get_schema_from_prev_response_v1(prev_response: dict) -> dict:
        try:
            if not isinstance(prev_response, dict):
                return None
            schema = prev_response.get('metric_schema_frozen')
            if isinstance(schema, dict):
                return schema
            pr = prev_response.get('primary_response') if isinstance(prev_response.get('primary_response'), dict) else None
            if isinstance(pr, dict) and isinstance(pr.get('metric_schema_frozen'), dict):
                return pr.get('metric_schema_frozen')
            return None
        except Exception:
            return None


    # (1) Force-apply sanitizer to schema_only rebuild (Analysis baseline)
    try:
        _fix2d82__orig_schema_only = globals().get('rebuild_metrics_from_snapshots_schema_only_fix16')
        if callable(_fix2d82__orig_schema_only) and (not getattr(_fix2d82__orig_schema_only, '_fix2d82_wrapped', False)):

            def rebuild_metrics_from_snapshots_schema_only_fix16(prev_response, snapshot_pool, web_context=None):  # noqa: F811
                rebuilt = _fix2d82__orig_schema_only(prev_response, snapshot_pool, web_context=web_context)
                schema = _fix2d82__get_schema_from_prev_response_v1(prev_response)
                if isinstance(rebuilt, dict) and rebuilt:
                    rebuilt2, sdbg = _fix2d82__sanitize_pmc_percent_keys_v1(rebuilt, metric_schema_frozen=schema)
                    rebuilt = rebuilt2
                    try:
                        if isinstance(prev_response, dict):
                            prev_response.setdefault('debug', {})
                            if isinstance(prev_response.get('debug'), dict):
                                prev_response['debug']['fix2d82_percent_sanitize_schema_only'] = sdbg
                    except Exception:
                        pass
                return rebuilt

            try:
                rebuild_metrics_from_snapshots_schema_only_fix16._fix2d82_wrapped = True
            except Exception:
                pass
            globals()['rebuild_metrics_from_snapshots_schema_only_fix16'] = rebuild_metrics_from_snapshots_schema_only_fix16
    except Exception:
        pass


    # (2) Force-apply sanitizer to previous_data before diff join
    try:
        _fix2d82__orig_compute = globals().get('compute_source_anchored_diff')
        if callable(_fix2d82__orig_compute) and (not getattr(_fix2d82__orig_compute, '_fix2d82_wrapped', False)):

            def compute_source_anchored_diff(previous_data: dict, web_context: dict = None) -> dict:  # noqa: F811
                try:
                    if isinstance(previous_data, dict):
                        schema = _fix2d82__get_schema_from_prev_response_v1(previous_data)
                        # locate pmc
                        pmc = previous_data.get('primary_metrics_canonical')
                        if not isinstance(pmc, dict) or not pmc:
                            pr = previous_data.get('primary_response') if isinstance(previous_data.get('primary_response'), dict) else None
                            if isinstance(pr, dict) and isinstance(pr.get('primary_metrics_canonical'), dict):
                                pmc = pr.get('primary_metrics_canonical')
                        if isinstance(pmc, dict) and pmc:
                            pmc2, sdbg = _fix2d82__sanitize_pmc_percent_keys_v1(pmc, metric_schema_frozen=schema)
                            try:
                                previous_data['primary_metrics_canonical'] = pmc2
                            except Exception:
                                pass
                            try:
                                previous_data.setdefault('primary_response', {})
                                if isinstance(previous_data.get('primary_response'), dict):
                                    previous_data['primary_response']['primary_metrics_canonical'] = pmc2
                            except Exception:
                                pass
                            try:
                                previous_data.setdefault('debug', {})
                                if isinstance(previous_data.get('debug'), dict):
                                    previous_data['debug']['fix2d82_prev_percent_sanitize'] = sdbg
                            except Exception:
                                pass
                except Exception:
                    pass

                return _fix2d82__orig_compute(previous_data, web_context=web_context)

            try:
                compute_source_anchored_diff._fix2d82_wrapped = True
            except Exception:
                pass
            globals()['compute_source_anchored_diff'] = compute_source_anchored_diff
    except Exception:
        pass

except Exception:
    pass


# FIX2D82_VERSION_FINAL_OVERRIDE (REQUIRED): ensure patch id is authoritative
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
    globals()['CODE_VERSION'] = CODE_VERSION
except Exception:
    pass


#
# - Removes FIX2D78/FIX2D79 percent-guard wrappers (superseded by definitive FIX2D82 sanitizer)
# - Ensures a single authoritative CODE_VERSION at end-of-file
# - Adds patch tracker entry
# FIX2D86_VERSION_FINAL_OVERRIDE (REQUIRED): keep patch id authoritative
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
    globals()['CODE_VERSION'] = CODE_VERSION
except Exception:
    pass

# --- Streamlit exec display guard: avoid dumping long function docs in UI ---
try:
    _fn = globals().get("diff_metrics_by_name")
    if callable(_fn):
        _fn.__doc__ = ""
except Exception:
    pass

# FIX2D86: final wrapper — sanitize schema-only rebuild output for percent keys
try:
    _fix2d86__orig_schema_only = globals().get("rebuild_metrics_from_snapshots_schema_only_fix16")
    if callable(_fix2d86__orig_schema_only) and (not getattr(_fix2d86__orig_schema_only, "_fix2d86_wrapped", False)):

        def rebuild_metrics_from_snapshots_schema_only_fix16(prev_response, snapshot_pool, web_context=None):  # noqa: F811
            rebuilt = _fix2d86__orig_schema_only(prev_response, snapshot_pool, web_context=web_context)
            try:
                schema = {}
                if isinstance(prev_response, dict):
                    schema = prev_response.get("metric_schema_frozen") or {}
                if isinstance(rebuilt, dict) and rebuilt:
                    rebuilt2, sdbg = _fix2d86_sanitize_pmc_percent_year_tokens_v1(
                        pmc=rebuilt,
                        metric_schema_frozen=schema if isinstance(schema, dict) else {},
                        label="schema_only_rebuild_fix16_final_wrap",
                    )
                    rebuilt = rebuilt2
                    try:
                        if isinstance(prev_response, dict):
                            prev_response.setdefault("debug", {})
                            if isinstance(prev_response.get("debug"), dict):
                                prev_response["debug"]["fix2d86_percent_year_token_sanitize_schema_only"] = sdbg
                    except Exception:
                        pass
            except Exception:
                pass
            return rebuilt

        try:
            rebuild_metrics_from_snapshots_schema_only_fix16._fix2d86_wrapped = True
        except Exception:
            pass

        globals()["rebuild_metrics_from_snapshots_schema_only_fix16"] = rebuild_metrics_from_snapshots_schema_only_fix16
except Exception:
    pass


# REFACTOR04: VERSION FINAL OVERRIDE (LAST-WINS)
# - This file contains legacy CODE_VERSION bumps from earlier phases.
# - Ensure the refactor patch id remains authoritative.
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
    globals()["CODE_VERSION"] = CODE_VERSION
except Exception:
    pass

# Goal:
#   - Provide a stable gate for refactor/consolidation work.
#   - Executes: Analysis (headless) -> Evolution (source-anchored) and asserts invariants.
#
# Invocation:
#   python REFACTOR04_full_codebase_streamlit_safe.py --run_refactor_harness
#   or RUN_REFACTOR_HARNESS=1
#
# Optional env overrides:
#   REFACTOR_HARNESS_QUERY                      - analysis question text
#   REFACTOR_HARNESS_NUM_SOURCES               - int (default: 3)
#   REFACTOR_HARNESS_EXTRA_URLS                - newline-separated URLs (applied to both analysis+evolution)
#   REFACTOR_HARNESS_EXTRA_URLS_ANALYSIS       - newline-separated URLs (analysis only)
#   REFACTOR_HARNESS_EXTRA_URLS_EVOLUTION      - newline-separated URLs (evolution only)
#   REFACTOR_HARNESS_FORCE_REBUILD             - 1/0 (default: 1)
#   REFACTOR_HARNESS_REPORT_PATH               - directory path for JSON report (default: cwd)

def _refactor01__bool(v, default=False):
    try:
        s = str(v).strip().lower()
        if s in ("1", "true", "yes", "y", "on"):
            return True
        if s in ("0", "false", "no", "n", "off"):
            return False
    except Exception:
        pass
    return bool(default)

def _refactor01__parse_urls(raw):
    urls = []
    try:
        for line in str(raw or "").splitlines():
            u = line.strip()
            if not u:
                continue
            if u.startswith("http://") or u.startswith("https://"):
                urls.append(u)
    except Exception:
        pass
    # de-dupe while preserving order
    out = []
    seen = set()
    for u in urls:
        if u in seen:
            continue
        seen.add(u)
        out.append(u)
    return out

def _refactor01__safe_now_iso():
    try:
        if callable(globals().get("now_utc")):
            return now_utc().isoformat()
    except Exception:
        pass
    try:
        from datetime import datetime, timezone
        return datetime.now(timezone.utc).isoformat()
    except Exception:
        return ""

def _refactor01__safe_get_schema_and_pmc(primary_data: dict):
    """Mirror the minimal canonicalization steps used by the Analysis UI."""
    if not isinstance(primary_data, dict):
        return {}, {}, {}

    # Ensure primary_metrics_canonical is split into ok/provisional deterministically
    try:
        _pmc_raw = primary_data.get("primary_metrics_canonical") or {}
        _split = globals().get("_fix2d58b_split_primary_metrics_canonical")
        if callable(_split):
            _pmc_ok, _pmc_prov = _split(_pmc_raw)
            if isinstance(_pmc_ok, dict):
                primary_data["primary_metrics_canonical"] = _pmc_ok
            if isinstance(_pmc_prov, dict) and _pmc_prov:
                primary_data["primary_metrics_provisional"] = _pmc_prov
    except Exception:
        pass

    # Freeze schema if missing
    try:
        if (not isinstance(primary_data.get("metric_schema_frozen"), dict)) and isinstance(primary_data.get("primary_metrics_canonical"), dict) and primary_data.get("primary_metrics_canonical"):
            _freeze = globals().get("freeze_metric_schema")
            if callable(_freeze):
                primary_data["metric_schema_frozen"] = _freeze(primary_data.get("primary_metrics_canonical") or {})
    except Exception:
        pass

    schema = primary_data.get("metric_schema_frozen") if isinstance(primary_data.get("metric_schema_frozen"), dict) else {}
    pmc = primary_data.get("primary_metrics_canonical") if isinstance(primary_data.get("primary_metrics_canonical"), dict) else {}
    prov = primary_data.get("primary_metrics_provisional") if isinstance(primary_data.get("primary_metrics_provisional"), dict) else {}
    return schema, pmc, prov

def _refactor02_run_harness_v2():
    import os, sys, json, traceback

    # REFACTOR12: ensure version lock + final bindings are applied before harness assertions
    try:
        _yureeka_lock_version_globals_v1()
        _yureeka_ensure_final_bindings_v1()
    except Exception:
        pass

    query = str(os.getenv("REFACTOR_HARNESS_QUERY") or "").strip()
    if not query:
        # Safe default (user can override via env)
        query = "Global EV sales 2024 and global EV market share 2025"

    try:
        num_sources = int(str(os.getenv("REFACTOR_HARNESS_NUM_SOURCES") or "3").strip())
    except Exception:
        num_sources = 3

    force_rebuild = _refactor01__bool(os.getenv("REFACTOR_HARNESS_FORCE_REBUILD", "1"), default=True)

    extra_urls_common = _refactor01__parse_urls(os.getenv("REFACTOR_HARNESS_EXTRA_URLS"))
    extra_urls_analysis = _refactor01__parse_urls(os.getenv("REFACTOR_HARNESS_EXTRA_URLS_ANALYSIS"))
    extra_urls_evolution = _refactor01__parse_urls(os.getenv("REFACTOR_HARNESS_EXTRA_URLS_EVOLUTION"))

    # common applies to both (unless already present)
    for u in extra_urls_common:
        if u not in extra_urls_analysis:
            extra_urls_analysis.append(u)
        if u not in extra_urls_evolution:
            extra_urls_evolution.append(u)

    analysis_run_id = ""
    evo_run_id = ""
    try:
        mk = globals().get("_inj_diag_make_run_id")
        if callable(mk):
            analysis_run_id = mk("analysis_harness")
            evo_run_id = mk("evolution_harness")
    except Exception:
        pass

    report = {
        "patch_id": "REFACTOR18",
        "code_version": _yureeka_get_code_version(),
            "authority_manifest_v1": _yureeka_authority_manifest_v1(),
        "run_ts_utc": _refactor01__safe_now_iso(),
        "config": {
            "query": query,
            "num_sources": int(num_sources),
            "force_rebuild": bool(force_rebuild),
            "extra_urls_analysis": list(extra_urls_analysis),
            "extra_urls_evolution": list(extra_urls_evolution),
        },
        "analysis": {},
        "evolution": {},
        "assertions": [],
        "status": "unknown",
    }

    def _assert(name, ok, detail=""):
        report["assertions"].append({
            "name": str(name),
            "pass": bool(ok),
            "detail": (str(detail)[:2000] if detail is not None else ""),
        })
        return bool(ok)

    ok_all = True


    # 0) Binding sanity: ensure we are running against the authoritative diff binding.
    try:
        _auth = _yureeka_get_authoritative_binding_tag_v1(globals().get("diff_metrics_by_name"))
    except Exception:
        _auth = None
    expected_auth = str(_yureeka_get_code_version() or "")
    ok_all = _assert("binding.diff_metrics_by_name_authoritative", (_auth == expected_auth), f"auth={_auth} expected={expected_auth}") and ok_all
    try:
        _obj = globals().get("diff_metrics_by_name")
        _auth_obj = globals().get("_YUREEKA_DIFF_METRICS_BY_NAME_AUTHORITATIVE")
        ok_all = _assert("binding.diff_fn_object_match", (_auth_obj is None) or (_obj is _auth_obj), f"obj={type(_obj)}") and ok_all
    except Exception:
        ok_all = _assert("binding.diff_fn_object_match", False, "exception") and ok_all

    try:
        fwc = globals().get("fetch_web_context")
        qp = globals().get("query_perplexity")
        if not callable(fwc):
            ok_all = _assert("analysis.fetch_web_context_defined", False, "fetch_web_context is not callable") and ok_all
            raise RuntimeError("fetch_web_context missing")
        if not callable(qp):
            ok_all = _assert("analysis.query_perplexity_defined", False, "query_perplexity is not callable") and ok_all
            raise RuntimeError("query_perplexity missing")

        web_context = fwc(
            query,
            num_sources=num_sources,
            extra_urls=extra_urls_analysis,
            diag_run_id=str(analysis_run_id or ""),
            diag_extra_urls_ui_raw="\n".join(extra_urls_analysis),
        )

        if not isinstance(web_context, dict) or not web_context.get("search_results"):
            # mirror UI fallback
            web_context = {
                "search_results": [],
                "scraped_content": {},
                "summary": "",
                "sources": [],
                "source_reliability": [],
            }

        primary_response = qp(query, web_context, query_structure=None)
        ok_all = _assert("analysis.primary_response_nonempty", bool(primary_response), "Primary model returned empty response") and ok_all
        if not primary_response:
            raise RuntimeError("primary_response empty")

        try:
            primary_data = json.loads(primary_response)
        except Exception as e:
            ok_all = _assert("analysis.primary_response_json_parse", False, f"JSON parse failed: {e}") and ok_all
            raise

        schema, pmc, prov = _refactor01__safe_get_schema_and_pmc(primary_data)

        # optional veracity scoring (non-fatal)
        veracity_scores = {}
        try:
            ev = globals().get("evidence_based_veracity")
            if callable(ev):
                veracity_scores = ev(primary_data, web_context) or {}
        except Exception:
            veracity_scores = {}

        analysis_out = {
            "question": query,
            "timestamp": _refactor01__safe_now_iso(),
            "primary_response": primary_data,
            "veracity_scores": veracity_scores,
            "web_sources": (web_context.get("sources", []) if isinstance(web_context, dict) else []),
            "code_version": _yureeka_get_code_version(),
            "authority_manifest_v1": _yureeka_authority_manifest_v1(),
            # ensure evolution can find these top-level as well
            "metric_schema_frozen": schema,
            "primary_metrics_canonical": pmc,
        }
        try:
            if isinstance(analysis_out.get("primary_response"), dict):
                analysis_out["primary_response"]["code_version"] = _yureeka_get_code_version()
        except Exception:
            pass

        # attach analysis-aligned snapshots (stable cache evolution should reuse)
        try:
            attach = globals().get("attach_source_snapshots_to_analysis")
            if callable(attach):
                analysis_out = attach(analysis_out, web_context)
        except Exception:
            pass

        report["analysis"] = {
            "diag_run_id": analysis_run_id,
            "schema_key_count": int(len(schema or {})),
            "pmc_key_count": int(len(pmc or {})),
            "baseline_sources_cache_count": int(len((analysis_out or {}).get("baseline_sources_cache") or [])),
        }

        ok_all = _assert("analysis.code_version_matches_lock", str((analysis_out or {}).get("code_version") or "") == _yureeka_get_code_version(), f"analysis.code_version={(analysis_out or {}).get('code_version')} lock={_yureeka_get_code_version()}") and ok_all
        try:
            _pr = (analysis_out or {}).get("primary_response") if isinstance(analysis_out, dict) else None
            ok_all = _assert("analysis.primary_response_code_version_matches_lock", str((_pr or {}).get("code_version") or "") == _yureeka_get_code_version(), f"analysis.primary_response.code_version={(_pr or {}).get('code_version')} lock={_yureeka_get_code_version()}") and ok_all
        except Exception:
            pass
        try:
            _fn = globals().get("diff_metrics_by_name")
            _tag = str(getattr(_fn, "__YUREEKA_AUTHORITATIVE_BINDING__", "") or "")
            ok_all = _assert("binding.diff_metrics_by_name_authoritative_tag", _tag == str(globals().get("_YUREEKA_FINAL_BINDINGS_VERSION") or ""), f"tag={_tag} final={globals().get('_YUREEKA_FINAL_BINDINGS_VERSION')}") and ok_all
        except Exception:
            pass

        ok_all = _assert("analysis.schema_nonempty", int(len(schema or {})) > 0, f"schema_key_count={len(schema or {})}") and ok_all
        ok_all = _assert("analysis.pmc_nonempty", int(len(pmc or {})) > 0, f"pmc_key_count={len(pmc or {})}") and ok_all


        def _h_has_currency(_s):
            try:
                s = str(_s or "").lower()
                return any(x in s for x in ("us$", "usd", "sgd", "eur", "gbp", "aud", "cny", "jpy", "$", "€", "£", "¥"))
            except Exception:
                return False

        def _h_has_percent(_s):
            try:
                s = str(_s or "").lower()
                return ("%" in s) or ("percent" in s)
            except Exception:
                return False

        bad_mag = []
        bad_cur = []
        bad_pct = []

        for _ckey, _mobj in (pmc or {}).items():
            if not isinstance(_mobj, dict):
                continue

            _raw = _mobj.get("raw")
            if not _raw:
                _ev = _mobj.get("evidence")
                if isinstance(_ev, list) and _ev and isinstance(_ev[0], dict):
                    _raw = _ev[0].get("raw") or _ev[0].get("snippet") or ""
                elif isinstance(_ev, dict):
                    _raw = _ev.get("raw") or _ev.get("snippet") or ""
            if not _raw:
                try:
                    _v = _mobj.get("value_norm") if _mobj.get("value_norm") is not None else _mobj.get("value")
                    _ut = (_mobj.get("unit") or _mobj.get("unit_tag") or "").strip()
                    if _v is not None and _ut:
                        _raw = f"{_v} {_ut}"
                    elif _v is not None:
                        _raw = str(_v)
                except Exception:
                    _raw = ""

            _unit_tag = (_mobj.get("unit") or _mobj.get("unit_tag") or _mobj.get("base_unit") or "").strip().lower()
            _ck = str(_ckey or "").lower()

            _is_pct = ("__percent" in _ck) or (_unit_tag == "percent")
            _is_cur = ("__currency" in _ck)
            _is_mag = ("__unit_" in _ck) or (not _is_pct and not _is_cur)

            if _is_mag:
                # magnitude/count-like must not carry currency/percent markers (cross-dimension leakage guard)
                if _h_has_currency(_raw) or _h_has_percent(_raw):
                    bad_mag.append({"canonical_key": str(_ckey), "raw": str(_raw)[:120]})

            if _is_cur:
                # currency keys should carry a currency marker in raw/unit
                if not _h_has_currency(_raw):
                    bad_cur.append({"canonical_key": str(_ckey), "raw": str(_raw)[:120]})

            if _is_pct:
                # percent keys must carry percent marker and must not bind bare years
                _bad_here = False
                if not _h_has_percent(_raw):
                    _bad_here = True
                try:
                    _v = _mobj.get("value_norm") if _mobj.get("value_norm") is not None else _mobj.get("value")
                    if isinstance(_v, (int, float)):
                        _iv = int(_v)
                        if 1900 <= _iv <= 2100 and abs(float(_v) - float(_iv)) < 1e-9:
                            _bad_here = True
                except Exception:
                    pass
                if _bad_here:
                    bad_pct.append({"canonical_key": str(_ckey), "raw": str(_raw)[:120], "value_norm": _mobj.get("value_norm")})

            if (len(bad_mag) >= 5) and (len(bad_cur) >= 5) and (len(bad_pct) >= 5):
                break

        ok_all = _assert("pmc.magnitude_keys_no_currency_or_percent_markers", (len(bad_mag) == 0), f"samples={bad_mag}") and ok_all
        ok_all = _assert("pmc.currency_keys_have_currency_marker", (len(bad_cur) == 0), f"samples={bad_cur}") and ok_all
        ok_all = _assert("pmc.percent_keys_have_percent_marker_and_not_yearlike", (len(bad_pct) == 0), f"samples={bad_pct}") and ok_all

    except Exception as e:
        report["analysis"]["error"] = f"{type(e).__name__}: {e}"
        report["analysis"]["traceback"] = traceback.format_exc()[:8000]
        report["status"] = "fail"
        # write report
        _dir = str(os.getenv("REFACTOR_HARNESS_REPORT_PATH") or os.getcwd())
        try:
            os.makedirs(_dir, exist_ok=True)
        except Exception:
            _dir = os.getcwd()
        fname = f"refactor_harness_report_REFACTOR19_{analysis_run_id or 'analysis'}_{evo_run_id or 'evo'}.json"
        fpath = os.path.join(_dir, fname)
        try:
            with open(fpath, "w", encoding="utf-8") as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
        except Exception:
            pass
        print("[REFACTOR10] Harness FAILED during analysis stage. Report:", fpath)
        return False

    evo_out = None
    try:
        evo_fn = globals().get("run_source_anchored_evolution")
        if not callable(evo_fn):
            ok_all = _assert("evolution.run_source_anchored_evolution_defined", False, "run_source_anchored_evolution is not callable") and ok_all
            raise RuntimeError("run_source_anchored_evolution missing")

        evo_out = evo_fn(
            analysis_out,
            web_context={
                "force_rebuild": bool(force_rebuild),
                "extra_urls": list(extra_urls_evolution),
                "diag_run_id": str(evo_run_id or ""),
                "diag_extra_urls_ui_raw": "\n".join(extra_urls_evolution),
            },
        )
        ok_all = _assert("evolution.output_dict", isinstance(evo_out, dict), f"type={type(evo_out)}") and ok_all
        if not isinstance(evo_out, dict):
            raise RuntimeError("evolution output not a dict")

        rows = evo_out.get("metric_changes_v2")
        if not isinstance(rows, list):
            rows = []

        dbg = evo_out.get("debug") if isinstance(evo_out.get("debug"), dict) else {}
        summary = dbg.get("diff_panel_v2_summary") if isinstance(dbg.get("diff_panel_v2_summary"), dict) else {}

        both_count = summary.get("both_count")
        prev_only = summary.get("prev_only_count")
        cur_only = summary.get("cur_only_count")
        rows_total = summary.get("rows_total")
        join_mode = summary.get("join_mode")

        report["evolution"] = {
            "diag_run_id": evo_run_id,
            "metric_changes_v2_rows": int(len(rows)),
            "diff_panel_v2_summary": summary,
        }


        try:
            if rows_total is not None:
                ok_all = _assert("diff.summary_rows_total_matches_len", int(rows_total) == int(len(rows)), f"rows_total={rows_total} len(rows)={len(rows)}") and ok_all
            if (both_count is not None) and (prev_only is not None) and (cur_only is not None) and (rows_total is not None):
                ok_all = _assert("diff.summary_partition_counts_match_total", (int(both_count) + int(prev_only) + int(cur_only)) == int(rows_total), f"both={both_count} prev_only={prev_only} cur_only={cur_only} rows_total={rows_total}") and ok_all

            _found = summary.get("found")
            _not_found = summary.get("not_found")
            if _found is not None:
                ok_all = _assert("diff.summary_found_gt_0", int(_found) > 0, f"found={_found}") and ok_all
            if _not_found is not None:
                ok_all = _assert("diff.summary_not_found_eq_0", int(_not_found) == 0, f"not_found={_not_found}") and ok_all

            kov = dbg.get("key_overlap_v1")
            if isinstance(kov, dict):
                _pc = kov.get("prev_count")
                _cc = kov.get("cur_count")
                _oc = kov.get("overlap_count")
                if _pc is not None:
                    ok_all = _assert("diff.key_overlap_prev_count_gt_0", int(_pc) > 0, f"prev_count={_pc}") and ok_all
                if _cc is not None:
                    ok_all = _assert("diff.key_overlap_cur_count_gt_0", int(_cc) > 0, f"cur_count={_cc}") and ok_all
                if (_pc is not None) and (_cc is not None) and (_oc is not None):
                    ok_all = _assert("diff.key_overlap_overlap_leq_min", int(_oc) <= min(int(_pc), int(_cc)), f"prev={_pc} cur={_cc} overlap={_oc}") and ok_all
        except Exception:
            ok_all = _assert("diff.summary_consistency_checks", False, "exception") and ok_all

        try:
            ok_all = _assert("evolution.code_version_matches_lock", str((evo_out or {}).get("code_version") or "") == _yureeka_get_code_version(), f"evolution.code_version={(evo_out or {}).get('code_version')} lock={_yureeka_get_code_version()}") and ok_all
        except Exception:
            pass

        # 1) diff rows exist and include both prev+cur
        any_both = False
        for r in rows:
            try:
                if (r.get("previous_value_norm") is not None) and (r.get("current_value_norm") is not None):
                    any_both = True
                    break
            except Exception:
                continue

        ok_all = _assert("diff.any_both_row_exists", bool(any_both), f"rows={len(rows)}") and ok_all

        # 2) summary both_count > 0 (if present)
        if both_count is not None:
            ok_all = _assert("diff.summary_both_count_gt_0", int(both_count) > 0, f"both_count={both_count}") and ok_all

        # 3) no global 'no_prev_metrics' failure mode (heuristic: not all rows are no_prev_metrics)
        if rows:
            no_prev_all = True
            for r in rows:
                ct = str(r.get("change_type") or "").strip().lower()
                if ct != "no_prev_metrics":
                    no_prev_all = False
                    break
            ok_all = _assert("diff.not_all_no_prev_metrics", (not no_prev_all), f"rows_total={len(rows)}") and ok_all

        # 4) percent-year token rule: percent-key prev/current must not look like a bare year
        bad_percent_year = []
        for r in rows:
            try:
                ckey = str(r.get("canonical_key") or "")
                unit_tag = str(r.get("current_unit_tag") or r.get("previous_unit_tag") or "")
                is_pct = ("__percent" in ckey) or (unit_tag == "percent") or ("percent" in ckey.lower())
                if not is_pct:
                    continue

                for fld in ("previous_value_norm", "current_value_norm"):
                    v = r.get(fld)
                    if v is None:
                        continue
                    try:
                        fv = float(v)
                    except Exception:
                        continue
                    if 1900.0 <= fv <= 2100.0 and abs(fv - round(fv)) < 1e-9:
                        bad_percent_year.append({"canonical_key": ckey, fld: v})
                        if len(bad_percent_year) >= 5:
                            break
                if len(bad_percent_year) >= 5:
                    break
            except Exception:
                continue

        ok_all = _assert("percent.prev_or_cur_not_yearlike_1900_2100", (len(bad_percent_year) == 0), f"samples={bad_percent_year}") and ok_all

        # 4b) REFACTOR04: guard against false unit_mismatch when both sides share the same scale tag on magnitude keys.
        bad_unit_mismatch_same_scale = []
        for r in rows:
            try:
                ckey = str(r.get("canonical_key") or "")
                if "__unit_" not in ckey:
                    continue
                if str(r.get("change_type") or "").strip().lower() != "unit_mismatch":
                    continue
                pu = str(r.get("previous_unit_tag") or "").upper().strip()
                cu = str(r.get("current_unit_tag") or "").upper().strip()
                if pu and cu and (pu == cu) and (pu in ("K", "M", "B", "T")):
                    bad_unit_mismatch_same_scale.append({
                        "canonical_key": ckey,
                        "previous_unit_tag": pu,
                        "current_unit_tag": cu,
                        "prev": r.get("previous_value_norm"),
                        "cur": r.get("current_value_norm"),
                    })
                    if len(bad_unit_mismatch_same_scale) >= 5:
                        break
            except Exception:
                continue
        ok_all = _assert(
            "unit_mismatch.not_triggered_when_same_scale_magnitude",
            (len(bad_unit_mismatch_same_scale) == 0),
            f"samples={bad_unit_mismatch_same_scale}",
        ) and ok_all


        # 5) internal count sanity (summary counts should align where possible)
        if isinstance(summary, dict) and both_count is not None and prev_only is not None and rows_total is not None:
            try:
                prev_key_count_summary = int(both_count) + int(prev_only)
                pmc_keys = list((analysis_out.get("primary_metrics_canonical") or {}).keys()) if isinstance(analysis_out.get("primary_metrics_canonical"), dict) else []
                ok_all = _assert("counts.prev_key_count_matches_baseline_pmc", prev_key_count_summary == len(pmc_keys), f"summary_prev={prev_key_count_summary} baseline_pmc={len(pmc_keys)}") and ok_all
                if str(join_mode or "") != "union":
                    ok_all = _assert("counts.rows_total_matches_prev_key_count_nonunion", int(rows_total) == prev_key_count_summary, f"rows_total={rows_total} prev_key_count={prev_key_count_summary} join_mode={join_mode}") and ok_all
            except Exception:
                pass

    except Exception as e:
        report["evolution"]["error"] = f"{type(e).__name__}: {e}"
        report["evolution"]["traceback"] = traceback.format_exc()[:8000]
        report["status"] = "fail"
        ok_all = False

    try:
        report["status"] = "pass" if ok_all else "fail"
        _dir = str(os.getenv("REFACTOR_HARNESS_REPORT_PATH") or os.getcwd())
        try:
            os.makedirs(_dir, exist_ok=True)
        except Exception:
            _dir = os.getcwd()
        fname = f"refactor_harness_report_REFACTOR08_{analysis_run_id or 'analysis'}_{evo_run_id or 'evo'}.json"
        fpath = os.path.join(_dir, fname)
        with open(fpath, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"[REFACTOR19] Harness {'PASSED' if ok_all else 'FAILED'}. Report: {fpath}")
    except Exception:
        pass

    return bool(ok_all)


#


# REFACTOR09: DIFF ENGINE CONSOLIDATION (WRAPPER)
#
# Purpose:
#   - Establish one stable, explicit implementation target for future extraction.
#   - Preserve current working behavior by delegating to the legacy implementation
#     captured at end-of-file time.
#   - Final bindings will point to this wrapper (single edit point).
def _refactor09_diff_metrics_by_name(prev_response: dict, cur_response: dict):
    """Authoritative diff entrypoint (REFACTOR09).

    REFACTOR97 downsizing:
      - Remove legacy capture plumbing; call the canonical implementation directly.
    """
    try:
        fn = globals().get("_yureeka_diff_metrics_by_name_wrap1")
        if callable(fn):
            return fn(prev_response, cur_response)
    except Exception:
        pass

    # Safe empty result (signature-compatible)
    return ([], 0, 0, 0, 0)

def _refactor28_schema_only_rebuild_authoritative_v1(
    prev_response: dict,
    baseline_sources_cache: list,
    web_context: dict = None
) -> dict:
    """Schema-driven deterministic rebuild from cached snapshots only.

    This is intentionally minimal:
      - It does NOT attempt free-form metric discovery.
      - It ONLY populates metrics declared in the frozen schema.
      - Candidate selection is driven by schema fields (keywords + unit family/tag).
      - Deterministic sorting ensures stable output ordering.

    Returns:
      Dict[str, Dict] shaped like primary_metrics_canonical.
    """
    import re

    # Why:
    #   - When anchors are not used (anchor_used:false), schema-only rebuild can still
    #     select unit-less year tokens (e.g., 2024/2025) for currency/percent metrics.
    #   - This patch hard-rejects candidates with no token-level unit evidence when
    #     the schema (or canonical_key suffix) implies a unit is required.
    #   - Also optionally emits compact debug metadata for top candidates/rejections.
    # Determinism:
    #   - Pure filtering + stable ordering; no refetch; no randomness.

    def _fix33_schema_unit_required(spec_unit_family: str, spec_unit_tag: str, canonical_key: str) -> bool:
        uf = str(spec_unit_family or "").strip().lower()
        ut = str(spec_unit_tag or "").strip().lower()
        ck = str(canonical_key or "").strip().lower()
        # Explicit unit families
        if uf in {"currency", "percent", "rate", "ratio"}:
            return True
        if ut in {"%", "percent"}:
            return True
        # Unit-sales metrics require a unit (prevents bare-year selection)
        if ck.endswith("__unit_sales") or ck.endswith("__units") or ck.endswith("__unit"):
            return True
        # Canonical-key suffix conventions (backstop)
        if ck.endswith("__currency") or ck.endswith("__percent") or ck.endswith("__rate") or ck.endswith("__ratio"):
            return True
        return False

    def _fix33_candidate_has_unit_evidence(c: dict) -> bool:
        if not isinstance(c, dict):
            return False
        # Any explicit unit/currency/% evidence is enough to qualify as "has unit".
        if str(c.get("unit_tag") or "").strip():
            return True
        if str(c.get("unit_family") or "").strip():
            return True
        if str(c.get("base_unit") or "").strip():
            return True
        if str(c.get("unit") or "").strip():
            return True
        if str(c.get("currency_symbol") or c.get("currency") or "").strip():
            return True
        if bool(c.get("is_percent") or c.get("has_percent")):
            return True
        mk = str(c.get("measure_kind") or "").strip().lower()
        if mk in {"money", "percent", "percentage", "rate", "ratio"}:
            return True
        toks = c.get("unit_tokens") or c.get("unit_evidence_tokens") or []
        if isinstance(toks, (list, tuple)) and len(toks) > 0:
            return True
        return False

    _fix33_dbg = False
    try:
        _fix33_dbg = bool((web_context or {}).get("debug_evolution") or ((prev_response or {}).get("debug") or {}).get("debug_evolution"))
    except Exception:
        pass
        _fix33_dbg = False


    # Resolve frozen schema (supports multiple storage locations)
    schema = None
    try:
        if isinstance(prev_response, dict):
            if isinstance(prev_response.get("metric_schema_frozen"), dict):
                schema = prev_response.get("metric_schema_frozen")
            elif isinstance(prev_response.get("primary_response"), dict) and isinstance(prev_response["primary_response"].get("metric_schema_frozen"), dict):
                schema = prev_response["primary_response"].get("metric_schema_frozen")
            elif isinstance(prev_response.get("results"), dict) and isinstance(prev_response["results"].get("metric_schema_frozen"), dict):
                schema = prev_response["results"].get("metric_schema_frozen")
    except Exception:
        pass
        schema = None

    if not isinstance(schema, dict) or not schema:
        return {}

    # Collect candidates from snapshots (no re-fetch)
    candidates = []
    if isinstance(baseline_sources_cache, list):
        for src in baseline_sources_cache:
            if not isinstance(src, dict):
                continue
            nums = src.get("extracted_numbers")
            if not isinstance(nums, list) or not nums:
                continue
            for n in nums:
                if not isinstance(n, dict):
                    continue
                # Filter junk deterministically (strict rebuild exclusion)
                if _candidate_disallowed_for_metric(n, None):
                    continue
                # Normalize a few fields to ensure stable downstream access
                c = dict(n)
                if not c.get("source_url"):
                    c["source_url"] = src.get("url", "") or src.get("source_url", "") or ""
                candidates.append(c)

    # Deterministic candidate ordering (no set/dict iteration surprises)
    def _cand_sort_key(c: dict):
        return (
            str(c.get("source_url") or ""),
            str(c.get("anchor_hash") or ""),
            int(c.get("start_idx") or 0),
            str(c.get("raw") or ""),
            str(c.get("unit_tag") or ""),
            str(c.get("unit_family") or ""),
            float(c.get("value_norm") or c.get("value") or 0.0),
        )

    candidates.sort(key=_cand_sort_key)
    # REFACTOR118: define local normalizer BEFORE using it for year-anchor indexing
    def _ref100_norm_text_v1(s: str) -> str:
        try:
            return re.sub(r"\s+", " ", (s or "").lower()).strip()
        except Exception:
            return (s or "").lower().strip()
    # REFACTOR100: Pre-index source snapshot text by URL for year-anchor scanning.
    _ref100_src_text_by_url = {}
    try:
        if isinstance(baseline_sources_cache, list):
            for _src in baseline_sources_cache:
                if not isinstance(_src, dict):
                    continue
                _u = str(_src.get("url") or _src.get("source_url") or "").strip()
                if not _u:
                    continue
                _t = _src.get("snapshot_text_excerpt") or _src.get("snapshot_text") or _src.get("clean_text") or _src.get("text") or ""
                if not isinstance(_t, str) or not _t.strip():
                    continue
                _tn = _ref100_norm_text_v1(_t)
                if len(_tn) > len(_ref100_src_text_by_url.get(_u, "")):
                    _ref100_src_text_by_url[_u] = _tn
    except Exception:
        _ref100_src_text_by_url = {}


    if not candidates:
        return {}

    # Deterministic schema-driven selection
    def _norm_text(s: str) -> str:
        return re.sub(r"\s+", " ", (s or "").lower()).strip()

    # REFACTOR100: Year-anchor gating for year-explicit canonical keys.
    #   - If canonical_key contains one or more year tokens (e.g., 2026, 2040, 2025),
    #     prefer candidates whose source text contains ALL required years.
    #   - If no strong (year-complete) candidate exists, fall back to the best weak
    #     candidate and mark used_fallback_weak=True for traceability.
    def _refactor100_required_year_tokens_from_key(canonical_key: str) -> list:
        try:
            ck = str(canonical_key or "")
        except Exception:
            ck = ""
        years = []
        try:
            for y in re.findall(r"(?<!\d)(19\d{2}|20\d{2}|21\d{2})(?!\d)", ck):
                if y and (y not in years):
                    years.append(y)
        except Exception:
            years = []
        out = []
        for y in years:
            try:
                iv = int(y)
                if 1900 <= iv <= 2100:
                    out.append(str(iv))
            except Exception:
                continue
        return out

    def _refactor100_year_anchor_scan(candidate: dict, required_year_tokens: list, ctx_norm: str, src_text_by_url: dict) -> tuple:
        """Return (found_years_list, year_ok_bool) for candidate against required years."""
        if not required_year_tokens:
            return ([], True)
        try:
            c = candidate if isinstance(candidate, dict) else {}
            surl = str(c.get("source_url") or c.get("url") or "").strip()
        except Exception:
            c = {}
            surl = ""
        try:
            raw_norm = _norm_text(str(c.get("raw") or c.get("display_value") or ""))
        except Exception:
            raw_norm = ""
        try:
            page_norm = src_text_by_url.get(surl, "") if isinstance(src_text_by_url, dict) else ""
        except Exception:
            page_norm = ""
        try:
            ctx2 = _norm_text(str(c.get("context") or ""))
        except Exception:
            ctx2 = ""
        hay = " ".join([str(ctx_norm or ""), str(ctx2 or ""), str(raw_norm or ""), str(page_norm or "")]).lower()
        found = []
        for y in required_year_tokens:
            try:
                if re.search(r"(?<!\d)" + re.escape(str(y)) + r"(?!\d)", hay):
                    if str(y) not in found:
                        found.append(str(y))
            except Exception:
                try:
                    if str(y) in hay and str(y) not in found:
                        found.append(str(y))
                except Exception:
                    pass
        year_ok = len(found) == len(required_year_tokens)
        return (found, bool(year_ok))

    out = {}

    for canonical_key in sorted(schema.keys()):
        spec = schema.get(canonical_key) or {}
        if not isinstance(spec, dict):
            continue

        spec_keywords = spec.get("keywords") or []
        if not isinstance(spec_keywords, list):
            spec_keywords = []
        spec_keywords_norm = [str(k).lower().strip() for k in spec_keywords if str(k).strip()]
        # REFACTOR118: investment-by-2040 sources often phrase this as spend/spending/capex; bridge keywords ONLY for this schema key
        if canonical_key == "global_ev_charging_investment_2040__currency":
            _rf118_extra = ["investment", "investments", "spend", "spending", "expenditure", "capex", "total spend", "annual spend", "market size"]
            for _k in _rf118_extra:
                _kn = str(_k).lower().strip()
                if _kn and _kn not in spec_keywords_norm:
                    spec_keywords_norm.append(_kn)

        spec_unit_tag = str(spec.get("unit_tag") or spec.get("unit") or "").strip()
        spec_unit_family = str(spec.get("unit_family") or "").strip()

        # Score candidates by schema keyword hits, then filter by unit constraints if present.

        # REFACTOR100: apply year-anchor gating for year-explicit canonical keys.

        best = None

        best_key = None

        best_strong = None

        best_strong_key = None

        best_strong_years = []

        best_weak = None

        best_weak_key = None

        best_weak_years = []

        _ref100_required_years = _refactor100_required_year_tokens_from_key(canonical_key)

        _ref100_top3_pairs = []  # (tie, summary_dict)

        _fix33_top = []
        _fix33_rej = {}

        for c in candidates:
            if _candidate_disallowed_for_metric(c, spec):
                continue
            if _refactor03_candidate_rejected_by_unit_family_v1(c, spec):
                continue
            try:
                if _refactor27_candidate_rejected_currency_date_fragment_v1(c, spec):
                    continue
            except Exception:
                pass

            # Why:
            # - Some sources contain many years (e.g., 2023, 2024) that can outscore true values.
            # - For currency-ish metrics, suppress candidates that look like bare years unless context clearly indicates money.
            # Determinism:
            # - Pure filter; does not invent candidates or refetch content.
            try:
                def _ai2_is_year_only(c: dict):
                    """Return True if candidate is a likely standalone year (1900-2100) with no unit."""
                    try:
                        c = c if isinstance(c, dict) else {}
                        # Prefer canonical numeric
                        v = c.get("value_norm")
                        if v is None:
                            v = c.get("value")
                        try:
                            iv = int(float(v))
                        except Exception:
                            return False
                        if iv < 1900 or iv > 2100:
                            return False
                        # Must be truly 4-digit (avoid 2023.5 etc)
                        try:
                            if abs(float(v) - float(iv)) > 1e-9:
                                return False
                        except Exception:
                            pass

                        # If the candidate itself signals time/year, do not treat as "junk year".
                        u = str(c.get("base_unit") or c.get("unit") or "").strip().lower()
                        ut = str(c.get("unit_tag") or "").strip().lower()
                        uf = str(c.get("unit_family") or "").strip().lower()
                        if "year" in u or "year" in ut or "year" in uf or "time" in uf:
                            return False

                        raw = str(c.get("raw") or "").strip()
                        sval = str(iv)

                        # - Some extractors store a wider raw window (e.g. includes '$721m ... in 2023')
                        # - Currency symbols elsewhere in raw should NOT make a year candidate non-year.
                        # - Only treat as non-year if the currency symbol is directly attached to the year.
                        try:
                            if re.search(r"(\$|usd|eur|gbp|aud|cad|sgd)\s*"+re.escape(sval)+r"\b", raw.lower()):
                                return False
                        except Exception:
                            pass

                        # If raw is basically just the year token (allow brackets/punctuation), it's year-only.
                        try:
                            raw2 = re.sub(r"[\s,.;:()\[\]{}<>]", "", raw)
                            if raw2 == sval:
                                return True
                        except Exception:
                            pass

                        # If raw contains multiple numbers, it's likely context; still treat as year-only
                        # when this candidate has no unit.
                        try:
                            nums = re.findall(r"\d{2,}", raw)
                            if len(nums) >= 2:
                                return True
                        except Exception:
                            pass

                        # If raw contains month names, likely a date; treat as year-only (we suppress dates too).
                        try:
                            if re.search(r"\b(jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)\b", raw.lower()):
                                return True
                        except Exception:
                            return True
                    except Exception:
                        return False
                def _ai2_schema_currencyish(sd: dict) -> bool:
                    try:
                        if not isinstance(sd, dict):
                            return False
                        u = str(sd.get('unit') or sd.get('base_unit') or '').lower()
                        if any(x in u for x in ('usd','sgd','eur','gbp','jpy','$','€','£')):
                            return True
                        # heuristic keywords on definition (safe, schema-driven-ish)
                        nm = str(sd.get('name') or '').lower()
                        if any(x in nm for x in ('revenue','sales','cost','price','capex','opex','investment','spend','spending','expenditure','value')):
                            return True
                        return False
                    except Exception:
                        return False

                _sd = locals().get('schema_def')
                if _ai2_schema_currencyish(_sd) and _ai2_is_year_only(c):
                    continue

                # Why: year tokens (e.g., 2025) can outrank true percent values when unit evidence is weak.
                # Safe: only suppress when candidate has no explicit unit and looks like a bare year.
                try:
                    if _ai2_is_year_only(c):
                        _sd_name = str((_sd or {}).get('name') or '').lower()
                        _sd_ckey = str((_sd or {}).get('canonical_key') or ckey or '').lower()
                        _sd_unit_tag = str((_sd or {}).get('unit_tag') or '').lower()
                        _sd_unit_family = str((_sd or {}).get('unit_family') or '').lower()
                        if ('cagr' in _sd_name) or ('cagr' in _sd_ckey) or (_sd_unit_tag in ('percent','pct')) or (_sd_unit_family in ('percent','ratio','rate')):
                            continue
                except Exception:
                    pass
            except Exception:
                pass

            ctx = _norm_text(c.get("context_snippet") or c.get("context") or "")
            if not ctx:
                continue

            # Keyword hits: schema-driven (no external heuristics)
            hits = 0
            if spec_keywords_norm:
                for kw in spec_keywords_norm:
                    if kw and kw in ctx:
                        hits += 1

            if spec_keywords_norm and hits == 0:
                continue

            # Unit constraints (only if schema declares them)
            if spec_unit_family:
                if str(c.get("unit_family") or "").strip() != spec_unit_family:
                    # allow a unit_tag-only match when family is missing in candidate
                    if not (spec_unit_tag and str(c.get("unit_tag") or "").strip() == spec_unit_tag):
                        continue

            if spec_unit_tag:
                # if a tag is specified, prefer exact tag matches
                if str(c.get("unit_tag") or "").strip() != spec_unit_tag:
                    # allow family match when tag differs
                    if not (spec_unit_family and str(c.get("unit_family") or "").strip() == spec_unit_family):
                        continue

            try:
                _vnorm = c.get("value_norm", None)
                if _vnorm is None:
                    _vnorm = c.get("value", None)
                _is_year = _is_yearish_value(_vnorm)
                _mk0 = str(c.get("measure_kind") or "").strip().lower()
                _cand_ut0 = str(c.get("unit_tag") or "").strip()
                _cand_fam0 = str(c.get("unit_family") or "").strip().lower()
                _is_pct0 = bool(c.get("is_percent") or c.get("has_percent") or (_cand_ut0 == "%") or (_cand_fam0 == "percent"))
                _has_curr0 = bool(str(c.get("currency_symbol") or c.get("currency") or "").strip())
                _has_unit_ev0 = bool(_cand_ut0 or _cand_fam0 or _is_pct0 or _has_curr0 or str(c.get("base_unit") or c.get("unit") or "").strip())
                if _is_year and (not _has_unit_ev0) and (not _is_pct0) and (not _has_curr0) and (_mk0 in ("magnitude_other", "count_units", "count", "number", "")):
                    try:
                        _fix41afc5_dbg2["rejected_year_only"] = int(_fix41afc5_dbg2.get("rejected_year_only", 0) or 0) + 1
                    except Exception:
                        pass
                    continue
            except Exception:
                pass

            try:
                _req = _fix33_schema_unit_required(spec_unit_family, spec_unit_tag, canonical_key)
                _has_unit_ev = _fix33_candidate_has_unit_evidence(c)
                # unit_sales keys represent quantities; they must never take a bare year token as the value.
                try:
                    if str(canonical_key or '').strip().lower().endswith('__unit_sales'):
                        _v = c.get('value_norm', None)
                        if _v is None:
                            _v = c.get('value', None)
                        if _is_yearish_value(_v):
                            if _fix33_dbg:
                                try:
                                    _fix33_rej['rejected_year_for_unit_sales'] = int(_fix33_rej.get('rejected_year_for_unit_sales', 0) or 0) + 1
                                except Exception:
                                    pass
                            continue
                except Exception:
                    pass

                if _req and not _has_unit_ev:
                    # Track rejection (debug)
                    if _fix33_dbg:
                        try:
                            _fix33_rej["missing_unit_required"] = int(_fix33_rej.get("missing_unit_required", 0) or 0) + 1
                        except Exception:
                            pass
                    continue


                # Track top candidates (debug)
                if _fix33_dbg:
                    try:
                        _fix33_top.append({
                            "raw": c.get("raw"),
                            "value_norm": c.get("value_norm"),
                            "unit_tag": c.get("unit_tag"),
                            "unit_family": c.get("unit_family"),
                            "base_unit": c.get("base_unit") or c.get("unit"),
                            "measure_kind": c.get("measure_kind"),
                            "hits": hits,
                            "has_unit_ev": bool(_has_unit_ev),
                            "source_url": c.get("source_url"),
                            "anchor_hash": c.get("anchor_hash"),
                        })
                    except Exception:
                        pass
            except Exception:
                pass

            # Deterministic tie-break:

            #   (-hits, then stable candidate identity tuple)

            tie = (-hits,) + _cand_sort_key(c)


            # REFACTOR100: compute year_ok/year_found early (safe ordering)

            year_found, year_ok = _refactor100_year_anchor_scan(c, _ref100_required_years, ctx, _ref100_src_text_by_url)


            # Track top-3 candidates summary for year-anchored keys only (compact, deterministic)

            if _ref100_required_years:

                try:

                    _sum = {

                        "url": c.get("source_url"),

                        "score": int(hits),

                        "found_years": list(year_found),

                        "year_ok": bool(year_ok),

                    }

                    _u = str(_sum.get("url") or "").strip()
                    _replaced = False
                    if _u:
                        for _i, (_t, _s) in enumerate(list(_ref100_top3_pairs)):
                            try:
                                if str((_s or {}).get("url") or "").strip() == _u:
                                    # Keep the better (lower) tie for the same URL
                                    if tie < _t:
                                        _ref100_top3_pairs[_i] = (tie, _sum)
                                    _replaced = True
                                    break
                            except Exception:
                                pass
                    if not _replaced:
                        _ref100_top3_pairs.append((tie, _sum))

                    _ref100_top3_pairs.sort(key=lambda p: p[0])

                    if len(_ref100_top3_pairs) > 3:

                        _ref100_top3_pairs = _ref100_top3_pairs[:3]

                except Exception:

                    pass


            if _ref100_required_years:

                if year_ok:

                    if best_strong is None or tie < best_strong_key:

                        best_strong = c

                        best_strong_key = tie

                        best_strong_years = list(year_found)

                else:

                    if best_weak is None or tie < best_weak_key:

                        best_weak = c

                        best_weak_key = tie

                        best_weak_years = list(year_found)

            else:

                if best is None or tie < best_key:

                    best = c

                    best_key = tie

        # REFACTOR113: hard year-anchor enforcement for year-stamped schema keys (no weak fallback).

        _ref100_used_fallback_weak = False

        _ref100_winner_years = []

        _ref100_winner_has_all_years = None

        _ref113_missing_reason = ""

        if _ref100_required_years:

            if isinstance(best_strong, dict):

                best = best_strong

                _ref100_winner_years = list(best_strong_years or [])

                _ref100_winner_has_all_years = True

            elif isinstance(best_weak, dict):

                # Weak winner exists but lacks required year tokens -> treat as missing.
                best = None

                _ref100_winner_years = list(best_weak_years or [])

                _ref100_winner_has_all_years = False

                _ref113_missing_reason = "year_anchor_missing_required_year_tokens"

            else:

                best = None

                _ref100_winner_has_all_years = False

                _ref113_missing_reason = "year_anchor_no_candidate"


        # REFACTOR113: For year-anchored schema keys, emit a placeholder row (value=None)
        # instead of incorrectly binding a weak candidate or dropping the row.
        if _ref100_required_years and not isinstance(best, dict):

            best = {}


        if not isinstance(best, dict):

            continue

        # Emit a minimal canonical metric row (schema-driven, deterministic)
        metric = {
            "name": spec.get("name") or spec.get("canonical_id") or canonical_key,
            "value": best.get("value"),
            "unit": best.get("unit") or spec.get("unit") or "",
            "unit_tag": best.get("unit_tag") or spec.get("unit_tag") or "",
            "unit_family": best.get("unit_family") or spec.get("unit_family") or "",
            "currency_code": best.get("currency_code") or "",
            "base_unit": best.get("base_unit") or best.get("unit_tag") or spec.get("unit_tag") or "",
            "multiplier_to_base": best.get("multiplier_to_base") if best.get("multiplier_to_base") is not None else 1.0,
            "value_norm": best.get("value_norm") if best.get("value_norm") is not None else best.get("value"),
            "canonical_id": spec.get("canonical_id") or spec.get("canonical_key") or canonical_key,
            "canonical_key": canonical_key,
            "dimension": spec.get("dimension") or "",
            "original_name": spec.get("name") or "",
            "geo_scope": "unknown",
            "geo_name": "",
            "is_proxy": False,
            "proxy_type": "",
            "provenance": {
                "method": "schema_keyword_match",
                "best_candidate": {
                    "raw": best.get("raw"),
                    "source_url": best.get("source_url"),
                    "context_snippet": best.get("context_snippet"),
                    "anchor_hash": best.get("anchor_hash"),
                    "start_idx": best.get("start_idx"),
                    "end_idx": best.get("end_idx"),
                },
            },
        }

        # REFACTOR100: attach year-anchor selection provenance for traceability (only when applicable)
        try:
            if _ref100_required_years and isinstance(metric, dict):
                metric.setdefault("provenance", {})
                try:
                    top3 = [p[1] for p in (_ref100_top3_pairs or [])]
                except Exception:
                    top3 = []
                metric["provenance"]["selection_year_anchor_v1"] = {
                    "canonical_key": canonical_key,
                    "required_year_tokens": list(_ref100_required_years or []),
                    "winner_has_all_years": bool(_ref100_winner_has_all_years),
                    "winner_found_years": list(_ref100_winner_years or []),
                    "used_fallback_weak": bool(_ref100_used_fallback_weak),
                    "top3": list(top3 or []),
                }
        except Exception:
            pass



        # REFACTOR113: explicit missing reason when year-anchor gating blocks binding
        try:
            if _ref100_required_years and isinstance(metric, dict) and _ref113_missing_reason:
                metric.setdefault("provenance", {})
                metric["provenance"]["missing_reason_v1"] = str(_ref113_missing_reason)
        except Exception:
            pass

        try:
            if _fix33_dbg and isinstance(metric, dict):
                try:
                    _fix33_top_sorted = sorted(
                        _fix33_top,
                        key=lambda d: (-(int(d.get("hits") or 0)), str(d.get("value_norm") or ""), str(d.get("raw") or "")),
                    )
                except Exception:
                    pass
                    _fix33_top_sorted = _fix33_top
                metric.setdefault("provenance", {})
                metric["provenance"]["fix33_top_candidates"] = list(_fix33_top_sorted[:10])
                metric["provenance"]["fix33_rejected_reason_counts"] = dict(_fix33_rej or {})
        except Exception:
            pass
        out[canonical_key] = metric

    return out


# REFACTOR28: define the authoritative FIX16 schema-only wrapper directly on top of the authoritative base.
try:
    _refactor28__schema_only_wrapped = globals().get("_refactor28__schema_only_wrapped", False)
except Exception:
    _refactor28__schema_only_wrapped = False

if not _refactor28__schema_only_wrapped:
    def rebuild_metrics_from_snapshots_schema_only_fix16(prev_response, snapshot_pool, web_context=None):  # noqa: F811
        """Authoritative schema-only rebuild wrapper (REFACTOR28).

        Contract:
          - Deterministic selection from baseline snapshots (no re-fetch).
          - Preserves percent-year poisoning sanitization for percent keys.
          - Ensures currency date-fragment candidates (e.g., 'July 01, 2025') are not eligible.
        """
        rebuilt = {}
        try:
            rebuilt = _refactor28_schema_only_rebuild_authoritative_v1(prev_response, snapshot_pool, web_context=web_context)
        except Exception:
            rebuilt = {}

        # Preserve FIX2D86 sanitization: percent keys must not bind to bare year tokens.
        try:
            schema = {}
            if isinstance(prev_response, dict):
                schema = prev_response.get("metric_schema_frozen") or (prev_response.get("results") or {}).get("metric_schema_frozen") or {}
            if isinstance(rebuilt, dict) and rebuilt:
                rebuilt2, _sdbg = _fix2d86_sanitize_pmc_percent_year_tokens_v1(
                    pmc=rebuilt,
                    metric_schema_frozen=schema if isinstance(schema, dict) else {},
                    label="schema_only_rebuild_refactor28_final",
                )
                rebuilt = rebuilt2
        except Exception:
            pass

        return rebuilt

    try:
        rebuild_metrics_from_snapshots_schema_only_fix16._fix2d86_wrapped = True  # type: ignore[attr-defined]
        rebuild_metrics_from_snapshots_schema_only_fix16._refactor28_authoritative = True  # type: ignore[attr-defined]
    except Exception:
        pass

    # Rebind canonical symbol names to the authoritative wrapper.
    try:
        globals()["rebuild_metrics_from_snapshots_schema_only_fix16"] = rebuild_metrics_from_snapshots_schema_only_fix16
    except Exception:
        pass
    try:
        globals()["rebuild_metrics_from_snapshots_schema_only"] = rebuild_metrics_from_snapshots_schema_only_fix16
    except Exception:
        pass

    try:
        globals()["_refactor28__schema_only_wrapped"] = True
    except Exception:
        pass

# REFACTOR37 patch tracker
# REFACTOR38 patch tracker
# - We intentionally call main() only after ALL patch blocks and helper defs have executed,
#   so late overrides (diff engine, schema-only rebuild, etc.) are active during runs.
# REFACTOR37: Crash-proof wrapper for run_source_anchored_evolution()
#
# Context:
# - This codebase historically had multiple definitions of run_source_anchored_evolution
#   (FIX24 wrapper + earlier entrypoints).
# - A deep NoneType.get() crash inside the FIX24 wrapper can bubble up to the Streamlit UI
#   and abort the run.
#
# Behavior:
# - Preserve the existing implementation as _REFACTOR37_RUN_SOURCE_ANCHORED_EVOLUTION_IMPL
#     * coerces inputs to dict
#     * catches all exceptions and returns a renderer-safe failed payload with traceback
try:
    _REFACTOR37_RUN_SOURCE_ANCHORED_EVOLUTION_IMPL = run_source_anchored_evolution
except Exception:
    _REFACTOR37_RUN_SOURCE_ANCHORED_EVOLUTION_IMPL = None


#
# Motivation:
# - Some late patch blocks may overwrite results['baseline_sources_cache'] with an injected-only
#   placeholder row (while results['baseline_sources_cache_current'] still contains the full
#   current pool). This can confuse UI diagnostics and harness messaging.
#
# Behavior:
# - If baseline_sources_cache_current (or source_results) is a list with more rows than
#   baseline_sources_cache, widen baseline_sources_cache to match the full current pool.
# - Debug-only stamp under results.debug.refactor83_source_cache_normalize_v1 when applied.
#
# Safety:
# - Does NOT affect metric selection/diffing/stability. Payload-only normalization.
# - Never raises.
def _refactor83_normalize_evolution_source_caches_v1(payload: dict) -> dict:
    if not isinstance(payload, dict):
        return payload

    # REFACTOR85: Some evolution payloads have a nested "results" mirror dict
    # (e.g., res["results"]={"code_version", ...}) while the real results envelope
    # lives at the top-level of this dict. Prefer the dict that actually contains
    # evolution-run fields like status/metric_changes/source caches.
    _inner = payload.get("results") if isinstance(payload.get("results"), dict) else None
    _payload_looks_like_results = bool(
        isinstance(payload.get("status"), str)
        or isinstance(payload.get("metric_changes"), list)
        or isinstance(payload.get("baseline_sources_cache_current"), list)
        or isinstance(payload.get("source_results"), list)
    )
    if _payload_looks_like_results:
        res = payload
    elif isinstance(_inner, dict):
        res = _inner
    else:
        res = payload
    if not isinstance(res, dict):
        return payload

    cur = res.get("baseline_sources_cache_current")
    if not isinstance(cur, list) or not cur:
        cur = res.get("source_results")
    base = res.get("baseline_sources_cache")

    if isinstance(cur, list):
        base_list = base if isinstance(base, list) else []
        if len(cur) > len(base_list):
            try:
                res["baseline_sources_cache"] = cur
            except Exception:
                pass
            try:
                dbg = res.setdefault("debug", {})
                if isinstance(dbg, dict):
                    d = dbg.setdefault("refactor83_source_cache_normalize_v1", {})
                    if isinstance(d, dict):
                        d.update({
                            "applied": True,
                            "before_baseline_sources_cache_rows": int(len(base_list)),
                            "after_baseline_sources_cache_rows": int(len(cur)),
                            "reason": "widen_baseline_sources_cache_to_match_current_pool",
                        })
            except Exception:
                pass

    # REFACTOR113: normalize per-source status when status_detail indicates success
    try:
        _keys = ["baseline_sources_cache_current", "source_results", "baseline_sources_cache"]
        _touched = 0
        _changed = 0
        for _k in _keys:
            rows = res.get(_k)
            if not isinstance(rows, list):
                continue
            for r in rows:
                if not isinstance(r, dict):
                    continue
                _touched += 1
                sd = str(r.get("status_detail") or r.get("detail") or "").strip().lower()
                st = str(r.get("status") or "").strip().lower()
                if sd == "success" and st in ("failed", "error", ""):
                    try:
                        r["status"] = "success"
                        _changed += 1
                    except Exception:
                        pass
        if _changed:
            try:
                res.setdefault("debug", {})
                if isinstance(res.get("debug"), dict):
                    res["debug"].setdefault("refactor83_status_normalize_v1", {})
                    if isinstance(res["debug"].get("refactor83_status_normalize_v1"), dict):
                        res["debug"]["refactor83_status_normalize_v1"].update({
                            "applied": True,
                            "touched_rows": int(_touched),
                            "changed_rows": int(_changed),
                        })
            except Exception:
                pass
    except Exception:
        pass

    return payload


def run_source_anchored_evolution(previous_data: dict, web_context: dict = None) -> dict:
    # Input coercion (avoid None.get)
    if not isinstance(previous_data, dict):
        previous_data = {}
    if web_context is None or not isinstance(web_context, dict):
        web_context = {}

    # =========================
    # REFACTOR111: deterministically override stale baseline_data with latest Analysis snapshot
    # =========================
    _ref111_prev_pick_dbg = {}
    try:
        _picker = globals().get("_refactor111_pick_latest_prev_snapshot_v1")
        if callable(_picker):
            _sel, _ref111_prev_pick_dbg = _picker(previous_data or {}, web_context=web_context)
            if isinstance(_sel, dict) and _sel:
                previous_data = _sel
    except Exception:
        pass

    def _fail(msg: str, tb: str = "") -> dict:
        out = {
            "status": "failed",
            "message": msg,
            "sources_checked": 0,
            "sources_fetched": 0,
            "numbers_extracted_total": 0,
            "stability_score": 0.0,
            "summary": {
                "total_metrics": 0,
                "metrics_found": 0,
                "metrics_increased": 0,
                "metrics_decreased": 0,
                "metrics_unchanged": 0,
            },
            "metric_changes": [],
            "source_results": [],
            "interpretation": "Evolution failed.",
            "code_version": str(globals().get("_YUREEKA_CODE_VERSION_LOCK") or ""),
            "debug": {
                "refactor37": {
                    "error": msg,
                }
            },
        }

        # REFACTOR111: always attach prev snapshot pick debug (if available)
        try:
            if isinstance(out.get("debug"), dict):
                out["debug"]["prev_snapshot_pick_v1"] = _ref111_prev_pick_dbg
        except Exception:
            pass

        if tb:
            try:
                out["debug"]["refactor37"]["traceback"] = tb
            except Exception:
                pass
            try:
                _cs = ""
                for _ln in str(tb).splitlines():
                    _lns = _ln.strip()
                    if _lns.startswith("File "):
                        _cs = _lns
                if _cs:
                    try:
                        out["debug"]["refactor37"]["callsite"] = _cs
                    except Exception:
                        pass
                    try:
                        out["message"] = f"{msg} | {_cs}"
                    except Exception:
                        pass
            except Exception:
                pass
        return out

    impl = _REFACTOR37_RUN_SOURCE_ANCHORED_EVOLUTION_IMPL
    if not callable(impl):
        return _fail("run_source_anchored_evolution implementation is not callable.")

    try:
        _res = impl(previous_data, web_context=web_context)
        if not isinstance(_res, dict):
            import traceback as _tb
            return _fail("run_source_anchored_evolution returned non-dict result (impl)", tb=_tb.format_stack())
        try:
            _res = _refactor83_normalize_evolution_source_caches_v1(_res)
        except Exception:
            pass

        # REFACTOR111: attach prev snapshot pick debug to successful output
        try:
            _res.setdefault("debug", {})
            if isinstance(_res.get("debug"), dict):
                _res["debug"]["prev_snapshot_pick_v1"] = _ref111_prev_pick_dbg
        except Exception:
            pass

        return _res
    except TypeError:
        # Backward-compat: some historical defs accept only previous_data
        try:
            _res = impl(previous_data)
            if not isinstance(_res, dict):
                import traceback as _tb
                return _fail("run_source_anchored_evolution returned non-dict result (impl fallback)", tb=_tb.format_stack())
            try:
                _res = _refactor83_normalize_evolution_source_caches_v1(_res)
            except Exception:
                pass

            # REFACTOR111: attach prev snapshot pick debug to successful output (fallback path)
            try:
                _res.setdefault("debug", {})
                if isinstance(_res.get("debug"), dict):
                    _res["debug"]["prev_snapshot_pick_v1"] = _ref111_prev_pick_dbg
            except Exception:
                pass

            return _res
        except Exception as e:
            import traceback as _tb
            return _fail(f"run_source_anchored_evolution crashed: {e}", tb=_tb.format_exc())
    except Exception as e:
        import traceback as _tb
        return _fail(f"run_source_anchored_evolution crashed: {e}", tb=_tb.format_exc())

# Why:
# - Streamlit can execute main() before later end-of-file patch-tracker "ADD" blocks run.
# - This block registers the current patch *before* main() executes, so harness/version checks see an up-to-date tracker.
try:
    if __name__ == "__main__":
        if not bool(globals().get("_REFACTOR01_HARNESS_REQUESTED")):
            main()
except Exception:
    try:
        import streamlit as st
        st.exception(Exception(f"Yureeka app crashed during main() execution ({_yureeka_get_code_version()})."))
    except Exception:
        pass

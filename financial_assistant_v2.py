# ===============================================================================
# YUREEKA AI RESEARCH ASSISTANT v7.41
# With Web Search, Evidence-Based Verification, Confidence Scoring
# SerpAPI Output with Evolution Layer Version
# Updated SerpAPI parameters for stable output
# Deterministic Output From LLM
# Deterministic Evolution Core Using Python Diff Engine
# Anchored Evolution Analysis Using JSON As Input Into Model
# Implementation of Source-Based Evolution
# Saving of JSON output Files into Google Sheets
# Canonical Metric Registry + Semantic Hashing of Findings
# Removal of Evolution Decisions from LLM
# Further Enhancements to Minimize Evolution Drift (Metric)
# Saving of Extraction Cache in JSON
# Prioritize High Quality Sources With Source Freshness Tracking
# Timestamps = Timezone Naive
# Improved Stability of Handling of Duplicate Canonicalized IDs
# Deterministic Main and Side Topic Extractor
# Range Aware Canonical Metrics
# Range + Source Attribution
# Proxy Labeler + Geo Tagging
# Improved Main Topic + Side Topic Extractor Using Deterministic-->NLP-->LLM layer
# Guardrails For Main + Side Topic Handling
# Numeric Consistency Scores
# Multi-Side Enumerations
# Dashboard Unit Presentation Fixes (Main + Evolution)
# Domain-Agnostic Question Profiling
# Baseline Caching Contains HTTP Validators + Numeric Data
# URL canonicalization
# Evolution Layer Leverage On New Analysis Pipeline to Minimise Volatility
# Canonicalization of Evolution Layer Metrics To Match Analysis Layer
# Fix URL/path Collapese Issue Causing + Tighten Evolution Extraction (Topic Gating)
# canonical-key-first matching
# Evolution Pipeline to Consume analysis upstream artifacts
# safety-net hard gates (minimal) before matching
# Tighten canonical identity + unit-family constraints
# Fingerprint freshness gating to evolution
# Fix SerpAPI access and fetching
# Keeps your snapshot-friendly scraped_meta (with extracted numbers + fingerprint fields)
# Safe fallback scraper when ScrapingDog is unavailable
# Prevent caching “empty results” from SerpAPI (no poisoned cache)
# Restoration of Range Estimates For Metrics
# Improved Junk Tagging and Rejection
# One Canononical Operator for Analysis + Evolution Layers
# Metric Aware Range Construction Everywhere
# Anchor Matching Correctness
# Unit Measure + Attribute Association e.g. M + units (sold)
# Enriched metric_schema_frozen (analysis side)
# THIS VERSION HAS THE PLUMBING LOCKED-DOWN
# ONLY THE METRIC EXTRACTION LAYER FOR EVOLUTION REQUIRES WORK
# ================================================================================

import io
import os
import re
import json
import requests
import pandas as pd
import plotly.express as px
import streamlit as st


# PATCH FIX2D64 (SHADOW MODE): Canonical Identity Spine module import (no behavior change).
# Enabled only if caller sets web_context["enable_spine_shadow_fix2d64"]=True or env ENABLE_SPINE_SHADOW_FIX2D64=1.
try:
    import canonical_identity_spine as _canonical_identity_spine_fix2d64
except Exception:
    pass
    _canonical_identity_spine_fix2d64 = None
import base64
import hashlib
import numpy as np
import difflib
import gspread
from pypdf import PdfReader
from pathlib import Path
from google.oauth2.service_account import Credentials
from dataclasses import dataclass
from typing import Dict, List, Optional, Any, Union, Tuple
from sentence_transformers import SentenceTransformer, util
from transformers import pipeline
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup
from collections import Counter
from pydantic import BaseModel, Field, ValidationError, ConfigDict

# =========================
# VERSION STAMP (LOCKED)
# =========================
# REFACTOR12: single-source-of-truth version lock.
# - All JSON outputs must stamp using _yureeka_get_code_version().
# - The getter is intentionally "frozen" via a default arg to prevent late overrides.
_YUREEKA_CODE_VERSION_LOCK = 'REFACTOR57'
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK

def _yureeka_get_code_version(_lock=_YUREEKA_CODE_VERSION_LOCK):
    try:
        return str(_lock)
    except Exception:
        return "UNKNOWN"





def _yureeka_authority_manifest_v1() -> dict:
    """Additive debug helper: capture which 'last-wins' definitions are actually active at runtime.

    This is used during the downsizing phase to ensure deletions don't accidentally swap authority.
    Streamlit-safe: pure introspection (no IO/network).
    """
    def _fn_meta(name: str):
        try:
            fn = globals().get(name)
            if fn is None:
                return {"present": False}
            meta = {"present": True, "type": str(type(fn))}
            if callable(fn):
                try:
                    meta.update({
                        "name": str(getattr(fn, "__name__", "") or ""),
                        "qualname": str(getattr(fn, "__qualname__", "") or ""),
                        "module": str(getattr(fn, "__module__", "") or ""),
                        "id": str(id(fn)),
                    })
                except Exception:
                    pass
                try:
                    c = getattr(fn, "__code__", None)
                    if c is not None:
                        meta["firstlineno"] = int(getattr(c, "co_firstlineno", -1) or -1)
                        meta["filename"] = str(getattr(c, "co_filename", "") or "")
                except Exception:
                    pass
            return meta
        except Exception as e:
            return {"present": False, "error": f"{e}"}

    try:
        keys = [
            # evolution / snapshots
            "run_source_anchored_evolution",
            "compute_source_anchored_diff",
            "attach_source_snapshots_to_analysis",
            # diff panel + join engine
            "build_diff_metrics_panel_v2__rows_fix2d2i",
            "build_diff_metrics_panel_v2__rows",
            "build_diff_metrics_panel_v2",
            "diff_metrics_by_name",
            "_refactor09_diff_metrics_by_name",
            "metric_changes_v2",
        ]
    except Exception:
        keys = []

    out = {"code_version": str(globals().get("_YUREEKA_CODE_VERSION_LOCK") or ""), "targets": {}}
    for k in keys:
        out["targets"][k] = _fn_meta(k)
    return out
def _yureeka_runtime_identity_v1():
    """Additive debug helper: identify the running script reliably (helps diagnose stale-version runs)."""
    try:
        import os, sys, platform, hashlib, datetime
        out = {
            "code_version": _yureeka_get_code_version(),
            "authority_manifest_v1": _yureeka_authority_manifest_v1(),
            "code_version_lock": globals().get("_YUREEKA_CODE_VERSION_LOCK"),
        }
        try:
            out["__file__"] = __file__
        except Exception:
            out["__file__"] = None
        try:
            out["cwd"] = os.getcwd()
        except Exception:
            out["cwd"] = None
        try:
            out["pid"] = os.getpid()
        except Exception:
            out["pid"] = None
        try:
            out["python"] = sys.version.split()[0]
        except Exception:
            out["python"] = None
        try:
            out["platform"] = platform.platform()
        except Exception:
            out["platform"] = None
        try:
            out["now_utc"] = datetime.datetime.now(datetime.timezone.utc).isoformat()
        except Exception:
            out["now_utc"] = None

        # File signature (best-effort; safe in Streamlit)
        try:
            p = out.get("__file__")
            if isinstance(p, str) and p and os.path.exists(p):
                with open(p, "rb") as f:
                    b = f.read()
                out["file_sha1_12"] = hashlib.sha1(b).hexdigest()[:12]
                out["file_bytes"] = int(len(b))
        except Exception:
            pass

        return out
    except Exception:
        return {"code_version": _yureeka_get_code_version(),
            "authority_manifest_v1": _yureeka_authority_manifest_v1(), "code_version_lock": globals().get("_YUREEKA_CODE_VERSION_LOCK")}


def _yureeka_lock_version_globals_v1():
    """Re-assert global version vars for observability (does not affect the frozen getter)."""
    try:
        v = _yureeka_get_code_version()
        globals()["_YUREEKA_CODE_VERSION_LOCK"] = v
        globals()["CODE_VERSION"] = v
    except Exception:
        pass


def _yureeka_set_authoritative_binding_v1(fn, tag: str) -> bool:
    """Best-effort: stamp the authoritative binding tag onto the callable.
    Falls back to globals if the callable doesn't support attribute assignment.
    """
    ok = False
    try:
        setattr(fn, "__YUREEKA_AUTHORITATIVE_BINDING__", str(tag))
        ok = True
    except Exception:
        ok = False
    try:
        globals()["_YUREEKA_DIFF_METRICS_BY_NAME_AUTHORITATIVE"] = fn
    except Exception:
        pass
    try:
        globals()["_YUREEKA_DIFF_METRICS_BY_NAME_AUTHORITATIVE_TAG"] = str(tag)
    except Exception:
        pass
    return bool(ok)

def _yureeka_get_authoritative_binding_tag_v1(fn) -> str:
    """Return the authoritative binding tag for the given callable (attr first, then globals fallback)."""
    try:
        v = getattr(fn, "__YUREEKA_AUTHORITATIVE_BINDING__", None)
        if v:
            return str(v)
    except Exception:
        pass
    try:
        v = globals().get("_YUREEKA_DIFF_METRICS_BY_NAME_AUTHORITATIVE_TAG")
        if v:
            return str(v)
    except Exception:
        pass
    return ""

def _yureeka_ensure_final_bindings_v1():
    """Ensure FINAL BINDINGS tags are always present and consistent with the version lock.

    REFACTOR18: make the diff binding authority signal *non-optional*:
      - Try to stamp __YUREEKA_AUTHORITATIVE_BINDING__ on the callable.
      - Always mirror the tag into globals as a fallback (some callable wrappers reject setattr).
    """
    try:
        v = _yureeka_get_code_version()
        globals()["_YUREEKA_FINAL_BINDINGS_VERSION"] = v
    except Exception:
        pass

    # Resolve active diff entrypoint (best-effort)
    fn = None
    bound_from = ""
    try:
        fn = globals().get("diff_metrics_by_name")
    except Exception:
        fn = None

    if callable(fn):
        bound_from = "diff_metrics_by_name"
    else:
        for _cand_name in [
            "_yureeka_diff_metrics_by_name_v24",
            "diff_metrics_by_name_V24_BASE",
            "diff_metrics_by_name",
            "_refactor09_diff_metrics_by_name",
            "diff_metrics_by_name_FIX41_V34C_UNWRAP",
            "diff_metrics_by_name_FIX41_V34_ANCHOR_JOIN",
            "diff_metrics_by_name_FIX40_V32_PREFER_PMC",
            "diff_metrics_by_name_FIX34_V24_STRICT",
            "diff_metrics_by_name_FIX33_V23_CANONICAL_CLEAR",
            "diff_metrics_by_name_FIX2D34",
        ]:
            try:
                _cand = globals().get(_cand_name)
            except Exception:
                _cand = None
            if callable(_cand):
                fn = _cand
                bound_from = _cand_name
                try:
                    globals()["diff_metrics_by_name"] = fn
                except Exception:
                    pass
                break

    try:
        globals()["_YUREEKA_DIFF_METRICS_BY_NAME_BOUND_FROM"] = bound_from
    except Exception:
        pass

    try:
        if callable(fn):
            _yureeka_set_authoritative_binding_v1(fn, _yureeka_get_code_version())
    except Exception:
        pass


# assert globals early for debugging (safe no-op)

_yureeka_lock_version_globals_v1()
_yureeka_ensure_final_bindings_v1()


_YUREEKA_DEBUG_PLAYBOOK_MD_V1 = """## Debug Playbook (REFACTOR22)

This file is **single-file Streamlit-safe** and is intentionally refactored in small, testable steps.
The refactor harness (when enabled) is the authority for “did we preserve behavior?”.

### What to check first (fast triage)
1) **code_version** in both Analysis and Evolution JSON must match this file’s `_YUREEKA_CODE_VERSION_LOCK`.
2) Evolution must show **previous_data_rehydrated: true** when running after an Analysis baseline.
3) Diff Panel V2 must emit **metric_changes_v2** rows with both **prev_value_norm** and **cur_value_norm** for “both-side” metrics.

### If metric_changes_v2 is empty
- Confirm Analysis was run first and produced **primary_metrics_canonical**.
- Confirm Evolution JSON has **previous_data_rehydrated: true** and the expected **previous_timestamp**.
- Inspect `results.debug.diff_panel_v2_trace_v1` (if present) and row-level `diag.diff_join_trace_v1`.

### If code_version looks wrong
- Streamlit can reuse an older loaded module if the process isn’t restarted.
- This refactor uses a **version lock**: outputs stamp via `_yureeka_get_code_version()` (not `CODE_VERSION`).

### Key debug fields
- `debug.binding_manifest_v1`: Which entrypoints were resolved (Diff Panel V2 + legacy diff).
- `debug.canonical_for_render_v1`: Presence indicator for canonical-for-render debug (should be present).
- `metric_changes_v2[*].diag`: Per-row join + current-source trace.

"""


def _yureeka_show_debug_playbook_in_streamlit_v1():
    """Streamlit-safe helper: show an optional playbook expander without affecting JSON outputs."""
    try:
        if not hasattr(st, "sidebar"):
            return
        with st.sidebar.expander("Debug playbook", expanded=False):
            st.markdown(_YUREEKA_DEBUG_PLAYBOOK_MD_V1)
    except Exception:
        pass

# ============================================================
# ============================================================
# ============================================================

# ============================================================
# ============================================================
# ============================================================
# ============================================================
# ============================================================
# ============================================================

# ============================================================
# ============================================================

# ============================================================

# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR25
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR25":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR25",
            "date": "2026-01-24",
            "summary": "Add production-only Analysis→Evolution run delta column for metric changes. Standardize top-level timestamps to UTC (+00:00), compute/stamp run_timing_v1 (delta_seconds/human) in Evolution results, and gate per-row delta display when current metric is sourced from injected URLs.",
            "files": ["REFACTOR25_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR24"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# PATCH TRACKER V1 (ADD): REFACTOR26
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR26":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR26",
            "date": "2026-01-24",
            "summary": "Tighten and centralize current metric source_url attribution for reliable row-level injection gating. Add a hydrator that fills primary_metrics_canonical[*].source_url from evidence/provenance, expose current_source_url fields on diff rows, and enhance per-row injection detection to prefer row-attributed URLs before falling back to canonical maps.",
            "files": ["REFACTOR26_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR25"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# PATCH TRACKER V1 (ADD): REFACTOR27
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR27":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR27",
            "date": "2026-01-24",
            "summary": "Harden unit comparability and candidate eligibility for currency metrics. Reject date-fragment currency candidates (e.g., 'July 01, 2025') during schema-only rebuild, and strengthen currency unit mismatch detection so mixed scale/code representations do not emit nonsense deltas. Also expose current_source_url on diff rows for clearer row-level injection attribution.",
            "files": ["REFACTOR27_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR26"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass





# PATCH TRACKER V1 (ADD): REFACTOR28
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR28":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR28",
            "date": "2026-01-24",
            "summary": "Consolidate schema-only rebuild authority to eliminate stale wrapper capture chains and ensure REFACTOR27 candidate filters (especially currency date-fragment rejection) are active at runtime. This prevents day-of-month tokens like '01' from binding as currency values, restoring comparable currency baselines while preserving percent-year poisoning sanitization.",
            "files": ["REFACTOR28_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR27"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# PATCH TRACKER V1 (ADD): REFACTOR29
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR29":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR29",
            "date": "2026-01-24",
            "summary": "Refine REFACTOR25 run-delta harness and diagnostics: replace overly-strict global injection assertion with per-row gating stats (injected rows must have blank delta, production rows should show delta when available). Persist row_delta_gating_v1 into run_timing_v1 for easier debugging, without changing schema/key grammar or diff behavior.",
            "files": ["REFACTOR29_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR28"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass



# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR30
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR30":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR30",
            "date": "2026-01-24",
            "summary": "Fix REFACTOR29 run_timing_v1 row_delta_gating_v1 double-counting: apply per-row Analysis→Evolution delta stamping once (primary metric_changes list) and propagate to metric_changes_v2 by canonical_key, so diagnostic counts match the displayed table while keeping injection gating behavior unchanged.",
            "files": ["REFACTOR30_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR29"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR31
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR31":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR31",
            "date": "2026-01-24",
            "summary": "Add runtime_identity_v1 stamp (code_version lock + __file__ + SHA1) to Analysis/Evolution debug for diagnosing stale-version runs; and harden run_timing_v1 row_delta_gating_v1 stats to count unique canonical_keys only (prevents double-counting when both metric_changes and metric_changes_v2 exist). No schema/key-grammar changes.",
            "files": ["REFACTOR31_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR30"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR32
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR32":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR32",
            "date": "2026-01-24",
            "summary": "Clarify injected URL semantics: treat only UI-provided extra_urls_ui(_raw) (or explicit internal marker) as injected for debug + run-delta gating, preventing production source URLs from being misclassified as injected. Add __yureeka_extra_urls_are_injection_v1 markers when Evolution wires injected URLs into web_context['extra_urls']. No schema/key-grammar changes.",
            "files": ["REFACTOR32_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR31"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR33
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR33":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR33",
            "date": "2026-01-24",
            "summary": "Downsize footprint by deleting shadowed duplicate top-level function definitions and redundant metric_changes_legacy preservation block, keeping only the final authoritative implementations. No schema/key-grammar changes.",
            "files": ["REFACTOR33_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR32"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR34
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR34":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR34",
            "date": "2026-01-24",
            "summary": "Fix a missing return in rebuild_metrics_from_snapshots_schema_only_fix17 that caused schema-only rebuilds to return None, breaking Analysis primary_metrics_canonical persistence and Evolution diffing after REFACTOR33 deletions. No schema/key-grammar changes.",
            "files": ["REFACTOR34_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR33"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# PATCH TRACKER V1 (ADD): REFACTOR24
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR24":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR24",
            "date": "2026-01-23",
            "summary": "Fix REFACTOR23 syntax regression (mis-indented try block) and make currency-aware unit_cmp construction consistent across all get_canonical_value_and_unit() definitions (USD:B, EUR:B, etc.) so cross-currency deltas are vetoed deterministically.",
            "files": ["REFACTOR24_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR23"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# PATCH TRACKER V1 (ADD): REFACTOR23
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR23":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR23",
            "date": "2026-01-23",
            "summary": "Unit consistency hardening: carry currency_code through candidate canonicalization & schema-only rebuild; include currency_code in diff unit_cmp token for currency metrics (detect USD vs EUR rather than silently comparing); and fix a small anchor-rebuild NameError to keep anchor path safe.",
            "files": ["REFACTOR23_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR22"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# PATCH TRACKER V1 (ADD): REFACTOR22
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR22":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR22",
            "date": "2026-01-23",
            "summary": "Fix unit-family noise for yearlike tokens: normalize_unit_family() no longer infers percent/currency/magnitude from surrounding context when unit_tag is empty and raw token is a plain 4-digit year (1900–2100). This reduces unit inconsistencies in baseline_sources_cache and prevents misleading 'percent_tag' traces on year/range endpoints, without changing canonical binding or diff behavior.",
            "files": ["REFACTOR23_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR21"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# PATCH TRACKER V1 (ADD): REFACTOR21
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR21":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR21",
            "date": "2026-01-23",
            "summary": "Harden unit inference against year/range artifacts: mark 4-digit year tokens as junk (year_token) when unitless, prevent context-driven unit backfill for yearlike candidates, and tag negative endpoints produced by hyphen ranges (e.g., '151-300' -> '-300') as junk (hyphen_range_negative_endpoint). Reduces unit inconsistencies and percent/currency 'poisoning' from nearby context.",
            "files": ["REFACTOR21_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR20"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# PATCH TRACKER V1 (ADD): REFACTOR20
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR20":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR20",
            "date": "2026-01-23",
            "summary": "Fix false currency evidence hits caused by substring matches (e.g., 'eur'/'euro' inside 'Europe'). Introduce boundary-aware currency detection helper and use it in infer_unit_tag_from_context() and normalize_unit_family(), preventing 'million units' candidates from being misclassified as currency in Europe contexts.",
            "files": ["REFACTOR20_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR19"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# PATCH TRACKER V1 (ADD): REFACTOR19
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR19":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR19",
            "date": "2026-01-23",
            "summary": "Restore Analysis/Evolution parity for schema_only_rebuild outputs by including unit_tag, unit_family, base_unit, and multiplier_to_base (plus raw) on rebuilt primary_metrics_canonical entries. This keeps diff comparability deterministic and prevents downstream re-parsing of current values.",
            "files": ["REFACTOR19_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR18"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# PATCH TRACKER V1 (ADD): REFACTOR18
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR18":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR18",
            "date": "2026-01-23",
            "summary": "Harden authoritative diff binding signal: ensure diff_metrics_by_name always carries a reliable __YUREEKA_AUTHORITATIVE_BINDING__ tag (with globals fallback when callable wrappers reject setattr). Make binding_manifest_v1 self-contained (use local bound_from values) and update harness report IDs to the current refactor version for cleaner diagnostics.",
            "files": ["REFACTOR18_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR17"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# PATCH TRACKER V1 (ADD): REFACTOR17
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR17":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR17",
            "date": "2026-01-23",
            "summary": "Add concise in-file debug playbook and surface an authoritative binding manifest for Diff Panel V2 entrypoint (plus legacy diff_metrics_by_name when available). Improves manifest resilience when Streamlit triggers execution before later defs.",
            "files": ["REFACTOR17_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR16"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR16
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR16":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR16",
            "date": "2026-01-23",
            "summary": "Hard-lock CODE_VERSION to REFACTOR16 across legacy override sites; expand FINAL BINDINGS candidate set to include _yureeka_diff_metrics_by_name_v24; make binding_manifest_v1 resolve/report the actual diff entrypoint even when Streamlit executes before later diff wrapper defs.",
            "files": ["REFACTOR16_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR15"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR01
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "REFACTOR02",
        "date": "2026-01-21",
        "summary": "Add refactor regression harness (Analysis→Evolution) gated by explicit flag; emits JSON report + asserts diff invariants (both_count > 0, no prev-metrics sentinel, percent-year token rule).",
        "files": ["REFACTOR02_full_codebase_streamlit_safe.py"],
        "supersedes": ["FIX2D86"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR03
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR03":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR03",
            "date": "2026-01-21",
            "summary": "Fix REFACTOR02 regression: enforce unit-family + scale eligibility in schema-only rebuild; detect unit mismatch in diff panel and mark as unit_mismatch (avoid bogus B vs M diffs).",
            "files": ["REFACTOR03_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR02"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass



# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR04
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR04":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR10",
            "date": "2026-01-21",
            "summary": "Fix false unit_mismatch caused by context_snippet percent leakage; enrich schema-anchored rebuilt PMC with unit_tag/unit_family/multiplier_to_base; tighten unit-family evidence checks to prefer token/raw evidence over broad context for magnitude keys; update refactor harness labels to REFACTOR04.",
            "files": ["REFACTOR04_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR03"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR05
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR05":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR05",
            "date": "2026-01-21",
            "summary": "Selection gating + harness fix: block currency/percent candidates from __unit_* keys, promote raw/unit metadata into PMC for parity, and make harness read evidence lists.",
            "files": ["REFACTOR05_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR04"],
        })
except Exception:
    pass




# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR06

# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR07
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR07":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR07",
            "date": "2026-01-22",
            "summary": "Freeze versioning as single-source-of-truth using a refactor version lock; ensure JSON outputs use the locked version; add binding_manifest_v1 and harness assertions for version + authoritative diff binding; update FINAL BINDINGS and harness report labels to REFACTOR07.",
            "files": ["REFACTOR07_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR06"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass



# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR08
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR08":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR08",
            "date": "2026-01-22",
            "summary": "Enhance refactor regression harness with consistent REFACTOR08 labels/versioning, dynamic authoritative diff binding expectation, and summary consistency checks (rows_total/partition/found/not_found/key_overlap). Update FINAL BINDINGS tag and locked CODE_VERSION to REFACTOR08.",
            "files": ["REFACTOR08_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR07"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR06":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR06",
            "date": "2026-01-22",
            "summary": "Freeze authoritative runtime bindings: add FINAL BINDINGS section for diff_metrics_by_name, tag authoritative function for harness verification, and update harness report/version labels.",
            "files": ["REFACTOR06_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR05"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR02
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "REFACTOR02",
        "date": "2026-01-21",
        "summary": "Harden candidate eligibility against cross-dimension leakage (magnitude/count vs currency/percent) and enforce percent-year token rejection in eligibility; upgrade refactor harness invariants (baseline PMC dimensional sanity + percent-year check on prev+cur).",
        "files": ["REFACTOR02_full_codebase_streamlit_safe.py"],
        "supersedes": ["REFACTOR01"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# ============================================================
# REFACTOR02: HARNESS FLAG (ADDITIVE)
# - Streamlit-safe: does nothing unless explicitly invoked.
# Invocation:
#   - python REFACTOR02_full_codebase_streamlit_safe.py --run_refactor_harness
#   - or set RUN_REFACTOR_HARNESS=1
# NOTE:
#   - Under Streamlit runtime, the harness is forcibly disabled to prevent sys.exit()
#     from terminating the Streamlit server / failing health checks.
# ============================================================
try:
    import os as _rf01_os
    import sys as _rf01_sys

    def _rf01__running_under_streamlit() -> bool:
        try:
            argv = _rf01_sys.argv or []
            if any("streamlit" in str(a).lower() for a in argv[:5]):
                return True
        except Exception:
            pass
        try:
            if "streamlit" in _rf01_sys.modules:
                return True
            if "streamlit.runtime.scriptrunner" in _rf01_sys.modules:
                return True
        except Exception:
            pass
        try:
            for _k in (
                "STREAMLIT_SERVER_PORT",
                "STREAMLIT_SERVER_ADDRESS",
                "STREAMLIT_SERVER_HEADLESS",
                "STREAMLIT_BROWSER_GATHER_USAGE_STATS",
            ):
                if _rf01_os.getenv(_k) is not None:
                    return True
        except Exception:
            pass
        return False

    _rf01__is_streamlit = _rf01__running_under_streamlit()

    _REFACTOR01_HARNESS_REQUESTED = (
        (("--run_refactor_harness" in (_rf01_sys.argv or []))
         or (str(_rf01_os.getenv("RUN_REFACTOR_HARNESS", "")).strip().lower() in ("1", "true", "yes", "y")))
        and (not _rf01__is_streamlit)
    )
except Exception:
    _REFACTOR01_HARNESS_REQUESTED = False

# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR46
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "REFACTOR46",
        "date": "2026-01-25",
        "summary": "Prevent refactor harness from terminating Streamlit runtime (disable harness under Streamlit; double-guard EOF harness dispatch).",
        "files": ["REFACTOR46_full_codebase_streamlit_safe.py"],
        "supersedes": ["REFACTOR45"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D71
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D71",
        "date": "2026-01-19",
        "summary": "Commit schema-keyed baseline canonical metrics during Analysis: if schema authority selection yields no winners but baseline_schema_metrics_v1 is non-empty, promote that schema-keyed (auditable proxy) map into primary_metrics_canonical so Evolution has prev canonical metrics for metric_changes_v2 diffing.",
        "files": ["FIX2D71_full_codebase.py"],
        "supersedes": ["FIX2D70"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D70
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D70",
        "date": "2026-01-19",
        "summary": "Controlled schema-candidate reconciliation during schema-anchored rebuild: relax key-year matching (±1 for single-year keys, overlap for ranges) and keyword gating only when strict prefilter yields zero candidates, while emitting FIX2D70 rejection counts and relax flags for audit. This closes the last-mile binding gap without reintroducing heuristic matching.",
        "files": ["FIX2D70_full_codebase.py"],
        "supersedes": ["FIX2D69B"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D69
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D69",
        "date": "2026-01-19",
        "summary": "Hard-wire numeric extraction on injected snapshot_text: when injected placeholders are fetched (FIX41AFC16), convert HTML to plain text if needed, always extract numbers from the non-empty text, and store content_len/clean_text_len plus FIX2D68 extraction diagnostics and errors. Also defensively initialize observed_rows_filtered_noninjected to prevent UnboundLocalError in Diff Panel V2 summary.",
        "files": ["FIX2D69_full_codebase.py"],
        "supersedes": ["FIX2D68"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass
  # PATCH FIX2D64: add canonical_identity_spine shadow-mode module + regressions (no behavior change)

# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D66G
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D66G",
        "date": "2026-01-19",
        "summary": "Google Sheets write resiliency: always mirror saved analyses into session_state history; if Sheets write fails, set a flag and record _SHEETS_LAST_WRITE_ERROR so get_history() falls back to session_state when Sheet reads are empty. Prevents Evolution from being blocked by transient Sheets save failures. No changes to extraction/diffing.",
        "files": ["FIX2D66G_full_codebase.py"],
        "supersedes": ["FIX2D66"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass



# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D66
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D66",
        "date": "2026-01-19",
        "summary": "Deterministic injected-URL admission: promote UI raw/diag injection fields into web_context.extra_urls and synthesize diag_injected_urls when missing, so inj_diag/inj_trace_v1 reliably reflect injected URLs in snapshot pool and hash inputs (auditable). No UI/diff changes.",
        "files": ["FIX2D66_full_codebase.py"],
        "supersedes": ["FIX2D65D"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass







# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D65A
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D65A",
        "date": "2026-01-19",
        "summary": "Hotfix for FIX2D65: repair syntax-corrupted duplicate selector block; make yearlike prune non-fatal (never empties pool); make 'rebuild empty with snapshots' non-fatal so Evolution can still emit JSON diagnostics.",
        "files": ["FIX2D65A_full_codebase.py"],
        "supersedes": ["FIX2D65"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D58F
# ============================================================

# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D41
# ============================================================


# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D44
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D44",
        "date": "2026-01-17",
        "summary": "Fix Analysis baseline schema baseline materialisation: define _core in attach_source_snapshots_to_analysis so FIX2D31/FIX2D38 baseline_schema_metrics_v1 builder executes; emit results.baseline_schema_metrics_v1 for Evolution diff join.",
        "files": ["FIX2D44.py"],
        "supersedes": ["FIX2D43"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1

except Exception:
    pass

# =========================
# PATCH FIX2D59 (ADDITIVE): Canonical Identity Resolver (shared authority)
# - Introduces a single deterministic identity tuple and a schema-first resolver.
# - Cuts old direct key-mint paths by routing canonical_key assignment through the resolver.
# =========================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D59",
        "summary": "Canonical identity resolver v1: define identity tuple + schema-first resolver; route canonical key minting through resolver to align Analysis and Evolution key authority.",
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# =========================
# PATCH FIX2D60 (ADDITIVE): schema-only canonical enforcement + yearlike rejection at schema-only commit
# - Analysis: after identity rekey, keep ONLY schema-bound metrics in primary_metrics_canonical.
# - Evolution schema_only rebuild: never allow a bare year token to commit for expected_kind=='unit' keys.
# =========================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D60",
        "summary": "Enforce schema-only canonical store (Analysis) and hard-reject bare-year candidates for unit/count keys at schema_only_rebuild commit point (Evolution).",
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# =========================
# PATCH FIX2D61 (ADDITIVE): Schema Promotion Path (Option A)
# - Propose schema entries from primary_metrics_provisional
# - Allow deterministic promotion into metric_schema_frozen (analysis-side)
# - Record proposals and promotions for audit
# =========================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D61",
        "summary": "Option A schema extension: generate promotion proposals from primary_metrics_provisional and (optionally) promote them into metric_schema_frozen with full audit metadata; enables closing remaining coverage gaps without reintroducing heuristic canonical minting.",
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# =========================
# PATCH FIX2D62 (ADDITIVE): Time token normalization into identity tuple
# - Split embedded year/YTD/forecast tokens out of metric_token and into time_scope.
# - Resolver matches schema on metric_token + '_' + time_scope, preventing 2024/2025 being glued into metric_token.
# - Also adds required except/pass closure for early patch-tracker try block.
# =========================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D62",
        "summary": "Normalize time tokens into identity tuple (year/YTD/forecast) + schema-first resolver uses metric_token+time_scope to match schema; prevents 2024/2025 contamination of metric_token and improves Analysis/Evolution convergence.",
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# =========================
# PATCH FIX2D63 (ADDITIVE): schema_only_rebuild yearlike hardening for unit/count metrics
# - Fixes a variable typo that could disable FIX2D2U gating in the prefilter.
# - Rejects yearlike numeric candidates for unit/count schema keys unless they carry unit evidence.
# - Records reject counts under _evolution_rebuild_debug.fix2d63_reject_yearlike_no_unit_evidence.
# =========================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D63",
        "summary": "Harden schema_only_rebuild_fix17 against injected-year pollution for unit/count metrics: fix _c variable typo in FIX2D2U gate and reject yearlike candidates without unit evidence upstream.",
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# =========================
# PATCH FIX2D64 (ADDITIVE): Canonical Identity Spine V1 (shadow mode)
# - Introduces canonical_identity_spine.py as the single future authority for identity normalization,
#   schema-first key resolution, and value selection (incl. yearlike hard rejection immune to window backfill).
# - Adds minimal regression tests as callable self-checks (no runtime behavior change unless explicitly enabled).
# =========================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D64",
        "summary": "Add Canonical Identity Spine V1 module (shadow mode only) + minimal regressions: centralizes identity tuple, schema-first resolver contract, and value selection with yearlike rejection immune to context unit backfill.",
        "files": ["canonical_identity_spine.py", "FIX2D64_full_codebase.py"],
        "supersedes": ["FIX2D63"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# =========================
# PATCH FIX2D65 (AUTHORITY TAKEOVER): Canonical Identity Spine V1 becomes the only authority
# - Rewire Analysis + Evolution to resolve canonical keys via canonical_identity_spine.resolve_key_v1 (schema-first)
# - Enforce no-canonical-outside-spine gate at primary_metrics_canonical commit and schema_only_rebuild selection
# - Prune yearlike candidates for unit/count metrics even when unit evidence was context/window backfilled
# =========================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D65",
        "date": "2026-01-19",
        "summary": "Authority takeover: make Canonical Identity Spine V1 the only key-resolution authority (Analysis+Evolution) and add hard gates; prune yearlike candidates for unit/count metrics immune to context unit backfill.",
        "files": ["canonical_identity_spine.py", "FIX2D65_full_codebase.py"],
        "supersedes": ["FIX2D64"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass



# PATCH TRACKER V1 (ADD): FIX2D40
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D40",
        "summary": "Analysis: when schema is frozen, remap best-fit baseline metrics from generic canonical keys onto schema canonical keys (one-to-one) to enable baseline diffing; stamps explicit schema_remap audit fields; retains FIX2D39 hard unit/dimension rejection.",
        "files": ["FIX2D40.py"],
        "supersedes": ["FIX2D39"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# PATCH TRACKER V1 (ADD): FIX2D32
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D32",
        "date": "2026-01-17",
        "summary": "Diff Panel V2: treat anchor_hash mismatches as still diffable when canonical_key + unit-family gates pass; stamp row-level anchor_mismatch_diffable_v1 diagnostics and count such joins for audit.",
        "files": ["FIX2D32.py"],
        "supersedes": ["FIX2D31"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass



# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D33
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D33",
        "date": "2026-01-17",
        "summary": "Analysis schema-primary rebuild: baseline commitment for schema keys by backfilling missing value_norm from raw/value via deterministic parser when selector chooses a candidate but value_norm is None; improves baseline comparability without weakening semantic gates.",
        "files": ["FIX2D33.py"],
        "supersedes": ["FIX2D32"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D25
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D25",
        "date": "2026-01-16",
        "summary": "Re-enable Analysis→Evolution diffing by adding deterministic, unit-family-guarded inference for baseline keys in Diff Panel V2 when ckey/anchor joins miss; keep FIX2D20/FIX2D24 tracing and yearlike current blocking.",
        "files": ["FIX2D25.py"],
        "supersedes": ["FIX2D23"],
    })


    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2D",
        "date": "2026-01-16",
        "summary": "Fix Diff Panel V2 crash (prev_v/cur_v NameError) by using correctly scoped norm variables in traces; simplify end-of-file version stamping to a single final override.",
        "files": ["FIX2D2D.py"],
        "supersedes": ["FIX2D2C"],
    })

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2E",
        "date": "2026-01-16",
        "summary": "Make Diff Panel V2 binding inference authoritative in the active FIX2J override path by committing inferred current_value/current_value_norm/current_source/current_method when joins miss; add explicit inference_commit trace; keep FIX2D24 year blocking and unit-first eligibility.",
        "files": ["FIX2D2E.py"],
        "supersedes": ["FIX2D2D"],
    })


    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2I",
        "date": "2026-01-16",
        "summary": "Enable binding inference fallback when a joined current value is blocked as unitless yearlike; add unit-family backfill for extracted_numbers pool candidates and trace the backfill/override in Diff Panel V2 __rows.",
        "files": ["FIX2D2I.py"],
        "supersedes": ["FIX2D2G"],
    })

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2J",
        "date": "2026-01-16",
        "summary": "Deterministic unit/measure classifier for extracted numeric candidates: backfill unit_family from unit_tag and currency evidence in context; correct measure_kind/measure_assoc for currency-like candidates; attach classifier trace fields.",
        "files": ["FIX2D2J.py"],
        "supersedes": ["FIX2D2I"],
    })

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2K",
        "date": "2026-01-16",
        "summary": "Context-driven unit backfill when unit_tag is empty, plus unit_family/measure_kind corrections trace (context_unit_backfill_v1).",
    })

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2Z",
        "date": "2026-01-17",
        "summary": "Make injected candidates first-class for Diff Panel inference by unwrapping injected scraped_meta into extracted_numbers pools; enforce hard unit-family rejection (percent/currency/units/magnitude) in fallback inference scoring to prevent leakage.",
        "files": ["FIX2D2Z.py"],
        "supersedes": ["FIX2D2Y"],
    })



    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D30",
        "date": "2026-01-17",
        "summary": "Contextual unit-family correction: prevent magnitude tags (e.g., M/million) in 'million units / units sold / chargers / vehicles' contexts from being misclassified as currency; remove keyword-only currency upgrades to enable clean baseline comparability without weakening hard unit-family rejection.",
        "files": ["FIX2D30.py"],
        "supersedes": ["FIX2D2Z"],
    })

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D31",
        "date": "2026-01-17",
        "summary": "Option A schema authority: when metric_schema_frozen is present in Analysis, rebuild primary_metrics_canonical by running the authoritative Analysis selector (_analysis_canonical_final_selector_v1) constrained to the frozen schema keys. This makes Analysis emit schema-aligned baseline metrics so Evolution injection can overlap and Diff Panel V2 can activate without weakening semantics.",
        "files": ["FIX2D31.py"],
        "supersedes": ["FIX2D30"],
    })

    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2U",
        "date": "2026-01-17",
        "summary": "Introduce shared semantic eligibility gate (Analysis parity) using local context_snippet; enforce it in Evolution schema-only rebuild(s) and Analysis selector to prevent cross-metric pollution (e.g., China sales value mapping into chargers 2040).",
        "files": ["FIX2D2U.py"],
        "supersedes": ["FIX2D2T"],
    })

except Exception:
    pass


# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D26
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D26",
        "date": "2026-01-16",
        "summary": "Unit-first, context-bound inference candidate picker for Diff Panel V2 (Analysis→Evolution). Prefers percent/units/currency matches with keyword binding; rejects bare-year tokens pre-score; adds per-row trace counters.",
        "files": ["FIX2D26.py"],
        "supersedes": ["FIX2D25"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D28
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D28",
        "date": "2026-01-16",
        "summary": "Close Diff Panel V2 binding gap: when inference selects a current value, commit it into UI/diff-read fields (current_value, current_value_norm, current_source, current_method) and mark baseline_is_comparable once all guards pass.",
        "files": ["FIX2D28.py"],
        "supersedes": ["FIX2D27"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D29
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D29",
        "date": "2026-01-16",
        "summary": "Fix FIX2D28 insertion placement and complete write-through: commit inference/joined current values into metric_changes fields used by UI/diff (current_value, current_value_norm, current_source, current_method) and set baseline_is_comparable when numeric.",
        "files": ["FIX2D29.py"],
        "supersedes": ["FIX2D28"],
    })

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2A",
        "date": "2026-01-16",
        "summary": "Enable guarded inference in Diff Panel V2 regardless of join mode; add explicit inference gate + attempted traces so binding inference can commit current values.",
        "files": ["FIX2D2A.py"],
    })

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2B",
        "date": "2026-01-16",
        "summary": "Correct version stamping for FIX2D2A runtime by bumping CODE_VERSION and adding final end-of-file override to prevent legacy late assignments from masking patch id.",
        "files": ["FIX2D2B.py"],
        "supersedes": ["FIX2D2A"],
    })

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2C",
        "date": "2026-01-16",
        "summary": "Fix Diff Panel V2 NameError by defining guarded inference gate in the active builder (build_diff_metrics_panel_v2) and emitting explicit inference gate trace; no heuristic changes.",
        "files": ["FIX2D2C.py"],
        "supersedes": ["FIX2D2B"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass



# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D18
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D18",
        "date": "2026-01-15",
        "summary": "Re-enable schema-only rebuild eligibility gates (domain token + unit-family) and strengthen unit-sales expectations to prevent bare-year (e.g., 2030) contamination; improves baseline comparables for Analysis→Evolution diffing.",
        "files": ["FIX2D18.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D19
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D19",
        "date": "2026-01-16",
        "summary": "Harden schema_only_rebuild_fix17 with required domain-token binding (prevents generic keyword matches) and add deterministic baseline soft-match fallback in Diff Panel V2 to enable Analysis→Evolution comparable diffs when strict joins fail.",
        "files": ["FIX2D19.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass



# PATCH TRACKER V1 (ADD): FIX2D20
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D20",
        "date": "2026-01-15",
        "summary": "Diagnostic-first trace: record every year-like (1900-2100) value committed to primary_metrics_canonical, including callsite tags and metric object metadata; also disable FIX2D18/FIX2D19 logic while tracing.",
        "files": ["FIX2D20.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# PATCH TRACKER V1 (ADD): FIX2D24
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D24",
        "date": "2026-01-16",
        "summary": "Last-mile guard for dashboard Current: block unitless year-like values (1900-2100, including 2030.0) from metric_changes hydration for non-year metrics; keep FIX2D20 tracing; supersedes FIX2D23 observed-only filter.",
        "files": ["FIX2D24.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# PATCH TRACKER V1 (ADD): FIX2D21
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D21",
        "date": "2026-01-16",
        "summary": "Evolution baseline-key schema: derive metric_schema_frozen from Analysis primary_metrics_canonical keys, and fix bare-year detection to reject tokens like 2030.0/2024.0; keep FIX2D20 year-commit tracing for verification.",
        "files": ["FIX2D21.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass




# PATCH TRACKER V1 (ADD): FIX2D22
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D22",
        "date": "2026-01-16",
        "summary": "Schema-only rebuild: enforce *eligibility-before-scoring* (hard reject bare-year tokens incl 2024/2030 and require unit-family + required domain tokens) so years cannot win; supersedes FIX2D21 selector hardening but retains baseline-key schema derivation.",
        "files": ["FIX2D22.py"],
        "supersedes": ["FIX2D21"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass
# =====================================================================
# PATCH FIX2D20 (ADD): Disable earlier speculative selection tweaks while tracing
# =====================================================================
_FIX2D20_DISABLE_FIX2D18 = False
_FIX2D20_DISABLE_FIX2D19 = False

# =====================================================================
# PATCH FIX2D20 (ADD): Year-like commit tracing for primary_metrics_canonical
# - We have repeated evidence of year tokens (e.g., 2024/2030) being committed
#   with method/unit missing. This patch records *where* and *what* is being
#   committed so we can fix the correct choke point without more speculation.
# =====================================================================

def _fix2d20_is_yearish_value(v):
    try:
        if v is None:
            return False
        fv = float(v)
        iv = int(fv)
        if abs(fv - iv) > 1e-6:
            return False
        return 1900 <= iv <= 2100
    except Exception:
        return False


def _fix2d20_is_yearish_token(raw: str) -> bool:
    try:
        s = str(raw or '').strip()
        if not s:
            return False
        # allow "2030" or "2030.0"
        if re.fullmatch(r"\d{4}(?:\.0+)?", s):
            iv = int(float(s))
            return 1900 <= iv <= 2100
        return False
    except Exception:
        return False


def _fix2d20_trace_year_like_commits(output: dict, stage: str, callsite: str) -> None:
    try:
        if not isinstance(output, dict):
            return
        results = output.get('results')
        if not isinstance(results, dict):
            return
        pmc = results.get('primary_metrics_canonical')
        if not isinstance(pmc, dict) or not pmc:
            return
        dbg = results.setdefault('debug', {})
        if not isinstance(dbg, dict):
            return
        trace = dbg.setdefault('fix2d20_year_commit_trace_v1', {})
        if not isinstance(trace, dict):
            return
        trace.setdefault('stage', str(stage or ''))
        trace.setdefault('events', [])
        trace.setdefault('callsite_counts', {})
        events = trace.get('events')
        if not isinstance(events, list):
            events = []
            trace['events'] = events
        ccounts = trace.get('callsite_counts')
        if not isinstance(ccounts, dict):
            ccounts = {}
            trace['callsite_counts'] = ccounts

        max_events = 80
        for k, mobj in pmc.items():
            if not isinstance(mobj, dict):
                continue
            v = mobj.get('value_norm')
            raw = mobj.get('raw')
            if not (_fix2d20_is_yearish_value(v) or _fix2d20_is_yearish_token(raw)):
                continue
            unit = mobj.get('unit_tag')
            method = mobj.get('method')
            src = mobj.get('source_url')
            ev = {
                'canonical_key': str(k),
                'value_norm': v,
                'raw': raw,
                'unit_tag': unit,
                'method': method,
                'source_url': src,
                'callsite': str(callsite or ''),
            }
            # small context sample if present
            evd = mobj.get('evidence')
            if isinstance(evd, dict):
                ctx = evd.get('context_snippet') or evd.get('context')
                if ctx:
                    ev['context_snippet'] = str(ctx)[:240]
                ev['evidence_method'] = evd.get('method')
                ev['evidence_raw'] = evd.get('raw')
            diag = mobj.get('diag')
            if isinstance(diag, dict):
                ev['diag_keys'] = sorted([str(x) for x in diag.keys()])[:12]
            events.append(ev)
            ccounts[str(callsite or '')] = int(ccounts.get(str(callsite or ''), 0) or 0) + 1
            if len(events) >= max_events:
                break

        trace['events'] = events
        trace['callsite_counts'] = ccounts
    except Exception:
        return

# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D11c
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D11c",
        "date": "2026-01-15",
        "summary": "Fix indentation/scope of FIX2D11 render fallback by ensuring it remains inside compute_source_anchored_diff (4-space indent) to avoid parse-time try/indent errors.",
        "files": ["FIX2D11c.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass



# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D12
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D12",
        "date": "2026-01-15",
        "summary": "Fix Diff Panel V2 premature return that prevented row emission; ensure UNION mode can emit current-only rows (added) and populate Current column.",
        "files": ["FIX2D12.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D15
# NOTE: FIX2D15 supersedes and removes FIX2D15 year-token guard implementation (replaced with stricter schema-only eligibility gates).
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D15",
        "date": "2026-01-15",
        "summary": "Reject bare-year tokens (e.g., 2030) during schema-only rebuild for non-year metrics; add diagnostics to prevent year pollution and restore stable baseline comparables.",
        "files": ["FIX2D15.py"],
    })
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D16",
        "date": "2026-01-15",
        "summary": "Add soft-match fallback to fill current values for baseline rows when anchor/ckey join fails; add last-mile bare-year reject for schema-only rebuild promotions. Disable FIX2D15 gating.",
        "files": ["FIX2D16.py"],
    })
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D17",
        "date": "2026-01-15",
        "summary": "Harden canonical selector: reject bare-year tokens as metric values and require domain keyword overlap to prevent cross-metric pollution; deprecate FIX2D16 soft-match/year guards.",
        "files": ["FIX2D17.py"],
    })

    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D13
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D13",
        "date": "2026-01-15",
        "summary": "Add baseline-focused diff semantics to Diff Panel V2: classify rows as comparable/added/not_found and emit baseline delta fields + summary counters without requiring injected URLs in Analysis.",
        "files": ["FIX2D13.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass



# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D11b
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D11b",
        "date": "2026-01-15",
        "summary": "Syntax-safe render-gate fallback: union-mode unanchored canonical_for_render without introducing nested try blocks.",
        "files": ["FIX2D11b.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass



# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D11
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D11",
        "date": "2026-01-15",
        "summary": "Render gate fallback in union mode: if V28 anchor-enforce yields 0 hits, populate canonical_for_render from current primary_metrics_canonical and label as unanchored-for-render.",
        "files": ["FIX2D11.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass



# ============================================================
# PATCH START: FIX2D10_MATERIALIZE_OUTPUT_DEBUG_CANONICAL_FOR_RENDER_V1
# Purpose:
#   The Evolution dashboard diagnostics expect output_debug.canonical_for_render_v1
#   to exist. Some code paths populate current canonical under results.primary_metrics_canonical
#   (or nested results.results.*). This patch materializes output_debug.canonical_for_render_v1
#   from results.primary_metrics_canonical (post-promotion) to eliminate false 'missing' signals
#   and allow Current column hydration.
# ============================================================
def _fix2d10_materialize_output_debug_canonical_for_render_v1(output_obj):
    diag = {
        "applied": False,
        "source": None,
        "count": 0,
        "keys_sample": [],
    }
    try:
        if not isinstance(output_obj, dict):
            return diag
        results = output_obj.get("results")
        if not isinstance(results, dict):
            return diag
        # prefer top-level results.primary_metrics_canonical
        pmc = results.get("primary_metrics_canonical")
        src_name = "results.primary_metrics_canonical"
        if not (isinstance(pmc, dict) and pmc):
            nested = results.get("results")
            if isinstance(nested, dict) and isinstance(nested.get("primary_metrics_canonical"), dict) and nested.get("primary_metrics_canonical"):
                pmc = nested.get("primary_metrics_canonical")
                src_name = "results.results.primary_metrics_canonical"
        if not (isinstance(pmc, dict) and pmc):
            diag["source"] = "none"
            return diag

        # materialize output_debug.canonical_for_render_v1
        od = output_obj.get("output_debug")
        if not isinstance(od, dict):
            od = {}
            output_obj["output_debug"] = od
        od["canonical_for_render_v1"] = dict(pmc)
        diag["applied"] = True
        diag["source"] = src_name
        diag["count"] = len(pmc)
        try:
            diag["keys_sample"] = list(pmc.keys())[:10]
        except Exception:
            pass
            diag["keys_sample"] = []

        try:
            results.setdefault("debug", {})
            results["debug"]["fix2d10_materialize_output_debug_canonical_for_render_v1"] = diag
        except Exception:
            return diag
    except Exception:
        return diag
# ============================================================
# PATCH END: FIX2D10_MATERIALIZE_OUTPUT_DEBUG_CANONICAL_FOR_RENDER_V1


# ============================================================
# PATCH START: FIX2D73_HISTORYFULL_PREV_CANON_PROMOTION_V1
# Purpose:
#   - HistoryFull rehydrate can return a full prior analysis payload where baseline
#     canonical metrics live under nested containers (e.g., results.primary_metrics_canonical).
#   - Evolution diffing (Diff Panel V2) expects previous_data.primary_metrics_canonical
#     (and/or previous_data.primary_response.primary_metrics_canonical) to exist.
# What:
#   - Promote nested baseline canonical metrics + schema into top-level keys on the
#     rehydrated previous payload, and mirror into primary_response.
#   - Emit compact diagnostics counts (safe, additive).
# ============================================================

def _fix2d73_promote_rehydrated_prevdata_v1(prev_full: dict) -> dict:
    diag = {
        "applied": False,
        "pmc_before": 0,
        "pmc_after": 0,
        "pmc_source": None,
        "notes": [],
    }
    try:
        if not isinstance(prev_full, dict):
            return prev_full

        # Count before
        try:
            if isinstance(prev_full.get("primary_metrics_canonical"), dict):
                diag["pmc_before"] = int(len(prev_full.get("primary_metrics_canonical") or {}))
        except Exception:
            pass

        # Find candidate pmc in common nested locations
        pmc = None
        src = None
        if isinstance(prev_full.get("primary_metrics_canonical"), dict) and prev_full.get("primary_metrics_canonical"):
            pmc = prev_full.get("primary_metrics_canonical")
            src = "prev_full.primary_metrics_canonical"
        elif isinstance(prev_full.get("primary_response"), dict) and isinstance(prev_full["primary_response"].get("primary_metrics_canonical"), dict) and prev_full["primary_response"].get("primary_metrics_canonical"):
            pmc = prev_full["primary_response"].get("primary_metrics_canonical")
            src = "prev_full.primary_response.primary_metrics_canonical"
        elif isinstance(prev_full.get("results"), dict) and isinstance(prev_full["results"].get("primary_metrics_canonical"), dict) and prev_full["results"].get("primary_metrics_canonical"):
            pmc = prev_full["results"].get("primary_metrics_canonical")
            src = "prev_full.results.primary_metrics_canonical"
        elif isinstance(prev_full.get("results"), dict) and isinstance(prev_full["results"].get("primary_response"), dict) and isinstance(prev_full["results"]["primary_response"].get("primary_metrics_canonical"), dict) and prev_full["results"]["primary_response"].get("primary_metrics_canonical"):
            pmc = prev_full["results"]["primary_response"].get("primary_metrics_canonical")
            src = "prev_full.results.primary_response.primary_metrics_canonical"
        elif isinstance(prev_full.get("results"), dict) and isinstance(prev_full["results"].get("results"), dict) and isinstance(prev_full["results"]["results"].get("primary_metrics_canonical"), dict) and prev_full["results"]["results"].get("primary_metrics_canonical"):
            pmc = prev_full["results"]["results"].get("primary_metrics_canonical")
            src = "prev_full.results.results.primary_metrics_canonical"

        # Promote to top-level + mirror into primary_response
        if isinstance(pmc, dict) and pmc:
            if not (isinstance(prev_full.get("primary_metrics_canonical"), dict) and prev_full.get("primary_metrics_canonical")):
                prev_full["primary_metrics_canonical"] = dict(pmc)
                diag["notes"].append("promoted_top_level_primary_metrics_canonical")
                diag["applied"] = True
            if not isinstance(prev_full.get("primary_response"), dict):
                prev_full["primary_response"] = {}
                diag["notes"].append("created_primary_response")
            if isinstance(prev_full.get("primary_response"), dict):
                if not (isinstance(prev_full["primary_response"].get("primary_metrics_canonical"), dict) and prev_full["primary_response"].get("primary_metrics_canonical")):
                    prev_full["primary_response"]["primary_metrics_canonical"] = dict(prev_full.get("primary_metrics_canonical") or {})
                    diag["notes"].append("filled_primary_response.primary_metrics_canonical")
                    diag["applied"] = True

            diag["pmc_source"] = src

        # Count after
        try:
            if isinstance(prev_full.get("primary_metrics_canonical"), dict):
                diag["pmc_after"] = int(len(prev_full.get("primary_metrics_canonical") or {}))
        except Exception:
            pass

        # Attach diag
        try:
            prev_full.setdefault("debug", {})
            if isinstance(prev_full.get("debug"), dict):
                prev_full["debug"]["fix2d73_historyfull_load_counts"] = dict(diag)
        except Exception:
            pass

        return prev_full
    except Exception:
        return prev_full

# ============================================================
# PATCH END: FIX2D73_HISTORYFULL_PREV_CANON_PROMOTION_V1

# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D73
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D73",
        "date": "2026-01-20",
        "summary": "HistoryFull persistence gap: promote baseline primary_metrics_canonical into rehydrated previous_data + ensure compute_source_anchored_diff prev_response carries canonical metrics so Diff Panel V2 can compute deltas.",
        "files": ["FIX2D73_full_codebase.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D75
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D75",
        "date": "2026-01-20",
        "summary": "Option B fork: materialize Analysis baseline primary_metrics_canonical (schema-anchored rebuild) and persist it (incl. primary_response) so HistoryFull replay exposes previous canonical values for diffing.",
        "files": ["FIX2D75_full_codebase.py"],
        "supersedes": ["FIX2D73"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D10
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D10",
        "date": "2026-01-15",
        "summary": "Materialize output_debug.canonical_for_render_v1 from results.primary_metrics_canonical so dashboard can hydrate Current and diagnostics stop falsely flagging missing.",
        "files": ["FIX2D10.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass
# ============================================================


# ============================================================
# PATCH START: FIX2D9_SCHEMA_ANCHORED_REBUILD_V1
# Purpose:
#   Force current-side canonical rebuild to be schema-anchored
#   to the Analysis (prev_response) schema universe, so keys
#   overlap and "Current" can populate.
#
# Strategy:
#   Prefer rebuild_metrics_from_snapshots_schema_only_fix16 if
#   callable; fallback to rebuild_metrics_from_snapshots_analysis_canonical_v1.
#
#   Render/diff-facing only; does not change hashing/snapshots.
# ============================================================

def _fix2d9_schema_anchored_rebuild_current_metrics_v1(prev_response, pool, web_context=None):
    diag = {
        "applied": False,
        "fn": None,
        "count": 0,
        "keys_sample": [],
        "reason": None,
    }
    try:
        if pool is None:
            diag["reason"] = "pool_none"
            return None, diag
        if not isinstance(pool, list) or not pool:
            diag["reason"] = "pool_empty"
            return None, diag

        fn = globals().get("rebuild_metrics_from_snapshots_schema_only_fix16")
        fn_name = "rebuild_metrics_from_snapshots_schema_only_fix16"
        if not callable(fn):
            fn = globals().get("rebuild_metrics_from_snapshots_analysis_canonical_v1")
            fn_name = "rebuild_metrics_from_snapshots_analysis_canonical_v1"

        if not callable(fn):
            diag["reason"] = "fn_missing"
            return None, diag

        diag["fn"] = fn_name

        try:
            rebuilt = fn(prev_response, pool, web_context=web_context)
        except TypeError:
            rebuilt = fn(prev_response, pool)


        # REFACTOR04: enrich rebuilt PMC metrics with unit_tag/unit_family/multiplier_to_base for parity + diffing.
        try:
            if isinstance(rebuilt, dict) and rebuilt:
                rebuilt = _refactor04_enrich_pmc_units_v1(rebuilt, prev_response=prev_response)
        except Exception:
            pass

        if isinstance(rebuilt, dict) and rebuilt:
            diag["applied"] = True
            diag["count"] = len(rebuilt)
            try:
                diag["keys_sample"] = list(rebuilt.keys())[:10]
            except Exception:
                pass
                diag["keys_sample"] = []
            return dict(rebuilt), diag

        diag["reason"] = "rebuilt_empty_or_non_dict"
        return None, diag
    except Exception as _e:
        diag["reason"] = "exception:" + str(type(_e).__name__)
        return None, diag

# ============================================================
# PATCH END: FIX2D9_SCHEMA_ANCHORED_REBUILD_V1

# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D9
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D9",
        "date": "2026-01-15",
        "summary": "Schema-anchored current-side canonical rebuild for diff/render: prefer schema_only rebuild so keys overlap and Current can populate.",
        "files": ["FIX2D9.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# ============================================================



# ============================================================
# PATCH START: FIX2D8_PROMOTE_NESTED_RESULTS_V1
# Purpose:
#   Normalize output shape by promoting nested results.results.*
#   up into results.* so downstream diff/render can see current
#   canonical metrics in the expected location.
#   (Additive, deterministic)
# ============================================================
def _fix2d8_promote_nested_results_v1(output_obj):
    diag = {
        "applied": False,
        "promoted_primary_metrics_canonical": 0,
        "promoted_primary_response": 0,
        "notes": []
    }
    try:
        if not isinstance(output_obj, dict):
            return diag
        results = output_obj.get("results")
        if not isinstance(results, dict):
            return diag
        nested = results.get("results")
        if not isinstance(nested, dict):
            return diag

        nested_pmc = nested.get("primary_metrics_canonical")
        top_pmc = results.get("primary_metrics_canonical")
        if isinstance(nested_pmc, dict) and nested_pmc and (not isinstance(top_pmc, dict) or not top_pmc):
            results["primary_metrics_canonical"] = dict(nested_pmc)
            diag["promoted_primary_metrics_canonical"] = len(nested_pmc)
            diag["applied"] = True

        nested_pr = nested.get("primary_response")
        top_pr = results.get("primary_response")
        if isinstance(nested_pr, dict) and nested_pr and (not isinstance(top_pr, dict) or not top_pr):
            results["primary_response"] = dict(nested_pr)
            diag["promoted_primary_response"] = 1
            diag["applied"] = True

        # Ensure primary_response.primary_metrics_canonical exists
        try:
            if isinstance(results.get("primary_response"), dict) and isinstance(results.get("primary_metrics_canonical"), dict):
                if "primary_metrics_canonical" not in results["primary_response"]:
                    results["primary_response"]["primary_metrics_canonical"] = results["primary_metrics_canonical"]
                    diag["notes"].append("filled_primary_response.primary_metrics_canonical_from_results")
                    diag["applied"] = True
        except Exception:
            pass


        # FIX2D42: Ensure baseline_schema_metrics_v1 is promoted and visible under primary_response (and results)
        try:
            # Promote baseline_schema_metrics_v1 from nested if present
            _nested_bsm = None
            try:
                _nested_bsm = nested.get("baseline_schema_metrics_v1")
            except Exception:
                pass
                _nested_bsm = None
            if isinstance(_nested_bsm, dict) and _nested_bsm and (not isinstance(results.get("baseline_schema_metrics_v1"), dict) or not results.get("baseline_schema_metrics_v1")):
                results["baseline_schema_metrics_v1"] = dict(_nested_bsm)
                diag["notes"].append("promoted_results.baseline_schema_metrics_v1_from_nested")
                diag["applied"] = True

            # Ensure primary_response.baseline_schema_metrics_v1 exists when available
            if isinstance(results.get("primary_response"), dict):
                _bsm_top = results.get("baseline_schema_metrics_v1")
                if not isinstance(_bsm_top, dict):
                    _bsm_top = None
                if isinstance(_bsm_top, dict) and _bsm_top and ("baseline_schema_metrics_v1" not in results["primary_response"]):
                    results["primary_response"]["baseline_schema_metrics_v1"] = _bsm_top
                    diag["notes"].append("filled_primary_response.baseline_schema_metrics_v1_from_results")
                    diag["applied"] = True
        except Exception:
            pass
        # FIX2D42: Ensure baseline_schema_metrics_v1 is promoted and visible under primary_response (and results)
        try:
            # Promote baseline_schema_metrics_v1 from nested if present
            _nested_bsm = None
            try:
                _nested_bsm = nested.get("baseline_schema_metrics_v1")
            except Exception:
                pass
                _nested_bsm = None
            if isinstance(_nested_bsm, dict) and _nested_bsm and (not isinstance(results.get("baseline_schema_metrics_v1"), dict) or not results.get("baseline_schema_metrics_v1")):
                results["baseline_schema_metrics_v1"] = dict(_nested_bsm)
                diag["notes"].append("promoted_results.baseline_schema_metrics_v1_from_nested")
                diag["applied"] = True
            # If analysis builder stored it at results.baseline_schema_metrics_v1, mirror into primary_response
            if isinstance(results.get("primary_response"), dict) and isinstance(results.get("baseline_schema_metrics_v1"), dict):
                if "baseline_schema_metrics_v1" not in results["primary_response"]:
                    results["primary_response"]["baseline_schema_metrics_v1"] = results.get("baseline_schema_metrics_v1")
                    diag["notes"].append("filled_primary_response.baseline_schema_metrics_v1_from_results")
                    diag["applied"] = True
        except Exception:
            pass


        try:
            results.setdefault("debug", {})
            results["debug"]["fix2d8_promote_nested_results_v1"] = diag
        except Exception:
            return diag
    except Exception as _e:
        try:
            diag["notes"].append("exception:" + str(type(_e).__name__))
        except Exception:
            return diag
# ============================================================
# PATCH END: FIX2D8_PROMOTE_NESTED_RESULTS_V1

# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D8
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D8",
        "date": "2026-01-15",
        "summary": "Normalize output shape by promoting nested results.results.* into results.* for diff/render Current hydration.",
        "files": ["FIX2D8_fixed.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass
# ============================================================


# ============================================================
# PATCH START: FIX2D6_EXECUTION_STAMP_AND_ASSERT_V1
# Purpose:
#   - Assert the running code version at runtime (fail fast if wrong file imported)
#   - Emit execution stamp into results.debug so JSON proves which code ran
#   - Emit join mode into results.debug
# ============================================================
EXPECTED_CODE_VERSION_FIX2D6 = "FIX2D6"

def _fix2d6_assert_and_stamp_runtime_v1(output_obj, join_mode=None):
    """Fail-fast version assert + JSON-visible execution stamp."""
    # Hard assert: if this file isn't the one executing, stop immediately.
    if str(globals().get("CODE_VERSION", "")).strip() != EXPECTED_CODE_VERSION_FIX2D6:
        raise RuntimeError(
            "FIX2D6 runtime assert failed: CODE_VERSION=%r expected=%r"
            % (globals().get("CODE_VERSION"), EXPECTED_CODE_VERSION_FIX2D6)
        )
    try:
        if isinstance(output_obj, dict):
            output_obj.setdefault("results", {}).setdefault("debug", {})
            output_obj["results"]["debug"]["__exec_code_version"] = globals().get("CODE_VERSION")
            try:
                output_obj["results"]["debug"]["__exec_join_mode"] = join_mode
            except Exception:
                pass
    except Exception:
        pass

# ============================================================
# PATCH END: FIX2D6_EXECUTION_STAMP_AND_ASSERT_V1

# ============================================================
# PATCH START: FIX2D6_BUILD_DIFF_KEY_UNIVERSE_V1
# Purpose: Construct diff row key universe (strict vs union)
# ============================================================
def _build_diff_key_universe(prev_keys, cur_keys):
    join_mode = None
    try:
        join_mode = _fix2d6_get_diff_join_mode_v1()
    except Exception:
        pass
        join_mode = "strict"
    if join_mode == "union":
        return sorted(set(prev_keys) | set(cur_keys)), join_mode
    return sorted(set(prev_keys)), join_mode
# ============================================================
# PATCH END: FIX2D6_BUILD_DIFF_KEY_UNIVERSE_V1
# ============================================================

# ============================================================



# ============================================================
# PATCH START: FIX2D6_HARDCODE_JOIN_MODE_V1
# Purpose:
#   Allow a hardcoded override for diff join mode (demo/debug).
#   If FORCE_DIFF_JOIN_MODE is set (e.g. "union"), it overrides
#   EVO_DIFF_JOIN_MODE environment variable.
# ============================================================
FORCE_DIFF_JOIN_MODE = "union"   # set to None to restore env-based behavior

def _fix2d6_get_diff_join_mode_v1():
    try:
        if FORCE_DIFF_JOIN_MODE:
            return str(FORCE_DIFF_JOIN_MODE).strip().lower()
    except Exception:
        pass
    try:
        import os as _os
        return str(_os.getenv("EVO_DIFF_JOIN_MODE", "strict")).strip().lower()
    except Exception:
        return "strict"
# ============================================================
# PATCH END: FIX2D6_HARDCODE_JOIN_MODE_V1
# ============================================================

# =====================================================================
# PATCH TRACKER V1 (ADD): minimal patch tracker for consolidation
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D1",
        "date": "2026-01-15",
        "summary": "Alias canonical rebuild functions to avoid fn_missing; harden Diff Panel V2 wrapper to prevent unbound summary crash.",
        "files": ["fix41afc19_evo_fix16_anchor_rebuild_override_v1_fix2af_fetch_failure_visibility_and_hardening_v1.py"],
    })

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D3",
        "date": "2026-01-15",
        "summary": "Fix FIX41AFC19 v19 display-rebuild pool resolution + callable lookup; harden Diff Panel V2 injected-set detection (support cur_response without debug wrapper).",
        "files": ["FIX2D3.py"],
    })


    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D4",
        "date": "2026-01-15",
        "summary": "Add debug.key_overlap_v1 to explicitly report prev/cur canonical key counts, overlap, and target key presence for deterministic diff feasibility checks.",
        "files": ["FIX2D4.py"],
    })

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D5",
        "date": "2026-01-15",
        "summary": "Mirror canonical_for_render_v1 diagnostics into results.debug so dashboard/diff diagnostics can see it; additive only.",
        "files": ["FIX2D5.py"],
    })
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D6",
        "date": "2026-01-15",
        "summary": "Option B engine completeness: Diff Panel V2 row universe can be prev∪cur (union) behind EVO_DIFF_JOIN_MODE flag; adds added/removed change_type and summary counts; default remains strict.",
        "files": ["FIX2D6.py"],
    })

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D6_HARDCODE",
        "date": "2026-01-15",
        "summary": "Hardcode diff join mode override via FORCE_DIFF_JOIN_MODE and route Diff Panel V2 join-mode selection through helper.",
        "files": ["FIX2D6.py"],
    })

    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# =====================================================================
# PATCH START: FIX2D4_key_overlap_debug_v1
# Purpose:
#   Emit explicit canonical key overlap diagnostics between previous and
#   current canonical metrics to make diff feasibility observable.
#   (Additive, no behavior change)
# =====================================================================

def _emit_key_overlap_debug_v1(prev_metrics, cur_metrics, target_key=None):
    try:
        prev_keys = set(prev_metrics.keys()) if isinstance(prev_metrics, dict) else set()
        cur_keys = set(cur_metrics.keys()) if isinstance(cur_metrics, dict) else set()
        overlap = prev_keys.intersection(cur_keys)
        return {
            "prev_count": len(prev_keys),
            "cur_count": len(cur_keys),
            "overlap_count": len(overlap),
            "overlap_sample": list(sorted(overlap))[:10],
            "target_key": target_key,
            "target_present_prev": (target_key in prev_keys) if target_key else None,
            "target_present_cur": (target_key in cur_keys) if target_key else None,
        }
    except Exception as _e:
        return {
            "error": "key_overlap_exception",
            "exception": str(type(_e).__name__),
        }

# =====================================================================
# PATCH END: FIX2D4_key_overlap_debug_v1
# =====================================================================
# =====================================================================
# PATCH FIX2D1 START: Fix fn_missing + Diff Panel V2 summary crash
# - Adds best-effort aliases so FIX41AFC19 display rebuild can find a callable
# - Wraps Diff Panel V2 _impl call so summary is always defined (no UnboundLocalError)
# =====================================================================
def _fix2d1_first_callable_name(candidates):
    try:
        for name in candidates:
            fn = globals().get(name)
            if callable(fn):
                return name
    except Exception:
        return None
    return None

def _fix2d1_find_callable_by_contains(substr, deny_exact=None):
    try:
        for k, v in globals().items():
            if deny_exact and k == deny_exact:
                continue
            if substr in k and callable(v):
                return k
    except Exception:
        return None
    return None

# Ensure canonical rebuild symbol exists under the exact names FIX41AFC19 expects
try:
    if not callable(globals().get("rebuild_metrics_from_snapshots_analysis_canonical_v1")):
        _alt = _fix2d1_first_callable_name([
            "rebuild_metrics_from_snapshots_analysis_canonical_v1_IMPL",
            "rebuild_metrics_from_snapshots_analysis_canonical_v1_base",
            "rebuild_metrics_from_snapshots_analysis_canonical_v1_BASE",
        ]) or _fix2d1_find_callable_by_contains("rebuild_metrics_from_snapshots_analysis_canonical", deny_exact="rebuild_metrics_from_snapshots_analysis_canonical_v1")
        if _alt and callable(globals().get(_alt)):
            globals()["rebuild_metrics_from_snapshots_analysis_canonical_v1"] = globals()[_alt]

    if not callable(globals().get("rebuild_metrics_from_snapshots_schema_only_fix16")):
        _alt = _fix2d1_first_callable_name([
            "rebuild_metrics_from_snapshots_schema_only_fix16_IMPL",
            "rebuild_metrics_from_snapshots_schema_only_fix16_base",
            "rebuild_metrics_from_snapshots_schema_only_fix16_BASE",
        ]) or _fix2d1_find_callable_by_contains("rebuild_metrics_from_snapshots_schema_only_fix16", deny_exact="rebuild_metrics_from_snapshots_schema_only_fix16")
        if _alt and callable(globals().get(_alt)):
            globals()["rebuild_metrics_from_snapshots_schema_only_fix16"] = globals()[_alt]
except Exception:
    pass
# =====================================================================
# PATCH FIX2D1 END (part 1): aliasing
# =====================================================================

# =====================================================================
# PATCH FIX2AF_FETCH_FAILURE_VISIBILITY_AND_PREEMPTIVE_HARDENING_V1 (ADDITIVE)
# - URL shape normalizer (boundary before scraping)
# - Scrape ledger keyed by url_norm w/ stage+reason
# - Scraped text accessor to avoid meta-key drift
# - Fetch-failure visibility (status/textlen/classification)
# =====================================================================

def _fix2af_norm_url(u: str) -> str:
    try:
        s = str(u or "").strip()
        if not s:
            return ""
        _norm = globals().get("_inj_diag_norm_url_list")
        if callable(_norm):
            try:
                out = _norm([s])
                if out and isinstance(out, list):
                    return str(out[0] or "")
            except Exception:
                pass
        if s.startswith("http://"):
            s = "https://" + s[len("http://"):]
        if "#" in s:
            s = s.split("#", 1)[0]
        return s.rstrip("/")
    except Exception:
        return ""

def _fix2af_normalize_url_items(urls):
    diag = {
        "input_type": type(urls).__name__,
        "input_len": 0,
        "flattened_len": 0,
        "string_urls": 0,
        "dict_urls": 0,
        "dropped_non_url": 0,
        "mixed_shape": False,
        "nested_lists": 0,
        "samples_dropped": [],
    }
    out = []
    def _emit(u):
        nu = _fix2af_norm_url(u)
        if nu:
            out.append(nu)
        else:
            diag["dropped_non_url"] += 1
            if len(diag["samples_dropped"]) < 10:
                diag["samples_dropped"].append(str(u)[:200])
    def _walk(x):
        if x is None:
            return
        if isinstance(x, (list, tuple)):
            diag["nested_lists"] += 1
            for y in x:
                _walk(y)
            return
        if isinstance(x, dict):
            diag["dict_urls"] += 1
            for k in ("url", "href", "link"):
                if k in x and x.get(k):
                    _emit(x.get(k))
                    return
            diag["dropped_non_url"] += 1
            if len(diag["samples_dropped"]) < 10:
                diag["samples_dropped"].append(str(x)[:200])
            return
        diag["string_urls"] += 1
        _emit(x)

    try:
        if urls is None:
            diag["input_len"] = 0
        elif isinstance(urls, (list, tuple)):
            diag["input_len"] = len(urls)
            for it in urls:
                _walk(it)
        else:
            diag["input_len"] = 1
            _walk(urls)
    except Exception:
        pass

    diag["flattened_len"] = len(out)
    diag["mixed_shape"] = (diag["dict_urls"] > 0 and diag["string_urls"] > 0)

    seen = set()
    dedup = []
    for u in out:
        if u in seen:
            continue
        seen.add(u)
        dedup.append(u)
    return dedup, diag

def _fix2af_scraped_text_accessor(x):
    try:
        if x is None:
            return ""
        if isinstance(x, str):
            return x
        if isinstance(x, bytes):
            try:
                return x.decode("utf-8", errors="ignore")
            except Exception:
                return ""
        if isinstance(x, dict):
            for k in ("text", "clean_text", "content", "body", "html"):
                v = x.get(k)
                if isinstance(v, str) and v.strip():
                    return v
                if isinstance(v, bytes):
                    try:
                        return v.decode("utf-8", errors="ignore")
                    except Exception:
                        pass
            if "data" in x and isinstance(x["data"], dict):
                return _fix2af_scraped_text_accessor(x["data"])
            return ""
        return str(x)
    except Exception:
        return ""

def _fix2af_classify_fetch_failure(status, txt):
    try:
        s = str(status or "").lower()
        tlen = len(txt or "")
        if (not s or s == "success_direct") and tlen > 0:
            return "ok"
        if "timeout" in s:
            return "timeout"
        if "captcha" in s or "forbidden" in s or "blocked" in s or "403" in s:
            return "blocked"
        if "paywall" in s:
            return "paywall"
        if "pdf" in s and (tlen == 0 or "no_text" in s):
            return "pdf_no_text"
        if "no_text" in s or tlen == 0 or "empty" in s:
            return "no_text"
        if "redirect" in s:
            return "redirect"
        if "error" in s or "exception" in s or "fail" in s:
            return "error"

        # PATCH FIX2D58G (ADDITIVE): magnitude/count units + 'sales' implies unit_sales
        # - Some sources label unit counts as "million units" while metric_name is "... Sales ...".
        # - If we see sales language AND a magnitude-like unit, bind to unit_sales.
        try:
            if ("sales" in n or "sold" in n) and any(tok in u for tok in ("million", "billion", "thousand", "units", "unit", "vehicles", "pcs", "pieces")):
                return "unit_sales"
        except Exception:
            return "unknown"
    except Exception:
        return "unknown"

def _fix2af_ledger_put(ledger: dict, url_raw: str, stage: str, reason: str = "", extra: dict = None):
    try:
        if ledger is None:
            return
        u = _fix2af_norm_url(url_raw)
        if not u:
            return
        rec = ledger.get(u) or {"url_norm": u, "stages": []}
        rec["stages"].append({
            "stage": str(stage or ""),
            "reason": str(reason or ""),
            "extra": extra if isinstance(extra, dict) else {},
        })
        ledger[u] = rec
    except Exception:
        pass

_fix2af_last_scrape_ledger = {}
# END PATCH FIX2AF_FETCH_FAILURE_VISIBILITY_AND_PREEMPTIVE_HARDENING_V1
# =====================================================================

# =====================================================================
# PATCH V21_VERSION_BUMP (ADDITIVE): bump CODE_VERSION for audit
# =====================================================================
#CODE_VERSION = 'REFACTOR45'

# =====================================================================
# PATCH V22_VERSION_BUMP (ADDITIVE): bump CODE_VERSION for audit
# =====================================================================
#CODE_VERSION = 'fix41afc19_evo_fix16_anchor_rebuild_override_v1_fix2b_hardwire_v22'
# PATCH FIX41AFC6 (ADD): bump CODE_VERSION to new patch filename
#CODE_VERSION = "REFACTOR39"

# =====================================================================
# PATCH FIX41T (ADDITIVE): bump CODE_VERSION marker for this patched build
# - Purely a version label for debugging/traceability.
# - Does NOT alter runtime logic.
# =====================================================================
#CODE_VERSION = "REFACTOR39"
# =====================================================================
# PATCH FIX41U (ADDITIVE): bump CODE_VERSION marker for this patched build
# =====================================================================
#CODE_VERSION = "REFACTOR39"
# =====================================================================
# PATCH FIX41J (ADD): bump CODE_VERSION to this file version (additive override)
# PATCH FIX40 (ADD): prior CODE_VERSION preserved above
# PATCH FIX33E (ADD): previous CODE_VERSION was: CODE_VERSION = "REFACTOR39"  # PATCH FIX33D (ADD): set CODE_VERSION to filename
# PATCH FIX33D (ADD): previous CODE_VERSION was: CODE_VERSION = "REFACTOR39"
# =====================================================================
# PATCH FINAL (ADDITIVE): end-state single bump label (non-breaking)
# NOTE: We do not overwrite CODE_VERSION to avoid any legacy coupling.
# =====================================================================
# PATCH FIX41AFC18 (ADDITIVE): bump CODE_VERSION to this file version
# =====================================================================
#CODE_VERSION = "REFACTOR39"
# =====================================================================
# Consumers can prefer ENDSTATE_FINAL_VERSION when present.
# =====================================================================
ENDSTATE_FINAL_VERSION = "v7_41_endstate_final_1"
INJ_TRACE_PATCH_VERSION = "fix41q_inj_trace_v1_always_emit"
# =====================================================================

# =====================================================================
# PATCH ES2/ES8/ES9 (ADDITIVE): shared determinism helpers for drift=0
# - Deterministic sorting / tie-breaking helpers
# - Deterministic candidate index builder (anchor_hash -> best candidate)
# - Lightweight schema + universe hashing for convergence checks
# - One-button end-state validation harness (callable)
# NOTE: Additive only; existing logic remains intact.
# =====================================================================
import hashlib as _es_hashlib

def _es_hash_text(s: str) -> str:
    try:
        return _es_hashlib.sha256((s or "").encode("utf-8")).hexdigest()
    except Exception:
        return ""

def _es_stable_sort_key(v):
    """
    Deterministic sort key that never relies on Python's randomized hash().
    Keeps ordering stable across runs for mixed types.
    """
    try:
        if v is None:
            return (0, "")
        if isinstance(v, (int, float)):
            return (1, f"{v:.17g}")
        if isinstance(v, str):
            return (2, v)
        if isinstance(v, bytes):
            return (3, v.decode("utf-8", "ignore"))
        if isinstance(v, dict):
            items = sorted(((str(k), _es_stable_sort_key(vv)) for k, vv in v.items()), key=lambda x: x[0])
            return (4, str(items))
        if isinstance(v, (list, tuple, set)):
            lst = list(v)
            try:
                lst.sort(key=_es_stable_sort_key)
            except Exception:
                pass
                lst = sorted(lst, key=lambda x: str(x))
            return (5, str([_es_stable_sort_key(x) for x in lst]))
        return (9, str(v))
    except Exception:
        return (9, str(v))

def _es_sorted_pairs_from_sources_cache(baseline_sources_cache):
    pairs = []
    for sr in (baseline_sources_cache or []):
        if not isinstance(sr, dict):
            continue
        u = (sr.get("source_url") or sr.get("url") or "").strip()
        fp = (sr.get("source_fingerprint") or sr.get("fingerprint") or sr.get("content_fingerprint") or "").strip()
        if u and fp:
            pairs.append((u, fp))
    pairs.sort(key=lambda t: (t[0], t[1]))
    return pairs

def _es_compute_canonical_universe_hash(primary_metrics_canonical: dict, metric_schema_frozen: dict) -> str:
    try:
        keys = set()
        if isinstance(primary_metrics_canonical, dict):
            keys.update([str(k) for k in primary_metrics_canonical.keys()])
        if isinstance(metric_schema_frozen, dict):
            keys.update([str(k) for k in metric_schema_frozen.keys()])
        return _es_hash_text("|".join(sorted(keys)))
    except Exception:
        return ""

def _es_compute_schema_hash(metric_schema_frozen: dict) -> str:
    """
    Deterministic hash of schema fields that affect numeric comparisons.
    Keeps it lightweight: tolerances + units + scale hints only.
    """
    try:
        if not isinstance(metric_schema_frozen, dict):
            return ""
        rows = []
        for k in sorted(metric_schema_frozen.keys()):
            s = metric_schema_frozen.get(k) or {}
            if not isinstance(s, dict):
                continue
            abs_eps = s.get("abs_eps", s.get("ABS_EPS"))
            rel_eps = s.get("rel_eps", s.get("REL_EPS"))
            unit = s.get("unit") or s.get("units") or ""
            scale = s.get("scale") or s.get("magnitude") or ""
            rows.append(f"{k}::abs={abs_eps}::rel={rel_eps}::unit={unit}::scale={scale}")
        return _es_hash_text("|".join(rows))
    except Exception:
        return ""

def _es_build_candidate_index_deterministic(baseline_sources_cache):
    """
    Deterministically build anchor_hash -> candidate map.
    If multiple candidates share the same anchor_hash, choose the best by a stable
    tie-breaker that prefers:
      - higher anchor_confidence
      - longer context_snippet (more evidence)
      - stable context_hash / numeric value / unit
      - stable source_url
    """
    try:
        buckets = {}
        for sr in (baseline_sources_cache or []):
            if not isinstance(sr, dict):
                continue
            su = sr.get("source_url") or sr.get("url") or ""
            for cand in (sr.get("extracted_numbers") or []):
                if not isinstance(cand, dict):
                    continue
                ah = cand.get("anchor_hash") or cand.get("anchor") or ""
                if not ah:
                    continue
                c2 = dict(cand)
                if "source_url" not in c2:
                    c2["source_url"] = su
                buckets.setdefault(ah, []).append(c2)

        out = {}
        for ah in sorted(buckets.keys()):
            cands = buckets.get(ah) or []
            def _cand_key(c):
                try:
                    conf = c.get("anchor_confidence")
                    conf_key = -(float(conf) if conf is not None else 0.0)
                except Exception:
                    pass
                    conf_key = 0.0
                ctx = (c.get("context_snippet") or c.get("context") or "")
                ctx_len = -len(str(ctx))
                ctx_hash = c.get("context_hash") or ""
                val = c.get("value")
                unit = c.get("unit") or ""
            # PATCH FIX27 (ADDITIVE): Eligibility gate BEFORE scoring.
            # Reject bare-year tokens for non-year metrics when there is no token unit evidence.
            if expected_kind != "year":
                raw_token = (c.get("raw") or "").strip()
                if _fix27_is_bare_year_token(raw_token, c.get("value_norm")) and not _fix27_has_any_unit_evidence(c):
                    continue
            # Typed metrics require explicit token-level unit evidence
            if expected_kind == "currency" and not _fix27_has_currency_evidence(c):
                continue
            if expected_kind == "percent" and not _fix27_has_percent_evidence(c):
                continue
            if expected_kind == "unit" and not _fix27_has_any_unit_evidence(c):
                continue
                su = c.get("source_url") or ""
                return (conf_key, ctx_len, str(ctx_hash), _es_stable_sort_key(val), str(unit), str(su))
            cands_sorted = sorted(cands, key=_cand_key)
            out[ah] = cands_sorted[0] if cands_sorted else None
        return out
    except Exception:
        return {}

def end_state_validation_harness(baseline_analysis: dict, evolution_output: dict, min_stability: float = 99.9) -> dict:
    """
    PATCH ES9 (ADDITIVE): one-button end-state validation (warn-only helper)
    Use this to assert drift=0 on identical inputs.

    Returns a dict with pass/fail booleans and diagnostic fields.
    This does NOT mutate inputs.
    """
    report = {
        "passed": False,
        "checks": {},
        "notes": [],
    }
    try:
        base_prev = baseline_analysis or {}
        evo = evolution_output or {}

        # Snapshot hash
        base_snap = base_prev.get("source_snapshot_hash") or base_prev.get("results", {}).get("source_snapshot_hash")
        evo_snap = evo.get("source_snapshot_hash")

        # Universe + schema hashes
        base_uni = base_prev.get("canonical_universe_hash") or base_prev.get("results", {}).get("canonical_universe_hash")
        base_sch = base_prev.get("schema_hash") or base_prev.get("results", {}).get("schema_hash")
        evo_uni = evo.get("canonical_universe_hash")
        evo_sch = evo.get("schema_hash")

        report["checks"]["snapshot_hash_match"] = bool(base_snap and evo_snap and base_snap == evo_snap)
        report["checks"]["canonical_universe_hash_match"] = bool(base_uni and evo_uni and base_uni == evo_uni)
        report["checks"]["schema_hash_match"] = bool(base_sch and evo_sch and base_sch == evo_sch)

        # Stability threshold (warn-only semantics: "passed" includes match + stability)
        try:
            st = float(evo.get("stability_score") or 0.0)
        except Exception:
            pass
            st = 0.0
        report["checks"]["stability_meets_threshold"] = bool(st + 1e-9 >= float(min_stability))

        # Drift suspicion flag (if your pipeline sets it)
        report["checks"]["drift_suspected_flag_false"] = (evo.get("drift_suspected") is False)

        # Final pass condition
        report["passed"] = (
            report["checks"]["snapshot_hash_match"]
            and report["checks"]["canonical_universe_hash_match"]
            and report["checks"]["schema_hash_match"]
            and report["checks"]["stability_meets_threshold"]
        )

        if not report["passed"]:
            report["notes"].append("If hashes match but stability is low, inspect candidate tie-breaks and ordering.")
    except Exception:
        pass
        report["notes"].append("Validation harness encountered an exception (non-fatal).")
    return report
# =====================================================================

            # =========================


# =========================================================
# GOOGLE SHEETS HISTORY STORAGE
# =========================================================

SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive"
]
MAX_HISTORY_ITEMS = 50

@st.cache_resource
def get_google_sheet():
    """Connect to Google Sheet (cached connection)"""
    try:
        creds = Credentials.from_service_account_info(
            dict(st.secrets["gcp_service_account"]),
            scopes=SCOPES
        )
        client = gspread.authorize(creds)

        # ===================== PATCH GS1 (ADDITIVE): prefer explicit History worksheet =====================
        # Why:
        # - Your spreadsheet contains multiple tabs (e.g., "New Analysis", "History", "HistoryFull", "Snapshots")
        # - sheet1 is often NOT "History", so get_history() reads the wrong tab and sees "no analyses"
        # Behavior:
        # - Default worksheet_title = "History" (override via secrets: google_sheets.history_worksheet)
        # - Fallback to sheet1 only if the worksheet doesn't exist
        spreadsheet_name = (
            st.secrets.get("google_sheets", {}).get("spreadsheet_name", "Yureeka_JSON")
        )
        ss = client.open(spreadsheet_name)

        worksheet_title = st.secrets.get("google_sheets", {}).get("history_worksheet", "History")
        try:
            sheet = ss.worksheet(worksheet_title)
        except Exception:
            pass
            sheet = ss.sheet1
        # =================== END PATCH GS1 (ADDITIVE) ===================

        # Ensure headers exist - handle response object
        try:
            headers = sheet.row_values(1)
            if not headers or len(headers) == 0 or headers[0] != "id":
                # update() returns a response object in newer gspread - ignore it
                _ = sheet.update('A1:E1', [["id", "timestamp", "question", "confidence", "data"]])
        except gspread.exceptions.APIError:
            _ = sheet.update('A1:E1', [["id", "timestamp", "question", "confidence", "data"]])
        except Exception:
            pass  # Headers probably already exist

        return sheet

    except gspread.exceptions.SpreadsheetNotFound:
        st.error("❌ Spreadsheet not found. Create 'Yureeka_JSON' (or your configured name) and share with service account.")
        return None
    except Exception as e:
        error_str = str(e)
        # Ignore Response [200] - it's actually success
        if "Response [200]" in error_str:
            # This means the connection worked, try to return the sheet anyway
            try:
                creds = Credentials.from_service_account_info(
                    dict(st.secrets["gcp_service_account"]),
                    scopes=SCOPES
                )
                client = gspread.authorize(creds)

                # ===================== PATCH GS1b (ADDITIVE): same worksheet selection in fallback =====================
                spreadsheet_name = st.secrets.get("google_sheets", {}).get("spreadsheet_name", "Yureeka_JSON")
                ss = client.open(spreadsheet_name)
                worksheet_title = st.secrets.get("google_sheets", {}).get("history_worksheet", "History")
                try:
                    return ss.worksheet(worksheet_title)
                except Exception:
                    return ss.sheet1
                # =================== END PATCH GS1b (ADDITIVE) ===================
            except:
                pass
        st.error(f"❌ Failed to connect to Google Sheets: {e}")
        return None

def generate_analysis_id() -> str:
    """Generate unique ID for analysis"""
    return f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hashlib.md5(str(datetime.now().timestamp()).encode()).hexdigest()[:6]}"


# =====================================================================
# PATCH AI_A (ADDITIVE): emit metric_anchors in analysis payload (analysis-time)
# Why:
# - Evolution/diff are now anchor-driven; analysis must persist a deterministic
#   canonical_key -> anchor_hash mapping for drift=0 convergence.
# - Some UI/Sheets wrappers omit anchors unless explicitly emitted.
# Determinism:
# - Only uses existing evidence/candidates already present in the analysis payload.
# - No re-fetching; no heuristic matching.
# =====================================================================
def _emit_metric_anchors_in_analysis_payload(analysis_obj: dict) -> dict:
    try:
        if not isinstance(analysis_obj, dict):
            return analysis_obj

        # If already present and non-empty, keep as-is
        existing = analysis_obj.get("metric_anchors")
        if isinstance(existing, dict) and existing:
            return analysis_obj

        # Identify the primary response container (some payloads store it nested)
        pr = analysis_obj.get("primary_response") if isinstance(analysis_obj.get("primary_response"), dict) else None
        pr = pr or analysis_obj

        # Canonical metrics (preferred)
        pmc = pr.get("primary_metrics_canonical") if isinstance(pr, dict) else None
        if not isinstance(pmc, dict) or not pmc:
            pmc = analysis_obj.get("primary_metrics_canonical") if isinstance(analysis_obj.get("primary_metrics_canonical"), dict) else {}

        # Candidate lookup table from baseline snapshots (if present)
        bsc = None
        try:
            r = analysis_obj.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache"), list):
                bsc = r.get("baseline_sources_cache")
        except Exception:
            pass
            bsc = None
        if bsc is None and isinstance(analysis_obj.get("baseline_sources_cache"), list):
            bsc = analysis_obj.get("baseline_sources_cache")

        def _safe_str(x):
            try:
                return str(x).strip()
            except Exception:
                return ""

        # Build (anchor_hash -> best candidate) index deterministically
        anchor_to_candidate = {}
        cand_to_candidate = {}
        try:
            if isinstance(bsc, list):
                for sr in bsc:
                    if not isinstance(sr, dict):
                        continue
                    surl = sr.get("source_url") or sr.get("url")
                    for n in (sr.get("extracted_numbers") or []):
                        if not isinstance(n, dict):
                            continue
                        ah = _safe_str(n.get("anchor_hash"))
                        cid = _safe_str(n.get("candidate_id"))
                        if ah and ah not in anchor_to_candidate:
                            anchor_to_candidate[ah] = dict(n, source_url=n.get("source_url") or surl)
                        if cid and cid not in cand_to_candidate:
                            cand_to_candidate[cid] = dict(n, source_url=n.get("source_url") or surl)
        except Exception:
            pass

        metric_anchors = {}

        # Deterministic iteration for stable JSON output
        for ckey in sorted([str(k) for k in (pmc or {}).keys()]):
            m = pmc.get(ckey)
            if not isinstance(m, dict):
                continue

            # Pick anchor identifiers from evidence first (most authoritative)
            ev = m.get("evidence") or []
            if not isinstance(ev, list):
                ev = []

            best = None
            for e in ev:
                if not isinstance(e, dict):
                    continue
                ah = _safe_str(e.get("anchor_hash") or e.get("anchor"))
                cid = _safe_str(e.get("candidate_id"))
                if ah or cid:
                    best = e
                    break

            # Fallback: sometimes metric row carries anchor_hash directly
            if best is None:
                best = {
                    "anchor_hash": m.get("anchor_hash") or m.get("anchor"),
                    "candidate_id": m.get("candidate_id"),
                    "source_url": m.get("source_url") or m.get("url"),
                    "context_snippet": m.get("context_snippet") or m.get("context"),
                    "anchor_confidence": m.get("anchor_confidence"),
                }

            ah = _safe_str(best.get("anchor_hash") or best.get("anchor"))
            cid = _safe_str(best.get("candidate_id"))
            surl = best.get("source_url") or best.get("url")
            ctx = best.get("context_snippet") or best.get("context")
            aconf = best.get("anchor_confidence")

            # Enrich from candidate index if needed
            if (not surl) or (not ctx):
                cand = None
                if ah and ah in anchor_to_candidate:
                    cand = anchor_to_candidate.get(ah)
                elif cid and cid in cand_to_candidate:
                    cand = cand_to_candidate.get(cid)
                if isinstance(cand, dict):
                    surl = surl or (cand.get("source_url") or cand.get("url"))
                    ctx = ctx or (cand.get("context_snippet") or cand.get("context"))

            # Only emit if we actually have an anchor id
            if not (ah or cid):
                continue

            try:
                if isinstance(ctx, str):
                    ctx = ctx.strip()[:220]
                else:
                    ctx = None
            except Exception:
                pass
                ctx = None

            try:
                aconf = float(aconf) if aconf is not None else None
            except Exception:
                pass
                aconf = None

            metric_anchors[ckey] = {
                "canonical_key": ckey,
                "anchor_hash": ah or None,
                "candidate_id": cid or None,
                "source_url": surl or None,
                "context_snippet": ctx,
                "anchor_confidence": aconf,
            }

            # -----------------------------------------------------------------
            # PATCH AI_B (ADDITIVE): also backfill anchor fields onto the metric row
            # -----------------------------------------------------------------
            try:
                if ah and not _safe_str(m.get("anchor_hash")):
                    m["anchor_hash"] = ah
                if cid and not _safe_str(m.get("candidate_id")):
                    m["candidate_id"] = cid
                if surl and not (m.get("source_url") or m.get("url")):
                    m["source_url"] = surl
                if ctx and not (m.get("context_snippet") or m.get("context")):
                    m["context_snippet"] = ctx
                if aconf is not None and m.get("anchor_confidence") is None:
                    m["anchor_confidence"] = aconf
            except Exception:
                pass
            # -----------------------------------------------------------------

        if metric_anchors:
            # -----------------------------------------------------------------
            # PATCH AI_C (ADDITIVE): persist in all common locations
            # -----------------------------------------------------------------
            try:
                analysis_obj["metric_anchors"] = metric_anchors
            except Exception:
                pass
            try:
                if isinstance(pr, dict):
                    pr.setdefault("metric_anchors", metric_anchors)
            except Exception:
                pass
            try:
                analysis_obj.setdefault("results", {})
                if isinstance(analysis_obj["results"], dict):
                    analysis_obj["results"].setdefault("metric_anchors", metric_anchors)
            except Exception:
                pass
            # -----------------------------------------------------------------

        return analysis_obj
    except Exception:
        return analysis_obj
# =====================================================================

def add_to_history(analysis: dict) -> bool:
    """
    Save analysis to Google Sheet (or session fallback).

    ADDITIVE end-state wiring:
      - If a baseline source cache exists, build & store:
          * evidence_records (structured, cached)
          * metric_anchors (baseline metrics anchored to evidence)
      - Prevent Google Sheets 50,000-char single-cell limit errors by shrinking only
        the JSON payload written into the single "analysis json" cell when necessary.

    Backward compatible:
      - Only adds keys; does not remove existing fields.
      - Never blocks saving if enrichment fails.
      - If Sheets unavailable, falls back to session_state.
    """

    # REFACTOR36: harden against None callers (prevents NoneType.get crashes)
    if not isinstance(analysis, dict):
        return False


    # =====================================================================
    # PATCH AI_A_CALL (ADDITIVE): ensure metric_anchors emitted before persistence
    # =====================================================================
    try:
        analysis = _emit_metric_anchors_in_analysis_payload(analysis)
    except Exception:
        pass
    # =====================================================================


    # =====================================================================
    import json
    import re
    import streamlit as st
    from datetime import datetime

    SHEETS_CELL_LIMIT = 50000

    # -----------------------
    # PATCH A1 (ADDITIVE): robustly locate baseline_sources_cache
    # - Added primary_response.baseline_sources_cache as extra fallback
    # -----------------------
    baseline_cache = (
        analysis.get("baseline_sources_cache")
        or (analysis.get("primary_response", {}) or {}).get("baseline_sources_cache")
        or (analysis.get("results", {}) or {}).get("baseline_sources_cache")
        or (analysis.get("results", {}) or {}).get("source_results")
    )

    # -----------------------
    # PATCH A2 (ADDITIVE): build evidence_records deterministically
    # -----------------------
    def _build_evidence_records_from_baseline_cache(baseline_cache_obj):
        records = []
        if not isinstance(baseline_cache_obj, list):
            return records

        # helper: safe sha1 fallback if needed
        def _sha1(s: str) -> str:
            try:
                import hashlib
                return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()
            except Exception:
                return ""

        for sr in baseline_cache_obj:
            if not isinstance(sr, dict):
                continue
            url = sr.get("url") or ""
            fp = sr.get("fingerprint")
            fetched_at = sr.get("fetched_at")

            nums = sr.get("extracted_numbers") or []
            clean_nums = []

            if isinstance(nums, list):
                for n in nums:
                    if not isinstance(n, dict):
                        continue

                    # optional canonicalization hook
                    try:
                        fn = globals().get("canonicalize_numeric_candidate")
                        if callable(fn):
                            n = fn(dict(n))
                    except Exception:
                        pass
                        n = dict(n)

                    raw = (n.get("raw") or "").strip()
                    ctx = (n.get("context_snippet") or n.get("context") or "").strip()
                    anchor_hash = n.get("anchor_hash") or _sha1(f"{url}|{raw}|{ctx[:240]}")

                    clean_nums.append({
                        "value": n.get("value"),
                        "unit": n.get("unit"),
                        "unit_tag": n.get("unit_tag"),
                        "unit_family": n.get("unit_family"),
                        "base_unit": n.get("base_unit"),
                        "multiplier_to_base": n.get("multiplier_to_base"),
                        "value_norm": n.get("value_norm"),

                        "raw": raw,
                        "context_snippet": ctx[:240],
                        "anchor_hash": anchor_hash,
                "candidate_id": hashlib.sha1(str(anchor_hash or "").encode("utf-8")).hexdigest()[:16] if anchor_hash else None,

            # =====================================================================
            # PATCH AI2 (ADDITIVE): anchor integrity fields
            # - candidate_id is a stable short id derived from anchor_hash
            # - anchor_basis documents what the anchor_hash was built from
            # =====================================================================
            "candidate_id": (str(anchor_hash)[:16] if anchor_hash else None),
            "anchor_basis": "url|raw|context",
            # =====================================================================
                        "source_url": n.get("source_url") or url,

                        "start_idx": n.get("start_idx"),
                        "end_idx": n.get("end_idx"),

                        "is_junk": bool(n.get("is_junk")) if isinstance(n.get("is_junk"), bool) else False,
                        "junk_reason": n.get("junk_reason") or "",

                        "measure_kind": n.get("measure_kind"),
                        "measure_assoc": n.get("measure_assoc"),
                    })

            # stable ordering (prefer your helper if present)
            try:
                if "sort_snapshot_numbers" in globals() and callable(globals()["sort_snapshot_numbers"]):
                    clean_nums = sort_snapshot_numbers(clean_nums)
                else:
                    clean_nums = sorted(
                        clean_nums,
                        key=lambda x: (str(x.get("anchor_hash") or ""), str(x.get("raw") or ""))
                    )
            except Exception:
                pass

            records.append({
                "url": url,
                "fetched_at": fetched_at,
                "fingerprint": fp,
                "numbers": clean_nums,
            })

        # stable ordering (prefer helper if present)
        try:
            if "sort_evidence_records" in globals() and callable(globals()["sort_evidence_records"]):
                records = sort_evidence_records(records)
            else:
                records = sorted(records, key=lambda r: str(r.get("url") or ""))
        except Exception:
            return records

    # -----------------------
    # PATCH A3 (ADDITIVE): build metric_anchors deterministically (schema-first if present)
    # -----------------------
    def _build_metric_anchors(primary_metrics_canonical, evidence_records):
        """
        Build a deterministic metric_anchors mapping for drift=0.

        PATCH AI1 (ADDITIVE): Anchor integrity
        - Prefer the anchor_hash/candidate_id already chosen during analysis (metric["evidence"]).
        - Fall back to scanning evidence_records for a candidate with the same anchor_hash/candidate_id.
        - As a last resort, pick a best candidate deterministically by (abs(value_norm-target), context length).
        - NEVER invent anchors; if no usable candidate, omit the anchor for that metric.
        """
        anchors = {}
        if not isinstance(primary_metrics_canonical, dict) or not primary_metrics_canonical:
            return anchors
        if not isinstance(evidence_records, list):
            evidence_records = []

        import hashlib

        def _sha1(s: str) -> str:
            try:
                return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()
            except Exception:
                return ""

        def _ensure_anchor_fields(c: dict, source_url: str = "") -> dict:
            c = c if isinstance(c, dict) else {}
            # context snippet normalization
            ctx = c.get("context_snippet") or c.get("context") or ""
            if isinstance(ctx, str):
                ctx = ctx.strip()[:240]
            else:
                ctx = ""
            raw = c.get("raw")
            if raw is None:
                # stable raw representation
                v = c.get("value_norm") if c.get("value_norm") is not None else c.get("value")
                u = c.get("base_unit") or c.get("unit") or ""
                raw = f"{v}{u}"
            raw = str(raw)[:120]

            ah = c.get("anchor_hash") or c.get("anchor") or ""
            if not ah:
                ah = _sha1(f"{source_url}|{raw}|{ctx}")
                if ah:
                    c["anchor_hash"] = ah

            if not c.get("candidate_id") and ah:
                c["candidate_id"] = str(ah)[:16]

            # keep normalized ctx/source_url for downstream
            if source_url and not c.get("source_url"):
                c["source_url"] = source_url
            if ctx and not c.get("context_snippet"):
                c["context_snippet"] = ctx

            return c

        # Pre-index evidence_records by (anchor_hash, candidate_id)
        anchor_index = {}
        candidate_index = {}
        value_index = {}  # ckey -> list of candidates (for fallback)

        for rec in evidence_records:
            if not isinstance(rec, dict):
                continue
            url = rec.get("source_url") or rec.get("url") or ""
            for c in (rec.get("candidates") or rec.get("extracted_numbers") or []):
                if not isinstance(c, dict):
                    continue
                c = _ensure_anchor_fields(c, url)
                ah = c.get("anchor_hash")
                cid = c.get("candidate_id")
                if ah and ah not in anchor_index:
                    anchor_index[ah] = c
                if cid and cid not in candidate_index:
                    candidate_index[cid] = c

        # Determine anchors per metric
        for ckey, m in primary_metrics_canonical.items():
            if not isinstance(m, dict):
                continue

            # --- Preferred: use the analysis-chosen evidence (integrity) ---
            chosen = None
            ev = m.get("evidence") or []
            if isinstance(ev, list) and ev:
                # pick first usable evidence deterministically
                for e in ev:
                    if not isinstance(e, dict):
                        continue
                    url = e.get("source_url") or e.get("url") or ""
                    e2 = _ensure_anchor_fields(dict(e), url)
                    ah = e2.get("anchor_hash")
                    cid = e2.get("candidate_id")
                    if ah or cid:
                        chosen = e2
                        break

            # --- Fallback 1: resolve by anchor_hash/candidate_id in evidence_records ---
            if isinstance(chosen, dict):
                ah = chosen.get("anchor_hash")
                cid = chosen.get("candidate_id")
                if ah and ah in anchor_index:
                    chosen = dict(anchor_index[ah])
                elif cid and cid in candidate_index:
                    chosen = dict(candidate_index[cid])

            # --- Fallback 2: deterministic best-by-value in the same source_url (if any) ---
            if not isinstance(chosen, dict) or not (chosen.get("anchor_hash") or chosen.get("candidate_id")):
                # gather candidates from evidence_records that match the metric's preferred source_url (if known)
                preferred_url = ""
                try:
                    if isinstance(ev, list) and ev:
                        preferred_url = str((ev[0] or {}).get("source_url") or (ev[0] or {}).get("url") or "")
                except Exception:
                    pass
                    preferred_url = ""

                target = m.get("value_norm")
                try:
                    target = float(target) if target is not None else None
                except Exception:
                    pass
                    target = None

                pool = []
                for rec in evidence_records:
                    if not isinstance(rec, dict):
                        continue
                    url = str(rec.get("source_url") or rec.get("url") or "")
                    if preferred_url and url != preferred_url:
                        continue
                    for c in (rec.get("candidates") or rec.get("extracted_numbers") or []):
                        if not isinstance(c, dict):
                            continue
                        cc = _ensure_anchor_fields(dict(c), url)
                        pool.append(cc)

                if pool:
                    def _score(cc):
                        ctx = cc.get("context_snippet") or ""
                        try:
                            v = cc.get("value_norm")
                            v = float(v) if v is not None else None
                        except Exception:
                            pass
                            v = None
                        dv = abs(v - target) if (v is not None and target is not None) else 1e30
                        return (dv, -len(str(ctx)), str(cc.get("anchor_hash") or ""), str(url))
                    pool.sort(key=_score)
                    chosen = pool[0]

            if not isinstance(chosen, dict):
                continue

            # emit anchor record (stable shape)
            anchors[ckey] = {
                "canonical_key": ckey,
                "anchor_hash": chosen.get("anchor_hash"),
            # =====================================================================
# PATCH AI3 (ADDITIVE): anchor integrity fingerprint (analysis-time)
# Why:
# - Provides a stable, inspectable signature tying the anchor to a specific
#   candidate (url + anchor_hash + value_norm + base_unit).
# - Helps detect silent anchor drift across analysis/evolution.
# =====================================================================
"anchor_integrity": {
    "candidate_id": chosen.get("candidate_id"),
    "value_norm": chosen.get("value_norm"),
    "base_unit": chosen.get("base_unit") or chosen.get("unit"),
    "fingerprint": chosen.get("fingerprint"),
    "integrity_hash": _es_hash_text(
        f"{ckey}|{chosen.get('anchor_hash')}|{chosen.get('source_url') or chosen.get('url') or ''}|{chosen.get('value_norm')}|{chosen.get('base_unit') or chosen.get('unit') or ''}"
    ) if callable(globals().get("_es_hash_text")) else None,
},
# =====================================================================
                "candidate_id": chosen.get("candidate_id"),
                "source_url": chosen.get("source_url") or chosen.get("url"),
                "context_snippet": chosen.get("context_snippet") or chosen.get("context"),
                "anchor_confidence": chosen.get("anchor_confidence") or chosen.get("confidence"),
            }

        # deterministic ordering (stable JSON)
        try:
            anchors = dict(sorted(anchors.items(), key=lambda kv: str(kv[0])))
        except Exception:
            return anchors
        def _tokenize(s: str):
            return [t for t in re.findall(r"[a-z0-9]+", (s or "").lower()) if len(t) > 2]

        # PATCH A3.1 (ADDITIVE): tiny float helper for deterministic closeness scoring
        def _to_float(x):
            try:
                return float(x)
            except Exception:
                return None

        # PATCH A3.9 (ADDITIVE): currency evidence helper
        # - Needed because many currency metrics appear as magnitude-tagged numbers (e.g., "40.7M")
        #   with currency implied in nearby context ("USD", "revenue", "$", etc.)
        def _has_currency_evidence(raw: str, ctx: str) -> bool:
            r = (raw or "")
            c = (ctx or "").lower()
            if any(s in r for s in ["$", "S$", "€", "£"]):
                return True
            if any(code in c for code in [" usd", "sgd", " eur", " gbp", " aud", " cad", " jpy", " cny", " rmb"]):
                return True
            strong_kw = [
                "revenue", "turnover", "valuation", "valued at", "market value", "market size",
                "sales value", "net profit", "operating profit", "gross profit",
                "ebitda", "earnings", "income", "capex", "opex"
            ]
            if any(k in c for k in strong_kw):
                return True
            return False
        # =========================

        # flatten candidates
        all_nums = []
        for rec in evidence_records:
            if not isinstance(rec, dict):
                continue
            for n in (rec.get("numbers") or []):
                if isinstance(n, dict):
                    all_nums.append(n)

        # PATCH A3.2 (ADDITIVE): normalize_unit_tag + unit_family hooks (if present)
        _norm_tag_fn = globals().get("normalize_unit_tag")
        _unit_family_fn = globals().get("unit_family")

        for ckey, m in primary_metrics_canonical.items():
            if not isinstance(m, dict):
                continue

            schema = (metric_schema_frozen or {}).get(ckey) if isinstance(metric_schema_frozen, dict) else None
            expected_family = (schema.get("unit_family") or "").lower().strip() if isinstance(schema, dict) else ""
            expected_unit = (schema.get("unit") or "").strip() if isinstance(schema, dict) else ""
            expected_dim = (schema.get("dimension") or "").lower().strip() if isinstance(schema, dict) else ""

            # tokens: schema keywords + metric name tokens
            toks = []
            if isinstance(schema, dict):
                for k in (schema.get("keywords") or []):
                    toks.extend(_tokenize(str(k)))
            toks.extend(_tokenize(m.get("name") or m.get("original_name") or ""))
            toks = list(dict.fromkeys(toks))[:40]

            best = None
            best_key = None

            # PATCH A3.3 (ADDITIVE): metric value reference for closeness bonus
            m_val = _to_float(m.get("value_norm") if m.get("value_norm") is not None else m.get("value"))

            # PATCH A3.4 (ADDITIVE): normalized expected tag (schema unit may be "M", "%", etc.)
            exp_tag = expected_unit
            try:
                if callable(_norm_tag_fn):
                    exp_tag = _norm_tag_fn(expected_unit)
            except Exception:
                pass

            # PATCH A3.10 (ADDITIVE): metric unit_tag (if available) to gate closeness bonus
            m_tag = (m.get("unit_tag") or "").strip()

            for cand in all_nums:
                if cand.get("is_junk") is True:
                    continue

                ctx = cand.get("context_snippet") or ""
                c_ut = (cand.get("unit_tag") or "").strip()
                c_fam = (cand.get("unit_family") or "").lower().strip()

                # =========================
                # PATCH A3.5 (ADDITIVE): derive candidate family if missing
                # - prevents leakage when unit_family wasn't populated upstream
                # =========================
                if not c_fam:
                    try:
                        if callable(_unit_family_fn):
                            c_fam = str(_unit_family_fn(c_ut or "") or "").lower().strip()
                    except Exception:
                        pass
                # =========================

                # =========================
                # PATCH A3.7 (ADDITIVE): prefer unit_tag matching (normalized) over raw unit matching
                # PATCH A3.11 (ADDITIVE): extend normalization fallback to raw/context
                # - helps older snapshots where unit_tag/unit may be empty but raw/context carries scale ("million", "%")
                # =========================
                cand_tag = c_ut
                try:
                    if callable(_norm_tag_fn):
                        cand_tag = _norm_tag_fn(c_ut or cand.get("unit") or cand.get("raw") or ctx)
                except Exception:
                    pass
                # =========================

                # =========================
                # PATCH A3.6 (FIX): schema-first family gate with currency exception
                # - Currency metrics often appear as magnitude candidates ("40.7M") + currency evidence in context.
                # - We allow cand_fam == "magnitude" for expected_family == "currency" ONLY when currency evidence exists.
                # =========================
                if expected_family in ("percent", "currency", "magnitude", "energy"):
                    if expected_family == "currency":
                        if c_fam not in ("currency", "magnitude"):
                            continue
                        if c_fam == "magnitude" and not _has_currency_evidence(cand.get("raw", ""), ctx):
                            continue
                    else:
                        if (c_fam or "") != expected_family:
                            continue
                # =========================

                # dimension/meaning gate using measure_kind when present (soft but helpful)
                mk = cand.get("measure_kind")
                if expected_dim == "percent" and mk and mk not in ("share_pct", "growth_pct", "percent_other"):
                    continue
                if expected_dim == "currency" and mk and mk == "count_units":
                    continue

                c_tokens = set(_tokenize(ctx))
                overlap = sum(1 for t in toks if t in c_tokens) if toks else 0
                score = overlap / max(1, len(toks))

                bonus = 0.0

                # =========================
                # PATCH A3.7 (ADDITIVE): tag-based unit bonus (stronger)
                # =========================
                if exp_tag and cand_tag and cand_tag == exp_tag:
                    bonus += 0.07
                # keep a small legacy bonus if exact unit string matches too
                if expected_unit and (str(cand.get("unit") or "").strip() == expected_unit):
                    bonus += 0.03
                # =========================

                # =========================
                # PATCH A3.8 (ADDITIVE): deterministic value closeness bonus (guarded)
                # - Only apply when units are comparable (tag match or both use value_norm).
                # - Prevents misleading closeness when one side is normalized and the other isn't.
                # =========================
                c_val = _to_float(cand.get("value_norm") if cand.get("value_norm") is not None else cand.get("value"))
                comparable = False
                if m_tag and cand_tag and m_tag == cand_tag:
                    comparable = True
                elif (m.get("value_norm") is not None) and (cand.get("value_norm") is not None):
                    comparable = True

                if comparable and m_val is not None and c_val is not None:
                    denom = max(1e-9, abs(m_val))
                    rel_err = abs(c_val - m_val) / denom
                    if rel_err <= 0.02:
                        bonus += 0.06
                    elif rel_err <= 0.10:
                        bonus += 0.03
                # =========================

                score = float(score + bonus)

                # stable tie-breaker
                key = (
                    score,
                    str(cand.get("source_url") or ""),
                    str(cand.get("anchor_hash") or ""),
                    str(cand.get("raw") or ""),
                )

                if best_key is None or key > best_key:
                    best_key = key
                    best = cand

            if best and best_key and best_key[0] >= 0.10:
                anchors[ckey] = {
                    # =========================
                    # PATCH MA1 (ADDITIVE): legacy compat fields
                    # =========================
                    "metric_id": ckey,
                    "metric_name": (m.get("name") or m.get("original_name") or ckey),
                    # =========================

                    "canonical_key": ckey,
                    "anchor_hash": best.get("anchor_hash"),
                    "source_url": best.get("source_url"),
                    "raw": best.get("raw"),
                    "unit": best.get("unit"),
                    "unit_tag": best.get("unit_tag"),
                    "unit_family": best.get("unit_family"),
                    "base_unit": best.get("base_unit"),
                    "value": best.get("value"),
                    "value_norm": (best.get("value") if best.get("value") is not None else best.get("value_norm")),
                    "measure_kind": best.get("measure_kind"),
                    "measure_assoc": best.get("measure_assoc"),
                    "context_snippet": (best.get("context_snippet") or "")[:220],
                    "anchor_confidence": float(min(100.0, best_key[0] * 100.0)),

                    # =========================
                    # PATCH A3.12 (ADDITIVE): optional fingerprint passthrough (if present)
                    # - Useful later for evolution/debugging; harmless if missing.
                    # =========================
                    "fingerprint": best.get("fingerprint"),
                    # =========================
                }
            else:
                anchors[ckey] = {
                    # =========================
                    # PATCH MA1 (ADDITIVE): legacy compat fields
                    # =========================
                    "metric_id": ckey,
                    "metric_name": (m.get("name") or m.get("original_name") or ckey),
                    # =========================

                    "canonical_key": ckey,
                    "anchor_hash": None,
                    "source_url": None,
                    "raw": None,
                    "anchor_confidence": 0.0,

                    # PATCH A3.12 (ADDITIVE): keep key present for stable shape
                    "fingerprint": None,
                }

        # stable ordering (prefer helper if present)
        try:
            if "sort_metric_anchors" in globals() and callable(globals()["sort_metric_anchors"]):
                ordered = sort_metric_anchors(list(anchors.values()))
                anchors = {
                    a.get("canonical_key"): a
                    for a in ordered
                    if isinstance(a, dict) and a.get("canonical_key")
                }
        except Exception:
            pass

        # =====================================================================
        # PATCH AI_ANCHHASH1 (ADDITIVE): propagate anchor_hash into metric rows
        # Why:
        # - Drift=0 requires prev metrics to carry anchor_hash so diff can compare
        #   prev_anchor_hash vs cur_anchor_hash deterministically.
        # - We ONLY copy existing anchor_hash from anchors map; no fabrication.
        # =====================================================================
        try:
            if isinstance(primary_metrics_canonical, dict) and isinstance(anchors, dict):
                for _ck, _a in anchors.items():
                    if not isinstance(_a, dict):
                        continue
                    _ah = _a.get("anchor_hash") or _a.get("anchor")
                    if not _ah:
                        continue
                    _mrow = primary_metrics_canonical.get(_ck)
                    if isinstance(_mrow, dict) and not _mrow.get("anchor_hash"):
                        _mrow["anchor_hash"] = _ah
        except Exception:
            pass
        # =====================================================================
        return anchors

    # -----------------------
    # PATCH A4 (ADDITIVE): enrich analysis (never block saving)
    # -----------------------
    try:
        if isinstance(baseline_cache, list) and baseline_cache:
            evidence_records = _build_evidence_records_from_baseline_cache(baseline_cache)

            # =========================
            # PATCH A4.1 (ADDITIVE): evidence layer versioning (pipeline attribution)
            # - Use CODE_VERSION if available; else keep numeric fallback
            # =========================
            try:
                cv = globals().get("CODE_VERSION")
                analysis.setdefault("evidence_layer_version", cv or 1)
            except Exception:
                pass
                analysis.setdefault("evidence_layer_version", 1)
            analysis.setdefault("evidence_layer_schema_version", 1)
            # =========================

            # stash on analysis (additive)
            analysis["evidence_records"] = evidence_records

            # build anchors using canonical metrics + frozen schema if present
            primary_resp = analysis.get("primary_response") or {}
            if isinstance(primary_resp, dict):
                pmc = primary_resp.get("primary_metrics_canonical") or analysis.get("primary_metrics_canonical") or {}
                schema = primary_resp.get("metric_schema_frozen") or analysis.get("metric_schema_frozen") or {}
            else:
                pmc = analysis.get("primary_metrics_canonical") or {}
                schema = analysis.get("metric_schema_frozen") or {}

            metric_anchors = _build_metric_anchors(pmc, schema, evidence_records)
            # =====================================================================
            # PATCH ANCH_EMIT1 (ADDITIVE): emit metric_anchors into analysis payload
            # Why:
            # - Evolution (and diff) expects anchors to be discoverable without guessing.
            # - Some storage paths wrap/summarize analysis objects; we persist anchors
            #   at multiple stable locations to survive those wrappers.
            # Determinism:
            # - Anchors are derived only from existing evidence_records / schema / pmc.
            # - No re-fetching; no heuristic matching.
            # =====================================================================
            try:
                if isinstance(metric_anchors, dict) and metric_anchors:
                    # Top-level (preferred)
                    if not isinstance(analysis.get("metric_anchors"), dict):
                        analysis["metric_anchors"] = metric_anchors

                    # Under primary_response (common for older shapes)
                    pr = analysis.get("primary_response")
                    if isinstance(pr, dict) and not isinstance(pr.get("metric_anchors"), dict):
                        pr["metric_anchors"] = metric_anchors

                    # Under results (some evolution lookups)
                    res = analysis.get("results")
                    if isinstance(res, dict) and not isinstance(res.get("metric_anchors"), dict):
                        res["metric_anchors"] = metric_anchors

                    # Lightweight debug hint for wrappers
                    try:
                        dbg = analysis.get("debug")
                        if not isinstance(dbg, dict):
                            dbg = {}
                            analysis["debug"] = dbg
                        dbg.setdefault("metric_anchor_count", int(len(metric_anchors)))
                    except Exception:
                        pass
            except Exception:
                pass
            # =====================================================================

            analysis["metric_anchors"] = metric_anchors
            # =====================================================================
            # =====================================================================
            # PATCH AI4 (ADDITIVE): anchor integrity audit (analysis-time)
            # Why:
            # - Detect duplicate anchor_hash values across canonical keys.
            # - Detect missing anchor_hash on anchors and on baseline canonical metrics.
            # - Provide non-breaking debug/audit fields for drift investigations.
            # Notes:
            # - Purely additive; does not mutate anchors beyond attaching audit metadata.
            # =====================================================================
            try:
                if isinstance(analysis, dict):
                    _ma = analysis.get('metric_anchors')
                    _pmc = analysis.get('primary_metrics_canonical')
                    _dup = {}  # anchor_hash -> [canonical_key,...]
                    _missing_anchor = []
                    _missing_metric_anchor = []
                    if isinstance(_ma, dict):
                        for _ck, _a in _ma.items():
                            if not isinstance(_a, dict):
                                _missing_anchor.append(str(_ck))
                                continue
                            _ah = _a.get('anchor_hash') or _a.get('anchor')
                            if not _ah:
                                _missing_anchor.append(str(_ck))
                                continue
                            _ah = str(_ah)
                            _dup.setdefault(_ah, []).append(str(_ck))
                    # anchors missing on baseline metrics (best-effort diagnostic)
                    if isinstance(_pmc, dict):
                        for _ck, _m in _pmc.items():
                            if not isinstance(_m, dict):
                                continue
                            if not (_m.get('anchor_hash') or _m.get('anchor') or _m.get('candidate_id')):
                                _missing_metric_anchor.append(str(_ck))
                    _dup_only = {k: v for k, v in _dup.items() if isinstance(v, list) and len(v) > 1}
                    analysis['anchor_integrity_audit'] = {
                        'anchor_count': int(len(_ma)) if isinstance(_ma, dict) else 0,
                        'duplicate_anchor_hash_count': int(len(_dup_only)),
                        'duplicate_anchor_hash_examples': dict(list(_dup_only.items())[:10]) if _dup_only else {},
                        'missing_anchor_hash_count': int(len(_missing_anchor)),
                        'missing_anchor_hash_examples': _missing_anchor[:20],
                        'metrics_missing_any_anchor_id_count': int(len(_missing_metric_anchor)),
                        'metrics_missing_any_anchor_id_examples': _missing_metric_anchor[:20],
                    }
            except Exception:
                pass
            # =====================================================================

    # -----------------------
    # Existing Google Sheet save behavior (guarded)
    # -----------------------
    except Exception:
        pass

    # =====================================================================
    # PATCH FIX2D75 (OPTION B): Materialize baseline primary_metrics_canonical for persistence
    # Why:
    # - HistoryFull replay/diff requires Analysis baseline canonical values (not just schema).
    # - Prior runs persisted metric_schema_frozen but had no primary_metrics_canonical map.
    # - Option B: compute once during Analysis and persist the decided map.
    # Behavior:
    # - If primary_metrics_canonical is missing/empty, rebuild deterministically from
    #   metric_schema_frozen + baseline_sources_cache using the schema-anchored rebuild.
    # - Mirror into analysis.primary_response.primary_metrics_canonical so Sheets minimal
    #   fallback still carries it.
    # - Emit debug counts for verification.
    # =====================================================================
    try:
        if isinstance(analysis, dict):
            _already = None
            if isinstance(analysis.get('primary_metrics_canonical'), dict) and analysis.get('primary_metrics_canonical'):
                _already = 'analysis.primary_metrics_canonical'
            elif isinstance(analysis.get('primary_response'), dict) and isinstance(analysis['primary_response'].get('primary_metrics_canonical'), dict) and analysis['primary_response'].get('primary_metrics_canonical'):
                _already = 'analysis.primary_response.primary_metrics_canonical'
            elif isinstance(analysis.get('results'), dict) and isinstance(analysis['results'].get('primary_metrics_canonical'), dict) and analysis['results'].get('primary_metrics_canonical'):
                _already = 'analysis.results.primary_metrics_canonical'

            if not _already:
                _pool = None
                try:
                    if isinstance(analysis.get('baseline_sources_cache'), list):
                        _pool = analysis.get('baseline_sources_cache')
                    elif isinstance(analysis.get('results'), dict) and isinstance(analysis['results'].get('baseline_sources_cache'), list):
                        _pool = analysis['results'].get('baseline_sources_cache')
                except Exception:
                    _pool = None

                _schema_ok = isinstance(analysis.get('metric_schema_frozen'), dict) and bool(analysis.get('metric_schema_frozen'))

                _rebuilt = None
                _diag = None
                if _schema_ok and isinstance(_pool, list) and _pool:
                    try:
                        _rebuilt, _diag = _fix2d9_schema_anchored_rebuild_current_metrics_v1(
                            analysis,
                            _pool,
                            web_context=analysis.get('web_context') if isinstance(analysis.get('web_context'), dict) else None,
                        )

                        # FIX2D86: sanitize rebuilt baseline PMC so __percent keys cannot bind bare year tokens (e.g., 2040)
                        try:
                            if isinstance(_rebuilt, dict) and _rebuilt:
                                _schema_fix2d86 = analysis.get("metric_schema_frozen") if isinstance(analysis.get("metric_schema_frozen"), dict) else {}
                                _rebuilt2, _dbg_fix2d86 = _fix2d86_sanitize_pmc_percent_year_tokens_v1(
                                    pmc=_rebuilt,
                                    metric_schema_frozen=_schema_fix2d86,
                                    label="fix2d75_materialize_pmc",
                                )
                                _rebuilt = _rebuilt2
                                try:
                                    analysis.setdefault("debug", {})
                                    if isinstance(analysis.get("debug"), dict):
                                        analysis["debug"]["fix2d86_percent_year_token_sanitize_materialize"] = _dbg_fix2d86
                                except Exception:
                                    pass
                        except Exception:
                            pass


                        # FIX2D86: sanitize rebuilt baseline PMC so __percent keys cannot bind bare year tokens (e.g., 2040)
                        try:
                            if isinstance(_rebuilt, dict) and _rebuilt:
                                _schema_fix2d86 = analysis.get("metric_schema_frozen") if isinstance(analysis.get("metric_schema_frozen"), dict) else {}
                                _rebuilt2, _dbg_fix2d86 = _fix2d86_sanitize_pmc_percent_year_tokens_v1(
                                    pmc=_rebuilt,
                                    metric_schema_frozen=_schema_fix2d86,
                                    label="fix2d75_materialize_pmc",
                                )
                                _rebuilt = _rebuilt2
                                try:
                                    analysis.setdefault("debug", {})
                                    if isinstance(analysis.get("debug"), dict):
                                        analysis["debug"]["fix2d86_percent_year_token_sanitize_materialize"] = _dbg_fix2d86
                                except Exception:
                                    pass
                        except Exception:
                            pass


                        # FIX2D86: sanitize rebuilt baseline PMC so __percent keys cannot bind bare year tokens (e.g., 2040)
                        try:
                            if isinstance(_rebuilt, dict) and _rebuilt:
                                _schema_fix2d86 = analysis.get("metric_schema_frozen") if isinstance(analysis.get("metric_schema_frozen"), dict) else {}
                                _rebuilt2, _dbg_fix2d86 = _fix2d86_sanitize_pmc_percent_year_tokens_v1(
                                    pmc=_rebuilt,
                                    metric_schema_frozen=_schema_fix2d86,
                                    label="fix2d75_materialize_pmc",
                                )
                                _rebuilt = _rebuilt2
                                try:
                                    analysis.setdefault("debug", {})
                                    if isinstance(analysis.get("debug"), dict):
                                        analysis["debug"]["fix2d86_percent_year_token_sanitize_materialize"] = _dbg_fix2d86
                                except Exception:
                                    pass
                        except Exception:
                            pass

                    except Exception:
                        _rebuilt, _diag = (None, None)

                # REFACTOR35: guard against schema-only rebuilds leaking debug keys into PMC
                # Keep only keys that exist in the frozen schema.
                try:
                    _schema_keys = set((analysis.get('metric_schema_frozen') or {}).keys()) if isinstance(analysis.get('metric_schema_frozen'), dict) else set()
                    if isinstance(_rebuilt, dict) and _schema_keys:
                        _rebuilt = {k: v for (k, v) in _rebuilt.items() if isinstance(k, str) and k in _schema_keys and isinstance(v, dict)}
                except Exception:
                    pass

                if isinstance(_rebuilt, dict) and _rebuilt:
                    try:
                        analysis['primary_metrics_canonical'] = dict(_rebuilt)
                    except Exception:
                        pass
                    try:
                        analysis.setdefault('primary_response', {})
                        if isinstance(analysis.get('primary_response'), dict):
                            analysis['primary_response']['primary_metrics_canonical'] = dict(_rebuilt)
                    except Exception:
                        pass
                    try:
                        analysis.setdefault('results', {})
                        if isinstance(analysis.get('results'), dict):
                            analysis['results']['primary_metrics_canonical'] = dict(_rebuilt)
                    except Exception:
                        pass

                # Debug
                try:
                    analysis.setdefault('debug', {})
                    if isinstance(analysis.get('debug'), dict):
                        analysis['debug']['fix2d75_materialize_pmc'] = {
                            'had_existing': bool(_already),
                            'existing_source': str(_already or ''),
                            'schema_present': bool(_schema_ok),
                            'pool_count': int(len(_pool)) if isinstance(_pool, list) else 0,
                            'rebuilt_count': int(len(_rebuilt)) if isinstance(_rebuilt, dict) else 0,
                            'rebuilt_diag': _diag if isinstance(_diag, dict) else {},
                        }
                except Exception:
                    pass
            else:
                try:
                    analysis.setdefault('debug', {})
                    if isinstance(analysis.get('debug'), dict):
                        analysis['debug']['fix2d75_materialize_pmc'] = {
                            'had_existing': True,
                            'existing_source': str(_already),
                        }
                except Exception:
                    pass
    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH D (ADDITIVE): propagate metric_anchors onto metric rows + evidence
    # Why:
    # - Drift=0 depends on analysis and evolution sharing the SAME anchor IDs.
    # - Some downstream code paths expect anchor_hash on the metric row itself
    #   and/or inside evidence entries (not only in analysis["metric_anchors"]).
    # - This patch copies existing anchor metadata only (no fabrication, no refetch).
    # =====================================================================
    try:
        import re
        import hashlib

        def _norm_ctx(s: str) -> str:
            try:
                return re.sub(r"\s+", " ", (s or "").strip())
            except Exception:
                return (s or "").strip()

        def _compute_anchor_hash_fallback(url: str, ctx: str) -> str:
            try:
                u = (url or "").strip()
                c = _norm_ctx(ctx or "")
                if not u or not c:
                    return ""
                return hashlib.sha1((u + "||" + c).encode("utf-8")).hexdigest()[:16]
            except Exception:
                return ""

        def _compute_anchor_hash(url: str, ctx: str) -> str:
            try:
                fn = globals().get("compute_anchor_hash")
                if callable(fn):
                    return str(fn(url, ctx) or "")
            except Exception:
                return _compute_anchor_hash_fallback(url, ctx)

        # Locate canonical metrics dict (prefer primary_response)
        _pmc = None
        _pr0 = analysis.get("primary_response") if isinstance(analysis, dict) else None
        if isinstance(_pr0, dict) and isinstance(_pr0.get("primary_metrics_canonical"), dict):
            _pmc = _pr0.get("primary_metrics_canonical")
        if _pmc is None and isinstance(analysis, dict) and isinstance(analysis.get("primary_metrics_canonical"), dict):
            _pmc = analysis.get("primary_metrics_canonical")

        if isinstance(metric_anchors, dict) and isinstance(_pmc, dict):
            for _ckey, _a in metric_anchors.items():
                if not isinstance(_ckey, str) or not _ckey:
                    continue
                if not isinstance(_a, dict) or not _a:
                    continue

                _m = _pmc.get(_ckey)
                if not isinstance(_m, dict):
                    continue

                _ah = str(_a.get("anchor_hash") or _a.get("anchor") or "").strip()
                _src = str(_a.get("source_url") or _a.get("url") or "").strip()
                _ctx = _a.get("context_snippet") or _a.get("context") or ""
                _ctx = _ctx.strip() if isinstance(_ctx, str) else ""

                # Copy onto metric row (only if missing)
                if _ah and not _m.get("anchor_hash"):
                    _m["anchor_hash"] = _ah
                if _src and not (_m.get("source_url") or _m.get("url")):
                    _m["source_url"] = _src
                if _ctx and not (_m.get("context_snippet") or _m.get("context")):
                    _m["context_snippet"] = _ctx[:220]

                # Pass through extra metadata if present (additive)
                if _a.get("anchor_confidence") is not None and _m.get("anchor_confidence") is None:
                    _m["anchor_confidence"] = _a.get("anchor_confidence")
                if _a.get("candidate_id") and not _m.get("candidate_id"):
                    _m["candidate_id"] = _a.get("candidate_id")
                if _a.get("fingerprint") and not _m.get("fingerprint"):
                    _m["fingerprint"] = _a.get("fingerprint")

                # Ensure evidence entries carry anchor_hash (deterministic; no new evidence)
                _ev = _m.get("evidence")
                if isinstance(_ev, list) and _ev:
                    for _e in _ev:
                        if not isinstance(_e, dict):
                            continue
                        if _e.get("anchor_hash"):
                            continue
                        _e_url = str(_e.get("url") or _e.get("source_url") or _src or "").strip()
                        _e_ctx = _e.get("context_snippet") or _e.get("context") or _ctx or ""
                        _e_ctx = _e_ctx.strip() if isinstance(_e_ctx, str) else ""
                        _eh = _compute_anchor_hash(_e_url, _e_ctx)
                        if _eh:
                            _e["anchor_hash"] = _eh
    except Exception:
        pass
    # =====================================================================


    def _try_make_sheet_json(obj: dict) -> str:
        try:
            fn = globals().get("make_sheet_safe_json")
            if callable(fn):
                return fn(obj)
        except Exception:
            return json.dumps(obj, ensure_ascii=False, default=str)

    def _shrink_for_sheets(original: dict) -> dict:
        base_copy = dict(original)
        s = _try_make_sheet_json(base_copy)
        if isinstance(s, str) and len(s) <= SHEETS_CELL_LIMIT:
            return base_copy

        reduced = dict(base_copy)
        removed = []

        for k in [
            "evidence_records",
            "baseline_sources_cache",
            "metric_anchors",
            "source_results",
            "web_context",
            "scraped_meta",
            "raw_sources",
            "raw_text",
            "debug",
        ]:
            if k in reduced:
                reduced.pop(k, None)
                removed.append(k)

        reduced.setdefault("_sheet_write", {})
        if isinstance(reduced["_sheet_write"], dict):
            reduced["_sheet_write"]["truncated"] = True
            reduced["_sheet_write"]["removed_keys"] = removed[:50]

        s2 = _try_make_sheet_json(reduced)
        if isinstance(s2, str) and len(s2) <= SHEETS_CELL_LIMIT:
            return reduced

        return {
            "question": original.get("question"),
            "timestamp": original.get("timestamp"),
            "final_confidence": original.get("final_confidence"),
            "question_profile": original.get("question_profile"),
            "primary_response": original.get("primary_response") or {},
            "_sheet_write": {
                "truncated": True,
                "mode": "minimal_fallback",
                "note": "Full analysis too large for single Google Sheets cell (50k limit).",
            },
        }

    # Try Sheets
    try:
        sheet = get_google_sheet()
    except Exception:
        pass
        sheet = None

    if not sheet:
        if "analysis_history" not in st.session_state:
            st.session_state.analysis_history = []
        st.session_state.analysis_history.append(analysis)
        try:
            st.session_state["last_analysis"] = analysis
        except Exception:
            return False

    try:
        analysis_id = generate_analysis_id()


        # =====================================================================
        # PATCH ES1F (ADDITIVE): persist full snapshots + pointer for Sheets rows
        # - If full baseline_sources_cache exists (list-shaped), store it outside
        #   Sheets keyed by source_snapshot_hash, and attach pointer fields into
        #   analysis/results for deterministic evolution rehydration.
        # - Pure enrichment only (no refetch, no heuristics).
        # =====================================================================
        try:
            _bsc = None
            if isinstance(analysis, dict):
                _bsc = analysis.get("results", {}).get("baseline_sources_cache") or analysis.get("baseline_sources_cache")


            # =================================================================
            # PATCH SS6B (ADDITIVE): if snapshots were already summarized away,
            # rebuild minimal snapshot shape from evidence_records (deterministic).
            # This enables snapshot persistence even when baseline_sources_cache
            # is a summary dict in the main analysis object.
            # =================================================================
            try:
                if (not isinstance(_bsc, list)) and isinstance(analysis, dict):
                    _er = None
                    # prefer nested results evidence_records first
                    if isinstance(analysis.get("results"), dict):
                        _er = analysis["results"].get("evidence_records")
                    if _er is None:
                        _er = analysis.get("evidence_records")
                    _rebuilt = build_baseline_sources_cache_from_evidence_records(_er)
                    if isinstance(_rebuilt, list) and _rebuilt:
                        _bsc = _rebuilt
            except Exception:
                pass
            # =================================================================

            if isinstance(_bsc, list) and _bsc:
                _ssh = compute_source_snapshot_hash(_bsc)

                # =========================
                # PATCH A2 (ADD): also compute snapshot hash v2 for stronger identity
                # =========================
                _ssh_v2 = None
                try:
                    _ssh_v2 = compute_source_snapshot_hash_v2(_bsc)
                except Exception:
                    pass
                    _ssh_v2 = None
                if _ssh:
                    # =============================================================
                    # PATCH SS4 (ADDITIVE): store snapshots to Snapshots worksheet when possible
                    # - Persists full baseline_sources_cache in a dedicated worksheet tab.
                    # - Falls back to local snapshot_store file if Sheets snapshot store unavailable.
                    # - Pointer ref stored as 'gsheet:Snapshots:<hash>' when successful.
                    # =============================================================
                    _gs_ref = ""
                    _gs_ref_v2 = ""
                    try:
                        _gs_ref = store_full_snapshots_to_sheet(_bsc, _ssh, worksheet_title="Snapshots")
                        # =========================
                        # PATCH A3 (ADD): mirror-write snapshots under v2 hash as well
                        # =========================
                        if _ssh_v2 and isinstance(_ssh_v2, str) and _ssh_v2 != _ssh:
                            try:
                                _gs_ref_v2 = store_full_snapshots_to_sheet(_bsc, _ssh_v2, worksheet_title="Snapshots")
                            except Exception:
                                _gs_ref_v2 = ""
                    except Exception:
                        _gs_ref = ""
                        _gs_ref_v2 = ""

                    _ref = ""
                    try:
                        _ref = store_full_snapshots_local(_bsc, _ssh)
                    except Exception:
                        _ref = ""

                    analysis["source_snapshot_hash"] = analysis.get("source_snapshot_hash") or _ssh
                    analysis.setdefault("results", {})
                    if isinstance(analysis["results"], dict):
                        analysis["results"]["source_snapshot_hash"] = analysis["results"].get("source_snapshot_hash") or _ssh
                        # PATCH A4 (ADD): store v2 hash in results for downstream consumers
                        try:
                            if _ssh_v2:
                                analysis["results"]["source_snapshot_hash_v2"] = analysis["results"].get("source_snapshot_hash_v2") or _ssh_v2
                                # =========================
                                # PATCH FIX37 (ADD): stable snapshot hash alias for fastpath alignment
                                # - Prefer v2 (stable) when present; fall back to legacy v1.
                                # =========================
                                try:
                                    _ssh_stable = _ssh_v2 or _ssh
                                    if _ssh_stable:
                                        analysis["source_snapshot_hash_stable"] = analysis.get("source_snapshot_hash_stable") or _ssh_stable
                                        analysis["results"]["source_snapshot_hash_stable"] = analysis["results"].get("source_snapshot_hash_stable") or _ssh_stable
                                except Exception:
                                    pass
                        except Exception:
                            pass

                    if _ref:
                        analysis["snapshot_store_ref"] = analysis.get("snapshot_store_ref") or _ref
                        if isinstance(analysis["results"], dict):
                            analysis["results"]["snapshot_store_ref"] = analysis["results"].get("snapshot_store_ref") or _ref
                            # PATCH A5 (ADD): v2 snapshot ref for convenience
                            try:
                                if _ssh_v2 and _gs_ref_v2:
                                    analysis["results"]["snapshot_store_ref_v2"] = analysis["results"].get("snapshot_store_ref_v2") or _gs_ref_v2
                            except Exception:
                                pass
                    # =============================================================
                    # PATCH SS4B (ADDITIVE): prefer Sheets snapshot ref when available
                    # =============================================================
                    try:
                        if _gs_ref:
                            analysis["snapshot_store_ref"] = _gs_ref
                            if isinstance(analysis.get("results"), dict):
                                analysis["results"]["snapshot_store_ref"] = _gs_ref
                    except Exception:
                        pass

                    # =============================================================
                    # PATCH REFACTOR41 (ADDITIVE): stable snapshot store ref + write debug
                    # - Avoid advertising a v2 gsheet ref unless it was actually written successfully.
                    # - Provide a stable ref that always points to a verified store (v2 sheet > v1 sheet > local).
                    # - Emit a compact debug manifest for diagnosing snapshot write failures.
                    # =============================================================
                    try:
                        _stable_ref = (_gs_ref_v2 or _gs_ref or (analysis.get("snapshot_store_ref") if isinstance(analysis, dict) else "") or (_ref if "_ref" in locals() else "") or "")
                        if _stable_ref and isinstance(analysis, dict):
                            analysis["snapshot_store_ref_stable"] = analysis.get("snapshot_store_ref_stable") or _stable_ref
                            if isinstance(analysis.get("results"), dict):
                                analysis["results"]["snapshot_store_ref_stable"] = analysis["results"].get("snapshot_store_ref_stable") or _stable_ref
                    except Exception:
                        pass

                    try:
                        if isinstance(analysis, dict) and isinstance(analysis.get("results"), dict):
                            _dbg = analysis["results"].get("debug")
                            if not isinstance(_dbg, dict):
                                _dbg = {}
                                analysis["results"]["debug"] = _dbg
                            _dbg["snapshot_store_write_v1"] = {
                                "ssh_v1": str(_ssh or ""),
                                "ssh_v2": str(_ssh_v2 or ""),
                                "gs_ref_v1": str(_gs_ref or ""),
                                "gs_ref_v2": str(_gs_ref_v2 or ""),
                                "local_ref_v1": str(_ref or ""),
                                "final_snapshot_store_ref": str((analysis.get("snapshot_store_ref") or "") if isinstance(analysis, dict) else ""),
                                "final_snapshot_store_ref_stable": str((analysis.get("snapshot_store_ref_stable") or "") if isinstance(analysis, dict) else ""),
                            }
                            # =============================================================
                            # REFACTOR54 (ADDITIVE): snapshot round-trip verification
                            # - After writing snapshot_store_ref_stable, attempt to load it back
                            #   (sheet or local path) and record basic success/failure stats.
                            # - Best-effort only; never blocks persistence.
                            # =============================================================
                            try:
                                import os as _os, json as _json
                                _stable_ref_rt = str(analysis.get("snapshot_store_ref_stable") or "")
                                _rt = {
                                    "stable_ref": _stable_ref_rt,
                                    "origin": "",
                                    "expected_count": 0,
                                    "loaded_count": 0,
                                    "ok": False,
                                    "note": "",
                                }
                                try:
                                    _rt["expected_count"] = int(len(_bsc)) if isinstance(_bsc, list) else 0
                                except Exception:
                                    _rt["expected_count"] = 0

                                _loaded = None
                                try:
                                    if _stable_ref_rt.startswith("gsheet:Snapshots:"):
                                        _h = ""
                                        try:
                                            _h = _stable_ref_rt.split(":", 2)[-1].strip()
                                        except Exception:
                                            _h = ""
                                        if _h:
                                            _loaded = load_full_snapshots_from_sheet(_h, worksheet_title="Snapshots")
                                            _rt["origin"] = "sheet"
                                except Exception:
                                    _loaded = None

                                try:
                                    if _loaded is None and _stable_ref_rt and _os.path.exists(_stable_ref_rt):
                                        with open(_stable_ref_rt, "r", encoding="utf-8") as _f:
                                            _loaded = _json.load(_f)
                                        _rt["origin"] = "path"
                                except Exception:
                                    _loaded = None

                                try:
                                    _rt["loaded_count"] = int(len(_loaded)) if isinstance(_loaded, list) else 0
                                except Exception:
                                    _rt["loaded_count"] = 0

                                try:
                                    if _rt["loaded_count"] > 0:
                                        if _rt["expected_count"] > 0:
                                            _rt["ok"] = bool(_rt["loaded_count"] == _rt["expected_count"])
                                            if not _rt["ok"]:
                                                _rt["note"] = "count_mismatch"
                                        else:
                                            _rt["ok"] = True
                                    else:
                                        _rt["ok"] = False
                                        _rt["note"] = "empty_or_unreadable"
                                except Exception:
                                    pass

                                _dbg["snapshot_roundtrip_v1"] = _rt
                            except Exception:
                                pass
                    except Exception:
                        pass

        except Exception:
            pass
        # =====================================================================

        payload_for_sheets = _shrink_for_sheets(analysis)
        payload_json = _try_make_sheet_json(payload_for_sheets)

        # =====================================================================
        # PATCH A5 (BUGFIX, REQUIRED): never write invalid JSON to Sheets
        # - Previous hard truncation produced non-JSON (prefix + random suffix),
        #   causing history loaders (json.loads) to skip the row entirely.
        # - This wrapper guarantees valid JSON even when we must truncate.
        # =====================================================================
        if isinstance(payload_json, str) and len(payload_json) > SHEETS_CELL_LIMIT:
            try:
                payload_json = json.dumps(
                    {
                        "_sheet_write": {
                            "truncated": True,
                            "mode": "hard_truncation_wrapper",
                            "note": "Payload exceeded Google Sheets single-cell limit; stored preview only.",
                        },
                        # keep a preview for debugging/UI; still parseable JSON
                        "preview": payload_json[: max(0, SHEETS_CELL_LIMIT - 600)],
                        "analysis_id": analysis_id,
                        "timestamp": analysis.get("timestamp", _yureeka_now_iso_utc()),
                        "question": (analysis.get("question", "") or "")[:200],
                    },
                    ensure_ascii=False,
                    default=str,
                )
            except Exception:
                pass
                # ultra-safe fallback: still valid JSON
                payload_json = '{"_sheet_write":{"truncated":true,"mode":"hard_truncation_wrapper","note":"json.dumps failed"}}'
        # =====================================================================
        # =====================================================================
        # PATCH HF_PERSIST1 (ADDITIVE): Persist full payload to HistoryFull when History cell is wrapped/truncated
        # Why:
        # - Evolution rebuild requires schema/anchors which may be lost in a sheets-safe wrapper
        # - HistoryFull stores the full JSON keyed by analysis_id for later rehydration
        # Behavior:
        # - If payload_json indicates truncation/wrapper OR is very large, write full payload to HistoryFull
        # - Attach a pointer full_store_ref to both analysis and the wrapper object (when possible)
        # =====================================================================
        try:
            is_truncated = False
            try:
                if isinstance(payload_json, str) and ('"_sheet_write"' in payload_json or '"_sheets_safe"' in payload_json):
                    # quick signal; parse if possible
                    try:
                        _pj = json.loads(payload_json)
                        sw = _pj.get("_sheet_write") if isinstance(_pj, dict) else None
                        if isinstance(sw, dict) and sw.get("truncated") is True:
                            is_truncated = True
                        if _pj.get("_sheets_safe") is True:
                            is_truncated = True
                    except Exception:
                        pass
                        # if we can't parse and it's huge, treat as truncated risk
                        if len(payload_json) > 45000:
                            is_truncated = True
                elif isinstance(payload_json, str) and len(payload_json) > 45000:
                    is_truncated = True
            except Exception:
                pass

            if is_truncated:
                full_payload_json = ""
                # FIX2D73: save-side debug counts for baseline canonical metrics persistence
                try:
                    _fix2d73_pmc_count = 0
                    _fix2d73_pmc_src = None
                    if isinstance(analysis, dict):
                        if isinstance(analysis.get("primary_metrics_canonical"), dict) and analysis.get("primary_metrics_canonical"):
                            _fix2d73_pmc_count = len(analysis.get("primary_metrics_canonical") or {})
                            _fix2d73_pmc_src = "analysis.primary_metrics_canonical"
                        elif isinstance(analysis.get("primary_response"), dict) and isinstance(analysis["primary_response"].get("primary_metrics_canonical"), dict) and analysis["primary_response"].get("primary_metrics_canonical"):
                            _fix2d73_pmc_count = len(analysis["primary_response"].get("primary_metrics_canonical") or {})
                            _fix2d73_pmc_src = "analysis.primary_response.primary_metrics_canonical"
                        elif isinstance(analysis.get("results"), dict) and isinstance(analysis["results"].get("primary_metrics_canonical"), dict) and analysis["results"].get("primary_metrics_canonical"):
                            _fix2d73_pmc_count = len(analysis["results"].get("primary_metrics_canonical") or {})
                            _fix2d73_pmc_src = "analysis.results.primary_metrics_canonical"
                    analysis.setdefault("debug", {})
                    if isinstance(analysis.get("debug"), dict):
                        analysis["debug"]["fix2d73_historyfull_save_counts"] = {
                            "primary_metrics_canonical_count": int(_fix2d73_pmc_count),
                            "primary_metrics_canonical_source": str(_fix2d73_pmc_src or ""),
                        }
                except Exception:
                    pass
                try:
                    full_payload_json = json.dumps(analysis, ensure_ascii=False, default=str)
                except Exception:
                    pass
                    full_payload_json = ""

                if full_payload_json:
                    ok_full = write_full_history_payload_to_sheet(analysis_id, full_payload_json, worksheet_title="HistoryFull")
                    if ok_full:
                        ref = f"gsheet:HistoryFull:{analysis_id}"
                        try:
                            analysis["full_store_ref"] = ref
                        except Exception:
                            pass
                        # If payload_json is a wrapper dict, embed ref too
                        try:
                            _pj2 = json.loads(payload_json)
                            if isinstance(_pj2, dict):
                                _pj2["full_store_ref"] = ref
                                sw2 = _pj2.get("_sheet_write")
                                if isinstance(sw2, dict):
                                    sw2["full_store_ref"] = ref
                                    _pj2["_sheet_write"] = sw2
                                payload_json = json.dumps(_pj2, ensure_ascii=False, default=str)
                        except Exception:
                            pass
        except Exception:
            pass
        # =====================================================================


        row = [
            analysis_id,
            analysis.get("timestamp", _yureeka_now_iso_utc()),
            (analysis.get("question", "") or "")[:100],
            str(analysis.get("final_confidence", "")),
            payload_json,
        ]
        sheet.append_row(row, value_input_option="RAW")

        # PATCH FIX2D66G (ADD): also persist into session history even when Sheets is primary
        # - This prevents Evolution from being blocked when a Sheets write succeeds/fails intermittently.
        try:
            if "analysis_history" not in st.session_state:
                st.session_state.analysis_history = []
            st.session_state.analysis_history.append(analysis)
            # If Sheets succeeded, clear any prior write-failure forcing.
            st.session_state.pop("fix2d66_force_session_history", None)
        except Exception:
            pass

        try:
            st.session_state["last_analysis"] = analysis
        except Exception:
            pass

        return True

    except Exception as e:
        st.warning(f"⚠️ Failed to save to Google Sheets: {e}")
        # PATCH FIX2D66G (ADDITIVE): mark Sheets write failure so history can fall back to session state
        try:
            globals()["_SHEETS_LAST_WRITE_ERROR"] = str(e)
        except Exception:
            pass
        try:
            st.session_state["fix2d66_force_session_history"] = True
        except Exception:
            pass
        if "analysis_history" not in st.session_state:
            st.session_state.analysis_history = []
        st.session_state.analysis_history.append(analysis)
        try:
            st.session_state["last_analysis"] = analysis
        except Exception:
            pass

        return False


def normalize_unit_tag(unit_str: str) -> str:
    """
    Canonical unit tags used for drift=0 comparisons.
    """
    u = (unit_str or "").strip()
    if not u:
        return ""
    ul = u.lower().replace(" ", "")

    # energy units
    if ul == "twh":
        return "TWh"
    if ul == "gwh":
        return "GWh"
    if ul == "mwh":
        return "MWh"
    if ul == "kwh":
        return "kWh"
    if ul == "wh":
        return "Wh"

    # magnitudes
    if ul in ("t", "trillion", "tn"):
        return "T"
    if ul in ("b", "bn", "billion"):
        return "B"
    if ul in ("m", "mn", "mio", "million"):
        return "M"
    if ul in ("k", "thousand", "000"):
        return "K"



    # composite phrases (e.g. "million units")
    # Some extractors pass unit strings like "million units" as a single tag.
    # Normalize these into the same magnitude tags used elsewhere (M/B/T/K) so
    # unit_family can be typed deterministically.
    if ("unit" in ul) or ("units" in ul):
        if ("trillion" in ul) or ul.startswith("tn") or ul.startswith("t") and ul.endswith("units"):
            return "T"
        if ("billion" in ul) or ul.startswith("bn"):
            return "B"
        if ("million" in ul) or ul.startswith("mn") or ul.startswith("mio"):
            return "M"
        if ("thousand" in ul) or ul.startswith("k"):
            return "K"

    # percent
    if ul in ("%", "pct", "percent"):
        return "%"

    return u


def unit_family(unit_tag: str) -> str:
    """
    Unit family classifier for gating.
    """
    ut = (unit_tag or "").strip()

    if ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
        return "energy"
    if ut == "%":
        return "percent"
    if ut in ("T", "B", "M", "K"):
        return "magnitude"

    return ""

# =========================
# PATCH FIX2D2J (ADDITIVE): normalize_unit_family alias + currency-aware family
# - Many extracted candidates arrive with unit_family='' due to legacy drift.
# - Provide a stable, analysis/evolution-shared unit_family normalizer.
# - Currency requires context evidence; caller may pass ctx/raw for upgrade.
# =========================


# =========================
# PATCH FIX2D2K (ADDITIVE): context-driven unit backfill for unitless candidates
# - Some sources yield numbers without an attached unit token (unit_tag="").
# - We conservatively infer unit_tag/unit_family from nearby context text.
# - This does NOT weaken FIX2D24 year-blocking; it only restores missing unit metadata.
# =========================
import re as _re_fix2d2k

# =========================
# REFACTOR20 (BUGFIX): boundary-aware currency evidence detector
# - Prevent false positives like 'eur'/'euro' inside 'Europe' from upgrading unit_family to currency.
# - Treat currency codes/words as tokens (word-boundary), while allowing symbol markers ($, €, £, ¥).
# =========================
def _yureeka_has_currency_evidence_v1(text: str) -> bool:
    try:
        t = (text or "").lower()
    except Exception:
        t = str(text or "")
        t = t.lower()

    # strong symbol markers
    if any(sym in t for sym in ("$", "€", "£", "¥")):
        return True

    # common composite tokens
    if "us$" in t or "s$" in t:
        return True

    # currency codes as tokens (avoid substrings inside other words)
    try:
        if _re_fix2d2k.search(r"\b(usd|sgd|eur|gbp|jpy|cny|rmb|aud|cad|inr|krw|chf|hkd|nzd)\b", t):
            return True
        if _re_fix2d2k.search(r"\b(dollar|dollars|euro|euros|pound|pounds|yen|yuan|rupee|rupees)\b", t):
            return True
    except Exception:
        pass

    return False



def infer_unit_tag_from_context(ctx: str, raw: str = ""):
    """Return (unit_tag, unit_family, matched_phrase, excerpt)."""
    c = (ctx or "")
    r = (raw or "")
    cl = (c + " " + r).lower()

    # percent signals
    if "%" in cl or "percent" in cl or "pct" in cl or "market share" in cl or "share" in cl:
        return "%", "percent", "percent/market_share", (c[:160] if c else r[:160])

    # currency signals
    if _yureeka_has_currency_evidence_v1(cl):
        return "USD", "currency", "currency_marker", (c[:160] if c else r[:160])
    if any(k in cl for k in ["revenue", "market size", "market value", "valuation", "valued", "worth", "price"]):
        # keyword-only currency is weaker; require a magnitude word to reduce false positives
        if any(w in cl for w in ["billion", "bn", "million", "mn", "trillion", "tn"]):
            return "USD", "currency", "currency_keyword", (c[:160] if c else r[:160])

    # magnitude / unit-sales style signals
    # detect explicit magnitude words, and also "units".
    if "million" in cl or " mn" in cl or "mio" in cl:
        if "unit" in cl or "vehicle" in cl or "sales" in cl:
            return "M", "magnitude", "million_units", (c[:160] if c else r[:160])
        return "M", "magnitude", "million", (c[:160] if c else r[:160])
    if "billion" in cl or " bn" in cl:
        return "B", "magnitude", "billion", (c[:160] if c else r[:160])
    if "trillion" in cl or " tn" in cl:
        return "T", "magnitude", "trillion", (c[:160] if c else r[:160])
    if "thousand" in cl or "k " in cl:
        return "K", "magnitude", "thousand", (c[:160] if c else r[:160])

    return "", "", "", (c[:160] if c else r[:160])
def normalize_unit_family(unit_tag: str, ctx: str = "", raw: str = "") -> str:
    """
    Deterministic unit-family normalization.

    Goals (FIX2D30):
    - Keep 'M'/'million' candidates in "million units / units sold / chargers" contexts as *magnitude* (or unit-count/sales downstream),
      preventing false 'currency' upgrades that block baseline comparability.
    - Only label a candidate as 'currency' when explicit currency evidence exists (symbols/codes/words), not just generic keywords like "market".

    Notes:
    - This helper is intentionally conservative.
    - Hard unit-family rejection (Diff Panel inference) remains the enforcement point; this just fixes upstream family typing.
    """
    ut = (unit_tag or "").strip()
    fam = unit_family(ut)

    # REFACTOR22: Do not infer unit family from surrounding context for plain yearlike tokens
    # when unit_tag is missing. Year/range endpoints (e.g., '2026–2040') commonly sit next to
    # '%' or currency symbols and can be mis-typed as percent/currency, creating noisy
    # unit inconsistencies in baseline_sources_cache. This is behavior-preserving for binding,
    # since yearlike tokens are not legitimate metric values for __percent/__currency keys.
    if fam == "" and ut == "":
        try:
            import re as _re
            _rs = (raw or "").strip()
            if _re.fullmatch(r"(19|20)\d{2}", _rs or ""):
                # Only allow inference if the raw token itself contains explicit unit evidence.
                if not _re.search(r"[%$€£¥]", _rs):
                    return ""
        except Exception:
            pass


    # PATCH FIX2D2K: infer family from context when unit_tag is missing
    if fam == "" and ut == "":
        try:
            _itag, ifam, _phr, _ex = infer_unit_tag_from_context(ctx or "", raw or "")
        except Exception:
            pass
            _itag, ifam, _phr, _ex = "", "", "", ""
        if ifam:
            return ifam

    if fam == "magnitude":
        c = ((ctx or "") + " " + (raw or "")).lower()

        # Strong unit-count / unit-sales evidence: keep as magnitude.
        # This blocks the legacy false-positive path where 'M' + 'market' upgraded to currency even when the phrase is "million units".
        unit_evidence = [
            "million units",
            "units sold",
            "unit sales",
            "vehicles sold",
            "ev sales",
            "sales ytd",
            "ytd",
            "chargers",
            "charger",
            "charging points",
            "charging stations",
            "stations",
            "units",
        ]
        has_unit_evidence = any(u in c for u in unit_evidence)

        # Explicit currency markers only (symbols/codes/words)
        # REFACTOR20: boundary-aware currency detection (avoid "Europe" → "euro" false positives)
        has_currency_markers = _yureeka_has_currency_evidence_v1(c)

        if has_unit_evidence and not has_currency_markers:
            return "magnitude"

        if has_currency_markers:
            return "currency"

        # FIX2D30: Remove keyword-only currency upgrades (e.g., 'market', 'valuation') because they cause false positives
        # for phrases like "million units" that also mention "market".

    return fam
def infer_currency_code_from_text_v1(text: str) -> str:
    """
    Best-effort, deterministic currency code inference from raw/context strings.
    Returns an ISO-ish code (e.g., USD/EUR/GBP/JPY/SGD/AUD/CAD/HKD/CNY/KRW/INR) or "".
    """
    try:
        s = (text or "").strip().lower()
        if not s:
            return ""
        # Explicit codes first
        for code in ("usd","eur","gbp","jpy","cny","rmb","aud","cad","sgd","hkd","krw","inr","chf","sek","nok","dkk","nzd"):
            if re.search(r"\b" + re.escape(code) + r"\b", s):
                return "CNY" if code == "rmb" else code.upper()
        # Prefixed symbols
        if "us$" in s or "u.s.$" in s:
            return "USD"
        if "s$" in s:
            return "SGD"
        if "a$" in s:
            return "AUD"
        if "c$" in s:
            return "CAD"
        if "hk$" in s:
            return "HKD"
        # Unicode currency symbols
        if "€" in s:
            return "EUR"
        if "£" in s:
            return "GBP"
        if "¥" in s:
            return "JPY"
        if "₹" in s:
            return "INR"
        if "₩" in s:
            return "KRW"
        # Plain "$" is ambiguous; assume USD only if US markers exist; otherwise leave blank.
        if "$" in s and ("united states" in s or "u.s." in s or " us " in s or " usa" in s):
            return "USD"
    except Exception:
        pass
    return ""




def canonicalize_numeric_candidate(candidate: dict) -> dict:


    """


    Additive: attach canonical numeric fields to a candidate dict.


    Safe to call multiple times.



    PATCH AI4 (ADDITIVE): anchor integrity


    - Ensures anchor_hash + candidate_id are present when possible (derived if missing).


    - Does not change extraction behavior; only enriches fields.


    """


    import hashlib



    if not isinstance(candidate, dict):


        return {}



    # ---------- numeric value ----------


    v_raw = candidate.get("value_norm")


    v = None


    if v_raw is not None:


        try:


            v = float(v_raw)


        except Exception:
            pass


            v = None


    if v is None:


        try:


            v0 = candidate.get("value")


            if v0 is None:


                return candidate


            v = float(v0)


        except Exception:
            return candidate



    # ---------- unit normalization ----------


    try:


        ut = normalize_unit_tag(candidate.get("unit_tag") or candidate.get("unit") or "")


    except Exception:
        pass


        ut = str(candidate.get("unit_tag") or candidate.get("unit") or "").strip()

    # =========================
    # PATCH FIX2D2K (ADDITIVE): context-driven unit backfill when unit_tag is empty
    # - Some sources yield numbers without an attached unit token (unit_tag="").
    # - Infer unit_tag/unit_family from nearby context_snippet/raw without weakening FIX2D24.
    # - Attach per-candidate trace: context_unit_backfill_v1.
    # =========================
    ctx_s = (candidate.get("context") or candidate.get("context_snippet") or "")
    raw_s = (candidate.get("raw") or candidate.get("display_value") or "")
    context_unit_backfill_v1 = {"applied": False}
    if (ut or "").strip() == "":
        # Guard: do not infer %/currency units from surrounding context for plain year tokens.
        _skip_backfill_yearlike = False
        try:
            _vi = int(float(v)) if v is not None else None
            if _vi is not None and float(_vi) == float(v) and 1900 <= _vi <= 2100:
                import re as _re
                _rs = (raw_s or "").strip().lower()
                if not _re.search(r"[%$€£¥]|\b(us\$|s\$|usd|sgd|eur|gbp|jpy|aud|cad|chf)\b", _rs):
                    _skip_backfill_yearlike = True
        except Exception:
            _skip_backfill_yearlike = False

        if _skip_backfill_yearlike:
            context_unit_backfill_v1 = {"applied": False, "skipped": True, "reason": "yearlike_no_backfill"}
        else:
            itag, ifam, phr, ex = ("", "", "", "")
            try:
                itag, ifam, phr, ex = infer_unit_tag_from_context(ctx_s, raw_s)
            except Exception:
                pass
                itag, ifam, phr, ex = ("", "", "", "")
            if itag or ifam:
                if itag:
                    ut = itag
                    candidate["unit_tag"] = itag
                context_unit_backfill_v1 = {
                    "applied": True,
                    "matched_phrase": phr,
                    "inferred_unit_family": ifam,
                    "inferred_unit_tag": itag,
                    "window_excerpt": ex,
                }
    try:
        candidate["context_unit_backfill_v1"] = context_unit_backfill_v1
    except Exception:
        pass

    # =========================
    # PATCH FIX2D2J (ADDITIVE): deterministic unit/measure classifier
    # - Backfill unit_family using unit_tag and currency evidence in context
    # - Correct measure_kind/measure_assoc for currency-like candidates
    # - Attach small per-candidate trace fields for audit
    # =========================
    ctx_s = (candidate.get("context") or candidate.get("context_snippet") or "")
    raw_s = (candidate.get("raw") or candidate.get("display_value") or "")
    existing_fam = (candidate.get("unit_family") or "")

    try:
        fam = normalize_unit_family(ut, ctx=ctx_s, raw=raw_s)
    except Exception:
        pass
        fam = ""

    unit_family_backfilled = False
    if (not existing_fam) and fam:
        candidate["unit_family"] = fam
        unit_family_backfilled = True
    elif fam:
        # keep existing if set, but normalize obvious empties/whitespace
        if str(existing_fam).strip() == "":
            candidate["unit_family"] = fam
            unit_family_backfilled = True

    # REFACTOR24: infer/carry currency_code for currency candidates (used later for unit comparability)
    try:
        if fam == "currency" and not str(candidate.get("currency_code") or "").strip():
            _cc = infer_currency_code_from_text_v1((raw_s or "") + " " + (ctx_s or ""))
            if _cc:
                candidate["currency_code"] = _cc
    except Exception:
        pass

    # currency kind correction (only when evidence exists)
    measure_kind_corrected = False
    measure_assoc_corrected = False
    classifier_reason = ""
    if fam == "currency":
        classifier_reason = "currency_evidence"
        mk0 = str(candidate.get("measure_kind") or "").strip()
        if mk0 in ("", "count_units"):
            candidate["measure_kind"] = "currency"
            measure_kind_corrected = True
        ma0 = str(candidate.get("measure_assoc") or "").strip()
        cxl = (ctx_s or "").lower()
        assoc = None
        if "revenue" in cxl:
            assoc = "revenue"
        elif "market" in cxl and any(k in cxl for k in ["size", "value", "valuation"]):
            assoc = "market_value"
        elif "price" in cxl:
            assoc = "price"
        if assoc and (ma0 in ("", "units")):
            candidate["measure_assoc"] = assoc
            measure_assoc_corrected = True
    elif fam == "percent":
        classifier_reason = "percent_tag"
        mk0 = str(candidate.get("measure_kind") or "").strip()
        if mk0 == "":
            candidate["measure_kind"] = "percent"
            measure_kind_corrected = True

    # attach trace (compact)
    try:
        candidate["unit_measure_classifier_trace_v1"] = {
            "unit_tag": ut,
            "unit_family": candidate.get("unit_family") or fam,
            "context_unit_backfill_applied": bool((candidate.get("context_unit_backfill_v1") or {}).get("applied")),
            "unit_family_backfilled": bool(unit_family_backfilled),
            "measure_kind": candidate.get("measure_kind"),
            "measure_kind_corrected": bool(measure_kind_corrected),
            "measure_assoc": candidate.get("measure_assoc"),
            "measure_assoc_corrected": bool(measure_assoc_corrected),
            "reason": classifier_reason or ("from_unit_tag" if fam else "unknown"),
        }
    except Exception:
        pass




    # If candidate already has base_unit/multiplier_to_base, respect them


    base_unit = candidate.get("base_unit")


    mult = candidate.get("multiplier_to_base")



    try:


        mult = float(mult) if mult is not None else None


    except Exception:
        pass


        mult = None



    # Minimal deterministic mapping (extend as needed)


    if (not base_unit) or (mult is None):


        base_unit = ""


        mult = 1.0



        # percents


        if ut in ("%", "pct"):


            base_unit, mult = "%", 1.0



        # energy


        elif ut == "MWh":


            base_unit, mult = "Wh", 1e6


        elif ut == "kWh":


            base_unit, mult = "Wh", 1e3


        elif ut == "Wh":


            base_unit, mult = "Wh", 1.0



        # power


        elif ut == "GW":


            base_unit, mult = "W", 1e9


        elif ut == "MW":


            base_unit, mult = "W", 1e6


        elif ut == "kW":


            base_unit, mult = "W", 1e3


        elif ut == "W":


            base_unit, mult = "W", 1.0



        # mass


        elif ut in ("Mt", "million_tonnes", "million_tons"):


            base_unit, mult = "t", 1e6


        elif ut in ("kt", "kilo_tonnes", "kilo_tons"):


            base_unit, mult = "t", 1e3


        elif ut in ("t", "tonne", "tonnes", "ton", "tons"):


            base_unit, mult = "t", 1.0



        # count-ish


        elif ut in ("vehicles", "units", "count"):


            base_unit, mult = ut, 1.0



        else:


            # unknown unit: treat as-is


            base_unit, mult = (ut or str(candidate.get("unit") or "").strip()), 1.0



    # Only set defaults to avoid overriding existing enriched fields


    candidate.setdefault("unit_tag", ut)


    candidate.setdefault("unit_family", fam)


    candidate.setdefault("base_unit", base_unit)


    candidate.setdefault("multiplier_to_base", mult)



    # value_norm: if already present, do not overwrite


    if candidate.get("value_norm") is None:


        try:


            candidate["value_norm"] = float(v) * float(mult)


        except Exception:


            pass



    # ---------- anchor integrity ----------


    def _sha1(s: str) -> str:


        try:


            return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()


        except Exception:
            return ""



    ah = candidate.get("anchor_hash") or candidate.get("anchor")


    if not ah:


        # attempt deterministic derive if fields exist


        src = candidate.get("source_url") or candidate.get("url") or ""


        ctx = candidate.get("context_snippet") or candidate.get("context") or ""


        if isinstance(ctx, str):


            ctx = ctx.strip()[:240]


        else:


            ctx = ""


        raw = candidate.get("raw")


        if raw is None:


            raw = f"{candidate.get('value')}{candidate.get('unit') or ''}"


        ah = _sha1(f"{src}|{str(raw)[:120]}|{ctx}") if (src or ctx) else ""


        if ah:


            candidate["anchor_hash"] = ah



    if not candidate.get("candidate_id") and ah:


        candidate["candidate_id"] = str(ah)[:16]



    return candidate

def rebuild_metrics_from_snapshots(
    prev_response: dict,
    baseline_sources_cache: list,
    web_context: dict = None
) -> dict:
    """
    Deterministic rebuild using cached snapshots only.
    If sources unchanged, rebuilt metrics converge with analysis.

    Behavior:
      1) Primary: anchor_hash match via prev_response.metric_anchors
      2) Fallback: schema-first deterministic selection when anchor missing
         using metric_schema_frozen + context match + deterministic tie-break.

    NOTE: Dead/unreachable legacy code previously below an early return has been removed
    (explicitly approved).
    """
    import re
    import hashlib

    # =========================
    # PATCH RMS0 (ADDITIVE): typing imports for Dict/Any/List used below
    # - Prevents NameError if typing symbols are not imported globally.
    # =========================
    from typing import Dict, Any, List
    # =========================

    prev_response = prev_response if isinstance(prev_response, dict) else {}

    # =========================
    # PATCH RMS0.1 (ADDITIVE): accept anchors stored under alternate keys
    # - Backward compatible: does not change existing behavior if metric_anchors exists.
    # =========================
    prev_anchors = (
        prev_response.get("metric_anchors")
        or prev_response.get("anchors")
        or {}
    )
    # =========================

    if not isinstance(prev_anchors, dict):
        prev_anchors = {}

    rebuilt: Dict[str, Any] = {}

    # ---------- schema + canonical lookup ----------
    metric_schema = prev_response.get("metric_schema_frozen") or {}
    if not isinstance(metric_schema, dict):
        metric_schema = {}

    # =========================
    # PATCH RB2 (ADDITIVE): ensure baseline_sources_cache is a full list (rehydrate from snapshot store if needed)
    # - Handles cases where history rows store only a summarized baseline_sources_cache, but full snapshots exist
    #   in the Snapshots sheet (referenced by snapshot_store_ref / source_snapshot_hash).
    # =========================
    try:
        if (not isinstance(baseline_sources_cache, list)) or (isinstance(baseline_sources_cache, dict) and baseline_sources_cache.get("_summary") is True):
            # Prefer already-rehydrated cache on prev_response["results"]["baseline_sources_cache"]
            _maybe = (prev_response.get("results", {}) or {}).get("baseline_sources_cache")
            if isinstance(_maybe, list) and _maybe:
                baseline_sources_cache = _maybe
            else:
                store_ref = prev_response.get("snapshot_store_ref") or (prev_response.get("results", {}) or {}).get("snapshot_store_ref")
                source_hash = prev_response.get("source_snapshot_hash") or (prev_response.get("results", {}) or {}).get("source_snapshot_hash")
                if (not store_ref) and source_hash:
                    store_ref = f"gsheet:Snapshots:{source_hash}"
                if isinstance(store_ref, str) and store_ref.startswith("gsheet:Snapshots:"):
                    _hash = store_ref.split(":")[-1]
                    _full = load_full_snapshots_from_sheet(_hash)
                    if isinstance(_full, list) and _full:
                        baseline_sources_cache = _full
    except Exception:
        pass

    prev_can = prev_response.get("primary_metrics_canonical") or {}
    if not isinstance(prev_can, dict):
        prev_can = {}

    # =========================
    # PATCH RMS0.2 (ADDITIVE): compute full metric key universe
    # - Important: some metrics may not have anchors yet; we still must rebuild them
    #   (otherwise evolution "misses" metrics and diffs become unstable).
    # =========================
    metric_key_universe = set()
    try:
        metric_key_universe.update(list(prev_can.keys()))
        metric_key_universe.update(list(prev_anchors.keys()))
    except Exception:
        pass
        metric_key_universe = set(prev_can.keys()) if isinstance(prev_can, dict) else set()
    # =========================

    # ---------- deterministic candidate id (tie-breaker) ----------
    def _candidate_id(c: dict) -> str:
        try:
            url = str(c.get("source_url") or c.get("url") or "")
            ah = str(c.get("anchor_hash") or "")
            vn = c.get("value_norm")
            bu = str(c.get("base_unit") or c.get("unit") or c.get("unit_tag") or "")
            mk = str(c.get("measure_kind") or "")
            vn_s = ""
            if vn is not None:
                try:
                    vn_s = f"{float(vn):.12g}"
                except Exception:
                    pass
                    vn_s = str(vn)
            s = f"{url}|{ah}|{vn_s}|{bu}|{mk}"
            return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()
        except Exception:
            return ""

    # =====================================================================
    # PATCH RMS_E0 (ADDITIVE): small evidence extraction helper
    # - Ensures we consistently carry anchor/evidence fields onto rebuilt metrics.
    # - Purely additive; never affects selection logic.
    # =====================================================================
    def _extract_evidence_fields(c: dict) -> dict:
        if not isinstance(c, dict):
            return {}
        ctx = (c.get("context_snippet") or c.get("context") or "").strip()
        return {
            "raw": c.get("raw"),
            "candidate_id": c.get("candidate_id") or _candidate_id(c),
            "context_snippet": ctx[:240] if isinstance(ctx, str) else None,
            "measure_kind": c.get("measure_kind"),
            "measure_assoc": c.get("measure_assoc"),
            "start_idx": c.get("start_idx"),
            "end_idx": c.get("end_idx"),
            # optional passthroughs if upstream provides them
            "fingerprint": c.get("fingerprint"),
        }
    # =====================================================================

    # =====================================================================
    # PATCH RMS_E1 (ADDITIVE): anchor metadata getter
    # - Pull anchor_confidence (and any other safe fields) from prev_anchors entry.
    # - Helps diff/UI show confidence without recomputing.
    # =====================================================================
    def _anchor_meta(anchor_obj) -> dict:
        if isinstance(anchor_obj, dict):
            out = {}
            if anchor_obj.get("anchor_confidence") is not None:
                try:
                    out["anchor_confidence"] = float(anchor_obj.get("anchor_confidence"))
                except Exception:
                    pass
            # optional passthroughs if present
            if anchor_obj.get("source_url"):
                out["anchor_source_url"] = anchor_obj.get("source_url")
            if anchor_obj.get("raw"):
                out["anchor_raw"] = anchor_obj.get("raw")
            if anchor_obj.get("candidate_id"):
                out["anchor_candidate_id"] = anchor_obj.get("candidate_id")
            return out
        return {}
    # =====================================================================

    # ---------- collect candidates + anchor map ----------
    anchor_to_candidate: Dict[str, Dict[str, Any]] = {}
    all_candidates: List[Dict[str, Any]] = []

    for src in baseline_sources_cache or []:
        if not isinstance(src, dict):
            continue
        src_url = src.get("url") or src.get("source_url") or ""

        # =================================================================
        # PATCH RMS_E2 (ADDITIVE): capture source fingerprint on candidates
        # - Helps later debugging and “same source” proofs.
        # =================================================================
        src_fp = src.get("fingerprint")
        # =================================================================

        for c in (src.get("extracted_numbers") or []):
            if not isinstance(c, dict):
                continue

            # canonicalize if available (safe if repeated)
            try:
                c = canonicalize_numeric_candidate(dict(c))
            except Exception:
                pass
                c = dict(c)

            # ensure stable url carried through
            if not c.get("source_url"):
                c["source_url"] = src_url

            # =============================================================
            # PATCH RMS_E2 (ADDITIVE): attach fingerprint if missing
            # =============================================================
            if src_fp and not c.get("fingerprint"):
                c["fingerprint"] = src_fp
            # =============================================================

            ah = c.get("anchor_hash")
            if ah:
                if ah not in anchor_to_candidate:
                    anchor_to_candidate[ah] = c
                else:
                    old = anchor_to_candidate[ah]
                    if old.get("is_junk") and not c.get("is_junk"):
                        anchor_to_candidate[ah] = c

            all_candidates.append(c)

    # ---------- schema-first helpers ----------
    def _schema_for_key(metric_key: str) -> dict:
        d = metric_schema.get(metric_key)
        return d if isinstance(d, dict) else {}

    def _expected_from_schema(metric_key: str):
        d = _schema_for_key(metric_key)

        unit_family_s = str(d.get("unit_family") or "").strip().lower()
        dim_s = str(d.get("dimension") or "").strip().lower()
        unit_s = str(d.get("unit") or "").strip()
        name_l = str(d.get("name") or "").lower()

        expected_family = ""
        if unit_family_s in ("percent", "currency", "energy"):
            expected_family = unit_family_s
        else:
            ut = normalize_unit_tag(unit_s)
            if ut == "%":
                expected_family = "percent"
            elif ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
                expected_family = "energy"
            elif dim_s == "currency":
                expected_family = "currency"

        currencyish = (unit_family_s == "currency" or dim_s == "currency")

        expected_kind = None
        if expected_family == "percent":
            if any(k in name_l for k in ["growth", "cagr", "increase", "decrease", "yoy", "qoq", "mom", "rate"]):
                expected_kind = "growth_pct"
            else:
                expected_kind = "share_pct"
        if currencyish or expected_family == "currency":
            expected_kind = "money"
        if expected_kind is None and any(k in name_l for k in [
            "units", "unit sales", "vehicle sales", "vehicles sold", "sold",
            "deliveries", "shipments", "registrations", "volume"
        ]):
            expected_kind = "count_units"

        kw = d.get("keywords")
        schema_keywords = [str(x).strip() for x in kw] if isinstance(kw, list) else []
        schema_keywords = [x for x in schema_keywords if x]

        return expected_family, currencyish, expected_kind, schema_keywords, unit_s

    def _ctx_match_score(tokens: List[str], ctx: str) -> float:
        fn = globals().get("calculate_context_match")
        if callable(fn):
            try:
                return float(fn(tokens, ctx))
            except Exception:
                pass

        c = (ctx or "").lower()
        toks = [t.lower() for t in (tokens or []) if t and len(t) >= 2]
        if not toks:
            return 0.0
        hit = sum(1 for t in toks if t in c)
        return hit / max(1, len(toks))

    def _currency_evidence(raw: str, ctx: str) -> bool:
        r = (raw or "")
        c = (ctx or "").lower()
        if any(s in r for s in ["$", "S$", "€", "£"]):
            return True
        if any(code in c for code in [" usd", "sgd", " eur", " gbp", " aud", " cad", " jpy", " cny", " rmb"]):
            return True
        if any(k in c for k in ["revenue", "turnover", "valuation", "market size", "market value", "profit", "earnings", "ebitda"]):
            return True
        return False

    def _is_yearish_value(v) -> bool:
        try:
            iv = int(float(v))
            return 1900 <= iv <= 2099
        except Exception:
            return False

    # =========================
    # PATCH RMS_BASE (ADDITIVE): helper to overlay rebuilt fields onto prior canonical metric
    # - Keeps metric identity fields (name/canonical_key/dimension/etc.) stable for diffing.
    # - Only overwrites value-ish/source-ish fields with rebuilt candidate data.
    # =========================
    def _overlay_base(metric_key: str, patch: dict) -> dict:
        base = {}
        try:
            if isinstance(prev_can.get(metric_key), dict):
                base = dict(prev_can.get(metric_key) or {})
        except Exception:
            pass
            base = {}
        out = dict(base)
        try:
            if isinstance(patch, dict):
                out.update(patch)
        except Exception:
            return out
    # =========================

    # ---------- 1) primary rebuild by anchor ----------
    rebuilt_by_anchor = set()

    for metric_key, anchor in prev_anchors.items():
        ah = None
        if isinstance(anchor, dict):
            ah = anchor.get("anchor_hash") or anchor.get("anchor")
        elif isinstance(anchor, str):
            ah = anchor

        if ah and ah in anchor_to_candidate:
            c = anchor_to_candidate[ah]

            # =========================
            # PATCH RMS1 (ADDITIVE): overlay rebuilt candidate onto base canonical metric
            # - Keeps canonical identity fields intact for downstream diffs/UI.
            # =========================
            rebuilt[metric_key] = _overlay_base(metric_key, {
                "value": c.get("value"),
                "unit": c.get("unit"),
                "value_norm": c.get("value_norm"),
                "base_unit": c.get("base_unit"),
                "unit_tag": c.get("unit_tag"),
                "unit_family": c.get("unit_family"),
                "anchor_hash": ah,
                "source_url": c.get("source_url"),
                "context_snippet": (c.get("context_snippet") or c.get("context") or "")[:240],
                "measure_kind": c.get("measure_kind"),
                "measure_assoc": c.get("measure_assoc"),
                "rebuild_method": "anchor",

                # =============================================================
                # PATCH RMS_E3 (ADDITIVE): attach evidence + anchor metadata
                # - candidate_id used as stable ID for UI/debugging
                # - anchor_confidence helps diff/UI set match_confidence
                # =============================================================
                **_extract_evidence_fields(c),
                **_anchor_meta(anchor),
                # =============================================================
            })
            # =========================

            rebuilt_by_anchor.add(metric_key)

    # ---------- 2) fallback rebuild when anchor missing ----------
    # NOTE: existing loop only iterated prev_anchors.keys(); we keep it as-is,
    # and then add an extra additive loop to cover metrics without anchors. (PATCH RMS2)
    for metric_key in prev_anchors.keys():
        if metric_key in rebuilt_by_anchor:
            continue

        expected_family, currencyish, expected_kind, schema_keywords, schema_unit = _expected_from_schema(metric_key)

        # conservative fallback if schema is thin
        if not expected_family and metric_key in prev_can and isinstance(prev_can.get(metric_key), dict):
            pm = prev_can.get(metric_key) or {}
            ut = normalize_unit_tag(pm.get("unit") or schema_unit or "")
            if ut == "%":
                expected_family = "percent"
            elif ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
                expected_family = "energy"

        # tokens for context scoring
        tokens = []
        if schema_keywords:
            tokens = schema_keywords
        else:
            # fallback to build_metric_keywords(schema_name)
            schema_name = ""
            try:
                schema_name = str(_schema_for_key(metric_key).get("name") or "")
            except Exception:
                pass
                schema_name = ""
            fn_bmk = globals().get("build_metric_keywords")
            if callable(fn_bmk):
                try:
                    tokens = fn_bmk(schema_name or metric_key) or []
                except Exception:
                    pass
                    tokens = []
            else:
                tokens = []

        best = None
        best_key = None
        best_score = -1.0

        for c in all_candidates:
            if not isinstance(c, dict):
                continue

            # fallback skips junk (anchor path already handled above)
            if c.get("is_junk") is True:
                continue

            ctx = (c.get("context") or c.get("context_snippet") or "").strip()
            if not ctx:
                continue

            # stop timeline years contaminating non-year metrics
            if expected_family not in ("percent", "energy") and not (currencyish or expected_family == "currency"):
                if (c.get("unit_tag") in ("", None)) and _is_yearish_value(c.get("value")):
                    continue
            # =====================================================================
            # PATCH FIX41AFC5 (ADDITIVE): hard-reject year-only + unitless candidates (evolution rebuild parity)
            # Why:
            #   - Prevent "2024"/"2025" from being selected as metric values (especially count/magnitude_other)
            #   - Applies regardless of expected_family, but only when the candidate is unitless/non-percent.
            # Determinism:
            #   - Pure filtering; stable ordering; no refetch.
            # =====================================================================
            try:
                _vnorm = c.get("value_norm", None)
                if _vnorm is None:
                    _vnorm = c.get("value", None)
                _is_year = _is_yearish_value(_vnorm)
                _cand_ut0 = (c.get("unit_tag") or normalize_unit_tag(c.get("unit") or "") or "").strip()
                _cand_fam0 = (c.get("unit_family") or unit_family(_cand_ut0) or "").strip().lower()
                _mk0 = str(c.get("measure_kind") or "").strip().lower()
                _is_pct0 = bool(c.get("is_percent") or c.get("has_percent") or (_cand_ut0 == "%") or (_cand_fam0 == "percent"))
                _has_curr0 = bool(str(c.get("currency_symbol") or c.get("currency") or "").strip())
                _has_unit_ev0 = bool(_cand_ut0 or _cand_fam0 or _is_pct0 or _has_curr0)
                # year-only guard (unitless, non-percent, non-currency)
                if _is_year and (not _has_unit_ev0) and (not _is_pct0) and (not _has_curr0) and (_mk0 in ("magnitude_other", "count_units", "count", "number", "")):
                    try:
                        _fix41afc5_dbg["rejected_year_only"] = int(_fix41afc5_dbg.get("rejected_year_only", 0) or 0) + 1
                    except Exception:
                        pass
                    continue
                # magnitude_other guard (unitless, non-percent, non-currency)
                if (_mk0 == "magnitude_other") and (not _has_unit_ev0) and (not _is_pct0) and (not _has_curr0):
                    try:
                        _fix41afc5_dbg["rejected_magnitude_other_unitless"] = int(_fix41afc5_dbg.get("rejected_magnitude_other_unitless", 0) or 0) + 1
                    except Exception:
                        pass
                    continue
            except Exception:
                pass

            cand_ut = c.get("unit_tag") or normalize_unit_tag(c.get("unit") or "")
            cand_fam = (c.get("unit_family") or unit_family(cand_ut) or "").strip().lower()
            mk = c.get("measure_kind")

            # unit-family gating
            if expected_family == "percent":
                if cand_fam != "percent" and cand_ut != "%":
                    continue
            elif expected_family == "energy":
                if cand_fam != "energy":
                    continue
            elif currencyish or expected_family == "currency":
                if cand_fam not in ("currency", "magnitude"):
                    continue
                if not _currency_evidence(c.get("raw", ""), ctx):
                    continue
                if mk == "count_units":
                    continue

            # measure-kind gating (only if candidate provides it)
            if expected_kind and mk and mk != expected_kind:
                continue

            # normalize value for ranking
            try:
                c2 = canonicalize_numeric_candidate(dict(c))
            except Exception:
                pass
                c2 = c

            val_norm = c2.get("value_norm")
            if val_norm is None:
                try:
                    val_norm = float(c2.get("value"))
                except Exception:
                    pass
                    continue

            ctx_score = _ctx_match_score(tokens, ctx)
            if ctx_score <= 0.0:
                continue

            url = str(c2.get("source_url") or c2.get("url") or "")
            cid = c2.get("candidate_id") or _candidate_id({**c2, "value_norm": val_norm})

            # deterministic tie-break (max)
            key = (
                float(ctx_score),
                float(val_norm),
                url,
                str(cid),
            )

            if best_key is None or key > best_key:
                best_key = key
                best_score = float(ctx_score)
                best = {**c2, "value_norm": val_norm, "candidate_id": cid}

        if best:
            # =========================
            # PATCH RMS1 (ADDITIVE): overlay onto base canonical metric
            # =========================
            rebuilt[metric_key] = _overlay_base(metric_key, {
                "value": best.get("value"),
                "unit": best.get("unit") or best.get("unit_tag"),
                "value_norm": best.get("value_norm"),
                "base_unit": best.get("base_unit"),
                "unit_tag": best.get("unit_tag"),
                "unit_family": best.get("unit_family"),
                "anchor_hash": best.get("anchor_hash"),
                "source_url": best.get("source_url") or best.get("url"),
                "context_snippet": (best.get("context_snippet") or best.get("context") or "")[:240],
                "measure_kind": best.get("measure_kind"),
                "measure_assoc": best.get("measure_assoc"),
                "rebuild_method": "schema_fallback",
                "fallback_ctx_score": round(best_score, 6),
                "candidate_id": best.get("candidate_id"),

                # =============================================================
                # PATCH RMS_E4 (ADDITIVE): attach standardized evidence fields
                # - Ensures candidate_id/raw/context are always present when possible.
                # - Adds anchor_confidence derived from fallback_ctx_score.
                # =============================================================
                **_extract_evidence_fields(best),
                "anchor_confidence": float(min(100.0, max(0.0, best_score) * 100.0)) if best_score is not None else 0.0,
                # =============================================================
            })
            # =========================

    # =========================
    # PATCH RMS2 (ADDITIVE): ensure metrics without anchors are also rebuilt
    # - Your existing fallback loop only iterates prev_anchors.keys().
    # - This loop covers the remaining canonical metrics (prev_can keys) that are missing
    #   from prev_anchors, using the SAME schema-first logic (copied, not refactored).
    # - Additive: does not alter prior behavior for anchored metrics.
    # =========================
    for metric_key in (metric_key_universe or set()):
        if metric_key in rebuilt:
            continue

        expected_family, currencyish, expected_kind, schema_keywords, schema_unit = _expected_from_schema(metric_key)

        if not expected_family and metric_key in prev_can and isinstance(prev_can.get(metric_key), dict):
            pm = prev_can.get(metric_key) or {}
            ut = normalize_unit_tag(pm.get("unit") or schema_unit or "")
            if ut == "%":
                expected_family = "percent"
            elif ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
                expected_family = "energy"

        tokens = []
        if schema_keywords:
            tokens = schema_keywords
        else:
            schema_name = ""
            try:
                schema_name = str(_schema_for_key(metric_key).get("name") or "")
            except Exception:
                pass
                schema_name = ""
            fn_bmk = globals().get("build_metric_keywords")
            if callable(fn_bmk):
                try:
                    tokens = fn_bmk(schema_name or metric_key) or []
                except Exception:
                    pass
                    tokens = []
            else:
                tokens = []

        best = None
        best_key = None
        best_score = -1.0

        for c in all_candidates:
            if not isinstance(c, dict):
                continue
            if c.get("is_junk") is True:
                continue

            ctx = (c.get("context") or c.get("context_snippet") or "").strip()
            if not ctx:
                continue

            if expected_family not in ("percent", "energy") and not (currencyish or expected_family == "currency"):
                if (c.get("unit_tag") in ("", None)) and _is_yearish_value(c.get("value")):
                    continue

            cand_ut = c.get("unit_tag") or normalize_unit_tag(c.get("unit") or "")
            cand_fam = (c.get("unit_family") or unit_family(cand_ut) or "").strip().lower()
            mk = c.get("measure_kind")

            if expected_family == "percent":
                if cand_fam != "percent" and cand_ut != "%":
                    continue
            elif expected_family == "energy":
                if cand_fam != "energy":
                    continue
            elif currencyish or expected_family == "currency":
                if cand_fam not in ("currency", "magnitude"):
                    continue
                if not _currency_evidence(c.get("raw", ""), ctx):
                    continue
                if mk == "count_units":
                    continue

            if expected_kind and mk and mk != expected_kind:
                continue

            try:
                c2 = canonicalize_numeric_candidate(dict(c))
            except Exception:
                pass
                c2 = c

            val_norm = c2.get("value_norm")
            if val_norm is None:
                try:
                    val_norm = float(c2.get("value"))
                except Exception:
                    pass
                    continue

            ctx_score = _ctx_match_score(tokens, ctx)
            if ctx_score <= 0.0:
                continue

            url = str(c2.get("source_url") or c2.get("url") or "")
            cid = c2.get("candidate_id") or _candidate_id({**c2, "value_norm": val_norm})

            key = (
                float(ctx_score),
                float(val_norm),
                url,
                str(cid),
            )

            if best_key is None or key > best_key:
                best_key = key
                best_score = float(ctx_score)
                best = {**c2, "value_norm": val_norm, "candidate_id": cid}

        if best:
            rebuilt[metric_key] = _overlay_base(metric_key, {
                "value": best.get("value"),
                "unit": best.get("unit") or best.get("unit_tag"),
                "value_norm": best.get("value_norm"),
                "base_unit": best.get("base_unit"),
                "unit_tag": best.get("unit_tag"),
                "unit_family": best.get("unit_family"),
                "anchor_hash": best.get("anchor_hash"),
                "source_url": best.get("source_url") or best.get("url"),
                "context_snippet": (best.get("context_snippet") or best.get("context") or "")[:240],
                "measure_kind": best.get("measure_kind"),
                "measure_assoc": best.get("measure_assoc"),
                "rebuild_method": "schema_fallback_no_anchor",
                "fallback_ctx_score": round(best_score, 6),
                "candidate_id": best.get("candidate_id"),

                # =============================================================
                # PATCH RMS_E5 (ADDITIVE): attach standardized evidence fields
                # =============================================================
                **_extract_evidence_fields(best),
                "anchor_confidence": float(min(100.0, max(0.0, best_score) * 100.0)) if best_score is not None else 0.0,
                # =============================================================
            })
        else:
            # stable placeholder (do not fabricate)
            if isinstance(prev_can.get(metric_key), dict):
                rebuilt[metric_key] = _overlay_base(metric_key, {
                    "rebuild_method": "not_found_in_snapshots",

                    # =============================================================
                    # PATCH RMS_E6 (ADDITIVE): keep evidence fields present for stable shape
                    # =============================================================
                    "anchor_hash": None,
                    "source_url": None,
                    "context_snippet": None,
                    "raw": None,
                    "candidate_id": None,
                    "anchor_confidence": 0.0,
                    # =============================================================
                })
    # =========================

    # =====================================================================
    # PATCH RMS_FALLBACK1 (ADDITIVE): never return empty rebuild when we have a baseline universe
    # Why:
    #   - Source-anchored evolution is snapshot-gated; if snapshots exist but rebuild fails
    #     (missing anchors/schema mismatch/edge cases), returning {} causes evolution to hard-fail.
    #   - For determinism + drift-0 testing, we prefer a safe fallback that preserves the
    #     canonical metric universe from the previous analysis while emitting an explicit flag.
    #
    # Behavior:
    #   - If 'rebuilt' is empty/non-dict, fall back to prev_response['primary_metrics_canonical'].
    #   - Marks each metric with '_rebuild_fallback_used': True (additive field).
    #   - DOES NOT fabricate new values; it reuses previous canonical values only.
    # =====================================================================
    try:
        if not isinstance(rebuilt, dict) or not rebuilt:
            prev_universe = {}
            if isinstance(prev_response, dict):
                prev_universe = prev_response.get("primary_metrics_canonical") or {}
            if isinstance(prev_universe, dict) and prev_universe:
                rebuilt = {}
                for ck in sorted(prev_universe.keys()):
                    m = prev_universe.get(ck)
                    if isinstance(m, dict):
                        mm = dict(m)
                        mm["_rebuild_fallback_used"] = True
                        # Ensure ES7 fields exist (pure enrichment)
                        mm.setdefault("canonical_key", ck)
                        mm.setdefault("anchor_used", False)
                        mm.setdefault("anchor_confidence", 0.0)
                        rebuilt[ck] = mm
                # Add top-level marker (additive)
                try:
                    rebuilt["_rebuild_status"] = "fallback_prev_primary_metrics_canonical"
                except Exception:
                    pass
    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH FIX41AFC5 (ADDITIVE): attach eligibility-hardening debug counters
    # =====================================================================
    try:
        if isinstance(rebuilt, dict):
            rebuilt.setdefault("_fix41afc5_debug", {})
            if isinstance(rebuilt.get("_fix41afc5_debug"), dict):
                rebuilt["_fix41afc5_debug"].update(dict(_fix41afc5_dbg))

    # =====================================================================
    except Exception:
        pass

    # PATCH FIX2AD_INJ_ATTEMPT_GATING_DEBUG_V1 (ADDITIVE)
    # Purpose: Explain precisely why an injected URL may be "admitted" (diag) but not "attempted" (fetch).
    # Emits a small deterministic debug object into web_context['diag_injected_urls'].
    # - No behavior changes; debug only.
    # - Compares: raw extra_urls -> normalized extra_urls -> merged urls list.
    # =====================================================================
    try:
        if isinstance(web_context, dict):
            web_context.setdefault("diag_injected_urls", {})
            if isinstance(web_context.get("diag_injected_urls"), dict):
                _fx2ad_diag = web_context.get("diag_injected_urls")
                _fx2ad_raw_extra = []
                try:
                    _fx2ad_raw_extra = list((web_context or {}).get("extra_urls") or [])
                except Exception:
                    pass
                    _fx2ad_raw_extra = []
                _fx2ad_norm_extra = []
                try:
                    _fx2ad_norm_extra = _inj_diag_norm_url_list(_fx2ad_raw_extra)
                except Exception:
                    pass
                    _fx2ad_norm_extra = []
                _fx2ad_urls_list = []
                try:
                    _fx2ad_urls_list = [str(u or "").strip() for u in (urls or []) if str(u or "").strip()]
                except Exception:
                    pass
                    _fx2ad_urls_list = []
                _fx2ad_norm_urls_list = []
                try:
                    _fx2ad_norm_urls_list = _inj_diag_norm_url_list(_fx2ad_urls_list)
                except Exception:
                    pass
                    _fx2ad_norm_urls_list = []

                # If "admitted" exists (from earlier trace), compare it to extra_urls.
                _fx2ad_admitted_norm = []
                try:
                    _fx2ad_admitted_norm = _inj_diag_norm_url_list(_fx2ad_diag.get("admitted") or _fx2ad_diag.get("extra_urls_admitted") or [])
                except Exception:
                    pass
                    _fx2ad_admitted_norm = []

                _fx2ad_rows = []
                try:
                    for _u in _fx2ad_admitted_norm[:80]:
                        _row = {
                            "url": _u,
                            "in_web_context_extra_urls_norm": (_u in set(_fx2ad_norm_extra)),
                            "in_urls_after_merge_norm": (_u in set(_fx2ad_norm_urls_list)),
                        }
                        # Best-effort reason classification
                        if not _row["in_web_context_extra_urls_norm"]:
                            _row["gate_reason"] = "not_in_web_context_extra_urls"
                        elif not _row["in_urls_after_merge_norm"]:
                            _row["gate_reason"] = "not_merged_into_urls_list"
                        else:
                            _row["gate_reason"] = "present_in_urls_list"
                        _fx2ad_rows.append(_row)
                except Exception:
                    pass
                    _fx2ad_rows = _fx2ad_rows

                _fx2ad_diag["fix2ad_inj_attempt_gating_v1"] = {
                    "raw_extra_urls_count": len(_fx2ad_raw_extra),
                    "norm_extra_urls_count": len(_fx2ad_norm_extra),
                    "urls_list_count": len(_fx2ad_urls_list),
                    "urls_list_norm_count": len(_fx2ad_norm_urls_list),
                    "admitted_norm_count": len(_fx2ad_admitted_norm),
                    "admitted_gate_rows": _fx2ad_rows,
                    "sample_norm_extra_urls": _fx2ad_norm_extra[:20],
                }
    except Exception:
        pass


    # =====================================================================
    # PATCH FIX2Y_CANDIDATE_AUTOPSY_V1 (ADDITIVE): attach autopsy to web_context
    # =====================================================================
    try:
        if isinstance(web_context, dict):
            web_context["fix2y_candidate_autopsy_v1"] = _fix2y_autopsy
    except Exception:
        pass
    # =====================================================================
    # END PATCH FIX2Y_CANDIDATE_AUTOPSY_V1
    # =====================================================================

    return rebuilt




# =====================================================================
# PATCH RMS_MIN1 (ADDITIVE): Minimal schema-driven rebuild from snapshots
# ---------------------------------------------------------------------
# Goal:
#   - Provide a deterministic, evolution-safe metric rebuild that uses ONLY:
#       (a) baseline_sources_cache snapshots (and their extracted_numbers)
#       (b) frozen metric schema (metric_schema_frozen)
#   - No re-fetch, no LLM inference, no heuristic "best guess" beyond schema fields.
#
# Contract:
#   - Returns a dict shaped like primary_metrics_canonical:
#       { canonical_key: { ...metric fields... } }
#   - Deterministic tie-break ordering.
# =====================================================================


# =====================================================================
# PATCH F (deterministic): Explicit candidate exclusion in rebuild stage
#   - Enforce that ANY candidate flagged as junk is excluded from:
#       * candidate indexing
#       * candidate scoring
#       * final metric assignment
#   - Additionally, suppress "year-like" unitless tokens (e.g., 2024/2025) for
#     non-year metrics (currency/percent/rate/ratio/growth/etc.) to prevent
#     year fixation during evolution.
#   - Purely deterministic: no LLM, no refetch, no heuristics outside schema cues.
# =====================================================================

def _candidate_disallowed_for_metric(_cand: dict, _spec: dict = None) -> bool:
    """Return True if a snapshot candidate must not be used to assign a metric value."""
    if not isinstance(_cand, dict):
        return True

    # 1) Hard exclusion: explicit junk flags / reasons from extraction phase
    if _cand.get("is_junk") is True:
        return True
    jr = str(_cand.get("junk_reason") or "").strip().lower()
    if jr:
        # If a junk_reason exists, treat it as non-selectable deterministically.
        return True

    # 2) Deterministic anti-year-fixation: unitless year-like tokens are disallowed
    #    for most numeric metrics (unless schema clearly indicates a "year" metric).
    try:
        v = _cand.get("value_norm", _cand.get("value"))
        unitish = str(_cand.get("base_unit") or _cand.get("unit_tag") or _cand.get("unit") or "").strip()
        if unitish == "" and isinstance(v, (int, float)):
            if abs(float(v) - round(float(v))) < 1e-9:
                vi = int(round(float(v)))
                if 1900 <= vi <= 2100:
                    if isinstance(_spec, dict):
                        nm = str(_spec.get("name") or "").lower()
                        cid = str(_spec.get("canonical_id") or _spec.get("canonical_key") or "").lower()
                        kws = _spec.get("keywords") or []
                        kws_s = " ".join([str(k).lower() for k in kws]) if isinstance(kws, list) else str(kws).lower()

                        # Allow explicit year metrics
                        if ("year" in nm) or ("year" in cid) or ("founded" in nm) or ("since" in nm) or ("year" in kws_s):
                            return False

                        uf = str(_spec.get("unit_family") or "").lower().strip()
                        ut = str(_spec.get("unit_tag") or _spec.get("unit") or "").lower().strip()

                        # For common non-year metric families, exclude year-like tokens.
                        if uf in ("currency", "percent", "rate", "ratio", "growth", "share"):
                            return True
                        if "%" in ut:
                            return True
                        if any(w in nm for w in ("cagr", "revenue", "growth", "market", "sales", "profit", "margin", "volume")):
                            return True

                    # Default: unitless year-like token is not a valid metric value.
                    return True
    except Exception:
        return False


# ===============================
# REFACTOR03 (ADDITIVE)
# Unit-family + scale eligibility guardrails for schema-only rebuild
# and unit-mismatch detection for Diff Panel V2.
#
# Motivation (from REFACTOR02 JSONs):
# - A currency token like "US$ 996.3bn" was being selected for a magnitude/count schema key
#   (e.g., global_ev_chargers_2040__unit_count), causing nonsensical diffs (B vs M).
# - We fix this *at selection time* and also *at diff time* (so any future regressions are
#   surfaced as unit_mismatch rather than as a bogus increased/decreased classification).
# Determinism:
# - Pure filtering + stable logic; no refetch; no randomness.
# ===============================

def _refactor03_has_currency_evidence_v1(text: str) -> bool:
    try:
        t = (text or "").lower()
        if not t:
            return False
        # Common currency markers; keep conservative to avoid false positives.
        markers = ["us$", "usd", "eur", "€", "gbp", "£", "sgd", "s$", "aud", "cad", "jpy", "¥", "$"]
        return any(m in t for m in markers)
    except Exception:
        return False


def _refactor03_has_percent_evidence_v1(text: str) -> bool:
    try:
        t = (text or "").lower()
        if not t:
            return False
        return ("%" in t) or ("percent" in t) or ("pct" in t)
    except Exception:
        return False


def _refactor03_extract_text_from_metric_v1(metric: dict) -> str:
    try:
        if not isinstance(metric, dict):
            return ""
        parts = []
        # direct fields
        for k in ("raw", "context_snippet", "source_url", "name", "canonical_key"):
            v = metric.get(k)
            if v:
                parts.append(str(v))
        # evidence list
        ev = metric.get("evidence")
        if isinstance(ev, list):
            for e in ev[:5]:
                if isinstance(e, dict):
                    if e.get("raw"):
                        parts.append(str(e.get("raw")))
                    if e.get("context_snippet"):
                        parts.append(str(e.get("context_snippet")))
        return " | ".join([p for p in parts if p])
    except Exception:
        return ""



def _refactor04_unit_evidence_text_from_metric_v1(metric: dict, include_context: bool = False) -> str:
    """Return a *narrow* evidence string for unit-family checks.

    Rationale:
      - context_snippet can include nearby unrelated % / currency (e.g. CAGR lines),
        causing false unit_mismatch for magnitude/count metrics.
      - For magnitude keys we prefer token/raw evidence only.
    """
    try:
        if not isinstance(metric, dict):
            return ""
        parts = []
        # Prefer direct raw-ish fields first
        for k in ("raw", "value_raw", "value_text", "value_str"):
            v = metric.get(k)
            if v:
                parts.append(str(v))
        # Unit tags themselves (helps currency/percent keys when raw is short)
        for k in ("unit_tag", "unit", "base_unit", "unit_family"):
            v = metric.get(k)
            if v:
                parts.append(str(v))
        # Evidence list: always include evidence.raw; include context only if requested
        ev = metric.get("evidence")
        if isinstance(ev, list):
            for e in ev[:5]:
                if not isinstance(e, dict):
                    continue
                if e.get("raw"):
                    parts.append(str(e.get("raw")))
                if include_context and e.get("context_snippet"):
                    parts.append(str(e.get("context_snippet")))
        # Optionally include metric context_snippet (but keep it last)
        if include_context and metric.get("context_snippet"):
            parts.append(str(metric.get("context_snippet")))
        return " | ".join([p for p in parts if p])
    except Exception:
        return ""


def _refactor04_scale_multiplier_from_unit_tag_v1(unit_tag: str) -> float:
    """Convert common magnitude tags to a scale multiplier (K/M/B/T)."""
    try:
        t = str(unit_tag or "").upper().strip()
        if t == "K":
            return 1e3
        if t == "M":
            return 1e6
        if t == "B":
            return 1e9
        if t == "T":
            return 1e12
    except Exception:
        pass
    return 1.0


def _refactor04_get_metric_schema_frozen_v1(obj: dict) -> dict:
    """Best-effort retrieval of metric_schema_frozen from common nesting patterns."""
    try:
        if not isinstance(obj, dict):
            return {}
        if isinstance(obj.get("metric_schema_frozen"), dict):
            return obj.get("metric_schema_frozen") or {}
        pr = obj.get("primary_response")
        if isinstance(pr, dict) and isinstance(pr.get("metric_schema_frozen"), dict):
            return pr.get("metric_schema_frozen") or {}
        res = obj.get("results")
        if isinstance(res, dict):
            if isinstance(res.get("metric_schema_frozen"), dict):
                return res.get("metric_schema_frozen") or {}
            pr2 = res.get("primary_response")
            if isinstance(pr2, dict) and isinstance(pr2.get("metric_schema_frozen"), dict):
                return pr2.get("metric_schema_frozen") or {}
        return {}
    except Exception:
        return {}


def _refactor04_enrich_pmc_units_v1(pmc: dict, prev_response: dict = None) -> dict:
    """Ensure PMC rows carry unit_tag/unit_family/multiplier_to_base (and base_unit) for parity + diffing."""
    try:
        if not isinstance(pmc, dict) or not pmc:
            return pmc
        schema = _refactor04_get_metric_schema_frozen_v1(prev_response) if isinstance(prev_response, dict) else {}
        for ckey, m in list(pmc.items()):
            if not isinstance(m, dict):
                continue
            spec = schema.get(ckey) if isinstance(schema, dict) else None
            spec = spec if isinstance(spec, dict) else {}
            # unit_tag
            ut = m.get("unit_tag") or m.get("base_unit") or m.get("unit") or spec.get("unit_tag") or spec.get("unit") or ""
            ut = str(ut or "").strip()
            # unit_family
            uf = m.get("unit_family") or spec.get("unit_family") or _refactor03_unit_family_from_ckey_v1(ckey)
            uf = str(uf or "").strip()
            # multiplier_to_base (scale)
            mult = m.get("multiplier_to_base")
            if mult is None:
                mult = _refactor04_scale_multiplier_from_unit_tag_v1(ut)
            try:
                mult = float(mult)
            except Exception:
                mult = _refactor04_scale_multiplier_from_unit_tag_v1(ut)

            # write back (do not delete existing fields)
            if ut and not m.get("unit"):
                m["unit"] = ut
            if ut and not m.get("unit_tag"):
                m["unit_tag"] = ut
            if ut and not m.get("base_unit"):
                m["base_unit"] = ut
            if uf and not m.get("unit_family"):
                m["unit_family"] = uf
            if m.get("multiplier_to_base") is None and mult is not None:
                m["multiplier_to_base"] = mult
    except Exception:
        return pmc
    return pmc


def _refactor03_unit_family_from_ckey_v1(canonical_key: str) -> str:
    try:
        ck = str(canonical_key or "").lower()
        if "__currency" in ck:
            return "currency"
        if "__percent" in ck:
            return "percent"
        # treat all "__unit_*" as magnitude/count family
        if "__unit_" in ck:
            return "magnitude"
        return "unknown"
    except Exception:
        return "unknown"


def _refactor03_candidate_rejected_by_unit_family_v1(cand: dict, spec: dict = None) -> bool:
    """Return True if candidate is incompatible with schema's unit family / tag."""
    if not isinstance(cand, dict):
        return True
    if not isinstance(spec, dict):
        return False  # no schema => don't over-filter

    try:
        uf = str(spec.get("unit_family") or "").lower().strip()
        ut = str(spec.get("unit_tag") or spec.get("unit") or "").strip()
        unit_tag = str(cand.get("unit_tag") or cand.get("unit") or cand.get("base_unit") or "").strip()

        raw_core = str(cand.get("raw") or "")


        raw_ctx = " ".join([


            raw_core,


            str(cand.get("context_snippet") or ""),


            str(cand.get("context") or ""),


        ])


        # For magnitude/count metrics, avoid broad context leakage (%/currency nearby).


        raw_for = raw_ctx


        try:


            if uf not in ("currency", "money", "percent", "rate", "ratio", "growth", "share"):


                raw_for = raw_core.strip() or raw_ctx


        except Exception:


            raw_for = raw_core.strip() or raw_ctx


        is_cur = _refactor03_has_currency_evidence_v1(raw_for)


        is_pct = _refactor03_has_percent_evidence_v1(raw_for)

        # 1) unit-family gating
        if uf in ("currency", "money"):
            if not is_cur:
                return True
        elif uf in ("percent", "rate", "ratio", "growth", "share"):
            if not is_pct:
                return True
        else:
            # magnitude/count: reject obvious currency/percent
            if is_cur or is_pct:
                return True

        # 2) unit-tag scale gating for magnitude/count metrics (K/M/B)
        try:
            ut_up = ut.upper().strip()
            unit_up = unit_tag.upper().strip()
            if ut_up in ("K", "M", "B", "T") and unit_up in ("K", "M", "B", "T") and ut_up != unit_up:
                return True
        except Exception:
            pass

        return False
    except Exception:
        return False



def _refactor27_candidate_rejected_currency_date_fragment_v1(cand: dict, spec: dict = None) -> bool:
    """Reject date-fragment candidates like '01' in contexts such as 'July 01, 2025' for currency-ish metrics.

    Rationale:
      - Some news pages include datelines (e.g., 'July 01, 2025') near genuine currency values.
      - Weak context-based currency evidence can cause day-of-month tokens to outscore real values.
    Determinism:
      - Pure filter; does not invent candidates or refetch content.
    """
    try:
        if not isinstance(cand, dict) or not isinstance(spec, dict):
            return False
        uf = str(spec.get("unit_family") or "").lower().strip()
        if uf not in ("currency", "money"):
            return False

        raw = str(cand.get("raw") or "").strip()
        if not raw:
            return False

        # Only target tiny integer tokens that look like day-of-month (01..31)
        try:
            v = cand.get("value_norm")
            if v is None:
                v = cand.get("value")
            iv = int(float(v))
        except Exception:
            return False

        if iv < 1 or iv > 31:
            return False

        if not re.fullmatch(r"0?\d{1,2}", raw):
            return False

        ctx = " ".join([
            str(cand.get("context_snippet") or ""),
            str(cand.get("context") or ""),
        ]).lower()

        if not ctx:
            return False

        # Month + year pattern indicates this is very likely a dateline token
        months = (
            "jan", "january", "feb", "february", "mar", "march", "apr", "april",
            "may", "jun", "june", "jul", "july", "aug", "august", "sep", "sept", "september",
            "oct", "october", "nov", "november", "dec", "december",
        )
        if any(m in ctx for m in months) and re.search(r"\b(19|20)\d{2}\b", ctx):
            # If the raw itself directly carries currency markers, do not reject
            raw_l = raw.lower()
            if any(sym in raw_l for sym in ("$", "usd", "eur", "gbp", "sgd", "aud", "cad", "hk$", "us$")):
                return False
            return True

        return False
    except Exception:
        return False



def _refactor03_diff_unit_mismatch_v1(prev_key: str, prev_metric: dict, cur_metric: dict, prev_unit: str = None, cur_unit: str = None) -> bool:
    """Return True if the prev/current pair is not comparable due to unit family or scale mismatch."""
    try:
        expected = _refactor03_unit_family_from_ckey_v1(prev_key)

        # REFACTOR04: avoid unit-family false positives due to broad context_snippet leakage.
        # - For magnitude/count metrics, prefer token/raw evidence only.
        # - For currency/percent metrics, allow context to help detect markers.
        if str(expected or "").lower().strip() in ("currency", "percent"):
            prev_txt = _refactor04_unit_evidence_text_from_metric_v1(prev_metric, include_context=True)
            cur_txt = _refactor04_unit_evidence_text_from_metric_v1(cur_metric, include_context=True)
        else:
            prev_txt = _refactor04_unit_evidence_text_from_metric_v1(prev_metric, include_context=False)
            cur_txt = _refactor04_unit_evidence_text_from_metric_v1(cur_metric, include_context=False)
            # If raw evidence is missing, fall back to context-inclusive extraction.
            try:
                if not str(prev_txt or "").strip():
                    prev_txt = _refactor04_unit_evidence_text_from_metric_v1(prev_metric, include_context=True)
                if not str(cur_txt or "").strip():
                    cur_txt = _refactor04_unit_evidence_text_from_metric_v1(cur_metric, include_context=True)
            except Exception:
                pass

        prev_is_cur = _refactor03_has_currency_evidence_v1(prev_txt)
        cur_is_cur = _refactor03_has_currency_evidence_v1(cur_txt)
        prev_is_pct = _refactor03_has_percent_evidence_v1(prev_txt)
        cur_is_pct = _refactor03_has_percent_evidence_v1(cur_txt)

        # scale mismatch (K/M/B/T)
        try:
            pu = str(prev_unit or "").upper().strip()
            cu = str(cur_unit or "").upper().strip()
            if pu in ("K", "M", "B", "T") and cu in ("K", "M", "B", "T") and pu != cu:
                return True
        except Exception:
            pass

        # REFACTOR27 (ADDITIVE): currency code/scale mismatch guard (handles mixed representations like 'USD' vs 'B')
        try:
            if expected == "currency":
                pu = str(prev_unit or "").upper().strip()
                cu = str(cur_unit or "").upper().strip()

                def _split_code_scale(u: str):
                    u = str(u or "").upper().strip()
                    if not u:
                        return ("", "")
                    # Composite like 'USD:B'
                    if ":" in u:
                        parts = [p.strip() for p in u.split(":") if p is not None]
                        if len(parts) >= 2:
                            code = parts[0].upper()
                            sc = parts[1].upper()
                            if sc in ("K", "M", "B", "T"):
                                return (code, sc)
                            return (code, "")
                    # Composite like 'USD_B'
                    if "_" in u:
                        parts = [p.strip() for p in u.split("_") if p is not None]
                        if len(parts) >= 2:
                            code = parts[0].upper()
                            sc = parts[-1].upper()
                            if sc in ("K", "M", "B", "T"):
                                return (code, sc)
                            return (code, "")
                    # Pure scale token
                    if u in ("K", "M", "B", "T"):
                        return ("", u)
                    # Pure currency code
                    try:
                        if re.fullmatch(r"[A-Z]{3}", u):
                            return (u, "")
                    except Exception:
                        pass
                    return ("", "")

                p_code, p_sc = _split_code_scale(pu)
                c_code, c_sc = _split_code_scale(cu)

                # Prefer explicit unit-encoded code, else infer from evidence text
                p_code = p_code or infer_currency_code_from_text_v1(prev_txt)
                c_code = c_code or infer_currency_code_from_text_v1(cur_txt)

                if p_code and c_code and p_code != c_code:
                    return True

                # If only one side has a magnitude scale token (K/M/B/T), treat as mismatch.
                has_ps = p_sc in ("K", "M", "B", "T")
                has_cs = c_sc in ("K", "M", "B", "T")
                if (has_ps and (not has_cs)) or (has_cs and (not has_ps)):
                    return True

                # Both sides have a scale token and they differ.
                if has_ps and has_cs and p_sc != c_sc:
                    return True
        except Exception:
            pass


        # family mismatch
        if expected == "currency":
            if isinstance(prev_metric, dict) and prev_metric and (not prev_is_cur):
                return True
            if isinstance(cur_metric, dict) and cur_metric and (not cur_is_cur):
                return True
            return False
        if expected == "percent":
            if isinstance(prev_metric, dict) and prev_metric and (not prev_is_pct):
                return True
            if isinstance(cur_metric, dict) and cur_metric and (not cur_is_pct):
                return True
            return False

        # expected magnitude/count
        if prev_is_cur or cur_is_cur or prev_is_pct or cur_is_pct:
            return True

        return False
    except Exception:
        return False





# ===================== PATCH RMS_AWARE1 (ADDITIVE) =====================
# =================== END PATCH RMS_AWARE1 (ADDITIVE) ===================



def get_history(limit: int = MAX_HISTORY_ITEMS) -> List[Dict]:
    """Load analysis history from Google Sheet"""
    sheet = get_google_sheet()
    # PATCH FIX2D66G (ADDITIVE): if Sheets writes recently failed, allow History to fall back to session_state
    # This prevents Evolution from being blocked by transient Sheets failures.
    try:
        if st.session_state.get("fix2d66_force_session_history"):
            return st.session_state.get('analysis_history', [])
    except Exception:
        pass
    if not sheet:
        # Fallback to session state
        return st.session_state.get('analysis_history', [])

    try:
        # ============================================================
        # PATCH GH_KEY1 (ADDITIVE): Use the actual worksheet title as cache key
        # Why:
        # - Your sheet names are: 'Sheet1', 'Snapshots', 'HistoryFull'
        # - There is no worksheet called 'History'
        # - Using cache_key='History' can cache empty reads under the wrong key.
        # ============================================================
        _ws_title = getattr(sheet, "title", "") or "Sheet1"
        _cache_key = f"History::{_ws_title}"
        # ============================================================

        # Get all rows (skip header)
        values = []
        try:
            values = sheets_get_all_values_cached(sheet, cache_key=_cache_key)
        except Exception:
            pass
            values = []

        # ============================================================
        # PATCH GH_FALLBACK1 (ADDITIVE): One direct-read retry if cached read is empty
        # Why:
        # - If a prior transient read/429 produced an empty cached value,
        #   evolution may temporarily see no history even though rows exist.
        # ============================================================
        if not values or len(values) < 2:
            try:
                direct = sheet.get_all_values()
                if direct and len(direct) >= 2:
                    values = direct
            except Exception:
                pass
        # ============================================================

        all_rows = values[1:] if values and len(values) >= 2 else []

        # PATCH FIX2D66G (ADDITIVE): if Sheets read is empty but we have session history (e.g., write failed), use it
        try:
            if (not all_rows) and st.session_state.get('analysis_history'):
                return st.session_state.get('analysis_history', [])
        except Exception:
            pass


        # ============================================================
        # PATCH GH_RL1 (ADDITIVE): Rate-limit fallback for History reads
        # ============================================================
        try:
            if (not all_rows) and globals().get("_SHEETS_LAST_READ_ERROR"):
                if ("RESOURCE_EXHAUSTED" in str(_SHEETS_LAST_READ_ERROR)
                    or "Quota exceeded" in str(_SHEETS_LAST_READ_ERROR)
                    or "429" in str(_SHEETS_LAST_READ_ERROR)):
                    return st.session_state.get('analysis_history', [])
        except Exception:
            pass
        # ============================================================

        # Parse and return most recent
        history = []
        for row in all_rows[-limit:]:
            if len(row) >= 5:
                raw_cell = row[4]
                try:
                    data = json.loads(raw_cell)
                    data['_sheet_id'] = row[0]  # Keep track of sheet row ID

                    # (your existing GH2 / ES1G / GH1 / GH3 logic unchanged)
                    # ...
                    history.append(data)

                except json.JSONDecodeError:
                    # (your existing GH1 rescue logic unchanged)
                    continue

        # (your existing GH3 sort unchanged)
        return history

    except Exception as e:
        st.warning(f"⚠️ Failed to load from Google Sheets: {e}")
        return st.session_state.get('analysis_history', [])


def get_analysis_by_id(analysis_id: str) -> Optional[Dict]:
    """Get a specific analysis by ID"""
    sheet = get_google_sheet()
    if not sheet:
        return None

    try:
        # Find row with matching ID
        cell = sheet.find(analysis_id)
        if cell:
            row = sheet.row_values(cell.row)
            if len(row) >= 5:
                return json.loads(row[4])
    except Exception as e:
        st.warning(f"⚠️ Failed to find analysis: {e}")

    return None

def delete_from_history(analysis_id: str) -> bool:
    """Delete an analysis from history"""
    sheet = get_google_sheet()
    if not sheet:
        return False

    try:
        cell = sheet.find(analysis_id)
        if cell:
            sheet.delete_rows(cell.row)
            return True
    except Exception as e:
        st.warning(f"⚠️ Failed to delete: {e}")

    return False

def clear_history() -> bool:
    """Clear all history (keep headers)"""
    sheet = get_google_sheet()
    if not sheet:
        st.session_state.analysis_history = []
        return True

    try:
        # Get row count
        all_rows = sheets_get_all_values_cached(sheet, cache_key="History")
        if len(all_rows) > 1:
            # Delete all rows except header
            sheet.delete_rows(2, len(all_rows))
        return True
    except Exception as e:
        st.warning(f"⚠️ Failed to clear history: {e}")
        return False

def format_history_label(analysis: Dict) -> str:
    """Format a history item for dropdown display"""
    timestamp = analysis.get('timestamp', '')
    question = analysis.get('question', 'Unknown query')[:40]
    confidence = analysis.get('final_confidence', '')

    try:
        dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
        now = datetime.now()
        delta = now - dt.replace(tzinfo=None)

        if delta.total_seconds() < 3600:
            time_str = f"{int(delta.total_seconds() / 60)}m ago"
        elif delta.total_seconds() < 86400:
            time_str = f"{int(delta.total_seconds() / 3600)}h ago"
        elif delta.days == 1:
            time_str = "Yesterday"
        elif delta.days < 7:
            time_str = f"{delta.days}d ago"
        else:
            time_str = dt.strftime("%b %d")
    except:
        time_str = timestamp[:10] if timestamp else "Unknown"

    conf_str = f" ({confidence:.0f}%)" if isinstance(confidence, (int, float)) else ""
    return f"{time_str}: {question}...{conf_str}"

def get_history_options() -> List[Tuple[str, int]]:
    """Get formatted history options for dropdown"""
    history = get_history()
    options = []
    for i, analysis in enumerate(reversed(history)):  # Most recent first
        label = format_history_label(analysis)
        actual_index = len(history) - 1 - i
        options.append((label, actual_index))
    return options

# =========================================================
# 1. CONFIGURATION & API KEY VALIDATION
# =========================================================

def load_api_keys():
    """Load and validate API keys from secrets or environment"""

    # =====================================================================
    # PATCH FIX41AFC5 (ADDITIVE): debug counters for schema-only rebuild eligibility hardening
    # =====================================================================
    _fix41afc5_dbg2 = {"rejected_year_only": 0, "rejected_unitless": 0, "rejected_magnitude_other_unitless": 0}
    # =====================================================================

    # =====================================================================
    # PATCH FIX41AFC5 (ADDITIVE): debug counters for rebuild eligibility hardening
    # =====================================================================
    _fix41afc5_dbg = {"rejected_year_only": 0, "rejected_unitless": 0, "rejected_magnitude_other_unitless": 0}
    # =====================================================================
    try:
        PERPLEXITY_KEY = st.secrets.get("PERPLEXITY_API_KEY") or os.getenv("PERPLEXITY_API_KEY", "")
        GEMINI_KEY = st.secrets.get("GEMINI_API_KEY") or os.getenv("GEMINI_API_KEY", "")
        SERPAPI_KEY = st.secrets.get("SERPAPI_KEY") or os.getenv("SERPAPI_KEY", "")
        SCRAPINGDOG_KEY = st.secrets.get("SCRAPINGDOG_KEY") or os.getenv("SCRAPINGDOG_KEY", "")
    except Exception:
        pass
        PERPLEXITY_KEY = os.getenv("PERPLEXITY_API_KEY", "")
        GEMINI_KEY = os.getenv("GEMINI_API_KEY", "")
        SERPAPI_KEY = os.getenv("SERPAPI_KEY", "")
        SCRAPINGDOG_KEY = os.getenv("SCRAPINGDOG_KEY", "")

    # Validate critical keys
    if not PERPLEXITY_KEY or len(PERPLEXITY_KEY) < 10:
        st.error("❌ PERPLEXITY_API_KEY is missing or invalid")
        st.stop()

    if not GEMINI_KEY or len(GEMINI_KEY) < 10:
        st.error("❌ GEMINI_API_KEY is missing or invalid")
        st.stop()

    return PERPLEXITY_KEY, GEMINI_KEY, SERPAPI_KEY, SCRAPINGDOG_KEY

PERPLEXITY_KEY, GEMINI_KEY, SERPAPI_KEY, SCRAPINGDOG_KEY = load_api_keys()
PERPLEXITY_URL = "https://api.perplexity.ai/chat/completions"

# Configure Gemini
#genai.configure(api_key=GEMINI_KEY)
#gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')

# =========================================================
# 2. PYDANTIC MODELS
# =========================================================

class MetricDetail(BaseModel):
    """Individual metric with name, value, and unit"""
    name: str = Field(..., description="Metric name")
    value: Union[float, int, str] = Field(..., description="Metric value")
    unit: str = Field(default="", description="Unit of measurement")
    model_config = ConfigDict(extra='ignore')

class TopEntityDetail(BaseModel):
    """Entity in top_entities list"""
    name: str = Field(..., description="Entity name")
    share: Optional[str] = Field(None, description="Market share")
    growth: Optional[str] = Field(None, description="Growth rate")
    model_config = ConfigDict(extra='ignore')

class TrendForecastDetail(BaseModel):
    """Trend forecast item"""
    trend: str = Field(..., description="Trend description")
    direction: Optional[str] = Field(None, description="Direction indicator")
    timeline: Optional[str] = Field(None, description="Timeline")
    model_config = ConfigDict(extra='ignore')

class VisualizationData(BaseModel):
    chart_labels: List[str] = Field(default_factory=list)
    chart_values: List[Union[float, int]] = Field(default_factory=list)
    chart_title: Optional[str] = Field("Trend Analysis")
    chart_type: Optional[str] = Field("line")
    x_axis_label: Optional[str] = None
    y_axis_label: Optional[str] = None
    model_config = ConfigDict(extra='ignore')

class ComparisonBar(BaseModel):
    """Comparison bar chart data"""
    title: str = Field("Comparison", description="Chart title")
    categories: List[str] = Field(default_factory=list)
    values: List[Union[float, int]] = Field(default_factory=list)
    model_config = ConfigDict(extra='ignore')

class BenchmarkTable(BaseModel):
    """Benchmark table row"""
    category: str
    value_1: Union[float, int, str] = Field(default=0, description="Numeric value or string")
    value_2: Union[float, int, str] = Field(default=0, description="Numeric value or string")
    model_config = ConfigDict(extra='ignore')

class Action(BaseModel):
    """Investment/action recommendation"""
    recommendation: str = Field("Neutral", description="Buy/Hold/Sell/Neutral")
    confidence: str = Field("Medium", description="High/Medium/Low")
    rationale: str = Field("", description="Reasoning")
    model_config = ConfigDict(extra='ignore')

class LLMResponse(BaseModel):
    """Complete LLM response schema"""
    executive_summary: str = Field(..., description="High-level summary")
    primary_metrics: Dict[str, MetricDetail] = Field(default_factory=dict)
    key_findings: List[str] = Field(default_factory=list)
    top_entities: List[TopEntityDetail] = Field(default_factory=list)
    trends_forecast: List[TrendForecastDetail] = Field(default_factory=list)
    visualization_data: Optional[VisualizationData] = None
    comparison_bars: Optional[ComparisonBar] = None
    benchmark_table: Optional[List[BenchmarkTable]] = None
    sources: List[str] = Field(default_factory=list)
    confidence: Union[float, int] = Field(default=75)
    freshness: Optional[str] = Field(None)
    action: Optional[Action] = None
    model_config = ConfigDict(extra='ignore')

# =========================================================
# 3. PROMPTS
# =========================================================

RESPONSE_TEMPLATE = """
{
  "executive_summary": "3-4 sentence high-level answer",
  "primary_metrics": {
    "metric_1": {"name": "Key Metric 1", "value": 25.5, "unit": "%"},
    "metric_2": {"name": "Key Metric 2", "value": 623, "unit": "$B"}
  },
  "key_findings": [
    "Finding 1 with quantified impact",
    "Finding 2 explaining drivers"
  ],
  "top_entities": [
    {"name": "Entity 1", "share": "25%", "growth": "15%"}
  ],
  "trends_forecast": [
    {"trend": "Trend description", "direction": "↑", "timeline": "2025-2027"}
  ],
  "visualization_data": {
    "chart_labels": ["2023", "2024", "2025"],
    "chart_values": [100, 120, 145],
    "chart_title": "Market Growth",
    "chart_type": "line"
  },
  "comparison_bars": {
    "title": "Market Share",
    "categories": ["A", "B", "C"],
    "values": [45, 30, 25]
  },
  "benchmark_table": [
    {"category": "Company A", "value_1": 25.5, "value_2": 623}
  ],
  "sources": ["source1.com"],
  "confidence": 87,
  "freshness": "Dec 2024"
}
"""



SYSTEM_PROMPT = f"""You are a professional market research analyst.

CRITICAL RULES:
1. Return ONLY valid JSON. NO markdown, NO code blocks, NO extra text.
2. NO citation references like [1][2] inside strings.
3. Use double quotes for all keys and string values.
4. NO trailing commas in arrays or objects.
5. Escape internal quotes with backslash.
6. If the prompt includes "Query Structure", you MUST follow it:
   - Treat "MAIN QUESTION" as the primary topic and address it FIRST.
   - Treat "SIDE QUESTIONS" as secondary topics and address them AFTER the main topic.
   - Do NOT let a side question replace the main question just because it is more specific.
   - In executive_summary, clearly separate: "Main:" then "Side:" when side questions exist.


NUMERIC FIELD RULES (IMPORTANT):
- In benchmark_table: value_1 and value_2 MUST be numbers (never "N/A", "null", or text)
- If data unavailable, use 0 for benchmark_table values
- In primary_metrics: values can be numbers or strings with units (e.g., "25.5" or "25.5 billion")
- In top_entities: share and growth can be strings (e.g., "25%")

REQUIRED FIELDS (provide substantive data):

**executive_summary** - MUST be 4-6 complete sentences covering:
  • Sentence 1: Direct answer with specific quantitative data (market size, revenue, units, etc.)
  • Sentence 2: Major players or regional breakdown with percentages/numbers
  • Sentence 3: Key growth drivers or market dynamics
  • Sentence 4: Future outlook with projected CAGR, timeline, or target values
  • Sentence 5 (optional): Challenge, risk, or competitive dynamic

  BAD (too short): "The EV market is growing rapidly due to government policies."

  GOOD: "The global electric vehicle market reached 14.2 million units sold in 2023, representing 18% of total auto sales. China dominates with 60% market share, followed by Europe (25%) and North America (10%). Growth is driven by battery cost reductions (down 89% since 2010), expanding charging infrastructure, and stricter emission regulations in over 20 countries. The market is projected to grow at 21% CAGR through 2030, reaching 40 million units annually. However, supply chain constraints for lithium and cobalt remain key challenges."

- primary_metrics (3+ metrics with numbers)
- key_findings (3+ findings with quantitative details)
- top_entities (3+ companies/countries with market share %)
- trends_forecast (2+ trends with timelines)
- visualization_data (MUST have chart_labels and chart_values)
- benchmark_table (if included, value_1 and value_2 must be NUMBERS, not "N/A")

Even if web data is sparse, use your knowledge to provide complete, detailed analysis.

Output ONLY this JSON structure:
{RESPONSE_TEMPLATE}
"""

EVOLUTION_PROMPT_TEMPLATE = """You are a market research analyst performing an UPDATE ANALYSIS.

You have been given a PREVIOUS ANALYSIS from {time_ago}. Your task is to:
1. Search for CURRENT data on the same metrics and entities
2. Identify what has CHANGED vs what has STAYED THE SAME
3. Provide updated values where data has changed
4. Flag any metrics/entities that are no longer relevant or have new entries

PREVIOUS ANALYSIS:
==================
Question: {previous_question}
Timestamp: {previous_timestamp}

Previous Executive Summary:
{previous_summary}

Previous Key Metrics:
{previous_metrics}

Previous Top Entities:
{previous_entities}

Previous Key Findings:
{previous_findings}
==================

CRITICAL RULES:
1. Return ONLY valid JSON. NO markdown, NO code blocks.
2. For EACH metric, indicate if it INCREASED, DECREASED, or stayed UNCHANGED
3. Keep the SAME metric names as previous analysis for easy comparison
4. If a metric is no longer available, mark it as "discontinued"
5. If there's a NEW important metric, add it with status "new"


REQUIRED OUTPUT FORMAT:
{{
  "executive_summary": "Updated 4-6 sentence summary noting key changes since last analysis",
  "analysis_delta": {{
    "time_since_previous": "{time_ago}",
    "overall_trend": "improving/declining/stable",
    "major_changes": ["Change 1", "Change 2"],
    "data_freshness": "Q4 2024"
  }},
  "primary_metrics": {{
    "metric_key": {{
      "name": "Same metric name as before",
      "previous_value": 100,
      "current_value": 110,
      "unit": "$B",
      "change_pct": 10.0,
      "direction": "increased/decreased/unchanged",
      "status": "updated/discontinued/new"
    }}
  }},
  "key_findings": [
    "[UNCHANGED] Finding that remains true",
    "[UPDATED] Finding with new data",
    "[NEW] Completely new finding",
    "[REMOVED] Finding no longer relevant - reason"
  ],
  "top_entities": [
    {{
      "name": "Company A",
      "previous_share": "25%",
      "current_share": "27%",
      "previous_rank": 1,
      "current_rank": 1,
      "change": "increased",
      "status": "updated"
    }}
  ],
  "trends_forecast": [
    {{"trend": "Trend description", "direction": "↑", "timeline": "2025-2027", "confidence": "high/medium/low"}}
  ],
  "visualization_data": {{
    "chart_labels": ["Previous", "Current"],
    "chart_values": [100, 110],
    "chart_title": "Market Size Evolution"
  }},
  "sources": ["source1.com", "source2.com"],
  "confidence": 85,
  "freshness": "Dec 2024",
  "drift_summary": {{
    "metrics_changed": 2,
    "metrics_unchanged": 3,
    "entities_reshuffled": 1,
    "findings_updated": 4,
    "overall_stability_pct": 75
  }}
}}

NOW, search for CURRENT information to UPDATE the previous analysis.
Focus on finding CHANGES to the metrics and entities listed above.

User Question: {query}
"""

# =========================================================
# 4. MODEL LOADING
# =========================================================

@st.cache_resource(show_spinner="🔧 Loading AI models...")
def load_models():
    """Load and cache sentence transformer and classifier"""
    try:
        classifier = pipeline(
            "zero-shot-classification",
            model="facebook/bart-large-mnli",
            device=-1
        )
        embedder = SentenceTransformer("all-MiniLM-L6-v2")
        return classifier, embedder
    except Exception as e:
        st.error(f"❌ Model loading failed: {e}")
        st.stop()

domain_classifier, embedder = load_models()

# =========================================================
# 5. JSON REPAIR FUNCTIONS
# =========================================================

def repair_llm_response(data: dict) -> dict:
    """
    Repair common LLM JSON structure issues:

    - Convert primary_metrics from list -> dict (stable keys)
    - Normalize MetricDetail fields so currency+unit do NOT get lost:
        "29.8 S$B" / "S$29.8B" / "S$29.8 billion" -> value=29.8, unit="S$B"
        "$204.7B" -> value=204.7, unit="$B"
        "9.8%" -> value=9.8, unit="%"
    - Ensure top_entities and trends_forecast are lists
    - Fix visualization_data legacy keys (labels/values)
    - Fix benchmark_table numeric values
    - Remove 'action' block entirely (no longer used)
    - Add minimal required fields if missing

    NOTE: This function is intentionally conservative: it normalizes obvious formatting
    without trying to "invent" missing values.
    """
    if not isinstance(data, dict):
        return {}

    def _to_list(x):
        if x is None:
            return []
        if isinstance(x, list):
            return x
        if isinstance(x, dict):
            return list(x.values())
        return []

    def _coerce_number(s: str):
        try:
            return float(str(s).replace(",", "").strip())
        except Exception:
            return None

    def _normalize_metric_item(item: dict) -> dict:
        """
        Normalize a single metric dict in-place-ish and return it.

        Goal: preserve currency + magnitude in `unit`, keep `value` numeric when possible.
        """
        if not isinstance(item, dict):
            return {"name": "N/A", "value": "N/A", "unit": ""}

        name = item.get("name")
        if not isinstance(name, str) or not name.strip():
            name = "N/A"
        item["name"] = name

        raw_val = item.get("value")
        raw_unit = item.get("unit")

        unit = (raw_unit or "")
        if not isinstance(unit, str):
            unit = str(unit)

        # If already numeric and unit looks okay, keep as-is
        if isinstance(raw_val, (int, float)) and isinstance(unit, str):
            item["unit"] = unit.strip()
            return item

        # Try to parse string value forms like:
        # "S$29.8B", "29.8 S$B", "$ 204.7 billion", "9.8%", "12 percent"
        if isinstance(raw_val, str):
            txt = raw_val.strip()

            # Also allow unit to carry the number sometimes (rare but happens)
            # e.g. value="29.8", unit="S$B" is already fine.
            # But if unit is empty and txt contains unit, we extract.
            # Percent detection
            if re.search(r'(%|\bpercent\b)', txt, flags=re.I):
                num = _coerce_number(re.sub(r'[^0-9\.\-\,]+', '', txt))
                if num is not None:
                    item["value"] = num
                    item["unit"] = "%"
                    return item

            # Currency detection
            currency = ""
            # Normalize currency tokens in either value or unit
            combo = f"{txt} {unit}".strip()

            if re.search(r'\bSGD\b', combo, flags=re.I) or "S$" in combo.upper():
                currency = "S$"
            elif re.search(r'\bUSD\b', combo, flags=re.I) or "$" in combo:
                currency = "$"

            # Magnitude detection
            # Accept: T/B/M/K, or words
            mag = ""
            if re.search(r'\btrillion\b', combo, flags=re.I):
                mag = "T"
            elif re.search(r'\bbillion\b', combo, flags=re.I):
                mag = "B"
            elif re.search(r'\bmillion\b', combo, flags=re.I):
                mag = "M"
            elif re.search(r'\bthousand\b', combo, flags=re.I):
                mag = "K"
            else:
                m = re.search(r'([TBMK])\b', combo.replace(" ", ""), flags=re.I)
                if m:
                    mag = m.group(1).upper()

            # Extract numeric
            num = _coerce_number(re.sub(r'[^0-9\.\-\,]+', '', txt))
            if num is not None:
                # If unit was present and meaningful (and already includes %), keep it
                if unit.strip() == "%":
                    item["value"] = num
                    item["unit"] = "%"
                    return item

                # Build unit as currency+magnitude when any found
                # If neither found, keep existing unit (may be e.g. "years", "points")
                if currency or mag:
                    item["value"] = num
                    item["unit"] = f"{currency}{mag}".strip()
                    return item

                # No currency/mag detected: keep unit if provided; else blank
                item["value"] = num
                item["unit"] = unit.strip()
                return item

            # If we can’t parse into a number, at least preserve the original text
            item["value"] = txt
            item["unit"] = unit.strip()
            return item

        # Non-string, non-numeric (None, dict, list, etc.)
        if raw_val is None or raw_val == "":
            item["value"] = "N/A"
        else:
            item["value"] = str(raw_val)

        item["unit"] = unit.strip()
        return item

    # -------------------------
    # primary_metrics normalization
    # -------------------------
    metrics = data.get("primary_metrics")

    # list -> dict
    if isinstance(metrics, list):
        new_metrics = {}
        for i, item in enumerate(metrics):
            if not isinstance(item, dict):
                continue
            item = _normalize_metric_item(item)

            raw_name = item.get("name", f"metric_{i+1}")
            key = re.sub(r'[^a-z0-9_]', '', str(raw_name).lower().replace(" ", "_")).strip("_")
            if not key:
                key = f"metric_{i+1}"

            original_key = key
            j = 1
            while key in new_metrics:
                key = f"{original_key}_{j}"
                j += 1

            new_metrics[key] = item

        data["primary_metrics"] = new_metrics

    elif isinstance(metrics, dict):
        # Normalize each metric dict entry
        cleaned = {}
        for k, v in metrics.items():
            if isinstance(v, dict):
                cleaned[str(k)] = _normalize_metric_item(v)
            else:
                # If someone stored a scalar, wrap it
                cleaned[str(k)] = _normalize_metric_item({"name": str(k), "value": v, "unit": ""})
        data["primary_metrics"] = cleaned

    else:
        data["primary_metrics"] = {}

    # -------------------------
    # list-like fields
    # -------------------------
    data["top_entities"] = _to_list(data.get("top_entities"))
    data["trends_forecast"] = _to_list(data.get("trends_forecast"))
    data["key_findings"] = _to_list(data.get("key_findings"))

    # Ensure strings in key_findings
    data["key_findings"] = [str(x) for x in data["key_findings"] if x is not None and str(x).strip()]

    # -------------------------
    # visualization_data legacy keys
    # -------------------------
    if isinstance(data.get("visualization_data"), dict):
        viz = data["visualization_data"]
        if "labels" in viz and "chart_labels" not in viz:
            viz["chart_labels"] = viz.pop("labels")
        if "values" in viz and "chart_values" not in viz:
            viz["chart_values"] = viz.pop("values")

        # Coerce chart_labels/values types gently
        if "chart_labels" in viz and not isinstance(viz["chart_labels"], list):
            viz["chart_labels"] = [str(viz["chart_labels"])]
        if "chart_values" in viz and not isinstance(viz["chart_values"], list):
            viz["chart_values"] = [viz["chart_values"]]

    # -------------------------
    # benchmark_table numeric cleaning
    # -------------------------
    if isinstance(data.get("benchmark_table"), list):
        cleaned_table = []
        for row in data["benchmark_table"]:
            if not isinstance(row, dict):
                continue

            if "category" not in row:
                row["category"] = "Unknown"

            for key in ["value_1", "value_2"]:
                if key not in row:
                    row[key] = 0
                    continue

                val = row.get(key)
                if isinstance(val, str):
                    val_upper = val.upper().strip()
                    if val_upper in ["N/A", "NA", "NULL", "NONE", "", "-", "—"]:
                        row[key] = 0
                    else:
                        try:
                            cleaned = re.sub(r'[^\d.-]', '', val)
                            row[key] = float(cleaned) if '.' in cleaned else int(cleaned) if cleaned else 0
                        except Exception:
                            pass
                            row[key] = 0
                elif isinstance(val, (int, float)):
                    pass
                else:
                    row[key] = 0

            cleaned_table.append(row)

        data["benchmark_table"] = cleaned_table

    # -------------------------
    # Remove action block entirely
    # -------------------------
    data.pop("action", None)

    # -------------------------
    # Minimal required top-level fields
    # -------------------------
    if not isinstance(data.get("executive_summary"), str) or not data.get("executive_summary", "").strip():
        data["executive_summary"] = "No executive summary provided."

    if not isinstance(data.get("sources"), list):
        data["sources"] = []

    if "confidence" not in data:
        data["confidence"] = 60

    if not isinstance(data.get("freshness"), str) or not data.get("freshness", "").strip():
        data["freshness"] = "Current"

    return data


def validate_numeric_fields(data: dict, context: str = "LLM Response") -> None:
    """
    Guardrail logger (and gentle coercer) for numeric lists used in charts/tables.

    We keep this lightweight: warn when strings appear where numbers are expected,
    and attempt to coerce when safe.
    """
    if not isinstance(data, dict):
        return

    # Check benchmark_table
    if "benchmark_table" in data and isinstance(data["benchmark_table"], list):
        for i, row in enumerate(data["benchmark_table"]):
            if isinstance(row, dict):
                for key in ["value_1", "value_2"]:
                    val = row.get(key)
                    if isinstance(val, str):
                        st.warning(
                            f"⚠️ {context}: benchmark_table[{i}].{key} is string: '{val}' (coercing to 0 if invalid)"
                        )
                        try:
                            cleaned = re.sub(r"[^\d\.\-]", "", val)
                            row[key] = float(cleaned) if cleaned else 0
                        except Exception:
                            pass
                            row[key] = 0

    # Check visualization_data chart_values
    viz = data.get("visualization_data")
    if isinstance(viz, dict):
        vals = viz.get("chart_values")
        if isinstance(vals, list):
            new_vals = []
            for j, v in enumerate(vals):
                if isinstance(v, (int, float)):
                    new_vals.append(v)
                elif isinstance(v, str):
                    try:
                        cleaned = re.sub(r"[^\d\.\-]", "", v)
                        new_vals.append(float(cleaned) if cleaned else 0.0)
                        st.warning(f"⚠️ {context}: visualization_data.chart_values[{j}] is string: '{v}' (coerced)")
                    except Exception:
                        pass
                        new_vals.append(0.0)
                else:
                    new_vals.append(0.0)
            viz["chart_values"] = new_vals


def preclean_json(raw: str) -> str:
    """
    Remove markdown fences and common citation markers before JSON parsing.
    Conservative: tries not to destroy legitimate JSON content.
    """
    if not raw or not isinstance(raw, str):
        return ""

    text = raw.strip()

    # Remove leading/trailing code fences (```json ... ```)
    text = re.sub(r'^\s*```(?:json)?\s*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\s*```\s*$', '', text)

    text = text.strip()

    # Remove common citation formats the model may append
    # [web:1], [1], (1) etc. (but avoid killing array syntax by being specific)
    text = re.sub(r'\[web:\d+\]', '', text, flags=re.IGNORECASE)
    text = re.sub(r'(?<!")\[\d+\](?!")', '', text)   # not inside quotes
    text = re.sub(r'(?<!")\(\d+\)(?!")', '', text)   # not inside quotes

    return text.strip()


def parse_json_safely(json_str: str, context: str = "LLM") -> dict:
    """
    Parse JSON with aggressive error recovery:
    1) Pre-clean markdown/citations
    2) Extract the *first* JSON object
    3) Repair common issues (unquoted keys, trailing commas, True/False/Null)
    4) Try parsing; if it fails, attempt a small set of pragmatic fixes
    """
    if json_str is None:
        return {}
    if not isinstance(json_str, str):
        json_str = str(json_str)

    if not json_str.strip():
        return {}

    cleaned = preclean_json(json_str)

    # Extract first JSON object (most LLM outputs are one object)
    match = re.search(r'\{.*\}', cleaned, flags=re.DOTALL)
    if not match:
        st.warning(f"⚠️ No JSON object found in {context} response")
        return {}

    json_content = match.group(0)

    # Structural repairs
    try:
        # Fix unquoted keys: {key: -> {"key":
        json_content = re.sub(
            r'([\{\,]\s*)([a-zA-Z_][a-zA-Z0-9_\-]*)(\s*):',
            r'\1"\2"\3:',
            json_content
        )

        # Remove trailing commas
        json_content = re.sub(r',\s*([\]\}])', r'\1', json_content)

        # Fix boolean/null capitalization
        json_content = re.sub(r':\s*True\b', ': true', json_content)
        json_content = re.sub(r':\s*False\b', ': false', json_content)
        json_content = re.sub(r':\s*Null\b', ': null', json_content)

    except Exception as e:
        st.warning(f"⚠️ {context}: Regex repair failed: {e}")

    # Attempt parse with a few passes
    attempts = 0
    last_err = None

    while attempts < 6:
        try:
            return json.loads(json_content)
        except json.JSONDecodeError as e:
            last_err = e
            msg = (e.msg or "").lower()

            # Pass 1: replace smart quotes
            if attempts == 0:
                json_content = (
                    json_content.replace("“", '"')
                                .replace("”", '"')
                                .replace("’", "'")
                )

            # Pass 2: single-quote keys/strings -> double quotes (limited)
            elif attempts == 1:
                # Only do this if it looks like single quotes dominate
                if json_content.count("'") > json_content.count('"'):
                    json_content = re.sub(r"\'", '"', json_content)

            # Pass 3: try removing control characters
            elif attempts == 2:
                json_content = re.sub(r"[\x00-\x1F\x7F]", "", json_content)

            # Pass 4: if unterminated string, try escaping a quote near the error
            elif "unterminated string" in msg or "unterminated" in msg:
                pos = e.pos
                # Try escaping a quote a bit before pos
                for i in range(pos - 1, max(0, pos - 200), -1):
                    if i < len(json_content) and json_content[i] == '"':
                        if i == 0 or json_content[i - 1] != "\\":
                            json_content = json_content[:i] + '\\"' + json_content[i + 1:]
                            break

            # Pass 5+: give up
            attempts += 1
            continue

    st.error(f"❌ Failed to parse JSON from {context}: {str(last_err)[:180] if last_err else 'unknown error'}")
    return {}




def parse_query_structure_safe(json_str: str, user_question: str) -> Dict:
    """
    Parse LLM-derived query structure with guaranteed deterministic fallback.
    Never raises, never returns empty dict.
    """
    parsed = parse_json_safely(json_str, context="LLM Query Structure")

    if isinstance(parsed, dict) and parsed:
        # Minimal schema validation
        if "main" in parsed or "category" in parsed:
            return parsed

    # 🔒 Deterministic fallback (NO LLM)
    return {
        "category": "unknown",
        "category_confidence": 0.0,
        "main": user_question,
        "side": []
    }


def extract_json_object(text: str) -> Optional[Dict]:
    """
    Best-effort extraction of the first JSON object from a string.
    Returns dict or None.
    """
    if not text or not isinstance(text, str):
        return None

    # Common cleanup
    cleaned = text.strip()
    cleaned = cleaned.replace("```json", "").replace("```", "").strip()

    # Fast path
    try:
        obj = json.loads(cleaned)
        if isinstance(obj, dict):
            return obj
    except Exception:
        pass

    # Regex: first {...} block (non-greedy)
    try:
        m = re.search(r"\{.*\}", cleaned, flags=re.DOTALL)
        if not m:
            return None
        candidate = m.group(0)
        obj = json.loads(candidate)
        if isinstance(obj, dict):
            return obj
    except Exception:
        return None

    return None


# =========================================================
# 6. WEB SEARCH FUNCTIONS
#   SERPAPI STABILITY CONFIGURATION
# =========================================================

# Fixed parameters to prevent geo/personalization variance

SERPAPI_STABILITY_CONFIG = {
    "gl": "us",                    # Fixed country
    "hl": "en",                    # Fixed language
    "google_domain": "google.com", # Fixed domain
    "nfpr": "1",                   # No auto-query correction
    "safe": "active",              # Consistent safe search
    "device": "desktop",           # Fixed device type
    "no_cache": "false",           # Allow Google caching (more stable)
}

# Preferred domains for consistent sourcing (sorted by priority)
PREFERRED_SOURCE_DOMAINS = [
    "statista.com", "reuters.com", "bloomberg.com", "imf.org", "wsj.com", "bcg.com", "opec.org",
    "worldbank.org", "mckinsey.com", "deloitte.com", "spglobal.com", "ft.com", "pwc.com", "semiconductors.org",
    "ft.com", "economist.com", "wsj.com", "forbes.com", "cnbc.com", "kpmg.com", "eia.org"
]

# Search results cache
_search_cache: Dict[str, Tuple[List[Dict], datetime]] = {}
SEARCH_CACHE_TTL_HOURS = 24

def get_search_cache_key(query: str) -> str:
    """Generate stable cache key for search query"""
    normalized = re.sub(r'\s+', ' ', query.lower().strip())
    normalized = re.sub(r'\b(today|current|latest|now|recent)\b', '', normalized)
    return hashlib.md5(normalized.encode()).hexdigest()[:16]

def get_cached_search_results(query: str) -> Optional[List[Dict]]:
    """
    Get cached search results if still valid.

    IMPORTANT:
    - Never treat cached empty results as valid.
      Returning [] here "poisons" the pipeline for hours and makes SerpAPI look broken.
    """
    try:
        cache_key = get_search_cache_key(query)
        if cache_key in _search_cache:
            cached_results, cached_time = _search_cache[cache_key]
            if datetime.now() - cached_time < timedelta(hours=SEARCH_CACHE_TTL_HOURS):
                # ✅ Do not reuse empty cache entries
                if isinstance(cached_results, list) and len(cached_results) == 0:
                    return None
                return cached_results
            # expired
            del _search_cache[cache_key]
    except Exception:
        return None
    return None


def cache_search_results(query: str, results: List[Dict]):
    """
    Cache search results.

    IMPORTANT:
    - Do NOT cache empty lists
    - Do NOT cache lists that contain no usable URLs
      (prevents "poisoned cache" that makes SerpAPI appear broken)
    """
    try:
        if not isinstance(query, str) or not query.strip():
            return
        if not isinstance(results, list) or not results:
            return

        # Require at least one usable url/link
        has_url = False
        for r in results:
            if isinstance(r, dict):
                u = (r.get("link") or r.get("url") or "").strip()
                if u:
                    has_url = True
                    break
            elif isinstance(r, str) and r.strip():
                has_url = True
                break

        if not has_url:
            return

        cache_key = get_search_cache_key(query)
        _search_cache[cache_key] = (results, datetime.now())
    except Exception:
        return


# =========================================================
# LLM RESPONSE CACHE - Prevents variance on identical inputs
# =========================================================
_llm_cache: Dict[str, Tuple[str, datetime]] = {}
LLM_CACHE_TTL_HOURS = 24  # Cache LLM responses for 24 hours

def get_llm_cache_key(query: str, web_context: Dict) -> str:
    """Generate cache key from query + source URLs"""
    # Include source URLs so cache invalidates if sources change
    source_urls = sorted(web_context.get("sources", [])[:5])
    cache_input = f"{query.lower().strip()}|{'|'.join(source_urls)}"
    return hashlib.md5(cache_input.encode()).hexdigest()[:20]

def get_cached_llm_response(query: str, web_context: Dict) -> Optional[str]:
    """Get cached LLM response if still valid"""
    cache_key = get_llm_cache_key(query, web_context)
    if cache_key in _llm_cache:
        cached_response, cached_time = _llm_cache[cache_key]
        if datetime.now() - cached_time < timedelta(hours=LLM_CACHE_TTL_HOURS):
            return cached_response
        del _llm_cache[cache_key]
    return None

def cache_llm_response(query: str, web_context: Dict, response: str):
    """Cache LLM response"""
    cache_key = get_llm_cache_key(query, web_context)
    _llm_cache[cache_key] = (response, datetime.now())


def sort_results_deterministically(results: List[Dict]) -> List[Dict]:
    """Sort results for consistent ordering"""
    def sort_key(r):
        link = r.get("link", "").lower()
        # Priority: preferred domains first, then alphabetical
        priority = 999
        for i, domain in enumerate(PREFERRED_SOURCE_DOMAINS):
            if domain in link:
                priority = i
                break
        return (priority, link)
    return sorted(results, key=sort_key)


def classify_source_reliability(source: str) -> str:
    """Classify source as High/Medium/Low quality"""
    source = source.lower() if isinstance(source, str) else ""

    high = ["gov", "imf", "worldbank", "central bank", "fed", "ecb", "reuters", "spglobal", "economist", "mckinsey", "bcg", "cognitive market research",
            "financial times", "wsj", "oecd", "bloomberg", "tradingeconomics", "deloitte", "hsbc", "imarc", "booz allen", "bakerinstitute.org", "wef",
           "kpmg", "semiconductors.org", "eu", "iea", "world bank", "opec", "jpmorgan", "citibank", "goldmansachs", "j.p. morgan", "oecd",
           "world bank", "sec", "federalreserve", "bls", "bea"]
    medium = ["wikipedia", "forbes", "cnbc", "yahoo", "ceic", "statista", "trendforce", "digitimes", "idc", "gartner", "marketwatch", "fortune", "investopedia"]
    low = ["blog", "medium.com", "wordpress", "ad", "promo"]

    for h in high:
        if h in source:
            return "✅ High"
    for m in medium:
        if m in source:
            return "⚠️ Medium"
    for l in low:
        if l in source:
            return "❌ Low"

    return "⚠️ Medium"

def source_quality_score(sources: List[str]) -> float:
    """Calculate average source quality (0-100)"""
    if not sources:
        return 50.0  # Lower default when no sources

    weights = {"✅ High": 100, "⚠️ Medium": 60, "❌ Low": 30}
    scores = [weights.get(classify_source_reliability(s), 60) for s in sources]
    return sum(scores) / len(scores) if scores else 50.0

@st.cache_data(ttl=3600, show_spinner=False)
def search_serpapi(query: str, num_results: int = 10) -> List[Dict]:
    """Search Google via SerpAPI with stability controls"""
    if not SERPAPI_KEY:
        return []

    # Check cache first (this is the ONLY cache we use - removed @st.cache_data to avoid conflicts)
    cached = get_cached_search_results(query)
    if cached:
        st.info("📦 Using cached search results")
        return cached

    # Aggressive query normalization for consistent searches
    query_normalized = query.lower().strip()

    # Remove temporal words that cause variance
    query_normalized = re.sub(r'\b(latest|current|today|now|recent|new|upcoming|this year|this month)\b', '', query_normalized)

    # Normalize whitespace
    query_normalized = re.sub(r'\s+', ' ', query_normalized).strip()

    # Add year for consistency
    if not re.search(r'\b20\d{2}\b', query_normalized):
        query_normalized = f"{query_normalized} 2024"

    # Build search terms
    query_lower = query_normalized
    industry_kw = ["industry", "market", "sector", "size", "growth", "players"]

    if any(kw in query_lower for kw in industry_kw):
        search_terms = f"{query_normalized} market size growth statistics"
        tbm, tbs = "", ""  # Organic results (more stable than news)
    else:
        search_terms = f"{query_normalized} finance economics data"
        tbm, tbs = "", ""  # Use organic for stability

    params = {
        "engine": "google",
        "q": search_terms,
        "api_key": SERPAPI_KEY,
        "num": num_results,
        "tbm": tbm,
        "tbs": tbs,
        **SERPAPI_STABILITY_CONFIG  # Add fixed location params
    }

    try:
        resp = requests.get("https://serpapi.com/search", params=params, timeout=10)
        resp.raise_for_status()
        data = resp.json()

        results = []

        # Prefer organic results (more stable than news)
        for item in data.get("organic_results", [])[:num_results]:
            results.append({
                "title": item.get("title", ""),
                "link": item.get("link", ""),
                "snippet": item.get("snippet", ""),
                "date": item.get("date", ""),
                "source": item.get("source", "")
            })

        # Fall back to news only if no organic results
        if not results:
            for item in data.get("news_results", [])[:num_results]:
                src = item.get("source", {})
                source_name = src.get("name", "") if isinstance(src, dict) else str(src)
                results.append({
                    "title": item.get("title", ""),
                    "link": item.get("link", ""),
                    "snippet": item.get("snippet", ""),
                    "date": item.get("date", ""),
                    "source": source_name
                })

        # Sort deterministically
        results = sort_results_deterministically(results)
        results = results[:num_results]

        # Cache results
        if results:
            cache_search_results(query, results)

        return results

    except Exception as e:
        st.warning(f"⚠️ SerpAPI error: {e}")
        return []



# =====================================================================
# PATCH INJ_DIAG_HELPERS (ADDITIVE): Injected-URL diagnostics helpers
# - Pure helpers (no control-flow changes)
# - Used to trace injected extra URLs across: UI -> intake -> scrape -> snapshots -> hashing -> rebuild
# =====================================================================
def _inj_diag_make_run_id(prefix: str = "run") -> str:
    """Short correlation id for a single analysis/evolution run."""
    try:
        import os, time, hashlib
        seed = f"{prefix}|{time.time()}|{os.getpid()}|{os.urandom(8).hex()}"
        return hashlib.sha256(seed.encode("utf-8")).hexdigest()[:12]
    except Exception:
        pass
        try:
            import random
            return f"{prefix}_{random.randint(100000,999999)}"
        except Exception:
            return f"{prefix}_unknown"


# =====================================================================
# PATCH INJ_URL_CANON_V1 (ADDITIVE): Canonicalize injected URLs
# - Strips common tracking/query parameters from injected URLs ONLY
# - Keeps scheme/host/path; preserves non-tracking query params (sorted)
# - Adds deterministic canonical form for stable admission/dedupe/hashing
# =====================================================================
def _canonicalize_injected_url(url: str) -> str:
    """Canonicalize injected URLs by stripping known tracking params.

    This is intentionally conservative and applied only to user-injected URLs
    (extra URLs), not to SERP-derived URLs.
    """
    try:
        from urllib.parse import urlsplit, urlunsplit, parse_qsl, urlencode
        u = str(url or "").strip()
        if not u:
            return ""
        if not (u.startswith("http://") or u.startswith("https://")):
            return u

        parts = urlsplit(u)
        # Normalize scheme/host case
        scheme = (parts.scheme or "").lower()
        netloc = (parts.netloc or "").lower()
        path = parts.path or ""
        fragment = ""  # drop fragments for stability

        # Tracking params to drop (exact match)
        drop_exact = {
            "guccounter", "guce_referrer", "guce_referrer_sig",
            "gclid", "fbclid", "msclkid", "mc_cid", "mc_eid",
            "ref", "ref_src",
        }
        # Drop prefixes (utm_*, etc.)
        drop_prefixes = ("utm_",)

        qs = []
        for k, v in parse_qsl(parts.query or "", keep_blank_values=True):
            kk = (k or "").strip()
            if not kk:
                continue
            k_lower = kk.lower()
            if k_lower in drop_exact:
                continue
            if any(k_lower.startswith(p) for p in drop_prefixes):
                continue
            qs.append((kk, v))

        # Sort query params for determinism
        qs_sorted = sorted(qs, key=lambda kv: (kv[0].lower(), str(kv[1])))

        query = urlencode(qs_sorted, doseq=True) if qs_sorted else ""
        return urlunsplit((scheme, netloc, path, query, fragment))
    except Exception:
        pass
        try:
            return str(url or "").strip()
        except Exception:
            return ""

def _inj_diag_norm_url_list(extra_urls: Any) -> list:
    """Normalize/dedupe injected URL list (http/https only) with canonicalization.

    NOTE: This is used for injected/extra URL diagnostics and admission wiring only.
    It canonicalizes by stripping known tracking params for stability.
    """
    out = []
    try:
        if extra_urls is None:
            return []
        items = extra_urls
        if isinstance(items, str):
            items = [u.strip() for u in items.splitlines()]
        if not isinstance(items, (list, tuple, set)):
            items = [str(items)]
        seen = set()
        for u in items:
            uu = str(u or "").strip()
            if not uu:
                continue
            if not (uu.startswith("http://") or uu.startswith("https://")):
                continue
            cu = _canonicalize_injected_url(uu) or uu
            if cu in seen:
                continue
            seen.add(cu)
            out.append(cu)
    except Exception:
        return []
    return out


def _yureeka_extract_injected_urls_v1(web_context: Any) -> list:
    """Extract UI-injected URLs deterministically.

    Contract:
      - Prefer Streamlit/UI fields: diag_extra_urls_ui(_raw), extra_urls_ui(_raw), and list variants.
      - If Evolution wired injected URLs into web_context['extra_urls'], it MUST also set
        __yureeka_extra_urls_are_injection_v1 / __yureeka_injected_urls_v1 so we can safely
        treat extra_urls as injected without misclassifying production source lists.
    """
    out: list = []
    try:
        if not isinstance(web_context, dict):
            return []
        # list variants (UI)
        _cand = web_context.get("diag_extra_urls_ui") or web_context.get("extra_urls_ui") or []
        if isinstance(_cand, str):
            # allow simple newline/comma separated
            for part in _cand.replace(",", "\n").split():
                if part.startswith("http://") or part.startswith("https://"):
                    out.append(part.strip())
            _cand = []
        if isinstance(_cand, (list, tuple)):
            out.extend([u for u in _cand if isinstance(u, str)])

        # ui_raw string variants
        _ui_raw = web_context.get("diag_extra_urls_ui_raw") or web_context.get("extra_urls_ui_raw") or ""
        if isinstance(_ui_raw, str) and _ui_raw.strip():
            for part in _ui_raw.replace(",", "\n").split():
                if part.startswith("http://") or part.startswith("https://"):
                    out.append(part.strip())

        # explicit internal marker fallback (set by Evolution wiring)
        _marked = bool(web_context.get("__yureeka_extra_urls_are_injection_v1"))
        _marked_list = web_context.get("__yureeka_injected_urls_v1")
        if _marked and isinstance(_marked_list, (list, tuple)):
            out.extend([u for u in _marked_list if isinstance(u, str)])
        if _marked and isinstance(web_context.get("extra_urls"), (list, tuple)):
            out.extend([u for u in web_context.get("extra_urls") if isinstance(u, str)])

        # normalize / de-dup (keep original strings, but stable ordering)
        out = [u.strip() for u in out if isinstance(u, str) and u.strip()]
        _seen = set()
        _uniq = []
        for u in out:
            if u not in _seen:
                _seen.add(u)
                _uniq.append(u)
        out = _uniq

        # canonicalize for stability where possible (strip tracking params)
        try:
            _norm = _inj_diag_norm_url_list(out)
            if isinstance(_norm, list) and _norm:
                # keep normalized, but only if it doesn't erase all
                out = _norm
        except Exception:
            pass

        return out
    except Exception:
        return []


def _inj_diag_set_hash(urls: list) -> str:
    """Stable sha256 of sorted URL list (for compact logging)."""
    try:
        import hashlib
        lst = [str(u or "").strip() for u in (urls or []) if str(u or "").strip()]
        lst = sorted(set(lst))
        payload = "|".join(lst)
        return hashlib.sha256(payload.encode("utf-8")).hexdigest()
    except Exception:
        return ""

def _inj_diag_hash_inputs_from_bsc(baseline_sources_cache: Any) -> list:
    """Extract deterministic URL inputs used by snapshot hashing (v1/v2 both include URL)."""
    urls = []
    try:
        if not isinstance(baseline_sources_cache, list):
            return []
        for sr in baseline_sources_cache:
            if not isinstance(sr, dict):
                continue
            u = (sr.get("source_url") or sr.get("url") or "").strip()
            if u:
                urls.append(u)
    except Exception:
        return []
    return sorted(set(urls))

# =====================================================================
# PATCH INJ_HASH_V1 (ADDITIVE): optional inclusion of injected URLs in snapshot hash inputs
# Default behavior is OFF to avoid disrupting locked fastpath.
#
# When enabled, injected URLs that were persisted (per diag_injected_urls.persisted*)
# but are missing from baseline_sources_cache will be added as *synthetic* source
# records (url-only) so that:
#   - source_snapshot_hash (v1/v2) reflects injected sources deterministically
#   - evolution rebuild sees the same snapshot pool and hash identity via persistence
#
# Safety:
#   - Does NOT modify fastpath logic.
#   - Does NOT change metric selection (synthetic records have no extracted_numbers).
#   - Only activates when INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH is True.
# =====================================================================
INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH = False  # ✅ default OFF (locked fastpath safe)
CODE_VERSION_INJ_HASH_V1 = "fix41r_inj_hash_optional_include"  # additive version marker

def _inj_hash_should_include() -> bool:
    """Single switch for inclusion; additive-only. Supports env override."""
    try:
        import os
        v = os.getenv("YUREEKA_INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH", "").strip().lower()
        if v in ("1", "true", "yes", "y", "on"):
            return True
        if v in ("0", "false", "no", "n", "off"):
            return False
    except Exception:
        return bool(globals().get("INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH", False))

# =====================================================================
# PATCH INJ_HASH_POLICY_ALIGN_V1 (Additive, policy-aligned)
# Goal:
#   - Align injected URL "new data" identity semantics with baseline sources:
#       If an injected URL is PERSISTED as a successful snapshot, it should
#       participate in snapshot hash inputs by default (unless explicitly disabled).
#   - Preserve existing safety switch INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH
#     and its env override for emergency forcing.
#
# Controls:
#   - Default behavior (policy-aligned): ON when persisted injected URLs exist.
#   - Explicit disable: env YUREEKA_EXCLUDE_INJECTED_URLS_FROM_SNAPSHOT_HASH=1
#   - Explicit force include: env YUREEKA_INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH=1
#
# Notes:
#   - Fastpath logic is NOT modified.
#   - This only affects hash identity input construction; metric selection remains unchanged.
# =====================================================================
INJECTED_URL_HASH_POLICY_ALIGN_WITH_BASELINE = True  # ✅ default ON (policy-aligned)

def _inj_hash_policy_explicit_disable() -> bool:
    try:
        import os
        v = os.getenv("YUREEKA_EXCLUDE_INJECTED_URLS_FROM_SNAPSHOT_HASH", "").strip().lower()
        return v in ("1", "true", "yes", "y", "on")
    except Exception:
        return False

def _inj_hash_policy_should_include(persisted_injected_urls) -> bool:
    """Policy-aligned include decision for injected URLs in hash identity.

    - If explicitly disabled via env, returns False.
    - If explicitly forced via existing switch/env, returns True.
    - Otherwise, when policy-align is enabled and persisted injected URLs exist, returns True.
    - Else, falls back to legacy _inj_hash_should_include().
    """
    try:
        if _inj_hash_policy_explicit_disable():
            return False
        # Respect existing forcing mechanism first
        if _inj_hash_should_include():
            return True
        if bool(globals().get("INJECTED_URL_HASH_POLICY_ALIGN_WITH_BASELINE", True)) and (persisted_injected_urls or []):
            return True
    except Exception:
        return _inj_hash_should_include()

def _inj_hash_add_synthetic_sources(
    baseline_sources_cache: Any,
    injected_persisted_urls: list,
    now_iso: str = ""
) -> tuple:
    """
    Return (bsc_augmented, added_urls, reasons_by_url) without mutating the original list.
    Synthetic records are url-only, deterministic, and safe for selection logic.
    """
    reasons = {}
    added = []
    try:
        bsc = list(baseline_sources_cache) if isinstance(baseline_sources_cache, list) else []
        inj = _inj_diag_norm_url_list(injected_persisted_urls or [])
        if not inj:
            return (bsc, added, reasons)

        existing = set(_inj_diag_hash_inputs_from_bsc(bsc))
        for u in inj:
            if u in existing:
                reasons[u] = "present_in_bsc"
                continue
            # Add synthetic source record (no numbers) so hash includes the URL deterministically
            added.append(u)
            reasons[u] = "added_synthetic_for_hash"
            bsc.append({
                "url": u,
                "source_url": u,
                "status": "fetched",
                "status_detail": "synthetic_injected_for_hash",
                "numbers_found": 0,
                "fetched_at": now_iso or "",
                "fingerprint": "",
                "extracted_numbers": [],
                "__inj_synthetic": True,
            })

        # Keep deterministic ordering identical to existing conventions
        bsc = sorted(bsc, key=lambda x: str((x or {}).get("url") or ""))
        return (bsc, added, reasons)
    except Exception:
        pass
        try:
            return (list(baseline_sources_cache) if isinstance(baseline_sources_cache, list) else [], [], {})
        except Exception:
            return ([], [], {})

# =====================================================================


# =====================================================================
# PATCH FIX2D66_INJECTION_ADMISSION (ADDITIVE)
# Purpose:
# - Make injected URL admission deterministic & auditable across Analysis/Evolution.
# - Promote UI raw/diag fields into web_context['extra_urls'] (the admission input).
# - Synthesize a minimal web_context['diag_injected_urls'] when fetch_web_context was bypassed.
# - Pure wiring/diagnostics only: no scraping, no selection changes.
# =====================================================================
_URL_RE_FIX2D66 = None

def _fix2d66_extract_urls_from_text(text: str) -> list:
    """Extract http(s) URLs from arbitrary text (stable, conservative)."""
    global _URL_RE_FIX2D66
    try:
        if _URL_RE_FIX2D66 is None:
            import re as _re
            _URL_RE_FIX2D66 = _re.compile(r"https?://[^\s\]\)\"\'<>]+", _re.I)
        if not isinstance(text, str):
            return []
        return [u.strip() for u in _URL_RE_FIX2D66.findall(text) if isinstance(u, str) and u.strip()]
    except Exception:
        return []


def _fix2d66_collect_injected_urls(web_context: dict, question_text: str = "") -> list:
    """Collect injected URLs from all known UI/diag fields + (optional) question text."""
    try:
        wc = web_context if isinstance(web_context, dict) else {}
        found = []
        # direct list fields
        for k in (
            'extra_urls',
            'diag_extra_urls_final', 'diag_extra_urls',
            'diag_extra_urls_ui', 'extra_urls_ui',
            'extra_urls_normalized', 'extra_urls_ui_norm',
        ):
            v = wc.get(k)
            if isinstance(v, (list, tuple)):
                found.extend([str(x).strip() for x in v if isinstance(x, str) and x.strip()])

        # raw string fields
        for k in (
            'diag_extra_urls_ui_raw', 'extra_urls_ui_raw',
            'diag_extra_urls_ui_text', 'extra_urls_ui_text',
        ):
            v = wc.get(k)
            if isinstance(v, str) and v.strip():
                # splitlines + commas + url regex
                for line in v.splitlines():
                    line = (line or '').strip()
                    if not line:
                        continue
                    for p in line.split(','):
                        p = (p or '').strip()
                        if p.startswith('http://') or p.startswith('https://'):
                            found.append(p)
                found.extend(_fix2d66_extract_urls_from_text(v))

        # diag payloads
        d = wc.get('diag_injected_urls') if isinstance(wc.get('diag_injected_urls'), dict) else {}
        if isinstance(d, dict):
            for k in ('ui_norm', 'intake_norm', 'admitted', 'persisted', 'persisted_norm', 'extra_urls', 'extra_urls_normalized'):
                v = d.get(k)
                if isinstance(v, (list, tuple)):
                    found.extend([str(x).strip() for x in v if isinstance(x, str) and x.strip()])
            vraw = d.get('ui_raw')
            if isinstance(vraw, str) and vraw.strip():
                found.extend(_fix2d66_extract_urls_from_text(vraw))

        # last resort: parse question text (helps when user pastes URL into prompt)
        if isinstance(question_text, str) and question_text.strip():
            found.extend(_fix2d66_extract_urls_from_text(question_text))

        # normalize
        try:
            norm = _inj_diag_norm_url_list(found)
        except Exception:
            pass
            norm = [u for u in found if isinstance(u, str) and u.startswith(('http://','https://'))]
        # stable de-dupe order
        seen = set(); out = []
        for u in norm:
            if not u or u in seen:
                continue
            seen.add(u); out.append(u)
        return out
    except Exception:
        return []



def _fix2d66_promote_injected_urls(web_context: dict, question_text: str = "", stage: str = "") -> dict:
    """Promote injected URLs into web_context.extra_urls and ensure diag_injected_urls exists."""
    try:
        if not isinstance(web_context, dict):
            return web_context
        inj = _fix2d66_collect_injected_urls(web_context, question_text=question_text)
        if not inj:
            return web_context

        # promote into extra_urls if empty/missing
        cur = web_context.get('extra_urls')
        if not isinstance(cur, (list, tuple)) or not cur:
            web_context['extra_urls'] = list(inj)
        else:
            # merge
            merged = []
            seen = set()
            for u in _inj_diag_norm_url_list(list(cur) + list(inj)):
                if not u or u in seen:
                    continue
                seen.add(u); merged.append(u)
            web_context['extra_urls'] = merged

        # ensure ui_raw presence (for inj_trace_v1)
        if not (isinstance(web_context.get('diag_extra_urls_ui_raw'), str) and web_context.get('diag_extra_urls_ui_raw').strip()):
            web_context['diag_extra_urls_ui_raw'] = "\n".join(inj)

        # ensure diag_injected_urls exists if upstream bypassed fetch_web_context
        if not isinstance(web_context.get('diag_injected_urls'), dict):
            web_context['diag_injected_urls'] = {}
        d = web_context['diag_injected_urls']
        if isinstance(d, dict):
            d.setdefault('run_id', web_context.get('diag_run_id') or _inj_diag_make_run_id(stage or 'run'))
            d.setdefault('ui_raw', web_context.get('diag_extra_urls_ui_raw') or "\n".join(inj))
            d.setdefault('ui_norm', list(inj))
            d.setdefault('intake_norm', list(inj))
            d.setdefault('admitted', list(inj))

        web_context.setdefault('debug', {})
        if isinstance(web_context.get('debug'), dict):
            web_context['debug'].setdefault('fix2d66', {})
            if isinstance(web_context['debug'].get('fix2d66'), dict):
                web_context['debug']['fix2d66'].update({
                    'promoted': True,
                    'count': int(len(inj)),
                    'set_hash': _inj_diag_set_hash(inj),
                    'stage': str(stage or ''),
                })
        return web_context
    except Exception:
        return web_context

# =====================================================================
# PATCH INJ_TRACE_V1_HELPERS (ADDITIVE): canonical injected-URL lifecycle trace builder
# Objective:
# - Emit ONE canonical diagnostic payload in a fixed location for every run:
#     results.debug.inj_trace_v1  (analysis outputs)
#     results.debug.inj_trace_v1  (evolution outputs; mirrored from output.debug)
# - Purely additive; does NOT alter fastpath logic or selection control flow.
# =====================================================================
def _inj_trace_v1_build(
    diag_injected_urls: dict,
    hash_inputs: list,
    stage: str = "analysis",
    path: str = "",
    rebuild_pool: list = None,
    rebuild_selected: list = None,
    hash_exclusion_reasons: dict = None,
) -> dict:
    try:
        d = diag_injected_urls if isinstance(diag_injected_urls, dict) else {}
        ui_raw = d.get("ui_raw") if isinstance(d.get("ui_raw"), (str, list)) else (d.get("extra_urls_ui_raw") or "")
        ui_norm = _inj_diag_norm_url_list(d.get("ui_norm") or d.get("extra_urls_ui_norm") or d.get("extra_urls_normalized") or [])
        intake_norm = _inj_diag_norm_url_list(d.get("intake_norm") or d.get("extra_urls_intake_norm") or d.get("extra_urls") or [])
        admitted_norm = _inj_diag_norm_url_list(d.get("admitted") or d.get("extra_urls_admitted") or [])
        persisted_norm = _inj_diag_norm_url_list(d.get("persisted") or d.get("persisted_norm") or [])

        attempted = d.get("attempted") if isinstance(d.get("attempted"), list) else []
        # Keep attempted minimal and stable
        attempted_min = []
        for a in attempted:
            if not isinstance(a, dict):
                continue
            attempted_min.append({
                "url": str(a.get("url") or ""),
                "status": str(a.get("status") or a.get("fetch_status") or ""),
                "reason": str(a.get("reason") or a.get("fail_reason") or ""),
                "content_len": a.get("content_len"),
            })

        hash_inputs_norm = _inj_diag_norm_url_list(hash_inputs or [])
        rebuild_pool_norm = _inj_diag_norm_url_list(rebuild_pool or [])
        rebuild_selected_norm = _inj_diag_norm_url_list(rebuild_selected or [])

        # Deterministic deltas (set-based; small lists)
        def _delta(a, b):
            try:
                return sorted(list(set(a or []) - set(b or [])))[:100]
            except Exception:
                return []

        deltas = {
            "ui_minus_intake": _delta(ui_norm, intake_norm),
            "intake_minus_admitted": _delta(intake_norm, admitted_norm),
            "admitted_minus_attempted": _delta(admitted_norm, [x.get("url") for x in attempted_min if isinstance(x, dict)]),
            "attempted_minus_persisted": _delta([x.get("url") for x in attempted_min if isinstance(x, dict)], persisted_norm),
            "persisted_minus_hash_inputs": _delta(persisted_norm, hash_inputs_norm),
            "hash_inputs_minus_rebuild_pool": _delta(hash_inputs_norm, rebuild_pool_norm) if rebuild_pool is not None else [],
            "rebuild_pool_minus_selected": _delta(rebuild_pool_norm, rebuild_selected_norm) if rebuild_selected is not None else [],
        }


        # === PATCH EVO_INJ_ADMISSION_REASON_CODES_V1 START ===
        # Purpose: make evolution/analysis admission & selection drops explain themselves with stable reason codes.
        # Purely additive: diagnostics only (does not alter fastpath, hashing, scrape, or rebuild behavior).
        admission_rejection_reasons = {}
        attempted_rejection_reasons = {}
        try:
            # Prefer explicit per-URL decisions if present (from other EVO admission tracing patches)
            _decisions = d.get("inj_admission_decisions") or d.get("admission_decisions") or {}
            if isinstance(_decisions, dict):
                for _u, _v in _decisions.items():
                    if not _u:
                        continue
                    if isinstance(_v, dict):
                        _decision = str(_v.get("decision") or "")
                        _reason = str(_v.get("reason_code") or _v.get("reason") or "")
                    else:
                        _decision = str(_v or "")
                        _reason = ""
                    if _decision.lower().startswith("reject"):
                        admission_rejection_reasons[str(_u)] = _reason or "rejected_by_merge"
        except Exception:
            pass

        # Heuristic reason coding for intake→admitted drops
        for _u in (deltas.get("intake_minus_admitted") or []):
            if not _u:
                continue
            if _u in admission_rejection_reasons:
                continue
            _rsn = ""
            try:
                if not str(_u).startswith(("http://", "https://")):
                    _rsn = "invalid_scheme"
                elif str(stage) == "evolution" and str(path).startswith(("fastpath", "fastpath_replay")):
                    # In fastpath/replay, extra URLs may be visible but not admitted into the scrape/hash universe by policy.
                    _rsn = "fastpath_replay_no_admission"
                else:
                    _rsn = "unknown_rejected_pre_admission"
            except Exception:
                pass
                _rsn = "unknown_rejected_pre_admission"
            admission_rejection_reasons[str(_u)] = _rsn

        # Heuristic reason coding for admitted→attempted drops
        _attempted_urls = [x.get("url") for x in attempted_min if isinstance(x, dict) and x.get("url")]
        for _u in (deltas.get("admitted_minus_attempted") or []):
            if not _u:
                continue
            if str(stage) == "evolution" and str(path).startswith(("fastpath", "fastpath_replay")):
                attempted_rejection_reasons[str(_u)] = "fastpath_replay_no_fetch"
            else:
                attempted_rejection_reasons[str(_u)] = "not_fetched_or_filtered_before_fetch"

        # Policy/context snapshot (small + stable)
        try:
            import os as _os
            policy = {
                "exclude_injected_from_hash_env": str(_os.getenv("YUREEKA_EXCLUDE_INJECTED_URLS_FROM_SNAPSHOT_HASH") or ""),
                "force_include_injected_in_hash_env": str(_os.getenv("YUREEKA_INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH") or ""),
                "evolution_calls_fetch_web_context": d.get("evolution_calls_fetch_web_context"),
                "evolution_fastpath_allows_injection": False,
            }
        except Exception:
            pass
            policy = {}
        # === PATCH EVO_INJ_ADMISSION_REASON_CODES_V1 END ===
        return {
            "run_id": str(d.get("run_id") or ""),
            "stage": str(stage or ""),
            "path": str(path or ""),
            "ui_raw": ui_raw,
            "ui_norm": ui_norm,
            "intake_norm": intake_norm,
            "admitted_norm": admitted_norm,
            "attempted": attempted_min,
            "persisted_norm": persisted_norm,
            "hash_inputs_norm": hash_inputs_norm,
            "rebuild_pool_norm": rebuild_pool_norm if rebuild_pool is not None else None,
            "rebuild_selected_norm": rebuild_selected_norm if rebuild_selected is not None else None,
            "counts": {
                "ui_norm": int(len(ui_norm)),
                "intake_norm": int(len(intake_norm)),
                "admitted_norm": int(len(admitted_norm)),
                "attempted": int(len(attempted_min)),
                "persisted_norm": int(len(persisted_norm)),
                "hash_inputs_norm": int(len(hash_inputs_norm)),
                "rebuild_pool_norm": int(len(rebuild_pool_norm)) if rebuild_pool is not None else None,
                "rebuild_selected_norm": int(len(rebuild_selected_norm)) if rebuild_selected is not None else None,
            },
            "set_hashes": {
                "ui_norm": _inj_diag_set_hash(ui_norm),
                "intake_norm": _inj_diag_set_hash(intake_norm),
                "admitted_norm": _inj_diag_set_hash(admitted_norm),
                "persisted_norm": _inj_diag_set_hash(persisted_norm),
                "hash_inputs_norm": _inj_diag_set_hash(hash_inputs_norm),
                "rebuild_pool_norm": _inj_diag_set_hash(rebuild_pool_norm) if rebuild_pool is not None else "",
                "rebuild_selected_norm": _inj_diag_set_hash(rebuild_selected_norm) if rebuild_selected is not None else "",
            },
            "deltas": deltas,
            "rejection_reasons": {
                "intake_minus_admitted": admission_rejection_reasons,
                "admitted_minus_attempted": attempted_rejection_reasons,
            },
            "policy": policy,
        }
    except Exception:
        return {"stage": str(stage or ""), "path": str(path or ""), "error": "inj_trace_build_failed"}

# =====================================================================
# PATCH FIX2D66_INJECTION_ADMISSION_HELPERS (ADDITIVE)
# Purpose:
# - Make injected URL admission deterministic and auditable across modes.
# - Promote UI/raw diagnostic fields into web_context.extra_urls when missing.
# - Build a minimal diag_injected_urls payload when fetch_web_context wasn't called
#   (common on evolution replay/fastpath).
# - Pure wiring + diagnostics: does NOT refetch, does NOT change selector logic.
# =====================================================================

def _fix2d66_extract_urls_from_text(text: str) -> list:
    try:
        if not isinstance(text, str):
            return []
        t = text.strip()
        if not t:
            return []
        # conservative URL matcher
        urls = re.findall(r"https?://[^\s\]\)\}\>\"']+", t)
        return [u.strip() for u in urls if isinstance(u, str) and u.strip()]
    except Exception:
        return []


def _fix2d66_collect_injected_urls_from_wc(web_context: dict, question: str = "") -> dict:
    """Return dict with ui_raw, ui_norm, intake_norm (normalized list) from many possible inputs."""
    wc = web_context if isinstance(web_context, dict) else {}

    # candidates from list-like fields
    candidates = []
    for k in (
        "extra_urls",
        "diag_extra_urls_final",
        "diag_extra_urls",
        "extra_urls_final",
        "diag_extra_urls_ui",
        "extra_urls_ui",
        "diag_extra_urls_ui_norm",
        "extra_urls_ui_norm",
    ):
        v = wc.get(k)
        if isinstance(v, (list, tuple)):
            candidates.extend([x for x in v if isinstance(x, str) and x.strip()])
        elif isinstance(v, str) and v.strip() and k.endswith("_final"):
            candidates.extend(_fix2d66_extract_urls_from_text(v))

    # raw text fields
    ui_raw = ""
    for k in (
        "diag_extra_urls_ui_raw",
        "extra_urls_ui_raw",
        "extra_sources_text",
        "extra_sources_text_tab1",
    ):
        v = wc.get(k)
        if isinstance(v, str) and v.strip():
            if not ui_raw:
                ui_raw = v
            candidates.extend(_fix2d66_extract_urls_from_text(v))
            # also split on newlines/commas
            for line in v.splitlines():
                for part in line.split(','):
                    part = (part or '').strip()
                    if part.startswith('http://') or part.startswith('https://'):
                        candidates.append(part)

    # also allow URLs embedded in the question text
    if isinstance(question, str) and question.strip():
        candidates.extend(_fix2d66_extract_urls_from_text(question))

    norm = []
    try:
        norm = _inj_diag_norm_url_list(candidates)
    except Exception:
        pass
        norm = [x for x in candidates if isinstance(x, str) and x]

    return {
        "ui_raw": ui_raw,
        "ui_norm": list(norm),
        "intake_norm": list(norm),
    }


def _fix2d66_promote_injection_in_web_context(web_context: dict, question: str = "") -> dict:
    """Mutate web_context in-place: ensure extra_urls + diag_injected_urls reflect UI/raw injection."""
    wc = web_context if isinstance(web_context, dict) else {}
    info = _fix2d66_collect_injected_urls_from_wc(wc, question=question)
    intake = info.get('intake_norm') or []

    # Promote into extra_urls when missing/empty
    try:
        cur = wc.get('extra_urls')
        needs = (not isinstance(cur, (list, tuple)) or not cur)
        if needs and intake:
            wc['extra_urls'] = list(intake)
    except Exception:
        pass

    # Ensure diag_injected_urls exists even if fetch_web_context wasn't called
    try:
        wc.setdefault('diag_injected_urls', {})
        if isinstance(wc.get('diag_injected_urls'), dict):
            d = wc['diag_injected_urls']
            # Fill minimally if absent
            d.setdefault('run_id', wc.get('diag_run_id') or _inj_diag_make_run_id('inj'))
            if info.get('ui_raw') and not d.get('ui_raw'):
                d['ui_raw'] = info.get('ui_raw')
            d.setdefault('ui_norm', info.get('ui_norm') or [])
            d.setdefault('intake_norm', intake)
            # If no admitted present, treat intake as admitted universe (diagnostic only)
            if not isinstance(d.get('admitted'), list) or not d.get('admitted'):
                d['admitted'] = list(intake)
    except Exception:
        pass

    # breadcrumb
    try:
        wc.setdefault('debug', {})
        if isinstance(wc.get('debug'), dict):
            wc['debug'].setdefault('fix2d66_injection', {})
            if isinstance(wc['debug'].get('fix2d66_injection'), dict):
                wc['debug']['fix2d66_injection'].update({
                    'promoted': bool(intake),
                    'intake_count': int(len(intake)),
                    'intake_set_hash': _inj_diag_set_hash(intake) if intake else '',
                })
    except Exception:
        return wc
# =====================================================================

# =====================================================================
# PATCH INJ_TRACE_V1_ENRICH_FROM_ARTIFACTS (ADDITIVE)
# Purpose:
# - Populate inj_trace_v1 attempted/persisted fields from *real* artifacts when
#   the upstream diag_injected_urls payload is partial (common in baseline/no-injection
#   or fastpath replay scenarios).
# - Pure diagnostics only: does NOT alter control flow, hashing, scraping, or selection.
#
# Artifacts supported:
#   - baseline_sources_cache (BSC): list of per-url snapshot dicts
#   - scraped_meta: dict keyed by url with status/status_detail/clean_text_len
# =====================================================================

def _inj_trace_v1_enrich_diag_from_bsc(diag: dict, baseline_sources_cache: list) -> dict:
    """Add attempted/persisted evidence into diag_injected_urls from baseline_sources_cache."""
    try:
        d = diag if isinstance(diag, dict) else {}
        bsc = baseline_sources_cache if isinstance(baseline_sources_cache, list) else []
        # If attempted already present, do not overwrite (avoid clobbering richer traces).
        if not isinstance(d.get("attempted"), list) or not d.get("attempted"):
            attempted = []
            for row in bsc:
                if not isinstance(row, dict):
                    continue
                u = str(row.get("url") or row.get("source_url") or "").strip()
                if not u:
                    continue
                st = str(row.get("status") or row.get("fetch_status") or "").strip() or "unknown"
                rs = str(row.get("status_detail") or row.get("fail_reason") or "").strip()
                clen = row.get("clean_text_len") or row.get("content_len") or 0
                try:
                    clen_i = int(clen)
                except Exception:
                    pass
                    clen_i = 0
                attempted.append({"url": u, "status": st, "reason": rs, "content_len": clen_i})
            if attempted:
                d["attempted"] = attempted

        # Persisted: if missing, derive from successful snapshot rows in BSC.
        if not isinstance(d.get("persisted"), (list, str)) or not d.get("persisted"):
            persisted = []
            for row in bsc:
                if not isinstance(row, dict):
                    continue
                u = str(row.get("url") or row.get("source_url") or "").strip()
                if not u:
                    continue
                st = str(row.get("status") or row.get("fetch_status") or "").lower().strip()
                if st in ("success", "ok", "fetched"):
                    persisted.append(u)
            if persisted:
                d["persisted"] = _inj_diag_norm_url_list(persisted)

        return d
    except Exception:
        return diag if isinstance(diag, dict) else {}

def _inj_trace_v1_enrich_diag_from_scraped_meta(diag: dict, scraped_meta: dict, extra_urls: list) -> dict:
    """Add attempted/persisted evidence into diag_injected_urls from scraped_meta (evolution-side)."""
    try:
        d = diag if isinstance(diag, dict) else {}
        sm = scraped_meta if isinstance(scraped_meta, dict) else {}
        xs = _inj_diag_norm_url_list(extra_urls or [])
        if not xs:
            return d

        # attempted rows for injected urls
        if not isinstance(d.get("attempted"), list) or not d.get("attempted"):
            attempted = []
            for u in xs:
                m = sm.get(u) if isinstance(sm.get(u), dict) else {}
                st = str(m.get("status") or m.get("fetch_status") or "").strip() or "unknown"
                rs = str(m.get("status_detail") or m.get("fail_reason") or "").strip()
                clen = m.get("clean_text_len") or m.get("content_len") or 0
                try:
                    clen_i = int(clen)
                except Exception:
                    pass
                    clen_i = 0
                attempted.append({"url": u, "status": st, "reason": rs, "content_len": clen_i})
            d["attempted"] = attempted

        # persisted success urls (only for injected)
        if not isinstance(d.get("persisted"), (list, str)) or not d.get("persisted"):
            persisted = []
            for a in (d.get("attempted") or []):
                if not isinstance(a, dict):
                    continue
                st = str(a.get("status") or "").lower().strip()
                if st in ("success", "ok", "fetched"):
                    u = str(a.get("url") or "").strip()
                    if u:
                        persisted.append(u)
            d["persisted"] = _inj_diag_norm_url_list(persisted)

        return d
    except Exception:
        return diag if isinstance(diag, dict) else {}
# =====================================================================

# =====================================================================

# =====================================================================

def scrape_url(url: str) -> Optional[str]:
    """
    Scrape webpage content.

    Priority:
      1) ScrapingDog (if SCRAPINGDOG_KEY is present)
      2) Safe fallback: direct requests + BeautifulSoup visible-text extraction

    Returns:
      - Clean visible text (<= 3000 chars) or None
    """
    import re

    url_s = (url or "").strip()
    if not url_s:
        return None

    def _clean_html_to_text(html: str) -> str:
        try:
            from bs4 import BeautifulSoup  # type: ignore
            soup = BeautifulSoup(html or "", "html.parser")
            for tag in soup(["script", "style", "nav", "footer", "header", "aside", "noscript", "form"]):
                try:
                    tag.decompose()
                except Exception:
                    pass
            txt = soup.get_text(separator="\n")
        except Exception:
            pass
            # fallback: strip tags
            txt = re.sub(r"(?is)<(script|style|noscript).*?>.*?</\1>", " ", html or "")
            txt = re.sub(r"(?is)<[^>]+>", " ", txt)
        # normalize whitespace
        lines = [ln.strip() for ln in (txt or "").splitlines() if ln.strip()]
        out = "\n".join(lines)
        out = re.sub(r"\n{3,}", "\n\n", out)
        return out.strip()

    def _direct_fetch(u: str) -> Optional[str]:
        try:
            headers = {
                "User-Agent": (
                    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/120.0.0.0 Safari/537.36"
                ),
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9",
                "Cache-Control": "no-cache",
                "Pragma": "no-cache",
            }
            resp = requests.get(u, headers=headers, timeout=12, allow_redirects=True)
            if resp.status_code >= 400:
                return None

            ctype = (resp.headers.get("Content-Type") or "").lower()
            if "application/pdf" in ctype:
                return None

            cleaned = _clean_html_to_text(resp.text or "")
            cleaned = cleaned.strip()
            if not cleaned:
                return None
            return cleaned[:3000]
        except Exception:
            return None

    # 1) ScrapingDog path (if configured)
    if globals().get("SCRAPINGDOG_KEY"):
        try:
            params = {"api_key": SCRAPINGDOG_KEY, "url": url_s, "dynamic": "false"}
            resp = requests.get("https://api.scrapingdog.com/scrape", params=params, timeout=15)
            if resp.status_code < 400:
                cleaned = _clean_html_to_text(resp.text or "").strip()
                if cleaned:
                    return cleaned[:3000]
        except Exception:
            pass  # fall through to direct fetch

    # 2) Safe fallback
    return _direct_fetch(url_s)


def fetch_web_context(
    query: str,
    num_sources: int = 3,
    *,
    fallback_mode: bool = False,
    fallback_urls: list = None,
    existing_snapshots: Any = None,   # <-- ADDITIVE
    # ============================================================
    # PATCH FWC_EXTRA_URLS1 (ADDITIVE)
    # ============================================================
    extra_urls: Any = None,
    # ============================================================
    # PATCH INJ_DIAG_FWC_ARGS (ADDITIVE): correlation + UI raw
    # ============================================================
    diag_run_id: str = "",
    diag_extra_urls_ui_raw: Any = None,
    # ============================================================
    # PATCH FWC_IDENTITY_ONLY1 (ADDITIVE): admission-only mode (no scraping)
    # ============================================================
    identity_only: bool = False,
    # ============================================================
    # PATCH FIX41AFC8 (ADDITIVE): force scrape extra_urls even if not admitted
    # - Default False: no behavior change.
    # - When True: normalized extra URLs will be appended to admitted list for scraping.
    # ============================================================
    force_scrape_extra_urls: bool = False,
    # ============================================================
    # PATCH FIX41AFC13 (ADDITIVE): force admit extra_urls into admitted list (pre-admission override)
    # - Default False: no behavior change.
    # - When True: normalized extra URLs will be appended to admitted list (not just scrape list),
    #   enabling deterministic admission of injected URLs when delta exists.
    # ============================================================
    force_admit_extra_urls: bool = False,
) -> dict:

    """
    Web context collector used by BOTH analysis + evolution.

    Enhancements:
    - Dashboard telemetry (sources found / HQ / admitted / scraped / success)
    - Keeps snapshot-friendly scraped_meta (fingerprint + extracted_numbers + numbers_found)
    - Uses scrape_url() which now has ScrapingDog + safe fallback scraper
    - Restores legacy contract: web_context["sources"] AND ["web_sources"]
    """
    import re
    from datetime import datetime, timezone

    # FIX2D66_PROMOTE_INJECTED_URLS_IN_ATTACH (ADDITIVE)
    try:
        _qtxt = str((analysis or {}).get('question') or (analysis or {}).get('query') or '')
        web_context = _fix2d66_promote_injected_urls(web_context or {}, question_text=_qtxt, stage='analysis_attach')
    except Exception:
        pass

    def _now_iso() -> str:
        return datetime.now(timezone.utc).isoformat()

    def _is_probably_url(s: str) -> bool:
        if not s or not isinstance(s, str):
            return False
        t = s.strip()
        if " " in t:
            return False
        if re.match(r"^https?://", t, flags=re.I):
            return True
        if re.match(r"^[a-z0-9.-]+\.[a-z]{2,}(/.*)?$", t, flags=re.I):
            return True
        return False

    def _normalize_url(s: str) -> str:
        t = (s or "").strip()
        if not t:
            return ""
        if re.match(r"^https?://", t, flags=re.I):
            return t
        if re.match(r"^[a-z0-9.-]+\.[a-z]{2,}(/.*)?$", t, flags=re.I):
            return "https://" + t
        return ""


    out = {
        "query": query,
        "sources": [],        # ✅ legacy key many downstream blocks expect
        "web_sources": [],    # ✅ newer key used by evolution/snapshots
        "search_results": [],
        "scraped_meta": {},
        "scraped_content": {},
        "errors": [],
        "status": "ok",
        "status_detail": "",
        "fetched_at": _now_iso(),
        "debug_counts": {},   # ✅ telemetry for dashboard + JSON debugging
    }

    # ---- ADDITIVE: snapshot reuse lookup (Change #3) ----
    snap_lookup = {}
    if isinstance(existing_snapshots, dict):
        snap_lookup = existing_snapshots
    elif isinstance(existing_snapshots, list):
        for s in existing_snapshots:
            if isinstance(s, dict) and s.get("url"):
                snap_lookup[str(s.get("url")).strip()] = s

    extractor_fp = get_extractor_fingerprint()
    # ----------------------------------------------------


    q = (query or "").strip()
    if not q:
        out["status"] = "no_query"
        out["status_detail"] = "empty_query"
        return out

    # -----------------------------
    # 1) Search (SerpAPI) OR fallback_urls
    # -----------------------------
    search_results = []
    urls_raw = []

    if not fallback_mode:
        try:
            sr = search_serpapi(q, num_results=10) or []
            if isinstance(sr, list):
                search_results = sr
        except Exception as e:
            out["errors"].append(f"search_failed:{type(e).__name__}")
            search_results = []

        out["search_results"] = search_results

        # Extract urls from results
        for r in (search_results or []):
            if isinstance(r, dict):
                u = (r.get("link") or r.get("url") or "").strip()
                if _is_probably_url(u):
                    urls_raw.append(u)
            elif isinstance(r, str):
                if _is_probably_url(r):
                    urls_raw.append(r.strip())

    else:
        # Evolution fallback: use provided URLs
        if isinstance(fallback_urls, list):
            for u in fallback_urls:
                if isinstance(u, str) and _is_probably_url(u.strip()):
                    urls_raw.append(u.strip())

    # -----------------------------
    # 2) Compute "HQ" counts (like old version)
    # -----------------------------
    total_found = len(search_results) if not fallback_mode else len(urls_raw)
    hq_count = 0

    try:
        fn_rel = globals().get("classify_source_reliability")
        if callable(fn_rel) and not fallback_mode:
            for r in (search_results or []):
                if not isinstance(r, dict):
                    continue
                u = (r.get("link") or "").strip()
                if not u:
                    continue
                label = fn_rel(u) or ""
                if "✅" in str(label):
                    hq_count += 1
    except Exception:
        pass
        hq_count = 0

    # -----------------------------
    # 3) Sanitize + normalize + dedupe
    # -----------------------------
    normed = []
    seen = set()
    for u in (urls_raw or []):
        nu = _normalize_url(u)
        if not nu:
            continue
        if nu in seen:
            continue
        seen.add(nu)
        normed.append(nu)

    # admitted for scraping (top N)
    try:
        n = int(num_sources or 3)
    except Exception:
        pass
        n = 3
    n = max(1, min(12, n))
    admitted = normed[:n] if not fallback_mode else normed  # fallback_mode typically wants all

    # =====================================================================
    # PATCH FIX41AFC8 (ADDITIVE): Force-scrape normalized extra URLs even if admission filters drop them
    #
    # Why:
    # - In evolution injection scenarios, extra URLs may be deliberately outside the normal
    #   admitted universe (domain allowlists, heuristics, etc.), but the user's intent is
    #   to attempt a fetch so the run can either persist a snapshot or fail with a concrete reason.
    #
    # Behavior:
    # - When force_scrape_extra_urls=True and normalized extras exist, append them into the
    #   admitted list (deduped, stable order) so downstream scraping attempts occur.
    #
    # Safety:
    # - Default is False (no change for normal runs).
    # - Never raises.
    # =====================================================================
    try:
        if bool(force_scrape_extra_urls):
            _fx8_extras = []
            if "_extras" in locals() and isinstance(_extras, list):
                _fx8_extras = [u for u in _extras if isinstance(u, str) and u.strip()]
            if _fx8_extras and isinstance(admitted, list):
                _seen = set([u for u in admitted if isinstance(u, str)])
                for _u in _fx8_extras:
                    if _u not in _seen:
                        admitted.append(_u)
                        _seen.add(_u)
                # breadcrumb for diagnostics
                try:
                    out.setdefault("debug_counts", {})
                    if isinstance(out.get("debug_counts"), dict):
                        out["debug_counts"].setdefault("fix41afc8", {})
                        if isinstance(out["debug_counts"].get("fix41afc8"), dict):
                            out["debug_counts"]["fix41afc8"].update({
                                "force_scrape_extra_urls": True,
                                "force_scrape_extra_urls_count": int(len(_fx8_extras)),
                            })
                except Exception:
                    pass
    except Exception:
        pass


    # ============================================================
    # PATCH FWC_EXTRA_URLS2 (ADDITIVE)
    # ============================================================
    try:
        _extras_in = extra_urls or []
        _extras = []
        _canon_map = {}
        if isinstance(_extras_in, str):
            _extras_in = [u.strip() for u in _extras_in.splitlines()]
        if isinstance(_extras_in, (list, tuple)):
            for u in _extras_in:
                u = str(u or "").strip()
                if not u:
                    continue
                if not (u.startswith("http://") or u.startswith("https://")):
                    continue
                _canon = _canonicalize_injected_url(u) or u
                _extras.append(_canon)
                try:
                    _canon_map[u] = _canon
                except Exception:
                    pass
        _seen = set()
        merged = []
        for u in _extras + (admitted or []):
            if u in _seen:
                continue
            _seen.add(u)
            merged.append(u)
        admitted = merged
        out.setdefault("debug", {})
        if isinstance(out.get("debug"), dict):
            out["debug"].setdefault("fwc_extra_urls", {})
            out["debug"]["fwc_extra_urls"]["extra_urls_count"] = int(len(_extras))
            out["debug"]["fwc_extra_urls"]["admitted_count_after_merge"] = int(len(admitted or []))
            out["debug"]["fwc_extra_urls"]["extra_urls"] = _extras[:20]
    except Exception:
        pass


    # =====================================================================
    # PATCH INJ_DIAG_FWC_STAGE (ADDITIVE): injected-URL stage checkpoints (A1-A3)
    # Records: UI->intake->admitted, and later enriches with scrape outcomes.
    # =====================================================================
    try:
        _diag_run = str(diag_run_id or "") or _inj_diag_make_run_id("analysis")
        out["diag_run_id"] = out.get("diag_run_id") or _diag_run

        _ui_raw = diag_extra_urls_ui_raw if diag_extra_urls_ui_raw is not None else extra_urls
        _ui_norm = _inj_diag_norm_url_list(_ui_raw)
        _intake_norm = list(_extras or []) if "_extras" in locals() and isinstance(_extras, list) else _inj_diag_norm_url_list(extra_urls)

        out["diag_injected_urls"] = {
            "run_id": _diag_run,
            "ui_raw": _ui_raw if isinstance(_ui_raw, (str, list, tuple)) else str(_ui_raw or ""),
            "ui_norm": _ui_norm,
            "intake_norm": _intake_norm,
            "admitted": list(admitted or []),
            "attempted": [],
            "persisted": [],
            "hash_inputs": [],
            "rebuild_pool": [],
            "rebuild_selected": [],
            "set_hashes": {
                "ui_norm": _inj_diag_set_hash(_ui_norm),
                "intake_norm": _inj_diag_set_hash(_intake_norm),
                "admitted": _inj_diag_set_hash(list(admitted or [])),
            },
            "canon_map": dict(_canon_map) if "_canon_map" in locals() else {},
            "deltas": {
                "ui_minus_intake": sorted(list(set(_ui_norm) - set(_intake_norm))),
                "intake_minus_admitted": sorted(list(set(_intake_norm) - set(list(admitted or [])))),
            },
        }
    except Exception:
        pass
    # =====================================================================


    # =====================================================================
    # PATCH FIX41AFC13 (ADDITIVE): Pre-admission override for extra_urls (injection lane)
    #
    # Goal:
    # - When force_admit_extra_urls is True, ensure normalized extra_urls are INCLUDED in the
    #   admitted list itself (not only the scrape list). This prevents injected URLs from dying
    #   at "intake_minus_admitted" and allows deterministic fetch/persist behavior.
    #
    # Safety:
    # - Default flag False => no behavior change.
    # - Never raises.
    # =====================================================================
    try:
        if force_admit_extra_urls:
            _fix41afc13_extra = _inj_diag_norm_url_list(extra_urls) if extra_urls else []
            if _fix41afc13_extra:
                _fix41afc13_before = list(admitted or [])
                _fix41afc13_set = set(_inj_diag_norm_url_list(_fix41afc13_before))
                _fix41afc13_added = []
                for _u in _fix41afc13_extra:
                    if _u and _u not in _fix41afc13_set:
                        _fix41afc13_before.append(_u)
                        _fix41afc13_set.add(_u)
                        _fix41afc13_added.append(_u)
                if _fix41afc13_added:
                    admitted = _fix41afc13_before
                    out.setdefault("debug_counts", {})
                    if isinstance(out.get("debug_counts"), dict):
                        out["debug_counts"].update({
                            "forced_admit_extra_urls_count": int(len(_fix41afc13_added)),
                        })
                    out.setdefault("debug", {})
                    if isinstance(out.get("debug"), dict):
                        out["debug"].setdefault("fix41afc13", {})
                        if isinstance(out["debug"].get("fix41afc13"), dict):
                            out["debug"]["fix41afc13"].update({
                                "forced_admit_applied": True,
                                "forced_admit_added": list(_fix41afc13_added),
                                "forced_admit_total_extra": int(len(_fix41afc13_extra)),
                            })
    except Exception:
        pass
    # =====================================================================

    out["sources"] = admitted
    out["web_sources"] = admitted

    # Telemetry before scrape
    out["debug_counts"].update({
        "total_found": int(total_found),
        "high_quality": int(hq_count),
        "admitted_for_scraping": int(len(admitted)),
        "fallback_mode": bool(fallback_mode),
    })

    # Dashboard info (restored)
    try:
        if not fallback_mode:
            st.info(
                f"🔍 Sources Found: **{out['debug_counts']['total_found']} total** | "
                f"**{out['debug_counts']['high_quality']} high-quality** | "
                f"Scraping **{out['debug_counts']['admitted_for_scraping']}**"
            )
        else:
            st.info(
                f"🧩 Fallback Sources: **{out['debug_counts']['admitted_for_scraping']}** (no SerpAPI search)"
            )
    except Exception:
        pass

    if not admitted:
        out["status"] = "no_sources"
        out["status_detail"] = "empty_sources_after_filter"
        return out

    # ============================================================
    # PATCH FWC_IDENTITY_ONLY2 (ADDITIVE): identity-only early return
    # ============================================================
    try:
        if bool(identity_only):
            out["status"] = out.get("status") or "ok"
            out["status_detail"] = out.get("status_detail") or "identity_only"
            return out
    except Exception:
        pass




    # -----------------------------
    # 4) Scrape + extract numbers (snapshot-friendly scraped_meta)
    # -----------------------------
    fn_fp = globals().get("fingerprint_text")
    fn_extract = globals().get("extract_numbers_with_context") or globals().get("extract_numeric_candidates") or globals().get("extract_numbers_from_text")

    scraped_attempted = 0
    scraped_ok_text = 0
    scraped_ok_numbers = 0
    scraped_failed = 0

    # optional progress bar
    progress = None
    try:
        progress = st.progress(0)
    except Exception:
        pass
        progress = None

    for i, url in enumerate(admitted):
        scraped_attempted += 1

        meta = {
            "url": url,
            "fetched_at": _now_iso(),
            "status": "failed",
            "status_detail": "",
            "content_type": "",
            "content_len": 0,
            "clean_text_len": 0,
            "fingerprint": None,
            "numbers_found": 0,
            "extracted_numbers": [],
            "content": "",
            "clean_text": "",
        }

        try:
            text = scrape_url(url)  # ✅ ScrapingDog + fallback inside scrape_url
            if not text or not str(text).strip():
                meta["status"] = "failed"
                meta["status_detail"] = "failed:no_text"
                scraped_failed += 1
                out["scraped_meta"][url] = meta
            else:
                cleaned = str(text).strip()
                meta["status"] = "fetched"
                meta["status_detail"] = "success"
                meta["content"] = cleaned
                meta["clean_text"] = cleaned
                meta["content_len"] = len(cleaned)
                meta["clean_text_len"] = len(cleaned)

                # fingerprint
                try:
                    if callable(fn_fp):
                        meta["fingerprint"] = fn_fp(cleaned)
                    else:
                        meta["fingerprint"] = fingerprint_text(cleaned) if callable(globals().get("fingerprint_text")) else None
                except Exception:
                    pass
                    meta["fingerprint"] = None

                # ---- ADDITIVE: reuse extracted_numbers when unchanged (Change #3) ----
                meta["extractor_fingerprint"] = extractor_fp
                prev = snap_lookup.get(url) if isinstance(snap_lookup, dict) else None
                if isinstance(prev, dict):
                    if prev.get("fingerprint") == meta.get("fingerprint") and prev.get("extractor_fingerprint") == extractor_fp:
                        prev_nums = prev.get("extracted_numbers")
                        if isinstance(prev_nums, list) and prev_nums:
                            meta["extracted_numbers"] = prev_nums
                            meta["numbers_found"] = len(prev_nums)
                            meta["reused_snapshot"] = True

                            out["scraped_meta"][url] = meta
                            out["scraped_content"][url] = cleaned

                            scraped_ok_text += 1
                            if meta["numbers_found"] > 0:
                                scraped_ok_numbers += 1

                            if progress:
                                try:
                                    progress.progress((i + 1) / max(1, len(admitted)))
                                except Exception:
                                    pass

                            continue
                meta["reused_snapshot"] = False
                # ---------------------------------------------------------------

                # numeric extraction (analysis-aligned if fn exists)

                def _fix2d69a_norm_extraction_result_REMOVED(_res):
                    """Return a list of number dicts from extractor result.

                    Accepts:
                      - list
                      - tuple/list of (numbers, meta) where numbers is list
                      - None / other -> []
                    """
                    try:
                        if _res is None:
                            return []
                        if isinstance(_res, list):
                            return _res
                        if isinstance(_res, tuple) or isinstance(_res, list):
                            # e.g. (nums, meta)
                            if len(_res) >= 1 and isinstance(_res[0], list):
                                return _res[0]
                        # sometimes a dict wrapper
                        if isinstance(_res, dict) and isinstance(_res.get('numbers'), list):
                            return _res.get('numbers') or []
                    except Exception:
                        pass
                    return []

                nums = []
                meta["fix2d68_extract_attempted"] = bool(callable(fn_extract))
                meta["fix2d68_extract_input_len"] = int(len(cleaned) if isinstance(cleaned, str) else 0)
                try:
                    meta["fix2d68_extract_input_head"] = (cleaned[:200] if isinstance(cleaned, str) else "")
                except Exception:
                    meta["fix2d68_extract_input_head"] = ""

                _fix2d68_errors = []
                if callable(fn_extract):
                    # Robust dispatcher: try source_url, then url, then plain. Do not fail silently.
                    for _mode in ("source_url", "url", "plain"):
                        try:
                            _tmp = None
                            if _mode == "source_url":
                                _tmp = fn_extract(cleaned, source_url=url)
                            elif _mode == "url":
                                _tmp = fn_extract(cleaned, url=url)
                            else:
                                _tmp = fn_extract(cleaned)
                            # FIX2D69A: normalize extractor return (list | (list, meta) | dict | None)
                            if _tmp is None:
                                nums = []
                            elif isinstance(_tmp, list):
                                nums = _tmp
                            elif isinstance(_tmp, tuple) and len(_tmp) >= 1 and isinstance(_tmp[0], list):
                                nums = _tmp[0]
                            elif isinstance(_tmp, dict) and isinstance(_tmp.get("extracted_numbers"), list):
                                nums = _tmp.get("extracted_numbers") or []
                            else:
                                nums = []
                            meta["fix2d68_extract_call_mode"] = _mode
                            break
                        except Exception as _e:
                            _fix2d68_errors.append({"mode": _mode, "error": repr(_e)})
                            nums = []

                if _fix2d68_errors:
                    meta["fix2d68_extract_errors"] = _fix2d68_errors

                # FIX2D69A: normalize extractor return (list | (list, meta) | None)
                _nums_norm = []
                try:
                    if nums is None:
                        _nums_norm = []
                    elif isinstance(nums, list):
                        _nums_norm = nums
                    elif isinstance(nums, tuple) and len(nums) >= 1 and isinstance(nums[0], list):
                        _nums_norm = nums[0]
                    elif isinstance(nums, dict) and isinstance(nums.get("extracted_numbers"), list):
                        _nums_norm = nums.get("extracted_numbers") or []
                    else:
                        _nums_norm = []
                except Exception:
                    _nums_norm = []
                nums = _nums_norm


                if isinstance(nums, list):
                    meta["extracted_numbers"] = nums
                    meta["numbers_found"] = len(nums)

                    # ---- ADDITIVE: stable IDs + ordering (Change #2 / Part 1) ----
                    urlv = meta.get("url") or url
                    fpv = meta.get("fingerprint") or ""

                    for n in (meta["extracted_numbers"] or []):
                        if isinstance(n, dict):
                            if "extracted_number_id" not in n:
                                n["extracted_number_id"] = make_extracted_number_id(urlv, fpv, n)
                            if not n.get("source_url"):
                                n["source_url"] = urlv

                    meta["extracted_numbers"] = sort_snapshot_numbers(meta["extracted_numbers"])
                    meta["numbers_found"] = len(meta["extracted_numbers"])
                    # --------------------------------------------------------------

                out["scraped_meta"][url] = meta
                out["scraped_content"][url] = cleaned

                scraped_ok_text += 1
                if meta["numbers_found"] > 0:
                    scraped_ok_numbers += 1

        except Exception as e:
            meta["status"] = "failed"
            meta["status_detail"] = f"failed:exception:{type(e).__name__}"
            scraped_failed += 1
            out["scraped_meta"][url] = meta
            out["errors"].append(meta["status_detail"])

        if progress:
            try:
                progress.progress((i + 1) / max(1, len(admitted)))
            except Exception:
                pass


    # =====================================================================
    # PATCH INJ_DIAG_FWC_POSTSCRAPE (ADDITIVE): finalize scrape outcomes (A3)
    # =====================================================================
    try:
        d = out.get("diag_injected_urls")
        if isinstance(d, dict):
            _inj = set(d.get("intake_norm") or [])
            sm = out.get("scraped_meta") or {}
            attempted = []
            persisted = []
            if isinstance(sm, dict):
                for u in sorted(_inj):
                    meta = sm.get(u) or {}
                    status = (meta.get("status") or "")
                    status_detail = (meta.get("status_detail") or "")
                    content = meta.get("clean_text") or meta.get("content") or ""
                    attempted.append({
                        "url": u,
                        "attempted": bool(u in (admitted or [])),
                        "fetch_status": "success" if (str(status_detail).startswith("success") or status == "fetched") else ("failed" if meta else "skipped"),
                        "fail_reason": (str(status_detail) or str(status) or "")[:80],
                        "content_len": int(len(content) if isinstance(content, str) else 0),
                        "numbers_found": int(meta.get("numbers_found") or 0),
                    })
                    if str(status_detail).startswith("success") or status == "fetched":
                        persisted.append(u)
            d["attempted"] = attempted
            d["persisted"] = persisted
            d.setdefault("set_hashes", {})
            if isinstance(d["set_hashes"], dict):
                d["set_hashes"]["persisted"] = _inj_diag_set_hash(persisted)
    except Exception:
        pass
    # =====================================================================

    out["debug_counts"].update({
        "scraped_attempted": int(scraped_attempted),
        "scraped_ok_text": int(scraped_ok_text),
        "scraped_ok_numbers": int(scraped_ok_numbers),
        "scraped_failed": int(scraped_failed),
    })

    # Dashboard scrape summary
    try:
        st.info(
            f"🧽 Scrape Results: **{out['debug_counts']['scraped_ok_text']} ok-text** | "
            f"**{out['debug_counts']['scraped_ok_numbers']} ok-numbers** | "
            f"**{out['debug_counts']['scraped_failed']} failed**"
        )
    except Exception:
        pass

    # status summarization

    # =====================================================================
    # PATCH FWC_EXTRA_URLS_TRACE2 (ADDITIVE): trace how injected URLs were handled
    # Why:
    # - When scenario B "extra URLs" are provided, it can be unclear whether they:
    #     (a) were normalized/deduped
    #     (b) were admitted into the scrape list
    #     (c) were successfully scraped
    #     (d) actually entered the snapshot-hash pool used by analysis/evolution
    # - This patch records a deterministic, non-invasive trace in web_context only.
    # =====================================================================
    try:
        if not isinstance(out.get("debug_counts"), dict):
            out["debug_counts"] = {}
        _dbg_counts = out["debug_counts"]

        _extra_trace = {
            "extra_urls_requested": list(extra_urls or []) if isinstance(extra_urls, list) else [],
            "extra_urls_normalized": list(_extras or []) if "_extras" in locals() and isinstance(_extras, list) else [],
            "extra_urls_admitted": [],
            "extra_urls_scraped": [],
            "extra_urls_in_hash_pool": [],
            "notes": [],
        }

        # Which extras actually made it into the final admitted URL list?
        try:
            _admitted_urls = []
            if "admitted" in locals() and isinstance(admitted, list):
                _admitted_urls = [u for u in admitted if isinstance(u, str) and u.strip()]
            _extra_set = set(_extra_trace["extra_urls_normalized"])
            _extra_trace["extra_urls_admitted"] = [u for u in _admitted_urls if u in _extra_set]
        except Exception:
            pass

        # How did each extra URL scrape?
        try:
            sm = out.get("scraped_meta") or {}
            if isinstance(sm, dict):
                for u in _extra_trace["extra_urls_normalized"]:
                    meta = sm.get(u) or {}
                    if isinstance(meta, dict) and meta:
                        content = meta.get("clean_text") or meta.get("content") or ""
                        fp = meta.get("fingerprint")
                        _extra_trace["extra_urls_scraped"].append({
                            "url": u,
                            "status": meta.get("status"),
                            "status_detail": meta.get("status_detail"),
                            "fingerprint": (fp[:16] if isinstance(fp, str) else fp),
                            "numbers_found": meta.get("numbers_found"),
                            "content_len": (len(content) if isinstance(content, str) else 0),
                            "content_type": meta.get("content_type") or "",
                        })
        except Exception:
            pass

        # Approximate "hash pool" membership (non-invasive):
        # we mark extras whose scrape produced a non-empty fingerprint + some text.
        try:
            sm = out.get("scraped_meta") or {}
            if isinstance(sm, dict):
                for row in (_extra_trace.get("extra_urls_scraped") or []):
                    u = row.get("url")
                    meta = sm.get(u) or {}
                    content = meta.get("clean_text") or meta.get("content") or ""
                    fp = meta.get("fingerprint")
                    if isinstance(fp, str) and fp and isinstance(content, str) and len(content) >= 200:
                        _extra_trace["extra_urls_in_hash_pool"].append(u)
        except Exception:
            pass

        out["extra_urls_debug"] = _extra_trace
        _dbg_counts["extra_urls_trace"] = {
            "requested": len(_extra_trace.get("extra_urls_requested") or []),
            "normalized": len(_extra_trace.get("extra_urls_normalized") or []),
            "admitted": len(_extra_trace.get("extra_urls_admitted") or []),
            "scraped": len(_extra_trace.get("extra_urls_scraped") or []),
            "in_hash_pool": len(_extra_trace.get("extra_urls_in_hash_pool") or []),
        }
    except Exception:
        pass
    # =====================================================================
    if scraped_ok_text == 0:
        out["status"] = "failed"
        out["status_detail"] = "no_usable_text"
    elif scraped_ok_numbers == 0:
        out["status"] = "partial"
        out["status_detail"] = "text_ok_numbers_empty"
    else:
        out["status"] = "success"
        out["status_detail"] = "ok"

    return out




def unit_clean_first_letter(unit: str) -> str:
    """Normalize units to first letter (T/B/M/K/%), ignoring $ and spaces."""
    if not unit:
        return ""
    u = unit.replace("$", "").replace(" ", "").strip().upper()
    return u[0] if u else ""

# =========================================================
# 7. LLM QUERY FUNCTIONS
# =========================================================

def query_perplexity(query: str, web_context: Dict, query_structure: Optional[Dict[str, Any]] = None) -> Optional[str]:
    """
    Query Perplexity and return a validated JSON string (LLMResponse-compatible).
    Removes 'action' and excludes None fields from output JSON.
    """
    if not PERPLEXITY_KEY:
        st.error("❌ PERPLEXITY_KEY not set.")
        return None

    query_structure = query_structure or {}
    structure_txt = ""
    ordering_contract = ""

    try:
        structure_txt, ordering_contract = build_query_structure_prompt(query_structure)
    except Exception:
        pass
        structure_txt = ""
        ordering_contract = ""

    # Web context: show top sources + snippets
    sources = (web_context.get("sources", []) if isinstance(web_context, dict) else []) or []
    search_results = (web_context.get("search_results", []) if isinstance(web_context, dict) else []) or []
    search_count = int(web_context.get("search_count", len(search_results)) if isinstance(web_context, dict) else 0)

    context_section = "WEB CONTEXT:\n"
    for url in sources[:6]:
        content = (web_context.get("scraped_content", {}) or {}).get(url) if isinstance(web_context, dict) else None
        if content:
            context_section += f"\n{url}:\n{str(content)[:800]}...\n"
        else:
            context_section += f"\n{url}\n"

    enhanced_query = (
        f"{context_section}\n"
        f"{SYSTEM_PROMPT}\n\n"
        f"User Question: {query}\n\n"
        f"{structure_txt}\n\n"
        f"{ordering_contract}\n"
        f"Web search returned {search_count} results.\n"
        f"Return ONLY valid JSON matching the template and include all required fields."
    )

    headers = {
        "Authorization": f"Bearer {PERPLEXITY_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "sonar",
        "temperature": 0.0,
        "max_tokens": 2400,
        "top_p": 1.0,
        "messages": [{"role": "user", "content": enhanced_query}]
    }

    try:
        resp = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=45)
        resp.raise_for_status()
        data = resp.json()

        if "choices" not in data:
            raise Exception("No 'choices' in Perplexity response")

        content = data["choices"][0]["message"]["content"]
        if not content or not content.strip():
            raise Exception("Empty Perplexity response")

        parsed = parse_json_safely(content, "Perplexity")
        if not parsed:
            return create_fallback_response(query, search_count, web_context)

        repaired = repair_llm_response(parsed)

        # Ensure action is removed even if present
        repaired.pop("action", None)

        validate_numeric_fields(repaired, "Perplexity")

        try:
            llm_obj = LLMResponse.model_validate(repaired)

            # Ensure action not present (belt + suspenders)
            if hasattr(llm_obj, "action"):
                llm_obj.action = None

            # Merge web sources
            if isinstance(web_context, dict) and web_context.get("sources"):
                existing = llm_obj.sources or []
                merged = list(dict.fromkeys(existing + web_context["sources"]))
                llm_obj.sources = merged[:10]
                llm_obj.freshness = "Current (web-enhanced)"

            result = llm_obj.model_dump_json(exclude_none=True)
            cache_llm_response(query, web_context, result)
            return result

        except ValidationError as e:
            st.warning(f"⚠️ Pydantic validation failed: {e}")
            return create_fallback_response(query, search_count, web_context)

    except Exception as e:
        st.error(f"❌ Perplexity API error: {e}")
        return create_fallback_response(query, search_count, web_context)


def query_perplexity_raw(prompt: str, max_tokens: int = 400, timeout: int = 30) -> str:
    """
    Raw Perplexity call that returns text only.
    IMPORTANT: Does NOT attempt to validate as LLMResponse.
    """
    headers = {
        "Authorization": f"Bearer {PERPLEXITY_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "sonar",
        "temperature": 0.0,
        "top_p": 1.0,
        "max_tokens": max_tokens,
        "messages": [{"role": "user", "content": prompt}],
    }

    resp = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=timeout)
    resp.raise_for_status()
    data = resp.json()
    return (data.get("choices", [{}])[0].get("message", {}) or {}).get("content", "") or ""

def create_fallback_response(query: str, search_count: int, web_context: Dict) -> str:
    """Create fallback response matching schema, excluding None fields and removing action."""
    fallback = LLMResponse(
        executive_summary=f"Analysis of '{query}' completed with {search_count} web sources. Schema validation used fallback structure.",
        primary_metrics={
            "sources": MetricDetail(name="Web Sources", value=search_count, unit="sources"),
            "quality": MetricDetail(name="Data Quality", value=70, unit="%")
        },
        key_findings=[
            f"Web search found {search_count} relevant sources.",
            "Primary model output required fallback due to format issues.",
            "Manual review of raw data recommended for accuracy."
        ],
        top_entities=[
            TopEntityDetail(name="Source 1", share="N/A", growth="N/A")
        ],
        trends_forecast=[
            TrendForecastDetail(trend="Schema validation used fallback", direction="⚠️", timeline="Now")
        ],
        visualization_data=VisualizationData(
            chart_labels=["Attempt"],
            chart_values=[search_count],
            chart_title="Search Results"
        ),
        sources=web_context.get("sources", []),
        confidence=60,
        freshness="Current (fallback)",
        action=None
    )

    return fallback.model_dump_json(exclude_none=True)


# =========================================================
# 7B. ANCHORED EVOLUTION QUERY
# =========================================================

def _ensure_metric_labels(metric_changes: list) -> list:
    """
    Backward/forward compatible label normalization:
    - guarantees a non-empty display label
    - adds aliases so different UIs render correctly: metric_name, metric, label
    """
    import re

    def _prettify(s: str) -> str:
        s = str(s or "").strip()
        if not s:
            return ""
        s = s.replace("__", " ").replace("_", " ")
        s = re.sub(r"\s+", " ", s).strip()
        return s[:120]

    out = []
    for row in (metric_changes or []):
        if not isinstance(row, dict):
            continue

        name = row.get("name")
        if isinstance(name, str):
            name = name.strip()
        else:
            name = ""

        # try to derive a label if name missing (canonical_key or metric_definition.name)
        if not name:
            md = row.get("metric_definition") if isinstance(row.get("metric_definition"), dict) else {}
            name = (md.get("name") or "").strip() if isinstance(md.get("name"), str) else ""
        if not name:
            ckey = row.get("canonical_key")
            name = _prettify(ckey) if ckey else "Unknown Metric"

        # write canonical label + aliases
        row["name"] = name
        row.setdefault("metric_name", name)
        row.setdefault("metric", name)
        row.setdefault("label", name)

        out.append(row)

    return out


def format_previous_metrics(metrics: Dict) -> str:
    """Format previous metrics for prompt"""
    if not metrics:
        return "No previous metrics available"

    lines = []
    for key, m in metrics.items():
        if isinstance(m, dict):
            lines.append(f"- {m.get('name', key)}: {m.get('value', 'N/A')} {m.get('unit', '')}")
    return "\n".join(lines) if lines else "No metrics"

def format_previous_entities(entities: List) -> str:
    """Format previous entities for prompt"""
    if not entities:
        return "No previous entities available"

    lines = []
    for i, e in enumerate(entities, 1):
        if isinstance(e, dict):
            lines.append(f"{i}. {e.get('name', 'Unknown')}: {e.get('share', 'N/A')} share, {e.get('growth', 'N/A')} growth")
    return "\n".join(lines) if lines else "No entities"

def format_previous_findings(findings: List) -> str:
    """Format previous findings for prompt"""
    if not findings:
        return "No previous findings available"

    lines = [f"- {f}" for f in findings if f]
    return "\n".join(lines) if lines else "No findings"

def calculate_time_ago(timestamp_str: str) -> str:
    """Calculate human-readable time difference"""
    try:
        prev_time = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
        delta = datetime.now() - prev_time.replace(tzinfo=None)

        hours = delta.total_seconds() / 3600
        if hours < 24:
            return f"{hours:.1f} hours ago"
        elif hours < 168:  # 7 days
            return f"{hours/24:.1f} days ago"
        elif hours < 720:  # 30 days
            return f"{hours/168:.1f} weeks ago"
        else:
            return f"{hours/720:.1f} months ago"
    except:
        return "unknown time ago"

def query_perplexity_anchored(query: str, previous_data: Dict, web_context: Dict, temperature: float = 0.1) -> str:
    """
    Query Perplexity with previous analysis as anchor.
    This produces an evolution-aware response that tracks changes.
    """

    prev_response = previous_data.get("primary_response", {})
    prev_timestamp = previous_data.get("timestamp", "")
    prev_question = previous_data.get("question", query)

    time_ago = calculate_time_ago(prev_timestamp)

    # Build the anchored prompt
    anchored_prompt = EVOLUTION_PROMPT_TEMPLATE.format(
        time_ago=time_ago,
        previous_question=prev_question,
        previous_timestamp=prev_timestamp,
        previous_summary=prev_response.get("executive_summary", "No previous summary"),
        previous_metrics=format_previous_metrics(prev_response.get("primary_metrics", {})),
        previous_entities=format_previous_entities(prev_response.get("top_entities", [])),
        previous_findings=format_previous_findings(prev_response.get("key_findings", [])),
        query=query
    )

    # Add web context if available
    if web_context.get("summary"):
        anchored_prompt = f"CURRENT WEB RESEARCH:\n{web_context['summary']}\n\n{anchored_prompt}"

    # API request
    headers = {
        "Authorization": f"Bearer {PERPLEXITY_KEY}",
        "Content-Type": "application/json"
    }


    payload = {
        "model": "sonar",
        "temperature": 0.0,      # DETERMINISTIC
        "max_tokens": 2500,
        "top_p": 1.0,            # DETERMINISTIC
        "messages": [{"role": "user", "content": anchored_prompt}]
    }

    try:
        resp = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=60)
        resp.raise_for_status()
        data = resp.json()

        if "choices" not in data:
            raise Exception("No choices in response")

        content = data["choices"][0]["message"]["content"]
        if not content:
            raise Exception("Empty response")

        # Parse JSON
        parsed = parse_json_safely(content, "Perplexity-Anchored")
        if not parsed:
            return create_anchored_fallback(query, previous_data, web_context)

        # Add sources from web context
        if web_context.get("sources"):
            existing = parsed.get("sources", [])
            parsed["sources"] = list(dict.fromkeys(existing + web_context["sources"]))[:10]

        return json.dumps(parsed)

    except Exception as e:
        st.error(f"❌ Anchored query error: {e}")
        return create_anchored_fallback(query, previous_data, web_context)

def create_anchored_fallback(query: str, previous_data: Dict, web_context: Dict) -> str:
    """Create fallback for anchored evolution query"""
    prev_response = previous_data.get("primary_response", {})

    fallback = {
        "executive_summary": f"Evolution analysis for '{query}' - model returned invalid format. Showing previous data.",
        "analysis_delta": {
            "time_since_previous": calculate_time_ago(previous_data.get("timestamp", "")),
            "overall_trend": "unknown",
            "major_changes": ["Unable to determine changes - API error"],
            "data_freshness": "Unknown"
        },
        "primary_metrics": prev_response.get("primary_metrics", {}),
        "key_findings": ["[UNCHANGED] " + f for f in prev_response.get("key_findings", [])[:3]],
        "top_entities": prev_response.get("top_entities", []),
        "trends_forecast": prev_response.get("trends_forecast", []),
        "sources": web_context.get("sources", []),
        "confidence": 50,
        "freshness": "Fallback",
        "drift_summary": {
            "metrics_changed": 0,
            "metrics_unchanged": len(prev_response.get("primary_metrics", {})),
            "entities_reshuffled": 0,
            "findings_updated": 0,
            "overall_stability_pct": 100
        }
    }
    return json.dumps(fallback)

# =========================================================
# 8. VALIDATION & SCORING
# =========================================================


def parse_number_with_unit(val_str: str) -> float:
    """
    Parse a numeric string into a comparable base scale.
    Returns a float in "millions" for currency/volume-like values.
    Percentages are returned as their numeric value (e.g., "9.8%" -> 9.8).

    Handles:
      - $58.3B, 58.3B, S$29.8B, 29.8 S$B, USD 21.18 B
      - 58.3 billion, 58.3 bn, 58.3 million, 58.3 mn, 570 thousand
      - 570,000 (interpreted as an absolute count -> converted to millions)
      - 9.8% (kept as 9.8)
    """
    if val_str is None:
        return 0.0

    s = str(val_str).strip()
    if not s:
        return 0.0

    s_low = s.lower()

    # If it's a percentage, return the raw percent number (not millions)
    if "%" in s_low:
        m = re.search(r'(-?\d+(?:\.\d+)?)', s_low)
        if not m:
            return 0.0
        try:
            return float(m.group(1))
        except Exception:
            return 0.0

    # Normalize: remove commas and common currency tokens/symbols
    # (keep letters because we need bn/mn/b/m/k detection)
    s_low = s_low.replace(",", " ")
    for token in ["s$", "usd", "sgd", "us$", "$", "€", "£", "aud", "cad"]:
        s_low = s_low.replace(token, " ")

    # Collapse whitespace
    s_low = re.sub(r"\s+", " ", s_low).strip()

    # Extract the first number
    m = re.search(r'(-?\d+(?:\.\d+)?)', s_low)
    if not m:
        return 0.0

    try:
        num = float(m.group(1))
    except Exception:
        return 0.0

    # Look at the remaining text after the number for unit words/suffix
    tail = s_low[m.end():].strip()

    # Decide multiplier (base = millions)
    # billions -> *1000, millions -> *1, thousands -> *0.001
    multiplier = 1.0

    # Word-based units
    if re.search(r'\b(trillion|tn)\b', tail):
        multiplier = 1_000_000.0  # trillion -> million
    elif re.search(r'\b(billion|bn)\b', tail):
        multiplier = 1000.0
    elif re.search(r'\b(million|mn)\b', tail):
        multiplier = 1.0
    elif re.search(r'\b(thousand|k)\b', tail):
        multiplier = 0.001
    else:
        # Suffix-style units (possibly with spaces), e.g. "29.8 b", "21.18 b", "58.3m"
        # We only look at the very first letter-ish token in tail.
        t0 = tail[:4].strip()  # enough to catch "b", "m", "k"
        if t0.startswith("b"):
            multiplier = 1000.0
        elif t0.startswith("m"):
            multiplier = 1.0
        elif t0.startswith("k"):
            multiplier = 0.001
        else:
            # No unit detected. If it's a big integer like 570000 (jobs, people),
            # interpret as an absolute count and convert to millions.
            # (570000 -> 0.57 million)
            if abs(num) >= 10000 and float(num).is_integer():
                multiplier = 1.0 / 1_000_000.0
            else:
                multiplier = 1.0

    return num * multiplier


def numeric_consistency_with_sources(primary_data: dict, web_context: dict) -> float:
    """Compare primary numbers vs source numbers"""
    primary_metrics = primary_data.get("primary_metrics", {})
    primary_numbers = []

    for metric in primary_metrics.values():
        if isinstance(metric, dict):
            val = metric.get("value")
            num = parse_number_with_unit(str(val))
            if num > 0:
                primary_numbers.append(num)

    if not primary_numbers:
        return 50.0  # Neutral when no metrics to compare

    # Extract source numbers with same parsing
    source_numbers = []
    search_results = web_context.get("search_results", [])

    for result in search_results:
        snippet = str(result.get("snippet", ""))
        # Match patterns like "$58.3B", "123M", "456 billion"
        patterns = [
            r'\$?(\d+(?:\.\d+)?)\s*([BbMmKk])',  # $58.3B
            r'(\d+(?:\.\d+)?)\s*(billion|million|thousand)',  # 58.3 billion
        ]

        for pattern in patterns:
            matches = re.findall(pattern, snippet, re.IGNORECASE)
            for num, unit in matches:
                source_numbers.append(parse_number_with_unit(f"{num}{unit[0].upper()}"))

    if not source_numbers:
        return 50.0  # Neutral when no source numbers found

    # Check agreement (within 25% tolerance)
    agreements = 0
    for p_num in primary_numbers:
        for s_num in source_numbers:
            if abs(p_num - s_num) / max(p_num, s_num, 1) < 0.25:
                agreements += 1
                break

    # Scale: 0 agreements = 30%, all agreements = 95%
    agreement_ratio = agreements / len(primary_numbers)
    agreement_pct = 30.0 + (agreement_ratio * 65.0)
    return min(agreement_pct, 95.0)

def numeric_consistency_with_sources_v2(primary_data: dict, web_context: dict) -> float:
    """
    Stable numeric consistency (0-100):
    - Evidence text: search_results snippets + web_context summary + scraped_content
    - Unit-aware parsing via parse_number_with_unit()
    - Range-aware (supports min/max if metric has a 'range' dict)
    - Downweights proxy metrics (is_proxy=True) so they don't tank the score
    """

    try:
        # Prefer canonical metrics if available (has is_proxy, range, etc.)
        metrics = primary_data.get("primary_metrics_canonical") or primary_data.get("primary_metrics") or {}
        if not isinstance(metrics, dict) or not metrics:
            return 50.0

        # -----------------------------
        # Build evidence text corpus
        # -----------------------------
        texts = []

        # 1) snippets
        sr = (web_context or {}).get("search_results") or []
        if isinstance(sr, list):
            for r in sr:
                if isinstance(r, dict):
                    snip = r.get("snippet", "")
                    if isinstance(snip, str) and snip.strip():
                        texts.append(snip)

        # 2) summary
        summary = (web_context or {}).get("summary") or ""
        if isinstance(summary, str) and summary.strip():
            texts.append(summary)

        # 3) scraped_content
        scraped = (web_context or {}).get("scraped_content") or {}
        if isinstance(scraped, dict):
            for _, content in scraped.items():
                if isinstance(content, str) and content.strip():
                    texts.append(content)

        evidence_text = "\n".join(texts)
        if not evidence_text.strip():
            return 45.0  # no evidence stored

        # -----------------------------
        # Extract numeric candidates from evidence text
        # -----------------------------
        # Keep this broad; parse_number_with_unit will normalize.
        patterns = [
            r'\$?\s?\d{1,3}(?:,\d{3})*(?:\.\d+)?\s*[BbMmKk]\b',                 # 29.8B, 570K, 1.2M
            r'\$?\s?\d+(?:\.\d+)?\s*(?:billion|million|thousand|bn|mn)\b',      # 29.8 billion, 29.8 bn
            r'\b\d{1,3}(?:,\d{3})+(?:\.\d+)?\b',                               # 570,000
            r'\b\d+(?:\.\d+)?\s*%\b',                                          # 9.8%
        ]

        evidence_numbers = []
        lowered = evidence_text.lower()

        for pat in patterns:
            for m in re.findall(pat, lowered, flags=re.IGNORECASE):
                n = parse_number_with_unit(str(m))
                if n and n > 0:
                    evidence_numbers.append(n)

        # If nothing extracted, don’t penalize too hard
        if not evidence_numbers:
            return 50.0

        # -----------------------------
        # Verify each metric against evidence numbers (tolerance match)
        # -----------------------------
        def _metric_candidates(m: dict) -> list:
            """Return list of candidate numeric values for a metric (range-aware)."""
            out = []
            if not isinstance(m, dict):
                return out

            # Range support: check min/max if present
            rng = m.get("range") if isinstance(m.get("range"), dict) else None
            if rng:
                if rng.get("min") is not None:
                    out.append(rng.get("min"))
                if rng.get("max") is not None:
                    out.append(rng.get("max"))

            # Also check main value
            if m.get("value") is not None:
                out.append(m.get("value"))

            return out

        def _parse_metric_num(val, unit_hint: str = "") -> float:
            # build a value+unit string so parse_number_with_unit has a chance
            if val is None:
                return 0.0
            s = str(val)
            if unit_hint and unit_hint.lower() not in s.lower():
                s = f"{s} {unit_hint}"
            return parse_number_with_unit(s)

        def _is_supported(target: float, evidence_nums: list, rel_tol: float = 0.25) -> bool:
            # same tolerance approach as v1 (25%)
            if not target or target <= 0:
                return False
            for e in evidence_nums:
                if e <= 0:
                    continue
                if abs(target - e) / max(target, e, 1) < rel_tol:
                    return True
            return False

        supported_w = 0.0
        total_w = 0.0

        for _, m in metrics.items():
            if not isinstance(m, dict):
                continue

            unit = str(m.get("unit") or "").strip()

            # proxy weighting
            is_proxy = bool(m.get("is_proxy"))
            w = 0.5 if is_proxy else 1.0

            cands = _metric_candidates(m)
            if not cands:
                continue

            # parse candidates into numeric values
            parsed_targets = []
            for c in cands:
                n = _parse_metric_num(c, unit_hint=unit)
                if n and n > 0:
                    parsed_targets.append(n)

            if not parsed_targets:
                continue

            total_w += w

            # supported if ANY candidate matches evidence
            if any(_is_supported(t, evidence_numbers, rel_tol=0.25) for t in parsed_targets):
                supported_w += w

        if total_w <= 0:
            return 50.0

        ratio = supported_w / total_w
        # Map: keep a soft floor so one miss doesn't tank the whole run
        score = 30.0 + (ratio * 65.0)  # same scale as v1 (30..95)
        return min(max(score, 20.0), 95.0)

    except Exception:
        return 45.0



def source_consensus(web_context: dict) -> float:
    """
    Calculate source consensus based on proportion of high-quality sources.
    Returns continuous score 0-100 based on quality distribution.
    """
    reliabilities = web_context.get("source_reliability", [])

    if not reliabilities:
        return 50.0  # Neutral when no sources

    total = len(reliabilities)
    high_count = sum(1 for r in reliabilities if "✅" in str(r))
    medium_count = sum(1 for r in reliabilities if "⚠️" in str(r))
    low_count = sum(1 for r in reliabilities if "❌" in str(r))

    # Weighted score: High=100, Medium=60, Low=30
    weighted_sum = (high_count * 100) + (medium_count * 60) + (low_count * 30)
    consensus_score = weighted_sum / total

    # Bonus for having multiple high-quality sources
    if high_count >= 3:
        consensus_score = min(100, consensus_score + 10)
    elif high_count >= 2:
        consensus_score = min(100, consensus_score + 5)

    return round(consensus_score, 1)

def evidence_based_veracity(primary_data: dict, web_context: dict) -> dict:
    """
    Evidence-driven veracity scoring.
    Returns breakdown of component scores and overall score (0-100).
    """
    breakdown = {}

    # 1. SOURCE QUALITY (35% weight)
    sources = primary_data.get("sources", [])
    src_score = source_quality_score(sources)
    breakdown["source_quality"] = round(src_score, 1)

    # 2. NUMERIC CONSISTENCY (30% weight)
    num_score = numeric_consistency_with_sources_v2(primary_data, web_context)
    breakdown["numeric_consistency"] = round(num_score, 1)

    # 3. CITATION DENSITY (20% weight)
    # FIXED: Higher score when sources support findings, not penalize detail
    sources_count = len(sources)
    findings_count = len(primary_data.get("key_findings", []))
    metrics_count = len(primary_data.get("primary_metrics", {}))

    # Total claims = findings + metrics
    total_claims = findings_count + metrics_count

    if total_claims == 0:
        citations_score = 40.0  # Low score for no claims
    else:
        # Ratio of sources to claims - ideal is ~0.5-1.0 sources per claim
        ratio = sources_count / total_claims
        if ratio >= 1.0:
            citations_score = 90.0  # Well-supported
        elif ratio >= 0.5:
            citations_score = 70.0 + (ratio - 0.5) * 40  # 70-90 range
        elif ratio >= 0.25:
            citations_score = 50.0 + (ratio - 0.25) * 80  # 50-70 range
        else:
            citations_score = ratio * 200  # 0-50 range

    breakdown["citation_density"] = round(min(citations_score, 95.0), 1)

    # 4. SOURCE CONSENSUS (15% weight)
    consensus_score = source_consensus(web_context)
    breakdown["source_consensus"] = round(consensus_score, 1)

    # Calculate weighted total
    total_score = (
        breakdown["source_quality"] * 0.35 +
        breakdown["numeric_consistency"] * 0.30 +
        breakdown["citation_density"] * 0.20 +
        breakdown["source_consensus"] * 0.15
    )

    breakdown["overall"] = round(total_score, 1)

    return breakdown

def calculate_final_confidence(
    base_conf: float,
    evidence_score: float
) -> float:
    """
    Calculate final confidence score.

    Formula balances model confidence with evidence quality:
    - Evidence has higher weight (65%) as it's more objective
    - Model confidence (35%) is adjusted by evidence quality

    This ensures:
    - High model + High evidence → High final (~85-90%)
    - High model + Low evidence → Medium final (~55-65%)
    - Low model + High evidence → Medium-High final (~70-80%)
    - Low model + Low evidence → Low final (~40-50%)
    """

    # Normalize inputs to 0-100 range
    base_conf = max(0, min(100, base_conf))
    evidence_score = max(0, min(100, evidence_score))

    # 1. EVIDENCE COMPONENT (65% weight) - Primary driver
    evidence_component = evidence_score * 0.65

    # 2. MODEL COMPONENT (35% weight) - Adjusted by evidence quality
    # When evidence is weak, model confidence is discounted
    evidence_multiplier = 0.5 + (evidence_score / 200)  # Range: 0.5 to 1.0
    model_component = base_conf * evidence_multiplier * 0.35

    final = evidence_component + model_component

    # Ensure result is in valid range
    return round(max(0, min(100, final)), 1)

# =========================================================
# 8A. DETERMINISTIC DIFF ENGINE
# Pure Python computation - no LLM variance
# =========================================================

@dataclass
class MetricDiff:
    """Single metric change record"""
    name: str
    old_value: Optional[float]
    new_value: Optional[float]
    old_raw: str  # Original string representation
    new_raw: str
    unit: str
    change_pct: Optional[float]
    change_type: str  # 'increased', 'decreased', 'unchanged', 'added', 'removed'

@dataclass
class EntityDiff:
    """Single entity ranking change record"""
    name: str
    old_rank: Optional[int]
    new_rank: Optional[int]
    old_share: Optional[str]
    new_share: Optional[str]
    rank_change: Optional[int]  # Positive = moved up
    change_type: str  # 'moved_up', 'moved_down', 'unchanged', 'added', 'removed'

@dataclass
class FindingDiff:
    """Single finding change record"""
    old_text: Optional[str]
    new_text: Optional[str]
    similarity: float  # 0-100
    change_type: str  # 'retained', 'modified', 'added', 'removed'

@dataclass
class EvolutionDiff:
    """Complete diff between two analyses"""
    old_timestamp: str
    new_timestamp: str
    time_delta_hours: Optional[float]
    metric_diffs: List[MetricDiff]
    entity_diffs: List[EntityDiff]
    finding_diffs: List[FindingDiff]
    stability_score: float  # 0-100
    summary_stats: Dict[str, int]

# =========================================================
# CANONICAL METRIC REGISTRY & SEMANTIC FINDING HASH
# Add this section after the dataclass definitions (around line 1587)
# =========================================================

# ------------------------------------
# CANONICAL METRIC REGISTRY
# Removes LLM control over metric identity
# ------------------------------------

# Metric type definitions with aliases
            # =========================
# PATCH MR1 (ADDITIVE): de-ambiguate "sales" so unit-sales doesn't map to Revenue
# - Remove standalone "sales" from Revenue aliases (too ambiguous)
# - Add money-explicit revenue phrases instead ("sales revenue", "sales value", etc.)
# - Add a couple of volume-style aliases under units_sold ("sales volume", "volume sales")
            # =========================

METRIC_REGISTRY = {
    # Market Size metrics
    "market_size": {
        "canonical_name": "Market Size",
        "aliases": [
            "market size", "market value", "market cap", "total market",
            "global market", "market valuation", "industry size",
            "total addressable market", "tam", "market worth"
        ],
        "unit_type": "currency",
        "category": "size"
    },
    "market_size_current": {
        "canonical_name": "Current Market Size",
        "aliases": [
            "2024 market size", "2025 market size", "current market",
            "present market size", "today market", "current year market",
            "market size 2024", "market size 2025"
        ],
        "unit_type": "currency",
        "category": "size"
    },
    "market_size_projected": {
        "canonical_name": "Projected Market Size",
        "aliases": [
            "projected market", "forecast market", "future market",
            "2026 market", "2027 market", "2028 market", "2029 market", "2030 market",
            "market projection", "expected market size", "estimated market"
        ],
        "unit_type": "currency",
        "category": "size"
    },

    # Growth metrics
    "cagr": {
        "canonical_name": "CAGR",
        "aliases": [
            "cagr", "compound annual growth", "compound growth rate",
            "annual growth rate", "growth rate", "yearly growth"
        ],
        "unit_type": "percentage",
        "category": "growth"
    },
    "yoy_growth": {
        "canonical_name": "YoY Growth",
        "aliases": [
            "yoy growth", "year over year", "year-over-year",
            "annual growth", "yearly growth rate", "growth percentage"
        ],
        "unit_type": "percentage",
        "category": "growth"
    },

    # Revenue metrics
    "revenue": {
        "canonical_name": "Revenue",
        "aliases": [
            "revenue",
            # =========================
            # PATCH MR1 (CHANGED): removed ambiguous standalone alias "sales"
            # =========================
            # "sales",
            # =========================
            "total revenue", "annual revenue",
            "yearly revenue", "gross revenue",

            # =========================
            # PATCH MR1 (ADDITIVE): money-explicit sales phrasing (revenue-like)
            # =========================
            "sales revenue",
            "revenue from sales",
            "sales value",
            "value of sales",
            "sales (value)",
            "turnover",  # common finance synonym
            # =========================
        ],
        "unit_type": "currency",
        "category": "financial"
    },

    # Market share
    "market_share": {
        "canonical_name": "Market Share",
        "aliases": [
            "market share", "share", "market portion", "market percentage",
            "share of market"
        ],
        "unit_type": "percentage",
        "category": "share"
    },

    # Volume metrics
    "units_sold": {
        "canonical_name": "Units Sold",
        "aliases": [
            "units sold", "unit sales", "volume", "units shipped",
            "shipments", "deliveries", "production volume",

            # =========================
            # PATCH MR1 (ADDITIVE): common unit-sales phrasing variants
            # =========================
            "sales volume",
            "volume sales",
            # =========================
        ],
        "unit_type": "count",
        "category": "volume"
    },

    # Pricing
    "average_price": {
        "canonical_name": "Average Price",
        "aliases": [
            "average price", "avg price", "mean price", "asp",
            "average selling price", "unit price"
        ],
        "unit_type": "currency",
        "category": "pricing"
    },

    # -------------------------
    # Country / Macro metrics
    # -------------------------
    "gdp": {
        "canonical_name": "GDP",
        "aliases": ["gdp", "gross domestic product", "economic output"],
        "unit_type": "currency",
        "category": "macro"
    },
    "gdp_per_capita": {
        "canonical_name": "GDP per Capita",
        "aliases": ["gdp per capita", "gdp/capita", "income per person", "per capita gdp"],
        "unit_type": "currency",
        "category": "macro"
    },
    "gdp_growth": {
        "canonical_name": "GDP Growth",
        "aliases": ["gdp growth", "economic growth", "growth rate of gdp", "real gdp growth"],
        "unit_type": "percentage",
        "category": "macro"
    },
    "population": {
        "canonical_name": "Population",
        "aliases": ["population", "population size", "number of people"],
        "unit_type": "count",
        "category": "macro"
    },
    "exports": {
        "canonical_name": "Exports",
        "aliases": ["exports", "export value", "total exports"],
        "unit_type": "currency",
        "category": "trade"
    },
    "imports": {
        "canonical_name": "Imports",
        "aliases": ["imports", "import value", "total imports"],
        "unit_type": "currency",
        "category": "trade"
    },
    "inflation": {
        "canonical_name": "Inflation",
        "aliases": ["inflation", "cpi", "consumer price index", "inflation rate"],
        "unit_type": "percentage",
        "category": "macro"
    },
    "interest_rate": {
        "canonical_name": "Interest Rate",
        "aliases": ["interest rate", "policy rate", "benchmark rate", "central bank rate"],
        "unit_type": "percentage",
        "category": "macro"
    }
}

            # =========================
# END PATCH MR1
            # =========================

# Year extraction pattern
YEAR_PATTERN = re.compile(r'(20\d{2})')

# ------------------------------------
# DETERMINISTIC QUESTION SIGNALS
# Drives metric table templates (no LLM)
# ------------------------------------

QUESTION_CATEGORY_TEMPLATES = {
    "country": [
        "gdp",
        "gdp_per_capita",
        "gdp_growth",
        "population",
        "exports",
        "imports",
        "inflation",
        "interest_rate",
    ],
    "industry": [
        "market_size_current",
        "market_size_projected",
        "cagr",
        "revenue",
        "market_share",
        "units_sold",
        "average_price",
    ],
}

def get_expected_metric_ids_for_category(category: str) -> List[str]:
    """
    Domain-agnostic mapping from a template/category string to expected metric IDs.

    Backward compatible:
      - accepts legacy categories like 'country', 'industry', 'company', 'generic'
      - also accepts template IDs like 'ENTITY_OVERVIEW_MARKET_LIGHT_V1', etc.

    NOTE:
    - This function returns a *default* set for a given template/category.
    - The profiler (classify_question_signals) can override/compose expected_metric_ids dynamically.
    """
    c_raw = (category or "unknown").strip()
    c = c_raw.lower().strip()

    # -------------------------
    # New generalized templates
    # -------------------------
    if c in {"entity_overview_country_light_v1", "entity_overview_country_v1"}:
        return [
            "population",
            "gdp_nominal",
            "gdp_per_capita",
            "gdp_growth",
            "inflation",
            "currency",
            "unemployment",
            "exports",
            "imports",
            "top_industries",
        ]

    if c in {"entity_overview_market_light_v1"}:
        return [
            "market_size_current",
            "market_size_projected",
            "cagr",
            "key_trends",
            "top_players",
        ]

    if c in {"entity_overview_market_heavy_v1"}:
        return [
            "market_size_current",
            "market_size_projected",
            "cagr",
            "key_trends",
            "top_players",
            "key_regions",
            "segments",
            "market_share",
            "revenue",
            "units_sold",
            "average_price",
        ]

    if c in {"entity_overview_company_light_v1", "entity_overview_company_v1"}:
        return [
            "revenue",
            "growth",
            "gross_margin",
            "operating_margin",
            "net_income",
            "market_cap",
            "valuation_multiple",
        ]

    if c in {"entity_overview_product_light_v1", "entity_overview_product_v1"}:
        return [
            "average_price",
            "units_sold",
            "market_share",
            "growth",
            "key_trends",
        ]

    if c in {"entity_overview_topic_v1", "generic_v1"}:
        return []

    # -------------------------
    # Legacy categories (still supported)
    # -------------------------
    if c == "country":
        return get_expected_metric_ids_for_category("ENTITY_OVERVIEW_COUNTRY_LIGHT_V1")

    if c == "industry":
        # legacy industry defaults to light market
        return get_expected_metric_ids_for_category("ENTITY_OVERVIEW_MARKET_LIGHT_V1")

    if c == "company":
        return get_expected_metric_ids_for_category("ENTITY_OVERVIEW_COMPANY_LIGHT_V1")

    if c == "generic":
        return []

    # fallback
    return []


def classify_question_signals(query: str) -> Dict[str, Any]:
    """
    Deterministically classify query and return:
      - category: high-level bucket used for templates (country | industry | company | generic)
      - expected_metric_ids: list[str]
      - signals: list[str] (debuggable reasons)
      - years: list[int]
      - regions: list[str]
      - intents: list[str] (market_size, growth_forecast, competitive_landscape, pricing, regulation, consumer_demand, supply_chain, investment, macro_outlook)
    """
    q_raw = (query or "").strip()
    q = q_raw.lower().strip()
    signals: List[str] = []

    if not q:
        return {
            "category": "generic",
            "expected_metric_ids": [],
            "signals": ["empty_query"],
            "years": [],
            "regions": [],
            "intents": []
        }

    # -------------------------
    # 1) Extract years (deterministic)
    # -------------------------
    years: List[int] = []
    try:
        year_matches = re.findall(r"\b(19|20)\d{2}\b", q_raw)
        # The regex above returns the first group; re-run with a non-capturing group to capture full year strings.
        year_matches_full = re.findall(r"\b(?:19|20)\d{2}\b", q_raw)
        years = sorted({int(y) for y in year_matches_full})
        if years:
            signals.append(f"years:{','.join(map(str, years[:8]))}")
    except Exception:
        pass
        years = []

    # -------------------------
    # 2) Extract regions/countries (best-effort deterministic; spaCy if available)
    # -------------------------
    regions: List[str] = []
    try:
        nlp = _try_spacy_nlp()
        if nlp:
            doc = nlp(q_raw)
            gpes = [ent.text.strip() for ent in getattr(doc, "ents", []) if ent.label_ in ("GPE", "LOC")]
            regions = []
            for g in gpes:
                if g and g.lower() not in [x.lower() for x in regions]:
                    regions.append(g)
            if regions:
                signals.append(f"regions_spacy:{','.join(regions[:6])}")
    except Exception:
        pass

    # Fallback: very lightweight region tokens
    if not regions:
        region_tokens = [
            "singapore", "malaysia", "indonesia", "thailand", "vietnam", "philippines",
            "china", "india", "japan", "korea", "australia",
            "usa", "united states", "europe", "uk", "united kingdom",
            "asean", "southeast asia", "sea", "global", "worldwide"
        ]
        hits = [t for t in region_tokens if t in q]
        if hits:
            # Keep original casing loosely (title-case single words)
            regions = [h.title() if " " not in h else h.upper() if h in ("usa", "uk") else h.title() for h in hits[:6]]
            signals.append(f"regions_kw:{','.join(hits[:6])}")

    # -------------------------
    # 3) Intent detection (domain-agnostic)
    # -------------------------
    intent_patterns: Dict[str, List[str]] = {
        "market_size": ["market size", "tam", "total addressable market", "how big", "size of the market", "market value"],
        "growth_forecast": ["cagr", "forecast", "projection", "by 20", "growth rate", "expected to", "outlook", "trend"],
        "competitive_landscape": ["key players", "competitors", "market share", "top companies", "leading players", "who are the players"],
        "pricing": ["pricing", "price", "asp", "average selling price", "cost", "margins"],
        "consumer_demand": ["demand", "users", "penetration", "adoption", "consumer", "customer", "behavior"],
        "supply_chain": ["supply", "capacity", "production", "manufacturing", "inventory", "shipment", "lead time"],
        "regulation": ["regulation", "policy", "law", "compliance", "tax", "tariff", "subsidy"],
        "investment": ["investment", "capex", "funding", "valuation", "roi", "profit", "ebitda"],
        "macro_outlook": ["gdp", "inflation", "interest rate", "policy rate", "exports", "imports", "currency", "exchange rate", "per capita"],
    }

    intents: List[str] = []
    for intent, pats in intent_patterns.items():
        if any(p in q for p in pats):
            intents.append(intent)

    # Small disambiguation: "by 2030" etc. strongly suggests forecast if years exist
    if years and "growth_forecast" not in intents and any(yr >= 2025 for yr in years):
        intents.append("growth_forecast")

    if intents:
        signals.append(f"intents:{','.join(intents[:10])}")

    # -------------------------
    # 4) Category decision (template driver)
    # -------------------------
    # Keep it coarse: country vs industry vs company vs generic
    country_kw = [
        "gdp", "per capita", "population", "exports", "imports",
        "inflation", "cpi", "interest rate", "policy rate", "central bank",
        "currency", "exchange rate"
    ]
    company_kw = ["revenue", "earnings", "profit", "ebitda", "guidance", "quarter", "fy", "10-k", "10q", "balance sheet"]
    industry_kw = [
        "market", "industry", "sector", "tam", "cagr", "market size", "market share",
        "key players", "competitors", "pricing", "forecast", "outlook"
    ]

    country_hits = [k for k in country_kw if k in q]
    company_hits = [k for k in company_kw if k in q]
    industry_hits = [k for k in industry_kw if k in q]

    # If macro intent is present, strongly bias to country
    if "macro_outlook" in intents and (regions or country_hits):
        category = "country"
        signals.append("category_rule:macro_outlook_bias_country")
    elif company_hits and not industry_hits:
        category = "company"
        signals.append(f"category_rule:company_keywords:{','.join(company_hits[:5])}")
    elif industry_hits and not country_hits:
        category = "industry"
        signals.append(f"category_rule:industry_keywords:{','.join(industry_hits[:5])}")
    elif industry_hits and country_hits:
        # tie-break: if market sizing/competitive signals exist -> industry; if macro_outlook -> country
        if "macro_outlook" in intents:
            category = "country"
            signals.append("category_rule:mixed_signals_macro_wins")
        else:
            category = "industry"
            signals.append("category_rule:mixed_signals_default_to_industry")
    else:
        category = "generic"
        signals.append("category_rule:no_template_keywords")

    # -------------------------
    # 5) Expected metric IDs (category + intent)
    # -------------------------
    expected_metric_ids: List[str] = []
    try:
        expected_metric_ids = get_expected_metric_ids_for_category(category) or []
    except Exception:
        pass
        expected_metric_ids = []

    # Add a few intent-driven metric IDs (only if your registry supports them)
    intent_metric_suggestions = {
        "market_size": ["market_size", "market_size_2024", "market_size_2025"],
        "growth_forecast": ["cagr", "forecast_period", "market_size_2030"],
        "competitive_landscape": ["market_share", "top_players"],
        "pricing": ["avg_price", "asp"],
        "consumer_demand": ["users", "penetration", "arpu"],
        "supply_chain": ["capacity", "shipments"],
        "investment": ["capex", "profit", "ebitda"],
        "macro_outlook": ["gdp", "inflation", "interest_rate", "exchange_rate"],
    }

    for intent in intents:
        for mid in intent_metric_suggestions.get(intent, []):
            if mid not in expected_metric_ids:
                expected_metric_ids.append(mid)

    return {
        "category": category,
        "expected_metric_ids": expected_metric_ids,
        "signals": signals,
        "years": years,
        "regions": regions,
        "intents": intents
    }


    def _contains_any(needle_list: List[str]) -> bool:
        return any(k in q for k in needle_list)

    # -------------------------
    # Determine intents
    # -------------------------
    intents: List[str] = []
    for intent, kws in intent_triggers.items():
        if _contains_any(kws):
            intents.append(intent)

    if intents:
        signals.append("intents:" + ",".join(sorted(set(intents))))

    # -------------------------
    # Determine entity_kind (best-effort heuristic)
    # -------------------------
    is_marketish = _contains_any(market_entity_kw) or any(i in intents for i in ["size", "growth", "forecast", "share", "segments", "players", "regions"])
    is_companyish = _contains_any(company_entity_kw) and not _contains_any(country_entity_kw)
    is_countryish = _contains_any(country_entity_kw) and not is_companyish
    is_productish = _contains_any(product_entity_kw) and not (is_marketish or is_countryish or is_companyish)

    if is_countryish:
        entity_kind = "country"
        signals.append("entity_kind:country")
    elif is_companyish:
        entity_kind = "company"
        signals.append("entity_kind:company")
    elif is_productish:
        entity_kind = "product"
        signals.append("entity_kind:product")
    elif is_marketish:
        entity_kind = "market"
        signals.append("entity_kind:market")
    else:
        entity_kind = "topic_general"
        signals.append("entity_kind:topic_general")

    # -------------------------
    # Determine scope
    # -------------------------
    is_comparative = _contains_any(comparative_kw)
    is_forecasty = _contains_any(forecast_kw) or bool(YEAR_PATTERN.findall(q_raw))

    # Broad overview should win when user explicitly asks for general explainer
    # BUT: if they also mention measurable intents (size/growth/forecast/etc.), treat as metrics_light.
    is_broad_phrase = _contains_any(broad_phrases)

    if is_comparative:
        scope = "comparative"
        signals.append("scope:comparative")
    elif is_forecasty and any(i in intents for i in ["forecast", "growth", "size"]):
        scope = "forecast_specific"
        signals.append("scope:forecast_specific")
    elif is_broad_phrase and not intents:
        scope = "broad_overview"
        signals.append("scope:broad_overview")
    else:
        # metrics light vs heavy
        heavy_asks = ["segments", "share", "volume", "regions", "players"]
        heavy_requested = any(i in intents for i in heavy_asks)
        if heavy_requested:
            scope = "metrics_heavy"
            signals.append("scope:metrics_heavy")
        else:
            scope = "metrics_light"
            signals.append("scope:metrics_light")

    # -------------------------
    # Map entity_kind -> category (backward compatible)
    # -------------------------
    if entity_kind == "country":
        category = "country"
    elif entity_kind == "company":
        category = "company"
    elif entity_kind in {"market", "product"}:
        category = "industry"
    else:
        category = "generic"

    # -------------------------
    # Choose generalized template + tiers
    # -------------------------
    # Tier meanings:
    #  1 = high extractability (size/growth/forecast)
    #  2 = medium (players/regions/basic segments)
    #  3 = low (granular channels, detailed splits) -> only if explicitly asked
    if category == "country":
        metric_template_id = "ENTITY_OVERVIEW_COUNTRY_LIGHT_V1" if scope != "metrics_heavy" else "ENTITY_OVERVIEW_COUNTRY_LIGHT_V1"
        metric_tiers_enabled = [1]
    elif category == "company":
        metric_template_id = "ENTITY_OVERVIEW_COMPANY_LIGHT_V1"
        metric_tiers_enabled = [1]
    elif category == "industry":
        if scope in {"metrics_heavy", "comparative"}:
            metric_template_id = "ENTITY_OVERVIEW_MARKET_HEAVY_V1"
            metric_tiers_enabled = [1, 2]
        else:
            metric_template_id = "ENTITY_OVERVIEW_MARKET_LIGHT_V1"
            metric_tiers_enabled = [1]
    else:
        metric_template_id = "ENTITY_OVERVIEW_TOPIC_V1"
        metric_tiers_enabled = []

    # -------------------------
    # Build expected_metric_ids dynamically from intents (domain-agnostic)
    # -------------------------
    # Slot -> metric id mapping (kept generic; avoids tourism specialization)
    # If you later add more canonical IDs, expand these mappings.
    market_slot_to_id = {
        "size_current": "market_size_current",
        "size_projected": "market_size_projected",
        "growth_cagr": "cagr",
        "growth_yoy": "growth",
        "share_key": "market_share",
        "volume_current": "units_sold",
        "price_avg": "average_price",
        "players_top": "top_players",
        "regions_key": "key_regions",
        "segments_basic": "segments",
        "trends": "key_trends",
        "revenue": "revenue",
    }

    company_slot_to_id = {
        "revenue": "revenue",
        "growth": "growth",
        "gross_margin": "gross_margin",
        "operating_margin": "operating_margin",
        "net_income": "net_income",
        "market_cap": "market_cap",
        "valuation_multiple": "valuation_multiple",
        "trends": "key_trends",
    }

    country_slot_to_id = {
        "population": "population",
        "gdp_nominal": "gdp_nominal",
        "gdp_per_capita": "gdp_per_capita",
        "gdp_growth": "gdp_growth",
        "inflation": "inflation",
        "currency": "currency",
        "unemployment": "unemployment",
        "exports": "exports",
        "imports": "imports",
        "top_industries": "top_industries",
        "trends": "key_trends",
    }

    # Determine slots from intents
    slots: List[str] = []
    if entity_kind == "country":
        # For countries: macro defaults if broad, otherwise macro intents
        if scope == "broad_overview":
            slots = ["population", "gdp_nominal", "gdp_per_capita", "gdp_growth", "inflation", "currency", "top_industries"]
        else:
            # If user asks for macro (or didn’t specify), still give a tight macro set
            slots = ["population", "gdp_nominal", "gdp_growth", "inflation", "currency"]
            if "macro" in intents:
                slots += ["unemployment", "exports", "imports"]

        mapper = country_slot_to_id

    elif entity_kind == "company":
        slots = ["revenue", "growth", "gross_margin", "operating_margin", "net_income", "market_cap", "valuation_multiple"]
        mapper = company_slot_to_id

    elif entity_kind in {"market", "product"}:
        # Tier 1 core (always when metrics_* scope)
        if scope == "broad_overview":
            slots = ["trends", "players_top"]
        else:
            slots = ["size_current", "growth_cagr"]
            if "forecast" in intents:
                slots.append("size_projected")
            if "trends" in intents:
                slots.append("trends")
            # Tier 2 (only when explicitly asked or heavy scope)
            if scope in {"metrics_heavy", "comparative"}:
                if "players" in intents:
                    slots.append("players_top")
                if "regions" in intents:
                    slots.append("regions_key")
                if "segments" in intents:
                    slots.append("segments_basic")
                if "share" in intents:
                    slots.append("share_key")
                if "volume" in intents:
                    slots.append("volume_current")
                if "price" in intents:
                    slots.append("price_avg")
            else:
                # metrics_light: include players/trends only if asked
                if "players" in intents:
                    slots.append("players_top")
                if "regions" in intents:
                    slots.append("regions_key")

        mapper = market_slot_to_id

    else:
        # topic_general
        slots = []
        mapper = {}

    expected_metric_ids = []
    for s in slots:
        mid = mapper.get(s)
        if mid:
            expected_metric_ids.append(mid)

    # If still empty but template provides defaults, use template defaults
    if not expected_metric_ids:
        expected_metric_ids = get_expected_metric_ids_for_category(metric_template_id)

    # De-dup while preserving order
    seen = set()
    expected_metric_ids = [x for x in expected_metric_ids if not (x in seen or seen.add(x))]

    # -------------------------
    # Preferred source classes (generic)
    # -------------------------
    if category == "country":
        preferred_source_classes = ["official_stats", "government", "reputable_org", "reference"]
    elif category == "company":
        preferred_source_classes = ["official_filings", "investor_relations", "reputable_org", "news"]
    elif category == "industry":
        preferred_source_classes = ["industry_association", "reputable_org", "official_stats", "news", "research_portal"]
    else:
        preferred_source_classes = ["reference", "official_stats", "reputable_org"]

    # Attach year detection signal
    years = sorted(set(YEAR_PATTERN.findall(q_raw))) if YEAR_PATTERN.findall(q_raw) else []
    if years:
        signals.append("years_detected:" + ",".join(years))

    return {
        "category": category,
        "expected_metric_ids": expected_metric_ids,
        "signals": signals,
        "entity_kind": entity_kind,
        "scope": scope,
        "metric_template_id": metric_template_id,
        "metric_tiers_enabled": metric_tiers_enabled,
        "preferred_source_classes": preferred_source_classes,
        "intents": sorted(set(intents)),
    }


def get_canonical_metric_id(metric_name: str) -> Tuple[str, str]:
    """
    Map a metric name to its canonical ID and display name.

    Returns:
        Tuple of (canonical_id, canonical_display_name)

    Example:
        "2024 Market Size" -> ("market_size_2024", "Market Size (2024)")
        "Global Market Value" -> ("market_size", "Market Size")
        "CAGR 2024-2030" -> ("cagr_2024_2030", "CAGR (2024-2030)")
    """
    import re

    if not metric_name:
        return ("unknown", "Unknown Metric")

    name_lower = metric_name.lower().strip()
    name_normalized = re.sub(r"[^\w\s]", " ", name_lower)
    name_normalized = re.sub(r"\s+", " ", name_normalized).strip()

    # Extract years
    years = YEAR_PATTERN.findall(metric_name)
    year_suffix = "_".join(sorted(years)) if years else ""

    # =========================
    # PATCH CM1 (ADDITIVE): intent signals to prevent "sales" -> "revenue" mis-maps
    # =========================
    name_words = set(name_normalized.split())

    # Explicit money intent (strong)
    money_tokens = {
        "revenue", "turnover", "valuation", "valued", "value", "market", "capex", "opex",
        "profit", "earnings", "ebitda", "income",
        "usd", "sgd", "eur", "gbp", "aud", "cad", "jpy", "cny", "rmb"
    }
    # Currency symbols appear in raw text sometimes
    has_currency_symbol = any(sym in metric_name for sym in ["$", "€", "£", "S$"])

    has_money_intent = bool(name_words & money_tokens) or has_currency_symbol

    # Explicit unit/count intent (strong)
    unit_tokens = {
        "unit", "units", "deliveries", "shipments", "registrations", "vehicles",
        "sold", "salesvolume", "volume", "pcs", "pieces"
    }
    # normalize joined token cases like "sales volume"
    joined = name_normalized.replace(" ", "")
    has_unit_intent = bool(name_words & unit_tokens) or any(t in joined for t in ["salesvolume", "unitsold", "vehiclesold"])
    # =========================

    # Find best matching registry entry
    best_match_id = None
    best_match_score = 0.0

    # =========================
    # PATCH CM2 (ADDITIVE): helper to identify revenue-like registry targets
    # =========================
    def _is_revenue_like(metric_id: str, config: dict) -> bool:
        mid = (metric_id or "").lower()
        cname = str((config or {}).get("canonical_name") or "").lower()
        # treat "market value" / "valuation" as currency-like too
        if any(k in cname for k in ["revenue", "market value", "valuation", "market size", "turnover"]):
            return True
        if any(k in mid for k in ["revenue", "market_value", "market_size", "valuation"]):
            return True
        return False
    # =========================

    for metric_id, config in METRIC_REGISTRY.items():
        for alias in config["aliases"]:
            # Remove years from alias for comparison
            alias_no_year = YEAR_PATTERN.sub("", alias).strip().lower()
            alias_no_year = re.sub(r"[^\w\s]", " ", alias_no_year)
            alias_no_year = re.sub(r"\s+", " ", alias_no_year).strip()

            name_no_year = YEAR_PATTERN.sub("", name_normalized).strip()

            # ---- base score from your existing logic ----
            score = 0.0

            # Exact match
            if alias_no_year == name_no_year and alias_no_year:
                score = 1.0

            # Containment match
            elif alias_no_year and (alias_no_year in name_no_year or name_no_year in alias_no_year):
                score = len(alias_no_year) / max(len(name_no_year), 1)

            # Word overlap match
            else:
                alias_words = set(alias_no_year.split())
                name_words_local = set(name_no_year.split())
                if alias_words and name_words_local:
                    overlap = len(alias_words & name_words_local) / len(alias_words | name_words_local)
                    score = max(score, overlap)

            # =========================
            # PATCH CM3 (ADDITIVE): disambiguation penalties/guards
            # - Block "sales" -> revenue when no money intent
            # - Block unit-intent -> revenue-like
            # - Require explicit money intent for revenue-like (soft guard, not hard stop)
            # =========================
            if score > 0.0:
                revenue_like = _is_revenue_like(metric_id, config)

                # If target is revenue-like but name has strong unit intent, penalize heavily
                if revenue_like and has_unit_intent and not has_money_intent:
                    score *= 0.20  # strong downweight

                # If target is revenue-like but name has NO money intent at all, penalize
                if revenue_like and not has_money_intent:
                    score *= 0.55  # moderate downweight

                # If name includes the word "sales" but no money intent, avoid mapping to revenue-like
                if revenue_like and ("sales" in name_no_year.split()) and not has_money_intent:
                    score *= 0.60

                # Conversely: if target is NOT revenue-like but name has money intent, slight penalty
                if (not revenue_like) and has_money_intent and ("sales" in name_no_year.split()) and not has_unit_intent:
                    score *= 0.85
            # =========================

            if score > best_match_score:
                best_match_id = metric_id
                best_match_score = score

            if best_match_score == 1.0:
                break

        if best_match_score == 1.0:
            break

    # Build canonical ID and display name
    if best_match_id and best_match_score > 0.4:
        config = METRIC_REGISTRY[best_match_id]
        canonical_base = best_match_id
        display_name = config["canonical_name"]

        if year_suffix:
            canonical_id = f"{canonical_base}_{year_suffix}"
            if len(years) == 1:
                display_name = f"{display_name} ({years[0]})"
            else:
                display_name = f"{display_name} ({'-'.join(years)})"
        else:
            canonical_id = canonical_base

        return (canonical_id, display_name)

    # Fallback: create ID from normalized name
    fallback_id = re.sub(r"\s+", "_", name_normalized)
    if year_suffix:
        fallback_id = f"{fallback_id}_{year_suffix}" if year_suffix not in fallback_id else fallback_id

    return (fallback_id, metric_name)

# ------------------------------------
# GEO + PROXY TAGGING (DETERMINISTIC)
# ------------------------------------

import re
from typing import Dict, Any, Tuple, List, Optional

REGION_KEYWORDS = {
    "APAC": ["apac", "asia pacific", "asia-pacific"],
    "SOUTHEAST_ASIA": ["southeast asia", "asean", "sea "],  # note space to reduce false matches
    "ASIA": ["asia"],
    "EUROPE": ["europe", "eu", "emea"],
    "NORTH_AMERICA": ["north america"],
    "LATAM": ["latin america", "latam"],
    "MIDDLE_EAST": ["middle east", "mena"],
    "AFRICA": ["africa"],
    "OCEANIA": ["oceania", "australia", "new zealand"],
}

GLOBAL_KEYWORDS = ["global", "worldwide", "world", "international", "across the world"]

# Minimal country map (expand deterministically over time)
COUNTRY_KEYWORDS = {
    "Singapore": ["singapore", "sg"],
    "United States": ["united states", "usa", "u.s.", "us"],
    "United Kingdom": ["united kingdom", "uk", "u.k.", "britain", "england"],
    "China": ["china", "prc"],
    "Japan": ["japan"],
    "India": ["india"],
    "Indonesia": ["indonesia"],
    "Malaysia": ["malaysia"],
    "Thailand": ["thailand"],
    "Vietnam": ["vietnam"],
    "Philippines": ["philippines"],
}

def infer_geo_scope(*texts: str) -> Dict[str, str]:
    """
    Deterministically infer geography from text.
    Returns {"geo_scope": "local|regional|global|unknown", "geo_name": "<name or ''>"}.
    Priority: country > region > global.
    """
    combined = " ".join([t for t in texts if isinstance(t, str) and t.strip()]).lower()
    if not combined:
        return {"geo_scope": "unknown", "geo_name": ""}

    # 1) Country/local (most specific)
    for country, kws in COUNTRY_KEYWORDS.items():
        for kw in kws:
            if kw in combined:
                return {"geo_scope": "local", "geo_name": country}

    # 2) Region
    for region_name, kws in REGION_KEYWORDS.items():
        for kw in kws:
            if kw in combined:
                pretty = region_name.replace("_", " ").title()
                return {"geo_scope": "regional", "geo_name": pretty}

    # 3) Global
    for kw in GLOBAL_KEYWORDS:
        if kw in combined:
            return {"geo_scope": "global", "geo_name": "Global"}

    return {"geo_scope": "unknown", "geo_name": ""}


# ---- Proxy labeling ----
# "Proxy" = adjacent metric that can help approximate the target but isn't the target definition.
# You can expand these sets deterministically.

PROXY_PATTERNS = [
    # (pattern, proxy_type, reason_template)
    (r"\bapparel\b|\bfashion\b|\bclothing\b", "adjacent_category", "Uses apparel/fashion as an adjacent proxy for streetwear."),
    (r"\bfootwear\b|\bsneaker\b|\bshoes\b", "subsegment", "Uses footwear/sneakers as a subsegment proxy for the broader market."),
    (r"\bresale\b|\bsecondary market\b", "channel_proxy", "Uses resale/secondary-market measures as a channel proxy."),
    (r"\be-?commerce\b|\bonline sales\b|\bsocial commerce\b", "channel_proxy", "Uses e-commerce indicators as a channel proxy."),
    (r"\btourism\b|\bvisitor\b|\btravel retail\b", "demand_driver", "Uses tourism indicators as a demand-driver proxy."),
    (r"\bsearch interest\b|\bgoogle trends\b|\bweb traffic\b", "interest_proxy", "Uses interest/attention measures as a proxy."),
]

# These are words that signal "core market size" style metrics (usually non-proxy if they match the user topic).
CORE_MARKET_PATTERNS = [
    r"\bmarket size\b",
    r"\bmarket value\b",
    r"\brevenue\b",
    r"\bsales\b",
    r"\bcagr\b",
    r"\bgrowth\b",
    r"\bprojected\b|\bforecast\b",
]

def infer_proxy_label(
    metric_name: str,
    question_text: str = "",
    category_hint: str = "",
    *extra_context: str
) -> Dict[str, Any]:
    """
    Deterministically label a metric as proxy/non-proxy.

    Returns fields:
      is_proxy: bool
      proxy_type: str
      proxy_reason: str
      proxy_confidence: float (0-1)
      proxy_target: str (best-guess target topic)
    """
    name = (metric_name or "").lower().strip()
    q = (question_text or "").lower().strip()
    ctx = " ".join([c for c in extra_context if isinstance(c, str)]).lower()

    combined = " ".join([name, q, ctx]).strip()

    # Default: not proxy
    out = {
        "is_proxy": False,
        "proxy_type": "",
        "proxy_reason": "",
        "proxy_confidence": 0.0,
        "proxy_target": ""
    }

    if not combined:
        return out

    # Best-effort target topic extraction (very light heuristic)
    # If you already have question signals elsewhere, you can pass them in category_hint/question_text.
    # Here we just keep a short phrase if present.
    proxy_target = ""
    if "streetwear" in q:
        proxy_target = "streetwear"
    elif "semiconductor" in q:
        proxy_target = "semiconductors"
    elif "battery" in q:
        proxy_target = "batteries"
    out["proxy_target"] = proxy_target

    # If metric name itself looks like core market patterns AND includes the target keyword, treat as non-proxy.
    # (prevents incorrectly labeling "Singapore streetwear market size" as proxy)
    core_like = any(re.search(p, name) for p in CORE_MARKET_PATTERNS)
    if core_like:
        # If it explicitly contains the topic keyword, strongly non-proxy
        if proxy_target and proxy_target in name:
            return out
        # If it says "streetwear market" in name, non-proxy even if target not detected
        if "streetwear" in name:
            return out

    # Detect proxies using patterns.
    for pat, ptype, reason in PROXY_PATTERNS:
        if re.search(pat, combined):
            out["is_proxy"] = True
            out["proxy_type"] = ptype
            out["proxy_reason"] = reason
            # Confidence: stronger if pattern appears in metric name; weaker if only in context.
            if re.search(pat, name):
                out["proxy_confidence"] = 0.9
            elif re.search(pat, ctx):
                out["proxy_confidence"] = 0.7
            else:
                out["proxy_confidence"] = 0.6
            return out

    return out


def merge_group_geo(group: List[Dict[str, Any]]) -> Tuple[str, str]:
    """
    Choose the most frequent geo tag within a merged group deterministically.
    Returns (geo_scope, geo_name).
    """
    items = []
    for g in group:
        s = g.get("geo_scope", "unknown")
        n = g.get("geo_name", "")
        if s and s != "unknown":
            items.append((s, n))

    if not items:
        return "unknown", ""

    counts: Dict[str, int] = {}
    for s, n in items:
        k = f"{s}|{n}"
        counts[k] = counts.get(k, 0) + 1

    best_k = max(counts.items(), key=lambda kv: kv[1])[0]  # deterministic tie via insertion order after stable sort
    s, n = best_k.split("|", 1)
    return s, n


def merge_group_proxy(group: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Merge proxy labels for duplicates deterministically.
    If ANY member is proxy -> merged metric is proxy.
    Choose the highest-confidence proxy candidate.
    """
    best = None
    best_conf = -1.0

    for g in group:
        is_proxy = bool(g.get("is_proxy", False))
        conf = float(g.get("proxy_confidence", 0.0) or 0.0)
        if is_proxy and conf > best_conf:
            best_conf = conf
            best = g

    if best is None:
        return {
            "is_proxy": False,
            "proxy_type": "",
            "proxy_reason": "",
            "proxy_confidence": 0.0,
            "proxy_target": "",
        }

    return {
        "is_proxy": True,
        "proxy_type": best.get("proxy_type", ""),
        "proxy_reason": best.get("proxy_reason", ""),
        "proxy_confidence": float(best.get("proxy_confidence", 0.0) or 0.0),
        "proxy_target": best.get("proxy_target", ""),
    }

# =========================
# FIX2D59 — Canonical Identity Resolver v1
#
# Exact identity tuple definition (v1):
#   IdentityTupleV1 := {
#       'metric_token':   str,  # schema concept token / canonical_id (concept-level)
#       'time_scope':     str,  # normalized time token (e.g. '2024', 'ytd_2025') if known
#       'geo_scope':      str,  # normalized geo token (e.g. 'global', 'us') if known
#       'dims':           tuple[str,...], # normalized dimension-value tokens (segment/category/channel) if known
#       'dimension':      str,  # 'currency'|'unit_sales'|'percent'|'count'|'index'|'unknown'
#       'unit_family':    str,  # 'currency'|'percent'|'magnitude'|'energy'|'index'|''
#       'unit_tag':       str,  # 'USD'|'%'|'M'|'GWh' etc (normalized)
#       'statistic':      str,  # e.g. 'level'|'yoy_pct'|'cagr'|'share' (optional)
#       'aggregation':    str,  # e.g. 'total'|'avg' (optional)
#   }
#
# Resolver contract:
#   resolve_canonical_identity_v1(identity, metric_schema) -> {
#       'canonical_key': str,          # schema canonical_key if bound, else provisional key
#       'bound': bool,                 # True iff schema-bound
#       'status': str,                 # 'SCHEMA_BOUND' | 'PROVISIONAL'
#       'matched_schema_key': str|''   # the schema key chosen, if any
#   }
#
# Rules:
#   1) Schema-first: if metric_schema contains a canonical_key that matches the identity tuple, return it.
#   2) No silent canonical minting: if identity is under-specified (dimension unknown, or unit_family missing when unit_tag present), return PROVISIONAL.
#   3) Deterministic: matching and tie-breaks must be stable across runs.
#
# Note:
#   This resolver is intended to be used by BOTH Analysis and Evolution finalizers.
# =========================

def normalize_metric_token_time_scope_v1(metric_token: str, time_scope: str = '') -> tuple:
    """Split embedded time tokens out of metric_token into time_scope (v1).

    Rules (deterministic):
      - Leading year prefix: '2024_global_ev_sales' -> metric_token='global_ev_sales', time_scope='2024'
      - Trailing year suffix: 'global_ev_sales_2024' -> metric_token='global_ev_sales', time_scope='2024'
      - YTD forms: 'global_ev_sales_ytd_2025' -> metric_token='global_ev_sales', time_scope='ytd_2025'
      - Forecast/projected forms: 'forecast_2035_sales' -> metric_token='sales', time_scope='forecast_2035'

    If time_scope is already provided, it is preserved.
    """
    mt = str(metric_token or '').strip().lower()
    ts = str(time_scope or '').strip().lower()
    if not mt:
        return '', ts
    if ts:
        return re.sub(r'_+', '_', mt).strip('_'), ts

    # ytd patterns
    m = re.search(r'(?:^|_)ytd[_-]?(20\d{2})(?:$|_)', mt)
    if m:
        year = m.group(1)
        ts = f'ytd_{year}'
        mt = re.sub(r'(?:^|_)ytd[_-]?%s(?:$|_)' % re.escape(year), '_', mt)

    # forecast/projected patterns (treat as forecast)
    m = re.search(r'(?:^|_)(forecast|projected|projection|estimate|estimated|target)[_-]?(20\d{2})(?:$|_)', mt)
    if m:
        year = m.group(2)
        ts = f'forecast_{year}'
        mt = re.sub(r'(?:^|_)(forecast|projected|projection|estimate|estimated|target)[_-]?%s(?:$|_)' % re.escape(year), '_', mt)

    # leading year
    m = re.match(r'^(20\d{2})_(.+)$', mt)
    if m and not ts:
        ts = m.group(1)
        mt = m.group(2)

    # trailing year
    m = re.match(r'^(.+?)_(20\d{2})$', mt)
    if m and not ts:
        mt = m.group(1)
        ts = m.group(2)

    mt = re.sub(r'_+', '_', mt).strip('_')
    ts = re.sub(r'_+', '_', ts).strip('_')
    return mt, ts

def build_identity_tuple_v1(*, metric_token: str, time_scope: str = '', geo_scope: str = '', dims=None,
                            dimension: str = '', unit_family: str = '', unit_tag: str = '',
                            statistic: str = '', aggregation: str = '') -> dict:
    'Construct a deterministic identity tuple (v1).'
    if dims is None:
        dims = ()
    if not isinstance(dims, (list, tuple)):
        dims = (str(dims),)
    metric_token, time_scope = normalize_metric_token_time_scope_v1(metric_token, time_scope)
    return {
        'metric_token': str(metric_token or '').strip().lower(),
        'time_scope': str(time_scope or '').strip().lower(),
        'geo_scope': str(geo_scope or '').strip().lower(),
        'dims': tuple([str(x or '').strip().lower() for x in list(dims) if str(x or '').strip()]),
        'dimension': str(dimension or '').strip().lower(),
        'unit_family': str(unit_family or '').strip().lower(),
        'unit_tag': str(unit_tag or '').strip(),
        'statistic': str(statistic or '').strip().lower(),
        'aggregation': str(aggregation or '').strip().lower(),
    }






def canonicalize_metrics(
    metrics: Dict,
    metric_schema: Dict = None,
    merge_duplicates_to_range: bool = True,
    question_text: str = "",
    category_hint: str = ""
) -> Dict:
    """
    Convert metrics to canonical IDs, but NEVER merge across incompatible dimensions.

    Key fix:
      - Adds deterministic 'dimension' classification and incorporates it into canonical keys.
      - Prevents revenue vs unit-sales from merging just because the year matches.
      - Keeps your geo + proxy tagging behavior.

    Output:
      canonicalized[canonical_key] -> metric dict with:
        - canonical_id (base id)
        - canonical_key (dimension-safe id you should use everywhere downstream)
        - dimension (currency | unit_sales | percent | count | index | unknown)
        - name (dimension-corrected display name)
    """
    import re  # ========================= PATCH C0 (ADDITIVE): missing import =========================

    if not isinstance(metrics, dict):
        return {}

    # =========================
    # PATCH C1 (ADDITIVE): safe helpers for canonical numeric fields
    # - Prefer existing normalize_unit_tag/unit_family/canonicalize_numeric_candidate if present.
    # - Never breaks if those helpers are missing.
    # =========================
    def _safe_normalize_unit_tag(u: str) -> str:
        try:
            fn = globals().get("normalize_unit_tag")
            if callable(fn):
                return fn(u or "")
        except Exception:
            pass
        # minimal fallback (kept conservative)
        uu = (u or "").strip()
        ul = uu.lower().replace(" ", "")
        # PATCH FIX2D58B (ADDITIVE): handle composite phrases like 'million units'
        # - Legacy extracted units often arrive as phrases (e.g., 'million units', 'billion USD').
        # - We deterministically map magnitude words even when other tokens are present.
        if 'trillion' in ul or ul.endswith('tn') or ' tn' in (uu.lower()):
            return 'T'
        if 'billion' in ul or ul.endswith('bn') or ' bn' in (uu.lower()):
            return 'B'
        if 'million' in ul or ul.endswith('mn') or ' mn' in (uu.lower()) or 'mio' in ul:
            return 'M'
        if 'thousand' in ul or ul.endswith('k') or ' k' in (uu.lower()):
            return 'K'
        if ul in ("%", "pct", "percent"):
            return "%"
        if ul in ("twh",):
            return "TWh"
        if ul in ("gwh",):
            return "GWh"
        if ul in ("mwh",):
            return "MWh"
        if ul in ("kwh",):
            return "kWh"
        if ul in ("wh",):
            return "Wh"
        if ul in ("t", "trillion", "tn"):
            return "T"
        if ul in ("b", "bn", "billion"):
            return "B"
        if ul in ("m", "mn", "mio", "million"):
            return "M"
        if ul in ("k", "thousand", "000"):
            return "K"
        return uu

    def _safe_unit_family(unit_tag: str) -> str:
        try:
            fn = globals().get("unit_family")
            if callable(fn):
                return fn(unit_tag or "")
        except Exception:
            pass
        ut = (unit_tag or "").strip()
        if ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
            return "energy"
        if ut == "%":
            return "percent"
        if ut in ("T", "B", "M", "K"):
            return "magnitude"
        # currency not reliably derived here (handled elsewhere)
        return ""
    # =========================

    def infer_metric_dimension(metric_name: str, unit_raw: str) -> str:
        n = (metric_name or "").lower()
        u = (unit_raw or "").strip().lower()

        # Percent
        if "%" in u or "percent" in n or "share" in n or "cagr" in n:
            return "percent"

        # Currency signals
        currency_tokens = ["$", "s$", "usd", "sgd", "eur", "€", "gbp", "£", "jpy", "¥", "cny", "rmb", "aud", "cad"]
        if any(t in u for t in currency_tokens) or any(t in n for t in ["revenue", "market value", "valuation", "value (", "usd", "sgd", "eur"]):
            return "currency"

        # Unit sales / shipments
        unit_tokens = ["unit", "units", "sold", "sales", "sales volume", "shipments", "registrations", "deliveries", "vehicles", "pcs", "pieces", "volume"]
        if any(t in n for t in unit_tokens) or any(t in u for t in ["unit", "units", "vehicle", "vehicles", "pcs", "pieces"]):
            return "unit_sales"

        # PATCH FIX2D58G (ADDITIVE): sales-like names + magnitude/count unit evidence => unit_sales
        # Handles cases like 'Global EV Sales 2024' with unit 'million units' where name contains 'sales'
        # but unit tokens may not include 'units' in the name itself.
        if ('sales' in n or 'ev sales' in n) and (
            ('million' in u) or ('billion' in u) or ('thousand' in u) or ('mn' in u) or ('bn' in u) or
            ('unit' in u) or ('units' in u) or ('vehicle' in u) or ('vehicles' in u)
        ):
            return 'unit_sales'

        # Pure counts
        if any(t in n for t in ["count", "number of", "install base", "installed base", "users", "subscribers"]) and "revenue" not in n:
            return "count"

        # Index / score
        if any(t in n for t in ["index", "score", "rating"]):
            return "index"

        return "unknown"

    def display_name_for_dimension(original_display: str, dim: str) -> str:
        if not original_display:
            return original_display

        od = original_display.strip()
        od_low = od.lower()

        if dim == "unit_sales":
            if "revenue" in od_low or "market value" in od_low or "valuation" in od_low:
                return re.sub(r"(?i)revenue|market value|valuation", "Unit Sales", od).strip()
            if od_low.startswith("sales"):
                return "Unit Sales" + od[len("Sales"):]
            if "sales" in od_low:
                return re.sub(r"(?i)sales", "Unit Sales", od).strip()
            return od

        if dim == "currency":
            if "unit sales" in od_low:
                return re.sub(r"(?i)unit sales", "Revenue", od).strip()
            return od

        if dim == "percent":
            if "unit sales" in od_low or "revenue" in od_low:
                return od
            return od

        return od

    candidates = []

    for key, metric in metrics.items():
        if not isinstance(metric, dict):
            continue

        original_name = metric.get("name", key)
        canonical_id, canonical_name = get_canonical_metric_id(original_name)

        # =========================
        # PATCH CM1 (ADDITIVE): registry-guided dimension hint
        # - If the canonical base metric is in METRIC_REGISTRY, use its unit_type
        #   as a strong prior for dimension classification.
        # - This reduces mislabel drift like "Revenue" being assigned as unit_sales
        #   (or vice-versa) purely from noisy LLM labels.
        #
        # NOTE (conflict fix, additive):
        # - Your prior code risked UnboundLocalError due to base_id scoping.
        # - We keep your legacy behavior, but guard it and define base_id upfront.
        # =========================

        registry_unit_type = ""

        # ---- PATCH CM1.A (ADDITIVE): define base_id upfront to prevent UnboundLocalError ----
        base_id = ""
        # -------------------------------------------------------------------------------

        try:
            # =========================
            # PATCH CM1.B (BUGFIX + ADDITIVE): registry base_id extraction
            # - canonical_id may contain underscores inside the base id (e.g., "market_size_2025")
            # - Find the LONGEST registry key that is a prefix of canonical_id.
            # =========================
            try:
                reg = globals().get("METRIC_REGISTRY")
                cid = str(canonical_id or "")
                if isinstance(reg, dict) and cid:
                    # choose the longest matching prefix key
                    for k in reg.keys():
                        ks = str(k)
                        if cid == ks or cid.startswith(ks + "_"):
                            if len(ks) > len(base_id):
                                base_id = ks

                    if base_id and isinstance(reg.get(base_id), dict):
                        registry_unit_type = (reg[base_id].get("unit_type") or "").strip().lower()
            except Exception:
                pass
                # keep safe defaults
                pass
            # =========================

            # -------------------------------------------------------------------
            # PATCH CM1.C (ADDITIVE): legacy code preserved, but guarded
            # - This block is redundant with CM1.B, but we keep it as requested.
            # - Guard prevents:
            #   (1) base_id undefined
            #   (2) overwriting registry_unit_type already computed above
            # -------------------------------------------------------------------
            if not registry_unit_type:
                reg = globals().get("METRIC_REGISTRY")
                if base_id and isinstance(reg, dict) and base_id in reg and isinstance(reg[base_id], dict):
                    registry_unit_type = (reg[base_id].get("unit_type") or "").strip().lower()
            # -------------------------------------------------------------------

        except Exception:
            pass
            registry_unit_type = ""

        # Map registry unit_type -> canonicalize_metrics dimension vocabulary
        # (keep it small + deterministic)
        if registry_unit_type:
            if registry_unit_type in ("currency",):
                registry_dim_hint = "currency"
            elif registry_unit_type in ("percentage", "percent"):
                registry_dim_hint = "percent"
            elif registry_unit_type in ("count",):
                # keep "unit_sales" vs "count" distinction:
                # registry says count; name-based inference decides "unit_sales" if it sees units/shipments/deliveries
                registry_dim_hint = "count"
            else:
                registry_dim_hint = ""
        else:
            registry_dim_hint = ""
        # =========================

        raw_unit = (metric.get("unit") or "").strip()

        # =========================
        # PATCH C2 (ADDITIVE): compute unit_tag/unit_family without changing existing unit behavior
        # - We keep your existing unit_norm logic for backwards compatibility.
        # - But we ALSO attach unit_tag + unit_family so downstream can gate deterministically.
        # =========================
        unit_tag = metric.get("unit_tag") or _safe_normalize_unit_tag(raw_unit)
        unit_family_tag = metric.get("unit_family") or _safe_unit_family(unit_tag)
        # =========================

        unit_norm = raw_unit.upper()  # keep original behavior (do not change)

        # =========================
        # PATCH TRACE1 (ADDITIVE): canonical key mint trace
        # - Adds an auditable trace showing *how* dimension + canonical_key were minted.
        # - This is intentionally local (no new dependencies) and additive only.
        # - Downstream UI/JSON can surface these fields to diagnose drift (e.g., __unknown).
        # =========================
        dim_inferred = infer_metric_dimension(str(original_name), raw_unit)
        dim = dim_inferred

        # =========================
        # PATCH CM1 (ADDITIVE): apply registry hint as override / guardrail
        # - If registry says currency/percent, force that dimension.
        # - If registry says count, prevent accidental "currency"/"percent".
        # =========================
        _trace_dim_override = ""
        if registry_dim_hint in ("currency", "percent"):
            dim = registry_dim_hint
            _trace_dim_override = "registry_force"
        elif registry_dim_hint == "count":
            # Allow unit_sales if name clearly indicates it; else keep "count"
            if dim in ("currency", "percent"):
                dim = "count"
                _trace_dim_override = "registry_guard"
        # =========================

        _ident = build_identity_tuple_v1(metric_token=canonical_id, dimension=dim, unit_family=unit_family_tag, unit_tag=unit_tag, geo_scope=str(metric.get('geo_scope') or ''), time_scope='')
        _res = resolve_canonical_identity_v1(_ident, metric_schema)
        canonical_key = str(_res.get('canonical_key') or f"{canonical_id}__{dim}")

        # =========================
        # PATCH TRACE1.B (ADDITIVE): attach trace onto metric_enriched.debug.key_mint_trace
        # NOTE: metric_enriched is created below; stash trace ingredients now.
        # =========================
        _key_mint_trace = {
            "mint_fn": "canonicalize_metrics",
            "canonical_id": canonical_id,
            "canonical_key": canonical_key,
            "original_name": original_name,
            "canonical_name": canonical_name,
            "raw_unit": raw_unit,
            "unit_norm": unit_norm,
            "unit_tag": unit_tag,
            "unit_family": unit_family_tag,
            "dim_inferred": dim_inferred,
            "dim_final": dim,
            "registry_unit_type": registry_unit_type,
            "registry_dim_hint": registry_dim_hint,
            "dim_override": _trace_dim_override,
            "key_mint_path": ("REGISTRY_OVERRIDE" if _trace_dim_override else "NAME_UNIT_INFER"),
            "identity_tuple_v1": _ident if '_ident' in locals() else {},
            "identity_resolve_v1": _res if '_res' in locals() else {},
        }

        parsed_val = parse_to_float(metric.get("value"))
        value_for_sort = parsed_val if parsed_val is not None else str(metric.get("value", ""))

        stable_sort_key = (
            str(original_name).lower().strip(),
            dim,
            unit_norm,
            str(value_for_sort),
            str(key),
        )

        geo = infer_geo_scope(
            str(original_name),
            str(metric.get("context_snippet", "")),
            str(metric.get("source", "")),
            str(metric.get("source_url", "")),
        )

        proxy = infer_proxy_label(
            str(original_name),
            str(question_text),
            str(category_hint),
            str(metric.get("context_snippet", "")),
            str(metric.get("source", "")),
            str(metric.get("source_url", "")),
        )

        # =========================
        # PATCH C3 (ADDITIVE): canonicalize numeric fields on the candidate metric dict
        # - If canonicalize_numeric_candidate exists, it will attach:
        #   unit_tag/unit_family/base_unit/multiplier_to_base/value_norm
        # - If not, we attach minimal fields ourselves (still additive).
        # =========================
        metric_enriched = dict(metric)  # never mutate caller's dict
        try:
            fn_can = globals().get("canonicalize_numeric_candidate")
            if callable(fn_can):
                metric_enriched = fn_can(metric_enriched)
        except Exception:
            pass

        # Ensure minimal canonical fields exist (additive)
        metric_enriched.setdefault("unit_tag", unit_tag)
        metric_enriched.setdefault("unit_family", unit_family_tag)

        # Add mint trace (additive). Keep it under debug to avoid polluting top-level.
        try:
            metric_enriched.setdefault("debug", {})
            if isinstance(metric_enriched.get("debug"), dict):
                metric_enriched["debug"]["key_mint_trace"] = _key_mint_trace
        except Exception:
            pass
        # =========================

        candidates.append({
            "canonical_id": canonical_id,
            "canonical_key": canonical_key,
            "canonical_name": display_name_for_dimension(canonical_name, dim),
            "original_name": original_name,

            # NOTE: store enriched metric
            "metric": metric_enriched,

            "unit": unit_norm,
            "parsed_val": parsed_val,
            "dimension": dim,
            "stable_sort_key": stable_sort_key,
            "geo_scope": geo["geo_scope"],
            "geo_name": geo["geo_name"],
            **proxy,
        })

    candidates.sort(key=lambda x: x["stable_sort_key"])

    grouped: Dict[str, List[Dict]] = {}
    for c in candidates:
        grouped.setdefault(c["canonical_key"], []).append(c)

    canonicalized: Dict[str, Dict] = {}

    for ckey, group in grouped.items():
        if len(group) == 1 or not merge_duplicates_to_range:
            g = group[0]
            m = g["metric"]

            # =========================
            # PATCH C4 (ADDITIVE): keep canonical numeric & semantic fields on output row
            # (only adds keys; does not remove/rename existing keys)
            # =========================
            out_row = {
                **m,
                "name": g["canonical_name"],
                "canonical_id": g["canonical_id"],
                "canonical_key": ckey,
                "dimension": g["dimension"],
                "original_name": g["original_name"],
                "geo_scope": g.get("geo_scope", "unknown"),
                "geo_name": g.get("geo_name", ""),
                "is_proxy": bool(g.get("is_proxy", False)),
                "proxy_type": g.get("proxy_type", ""),
                "proxy_reason": g.get("proxy_reason", ""),
                "proxy_confidence": float(g.get("proxy_confidence", 0.0) or 0.0),
                "proxy_target": g.get("proxy_target", ""),
            }
            # Ensure these exist if upstream provided them
            for k in ["anchor_hash", "source_url", "context_snippet", "measure_kind", "measure_assoc",
                      "unit_tag", "unit_family", "base_unit", "multiplier_to_base", "value_norm"]:
                if k in m and k not in out_row:
                    out_row[k] = m.get(k)
            canonicalized[ckey] = out_row
            # =========================
            continue

        # Merge duplicates within SAME dimension-safe canonical_key
        base = group[0]
        base_metric = dict(base["metric"])
        base_metric["name"] = base["canonical_name"]
        base_metric["canonical_id"] = base["canonical_id"]
        base_metric["canonical_key"] = ckey
        base_metric["dimension"] = base["dimension"]

        geo_scope, geo_name = merge_group_geo(group)
        base_metric["geo_scope"] = geo_scope
        base_metric["geo_name"] = geo_name

        merged_proxy = merge_group_proxy(group)
        base_metric.update(merged_proxy)

        vals = [g["parsed_val"] for g in group if g["parsed_val"] is not None]
        raw_vals = [str(g["metric"].get("value", "")) for g in group]
        orig_names = [g["original_name"] for g in group]

        units = [g["unit"] for g in group if g["unit"]]
        unit_base = units[0] if units else (base_metric.get("unit") or "")
        base_metric["unit"] = unit_base

        base_metric["original_names"] = orig_names
        base_metric["raw_values"] = raw_vals

        # =========================
        # PATCH C5 (ADDITIVE): optional canonical range using value_norm if present
        # - Keeps your existing "range" untouched.
        # - Adds "range_norm" when we can compute it.
        # =========================
        vals_norm = []
        for g in group:
            mm = g.get("metric") if isinstance(g, dict) else {}
            if isinstance(mm, dict) and mm.get("value_norm") is not None:
                try:
                    vals_norm.append(float(mm.get("value_norm")))
                except Exception:
                    pass
        # =========================

        # =====================================================================
        # PATCH ANCHOR_VAL1 (ADDITIVE): set metric value from selected evidence candidate
        # Why:
        # - Avoid median/aggregate drift between analysis and evolution.
        # - If we already chose a specific evidence candidate (candidate_id/anchor_hash),
        #   that candidate should determine the metric's reported value/value_norm/unit.
        # Determinism:
        # - Select the evidence row with highest confidence if present, else first.
        # - No re-fetching, no new extraction; uses existing evidence payload only.
        # =====================================================================
        _anchored_value_set = False
        try:
            _ev = base_metric.get("evidence")
            if isinstance(_ev, list) and _ev:
                # pick best evidence deterministically
                def _ev_score(e):
                    try:
                        c = e.get("confidence")
                        return float(c) if c is not None else 0.0
                    except Exception:
                        return 0.0
                _ev_sorted = sorted([e for e in _ev if isinstance(e, dict)], key=_ev_score, reverse=True)
                _best = _ev_sorted[0] if _ev_sorted else None

                if isinstance(_best, dict):
                    # Prefer canonical normalized fields if present
                    _bn = _best.get("value_norm")
                    _bu = _best.get("base_unit") or _best.get("unit")
                    _rawv = _best.get("raw") if _best.get("raw") is not None else _best.get("value")

                    if _bn is not None:
                        try:
                            base_metric["value_norm"] = float(_bn)
                        except Exception:
                            pass

                    # Preserve unit/base_unit
                    if _bu:
                        try:
                            base_metric["base_unit"] = str(_bu)
                        except Exception:
                            pass
                    if _best.get("unit"):
                        base_metric["unit"] = _best.get("unit")

                    # Preserve raw/value display from evidence (preferred)
                    if _rawv is not None:
                        base_metric["raw"] = _rawv
                        base_metric["value"] = _rawv

                    # Helpful debug: show that we anchored value from evidence
                    base_metric.setdefault("debug", {})
                    if isinstance(base_metric.get("debug"), dict):
                        base_metric["debug"]["value_origin"] = "evidence_best_candidate"
                        base_metric["debug"]["evidence_candidate_id"] = _best.get("candidate_id") or _best.get("anchor_hash")
                    _anchored_value_set = True
        except Exception:
            pass
        if vals and not _anchored_value_set:

            vals_sorted = sorted(vals)
            vmin, vmax = vals_sorted[0], vals_sorted[-1]
            vmed = vals_sorted[len(vals_sorted) // 2]
            base_metric["value"] = vmed
            base_metric["range"] = {
                "min": vmin,
                "max": vmax,
                "candidates": vals_sorted,
                "n": len(vals_sorted),
            }
        else:
            base_metric["range"] = {"min": None, "max": None, "candidates": [], "n": 0}

        if len(vals_norm) >= 2:
            vn = sorted(vals_norm)
            base_metric["range_norm"] = {
                "min": vn[0],
                "max": vn[-1],
                "candidates": vn,
                "n": len(vn),
                "unit": base_metric.get("base_unit") or base_metric.get("unit") or "",
            }
        # =========================

        canonicalized[ckey] = base_metric

    return canonicalized


# =========================
# PATCH FIX2D58B (ADDITIVE): quarantine unknown/under-specified canonical keys
# - Do NOT allow dimension=='unknown' or missing unit_family to enter primary_metrics_canonical.
# - Preserve them under primary_metrics_provisional with full debug trace for audit.
# =========================
def _fix2d58b_split_primary_metrics_canonical(pmc: dict):
    try:
        if not isinstance(pmc, dict):
            return {}, {}
        canonical_ok = {}
        provisional = {}
        for k, v in pmc.items():
            if not isinstance(v, dict):
                provisional[k] = v
                continue
            dim = str(v.get('dimension') or '').strip().lower()
            uf = str(v.get('unit_family') or '').strip().lower()
            ut = str(v.get('unit_tag') or '').strip()
            # If dimension already implies family, fill it deterministically (additive).
            if dim in ('currency', 'percent') and not uf:
                vv = dict(v)
                vv['unit_family'] = dim
                v = vv
                uf = dim
            # Quarantine: unknown dimension OR missing family when we at least have a unit tag.
            if dim == 'unknown' or (not uf and bool(ut)):
                vv = dict(v)
                vv.setdefault('debug', {})
                if isinstance(vv.get('debug'), dict):
                    vv['debug']['quarantined_v1'] = True
                    vv['debug']['quarantine_reason_v1'] = 'unknown_dimension_or_missing_unit_family'
                provisional[k] = vv
            else:
                canonical_ok[k] = v
        return canonical_ok, provisional
    except Exception:
        pass
        # Fail-safe: never drop metrics silently if the splitter errors
        return pmc if isinstance(pmc, dict) else {}, {}


# =========================
# PATCH FIX2D60 (ADDITIVE): enforce schema-only canonical store (Analysis)
# - After FIX2D59 rekeying, primary_metrics_canonical must contain ONLY schema-bound keys.
# - Any PROVISIONAL/UNSPECIFIED rows are moved into primary_metrics_provisional.
# =========================



def freeze_metric_schema(canonical_metrics: Dict) -> Dict:
    """
    Lock metric identity + expected schema for future evolution.

    Key fix:
      - Stores canonical_key (dimension-safe)
      - Stores dimension + unit family
      - Keywords include dimension hints to improve later matching
    """
    frozen = {}
    if not isinstance(canonical_metrics, dict):
        return frozen

    # =========================
    # PATCH F1 (ADDITIVE): prefer shared normalize_unit_tag/unit_family helpers if present
    # This improves consistency with extractor + attribution gating.
    # Falls back safely to old heuristics.
    # =========================
    def _normalize_unit_safe(u: str) -> str:
        try:
            fn = globals().get("normalize_unit_tag")
            if callable(fn):
                return fn(u or "")
        except Exception:
            return (u or "").strip()

    def _unit_family_safe(unit_raw: str, dim_hint: str = "") -> str:
        # 1) dimension-first (strongest signal)
        d = (dim_hint or "").strip().lower()
        if d in ("percent", "pct"):
            return "percent"
        if d in ("currency",):
            return "currency"
        if d in ("energy",):
            return "energy"
        if d in ("unit_sales", "count"):
            # You’ve been treating M/B/T as “magnitude” for counts; keep aligned.
            return "magnitude"
        if d in ("index", "score"):
            return "index"

        # 2) if you already have a unit-family helper in the codebase, use it
        try:
            fn = globals().get("unit_family")
            if callable(fn):
                uf = fn(_normalize_unit_safe(unit_raw))
                if isinstance(uf, str) and uf.strip():
                    return uf.strip().lower()
        except Exception:
            pass

        # 3) fallback to old heuristic (your original logic)
        u = (unit_raw or "").strip().lower()
        if not u:
            return "unknown"
        if "%" in u:
            return "percent"
        if any(t in u for t in ["$", "s$", "usd", "sgd", "eur", "€", "gbp", "£", "jpy", "¥", "cny", "rmb"]):
            return "currency"
        if any(t in u for t in ["b", "bn", "billion", "m", "mn", "million", "k", "thousand", "t", "trillion"]):
            return "magnitude"
        return "other"
    # =========================

    for ckey, m in canonical_metrics.items():
        if not isinstance(m, dict):
            continue

        dim = (m.get("dimension") or "").strip() or "unknown"
        name = m.get("name")
        unit = (m.get("unit") or "").strip()

        # =========================
        # PATCH F2 (ADDITIVE): compute unit_family using dimension-first logic
        # =========================
        uf = _unit_family_safe(unit, dim_hint=dim)
        # =========================

        # Keywords: name + dimension token to prevent cross-dimension matches later
        kws = extract_context_keywords(name or "") or []
        if dim and dim not in kws:
            kws.append(dim)
        if uf and uf not in kws:
            kws.append(uf)

        # =========================
        # PATCH F3 (ADDITIVE): preserve schema unit more safely
        # - Keep your existing behavior in 'unit' (backward compatible),
        #   BUT also add 'unit_tag' which is the canonicalized unit used downstream.
        # - This avoids the "SGD -> S" schema corruption that breaks currency gating.
        # =========================
        unit_tag = _normalize_unit_safe(unit)
        # Keep existing 'unit' output to avoid breaking consumers:
        unit_out = unit_clean_first_letter(unit.upper())
        # =========================

        frozen[ckey] = {
            "canonical_key": ckey,
            "canonical_id": m.get("canonical_id") or ckey.split("__", 1)[0],
            "dimension": dim,
            "name": name,

            # Existing field kept exactly (backward compatible)
            "unit": unit_out,

            # =========================
            # PATCH F3 (ADDITIVE): extra stable fields (non-breaking additions)
            # =========================
            "unit_tag": unit_tag,          # e.g., "%", "M", "B", "TWh"
            "unit_family": uf,             # e.g., "currency", "percent", "magnitude"
            # =========================

            "keywords": kws[:30],
        }


# =====================================================================
# PATCH FIX2U_EV_CHARGERS_SCHEMA_V1 (ADDITIVE)
# Purpose:
#   Extend frozen schema with new canonical keys for EV charging infrastructure
#   (e.g., global EV chargers by 2040) so injected charger content can be
#   canonicalised + diffed deterministically under the shared Analysis selector.
# Notes:
#   - Purely additive: does not modify existing schema entries
#   - Dimension uses "count" so existing unit-family logic treats it as magnitude
#   - unit_tag uses "M" (million-scale counts) but unit_family remains "magnitude"
# =====================================================================

def _fix2u_extend_metric_schema_ev_chargers(metric_schema_frozen: dict) -> dict:
    """Add EV charger infrastructure canonical keys to frozen schema (additive)."""
    try:
        if not isinstance(metric_schema_frozen, dict):
            metric_schema_frozen = {}
    except Exception:
        pass
        metric_schema_frozen = {}

    def _add_if_missing(ckey: str, entry: dict):
        try:
            if not ckey or not isinstance(entry, dict):
                return
            if ckey in metric_schema_frozen:
                return
            metric_schema_frozen[ckey] = entry
        except Exception:
            pass

    # Minimal v1 keys (Global, high-signal, year-qualified)
    # 1) Global EV chargers by 2040 (count)
    _add_if_missing(
        "global_ev_chargers_2040__unit_count",
        {
            "canonical_key": "global_ev_chargers_2040__unit_count",
            "canonical_id": "global_ev_chargers_2040",
            "dimension": "count",
            "name": "Global EV chargers (2040)",
            # Backward-compatible schema fields:
            "unit": "M",
            # Stable, non-breaking additions used by downstream gates:
            "unit_tag": "M",
            "unit_family": "magnitude",
            "keywords": [
                "ev", "electric", "vehicle",
                "charger", "chargers", "charging", "infrastructure", "network",
                "global", "worldwide",
                "2040", "count", "magnitude",
            ],
        },
    )

    # 2) Global EV charging investment by 2040 (currency) — only used if extracted evidence exists
    _add_if_missing(
        "global_ev_charging_investment_2040__currency",
        {
            "canonical_key": "global_ev_charging_investment_2040__currency",
            "canonical_id": "global_ev_charging_investment_2040",
            "dimension": "currency",
            "name": "Global EV charging investment (2040)",
            "unit": "U",      # backward-compatible first-letter field (unit_tag holds the real tag)
            "unit_tag": "USD",
            "unit_family": "currency",
            "keywords": [
                "ev", "electric", "vehicle",
                "charger", "chargers", "charging", "infrastructure", "network",
                "investment", "spend", "spending", "capex", "expenditure",
                "global", "worldwide",
                "2040", "currency", "usd",
            ],
        },
    )

    return metric_schema_frozen

# =====================================================================
# END PATCH FIX2U_EV_CHARGERS_SCHEMA_V1
# =====================================================================

    return frozen

# =====================================================================
# PATCH FIX2V_EV_CHARGERS_CAGR_SCHEMA_V1 (ADDITIVE)
#   - Adds a canonical slot for charger infrastructure CAGR (2026->2040)
#   - Keeps semantics single-sourced under metric_schema_frozen
# =====================================================================
def _fix2v_extend_metric_schema_ev_chargers_cagr(metric_schema_frozen: dict) -> dict:
    """Add charger CAGR schema key additively (safe no-op if already present)."""
    try:
        if not isinstance(metric_schema_frozen, dict):
            return metric_schema_frozen
        # Do not override if exists
        if "global_ev_chargers_cagr_2026_2040__percent" in metric_schema_frozen:
            return metric_schema_frozen

        metric_schema_frozen = dict(metric_schema_frozen)

        metric_schema_frozen["global_ev_chargers_cagr_2026_2040__percent"] = dict(
            metric_id="global_ev_chargers_cagr_2026_2040",
            canonical_id="global_ev_chargers_cagr_2026_2040",
            metric_name="Global EV charging infrastructure CAGR (2026–2040)",
            dimension="percent",
            unit_family="percent",
            unit_tag="percent",
            # Deterministic keyword allowlist (no fuzzy matching)
            keywords=[
                "global", "worldwide",
                "ev", "charging", "charging infrastructure",
                "cagr", "compound annual growth rate",
                "2026", "2040",
            ],
        )
        return metric_schema_frozen
    except Exception:
        return metric_schema_frozen
# =====================================================================
# END PATCH FIX2V_EV_CHARGERS_CAGR_SCHEMA_V1

# =====================================================================
# PATCH FIX2AB_GLOBAL_EV_SALES_YTD_2025_SCHEMA_V1 (ADDITIVE)
#   - Adds a canonical slot for Global EV sales (YTD 2025) in unit_sales
#   - Intended for deterministic diff testing using Rhomotion-style sources
# =====================================================================
def _fix2ab_extend_metric_schema_global_ev_sales_ytd_2025(metric_schema_frozen: dict) -> dict:
    """Add Global EV Sales (YTD 2025) schema key additively (safe no-op if already present)."""
    try:
        if not isinstance(metric_schema_frozen, dict):
            return metric_schema_frozen
        if "global_ev_sales_ytd_2025__unit_sales" in metric_schema_frozen:
            return metric_schema_frozen

        metric_schema_frozen = dict(metric_schema_frozen)

        metric_schema_frozen["global_ev_sales_ytd_2025__unit_sales"] = dict(
            metric_id="global_ev_sales_ytd_2025",
            canonical_id="global_ev_sales_ytd_2025",
            metric_name="Global EV sales (YTD 2025)",
            dimension="unit_sales",
            unit_family="magnitude",
            unit_tag="million units",
            unit="M",
            # Deterministic keyword allowlist (no fuzzy matching)
            keywords=[
                "global", "worldwide",
                "ev", "sales",
                "ytd", "year-to-date",
                "2025",
                "million", "units",
            ],
        )
        return metric_schema_frozen
    except Exception:
        return metric_schema_frozen
# =====================================================================
# END PATCH FIX2AB_GLOBAL_EV_SALES_YTD_2025_SCHEMA_V1
# =====================================================================


# =====================================================================



# =========================================================
# RANGE + SOURCE ATTRIBUTION (DETERMINISTIC, NO LLM)
# =========================================================

def stable_json_hash(obj: Any) -> str:
    import hashlib, json
    try:
        s = json.dumps(obj, sort_keys=True, ensure_ascii=False, separators=(",", ":"))
    except Exception:
        pass
        s = str(obj)
    return hashlib.sha256(s.encode("utf-8", errors="ignore")).hexdigest()

def make_extracted_number_id(source_url: str, fingerprint: str, n: Dict) -> str:
    payload = {
        "url": source_url or "",
        "fp": fingerprint or "",
        "start": n.get("start_idx"),
        "end": n.get("end_idx"),
        "value": n.get("value"),
        "unit": normalize_unit(n.get("unit") or ""),
        "raw": n.get("raw") or "",
        "ctx": " ".join((n.get("context_snippet") or "").split())[:240],
    }
    return stable_json_hash(payload)

def sort_snapshot_numbers(numbers: List[Dict]) -> List[Dict]:
    """
    Deterministic ordering for extracted_numbers in snapshots.

    Backward compatible + robust:
      - Uses start/end idx when present
      - Avoids hard dependency on normalize_unit() (may not exist)
      - Falls back to normalize_unit_tag() if available
    """

    # =========================
    # PATCH SS1 (ADDITIVE): safe unit normalizer
    # - Prefer normalize_unit() if it exists
    # - Else fall back to normalize_unit_tag() if present
    # - Else just return stripped unit
    # =========================
    _norm_unit_fn = globals().get("normalize_unit")
    _norm_tag_fn = globals().get("normalize_unit_tag")

    def _safe_norm_unit(u: str) -> str:
        u = (u or "").strip()
        try:
            if callable(_norm_unit_fn):
                return str(_norm_unit_fn(u) or "")
        except Exception:
            pass
        try:
            if callable(_norm_tag_fn):
                # normalize_unit_tag expects tags / unit-ish strings; still better than raw
                return str(_norm_tag_fn(u) or "")
        except Exception:
            return u
    # =========================

    def k(n: Dict[str, Any]):
        n = n or {}
        return (
            n.get("start_idx") if isinstance(n.get("start_idx"), int) else 10**18,
            n.get("end_idx") if isinstance(n.get("end_idx"), int) else 10**18,

            # stable identity ordering
            str(n.get("anchor_hash") or ""),

            # unit + value
            _safe_norm_unit(str(n.get("unit") or "")),
            str(n.get("unit_tag") or ""),
            str(n.get("value_norm") if n.get("value_norm") is not None else n.get("value")),

            # final tie-breakers
            str(n.get("raw") or ""),
            str(n.get("context_snippet") or n.get("context") or "")[:80],
        )

    return sorted((numbers or []), key=k)

def sort_evidence_records(records: List[Dict]) -> List[Dict]:
    """
    Deterministic ordering for evidence_records.

    Backward compatible:
      - Uses url + fingerprint (as you had)
      - Adds fetched_at as tie-breaker if present (non-breaking)
    """

    # =========================
    # PATCH SE1 (ADDITIVE): add fetched_at tie-breaker (optional)
    # =========================
    def k(r: Dict[str, Any]):
        r = r or {}
        return (
            str(r.get("url") or ""),
            str(r.get("fingerprint") or ""),
            str(r.get("fetched_at") or ""),
        )
    # =========================

    return sorted((records or []), key=k)

def sort_metric_anchors(anchors: List[Dict]) -> List[Dict]:
    # =========================
    # PATCH MA2 (ADDITIVE): canonical-first stable sort
    # - Prefer canonical_key (new)
    # - Fall back to metric_id/metric_name (legacy)
    # =========================
    return sorted(
        (anchors or []),
        key=lambda a: (
            str((a or {}).get("canonical_key") or ""),
            str((a or {}).get("metric_id") or ""),
            str((a or {}).get("metric_name") or ""),
            str((a or {}).get("source_url") or ""),
        ),
    )






def to_billions(value: float, unit_tag: str) -> Optional[float]:
    """Convert T/B/M tagged values into billions. Leaves % unchanged (returns None for % here)."""
    try:
        v = float(value)
    except Exception:
        return None

    if unit_tag == "T":
        return v * 1000.0
    if unit_tag == "B":
        return v
    if unit_tag == "M":
        return v / 1000.0
    return None


def build_metric_keywords(metric_name: str) -> List[str]:
    """Reuse your existing keyword extractor, but ensure we always have something."""
    kws = extract_context_keywords(metric_name) or []
    # Add simple fallback tokens (deterministic)
    for t in re.findall(r"[a-zA-Z]{4,}", str(metric_name).lower()):
        if t not in kws:
            kws.append(t)
    return kws[:25]


def extract_numbers_from_scraped_sources(
    scraped_content: Dict[str, str],
) -> List[Dict[str, Any]]:
    """
    Deterministically extract numeric candidates from all scraped source texts.
    Returns list of {url, value, unit_tag, raw, context}.
    """
    candidates: List[Dict[str, Any]] = []
    if not isinstance(scraped_content, dict):
        return candidates

    for url, content in scraped_content.items():
        if not content or not isinstance(content, str) or len(content) < 200:
            continue

        # =========================
        # PATCH 1 (ADDITIVE): pass source_url through (improves anchor stability)
        # =========================
        nums = extract_numbers_with_context(content, source_url=url)
        # =========================

        for n in nums:
            # =========================
            # PATCH 1 (ADDITIVE): prefer extractor-provided unit_tag if present; else normalize
            # =========================
            unit_tag = n.get("unit_tag")
            if not unit_tag:
                unit_tag = normalize_unit_tag(n.get("unit", ""))
            # =========================

            row = {
                "url": url,
                "value": n.get("value"),
                "unit_tag": unit_tag,
                "raw": n.get("raw", ""),
                "context": (n.get("context") or ""),
            }

            # =========================
            # PATCH 3 (ADDITIVE): preserve measure association tags if extractor provides them
            # =========================
            if "measure_kind" in n:
                row["measure_kind"] = n.get("measure_kind")
            if "measure_assoc" in n:
                row["measure_assoc"] = n.get("measure_assoc")
            # =========================

            # =========================
            # PATCH 1 (ADDITIVE): preserve extra fields if extractor provides them
            # (backwards compatible: we only add keys, never remove)
            # =========================
            for k in [
                "unit", "is_junk", "junk_reason", "anchor_hash",
                "start_idx", "end_idx", "context_snippet",
                "unit_family", "base_unit", "multiplier_to_base", "value_norm"
            ]:
                if k in n:
                    row[k] = n.get(k)
            # =========================

            # ============================================================
            # PATCH 9 (ADDITIVE): enforce canonical numeric fields uniformly
            # Why:
            #   - Some candidates may not carry unit_family/base_unit/value_norm yet
            #   - We want every candidate (analysis + evolution) to have the same
            #     canonical fields so diff + span logic is stable and drift-free.
            #
            # This is additive and safe to call multiple times.
            # ============================================================
            try:
                fn_can = globals().get("canonicalize_numeric_candidate")
                if callable(fn_can):
                    row = fn_can(row) or row
                else:
                    row = canonicalize_numeric_candidate(row) or row
            except Exception:
                pass

            # --- ADDITIVE: ensure canonical keys exist even if canonicalize failed ---
            row.setdefault("unit_family", unit_family(row.get("unit_tag", "") or ""))
            row.setdefault("base_unit", row.get("unit_tag", "") or "")
            row.setdefault("multiplier_to_base", 1.0)
            if row.get("value") is not None and row.get("value_norm") is None:
                try:
                    row["value_norm"] = float(row.get("value"))
                except Exception:
                    pass
            # ------------------------------------------------------------------------
            # ============================================================

            candidates.append(row)

    return candidates


def attribute_span_to_sources(
    metric_name: str,
    metric_unit: str,
    scraped_content: Dict[str, str],
    rel_tol: float = 0.08,
    # =========================
    # PATCH S1 (ADDITIVE): optional schema inputs (non-breaking)
    # - If provided, we enforce schema-first gating for drift stability.
    # - If not provided, we fall back to existing heuristic behavior.
    # =========================
    canonical_key: str = "",
    metric_schema: Dict[str, Any] = None,
    # =========================
) -> Dict[str, Any]:
    """
    Build a deterministic span (min/mid/max) for a metric, and attribute min/max to sources.
    Uses only scraped content + regex extractions (NO LLM).

    Schema-first behavior (when metric_schema/canonical_key provided):
      - Enforces unit_family and currency/count/percent gating from frozen schema
      - Uses measure_kind tags when available to avoid semantic leakage
      - Keeps deterministic tie-breaking
    """
    import re
    import hashlib

    unit_tag_hint = normalize_unit_tag(metric_unit)
    keywords = build_metric_keywords(metric_name)

    all_candidates = extract_numbers_from_scraped_sources(scraped_content)
    filtered: List[Dict[str, Any]] = []

    metric_l = (metric_name or "").lower()

    # =========================
    # PATCH S2 (ADDITIVE): resolve schema entry (if available)
    # =========================
    schema_entry = None
    if isinstance(metric_schema, dict) and canonical_key and isinstance(metric_schema.get(canonical_key), dict):
        schema_entry = metric_schema.get(canonical_key)
    # =========================

    # =========================
    # PATCH S3 (ADDITIVE): schema-derived expectations with safe fallbacks
    # =========================
    schema_unit_family = ""
    schema_dimension = ""
    schema_unit = ""
    if isinstance(schema_entry, dict):
        schema_unit_family = (schema_entry.get("unit_family") or "").strip().lower()
        schema_dimension = (schema_entry.get("dimension") or "").strip().lower()
        schema_unit = (schema_entry.get("unit") or "").strip()

    expected_family = ""
    if schema_unit_family in ("percent", "currency", "energy"):
        expected_family = schema_unit_family
    if not expected_family:
        ut = normalize_unit_tag(metric_unit)
        if ut == "%":
            expected_family = "percent"
        elif ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
            expected_family = "energy"
        else:
            expected_family = ""

    currencyish = False
    if schema_unit_family == "currency" or schema_dimension == "currency":
        currencyish = True
    if not currencyish:
        mu = (metric_unit or "").lower()
        if any(x in mu for x in ["usd", "sgd", "eur", "gbp", "$", "s$", "€", "£", "aud", "cad", "jpy", "cny", "rmb"]):
            currencyish = True
    if not currencyish and any(x in metric_l for x in ["revenue", "turnover", "valuation", "market value", "market size",
                                                       "profit", "earnings", "ebitda", "capex", "opex"]):
        currencyish = True
    # =========================

    # =========================
    # PATCH S4 (ADDITIVE): expected measure_kind (schema-first with fallback)
    # =========================
    expected_kind = None

    if expected_family == "percent":
        if any(k in metric_l for k in ["growth", "cagr", "increase", "decrease", "yoy", "qoq", "mom", "rate"]):
            expected_kind = "growth_pct"
        else:
            expected_kind = "share_pct"

    if currencyish:
        expected_kind = "money"

    if expected_kind is None and any(k in metric_l for k in [
        "units", "unit sales", "vehicle sales", "vehicles sold", "sold", "sales volume",
        "deliveries", "shipments", "registrations", "volume"
    ]):
        expected_kind = "count_units"
    # =========================

    # =========================
    # PATCH S5 (ADDITIVE): year-ish suppression helpers (unchanged behavior)
    # =========================
    metric_is_yearish = any(k in metric_l for k in ["year", "years", "fy", "fiscal", "calendar", "timeline", "target year"])

    def _looks_like_year_value(v) -> bool:
        try:
            iv = int(float(v))
            return 1900 <= iv <= 2099
        except Exception:
            return False

    def _ctx_has_year_range(ctx: str) -> bool:
        return bool(re.search(r"\b(19|20)\d{2}\s*(?:-|–|—|to)\s*(19|20)\d{2}\b", ctx or "", flags=re.I))
    # =========================

    # =========================
    # PATCH S6 (ADDITIVE): currency evidence check (used only when currencyish)
    # =========================
    def _has_currency_evidence(raw: str, ctx: str) -> bool:
        r = (raw or "")
        c = (ctx or "").lower()

        if any(s in r for s in ["$", "S$", "€", "£"]):
            return True
        if any(code in c for code in [" usd", "sgd", " eur", " gbp", " aud", " cad", " jpy", " cny", " rmb"]):
            return True

        strong_kw = [
            "revenue", "turnover", "valuation", "valued at", "market value", "market size",
            "sales value", "net profit", "operating profit", "gross profit",
            "ebitda", "earnings", "income", "capex", "opex"
        ]
        if any(k in c for k in strong_kw):
            return True
        return False
    # =========================

    # =========================================================================
    # PATCH S11 (ADDITIVE): deterministic candidate_id for tie-breaking
    # - Stable across runs, depends only on stable fields
    # - Used ONLY as final tie-breaker (won't change non-tie outcomes)
    # =========================================================================
    def _candidate_id(x: dict) -> str:
        try:
            url = str(x.get("url") or x.get("source_url") or "")
            ah = str(x.get("anchor_hash") or "")
            vn = x.get("value_norm")
            bu = str(x.get("base_unit") or x.get("unit") or x.get("unit_tag") or "")
            mk = str(x.get("measure_kind") or "")
            # normalize numeric string for stability
            vn_s = ""
            if vn is not None:
                try:
                    vn_s = f"{float(vn):.12g}"
                except Exception:
                    pass
                    vn_s = str(vn)
            s = f"{url}|{ah}|{vn_s}|{bu}|{mk}"
            return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()
        except Exception:
            return ""
    # =========================================================================

    for c in all_candidates:
        ctx = c.get("context", "")
        if not ctx:
            continue

        if c.get("is_junk") is True:
            continue

        if not metric_is_yearish:
            if (c.get("unit_tag") in ("", None)) and _looks_like_year_value(c.get("value")):
                continue
            if _looks_like_year_value(c.get("value")) and _ctx_has_year_range(ctx):
                continue

        ctx_score = calculate_context_match(keywords, ctx)
        if ctx_score <= 0.0:
            continue

        cand_ut = c.get("unit_tag") or normalize_unit_tag(c.get("unit") or "")
        cand_fam = (c.get("unit_family") or unit_family(cand_ut) or "").strip().lower()

        if expected_family:
            if expected_family == "percent" and cand_fam != "percent":
                continue
            if expected_family == "currency":
                if cand_fam not in ("currency", "magnitude"):
                    continue
                if not _has_currency_evidence(c.get("raw", ""), ctx):
                    continue
            if expected_family == "energy" and cand_fam != "energy":
                continue

        if expected_kind:
            mk = c.get("measure_kind")
            if mk and mk != expected_kind:
                continue

        val_norm = None
        if expected_family == "percent" or unit_tag_hint == "%":
            if cand_ut != "%":
                continue
            val_norm = c.get("value")

        elif expected_family == "energy":
            val_norm = c.get("value_norm")
            if val_norm is None:
                val_norm = c.get("value")

        elif currencyish or expected_family == "currency":
            if c.get("measure_kind") == "count_units":
                continue
            if cand_ut not in ("T", "B", "M"):
                continue
            val_norm = to_billions(c.get("value"), cand_ut)
            if val_norm is None:
                continue

        else:
            try:
                val_norm = float(c.get("value"))
            except Exception:
                pass
                continue

        row = {
            **c,
            "unit_tag": cand_ut,
            "unit_family": cand_fam,
            "value_norm": val_norm,
            "ctx_score": float(ctx_score),
        }

        # =========================
        # PATCH S11 (ADDITIVE): attach candidate_id (safe extra field)
        # =========================
        row.setdefault("candidate_id", _candidate_id(row))
        # =========================

        filtered.append(row)

    if not filtered:
        return {
            "span": None,
            "source_attribution": None,
            "evidence": []
        }

    # Deterministic selection: value_norm then ctx_score then url then candidate_id
    # =========================================================================
    # PATCH S12 (ADDITIVE): candidate_id as final tie-breaker
    # =========================================================================
    def min_key(x):
        return (
            float(x["value_norm"]),
            -float(x["ctx_score"]),
            str(x.get("url", "")),
            str(x.get("candidate_id", "")),
        )

    def max_key(x):
        return (
            -float(x["value_norm"]),
            -float(x["ctx_score"]),
            str(x.get("url", "")),
            str(x.get("candidate_id", "")),
        )
    # =========================================================================

    min_item = sorted(filtered, key=min_key)[0]
    max_item = sorted(filtered, key=max_key)[0]

    vmin = float(min_item["value_norm"])
    vmax = float(max_item["value_norm"])
    vmid = (vmin + vmax) / 2.0

    if expected_family == "percent" or unit_tag_hint == "%":
        unit_out = "%"
    elif currencyish or expected_family == "currency":
        unit_out = "billion USD"
    elif expected_family == "energy":
        unit_out = "Wh"
    else:
        unit_out = metric_unit or (schema_unit or "")

    evidence = []
    for it in sorted(filtered, key=lambda x: (-float(x["ctx_score"]), str(x.get("url", "")), str(x.get("candidate_id", ""))))[:12]:
        evidence.append({
            "url": it.get("url"),
            "raw": it.get("raw"),
            "unit_tag": it.get("unit_tag"),
            "unit_family": it.get("unit_family"),
            "measure_kind": it.get("measure_kind"),
            "measure_assoc": it.get("measure_assoc"),
            "value_norm": it.get("value_norm"),
            "candidate_id": it.get("candidate_id"),
            # PATCH EVID_AH1 (ADDITIVE): carry anchor_hash for evolution matching
            "anchor_hash": it.get("anchor_hash"),
            # PATCH EVID_AH2 (ADDITIVE): carry stable span fields when present
            "start_idx": it.get("start_idx"),
            "end_idx": it.get("end_idx"),
            # PATCH EVID_AH3 (ADDITIVE): carry normalized value basis when present
            "value_norm": it.get("value_norm"),
            "base_unit": it.get("base_unit"),
            "multiplier_to_base": it.get("multiplier_to_base"),  # PATCH S11: exposed for transparency
            "context_snippet": (it.get("context") or "")[:220],
            "context_score": round(float(it.get("ctx_score", 0.0)) * 100, 1),
        })

    return {
        "span": {
            "min": round(vmin, 4),
            "mid": round(vmid, 4),
            "max": round(vmax, 4),
            "unit": unit_out
        },
        "source_attribution": {
            "min": {
                "url": min_item.get("url"),
                "raw": min_item.get("raw"),
                "measure_kind": min_item.get("measure_kind"),
                "measure_assoc": min_item.get("measure_assoc"),
                "value_norm": min_item.get("value_norm"),
                "candidate_id": min_item.get("candidate_id"),  # PATCH S11
                "context_snippet": (min_item.get("context") or "")[:220],
                "context_score": round(float(min_item.get("ctx_score", 0.0)) * 100, 1),
            },
            "max": {
                "url": max_item.get("url"),
                "raw": max_item.get("raw"),
                "measure_kind": max_item.get("measure_kind"),
                "measure_assoc": max_item.get("measure_assoc"),
                "value_norm": max_item.get("value_norm"),
                "candidate_id": max_item.get("candidate_id"),  # PATCH S11
                "context_snippet": (max_item.get("context") or "")[:220],
                "context_score": round(float(max_item.get("ctx_score", 0.0)) * 100, 1),
            }
        },
        "evidence": evidence
    }



def add_range_and_source_attribution_to_canonical_metrics(
    canonical_metrics: Dict[str, Any],
    web_context: dict,
    # =========================
    # PATCH R1 (ADDITIVE): optional schema-first inputs
    # If provided, attribution uses frozen schema to avoid semantic/unit leakage.
    # =========================
    metric_schema: Dict[str, Any] = None,
    # =========================
) -> Dict[str, Any]:
    """
    Enrich canonical metrics with deterministic range + source attribution.

    IMPORTANT:
    - canonical_metrics is expected to be keyed by canonical_key (dimension-safe),
      i.e. the output of canonicalize_metrics().
    - Schema-first mode (recommended): pass metric_schema=metric_schema_frozen so
      attribute_span_to_sources() can enforce unit_family / measure_kind gates.
    - Backward compatible: if metric_schema not provided, attribution falls back
      to existing heuristic behavior inside attribute_span_to_sources().
    """
    enriched: Dict[str, Any] = {}
    if not isinstance(canonical_metrics, dict):
        return enriched

    scraped = (web_context or {}).get("scraped_content") or {}
    if not isinstance(scraped, dict):
        scraped = {}

    # =========================
    # PATCH R2 (ADDITIVE): resolve schema dict safely
    # =========================
    schema = metric_schema if isinstance(metric_schema, dict) else {}
    # =========================

    for ckey, m in canonical_metrics.items():
        if not isinstance(m, dict):
            continue

        metric_name = m.get("name") or m.get("original_name") or str(ckey)
        metric_unit = m.get("unit") or ""

        # =========================
        # PATCH R3 (BUGFIX): schema-first wiring (no undefined prev_response/ckey)
        # - canonical_key is the dict key (ckey)
        # - metric_schema is the frozen schema dict (if provided)
        # =========================
        span_pack = attribute_span_to_sources(
            metric_name=metric_name,
            metric_unit=metric_unit,
            scraped_content=scraped,
            canonical_key=str(ckey),
            metric_schema=schema,
        )
        # =========================

        mm = dict(m)

        # Preserve old behavior: only add keys (don’t remove anything)
        if isinstance(span_pack, dict):
            if span_pack.get("span") is not None:
                mm["source_span"] = span_pack.get("span")
            if span_pack.get("source_attribution") is not None:
                mm["source_attribution"] = span_pack.get("source_attribution")
            if span_pack.get("evidence") is not None:
                mm["evidence"] = span_pack.get("evidence")

        enriched[ckey] = mm

    return enriched





# ------------------------------------
# SEMANTIC FINDING HASH
# Removes wording-based churn from findings comparison
# ------------------------------------

# Semantic components to extract from findings
FINDING_PATTERNS = {
    # Growth/decline patterns
    "growth": [
        r'(?:grow(?:ing|th)?|increas(?:e|ing)|expand(?:ing)?|ris(?:e|ing)|up)\s*(?:by|at|of)?\s*(\d+(?:\.\d+)?)\s*%?',
        r'(\d+(?:\.\d+)?)\s*%?\s*(?:growth|increase|expansion|rise)',
    ],
    "decline": [
        r'(?:declin(?:e|ing)|decreas(?:e|ing)|fall(?:ing)?|drop(?:ping)?|down)\s*(?:by|at|of)?\s*(\d+(?:\.\d+)?)\s*%?',
        r'(\d+(?:\.\d+)?)\s*%?\s*(?:decline|decrease|drop|fall)',
    ],

    # Value patterns
    "value": [
        r'\$\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)?',
        r'(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)',
    ],

    # Ranking patterns
    "rank": [
        r'(?:lead(?:ing|er)?|top|first|largest|biggest|#1|number one)',
        r'(?:second|#2|runner.?up)',
        r'(?:third|#3)',
    ],

    # Trend patterns
    "trend_up": [
        r'(?:bullish|optimistic|positive|strong|robust|accelerat)',
    ],
    "trend_down": [
        r'(?:bearish|pessimistic|negative|weak|slow(?:ing)?|decelerat)',
    ],

    # Entity patterns (will be filled dynamically)
    "entities": []
}

# Common stop words to remove
STOP_WORDS = {
    'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
    'should', 'may', 'might', 'must', 'shall', 'can', 'to', 'of', 'in',
    'for', 'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through',
    'during', 'before', 'after', 'above', 'below', 'between', 'under',
    'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where',
    'why', 'how', 'all', 'each', 'few', 'more', 'most', 'other', 'some',
    'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',
    'too', 'very', 'just', 'also', 'now', 'new'
}


def extract_semantic_components(finding: str) -> Dict[str, Any]:
    """
    Extract semantic components from a finding.

    Example:
        "The market is growing at 15% annually" ->
        {
            "direction": "up",
            "percentage": 15.0,
            "subject": "market",
            "timeframe": "annual",
            "entities": [],
            "keywords": ["market", "growing", "annually"]
        }
    """
    if not finding:
        return {}

    finding_lower = finding.lower()
    components = {
        "direction": None,
        "percentage": None,
        "value": None,
        "value_unit": None,
        "subject": None,
        "timeframe": None,
        "entities": [],
        "keywords": []
    }

    # Extract direction
    for pattern in FINDING_PATTERNS["growth"]:
        match = re.search(pattern, finding_lower)
        if match:
            components["direction"] = "up"
            if match.groups():
                try:
                    components["percentage"] = float(match.group(1))
                except:
                    pass
            break

    if not components["direction"]:
        for pattern in FINDING_PATTERNS["decline"]:
            match = re.search(pattern, finding_lower)
            if match:
                components["direction"] = "down"
                if match.groups():
                    try:
                        components["percentage"] = float(match.group(1))
                    except:
                        pass
                break

    # Extract trend sentiment
    if not components["direction"]:
        for pattern in FINDING_PATTERNS["trend_up"]:
            if re.search(pattern, finding_lower):
                components["direction"] = "up"
                break
        for pattern in FINDING_PATTERNS["trend_down"]:
            if re.search(pattern, finding_lower):
                components["direction"] = "down"
                break

    # Extract value
    for pattern in FINDING_PATTERNS["value"]:
        match = re.search(pattern, finding_lower)
        if match:
            try:
                components["value"] = float(match.group(1))
                if len(match.groups()) > 1 and match.group(2):
                    components["value_unit"] = match.group(2)[0].upper()
            except:
                pass
            break

    # Extract timeframe
    timeframe_patterns = {
        "annual": r'\b(?:annual(?:ly)?|year(?:ly)?|per year|yoy|y-o-y)\b',
        "quarterly": r'\b(?:quarter(?:ly)?|q[1-4])\b',
        "monthly": r'\b(?:month(?:ly)?|per month)\b',
        "2024": r'\b2024\b',
        "2025": r'\b2025\b',
        "2026": r'\b2026\b',
        "2030": r'\b2030\b',
    }
    for tf_name, tf_pattern in timeframe_patterns.items():
        if re.search(tf_pattern, finding_lower):
            components["timeframe"] = tf_name
            break

    # Extract subject keywords
    words = re.findall(r'\b[a-z]{3,}\b', finding_lower)
    keywords = [w for w in words if w not in STOP_WORDS]
    components["keywords"] = keywords[:10]  # Limit to top 10

    # Identify likely subject
    subject_candidates = ["market", "industry", "sector", "segment", "revenue", "sales", "demand", "supply"]
    for word in keywords:
        if word in subject_candidates:
            components["subject"] = word
            break

    return components


def compute_semantic_hash(finding: str) -> str:
    """
    Compute a semantic hash for a finding that's invariant to wording changes.

    Two findings with the same meaning should produce the same hash.

    Example:
        "The market is growing at 15% annually" -> "up_15.0_market_annual"
        "Annual growth rate stands at 15%" -> "up_15.0_market_annual"
    """
    components = extract_semantic_components(finding)

    # Build hash components in consistent order
    hash_parts = []

    # Direction
    if components.get("direction"):
        hash_parts.append(components["direction"])

    # Percentage (rounded to avoid float issues)
    if components.get("percentage") is not None:
        hash_parts.append(f"{components['percentage']:.1f}")

    # Value with unit
    if components.get("value") is not None:
        val_str = f"{components['value']:.1f}"
        if components.get("value_unit"):
            val_str += components["value_unit"]
        hash_parts.append(val_str)

    # Subject
    if components.get("subject"):
        hash_parts.append(components["subject"])

    # Timeframe
    if components.get("timeframe"):
        hash_parts.append(components["timeframe"])

    # If we have enough components, use them for hash
    if len(hash_parts) >= 2:
        return "_".join(hash_parts)

    # Fallback: use sorted keywords
    keywords = sorted(components.get("keywords", []))[:5]
    if keywords:
        return "_".join(keywords)

    # Last resort: normalized text hash
    normalized = re.sub(r'\s+', ' ', finding.lower().strip())
    return hashlib.md5(normalized.encode()).hexdigest()[:12]


def compute_semantic_finding_diffs(old_findings: List[str], new_findings: List[str]) -> List[FindingDiff]:
    """
    Compute finding diffs using semantic hashing instead of text similarity.

    This ensures that findings with the same meaning but different wording
    are recognized as the same finding.
    """
    diffs = []
    matched_new_indices = set()

    # Compute hashes for all findings
    old_hashes = [(f, compute_semantic_hash(f), extract_semantic_components(f)) for f in old_findings if f]
    new_hashes = [(f, compute_semantic_hash(f), extract_semantic_components(f)) for f in new_findings if f]

    # Match by semantic hash
    for old_text, old_hash, old_components in old_hashes:
        best_match_idx = None
        best_match_score = 0

        for i, (new_text, new_hash, new_components) in enumerate(new_hashes):
            if i in matched_new_indices:
                continue

            # Exact hash match = same finding
            if old_hash == new_hash:
                best_match_idx = i
                best_match_score = 100
                break

            # Component-based similarity
            score = compute_component_similarity(old_components, new_components)
            if score > best_match_score:
                best_match_score = score
                best_match_idx = i

        if best_match_idx is not None and best_match_score >= 60:
            matched_new_indices.add(best_match_idx)
            new_text = new_hashes[best_match_idx][0]

            if best_match_score >= 90:
                change_type = 'retained'
            else:
                change_type = 'modified'

            diffs.append(FindingDiff(
                old_text=old_text,
                new_text=new_text,
                similarity=best_match_score,
                change_type=change_type
            ))
        else:
            # Finding removed
            diffs.append(FindingDiff(
                old_text=old_text,
                new_text=None,
                similarity=0,
                change_type='removed'
            ))

    # Find added findings
    for i, (new_text, new_hash, new_components) in enumerate(new_hashes):
        if i not in matched_new_indices:
            diffs.append(FindingDiff(
                old_text=None,
                new_text=new_text,
                similarity=0,
                change_type='added'
            ))

    return diffs


def compute_component_similarity(comp1: Dict, comp2: Dict) -> float:
    """
    Compute similarity between two finding component dictionaries.
    Returns a score from 0-100.
    """
    if not comp1 or not comp2:
        return 0

    score = 0
    weights = {
        "direction": 25,
        "percentage": 25,
        "value": 20,
        "subject": 15,
        "timeframe": 10,
        "keywords": 5
    }

    # Direction match
    if comp1.get("direction") and comp2.get("direction"):
        if comp1["direction"] == comp2["direction"]:
            score += weights["direction"]
    elif not comp1.get("direction") and not comp2.get("direction"):
        score += weights["direction"] * 0.5  # Both neutral

    # Percentage match (within 2% tolerance)
    if comp1.get("percentage") is not None and comp2.get("percentage") is not None:
        diff = abs(comp1["percentage"] - comp2["percentage"])
        if diff <= 2:
            score += weights["percentage"]
        elif diff <= 5:
            score += weights["percentage"] * 0.5

    # Value match (within 10% tolerance)
    if comp1.get("value") is not None and comp2.get("value") is not None:
        v1, v2 = comp1["value"], comp2["value"]
        # Normalize by unit
        if comp1.get("value_unit") == comp2.get("value_unit"):
            if v1 > 0 and v2 > 0:
                ratio = min(v1, v2) / max(v1, v2)
                if ratio >= 0.9:
                    score += weights["value"]
                elif ratio >= 0.8:
                    score += weights["value"] * 0.5

    # Subject match
    if comp1.get("subject") and comp2.get("subject"):
        if comp1["subject"] == comp2["subject"]:
            score += weights["subject"]

    # Timeframe match
    if comp1.get("timeframe") and comp2.get("timeframe"):
        if comp1["timeframe"] == comp2["timeframe"]:
            score += weights["timeframe"]

    # Keyword overlap
    kw1 = set(comp1.get("keywords", []))
    kw2 = set(comp2.get("keywords", []))
    if kw1 and kw2:
        overlap = len(kw1 & kw2) / len(kw1 | kw2)
        score += weights["keywords"] * overlap

    return score


# ------------------------------------
# UPDATED METRIC DIFF COMPUTATION
# Using canonical IDs
# ------------------------------------

def compute_metric_diffs_canonical(old_metrics: Dict, new_metrics: Dict) -> List[MetricDiff]:
    """
    Compute metric diffs using canonical IDs for stable matching.
    Range-aware via get_metric_value_span + spans_overlap.
    """
    diffs: List[MetricDiff] = []

    old_canonical = canonicalize_metrics(old_metrics)
    new_canonical = canonicalize_metrics(new_metrics)

    matched_new_ids = set()

    # Match by canonical ID
    for old_id, old_m in old_canonical.items():
        old_name = old_m.get("name", old_id)

        old_span = get_metric_value_span(old_m)
        old_raw = str(old_m.get("value", ""))
        old_unit = old_span.get("unit") or old_m.get("unit", "")
        old_val = old_span.get("mid")

        # -------------------------
        # Direct canonical ID match
        # -------------------------
        if old_id in new_canonical:
            new_m = new_canonical[old_id]
            matched_new_ids.add(old_id)

            new_raw = str(new_m.get("value", ""))
            new_span = get_metric_value_span(new_m)
            new_val = new_span.get("mid")
            new_unit = new_span.get("unit") or new_m.get("unit", old_unit)

            if spans_overlap(old_span, new_span, rel_tol=0.05):
                change_pct = 0.0
                change_type = "unchanged"
            else:
                change_pct = compute_percent_change(old_val, new_val)
                if change_pct is None or abs(change_pct) < 0.5:
                    change_type = "unchanged"
                elif change_pct > 0:
                    change_type = "increased"
                else:
                    change_type = "decreased"

            diffs.append(MetricDiff(
                name=old_name,
                old_value=old_val,
                new_value=new_val,
                old_raw=old_raw,
                new_raw=new_raw,
                unit=new_unit or old_unit,
                change_pct=change_pct,
                change_type=change_type
            ))
            continue  # important: don't fall into base-ID matching

        # -------------------------
        # Base ID match (strip years)
        # -------------------------
        base_id = re.sub(r'_\d{4}(?:_\d{4})*$', '', old_id)
        found = False

        for new_id, new_m in new_canonical.items():
            if new_id in matched_new_ids:
                continue

            new_base_id = re.sub(r'_\d{4}(?:_\d{4})*$', '', new_id)
            if base_id != new_base_id:
                continue

            matched_new_ids.add(new_id)
            found = True

            new_raw = str(new_m.get("value", ""))
            new_span = get_metric_value_span(new_m)
            new_val = new_span.get("mid")
            new_unit = new_span.get("unit") or new_m.get("unit", old_unit)

            if spans_overlap(old_span, new_span, rel_tol=0.05):
                change_pct = 0.0
                change_type = "unchanged"
            else:
                change_pct = compute_percent_change(old_val, new_val)
                if change_pct is None or abs(change_pct) < 0.5:
                    change_type = "unchanged"
                elif change_pct > 0:
                    change_type = "increased"
                else:
                    change_type = "decreased"

            diffs.append(MetricDiff(
                name=old_name,
                old_value=old_val,
                new_value=new_val,
                old_raw=old_raw,
                new_raw=new_raw,
                unit=new_unit or old_unit,
                change_pct=change_pct,
                change_type=change_type
            ))
            break

        if not found:
            diffs.append(MetricDiff(
                name=old_name,
                old_value=old_val,
                new_value=None,
                old_raw=old_raw,
                new_raw="",
                unit=old_unit,
                change_pct=None,
                change_type="removed"
            ))

    # Added metrics
    for new_id, new_m in new_canonical.items():
        if new_id in matched_new_ids:
            continue

        new_name = new_m.get("name", new_id)
        new_raw = str(new_m.get("value", ""))
        new_span = get_metric_value_span(new_m)
        new_val = new_span.get("mid")
        new_unit = new_span.get("unit") or new_m.get("unit", "")

        diffs.append(MetricDiff(
            name=new_name,
            old_value=None,
            new_value=new_val,
            old_raw="",
            new_raw=new_raw,
            unit=new_unit,
            change_pct=None,
            change_type="added"
        ))

    return diffs


# ------------------------------------
# NUMERIC PARSING (DETERMINISTIC)
# ------------------------------------

def parse_to_float(value: Any) -> Optional[float]:
    """
    Deterministically parse any value to float.
    Returns None if unparseable.
    """
    if value is None:
        return None
    if isinstance(value, (int, float)):
        return float(value)
    if not isinstance(value, str):
        return None

    # Clean string
    cleaned = value.strip().upper()
    cleaned = re.sub(r'[,$]', '', cleaned)

    # Handle empty/NA
    if cleaned in ['', 'N/A', 'NA', 'NULL', 'NONE', '-', '—']:
        return None

    # Extract multiplier
    multiplier = 1.0
    if 'TRILLION' in cleaned or cleaned.endswith('T'):
        multiplier = 1_000_000
        cleaned = re.sub(r'T(?:RILLION)?', '', cleaned)
    elif 'BILLION' in cleaned or cleaned.endswith('B'):
        multiplier = 1_000
        cleaned = re.sub(r'B(?:ILLION)?', '', cleaned)
    elif 'MILLION' in cleaned or cleaned.endswith('M'):
        multiplier = 1
        cleaned = re.sub(r'M(?:ILLION)?', '', cleaned)
    elif 'THOUSAND' in cleaned or cleaned.endswith('K'):
        multiplier = 0.001
        cleaned = re.sub(r'K(?:THOUSAND)?', '', cleaned)

    # Handle percentages
    if '%' in cleaned:
        cleaned = cleaned.replace('%', '')
        # Don't apply multiplier to percentages
        multiplier = 1.0

    try:
        return float(cleaned.strip()) * multiplier
    except (ValueError, TypeError):
        return None

def get_metric_value_span(metric: Dict) -> Dict[str, Any]:
    """
    Return a numeric span for a metric to support range-aware canonical metrics.

    Output:
      {
        "min": float|None,
        "max": float|None,
        "mid": float|None,   # representative value (median if range, else parsed value)
        "unit": str,         # normalized (upper/stripped), preserves %/$ units if present
        "is_range": bool
      }
    """
    if not isinstance(metric, dict):
        return {"min": None, "max": None, "mid": None, "unit": "", "is_range": False}

    unit = (metric.get("unit") or "").strip()

    # If metric already has a range object, prefer it
    r = metric.get("range")
    if isinstance(r, dict):
        vmin = r.get("min")
        vmax = r.get("max")
        # ensure numeric
        try:
            vmin = float(vmin) if vmin is not None else None
        except Exception:
            pass
            vmin = None
        try:
            vmax = float(vmax) if vmax is not None else None
        except Exception:
            pass
            vmax = None

        # Representative = median of candidates if provided, else midpoint of min/max
        candidates = r.get("candidates")
        nums = []
        if isinstance(candidates, list):
            for c in candidates:
                try:
                    nums.append(float(c))
                except Exception:
                    pass
        if nums:
            nums_sorted = sorted(nums)
            mid = nums_sorted[len(nums_sorted) // 2]
        else:
            mid = None
            if vmin is not None and vmax is not None:
                mid = (vmin + vmax) / 2.0
            elif vmin is not None:
                mid = vmin
            elif vmax is not None:
                mid = vmax

        return {
            "min": vmin,
            "max": vmax,
            "mid": mid,
            "unit": unit,
            "is_range": True
        }

    # Non-range metric: parse single numeric value
    val = parse_to_float(metric.get("value"))
    return {
        "min": val,
        "max": val,
        "mid": val,
        "unit": unit,
        "is_range": False
    }


def spans_overlap(a: Dict[str, Any], b: Dict[str, Any], rel_tol: float = 0.05) -> bool:
    """
    Decide whether two spans overlap "enough" to be considered stable.
    rel_tol provides a small widening to avoid false drift from rounding.
    """
    if not a or not b:
        return False
    a_min, a_max = a.get("min"), a.get("max")
    b_min, b_max = b.get("min"), b.get("max")

    if a_min is None or a_max is None or b_min is None or b_max is None:
        return False

    # Widen spans slightly
    a_pad = max(abs(a_max), abs(a_min), 1.0) * rel_tol
    b_pad = max(abs(b_max), abs(b_min), 1.0) * rel_tol

    a_min2, a_max2 = a_min - a_pad, a_max + a_pad
    b_min2, b_max2 = b_min - b_pad, b_max + b_pad

    return not (a_max2 < b_min2 or b_max2 < a_min2)


def compute_percent_change(old_val: Optional[float], new_val: Optional[float]) -> Optional[float]:
    """
    Compute percent change. Returns None if either value is None or old is 0.
    """
    if old_val is None or new_val is None:
        return None
    if old_val == 0:
        return None if new_val == 0 else float('inf')
    return round(((new_val - old_val) / abs(old_val)) * 100, 2)

# ------------------------------------
# NAME MATCHING (DETERMINISTIC)
# ------------------------------------

def normalize_name(name: str) -> str:
    """Normalize name for matching"""
    if not name:
        return ""
    n = name.lower().strip()
    n = re.sub(r'[^\w\s]', '', n)
    n = re.sub(r'\s+', ' ', n)
    return n

def name_similarity(name1: str, name2: str) -> float:
    """Compute similarity ratio between two names (0-1)"""
    n1 = normalize_name(name1)
    n2 = normalize_name(name2)
    if not n1 or not n2:
        return 0.0
    if n1 == n2:
        return 1.0
    # Check containment
    if n1 in n2 or n2 in n1:
        return 0.9
    # Sequence matcher
    return difflib.SequenceMatcher(None, n1, n2).ratio()

def find_best_match(name: str, candidates: List[str], threshold: float = 0.7) -> Optional[str]:
    """Find best matching name from candidates"""
    best_match = None
    best_score = threshold
    for candidate in candidates:
        score = name_similarity(name, candidate)
        if score > best_score:
            best_score = score
            best_match = candidate
    return best_match

# =========================================================
# DETERMINISTIC QUERY STRUCTURE EXTRACTION
# - Classify query into a known category (country / industry / etc.)
# - Extract main question + "side questions" deterministically
# - Optional: spaCy dependency parse (if installed)
# - Optional: embedding similarity (if sentence-transformers/sklearn installed)
# =========================================================

SIDE_CONNECTOR_PATTERNS = [
    r"\bimpact of\b",
    r"\beffect of\b",
    r"\binfluence of\b",
    r"\brole of\b",
    r"\bdriven by\b",
    r"\bcaused by\b",
    r"\bdue to\b",
    r"\bincluding\b",
    r"\bincluding but not limited to\b",
    r"\bwith a focus on\b",
    r"\bespecially\b",
    r"\bnotably\b",
    r"\bplus\b",
    r"\bas well as\b",
    r"\band also\b",
    r"\bvs\b",
    r"\bversus\b",
]

QUESTION_CATEGORIES = {
    "country": {
        "signals": [
            "gdp", "gdp per capita", "population", "inflation", "interest rate",
            "exports", "imports", "trade balance", "currency", "fx", "central bank",
            "unemployment", "fiscal", "budget", "debt", "sovereign", "country"
        ],
        "template_sections": [
            "GDP & GDP per capita", "Growth rates", "Population & demographics",
            "Key industries", "Exports & imports", "Currency & FX trends",
            "Interest rates & inflation", "Risks & outlook"
        ],
    },
    "industry": {
        "signals": [
            "market size", "tam", "cagr", "industry", "sector", "market",
            "key players", "competitive landscape", "drivers", "challenges",
            "regulation", "technology trends", "forecast"
        ],
        "template_sections": [
            "Total Addressable Market (TAM) / Market size", "Growth rates (CAGR/YoY)",
            "Key players", "Key drivers", "Challenges & risks",
            "Technology trends", "Regulatory / environmental factors", "Outlook"
        ],
    },
    "company": {
        "signals": [
            "revenue", "earnings", "profit", "margins", "guidance",
            "business model", "segments", "customers", "competitors",
            "valuation", "multiple", "pe ratio", "cash flow"
        ],
        "template_sections": [
            "Business overview", "Revenue / profitability", "Segments",
            "Competitive position", "Key risks", "Guidance / outlook"
        ],
    },
    "unknown": {
        "signals": [],
        "template_sections": [],
    }
}

def _normalize_q(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip())

def _cleanup_clause(text: str) -> str:
    t = _normalize_q(text)
    t = re.sub(r"^[,;:\-\s]+", "", t)
    t = re.sub(r"[,;:\-\s]+$", "", t)
    return t

def detect_query_category(query: str) -> Dict[str, Any]:
    """
    Deterministically classify query category using keyword signals.
    Returns: {"category": "...", "confidence": 0-1, "matched_signals": [...]}
    """
    q = (query or "").lower()
    best_cat = "unknown"
    best_hits = 0
    best_matched = []

    for cat, cfg in QUESTION_CATEGORIES.items():
        if cat == "unknown":
            continue
        matched = [s for s in cfg["signals"] if s in q]
        hits = len(matched)
        if hits > best_hits:
            best_hits = hits
            best_cat = cat
            best_matched = matched

    # simple confidence: saturate after ~6 hits
    conf = min(best_hits / 6.0, 1.0) if best_hits > 0 else 0.0
    return {"category": best_cat, "confidence": round(conf, 2), "matched_signals": best_matched[:8]}

# =========================================================
# 3A+. LAYERED QUERY STRUCTURE PARSER (Deterministic -> NLP -> Embeddings -> LLM fallback)
# =========================================================

_QUERY_SPLIT_PATTERNS = [
    r"\bas well as\b",
    r"\balong with\b",
    r"\bin addition to\b",
    r"\band\b",
    r"\bplus\b",
    r"\bvs\.?\b",
    r"\bversus\b",
    r",",
    r";",
]

_COUNTRY_OVERVIEW_SIGNALS = [
    "in general", "overview", "tell me about", "general", "profile", "facts about",
    "economy", "population", "gdp", "currency", "exports", "imports",
]

def _normalize_q(q: str) -> str:
    q = (q or "").strip()
    q = re.sub(r"\s+", " ", q)
    return q

def _split_clauses_deterministic(q: str) -> List[str]:
    """
    Deterministically split a question into ordered clauses.

    Supports:
    - comma/connector splits (",", "and", "as well as", "in addition to", etc.)
    - multi-side enumerations like:
        "in addition to: 1. X 2. Y"
        "including: (1) X (2) Y"
        "as well as: • X • Y"
    """
    if not isinstance(q, str):
        return []

    s = q.strip()
    if not s:
        return []

    # Normalize whitespace early (keep original casing if present; upstream may lowercase already)
    s = re.sub(r"\s+", " ", s).strip()

    # --- Step A: If there's an enumeration intro, split head vs tail ---
    # Examples: "in addition to:", "including:", "plus:", "as well as:"
    enum_intro = re.search(
        r"\b(in addition to|in addition|including|in addition to the following|as well as|plus)\b\s*:?\s*",
        s,
        flags=re.IGNORECASE,
    )

    head = s
    tail = ""

    if enum_intro:
        # Split at the FIRST occurrence of the enum phrase
        idx = enum_intro.start()
        # head is everything before the phrase if it exists, otherwise keep whole string
        # but we usually want "Tell me about X in general" to remain in head.
        head = s[:idx].strip().rstrip(",")
        tail = s[enum_intro.end():].strip()

        # If head is empty (e.g., query begins with "In addition to:"), treat everything as head
        if not head:
            head = s
            tail = ""

    clauses: List[str] = []

    # --- Step B: Split head using your existing connector patterns ---
    if head:
        parts = [head]
        for pat in _QUERY_SPLIT_PATTERNS:
            next_parts = []
            for p in parts:
                next_parts.extend(re.split(pat, p, flags=re.IGNORECASE))
            parts = next_parts

        for p in parts:
            p = p.strip(" ,;:.").strip()
            if p:
                clauses.append(p)

    # --- Step C: If tail exists, split as enumerated items/bullets ---
    if tail:
        # Split on "1.", "1)", "(1)", "•", "-", "*"
        # Keep it robust: find item starts, then slice.
        item_start = re.compile(r"(?:^|\s)(?:\(?\d+\)?[\.\)]|[•\-\*])\s+", flags=re.IGNORECASE)
        starts = [m.start() for m in item_start.finditer(tail)]

        if starts:
            # Build slices using detected starts
            spans = []
            for i, st0 in enumerate(starts):
                st = st0
                # Move start to the start of token (strip leading whitespace)
                while st < len(tail) and tail[st].isspace():
                    st += 1
                en = starts[i + 1] if i + 1 < len(starts) else len(tail)
                spans.append((st, en))

            for st, en in spans:
                item = tail[st:en].strip(" ,;:.").strip()
                # Remove the leading bullet/number token again (safety)
                item = re.sub(r"^\(?\d+\)?[\.\)]\s+", "", item)
                item = re.sub(r"^[•\-\*]\s+", "", item)
                item = item.strip(" ,;:.").strip()
                if item:
                    clauses.append(item)
        else:
            # If tail doesn't look enumerated, fall back to normal splitter on tail
            parts = [tail]
            for pat in _QUERY_SPLIT_PATTERNS:
                next_parts = []
                for p in parts:
                    next_parts.extend(re.split(pat, p, flags=re.IGNORECASE))
                parts = next_parts

            for p in parts:
                p = p.strip(" ,;:.").strip()
                if p:
                    clauses.append(p)

    # Final cleanup + dedupe while preserving order
    out: List[str] = []
    seen = set()
    for c in clauses:
        c2 = c.strip()
        if not c2:
            continue
        if c2.lower() in seen:
            continue
        seen.add(c2.lower())
        out.append(c2)

    return out



def _dedupe_clauses(clauses: List[str]) -> List[str]:
    seen = set()
    out = []
    for c in clauses:
        c2 = c.strip().lower()
        if not c2 or c2 in seen:
            continue
        seen.add(c2)
        out.append(c.strip())
    return out

def _choose_main_and_side(clauses: List[str]) -> Tuple[str, List[str]]:
    """
    Pick 'main' as the first clause; side = remainder.
    Deterministic, stable across runs.
    """
    clauses = _dedupe_clauses(clauses)
    if not clauses:
        return "", []
    main = clauses[0]
    side = clauses[1:]
    return main, side

def _try_spacy_nlp():
    """
    Optional NLP layer. If spaCy is installed, use it; otherwise return None.
    """
    try:
        import spacy  # type: ignore
        # Avoid heavy model loading; prefer blank model with sentencizer if no model available.
        try:
            nlp = spacy.load("en_core_web_sm")  # common if installed
        except Exception:
            pass
            nlp = spacy.blank("en")
            if "sentencizer" not in nlp.pipe_names:
                nlp.add_pipe("sentencizer")
        return nlp
    except Exception:
        return None

def _nlp_refine_clauses(query: str, clauses: List[str]) -> Dict[str, Any]:
    """
    Use dependency/NER cues to:
      - detect country-overview questions
      - improve main-vs-side decision (coordination / 'as well as' patterns)
    Returns partial overrides: {"main":..., "side":[...], "hints":{...}}
    """
    nlp = _try_spacy_nlp()
    if not nlp:
        return {"hints": {"nlp_used": False}}

    doc = nlp(_normalize_q(query))
    # Named entities that look like places
    gpes = [ent.text for ent in getattr(doc, "ents", []) if ent.label_ in ("GPE", "LOC")]
    gpes_norm = [g.strip() for g in gpes if g and len(g.strip()) > 1]

    # Coordination hint: if query has "as well as" or "and", keep deterministic split,
    # but try to pick the more "general" clause as main when overview signals exist.
    overview_hit = any(sig in (query or "").lower() for sig in _COUNTRY_OVERVIEW_SIGNALS)
    hints = {
        "nlp_used": True,
        "gpe_entities": gpes_norm[:5],
        "overview_signal_hit": bool(overview_hit),
    }

    main, side = _choose_main_and_side(clauses)

    # If overview signals + place entity present, bias main to the overview clause
    if overview_hit and gpes_norm:
        # choose clause with strongest overview signal density
        def score_clause(c: str) -> int:
            c = c.lower()
            return sum(1 for sig in _COUNTRY_OVERVIEW_SIGNALS if sig in c)
        scored = sorted([(score_clause(c), c) for c in clauses], key=lambda x: (-x[0], x[1]))
        if scored and scored[0][0] > 0:
            main = scored[0][1]
            side = [c for c in clauses if c != main]

    return {"main": main, "side": side, "hints": hints}

def _embedding_category_vote(query: str) -> Dict[str, Any]:
    """
    Deterministic 'embedding-like' similarity using TF-IDF (no external model downloads).
    Produces a category suggestion + confidence based on similarity to category descriptors.
    """
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore
        from sklearn.metrics.pairwise import cosine_similarity  # type: ignore
    except Exception:
        return {"category": "unknown", "confidence": 0.0, "method": "tfidf_unavailable"}

    q = _normalize_q(query).lower()
    if not q:
        return {"category": "unknown", "confidence": 0.0, "method": "tfidf_empty"}

    # Build deterministic descriptors from your registry
    cat_texts = []
    cat_names = []
    for cat, cfg in (QUESTION_CATEGORIES or {}).items():
        if not isinstance(cfg, dict) or cat == "unknown":
            continue
        signals = " ".join(cfg.get("signals", [])[:50])
        sections = " ".join((cfg.get("template_sections", []) or [])[:50])
        descriptor = f"{cat} {signals} {sections}".strip()
        if descriptor:
            cat_names.append(cat)
            cat_texts.append(descriptor)

    if not cat_texts:
        return {"category": "unknown", "confidence": 0.0, "method": "tfidf_no_registry"}

    vec = TfidfVectorizer(ngram_range=(1, 2), min_df=1, max_features=8000)
    X = vec.fit_transform(cat_texts + [q])
    sims = cosine_similarity(X[-1], X[:-1]).flatten()

    best_idx = int(sims.argmax()) if sims.size else 0
    best_sim = float(sims[best_idx]) if sims.size else 0.0
    best_cat = cat_names[best_idx] if cat_names else "unknown"

    # Map cosine similarity (~0-1) into a conservative confidence
    conf = max(0.0, min(best_sim / 0.35, 1.0))  # 0.35 sim ~= "high"
    return {"category": best_cat, "confidence": round(conf, 2), "method": "tfidf"}

def _llm_fallback_query_structure(query: str, web_context: Optional[Dict] = None) -> Optional[Dict[str, Any]]:
    """
    Last resort: ask LLM to output ONLY a small JSON query-structure object.
    Guardrail: do NOT let the LLM invent extra side questions unless the user explicitly enumerated them.
    This path must NOT validate against LLMResponse.
    """
    try:
        q = str(query or "").strip()
        if not q:
            return None

        # --- Detect explicit enumeration / list structure in the USER query ---
        # If the user wrote a list (1., 2), bullets, etc.), it's reasonable to accept multiple sides.
        enum_patterns = [
            r"(^|\n)\s*\d+\s*[\.\)]\s+",     # 1.  / 2)
            r"(^|\n)\s*[-•*]\s+",           # - item / • item
            r"(^|\n)\s*[a-zA-Z]\s*[\.\)]\s+"  # a) / b. etc.
        ]
        has_explicit_enumeration = any(re.search(p, q, flags=re.MULTILINE) for p in enum_patterns)

        # Deterministic baseline (what the system already extracted)
        # We use this to clamp LLM hallucinations.
        det_clauses = _split_clauses_deterministic(_normalize_q(q))
        det_main, det_side = _choose_main_and_side(det_clauses)
        det_side = _dedupe_clauses([s.strip() for s in (det_side or []) if isinstance(s, str) and s.strip()])

        prompt = (
            "Extract a query structure.\n"
            "Return ONLY valid JSON with keys:\n"
            "  category: one of [country, industry, company, finance, market, unknown]\n"
            "  category_confidence: number 0-1\n"
            "  main: string (the main question/topic)\n"
            "  side: array of strings (side questions)\n"
            "No extra keys, no commentary.\n\n"
            f"Query: {q}"
        )

        raw = query_perplexity_raw(prompt, max_tokens=250, timeout=30)

        # Parse
        if isinstance(raw, dict):
            parsed = raw
        else:
            if raw is None:
                raw = ""
            if not isinstance(raw, str):
                raw = str(raw)
            parsed = parse_json_safely(raw, "LLM Query Structure")

        if not isinstance(parsed, dict) or parsed.get("main") is None:
            return None

        # --- Clean/normalize fields ---
        llm_main = str(parsed.get("main") or "").strip()
        llm_side = parsed.get("side") if isinstance(parsed.get("side"), list) else []
        llm_side = [str(s).strip() for s in llm_side if s is not None and str(s).strip()]

        # Reject "invented" side items that look like generic outline bullets
        # (Only apply this rejection when the user did NOT explicitly enumerate a list.)
        def _looks_like_outline_item(s: str) -> bool:
            s2 = s.lower().strip()
            bad_starts = (
                "overview", "key", "key stats", "statistics", "major statistics",
                "policies", "infrastructure", "recent trends", "post-covid", "covid",
                "challenges", "opportunities", "drivers", "headwinds",
                "background", "introduction"
            )
            return any(s2.startswith(b) for b in bad_starts)

        # --- Guardrail policy ---
        # If user didn't enumerate, do NOT accept LLM expansion of side questions.
        if not has_explicit_enumeration:
            # Keep deterministic sides only. (You can allow 1 LLM side if deterministic found none.)
            final_side = det_side
            if not final_side and llm_side:
                # Allow at most one side item as a fallback, but avoid outline-like additions.
                cand = llm_side[0]
                final_side = [] if _looks_like_outline_item(cand) else [cand]
        else:
            # User enumerated: accept multiple sides, but still de-dupe and keep deterministic items first
            merged = []
            for s in (det_side + llm_side):
                s = str(s).strip()
                if not s:
                    continue
                if s not in merged:
                    merged.append(s)
            final_side = merged

        # If LLM main is empty or fragment-y, keep deterministic main
        bad_prefixes = ("as well as", "as well", "and ", "also ", "plus ", "as for ")
        if not llm_main or any(llm_main.lower().startswith(p) for p in bad_prefixes):
            llm_main = (det_main or "").strip()

        # Return only allowed keys
        out = {
            "category": parsed.get("category", "unknown") or "unknown",
            "category_confidence": parsed.get("category_confidence", 0.0),
            "main": llm_main,
            "side": final_side,
        }
        return out

    except Exception:
        return None


def _split_side_candidates(query: str) -> List[str]:
    """
    Deterministic splitting into clause candidates.
    We keep it conservative to avoid over-splitting.
    """
    q = _normalize_q(query)
    # Pull quoted strings as strong side-topic candidates
    quoted = re.findall(r"['\"]([^'\"]{2,80})['\"]", q)
    q_wo_quotes = re.sub(r"['\"][^'\"]{2,80}['\"]", " ", q)

    # Split on major separators
    parts = re.split(r"[;]|(?:\s+-\s+)|(?:\s+—\s+)", q_wo_quotes)
    parts = [p for p in (_cleanup_clause(x) for x in parts) if p]

    # Further split on side connectors
    connector_re = "(" + "|".join(SIDE_CONNECTOR_PATTERNS) + ")"
    expanded = []
    for p in parts:
        # break into chunks but keep connector words in-place by splitting into sentences first
        sub = re.split(r"\.\s+|\?\s+|\!\s+", p)
        for s in sub:
            s = _cleanup_clause(s)
            if not s:
                continue
            # if contains connector, split into [before, after...] using first connector
            m = re.search(connector_re, s.lower())
            if m:
                idx = m.start()
                before = _cleanup_clause(s[:idx])
                after = _cleanup_clause(s[idx:])
                if before:
                    expanded.append(before)
                if after:
                    expanded.append(after)
            else:
                expanded.append(s)

    # Add quoted items as standalone candidates (often side topics)
    for qitem in quoted:
        cleaned = _cleanup_clause(qitem)
        if cleaned:
            expanded.append(cleaned)

    # De-dupe while preserving order (deterministic)
    seen = set()
    out = []
    for x in expanded:
        k = x.lower()
        if k not in seen:
            seen.add(k)
            out.append(x)
    return out

def _extract_spacy_side_topics(query: str) -> List[str]:
    """
    Optional: use spaCy dependency parse if available.
    Extracts objects of 'impact/effect/role/influence' patterns.
    """
    try:
        import spacy  # type: ignore
        try:
            nlp = spacy.load("en_core_web_sm")  # type: ignore
        except Exception:
            return []
    except Exception:
        return []

    doc = nlp(query)
    side = []

    triggers = {"impact", "effect", "influence", "role"}
    for token in doc:
        if token.lemma_.lower() in triggers:
            # Look for "of X" attached to trigger
            for child in token.children:
                if child.dep_ == "prep" and child.text.lower() == "of":
                    pobj = next((c for c in child.children if c.dep_ in ("pobj", "dobj", "obj")), None)
                    if pobj is not None:
                        # take subtree as phrase
                        phrase = " ".join(t.text for t in pobj.subtree)
                        phrase = _cleanup_clause(phrase)
                        if phrase and phrase.lower() not in (s.lower() for s in side):
                            side.append(phrase)
    return side[:5]

def _embedding_similarity(a: str, b: str) -> Optional[float]:
    """
    Optional: compute cosine similarity using:
      - sentence-transformers (preferred) OR
      - sklearn TF-IDF fallback
    Returns None if unavailable.
    """
    a = _normalize_q(a)
    b = _normalize_q(b)
    if not a or not b:
        return None

    # 1) sentence-transformers (if installed)
    try:
        from sentence_transformers import SentenceTransformer  # type: ignore
        import numpy as np  # type: ignore
        model = SentenceTransformer("all-MiniLM-L6-v2")
        emb = model.encode([a, b], normalize_embeddings=True)
        sim = float(np.dot(emb[0], emb[1]))
        return max(min(sim, 1.0), -1.0)
    except Exception:
        pass

    # 2) sklearn TF-IDF cosine similarity (deterministic)
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore
        from sklearn.metrics.pairwise import cosine_similarity  # type: ignore
        vec = TfidfVectorizer(stop_words="english")
        X = vec.fit_transform([a, b])
        sim = float(cosine_similarity(X[0], X[1])[0, 0])
        return max(min(sim, 1.0), -1.0)
    except Exception:
        return None

def extract_query_structure(query: str) -> Dict[str, Any]:
    """
    Layered query structure extraction:
      1) Deterministic clause split -> main/side
      2) Deterministic category from keyword signals (detect_query_category)
      3) Optional NLP refinement (spaCy if available)
      4) Deterministic similarity vote (TF-IDF)
      5) LLM fallback ONLY if confidence remains low
    """
    q = _normalize_q(query)
    clauses = _split_clauses_deterministic(q)
    main, side = _choose_main_and_side(clauses)

    # --- Layer 1: deterministic keyword category ---
    det_cat = detect_query_category(q)
    category = det_cat.get("category", "unknown")
    cat_conf = float(det_cat.get("confidence", 0.0))

    debug = {
        "deterministic": {
            "clauses": clauses,
            "main": main,
            "side": side,
            "category": category,
            "confidence": cat_conf,
            "matched_signals": det_cat.get("matched_signals", []),
        }
    }

    # --- Layer 2: NLP refinement (optional) ---
    nlp_out = _nlp_refine_clauses(q, clauses)
    if isinstance(nlp_out, dict):
        hints = nlp_out.get("hints", {})
        debug["nlp"] = hints or {"nlp_used": False}

        # Override main/side if NLP produced them (guard against fragment-y mains)
        nlp_main = (nlp_out.get("main") or "").strip()
        if nlp_main:
            bad_prefixes = ("as well as", "as well", "and ", "also ", "plus ", "as for ")
            if not any(nlp_main.lower().startswith(p) for p in bad_prefixes):
                main = nlp_main

        if isinstance(nlp_out.get("side"), list):
            side = nlp_out["side"]

        # If NLP detects a place + overview cue, bias to "country"
        gpes = (hints or {}).get("gpe_entities", []) if isinstance(hints, dict) else []
        overview_hit = (hints or {}).get("overview_signal_hit", False) if isinstance(hints, dict) else False
        if overview_hit and gpes and cat_conf < 0.45:
            category = "country"
            cat_conf = max(cat_conf, 0.55)

    # --- Layer 3: embedding-style category vote ---
    emb_vote = _embedding_category_vote(q)
    debug["similarity_vote"] = emb_vote

    emb_cat = emb_vote.get("category", "unknown")
    emb_conf = float(emb_vote.get("confidence", 0.0))

    if cat_conf < 0.40 and emb_cat != "unknown" and emb_conf >= 0.45:
        category = emb_cat
        cat_conf = max(cat_conf, min(0.75, emb_conf))

    # --- Layer 4: LLM fallback if still ambiguous ---
    if cat_conf < 0.30:
        llm = _llm_fallback_query_structure(q)
        debug["llm_fallback_used"] = bool(llm)

        if isinstance(llm, dict):
            category = llm.get("category", category) or category
            try:
                cat_conf = float(llm.get("category_confidence", cat_conf))
            except Exception:
                pass

            llm_main = (llm.get("main") or "").strip()
            llm_side = llm.get("side") if isinstance(llm.get("side"), list) else []

            det_main = (main or "").strip()
            det_side = side or []

            def _overview_score(s: str) -> int:
                if not s:
                    return 0
                s2 = s.lower()
                signals = [
                    "in general", "overview", "background", "basic facts",
                    "at a glance", "tell me about", "describe", "introduction"
                ]
                return sum(1 for sig in signals if sig in s2)

            def _is_bad_main(s: str) -> bool:
                if not s or len(s) < 8:
                    return True
                return s.lower().startswith(
                    ("as well as", "as well", "and ", "also ", "plus ", "as for ")
                )

            merged_side = []
            for s in det_side + llm_side:
                s = str(s).strip()
                if s and s not in merged_side:
                    merged_side.append(s)

            det_score = _overview_score(det_main)
            llm_score = _overview_score(llm_main)

            if llm_main and not _is_bad_main(llm_main):
                if not det_main or llm_score > det_score:
                    main = llm_main

            side = merged_side

    side = _dedupe_clauses([s.strip() for s in (side or []) if s.strip()])

    return {
        "category": category or "unknown",
        "category_confidence": round(max(0.0, min(cat_conf, 1.0)), 2),
        "main": (main or "").strip(),
        "side": side,
        "debug": debug,
    }


def format_query_structure_for_prompt(qs: Optional[Dict[str, Any]]) -> str:
    if not qs or not isinstance(qs, dict):
        return ""

    parts = []
    parts.append("STRUCTURED QUESTION (DETERMINISTIC):")
    parts.append(f"- Category: {qs.get('category','unknown')} (conf {qs.get('category_confidence','')})")
    parts.append(f"- Main (answer this FIRST): {qs.get('main','')}")
    side = qs.get("side") or []

    if side:
        parts.append("- Side questions (answer AFTER main, in this exact order):")
        for i, s in enumerate(side[:10], 1):
            parts.append(f"  {i}. {s}")

    tmpl = qs.get("template_sections") or []
    if tmpl:
        parts.append("- Recommended response sections (use as headings if helpful):")
        for t in tmpl[:10]:
            parts.append(f"  - {t}")

    # Hard behavioral instruction to the LLM (kept short and explicit)
    parts.append(
        "RESPONSE RULES:\n"
        "1) Start by answering the MAIN request with general context.\n"
        "2) Then answer EACH side question explicitly (label them).\n"
        "3) Metrics/findings can include both main + side, but do not ignore the main.\n"
        "4) If you provide tourism/industry metrics, ALSO provide basic country/overview facts when main is an overview."
    )

    return "\n".join(parts).strip()


# ------------------------------------
# METRIC DIFF COMPUTATION
# ------------------------------------

def compute_metric_diffs(old_metrics: Dict, new_metrics: Dict) -> List[MetricDiff]:
    """
    Compute deterministic diffs between metric dictionaries.
    Returns list of MetricDiff objects.
    """
    diffs = []
    matched_new_keys = set()

    # Build lookup for new metrics by normalized name
    new_by_name = {}
    for key, m in new_metrics.items():
        if isinstance(m, dict):
            name = m.get('name', key)
            new_by_name[normalize_name(name)] = (key, m)

    # Process old metrics
    for old_key, old_m in old_metrics.items():
        if not isinstance(old_m, dict):
            continue

        old_name = old_m.get('name', old_key)
        old_raw = str(old_m.get('value', ''))
        old_unit = old_m.get('unit', '')
        old_val = parse_to_float(old_m.get('value'))

        # Find matching new metric
        norm_name = normalize_name(old_name)
        match = new_by_name.get(norm_name)

        if not match:
            # Try fuzzy matching
            best = find_best_match(old_name, [m.get('name', k) for k, m in new_metrics.items() if isinstance(m, dict)])
            if best:
                for k, m in new_metrics.items():
                    if isinstance(m, dict) and m.get('name', k) == best:
                        match = (k, m)
                        break

        if match:
            new_key, new_m = match
            matched_new_keys.add(new_key)

            new_raw = str(new_m.get('value', ''))
            new_val = parse_to_float(new_m.get('value'))
            new_unit = new_m.get('unit', old_unit)

            change_pct = compute_percent_change(old_val, new_val)

            # Determine change type
            if change_pct is None:
                change_type = 'unchanged'
            elif abs(change_pct) < 0.5:  # Less than 0.5% change = unchanged
                change_type = 'unchanged'
            elif change_pct > 0:
                change_type = 'increased'
            else:
                change_type = 'decreased'

            diffs.append(MetricDiff(
                name=old_name,
                old_value=old_val,
                new_value=new_val,
                old_raw=old_raw,
                new_raw=new_raw,
                unit=new_unit or old_unit,
                change_pct=change_pct,
                change_type=change_type
            ))
        else:
            # Metric was removed
            diffs.append(MetricDiff(
                name=old_name,
                old_value=old_val,
                new_value=None,
                old_raw=old_raw,
                new_raw='',
                unit=old_unit,
                change_pct=None,
                change_type='removed'
            ))

    # Find added metrics
    for new_key, new_m in new_metrics.items():
        if new_key in matched_new_keys:
            continue
        if not isinstance(new_m, dict):
            continue

        new_name = new_m.get('name', new_key)
        new_raw = str(new_m.get('value', ''))
        new_val = parse_to_float(new_m.get('value'))
        new_unit = new_m.get('unit', '')

        diffs.append(MetricDiff(
            name=new_name,
            old_value=None,
            new_value=new_val,
            old_raw='',
            new_raw=new_raw,
            unit=new_unit,
            change_pct=None,
            change_type='added'
        ))

    return diffs

# ------------------------------------
# ENTITY DIFF COMPUTATION
# ------------------------------------

def compute_entity_diffs(old_entities: List, new_entities: List) -> List[EntityDiff]:
    """
    Compute deterministic diffs between entity rankings.
    """
    diffs = []

    # Build lookups with ranks
    old_lookup = {}
    for i, e in enumerate(old_entities):
        if isinstance(e, dict):
            name = normalize_name(e.get('name', ''))
            old_lookup[name] = {
                'rank': i + 1,
                'share': e.get('share'),
                'original_name': e.get('name', '')
            }

    new_lookup = {}
    for i, e in enumerate(new_entities):
        if isinstance(e, dict):
            name = normalize_name(e.get('name', ''))
            new_lookup[name] = {
                'rank': i + 1,
                'share': e.get('share'),
                'original_name': e.get('name', '')
            }

    # All unique names
    all_names = set(old_lookup.keys()) | set(new_lookup.keys())

    for norm_name in all_names:
        old_data = old_lookup.get(norm_name)
        new_data = new_lookup.get(norm_name)

        if old_data and new_data:
            # Entity exists in both
            rank_change = old_data['rank'] - new_data['rank']  # Positive = moved up

            if rank_change > 0:
                change_type = 'moved_up'
            elif rank_change < 0:
                change_type = 'moved_down'
            else:
                change_type = 'unchanged'

            diffs.append(EntityDiff(
                name=new_data['original_name'],
                old_rank=old_data['rank'],
                new_rank=new_data['rank'],
                old_share=old_data['share'],
                new_share=new_data['share'],
                rank_change=rank_change,
                change_type=change_type
            ))
        elif old_data:
            # Entity removed
            diffs.append(EntityDiff(
                name=old_data['original_name'],
                old_rank=old_data['rank'],
                new_rank=None,
                old_share=old_data['share'],
                new_share=None,
                rank_change=None,
                change_type='removed'
            ))
        else:
            # Entity added
            diffs.append(EntityDiff(
                name=new_data['original_name'],
                old_rank=None,
                new_rank=new_data['rank'],
                old_share=None,
                new_share=new_data['share'],
                rank_change=None,
                change_type='added'
            ))

    # Sort by new rank (added entities at end)
    diffs.sort(key=lambda x: x.new_rank if x.new_rank else 999)
    return diffs

# ------------------------------------
# FINDING DIFF COMPUTATION
# ------------------------------------

def compute_finding_diffs(old_findings: List[str], new_findings: List[str]) -> List[FindingDiff]:
    """
    Compute deterministic diffs between findings using text similarity.
    """
    diffs = []
    matched_new_indices = set()

    # Match old findings to new
    for old_f in old_findings:
        if not old_f:
            continue

        best_match_idx = None
        best_similarity = 0.5  # Minimum threshold

        for i, new_f in enumerate(new_findings):
            if i in matched_new_indices or not new_f:
                continue

            sim = name_similarity(old_f, new_f)  # Reuse name similarity for text
            if sim > best_similarity:
                best_similarity = sim
                best_match_idx = i

        if best_match_idx is not None:
            matched_new_indices.add(best_match_idx)
            similarity_pct = round(best_similarity * 100, 1)

            if similarity_pct >= 90:
                change_type = 'retained'
            else:
                change_type = 'modified'

            diffs.append(FindingDiff(
                old_text=old_f,
                new_text=new_findings[best_match_idx],
                similarity=similarity_pct,
                change_type=change_type
            ))
        else:
            # Finding removed
            diffs.append(FindingDiff(
                old_text=old_f,
                new_text=None,
                similarity=0,
                change_type='removed'
            ))

    # Find added findings
    for i, new_f in enumerate(new_findings):
        if i in matched_new_indices or not new_f:
            continue

        diffs.append(FindingDiff(
            old_text=None,
            new_text=new_f,
            similarity=0,
            change_type='added'
        ))

    return diffs

# =========================================================
# 8C. DETERMINISTIC SOURCE EXTRACTION
# Extract metrics/entities directly from web snippets - NO LLM
# =========================================================

def extract_metrics_from_sources(web_context: Dict) -> Dict:
    """
    Extract numeric metrics directly from web search snippets.
    100% deterministic - no LLM involved.
    """
    extracted = {}
    search_results = web_context.get("search_results", [])

    # Patterns to match common metric formats
    patterns = [
        # Market size patterns
        (r'\$\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)\b', 'market_size'),
        (r'market\s+size[:\s]+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'market_size'),
        (r'valued\s+at\s+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'market_size'),
        (r'worth\s+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'market_size'),

        # Growth rate patterns
        (r'CAGR[:\s]+of?\s*(\d+(?:\.\d+)?)\s*%', 'cagr'),
        (r'(\d+(?:\.\d+)?)\s*%\s*CAGR', 'cagr'),
        (r'grow(?:th|ing)?\s+(?:at\s+)?(\d+(?:\.\d+)?)\s*%', 'growth_rate'),

        # Revenue patterns
        (r'revenue[:\s]+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'revenue'),

        # Year-specific values
        (r'(?:in\s+)?20\d{2}[:\s]+\$?\s*(\d+(?:\.\d+)?)\s*(trillion|billion|million|T|B|M)', 'year_value'),
    ]

    all_matches = []

    for result in search_results:
        snippet = result.get("snippet", "")
        title = result.get("title", "")
        source = result.get("source", "")
        text = f"{title} {snippet}".lower()

        for pattern, metric_type in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    value_str, unit = match[0], match[1] if len(match) > 1 else ''
                else:
                    value_str, unit = match, ''

                try:
                    value = float(value_str)

                    # Normalize unit
                    unit_lower = unit.lower() if unit else ''
                    if unit_lower in ['t', 'trillion']:
                        unit_normalized = 'T'
                        value_in_billions = value * 1000
                    elif unit_lower in ['b', 'billion']:
                        unit_normalized = 'B'
                        value_in_billions = value
                    elif unit_lower in ['m', 'million']:
                        unit_normalized = 'M'
                        value_in_billions = value / 1000
                    elif unit_lower == '%':
                        unit_normalized = '%'
                        value_in_billions = value  # Keep as-is for percentages
                    else:
                        unit_normalized = ''
                        value_in_billions = value

                    all_matches.append({
                        'type': metric_type,
                        'value': value,
                        'unit': unit_normalized,
                        'value_normalized': value_in_billions,
                        'source': source,
                        'raw': f"{value_str} {unit}".strip()
                    })
                except (ValueError, TypeError):
                    continue

    # Deduplicate and select best matches by type
    metrics_by_type = {}
    for match in all_matches:
        mtype = match['type']
        if mtype not in metrics_by_type:
            metrics_by_type[mtype] = []
        metrics_by_type[mtype].append(match)

    # For each type, take the most common value (mode) or median
    metric_counter = 0
    for mtype, matches in metrics_by_type.items():
        if not matches:
            continue

        # Group by similar values (within 10%)
        value_groups = []
        for m in matches:
            added = False
            for group in value_groups:
                if group and abs(m['value_normalized'] - group[0]['value_normalized']) / max(group[0]['value_normalized'], 0.001) < 0.1:
                    group.append(m)
                    added = True
                    break
            if not added:
                value_groups.append([m])

        # Take the largest group (most consensus)
        if value_groups:
            best_group = max(value_groups, key=len)
            representative = best_group[0]

            metric_counter += 1
            metric_key = f"extracted_{mtype}_{metric_counter}"

            # Map type to readable name
            type_names = {
                'market_size': 'Market Size',
                'cagr': 'CAGR',
                'growth_rate': 'Growth Rate',
                'revenue': 'Revenue',
                'year_value': 'Market Value'
            }

            extracted[metric_key] = {
                'name': type_names.get(mtype, mtype.replace('_', ' ').title()),
                'value': representative['value'],
                'unit': f"${representative['unit']}" if representative['unit'] in ['T', 'B', 'M'] else representative['unit'],
                'source_count': len(best_group),
                'sources': list(set(m['source'] for m in best_group))[:3]
            }

    return extracted


def extract_entities_from_sources(web_context: Dict) -> List[Dict]:
    """
    Extract company/entity names from web search snippets.
    100% deterministic - no LLM involved.
    """
    search_results = web_context.get("search_results", [])

    # Common market leaders that appear in financial contexts
    known_entities = [
        # Tech
        'apple', 'microsoft', 'google', 'alphabet', 'amazon', 'meta', 'facebook',
        'nvidia', 'tesla', 'intel', 'amd', 'qualcomm', 'broadcom', 'cisco',
        'ibm', 'oracle', 'salesforce', 'adobe', 'netflix', 'uber', 'airbnb',
        # Finance
        'jpmorgan', 'goldman sachs', 'morgan stanley', 'bank of america',
        'wells fargo', 'citigroup', 'blackrock', 'vanguard', 'fidelity',
        # Auto
        'toyota', 'volkswagen', 'ford', 'gm', 'general motors', 'honda',
        'bmw', 'mercedes', 'byd', 'nio', 'rivian', 'lucid',
        # Pharma
        'pfizer', 'johnson & johnson', 'roche', 'novartis', 'merck',
        'abbvie', 'eli lilly', 'astrazeneca', 'moderna', 'gilead',
        # Energy
        'exxon', 'chevron', 'shell', 'bp', 'totalenergies', 'conocophillips',
        # Consumer
        'walmart', 'costco', 'home depot', 'nike', 'starbucks', 'mcdonalds',
        'coca-cola', 'pepsi', 'procter & gamble', 'unilever',
        # Regions (for market share by region)
        'north america', 'europe', 'asia pacific', 'asia-pacific', 'apac',
        'china', 'united states', 'japan', 'germany', 'india', 'uk',
        'latin america', 'middle east', 'africa'
    ]

    entity_mentions = {}

    for result in search_results:
        snippet = result.get("snippet", "").lower()
        title = result.get("title", "").lower()
        text = f"{title} {snippet}"

        for entity in known_entities:
            if entity in text:
                # Try to extract market share if mentioned
                share_pattern = rf'{re.escape(entity)}[^.]*?(\d+(?:\.\d+)?)\s*%'
                share_match = re.search(share_pattern, text, re.IGNORECASE)

                share = None
                if share_match:
                    share = f"{share_match.group(1)}%"

                if entity not in entity_mentions:
                    entity_mentions[entity] = {'count': 0, 'shares': []}

                entity_mentions[entity]['count'] += 1
                if share:
                    entity_mentions[entity]['shares'].append(share)

    # Sort by mention count and build list
    sorted_entities = sorted(entity_mentions.items(), key=lambda x: x[1]['count'], reverse=True)

    entities = []
    for entity_name, data in sorted_entities[:10]:  # Top 10
        # Use most common share if available
        share = None
        if data['shares']:
            # Take the most common share value
            share_counts = Counter(data['shares'])
            share = share_counts.most_common(1)[0][0]

        entities.append({
            'name': entity_name.title(),
            'share': share,
            'growth': None,  # Can't reliably extract growth from snippets
            'mention_count': data['count']
        })

    return entities

# ------------------------------------
# STABILITY SCORE COMPUTATION
# ------------------------------------

def compute_stability_score(
    metric_diffs: List[MetricDiff],
    entity_diffs: List[EntityDiff],
    finding_diffs: List[FindingDiff]
) -> float:
    """
    Compute overall stability score (0-100).
    Higher = more stable (less change).
    """
    scores = []

    # Metric stability (40% weight)
    if metric_diffs:
        stable_metrics = sum(1 for m in metric_diffs if m.change_type == 'unchanged')
        small_change = sum(1 for m in metric_diffs if m.change_pct and abs(m.change_pct) < 10)
        metric_score = ((stable_metrics + small_change * 0.5) / len(metric_diffs)) * 100
        scores.append(('metrics', metric_score, 0.4))

    # Entity stability (35% weight)
    if entity_diffs:
        stable_entities = sum(1 for e in entity_diffs if e.change_type == 'unchanged')
        entity_score = (stable_entities / len(entity_diffs)) * 100
        scores.append(('entities', entity_score, 0.35))

    # Finding stability (25% weight)
    if finding_diffs:
        retained = sum(1 for f in finding_diffs if f.change_type in ['retained', 'modified'])
        finding_score = (retained / len(finding_diffs)) * 100
        scores.append(('findings', finding_score, 0.25))

    if not scores:
        return 100.0

    # Weighted average
    total_weight = sum(s[2] for s in scores)
    weighted_sum = sum(s[1] * s[2] for s in scores)
    return round(weighted_sum / total_weight, 1)

# ------------------------------------
# MAIN DIFF COMPUTATION
# ------------------------------------

def compute_evolution_diff(old_analysis: Dict, new_analysis: Dict) -> EvolutionDiff:
    """
    Main entry point: compute complete deterministic diff between two analyses.
    """
    old_response = old_analysis.get('primary_response', {})
    new_response = new_analysis.get('primary_response', {})

    # Timestamps
    old_ts = old_analysis.get('timestamp', '')
    new_ts = new_analysis.get('timestamp', '')

    # Calculate time delta
    time_delta = None
    try:
        old_dt = datetime.fromisoformat(old_ts.replace('Z', '+00:00'))
        new_dt = datetime.fromisoformat(new_ts.replace('Z', '+00:00'))
        time_delta = round((new_dt.replace(tzinfo=None) - old_dt.replace(tzinfo=None)).total_seconds() / 3600, 1)
    except:
        pass

    # Compute diffs using CANONICAL metric registry for stable matching
    metric_diffs = compute_metric_diffs_canonical(
        old_response.get('primary_metrics', {}),
        new_response.get('primary_metrics', {})
    )

    entity_diffs = compute_entity_diffs(
        old_response.get('top_entities', []),
        new_response.get('top_entities', [])
    )

    # Use SEMANTIC finding comparison (stable across wording changes)
    finding_diffs = compute_semantic_finding_diffs(
        old_response.get('key_findings', []),
        new_response.get('key_findings', [])
    )

    # Compute stability
    stability = compute_stability_score(metric_diffs, entity_diffs, finding_diffs)

    # Summary stats
    summary_stats = {
        'metrics_increased': sum(1 for m in metric_diffs if m.change_type == 'increased'),
        'metrics_decreased': sum(1 for m in metric_diffs if m.change_type == 'decreased'),
        'metrics_unchanged': sum(1 for m in metric_diffs if m.change_type == 'unchanged'),
        'metrics_added': sum(1 for m in metric_diffs if m.change_type == 'added'),
        'metrics_removed': sum(1 for m in metric_diffs if m.change_type == 'removed'),
        'entities_moved_up': sum(1 for e in entity_diffs if e.change_type == 'moved_up'),
        'entities_moved_down': sum(1 for e in entity_diffs if e.change_type == 'moved_down'),
        'entities_unchanged': sum(1 for e in entity_diffs if e.change_type == 'unchanged'),
        'entities_added': sum(1 for e in entity_diffs if e.change_type == 'added'),
        'entities_removed': sum(1 for e in entity_diffs if e.change_type == 'removed'),
        'findings_retained': sum(1 for f in finding_diffs if f.change_type == 'retained'),
        'findings_modified': sum(1 for f in finding_diffs if f.change_type == 'modified'),
        'findings_added': sum(1 for f in finding_diffs if f.change_type == 'added'),
        'findings_removed': sum(1 for f in finding_diffs if f.change_type == 'removed'),
    }

    return EvolutionDiff(
        old_timestamp=old_ts,
        new_timestamp=new_ts,
        time_delta_hours=time_delta,
        metric_diffs=metric_diffs,
        entity_diffs=entity_diffs,
        finding_diffs=finding_diffs,
        stability_score=stability,
        summary_stats=summary_stats
    )

# ------------------------------------
# LLM EXPLANATION (ONLY INTERPRETS DIFFS)
# ------------------------------------

def generate_diff_explanation_prompt(diff: EvolutionDiff, query: str) -> str:
    """
    Generate prompt for LLM to EXPLAIN computed diffs (not discover them).
    """
    # Build metric changes text
    metric_changes = []
    for m in diff.metric_diffs:
        if m.change_type == 'increased':
            metric_changes.append(f"- {m.name}: {m.old_raw} → {m.new_raw} ({m.change_pct:+.1f}%) INCREASED")
        elif m.change_type == 'decreased':
            metric_changes.append(f"- {m.name}: {m.old_raw} → {m.new_raw} ({m.change_pct:+.1f}%) DECREASED")
        elif m.change_type == 'added':
            metric_changes.append(f"- {m.name}: NEW metric added with value {m.new_raw}")
        elif m.change_type == 'removed':
            metric_changes.append(f"- {m.name}: REMOVED (was {m.old_raw})")

    # Build entity changes text
    entity_changes = []
    for e in diff.entity_diffs:
        if e.change_type == 'moved_up':
            entity_changes.append(f"- {e.name}: Rank {e.old_rank} → {e.new_rank} (moved UP {e.rank_change} positions)")
        elif e.change_type == 'moved_down':
            entity_changes.append(f"- {e.name}: Rank {e.old_rank} → {e.new_rank} (moved DOWN {abs(e.rank_change)} positions)")
        elif e.change_type == 'added':
            entity_changes.append(f"- {e.name}: NEW entrant at rank {e.new_rank}")
        elif e.change_type == 'removed':
            entity_changes.append(f"- {e.name}: DROPPED OUT (was rank {e.old_rank})")

    # Build findings changes text
    finding_changes = []
    for f in diff.finding_diffs:
        if f.change_type == 'added':
            finding_changes.append(f"- NEW: {f.new_text}")
        elif f.change_type == 'removed':
            finding_changes.append(f"- REMOVED: {f.old_text}")
        elif f.change_type == 'modified':
            finding_changes.append(f"- MODIFIED: '{f.old_text[:50]}...' → '{f.new_text[:50]}...'")

    prompt = f"""You are a market analyst explaining changes between two analysis snapshots.

    QUERY: {query}
    TIME ELAPSED: {diff.time_delta_hours:.1f} hours
    STABILITY SCORE: {diff.stability_score:.0f}%

    COMPUTED METRIC CHANGES:
    {chr(10).join(metric_changes) if metric_changes else "No significant metric changes"}

    COMPUTED ENTITY RANKING CHANGES:
    {chr(10).join(entity_changes) if entity_changes else "No ranking changes"}

    COMPUTED FINDING CHANGES:
    {chr(10).join(finding_changes) if finding_changes else "No finding changes"}

    SUMMARY STATS:
    - Metrics: {diff.summary_stats['metrics_increased']} increased, {diff.summary_stats['metrics_decreased']} decreased, {diff.summary_stats['metrics_unchanged']} unchanged
    - Entities: {diff.summary_stats['entities_moved_up']} moved up, {diff.summary_stats['entities_moved_down']} moved down
    - Findings: {diff.summary_stats['findings_added']} new, {diff.summary_stats['findings_removed']} removed

    YOUR TASK: Provide a 3-5 sentence executive interpretation of these changes.
    - What is the overall trend (improving/declining/stable)?
    - What are the most significant changes and why might they have occurred?
    - What should stakeholders pay attention to?

    Return ONLY a JSON object:
    {{
        "trend": "improving/declining/stable",
        "headline": "One sentence summary of key change",
        "interpretation": "3-5 sentence detailed interpretation",
        "watch_items": ["Item 1 to monitor", "Item 2 to monitor"]
    }}
    """
    return prompt

def get_llm_explanation(diff: EvolutionDiff, query: str) -> Dict:
    """
    Ask LLM to explain the computed diffs (not discover them).
    """
    prompt = generate_diff_explanation_prompt(diff, query)

    headers = {
        "Authorization": f"Bearer {PERPLEXITY_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "sonar",
        "temperature": 0.0,  # Deterministic
        "max_tokens": 500,
        "top_p": 1.0,
        "messages": [{"role": "user", "content": prompt}]
    }

    try:
        resp = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        content = data["choices"][0]["message"]["content"]

        parsed = parse_json_safely(content, "Explanation")
        if parsed:
            return parsed
    except Exception as e:
        st.warning(f"LLM explanation failed: {e}")

    # Fallback
    return {
        "trend": "stable" if diff.stability_score >= 70 else "changing",
        "headline": f"Analysis shows {diff.stability_score:.0f}% stability over {diff.time_delta_hours:.0f} hours",
        "interpretation": "Unable to generate detailed interpretation.",
        "watch_items": []
    }


# =========================================================
# 8B. EVOLUTION DASHBOARD RENDERING
# =========================================================

def render_evolution_results(diff: EvolutionDiff, explanation: Dict, query: str):
    """Render deterministic evolution results"""

    st.header("📈 Evolution Analysis")
    st.markdown(f"**Query:** {query}")

    # Overview metrics
    col1, col2, col3, col4 = st.columns(4)

    if diff.time_delta_hours:
        if diff.time_delta_hours < 24:
            time_str = f"{diff.time_delta_hours:.1f}h"
        else:
            time_str = f"{diff.time_delta_hours/24:.1f}d"
        col1.metric("Time Elapsed", time_str)
    else:
        col1.metric("Time Elapsed", "Unknown")

    col2.metric("Stability", f"{diff.stability_score:.0f}%")

    trend = explanation.get('trend', 'stable')
    trend_icon = {'improving': '📈', 'declining': '📉', 'stable': '➡️'}.get(trend, '➡️')
    col3.metric("Trend", f"{trend_icon} {trend.title()}")

    # Stability indicator
    if diff.stability_score >= 80:
        col4.success("🟢 Highly Stable")
    elif diff.stability_score >= 60:
        col4.warning("🟡 Moderate Changes")
    else:
        col4.error("🔴 Significant Drift")

    # Headline
    st.info(f"**{explanation.get('headline', 'Analysis complete')}**")

    st.markdown("---")

    # Interpretation

    # =====================================================================
    # PATCH FIX39 (ADDITIVE): enforce unit-required gate at render time
    # =====================================================================
    try:
        # best effort: use schema carried on diff (if any) else global latest schema
        schema = getattr(diff, "metric_schema_frozen", None)
        if not isinstance(schema, dict):
            schema = {}
        _fix39_sanitize_evolutiondiff_object(diff, schema)
    except Exception:
        pass

    st.subheader("📋 Interpretation")
    st.markdown(explanation.get('interpretation', 'No interpretation available'))

    # Watch items
    watch_items = explanation.get('watch_items', [])
    if watch_items:
        st.markdown("**🔔 Watch Items:**")
        for item in watch_items:
            st.markdown(f"- {item}")

    st.markdown("---")

    # Metric Changes Table
    st.subheader("💰 Metric Changes")
    if diff.metric_diffs:
        metric_rows = []

        def _fmt_currency_first(raw: str, unit: str) -> str:
            """
            Formats evolution metrics as:
            - S$29.8B
            - $120M
            - 29.8%
            """
            raw = (raw or "").strip()
            unit = (unit or "").strip()

            if not raw or raw == "-":
                return "-"

            # If already currency-first, trust it
            if raw.startswith("S$") or raw.startswith("$"):
                return raw

            # Percent case
            if unit == "%":
                return f"{raw}%"

            # Detect currency from unit
            currency = ""
            scale = unit.replace(" ", "")

            if scale.upper().startswith("SGD"):
                currency = "S$"
                scale = scale[3:]
            elif scale.upper().startswith("USD"):
                currency = "$"
                scale = scale[3:]
            elif scale.startswith("S$"):
                currency = "S$"
                scale = scale[2:]
            elif scale.startswith("$"):
                currency = "$"
                scale = scale[1:]

            # Human-readable units
            if unit.lower().endswith("billion"):
                return f"{currency}{raw} billion".strip()
            if unit.lower().endswith("million"):
                return f"{currency}{raw} million".strip()

            # Compact units (B/M/K)
            if scale.upper() in {"B", "M", "K"}:
                return f"{currency}{raw}{scale}".strip()

            # Fallback
            return f"{currency}{raw} {unit}".strip()

        for m in diff.metric_diffs:
            icon = {
                'increased': '📈', 'decreased': '📉', 'unchanged': '➡️',
                'added': '🆕', 'removed': '❌'
            }.get(m.change_type, '•')

            change_str = f"{m.change_pct:+.1f}%" if m.change_pct is not None else "-"

            prev_raw = m.old_raw or "-"
            curr_raw = m.new_raw or "-"

            metric_rows.append({
                "": icon,
                "Metric": m.name,
                "Previous": _fmt_currency_first(prev_raw, getattr(m, "unit", "") or ""),
                "Current":  _fmt_currency_first(curr_raw, getattr(m, "unit", "") or ""),
                "Change": change_str,
                "Status": m.change_type.replace('_', ' ').title()
            })

        st.dataframe(pd.DataFrame(metric_rows), hide_index=True, use_container_width=True)
    else:
        st.info("No metrics to compare")

    st.markdown("---")

    # Entity Changes Table
    st.subheader("🏢 Entity Ranking Changes")
    if diff.entity_diffs:
        entity_rows = []
        for e in diff.entity_diffs:
            icon = {
                'moved_up': '⬆️', 'moved_down': '⬇️', 'unchanged': '➡️',
                'added': '🆕', 'removed': '❌'
            }.get(e.change_type, '•')

            rank_str = f"{e.rank_change:+d}" if e.rank_change else "-"

            entity_rows.append({
                "": icon,
                "Entity": e.name,
                "Old Rank": e.old_rank or "-",
                "New Rank": e.new_rank or "-",
                "Rank Δ": rank_str,
                "Old Share": e.old_share or "-",
                "New Share": e.new_share or "-"
            })
        st.dataframe(pd.DataFrame(entity_rows), hide_index=True, use_container_width=True)
    else:
        st.info("No entities to compare")

    st.markdown("---")

    # Finding Changes
    st.subheader("🔍 Finding Changes")
    if diff.finding_diffs:
        added = [f for f in diff.finding_diffs if f.change_type == 'added']
        removed = [f for f in diff.finding_diffs if f.change_type == 'removed']
        modified = [f for f in diff.finding_diffs if f.change_type == 'modified']

        if added:
            st.markdown("**🆕 New Findings:**")
            for f in added:
                st.success(f"• {f.new_text}")

        if removed:
            st.markdown("**❌ Removed Findings:**")
            for f in removed:
                st.error(f"• ~~{f.old_text}~~")

        if modified:
            st.markdown("**✏️ Modified Findings:**")
            for f in modified:
                st.warning(f"• {f.new_text} *(similarity: {f.similarity:.0f}%)*")
    else:
        st.info("No findings to compare")

    st.markdown("---")

    # Summary Stats
    st.subheader("📊 Change Summary")
    stats = diff.summary_stats

    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("**Metrics:**")
        st.write(f"📈 {stats['metrics_increased']} increased")
        st.write(f"📉 {stats['metrics_decreased']} decreased")
        st.write(f"➡️ {stats['metrics_unchanged']} unchanged")

    with col2:
        st.markdown("**Entities:**")
        st.write(f"⬆️ {stats['entities_moved_up']} moved up")
        st.write(f"⬇️ {stats['entities_moved_down']} moved down")
        st.write(f"🆕 {stats['entities_added']} new")

    with col3:
        st.markdown("**Findings:**")
        st.write(f"✅ {stats['findings_retained']} retained")
        st.write(f"✏️ {stats['findings_modified']} modified")
        st.write(f"🆕 {stats['findings_added']} new")


# =========================================================
# 8D. SOURCE-ANCHORED EVOLUTION
# Re-fetch the SAME sources from previous analysis for true stability
# Enhanced fetch_url_content function to use scrapingdog as fallback
# =========================================================

def _extract_pdf_text_from_bytes(pdf_bytes: bytes, max_pages: int = 6, max_chars: int = 7000) -> Optional[str]:
    """
    Extract readable text from PDF bytes deterministically.
    Limits pages/chars for speed and consistent output.
    """
    try:
        reader = PdfReader(io.BytesIO(pdf_bytes))
        texts = []
        for i, page in enumerate(reader.pages[:max_pages]):
            t = page.extract_text() or ""
            t = t.replace("\x00", " ").strip()
            if t:
                texts.append(t)
        joined = "\n".join(texts).strip()
        if len(joined) < 200:
            return None
        return joined[:max_chars]
    except Exception:
        return None



def fetch_url_content(url: str) -> Optional[str]:
    """Fetch content from a specific URL with ScrapingDog fallback"""

    def extract_text(html: str) -> Optional[str]:
        """Extract clean text from HTML"""
        soup = BeautifulSoup(html, 'html.parser')
        for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
            tag.decompose()
        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        clean_text = ' '.join(line for line in lines if line)
        return clean_text[:5000] if len(clean_text) > 200 else None

    # Try 1: Direct request
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        }
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()

        if 'captcha' not in resp.text.lower():
            content = extract_text(resp.text)
            if content:
                return content
    except:
        pass

    # Try 2: ScrapingDog API
    if SCRAPINGDOG_KEY:
        try:
            api_url = "https://api.scrapingdog.com/scrape"
            params = {"api_key": SCRAPINGDOG_KEY, "url": url, "dynamic": "false"}
            resp = requests.get(api_url, params=params, timeout=30)
            if resp.status_code == 200:
                content = extract_text(resp.text)
                if content:
                    return content
        except:
            pass

    return None



def fetch_url_content_with_status(url: str, timeout: int = 25):
    """
    Fetch URL content and return (text, status_detail).

    status_detail:
      - "success"
      - "success_pdf"
      - "http_<code>"
      - "exception:<TypeName>"
      - "empty"
      - "success_scrapingdog"

    Hardened:
      - Uses browser-like headers for direct fetch
      - Falls back to ScrapingDog when blocked/empty and SCRAPINGDOG_KEY is available
      - Avoids returning binary garbage as "text"
    """
    import re
    import requests

    def _normalize_url(s: str) -> str:
        t = (s or "").strip()
        if not t:
            return ""
        if re.match(r"^https?://", t, flags=re.I):
            return t
        if re.match(r"^[a-z0-9.-]+\.[a-z]{2,}(/.*)?$", t, flags=re.I):
            return "https://" + t
        return ""

    url = _normalize_url(url)
    if not url:
        return None, "empty"

    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
    }

    # ---------- 1) Direct fetch ----------
    try:
        resp = requests.get(url, timeout=timeout, headers=headers, allow_redirects=True)

        ct = (resp.headers.get("content-type", "") or "").lower()

        if resp.status_code >= 400:
            # If blocked, try ScrapingDog fallback (optional)
            if resp.status_code in (401, 403, 429) and globals().get("SCRAPINGDOG_KEY"):
                txt = _fetch_via_scrapingdog(url, timeout=timeout)
                if txt and txt.strip():
                    return txt, "success_scrapingdog"
            return None, f"http_{resp.status_code}"

        # PDF handling
        if "application/pdf" in ct or url.lower().endswith(".pdf"):
            try:
                import io
                import pdfplumber  # type: ignore
                with pdfplumber.open(io.BytesIO(resp.content)) as pdf:
                    out = []
                    for page in pdf.pages[:20]:
                        t = page.extract_text() or ""
                        if t.strip():
                            out.append(t)
                text = "\n".join(out).strip()
                if not text:
                    return None, "empty"
                return text, "success_pdf"
            except Exception as e:
                return None, f"exception:{type(e).__name__}"

        # Text/HTML
        text = resp.text or ""
        # If empty or suspiciously short, attempt ScrapingDog (optional)
        if (not text.strip() or len(text.strip()) < 300) and globals().get("SCRAPINGDOG_KEY"):
            txt = _fetch_via_scrapingdog(url, timeout=timeout)
            if txt and txt.strip():
                return txt, "success_scrapingdog"

        if not text.strip():
            return None, "empty"

        return text, "success"

    except Exception as e:
        # ScrapingDog as last resort for network-y issues
        try:
            if globals().get("SCRAPINGDOG_KEY"):
                txt = _fetch_via_scrapingdog(url, timeout=timeout)
                if txt and txt.strip():
                    return txt, "success_scrapingdog"
        except Exception:
            return None, f"exception:{type(e).__name__}"


def _fetch_via_scrapingdog(url: str, timeout: int = 25) -> str:
    """
    Internal helper used by fetch_url_content_with_status.
    Returns raw HTML text from ScrapingDog (or "" on failure).
    """
    import requests

    key = globals().get("SCRAPINGDOG_KEY")
    if not key:
        return ""

    params = {"api_key": key, "url": url, "dynamic": "false"}
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        resp = requests.get("https://api.scrapingdog.com/scrape", params=params, headers=headers, timeout=timeout)
        if resp.status_code >= 400:
            return ""
        return resp.text or ""
    except Exception:
        return ""

def get_extractor_fingerprint() -> str:
    """
    Bump this string whenever you change extraction or normalization behavior.
    Used to decide whether cached extracted_numbers are still valid.
    """
    return "extract_v2_normunits_2026-01-02"



def extract_numbers_from_text(text: str) -> List[Dict]:
    """
    Backward-compatible wrapper.

    v7_34 tightening:
    - Delegate to extract_numbers_with_context() so junk suppression is applied consistently.
    """
    try:
        return extract_numbers_with_context(text or "", source_url="", max_results=600) or []
    except Exception:
        return []


def _parse_iso_dt(ts: Optional[str]) -> Optional[datetime]:
    if not ts:
        return None
    try:
        ts2 = ts.replace("Z", "+00:00")
        dt = datetime.fromisoformat(ts2)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception:
        return None

def now_utc() -> datetime:
    """Timezone-aware UTC now (prevents naive/aware datetime bugs)."""
    return datetime.now(timezone.utc)


def _yureeka_now_iso_utc() -> str:
    """UTC ISO-8601 timestamp with offset (e.g., 2026-01-23T11:13:15.665069+00:00)."""
    try:
        return now_utc().isoformat()
    except Exception:
        try:
            from datetime import datetime, timezone
            return datetime.now(timezone.utc).isoformat()
        except Exception:
            return ""



# =============================================================================
# REFACTOR26: Centralized source_url attribution helpers (schema-preserving)
# - Goal: ensure row-level injection gating can reliably attribute a current metric
#   to its source URL (production vs injected), even when source_url lives only
#   inside evidence/provenance structures.
# =============================================================================

def _refactor26_extract_metric_source_url_v1(_m: dict):
    """Best-effort extraction of a metric's source URL without changing schema."""
    if not isinstance(_m, dict):
        return None
    # Direct fields
    for k in ("source_url", "url", "source", "sourceURL", "sourceUrl"):
        try:
            v = _m.get(k)
            if isinstance(v, str) and v.strip():
                return v.strip()
        except Exception:
            pass

    # Evidence list
    try:
        ev = _m.get("evidence")
        if isinstance(ev, list):
            for e in ev:
                if isinstance(e, dict):
                    for k in ("source_url", "url"):
                        v = e.get(k)
                        if isinstance(v, str) and v.strip():
                            return v.strip()
    except Exception:
        pass

    # Winner/debug/provenance structures (common in this codebase)
    try:
        wd = _m.get("winner_candidate_debug")
        if isinstance(wd, dict):
            v = wd.get("source_url") or wd.get("url")
            if isinstance(v, str) and v.strip():
                return v.strip()
    except Exception:
        pass

    try:
        prov = _m.get("provenance") or _m.get("provenance_v1") or _m.get("diag")
        if isinstance(prov, dict):
            # Direct provenance URL
            v = prov.get("source_url") or prov.get("url")
            if isinstance(v, str) and v.strip():
                return v.strip()

            # Common nested winner shape: provenance.best_candidate.source_url
            bc = prov.get("best_candidate") or prov.get("best_candidate_v1") or prov.get("winner") or prov.get("best")
            if isinstance(bc, dict):
                v2 = bc.get("source_url") or bc.get("url")
                if isinstance(v2, str) and v2.strip():
                    return v2.strip()

            # Sometimes stored as list of candidates
            cands = prov.get("candidates") or prov.get("top_candidates") or prov.get("candidates_v1")
            if isinstance(cands, list):
                for c in cands:
                    if isinstance(c, dict):
                        v3 = c.get("source_url") or c.get("url")
                        if isinstance(v3, str) and v3.strip():
                            return v3.strip()
    except Exception:
        pass

    return None


def _refactor26_hydrate_primary_metrics_canonical_source_urls_v1(_pmc: dict) -> dict:
    """In-place: ensure pmc[*].source_url exists when discoverable from evidence."""
    if not isinstance(_pmc, dict):
        return _pmc
    for _ck, _m in list(_pmc.items()):
        if not isinstance(_m, dict):
            continue
        try:
            su = _m.get("source_url")
            if isinstance(su, str) and su.strip():
                continue
            su2 = _refactor26_extract_metric_source_url_v1(_m)
            if isinstance(su2, str) and su2.strip():
                _m["source_url"] = su2.strip()
        except Exception:
            pass
    return _pmc


def _refactor26_extract_row_current_source_url_v1(_row: dict):
    """Prefer row-attributed *current* URL fields before any fallbacks."""
    if not isinstance(_row, dict):
        return None
    for k in ("cur_source_url", "current_source_url", "current_source", "current_source_url_effective", "current_source_effective"):
        try:
            v = _row.get(k)
            if isinstance(v, str) and v.strip():
                return v.strip()
        except Exception:
            pass
    # As a last resort, some rows store the current URL in source_url (ambiguous)
    try:
        v = _row.get("source_url")
        if isinstance(v, str) and v.strip():
            return v.strip()
    except Exception:
        pass
    return None


def _refactor26_norm_url_for_compare_v1(_u: str):
    """Normalize URL for set-membership comparisons using existing normalizer when available."""
    if not isinstance(_u, str):
        return None
    u = _u.strip()
    if not u:
        return None
    try:
        fn = globals().get("_inj_diag_norm_url_list")
        if callable(fn):
            out = fn([u])
            if isinstance(out, list) and out and isinstance(out[0], str) and out[0].strip():
                return out[0].strip()
    except Exception:
        pass
    return u
def _yureeka_humanize_seconds_v1(delta_seconds) -> str:
    """Compact human format for a positive second delta (e.g., '1m 18s')."""
    try:
        if delta_seconds is None:
            return ""
        ds = float(delta_seconds)
        if ds < 0:
            ds = 0.0
        total = int(round(ds))
        mins, secs = divmod(total, 60)
        hrs, mins = divmod(mins, 60)
        days, hrs = divmod(hrs, 24)
        parts = []
        if days:
            parts.append(f"{days}d")
        if hrs:
            parts.append(f"{hrs}h")
        if mins:
            parts.append(f"{mins}m")
        parts.append(f"{secs}s")
        return " ".join(parts)
    except Exception:
        return ""



def _normalize_number_to_parse_base(value: float, unit: str) -> float:
    u = (unit or "").strip().upper()
    if u == "T":
        return value * 1_000_000
    if u == "B":
        return value * 1_000
    if u == "M":
        return value * 1
    if u == "K":
        return value * 0.001
    if u == "%":
        return value
    return value


def _refactor13_get_metric_change_rows_v1(out: dict):
    """
    Prefer V2 rows if available; fallback to legacy metric_changes.
    Returned list is safe (always list).
    """
    try:
        if isinstance(out, dict):
            rows = out.get("metric_changes_v2")
            if isinstance(rows, list) and rows:
                return rows
            rows = out.get("metric_changes")
            if isinstance(rows, list):
                return rows
    except Exception:
        pass
    return []


def _refactor13_recompute_summary_and_stability_v1(out: dict) -> None:
    """
    REFACTOR13: Make results.summary + stability_score reflect canonical-first diff rows.

    - summary.metrics_increased / decreased / unchanged are derived from change_type.
    - stability_score is computed from comparable rows:
        1) discrete score: unchanged + 0.5 * small_change(<10%)/N
        2) fallback when discrete would be 0: max(0, 100 - mean_abs_pct_change)
    """
    if not isinstance(out, dict):
        return

    rows = _refactor13_get_metric_change_rows_v1(out)

    # Count change types
    increased = decreased = unchanged = added = removed = 0
    for r in rows:
        try:
            ct = (r.get("change_type") if isinstance(r, dict) else None) or ""
            if ct == "increased":
                increased += 1
            elif ct == "decreased":
                decreased += 1
            elif ct == "unchanged":
                unchanged += 1
            elif ct == "added":
                added += 1
            elif ct == "removed":
                removed += 1
        except Exception:
            pass

    total = len(rows)

    # Update summary (authoritative for UI)
    try:
        s = out.setdefault("summary", {})
        if isinstance(s, dict):
            s["total_metrics"] = total
            # In our canonical-first world, "found" = row count (includes added/removed)
            s["metrics_found"] = total
            s["metrics_increased"] = increased
            s["metrics_decreased"] = decreased
            s["metrics_unchanged"] = unchanged
            # Preserve backward compatibility: do not remove existing keys
            s.setdefault("metrics_added", added)
            s.setdefault("metrics_removed", removed)
    except Exception:
        pass

    # Compute stability from comparable rows
    # NOTE: We treat "small change" as <10% only for *changed* rows (increased/decreased),
    # so unchanged rows are not double-counted (prevents >100% stability).
    comparable = []
    for r in rows:
        if not isinstance(r, dict):
            continue
        ct = r.get("change_type")
        if ct not in ("increased", "decreased", "unchanged"):
            continue
        # Prefer explicit comparability signal
        if r.get("baseline_is_comparable") is False:
            continue
        if r.get("unit_mismatch") is True:
            continue
        # Require numeric pct (or at least numeric norms)
        cp = r.get("change_pct")
        if isinstance(cp, (int, float)):
            comparable.append((float(cp), ct))
        else:
            # fallback if norms are numeric: compute pct safely
            pv = r.get("prev_value_norm")
            cv = r.get("cur_value_norm")
            try:
                if isinstance(pv, (int, float)) and isinstance(cv, (int, float)) and abs(float(pv)) > 1e-12:
                    comparable.append((((float(cv) - float(pv)) / float(pv)) * 100.0, ct))
            except Exception:
                pass

    stability = 100.0
    method = "no_comparable_rows"
    n = len(comparable)

    # REFACTOR51: track graded stats so stability remains meaningful with extreme deltas
    mean_abs_pct_raw = None
    mean_abs_pct_capped = None
    mean_abs_cap_used = None

    if n > 0:
        # Discrete stability:
        #   stability = (unchanged + 0.5 * small_change_changed_rows) / comparable_n * 100
        stable = 0
        small = 0
        abs_vals = []
        for cp, ct in comparable:
            try:
                a = abs(float(cp))
                abs_vals.append(a)
                if ct == "unchanged":
                    stable += 1
                elif ct in ("increased", "decreased") and a < 10.0:
                    small += 1
            except Exception:
                pass

        discrete = ((stable + (small * 0.5)) / float(max(1, n))) * 100.0
        if discrete > 0.0:
            # Clamp for safety (should already be <=100 with the counting rules above)
            stability = max(0.0, min(100.0, discrete))
            method = "discrete_unchanged_smallchange"
        else:
            # Graded fallback:
            #   Use a per-row cap to avoid a single extreme outlier driving mean_abs>=100 -> 0% stability.
            #   stability = 100 - mean(min(abs_pct, 100))  (clamped [0,100])
            if abs_vals:
                try:
                    mean_abs_pct_raw = sum(abs_vals) / float(len(abs_vals))
                except Exception:
                    mean_abs_pct_raw = None
                try:
                    mean_abs_cap_used = 100.0
                    mean_abs_pct_capped = sum((a if a <= mean_abs_cap_used else mean_abs_cap_used) for a in abs_vals) / float(len(abs_vals))
                except Exception:
                    mean_abs_pct_capped = None
                if isinstance(mean_abs_pct_capped, (int, float)):
                    stability = max(0.0, min(100.0, 100.0 - float(mean_abs_pct_capped)))
                    method = "graded_mean_abs_pct_capped"
                elif isinstance(mean_abs_pct_raw, (int, float)):
                    # last-resort: clamp the mean itself (previous behavior)
                    stability = max(0.0, 100.0 - min(100.0, float(mean_abs_pct_raw)))
                    method = "graded_mean_abs_pct"
                else:
                    stability = 0.0
                    method = "no_pct_values"
            else:
                stability = 0.0
                method = "no_pct_values"

    try:
        out["stability_score"] = round(float(stability), 1)
    except Exception:
        pass

    # Mirror into diff_panel_v2_summary for auditability
    try:
        dbg = out.setdefault("debug", {})
        if isinstance(dbg, dict):
            v2s = dbg.get("diff_panel_v2_summary")
            if isinstance(v2s, dict):
                v2s.setdefault("metrics_increased", increased)
                v2s.setdefault("metrics_decreased", decreased)
                v2s.setdefault("metrics_unchanged", unchanged)
                v2s.setdefault("metrics_added", added)
                v2s.setdefault("metrics_removed", removed)
                v2s.setdefault("stability_score_v1", round(float(stability), 1))
                v2s.setdefault("stability_method_v1", method)
                v2s.setdefault("stability_comparable_n_v1", n)

            dbg["refactor13_summary_stability_v1"] = {
                "rows_total": total,
                "comparable_n": n,
                "metrics_increased": increased,
                "metrics_decreased": decreased,
                "metrics_unchanged": unchanged,
                "metrics_added": added,
                "metrics_removed": removed,
                "stability_score": round(float(stability), 1),
                "stability_method": method,
            }
            _r13 = dbg.get("refactor13_summary_stability_v1")
            if isinstance(_r13, dict) and mean_abs_pct_raw is not None:
                try:
                    _r13["mean_abs_pct_raw"] = round(float(mean_abs_pct_raw), 2)
                except Exception:
                    pass
            if isinstance(_r13, dict) and mean_abs_pct_capped is not None:
                try:
                    _r13["mean_abs_pct_capped"] = round(float(mean_abs_pct_capped), 2)
                    _r13["mean_abs_pct_cap_used"] = mean_abs_cap_used
                except Exception:
                    pass
    except Exception:
        pass



def run_source_anchored_evolution(previous_data: dict, web_context: dict = None) -> dict:
    """
    Backward-compatible entrypoint used by the Streamlit Evolution UI.

    Enhancements:
      - Accept optional web_context so evolution can reuse same-run analysis upstream artifacts.
      - ALWAYS returns a dict with required keys (even on crash).
    """

    # REFACTOR36: coerce inputs to dict to avoid NoneType.get failures
    if not isinstance(previous_data, dict):
        previous_data = {}
    if web_context is None or not isinstance(web_context, dict):
        web_context = {}

    fn = globals().get("compute_source_anchored_diff")

    def _fail(msg: str) -> dict:
        return {
            "status": "failed",
            "message": msg,
            "sources_checked": 0,
            "sources_fetched": 0,
            "numbers_extracted_total": 0,
            "stability_score": 0.0,
            "summary": {
                "total_metrics": 0,
                "metrics_found": 0,
                "metrics_increased": 0,
                "metrics_decreased": 0,
                "metrics_unchanged": 0,
            },
            "metric_changes": [],
            "source_results": [],
            "interpretation": "Evolution failed.",
        }

    if not callable(fn):
        return _fail("compute_source_anchored_diff() is not defined, so source-anchored evolution cannot run.")

    try:
        # Support both old signature (previous_data) and new signature (previous_data, web_context)
        try:
            out = fn(previous_data, web_context=web_context)
        except TypeError:
            out = fn(previous_data)
    except Exception as e:
        return _fail(f"compute_source_anchored_diff crashed: {e}")

    if not isinstance(out, dict):
        return _fail("compute_source_anchored_diff returned a non-dict payload.")

    # Renderer-required defaults
    out.setdefault("status", "success")
    out.setdefault("message", "")
    out.setdefault("sources_checked", 0)
    out.setdefault("sources_fetched", 0)
    out.setdefault("numbers_extracted_total", 0)
    out.setdefault("stability_score", 0.0)
    out.setdefault("summary", {})
    out["summary"].setdefault("total_metrics", len(out.get("metric_changes") or []))
    out["summary"].setdefault("metrics_found", 0)
    out["summary"].setdefault("metrics_increased", 0)
    out["summary"].setdefault("metrics_decreased", 0)
    out["summary"].setdefault("metrics_unchanged", 0)
    out.setdefault("metric_changes", [])
    out.setdefault("source_results", [])
    out.setdefault("interpretation", "")
    # =====================================================================
    # PATCH FIX39 (ADDITIVE): sanitize evolution output before publish/render
    # =====================================================================
    try:
        _fix39_sanitize_metric_change_rows(out)
    except Exception:
        pass

    # PATCH FIX2D20 (ADD): trace year-like commits on evolution base-run output


    _fix2d20_trace_year_like_commits(out, stage='evolution', callsite='run_source_anchored_evolution_base')

    # =====================================================================
    # REFACTOR13 (ADDITIVE): recompute summary + stability from canonical-first diff rows
    # Ensures results.summary and stability_score reflect metric_changes_v2 (or legacy metric_changes).
    # =====================================================================
    try:
        _refactor13_recompute_summary_and_stability_v1(out)
    except Exception:
        pass



    return out
# =========================================================
# ROBUST EVOLUTION HELPERS (DETERMINISTIC)
# =========================================================

NON_DATA_CONTEXT_HINTS = [
    "table of contents", "cookie", "privacy", "terms", "copyright",
    "subscribe", "newsletter", "login", "sign in", "nav", "footer"
]


def _truncate_json_safely_for_sheets(json_str: str, max_chars: int = 45000) -> str:
    """
# =====================================================================
# PATCH FIX41G (ADDITIVE): Normalize web_context and capture force_rebuild
# Ensures the UI flag reaches the fastpath gate and is recorded in output.
# =====================================================================
if web_context is None or not isinstance(web_context, dict):
    web_context = {}
_fix41_force_rebuild_seen = False
try:
    _fix41_force_rebuild_seen = bool(web_context.get("force_rebuild"))
except Exception:
    pass
    _fix41_force_rebuild_seen = False
# =====================================================================

    PATCH TS1 (ADDITIVE): JSON-safe truncation wrapper
    - Ensures json.loads always succeeds for any returned value.
    - Stores a preview when oversized.
    """
    import json

    s = "" if json_str is None else str(json_str)
    if len(s) <= max_chars:
        return s

    preview_len = max(0, int(max_chars) - 700)
    wrapper = {
        "_sheets_safe": True,
        "_sheet_write": {
            "truncated": True,
            "mode": "json_wrapper",
            "note": "Payload exceeded cell limit; stored preview only.",
        },
        "preview": s[:preview_len],
    }
    try:
        return json.dumps(wrapper, ensure_ascii=False, default=str)
    except Exception:
        return '{"_sheets_safe":true,"_sheet_write":{"truncated":true,"mode":"json_wrapper","note":"json.dumps failed"}}'


def _truncate_for_sheets(s: str, max_chars: int = 45000) -> str:
    """Hard cap to stay under Google Sheets 50k/cell limit."""
    if s is None:
        return ""
    s = str(s)
    if len(s) <= max_chars:
        return s
    head = s[: int(max_chars * 0.75)]
    tail = s[- int(max_chars * 0.20):]
    return head + "\n...\n[TRUNCATED FOR GOOGLE SHEETS]\n...\n" + tail



def _summarize_heavy_fields_for_sheets(obj: dict) -> dict:
    """
    Summarize fields that commonly exceed the per-cell limit while keeping debug utility.
    Only used for Sheets serialization; does NOT modify your in-memory analysis dict.
    """
    if not isinstance(obj, dict):
        return {"_type": str(type(obj)), "value": str(obj)[:500]}

    out = dict(obj)

    # Common bloat fields
    if "scraped_meta" in out:
        sm = out.get("scraped_meta")
        if isinstance(sm, dict):
            compact = {}
            for url, meta in list(sm.items())[:12]:
                if isinstance(meta, dict):
                    compact[url] = {
                        "status": meta.get("status"),
                        "status_detail": meta.get("status_detail"),
                        "numbers_found": meta.get("numbers_found"),
                        "fingerprint": meta.get("fingerprint"),
                        "clean_text_len": meta.get("clean_text_len"),
                    }
            out["scraped_meta"] = {"_summary": True, "count": len(sm), "sample": compact}
        else:
            out["scraped_meta"] = {"_summary": True, "type": str(type(sm))}

    for big_key in ("source_results", "baseline_sources_cache", "baseline_sources_cache_compact"):
        if big_key in out:
            sr = out.get(big_key)
            if isinstance(sr, list):
                sample = []
                for item in sr[:2]:
                    if isinstance(item, dict):
                        item2 = dict(item)
                        if isinstance(item2.get("extracted_numbers"), list):
                            item2["extracted_numbers"] = {"_summary": True, "count": len(item2["extracted_numbers"])}
                        sample.append(item2)
                out[big_key] = {"_summary": True, "count": len(sr), "sample": sample}
            else:
                out[big_key] = {"_summary": True, "type": str(type(sr))}

    # If you store full scraped_content anywhere, summarize it too
    if "scraped_content" in out:
        sc = out.get("scraped_content")
        if isinstance(sc, dict):
            out["scraped_content"] = {"_summary": True, "count": len(sc), "keys_sample": list(sc.keys())[:10]}
        else:
            out["scraped_content"] = {"_summary": True, "type": str(type(sc))}

    # =====================================================================
    # PATCH SS2 (ADDITIVE, REQUIRED): summarize nested heavy fields under out["results"]
    # Why:
    # - Your biggest payload is typically results.baseline_sources_cache (full snapshots)
    # - The previous summarizer only handled top-level keys, so Sheets payload still exceeded limits
    # - This keeps the saved JSON smaller AND keeps json.loads(get_history) working reliably
    # =====================================================================
    try:
        r = out.get("results")
        if isinstance(r, dict):
            r2 = dict(r)

            for big_key in ("baseline_sources_cache", "source_results"):
                if big_key in r2:
                    sr = r2.get(big_key)
                    if isinstance(sr, list):
                        sample = []
                        for item in sr[:2]:
                            if isinstance(item, dict):
                                item2 = dict(item)
                                if isinstance(item2.get("extracted_numbers"), list):
                                    item2["extracted_numbers"] = {
                                        "_summary": True,
                                        "count": len(item2["extracted_numbers"])
                                    }
                                sample.append(item2)
                        r2[big_key] = {"_summary": True, "count": len(sr), "sample": sample}
                    else:
                        r2[big_key] = {"_summary": True, "type": str(type(sr))}

            out["results"] = r2
    except Exception:
        pass
    # =====================================================================

    return out



def make_sheet_safe_json(obj: dict, max_chars: int = 45000) -> str:
    """
    Serialize sheet-safe JSON under the cell limit.

    NOTE / CONFLICT:
      - The prior implementation used _truncate_for_sheets() on the JSON string, which can produce
        invalid JSON (cut mid-string). Invalid JSON rows are skipped by get_history() (json.loads fails),
        so evolution can't pick them up.
      - This patch preserves summarization but replaces raw string truncation with a JSON wrapper
        that is ALWAYS valid JSON.

    Output behavior:
      - If JSON fits: returns full compact JSON string.
      - If too large: returns a valid JSON wrapper with a preview + metadata.
    """
    import json

    # Keep existing behavior: summarize heavy fields
    compact = _summarize_heavy_fields_for_sheets(obj if isinstance(obj, dict) else {"value": obj})
    if isinstance(compact, dict):
        compact["_sheets_safe"] = True

    # Try to serialize
    try:
        s = json.dumps(compact, ensure_ascii=False, default=str)
    except Exception:
        pass
        # ultra-safe fallback (still return valid JSON)
        try:
            s = json.dumps({"_sheets_safe": True, "_sheet_write": {"error": "json.dumps failed"}}, ensure_ascii=False)
        except Exception:
            return '{"_sheets_safe":true,"_sheet_write":{"error":"json.dumps failed"}}'

    # If it fits, return as-is
    if isinstance(s, str) and len(s) <= int(max_chars or 45000):
        return s

    # =========================
    # PATCH SS1 (BUGFIX, REQUIRED): valid JSON wrapper when oversized
    # - Never return mid-string truncations that break json.loads in get_history().
    # =========================
    try:
        preview_len = max(0, int(max_chars or 45000) - 700)  # leave room for wrapper fields
        wrapper = {
            "_sheets_safe": True,
            "_sheet_write": {
                "truncated": True,
                "mode": "sheets_safe_wrapper",
                "note": "Payload exceeded cell limit; stored preview only. Full snapshots must be stored separately if needed.",
            },
            # Keep a preview for UI/debugging
            "preview": s[:preview_len],
        }

        # Optional: carry minimal identity fields for convenience (additive)
        if isinstance(obj, dict):
            wrapper["question"] = (obj.get("question") or "")[:200]
            wrapper["timestamp"] = obj.get("timestamp")
            wrapper["code_version"] = obj.get("code_version") or (obj.get("primary_response") or {}).get("code_version")

            # =========================
            # PATCH SS1B (ADDITIVE, REQUIRED FOR SNAPSHOT REHYDRATION):
            # Carry snapshot pointers even when the payload is wrapped.
            # Without these fields, evolution cannot rehydrate full snapshots
            # from the Snapshots worksheet (or local fallback) and will fail
            # the snapshot gate with "No valid snapshots".
            # =========================
            try:
                _ssh = obj.get("source_snapshot_hash") or (obj.get("results") or {}).get("source_snapshot_hash")
                _ref = obj.get("snapshot_store_ref") or (obj.get("results") or {}).get("snapshot_store_ref")
                if _ssh:
                    wrapper["source_snapshot_hash"] = _ssh
                if _ref:
                    wrapper["snapshot_store_ref"] = _ref
            except Exception:
                return json.dumps(wrapper, ensure_ascii=False, default=str)
    except Exception:
        return '{"_sheets_safe":true,"_sheet_write":{"truncated":true,"mode":"sheets_safe_wrapper","note":"wrapper failed"}}'


# =====================================================================
# PATCH ES1D (ADDITIVE): external snapshot store (local file-based)
# Purpose:
#   - Store full baseline_sources_cache outside Google Sheets when rows
#     are too large (Sheets wrapper / preview mode).
#   - Allow deterministic rehydration for evolution (no refetch).
# =====================================================================
def _snapshot_store_dir() -> str:
    import os
    d = os.path.join(os.getcwd(), "snapshot_store")
    try:
        os.makedirs(d, exist_ok=True)
    except Exception:
        pass
    return d

def store_full_snapshots_local(baseline_sources_cache: list, source_snapshot_hash: str) -> str:
    """
    Store full snapshots deterministically by hash. Returns a store ref string (path).
    Additive-only helper.
    """
    import os, json
    if not source_snapshot_hash:
        return ""
    if not isinstance(baseline_sources_cache, list) or not baseline_sources_cache:
        return ""

    path = ""
    try:
        _d = _snapshot_store_dir() or os.path.join(os.getcwd(), "snapshot_store")
        path = os.path.join(_d, f"{source_snapshot_hash}.json")
    except Exception:
        return ""
    try:
        # write-once semantics (deterministic)
        if os.path.exists(path) and os.path.getsize(path) > 0:
            return path
    except Exception:
        pass

    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(baseline_sources_cache, f, ensure_ascii=False, default=str)
        return path
    except Exception:
        return ""

def load_full_snapshots_local(snapshot_store_ref: str) -> list:
    """
    Load full snapshots from a store ref string (path). Returns [] if not available.
    """
    import json, os
    try:
        if not snapshot_store_ref or not isinstance(snapshot_store_ref, str):
            return []
        if not os.path.exists(snapshot_store_ref):
            return []
        with open(snapshot_store_ref, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data if isinstance(data, list) else []
    except Exception:
        return []

# =====================================================================
# PATCH ES1E (ADDITIVE): deterministic source_snapshot_hash helper
# =====================================================================
def compute_source_snapshot_hash(baseline_sources_cache: list) -> str:
    import hashlib
    pairs = []
    for sr in (baseline_sources_cache or []):
        if not isinstance(sr, dict):
            continue
        u = (sr.get("source_url") or sr.get("url") or "").strip()
        fp = (sr.get("fingerprint") or sr.get("content_fingerprint") or "").strip()
        if u:
            pairs.append((u, fp))
    pairs.sort()
    sig = "|".join([f"{u}#{fp}" for (u, fp) in pairs])
    return hashlib.sha256(sig.encode("utf-8")).hexdigest() if sig else ""
# =====================================================================
# =====================================================================
# PATCH SS6 (ADDITIVE): build full baseline_sources_cache from evidence_records
# Why:
# - Sheets-safe summarization may replace baseline_sources_cache/extracted_numbers
#   with summary dicts. However, evidence_records often remains available and is
#   already deterministic, snapshot-derived data.
# - This helper reconstructs the minimal snapshot shape needed for
#   source-anchored evolution WITHOUT re-fetching or heuristic matching.
# =====================================================================

            # =========================
# PATCH A (ADD): Snapshot hash v2 (stable, content-weighted)
# - Keeps v1 compute_source_snapshot_hash() for backward compatibility.
# - v2 includes url + status + fingerprint + (anchor_hash,value_norm,unit_tag) tuples (bounded) for stronger identity.
            # =========================
def compute_source_snapshot_hash_v2(baseline_sources_cache: list, max_items_per_source: int = 120) -> str:
    import hashlib
    import json

    try:
        sources = baseline_sources_cache if isinstance(baseline_sources_cache, list) else []
        parts = []
        for s in sources:
            if not isinstance(s, dict):
                continue
            url = str(s.get("url") or "")
            status = str(s.get("status") or "")
            status_detail = str(s.get("status_detail") or "")
            fingerprint = str(s.get("fingerprint") or "")

            nums = s.get("extracted_numbers") or s.get("numbers") or []
            # Sometimes stored in summarized form
            if isinstance(nums, dict) and nums.get("_summary") and isinstance(nums.get("count"), int):
                # no details available; just use summary
                num_tuples = [("summary_count", int(nums.get("count")))]
            else:
                num_list = nums if isinstance(nums, list) else []
                num_tuples = []
                for n in num_list[: int(max_items_per_source or 120)]:
                    if not isinstance(n, dict):
                        continue
                    ah = str(n.get("anchor_hash") or "")
                    vn = n.get("value_norm")
                    ut = str(n.get("unit_tag") or n.get("unit") or "")
                    # Use JSON for float stability + None handling
                    num_tuples.append((ah, vn, ut))
                # Deterministic order
                num_tuples = sorted(num_tuples, key=lambda t: (t[0], str(t[1]), t[2]))

            parts.append({
                "url": url,
                "status": status,
                "status_detail": status_detail,
                "fingerprint": fingerprint,
                "nums": num_tuples,
            })

        # Deterministic ordering of sources
        parts = sorted(parts, key=lambda d: (d.get("url",""), d.get("fingerprint",""), d.get("status","")))

        payload = json.dumps(parts, ensure_ascii=False, default=str, separators=(",", ":"))
        return hashlib.sha256(payload.encode("utf-8")).hexdigest()
    except Exception:
        pass
        # Ultra-safe fallback (still deterministic-ish)
        try:
            return hashlib.sha256(str(baseline_sources_cache).encode("utf-8")).hexdigest()
        except Exception:
            return "0"*64

def build_baseline_sources_cache_from_evidence_records(evidence_records):

    """

    Rebuild a minimal baseline_sources_cache from evidence_records deterministically.


    PATCH AI3 (ADDITIVE): anchor integrity

    - Ensures each candidate has anchor_hash + candidate_id (derived if missing)

    - Preserves analysis-aligned numeric normalization fields when present

    - Deterministic ordering by (source_url, fingerprint)

    """

    import hashlib


    if not isinstance(evidence_records, list) or not evidence_records:

        return []


    def _sha1(s: str) -> str:

        try:

            return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()

        except Exception:
            return ""


    def _ensure_anchor_fields(c: dict, source_url: str = "") -> dict:

        c = c if isinstance(c, dict) else {}

        ctx = c.get("context_snippet") or c.get("context") or ""

        if isinstance(ctx, str):

            ctx = ctx.strip()[:240]

        else:

            ctx = ""

        raw = c.get("raw")

        if raw is None:

            v = c.get("value_norm") if c.get("value_norm") is not None else c.get("value")

            u = c.get("base_unit") or c.get("unit") or ""

            raw = f"{v}{u}"

        raw = str(raw)[:120]


        ah = c.get("anchor_hash") or c.get("anchor") or ""

        if not ah:

            ah = _sha1(f"{source_url}|{raw}|{ctx}")

            if ah:

                c["anchor_hash"] = ah

        if not c.get("candidate_id") and ah:

            c["candidate_id"] = str(ah)[:16]

        if source_url and not c.get("source_url"):

            c["source_url"] = source_url

        if ctx and not c.get("context_snippet"):

            c["context_snippet"] = ctx

        return c


    by_url = {}

    for rec in evidence_records:

        if not isinstance(rec, dict):

            continue

        url = rec.get("source_url") or rec.get("url") or ""

        fp = rec.get("fingerprint") or ""

        # candidates may be stored under candidates or extracted_numbers depending on producer

        cand_list = rec.get("candidates")

        if not isinstance(cand_list, list):

            cand_list = rec.get("extracted_numbers")

        if not isinstance(cand_list, list):

            cand_list = []


        out_cands = []

        for c in cand_list:

            if not isinstance(c, dict):

                continue

            cc = _ensure_anchor_fields(dict(c), url)

            out_cands.append(cc)


        if not out_cands:

            continue


        key = (str(url), str(fp))

        by_url.setdefault(key, {"source_url": url, "fingerprint": fp, "extracted_numbers": []})

        by_url[key]["extracted_numbers"].extend(out_cands)


    rebuilt = list(by_url.values())


    # deterministic sort & per-source deterministic candidate order

    try:

        rebuilt.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))

        for s in rebuilt:

            if isinstance(s, dict) and isinstance(s.get("extracted_numbers"), list):

                s["extracted_numbers"] = sorted(

                    s["extracted_numbers"],

                    key=lambda c: (

                        str(c.get("anchor_hash") or ""),

                        str(c.get("candidate_id") or ""),

                        str(c.get("raw") or ""),

                        str(c.get("unit") or ""),

                    )

                )

    except Exception:

        pass


    # =====================================================================
    # PATCH FIX41AFC5 (ADDITIVE): attach eligibility-hardening debug counters
    # =====================================================================
    try:
        if isinstance(rebuilt, dict):
            rebuilt.setdefault("_fix41afc5_debug", {})
            if isinstance(rebuilt.get("_fix41afc5_debug"), dict):
                rebuilt["_fix41afc5_debug"].update(dict(_fix41afc5_dbg2))
    except Exception:
        pass
    # =====================================================================

    return rebuilt
def _sheets_now_ts():
    import time
    return time.time()

def _sheets_cache_get(key: str):
    try:
        item = _SHEETS_READ_CACHE.get(key)
        if not item:
            return None
        ts, val = item
        if (_sheets_now_ts() - ts) > _SHEETS_READ_CACHE_TTL_SEC:
            return None
        return val
    except Exception:
        return None

def _sheets_cache_set(key: str, val):
    try:
        _SHEETS_READ_CACHE[key] = (_sheets_now_ts(), val)
    except Exception:
        pass

def _is_sheets_rate_limit_error(err: Exception) -> bool:
    s = ""
    try:
        s = str(err) or ""
    except Exception:
        pass
        s = ""
    # Common markers seen via gspread/googleapiclient:
    markers = ["RESOURCE_EXHAUSTED", "Quota exceeded", "RATE_LIMIT_EXCEEDED", "429"]
    return any(m in s for m in markers)

def sheets_get_all_values_cached(ws, cache_key: str):
    """
    Cached wrapper for ws.get_all_values() with rate-limit fallback.
    cache_key should be stable for the worksheet (e.g., 'Snapshots', 'HistoryFull', 'History').
    """
    global _SHEETS_LAST_READ_ERROR
    key = f"get_all_values:{cache_key}"
    cached = _sheets_cache_get(key)
    if cached is not None:
        return cached
    try:
        # === PATCH SHEETS_CACHE1 (CONFLICT FIX, MINIMAL): call the underlying worksheet read ===
        # Previous draft accidentally recursed into itself and referenced an undefined variable.
        # This is a direct execution conflict fix (no behavior change intended beyond correctness).
        values = ws.get_all_values() if ws else []
        _sheets_cache_set(key, values)
        return values
    except Exception as e:
        _SHEETS_LAST_READ_ERROR = str(e)
        # Rate-limit fallback: return last cached value if we have one, else empty list
        if _is_sheets_rate_limit_error(e):
            stale = _SHEETS_READ_CACHE.get(key)
            if stale and isinstance(stale, tuple) and len(stale) == 2:
                return stale[1]
            return []
        raise

# =====================================================================
# PATCH SS2 (ADDITIVE): Google Sheets snapshot store (separate worksheet)
# Purpose:
#   - Persist full baseline_sources_cache inside the same Spreadsheet
#     but in a dedicated worksheet (tab), chunked across rows.
#   - Enables deterministic rehydration for evolution without refetch.
# Notes:
#   - Write-once semantics by source_snapshot_hash.
#   - Chunking and reassembly are deterministic (part_index ordering).
# =====================================================================
def get_google_spreadsheet():
    """Connect to Google Spreadsheet (cached connection if available)."""
    try:
        # If get_google_sheet() exists and already opened the spreadsheet as sheet.sheet1,
        # we re-open to obtain the Spreadsheet handle (additive; avoids refactoring).
        import streamlit as st
        from google.oauth2.service_account import Credentials
        import gspread

        SCOPES = [
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive",
        ]
        creds = Credentials.from_service_account_info(
            dict(st.secrets["gcp_service_account"]),
            scopes=SCOPES
        )
        client = gspread.authorize(creds)
        spreadsheet_name = st.secrets.get("google_sheets", {}).get("spreadsheet_name", "Yureeka_JSON")
        return client.open(spreadsheet_name)
    except Exception:
        return None

def _ensure_snapshot_worksheet(spreadsheet, title: str = "Snapshots"):
    """Ensure a worksheet tab exists for snapshot storage."""
    try:
        if not spreadsheet:
            return None
        try:
            ws = spreadsheet.worksheet(title)
            return ws
        except Exception:
            pass
            # Create with a reasonable default size; Sheets can expand.
            ws = spreadsheet.add_worksheet(title=title, rows=2000, cols=8)
            try:
                ws.append_row(
                    ["source_snapshot_hash", "part_index", "total_parts", "payload_part", "created_at", "code_version", "fingerprints_sig", "sha256"],
                    value_input_option="RAW",
                )
            except Exception:
                return ws
    except Exception:
        return None

def store_full_snapshots_to_sheet(baseline_sources_cache: list, source_snapshot_hash: str, worksheet_title: str = "Snapshots", chunk_chars: int = 20000) -> str:
    """
    Store full snapshots to a dedicated worksheet tab in chunked rows.
    Returns a ref string like: 'gsheet:Snapshots:<hash>'

    REFACTOR40 (BUGFIX):
    - Previously, the "write-once" gate used ws.findall(hash) and would treat ANY existing rows
      as "already written". If a prior write partially failed (rate-limit / quota / transient),
      we could end up with an incomplete snapshot stored under the hash forever, and subsequent
      runs would never repair it.
    - Now:
      * If rows exist, we first validate that the snapshot is actually loadable.
      * If not loadable, we attempt a repair write (append a fresh batch keyed by created_at).
      * After successful writes, we invalidate the worksheet read cache so recent snapshots
        are immediately retrievable.
    """
    import json, hashlib, zlib, base64, zlib, base64
    if not source_snapshot_hash:
        return ""
    if not isinstance(baseline_sources_cache, list) or not baseline_sources_cache:
        return ""

    try:
        ss = get_google_spreadsheet()
        ws = _ensure_snapshot_worksheet(ss, worksheet_title) if ss else None
        if not ws:
            return ""

        # If any rows exist for this hash, only short-circuit if it is actually loadable.
        try:
            existing = ws.findall(source_snapshot_hash)
            if existing:
                try:
                    _probe = load_full_snapshots_from_sheet(source_snapshot_hash, worksheet_title=worksheet_title)
                    if isinstance(_probe, list) and _probe:
                        return f"gsheet:{worksheet_title}:{source_snapshot_hash}"
                except Exception:
                    pass
        except Exception:
            pass

        payload = json.dumps(baseline_sources_cache, ensure_ascii=False, default=str)
        # =============================================================
        # REFACTOR42 (ADDITIVE): compress very large snapshot payloads to
        # reduce write volume / API calls (helps avoid rate limits).
        # Storage format:
        #   payload_part begins with 'zlib64:' then base64(zlib(json_bytes))
        # Backward compatible: loader detects/decompresses when prefix present.
        # =============================================================
        try:
            if isinstance(payload, str) and len(payload) > 120000:
                _raw = payload.encode("utf-8", errors="strict")
                _comp = zlib.compress(_raw, level=9)
                _b64 = base64.b64encode(_comp).decode("ascii")
                payload = "zlib64:" + _b64
        except Exception:
            pass

        sha = hashlib.sha256(payload.encode("utf-8")).hexdigest()

        # deterministic chunking
        chunk_size = max(1000, int(chunk_chars or 45000))
        parts = [payload[i:i+chunk_size] for i in range(0, len(payload), chunk_size)]
        total = len(parts)

        # Optional fingerprints signature (stable)
        pairs = []
        for sr in baseline_sources_cache:
            if isinstance(sr, dict):
                u = (sr.get("source_url") or sr.get("url") or "").strip()
                fp = (sr.get("fingerprint") or sr.get("content_fingerprint") or "").strip()
                if u:
                    pairs.append((u, fp))
        pairs.sort()
        fingerprints_sig = "|".join([f"{u}#{fp}" for (u, fp) in pairs]) if pairs else ""

        created_at = _yureeka_now_iso_utc()

        # best-effort: use global if exists
        code_version = ""
        try:
            code_version = globals().get("CODE_VERSION") or ""
        except Exception:
            code_version = ""

        rows = []
        for idx, part in enumerate(parts):
            rows.append([source_snapshot_hash, idx, total, part, created_at, code_version, fingerprints_sig, sha])

        wrote_all = False
        try:
            # Append in small batches to reduce API payload size / rate-limit failures.
            batch_size = 10
            import time
            _need_throttle = (len(rows) > 60)
            wrote = 0
            for i in range(0, len(rows), batch_size):
                chunk = rows[i:i+batch_size]
                ws.append_rows(chunk, value_input_option="RAW")
                wrote += len(chunk)
                try:
                    if _need_throttle:
                        time.sleep(0.15)
                except Exception:
                    pass
            wrote_all = (wrote == len(rows))
        except Exception:
            # Fall back to append_row loop; do NOT early-return on the first failure.
            success = 0
            for r in rows:
                try:
                    ws.append_row(r, value_input_option="RAW")
                    success += 1
                except Exception:
                    pass
            wrote_all = (success == len(rows))

        # If we believe we wrote all rows, invalidate snapshot read cache so we can re-load immediately.
        if wrote_all:
            try:
                # This cache key format matches sheets_get_all_values_cached()
                _cache_key = f"get_all_values:{worksheet_title}"
                _cache = globals().get("_SHEETS_READ_CACHE")
                if isinstance(_cache, dict):
                    _cache.pop(_cache_key, None)
            except Exception:
                pass
            return f"gsheet:{worksheet_title}:{source_snapshot_hash}"

        # Partial write: return empty ref to avoid pointing to a broken snapshot.
        return ""
    except Exception:
        return ""

def load_full_snapshots_from_sheet(source_snapshot_hash: str, worksheet_title: str = "Snapshots") -> list:
    """Load and reassemble full snapshots list from a dedicated worksheet.

    REFACTOR40 (BUGFIX):
    - Fix stale cache behavior: if the requested hash is not found in cached values, do a direct read once.
    - Fix partial-write repair behavior: if multiple write batches exist for the same hash, select the
      most recent *complete* batch (grouped by created_at), not a mixed/partial merge.
    """
    import json, hashlib
    if not source_snapshot_hash:
        return []
    try:
        ss = get_google_spreadsheet()
        ws = ss.worksheet(worksheet_title) if ss else None
        if not ws:
            return []

        def _read_cached():
            try:
                return sheets_get_all_values_cached(ws, cache_key=worksheet_title)
            except Exception:
                return []

        def _read_direct():
            try:
                return ws.get_all_values()
            except Exception:
                return []

        values = _read_cached()

        # If empty/too small, do one direct read to bypass stale empty cache.
        if not values or len(values) < 2:
            values = _read_direct()
            if not values or len(values) < 2:
                return []
            # Best-effort cache refresh
            try:
                _sheets_cache_set(f"get_all_values:{worksheet_title}", values)
            except Exception:
                pass

        header = values[0] or []
        # Expect at least: source_snapshot_hash, part_index, total_parts, payload_part
        try:
            col_h = header.index("source_snapshot_hash")
            col_i = header.index("part_index")
            col_t = header.index("total_parts")
            col_p = header.index("payload_part")
            col_ca = header.index("created_at") if "created_at" in header else None
            col_sha = header.index("sha256") if "sha256" in header else None
        except Exception:
            # If headers are missing/misaligned, bail safely
            return []

        def _safe_int(x):
            try:
                return int(x)
            except Exception:
                return 0

        def _parse_iso(s: str):
            try:
                # Lexicographic order works for ISO8601 UTC strings, but parse for safety.
                from datetime import datetime
                return datetime.fromisoformat(str(s).replace("Z", "+00:00"))
            except Exception:
                return None

        def _extract_best(values_table):
            rows = []
            for r in values_table[1:]:
                try:
                    if len(r) > col_h and r[col_h] == source_snapshot_hash:
                        rows.append(r)
                except Exception:
                    continue
            if not rows:
                return []

            # Group by created_at (per-write batch). If missing, fall back to a single group.
            groups = {}
            for r in rows:
                k = ""
                try:
                    if col_ca is not None and len(r) > col_ca:
                        k = str(r[col_ca] or "")
                except Exception:
                    k = ""
                if not k:
                    k = "legacy"
                groups.setdefault(k, []).append(r)

            candidates = []
            for created_at, grows in groups.items():
                try:
                    # Determine expected total parts
                    expected_total = 0
                    try:
                        if grows and len(grows[0]) > col_t:
                            expected_total = _safe_int(grows[0][col_t])
                    except Exception:
                        expected_total = 0
                    if expected_total <= 0:
                        continue

                    # Build part map (dedupe by part_index; keep longest payload_part)
                    part_map = {}
                    for rr in grows:
                        try:
                            pi = _safe_int(rr[col_i] if len(rr) > col_i else 0)
                            pp = rr[col_p] if len(rr) > col_p else ""
                            if pi not in part_map or (isinstance(pp, str) and len(pp) > len(part_map.get(pi, ""))):
                                part_map[pi] = pp or ""
                        except Exception:
                            continue

                    # Completeness check: must have all indices 0..expected_total-1
                    if len(part_map) < expected_total:
                        continue
                    missing = False
                    payload_parts = []
                    for i in range(expected_total):
                        if i not in part_map:
                            missing = True
                            break
                        payload_parts.append(part_map[i] or "")
                    if missing:
                        continue

                    payload = "".join(payload_parts)

                    # Optional integrity check
                    try:
                        if col_sha is not None:
                            exp = ""
                            try:
                                exp = (grows[0][col_sha] if len(grows[0]) > col_sha else "") or ""
                            except Exception:
                                exp = ""
                            if exp:
                                actual = hashlib.sha256(payload.encode("utf-8")).hexdigest()
                                if actual != exp:
                                    continue
                    except Exception:
                        pass
                    # JSON decode (supports REFACTOR42 compressed payloads)
                    try:
                        payload_json = payload
                        try:
                            # REFACTOR43 (BUGFIX): transparently decode 'zlib64:' compressed payloads.
                            if isinstance(payload_json, str) and payload_json.startswith("zlib64:"):
                                import base64, zlib
                                b64 = payload_json.split(":", 1)[1] if ":" in payload_json else ""
                                if not b64:
                                    continue
                                comp = base64.b64decode(b64.encode("ascii"), validate=False)
                                raw = zlib.decompress(comp)
                                payload_json = raw.decode("utf-8", errors="strict")
                        except Exception:
                            # If decoding fails, treat as invalid snapshot batch
                            continue

                        data = json.loads(payload_json)
                        if not isinstance(data, list) or not data:
                            continue
                    except Exception:
                        continue
                    # Candidate score: prefer latest created_at when parseable; else fallback to string.
                    dt = _parse_iso(created_at) if created_at and created_at != "legacy" else None
                    candidates.append((dt, created_at, data))
                except Exception:
                    continue

            if not candidates:
                return []
            # Choose best candidate: latest datetime if available else latest created_at string.
            candidates.sort(key=lambda x: (x[0] is not None, x[0] or x[1]), reverse=True)
            return candidates[0][2]

        best = _extract_best(values)

        # If the hash is missing or incomplete in cached values, do ONE direct refresh and retry.
        if not best:
            direct = _read_direct()
            if direct and len(direct) >= 2:
                try:
                    best = _extract_best(direct)
                except Exception:
                    best = []
                # refresh cache best-effort
                try:
                    _sheets_cache_set(f"get_all_values:{worksheet_title}", direct)
                except Exception:
                    pass

        return best if isinstance(best, list) else []
    except Exception:
        return []

# =====================================================================
# PATCH HF4 (ADDITIVE): HistoryFull payload rehydration support
# Why:
# - Evolution may receive a sheets-safe wrapper that omits primary_response,
#   metric_schema_frozen, metric_anchors, etc.
# - When wrapper includes full_store_ref ("gsheet:HistoryFull:<analysis_id>"),
#   we can deterministically load the full analysis payload (no re-fetch).
# Notes:
# - Additive only. Safe no-op if sheet/tab not present.
# =====================================================================

# ===================== PATCH HF_WRITE1 (ADDITIVE) =====================

def write_full_history_payload_to_sheet(analysis_id: str, payload: str, worksheet_title: str = "HistoryFull", chunk_size: int = 20000) -> bool:
    """Write a full analysis payload (string) into HistoryFull as chunked rows keyed by analysis_id.

    Additive helper to support oversized History cells:
      - Ensures HistoryFull headers exist
      - Splits payload into chunks
      - Stores sha256 for integrity
    """
    import hashlib
    if not analysis_id or not payload:
        return False
    try:
        ss = get_google_spreadsheet()
        if not ss:
            return False
        try:
            ws = ss.worksheet(worksheet_title)
        except Exception:
            pass
            # Create sheet if missing (best-effort)
            try:
                ws = ss.add_worksheet(title=worksheet_title, rows=2000, cols=10)
            except Exception:
                pass
                ws = ss.worksheet(worksheet_title)

        # Ensure headers exist
        try:
            headers = ws.row_values(1)
            if (not headers) or (len(headers) < 5) or headers[0] != "analysis_id":
                _ = ws.update('A1:E1', [["analysis_id", "part_index", "total_parts", "payload_part", "sha256"]])
        except Exception:
            pass
            try:
                _ = ws.update('A1:E1', [["analysis_id", "part_index", "total_parts", "payload_part", "sha256"]])
            except Exception:
                pass

        # Remove existing rows for this analysis_id (optional; keep additive and safe: do not delete to avoid refactor)
        # We will append new parts; loader will read the latest by order if duplicates exist.

        sha = hashlib.sha256(payload.encode("utf-8", errors="ignore")).hexdigest()
        parts = [payload[i:i+chunk_size] for i in range(0, len(payload), chunk_size)]
        total = len(parts) if parts else 0
        if total == 0:
            return False

        rows = []
        for i, part in enumerate(parts):
            rows.append([analysis_id, str(i), str(total), part, sha])

        # Append in one batch if possible
        try:
            ws.append_rows(rows, value_input_option="RAW")
        except Exception:
            pass
            # Fallback to per-row append
            for r in rows:
                try:
                    ws.append_row(r, value_input_option="RAW")
                except Exception:
                    return False
        return True
    except Exception:
        return False

# =================== END PATCH HF_WRITE1 (ADDITIVE) ===================

def load_full_history_payload_from_sheet(analysis_id: str, worksheet_title: str = "HistoryFull") -> dict:
    """
    Load the full analysis JSON payload from the HistoryFull worksheet.

    PATCH HF_LOAD_V2 (ADDITIVE):
    - Supports payloads split across multiple rows (chunked writes).
    - Supports current HistoryFull headers:
        analysis_id, part_index, total_parts, payload_json, created_at, code_version, sha256
    - Backward compatible with older variants:
        id, part_index, total_parts, payload_part / data, sha256
    - Deterministic stitching (sort by part_index) + safe JSON parse.

    PATCH HF_LOAD_V3 (ADDITIVE):
    - Verifies chunk completeness when total_parts is available (0..total_parts-1).
    - Optionally verifies sha256 when present (stitched string).
    - Does not change failure mode: still returns {} on any failure.
    """
    try:
        ss = get_google_spreadsheet()
        if not ss:
            return {}

        try:
            ws = ss.worksheet(worksheet_title)
        except Exception:
            return {}

        # Read all rows (prefer cached getter if present)
        try:
            fn = globals().get("sheets_get_all_values_cached")
            rows = fn(ws, cache_key=worksheet_title) if callable(fn) else (ws.get_all_values() or [])
        except Exception:
            pass
            try:
                rows = ws.get_all_values() or []
            except Exception:
                return {}

        if not rows or len(rows) < 2:
            return {}

        header = rows[0] or []
        body = rows[1:] or []

        def _col(name: str):
            try:
                return header.index(name)
            except Exception:
                return None

        # -----------------------------------------------------------------
        # PATCH HF_LOAD_V2_COLS (ADDITIVE): map to real sheet headers + legacy
        # -----------------------------------------------------------------
        c_id = _col("analysis_id")
        if c_id is None:
            c_id = _col("id")
        if c_id is None:
            c_id = 0  # last-ditch fallback

        c_part = _col("part_index")
        c_total = _col("total_parts")

        # IMPORTANT: your sheet uses payload_json
        c_payload = _col("payload_json")
        if c_payload is None:
            c_payload = _col("payload_part")
        if c_payload is None:
            c_payload = _col("data")
        if c_payload is None:
            c_payload = len(header) - 1  # last-ditch fallback

        c_sha = _col("sha256")
        # -----------------------------------------------------------------

        target_id = str(analysis_id).strip()
        if not target_id:
            return {}

        # (pidx:int|None, total:int|None, chunk:str, sha:str)
        parts_with_sha = []

        for r in body:
            try:
                if not r:
                    continue

                rid = r[c_id] if c_id < len(r) else ""
                rid = str(rid).strip()
                if rid != target_id:
                    continue

                chunk = r[c_payload] if c_payload < len(r) else ""
                chunk = chunk or ""
                if not isinstance(chunk, str):
                    chunk = str(chunk)

                # part_index (optional)
                pidx = None
                if c_part is not None and c_part < len(r):
                    try:
                        pidx = int(str(r[c_part]).strip())
                    except Exception:
                        pass
                        pidx = None

                # total_parts (optional)
                tparts = None
                if c_total is not None and c_total < len(r):
                    try:
                        tparts = int(str(r[c_total]).strip())
                    except Exception:
                        pass
                        tparts = None

                sha = ""
                if c_sha is not None and c_sha < len(r):
                    sha = str(r[c_sha] or "").strip()

                # keep even tiny chunks; concatenation is deterministic
                if chunk.strip() == "":
                    continue

                parts_with_sha.append((pidx, tparts, chunk, sha))
            except Exception:
                pass
                continue

        if not parts_with_sha:
            return {}

        # -----------------------------------------------------------------
        # PATCH HF_LOAD_V2_BUCKET (ADDITIVE): if duplicates exist, pick best sha bucket
        # Score = (unique part_index count, total payload length)
        # -----------------------------------------------------------------
        parts = []          # list[(pidx, chunk)]
        chosen_sha = ""     # sha bucket selected (if any)
        chosen_total = None # total_parts inferred for chosen bucket (if any)
        try:
            if any(s for _, _, _, s in parts_with_sha):
                buckets = {}
                for pidx, tparts, chunk, sha in parts_with_sha:
                    key = sha or "__no_sha__"
                    buckets.setdefault(key, []).append((pidx, tparts, chunk))

                def _score(items):
                    idxs = [i for i, _, _ in items if i is not None]
                    uniq = len(set(idxs)) if idxs else 0
                    total_len = sum(len(c or "") for _, _, c in items)
                    return (uniq, total_len)

                best_key = sorted(buckets.keys(), key=lambda k: _score(buckets[k]), reverse=True)[0]
                chosen_sha = "" if best_key == "__no_sha__" else best_key
                best_items = buckets[best_key]

                # Infer total_parts for this bucket (mode / max)
                try:
                    totals = [tp for _, tp, _ in best_items if isinstance(tp, int) and tp > 0]
                    chosen_total = max(totals) if totals else None
                except Exception:
                    pass
                    chosen_total = None

                parts = [(pidx, chunk) for (pidx, _tparts, chunk) in best_items]
            else:
                parts = [(pidx, chunk) for pidx, _tparts, chunk, _sha in parts_with_sha]
                # Infer total_parts (mode / max) even without sha
                try:
                    totals = [tp for _, tp, _, _ in parts_with_sha if isinstance(tp, int) and tp > 0]
                    chosen_total = max(totals) if totals else None
                except Exception:
                    pass
                    chosen_total = None
        except Exception:
            pass
            parts = [(pidx, chunk) for pidx, _tparts, chunk, _sha in parts_with_sha]
            try:
                totals = [tp for _, tp, _, _ in parts_with_sha if isinstance(tp, int) and tp > 0]
                chosen_total = max(totals) if totals else None
            except Exception:
                pass
                chosen_total = None
        # -----------------------------------------------------------------

        # Sort parts deterministically by part_index; None last
        def _sort_key(t):
            pidx, _ = t
            return (pidx is None, pidx if pidx is not None else 0)

        parts.sort(key=_sort_key)

        # -----------------------------------------------------------------
        # PATCH HF_LOAD_V3_COMPLETE (ADDITIVE): completeness check when total_parts known
        # - If total_parts is known and we have part_index values, require 0..total-1.
        # - If incomplete, return {} (do not attempt parse on partial payload).
        # -----------------------------------------------------------------
        try:
            if isinstance(chosen_total, int) and chosen_total > 0:
                idxs = [p for (p, _c) in parts if isinstance(p, int)]
                if idxs:
                    uniq = sorted(set(idxs))
                    expected = list(range(0, chosen_total))
                    if uniq != expected:
                        return {}
        except Exception:
            pass
        # -----------------------------------------------------------------

        # Stitch chunks
        full_json_str = "".join([chunk for _, chunk in parts]).strip()
        if not full_json_str:
            return {}

        # -----------------------------------------------------------------
        # PATCH HF_LOAD_V3_SHA (ADDITIVE): verify sha256 when present
        # - If chosen_sha exists, compare against sha256(stitched_bytes).
        # - If mismatch, return {} (treat as corrupted / wrong bucket).
        # -----------------------------------------------------------------
        try:
            if isinstance(chosen_sha, str) and chosen_sha:
                import hashlib
                digest = hashlib.sha256(full_json_str.encode("utf-8", errors="ignore")).hexdigest()
                if str(digest).lower() != str(chosen_sha).lower():
                    return {}
        except Exception:
            pass
        # -----------------------------------------------------------------

        import json
        try:
            obj = json.loads(full_json_str)
            if isinstance(obj, dict):
                # -----------------------------------------------------------------
                # PATCH HF_LOAD_V3_META (ADDITIVE): optional debug stamp (harmless)
                # Only attaches when parse succeeded.
                # -----------------------------------------------------------------
                try:
                    obj["_rehydration_debug"] = {
                        "worksheet": str(worksheet_title or ""),
                        "analysis_id": str(target_id),
                        "parts_used": int(len(parts)),
                        "total_parts_expected": int(chosen_total) if isinstance(chosen_total, int) else None,
                        "sha_verified": bool(chosen_sha),
                    }
                except Exception:
                    pass
                # -----------------------------------------------------------------
                return obj
            return {}
        except Exception:
            pass
            # PATCH HF_LOAD_V2_SALVAGE (ADDITIVE): common salvage for leading/trailing junk
            try:
                # Try to isolate first "{" and last "}" if accidental prefix/suffix exists
                a = full_json_str.find("{")
                b = full_json_str.rfind("}")
                if a != -1 and b != -1 and b > a:
                    obj2 = json.loads(full_json_str[a:b+1])
                    if isinstance(obj2, dict):
                        # (keep same meta stamp behavior)
                        try:
                            obj2["_rehydration_debug"] = {
                                "worksheet": str(worksheet_title or ""),
                                "analysis_id": str(target_id),
                                "parts_used": int(len(parts)),
                                "total_parts_expected": int(chosen_total) if isinstance(chosen_total, int) else None,
                                "sha_verified": bool(chosen_sha),
                                "salvaged": True,
                            }
                        except Exception:
                            return obj2
            except Exception:
                return {}

    except Exception:
        return {}

def fingerprint_text(text: str) -> str:
    """Stable short fingerprint for fetched content (deterministic)."""
    if not text:
        return ""
    normalized = re.sub(r"\s+", " ", text.strip().lower())
    return hashlib.md5(normalized.encode("utf-8")).hexdigest()[:12]

def attach_source_snapshots_to_analysis(analysis: dict, web_context: dict) -> dict:
    # PATCH FIX2D66: promote injected URLs from UI/raw fields into web_context for deterministic admission
    try:
        _q = str((analysis or {}).get('question') or '')
        _fix2d66_promote_injection_in_web_context(web_context, question=_q)
    except Exception:
        pass

    """
    Attach stable source snapshots (from web_context.scraped_meta) into analysis.

    Enhancements (v7_34 patch):
    - Ensures scraped_meta.extracted_numbers is always list-like
    - Adds RANGE capture per canonical metric using admitted snapshots:
        primary_metrics_canonical[ckey]["value_range"] = {min,max,n,examples}
      This restores earlier "range vs point estimate" behavior in a compatible way.
    """
    import re
    from datetime import datetime, timezone

    # FIX2D66_PROMOTE_INJECTED_URLS_IN_ATTACH (ADDITIVE)
    try:
        _qtxt = str((analysis or {}).get('question') or (analysis or {}).get('query') or '')
        web_context = _fix2d66_promote_injected_urls(web_context or {}, question_text=_qtxt, stage='analysis_attach')
    except Exception:
        pass

    def _now_iso() -> str:
        return datetime.now(timezone.utc).isoformat()

    def _fingerprint(text: str) -> str:
        try:
            fn = globals().get("fingerprint_text")
            if callable(fn):
                return fn(text)
        except Exception:
            pass
        try:
            import hashlib
            t = re.sub(r"\s+", " ", (text or "").strip().lower())
            return hashlib.md5(t.encode("utf-8", errors="ignore")).hexdigest()[:12]
        except Exception:
            return ""

    # =========================================================================
    # PATCH N1 (ADDITIVE): stable anchor_hash fallback helper for snapshots
    # - Does NOT change existing behavior if anchor_hash already present.
    # =========================================================================
    def _sha1(s: str) -> str:
        try:
            import hashlib
            return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()
        except Exception:
            return ""
    # =========================================================================

    # =========================================================================
    # PATCH N2 (ADDITIVE): optional canonicalizer hook for snapshot numbers
    # - Ensures unit_tag/unit_family/base_unit/value_norm are present when possible.
    # - No behavior change if helper missing.
    # =========================================================================
    _canon_fn = globals().get("canonicalize_numeric_candidate")
    def _maybe_canonicalize(n: dict) -> dict:
        try:
            if callable(_canon_fn):
                return _canon_fn(dict(n))
        except Exception:
            return dict(n)
    # =========================================================================

    def _parse_num(value, unit_hint=""):
        try:
            fn = globals().get("parse_human_number")
            if callable(fn):
                return fn(str(value), unit_hint)
        except Exception:
            pass
        # fallback
        try:
            s = str(value).strip().replace(",", "")
            if not s:
                return None
            return float(re.findall(r"-?\d+(?:\.\d+)?", s)[0])
        except Exception:
            return None

    def _unit_family_from_metric(mdef: dict) -> str:
        # prefer metric schema
        uf = (mdef or {}).get("unit_family") or ""
        uf = str(uf).lower().strip()
        if uf in ("percent", "pct"):
            return "PCT"
        if uf in ("currency",):
            return "CUR"
        if uf in ("magnitude", "unit_sales", "other"):
            return "MAG"
        return "OTHER"

    def _cand_unit_family(cunit: str, craw: str) -> str:
        u = (cunit or "").strip()
        r = (craw or "")
        uu = u.upper()
        ru = r.upper()

        # Percent
        if uu == "%" or "%" in ru:
            return "PCT"

        # Energy
        if any(x in (u or "").lower() for x in ["twh", "gwh", "mwh", "kwh"]) or any(x in (r or "").lower() for x in ["twh", "gwh", "mwh", "kwh"]):
            return "ENERGY"

        # Currency (symbol/code presence)
        #if any(x in ru for x in ["$", "USD", "SGD", "EUR", "GBP", "S$"]) or uu in ("USD", "SGD", "EUR", "GBP"):
        #    return "CUR"

        if re.search(r"(\$|S\$|€|£)\s*\d", r) or any(x in ru for x in ["USD", "SGD", "EUR", "GBP"]) or uu in ("USD","SGD","EUR","GBP"):
            return "CUR"


        # Magnitude (case-insensitive)
        if uu in ("K", "M", "B", "T") or (u or "").lower() in ("k", "m", "b", "t"):
            return "MAG"

        return "OTHER"

    def _tokenize(s: str):
        return [t for t in re.findall(r"[a-z0-9]+", (s or "").lower()) if len(t) > 2]

    def _safe_norm_unit_tag(x: str) -> str:
        try:
            fn = globals().get("normalize_unit_tag")
            if callable(fn):
                return fn(x or "")
        except Exception:
            return (x or "").strip()


    # -----------------------------
    # Build baseline_sources_cache from scraped_meta (snapshot-friendly)
    # -----------------------------
    baseline_sources_cache = []
    scraped_meta = (web_context or {}).get("scraped_meta") or {}
    if isinstance(scraped_meta, dict):
        for url, meta in scraped_meta.items():
            if not isinstance(meta, dict):
                continue
            nums = meta.get("extracted_numbers") or []
            if nums is None or not isinstance(nums, list):
                nums = []

            content = meta.get("content") or meta.get("clean_text") or (web_context.get("scraped_content", {}) or {}).get(url, "") or ""

            baseline_sources_cache.append({
                "url": url,
                "status": "fetched" if str(meta.get("status_detail", "")).startswith("success") or meta.get("status") == "fetched" else "failed",
                "status_detail": meta.get("status_detail") or meta.get("status") or "",
                "numbers_found": int(meta.get("numbers_found") or (len(nums) if isinstance(nums, list) else 0)),
                "fetched_at": meta.get("fetched_at") or _now_iso(),
                "fingerprint": meta.get("fingerprint") or _fingerprint(content),

                # =====================================================================
                # PATCH N1 (+ N2) (ADDITIVE): preserve full candidate record in snapshots
                # - This is critical for:
                #   * range gating (metric-aware)
                #   * schema-first attribution
                #   * evolution rebuild (anchor_hash + value_norm + unit_family)
                # - Backward compatible: only adds keys; existing keys unchanged.
                # =====================================================================
                "extracted_numbers": [
                    (lambda nn: {
                        "value": nn.get("value"),
                        "unit": nn.get("unit"),
                        "raw": nn.get("raw"),
                        "context_snippet": (nn.get("context_snippet") or nn.get("context") or "")[:240],

                        # keep existing anchor_hash if present; else stable fallback
                        "anchor_hash": (
                            nn.get("anchor_hash")
                            or _sha1(
                                f"{url}|{str(nn.get('raw') or '')}|{(nn.get('context_snippet') or nn.get('context') or '')[:240]}"
                            )
                        ),

                        "source_url": nn.get("source_url") or url,

                        # ---- Additive: junk tagging & deterministic offsets ----
                        "is_junk": nn.get("is_junk"),
                        "junk_reason": nn.get("junk_reason"),
                        "start_idx": nn.get("start_idx"),
                        "end_idx": nn.get("end_idx"),

                        # ---- Additive: normalized unit fields (if already present or canonicalized) ----
                        "unit_tag": nn.get("unit_tag"),
                        "unit_family": nn.get("unit_family"),
                        "base_unit": nn.get("base_unit"),
                        "multiplier_to_base": nn.get("multiplier_to_base"),
                        "value_norm": nn.get("value_norm"),

                        # ---- Additive: semantic association tags (if present) ----
                        "measure_kind": nn.get("measure_kind"),
                        "measure_assoc": nn.get("measure_assoc"),
                    })(_maybe_canonicalize(n))
                    for n in nums
                    if isinstance(n, dict)
                ]
                # =====================================================================
            })

    if baseline_sources_cache:

        # ---- ADDITIVE: stable ordering of snapshots (Change #2) ----
        for s in (baseline_sources_cache or []):
            if isinstance(s, dict) and isinstance(s.get("extracted_numbers"), list):

                # =========================================================================
                # PATCH N3 (ADDITIVE): guard sort_snapshot_numbers if not defined
                # =========================================================================
                try:
                    if "sort_snapshot_numbers" in globals() and callable(globals()["sort_snapshot_numbers"]):
                        s["extracted_numbers"] = sort_snapshot_numbers(s["extracted_numbers"])
                    else:
                        # safe fallback: anchor_hash then raw
                        s["extracted_numbers"] = sorted(
                            s["extracted_numbers"],
                            key=lambda x: (str((x or {}).get("anchor_hash") or ""), str((x or {}).get("raw") or ""))
                        )
                except Exception:
                    pass
                # =========================================================================

                s["numbers_found"] = len(s["extracted_numbers"])

        baseline_sources_cache = sorted(
            baseline_sources_cache,
            key=lambda x: str((x or {}).get("url") or "")
        )
        # -----------------------------------------------------------

        # =====================================================================
        # PATCH INJ_HASH_V1_APPLY (ADDITIVE): optionally include injected URLs in snapshot hash identity
        # - Adds *synthetic* url-only source records for injected URLs that were
        #   persisted (per diag) but are missing from baseline_sources_cache.
        # - Default OFF; only activates when INCLUDE_INJECTED_URLS_IN_SNAPSHOT_HASH is enabled.
        # - Does NOT alter fastpath logic or metric selection (synthetic has no numbers).
        # =====================================================================
        _inj_hash_added = []
        _inj_hash_reasons = {}
        try:
            _diag_local = {}
            if isinstance(web_context, dict):
                _diag_local = web_context.get("diag_injected_urls") or web_context.get("extra_urls_debug") or {}
            _persisted_for_hash = []
            if isinstance(_diag_local, dict):
                _persisted_for_hash = _inj_diag_norm_url_list(
                    _diag_local.get("persisted_norm") or _diag_local.get("persisted") or []
                )
            _incl_inj_hash = _inj_hash_policy_should_include(_persisted_for_hash)
            if _incl_inj_hash and _persisted_for_hash:
                _bsc_aug, _inj_hash_added, _inj_hash_reasons = _inj_hash_add_synthetic_sources(
                    baseline_sources_cache,
                    _persisted_for_hash,
                    now_iso=_now_iso(),
                )
                baseline_sources_cache = _bsc_aug
        except Exception:
            pass
            _inj_hash_added = []
            _inj_hash_reasons = {}
        # =====================================================================

        analysis["baseline_sources_cache"] = baseline_sources_cache
        analysis.setdefault("results", {})
        if isinstance(analysis["results"], dict):

            # =====================================================================
            # PATCH INJ_DIAG_ATTACH_SNAPSHOTS (ADDITIVE): propagate injected-URL trace into analysis
            # - Captures persisted snapshot URLs + exact hash input URL set (A4-A5)
            # - Does NOT alter any gating/selection logic.
            # =====================================================================
            try:
                _diag = {}
                if isinstance(web_context, dict):
                    _diag = web_context.get("diag_injected_urls") or web_context.get("extra_urls_debug") or {}

                _inj_urls = []
                try:
                    _inj_urls = _fix2d66_collect_injected_urls(web_context or {}, question_text=str((analysis or {}).get('question') or ''))
                except Exception:
                    pass
                    _inj_urls = []

                _snap_urls = _inj_diag_hash_inputs_from_bsc(baseline_sources_cache)
                _hash_inputs = _snap_urls

                _h_v1 = ""
                _h_v2 = ""
                try:
                    _h_v1 = compute_source_snapshot_hash(baseline_sources_cache)
                except Exception:
                    pass
                    _h_v1 = ""
                try:
                    _h_v2 = compute_source_snapshot_hash_v2(baseline_sources_cache)
                except Exception:
                    pass
                    _h_v2 = ""

                analysis.setdefault("results", {})
                if isinstance(analysis.get("results"), dict):
                    analysis["results"].setdefault("debug", {})
                    if isinstance(analysis["results"].get("debug"), dict):
                        analysis["results"]["debug"].setdefault("inj_diag", {})
                        analysis["results"]["debug"]["inj_diag"].update({
                            "run_id": str((web_context or {}).get("diag_run_id") or _diag.get("run_id") or ""),
                            "injected_urls": _inj_urls[:50],
                            "snapshot_pool_urls_count": int(len(_snap_urls)),
                            "snapshot_pool_urls_hash": _inj_diag_set_hash(_snap_urls),
                            "hash_input_urls_count": int(len(_hash_inputs)),
                            "hash_input_urls_hash": _inj_diag_set_hash(_hash_inputs),
                            "injected_in_snapshot_pool": sorted(list(set(_inj_urls) & set(_snap_urls)))[:50],
                            "injected_in_hash_inputs": sorted(list(set(_inj_urls) & set(_hash_inputs)))[:50],
                            "computed_hash_v1": _h_v1,
                            "computed_hash_v2": _h_v2,
                        })


                        # =====================================================================
                        # PATCH INJ_TRACE_V1_EMIT_ANALYSIS (ADDITIVE): always emit canonical trace
                        # Location: analysis.results.debug.inj_trace_v1
                        # =====================================================================
                        try:

                            # =====================================================================
                            # PATCH INJ_TRACE_V1_ENRICH_ANALYSIS_ARTIFACTS (ADDITIVE)
                            # Ensure inj_trace_v1 shows attempted/persisted evidence even when
                            # upstream diag_injected_urls is partial (e.g., baseline/no-injection).
                            # Diagnostics only.
                            # =====================================================================
                            try:
                                if isinstance(_diag, dict):
                                    _diag = _inj_trace_v1_enrich_diag_from_bsc(_diag, baseline_sources_cache)
                            except Exception:
                                pass
                            # =====================================================================

                            _trace = _inj_trace_v1_build(
                                diag_injected_urls=_diag if isinstance(_diag, dict) else {},
                                hash_inputs=_hash_inputs,
                                stage="analysis",
                                path="analysis",
                                rebuild_pool=None,
                                rebuild_selected=None,
                                hash_exclusion_reasons=(_inj_hash_reasons if isinstance(locals().get('_inj_hash_reasons'), dict) else {}),
                            )
                            analysis["results"]["debug"].setdefault("inj_trace_v1", {})
                            # Do not overwrite if already present; only fill/merge
                            if isinstance(analysis["results"]["debug"].get("inj_trace_v1"), dict):
                                analysis["results"]["debug"]["inj_trace_v1"].update(_trace)
                        except Exception:
                            pass
                        # =====================================================================

            except Exception:
                pass
            # =====================================================================

        analysis["results"]["baseline_sources_cache"] = baseline_sources_cache


    # -----------------------------
    # RANGE capture for canonical metrics
    # -----------------------------
    pmc = analysis.get("primary_response", {}).get("primary_metrics_canonical") if isinstance(analysis.get("primary_response"), dict) else analysis.get("primary_metrics_canonical")
    schema = analysis.get("primary_response", {}).get("metric_schema_frozen") if isinstance(analysis.get("primary_response"), dict) else analysis.get("metric_schema_frozen")

    # Support both placements (your JSON seems to store these at top-level primary_response)
    if pmc is None and isinstance(analysis.get("primary_response"), dict):
        pmc = analysis["primary_response"].get("primary_metrics_canonical")
    if schema is None and isinstance(analysis.get("primary_response"), dict):
        schema = analysis["primary_response"].get("metric_schema_frozen")

    if isinstance(pmc, dict) and isinstance(schema, dict) and baseline_sources_cache:
        # flatten candidates
        all_cands = []
        for sr in baseline_sources_cache:
            for n in (sr.get("extracted_numbers") or []):
                if isinstance(n, dict):
                    all_cands.append(n)

        for ckey, m in pmc.items():
            if not isinstance(m, dict):
                continue
            mdef = schema.get(ckey) or {}
            uf = _unit_family_from_metric(mdef)
            keywords = mdef.get("keywords") or []

            kw_tokens = []
            for k in (keywords or []):
                kw_tokens.extend(_tokenize(str(k)))

            kw_tokens.extend(_tokenize(m.get("name") or m.get("original_name") or ""))
            kw_tokens = list(dict.fromkeys([t for t in kw_tokens if len(t) > 2]))[:40]

            vals = []
            examples = []

            for cand in all_cands:
                craw = str(cand.get("raw") or "")
                cunit = str(cand.get("unit") or "")
                ctx = str(cand.get("context_snippet") or cand.get("context") or "")

                # family gate
                cf = _cand_unit_family(cunit, craw)
                if uf == "PCT" and cf != "PCT":
                    continue
                if uf == "CUR" and cf != "CUR":
                    continue
                # MAG: allow MAG/OTHER but avoid CUR/PCT
                if uf == "MAG" and cf in ("CUR", "PCT"):
                    continue

                # NEW (additive): metric-aware magnitude gate
                if uf == "MAG":

                    cand_tag = _safe_norm_unit_tag(cunit or craw)
                    exp_tag = _safe_norm_unit_tag((mdef.get("unit") or "") or (m.get("unit") or ""))


                    if exp_tag in ("K", "M", "B", "T"):
                        if cand_tag != exp_tag:
                            continue
                    else:
                        if cand_tag not in ("K", "M", "B", "T"):
                            continue

                # token overlap gate
                c_tokens = set(_tokenize(ctx))
                if kw_tokens:
                    overlap = sum(1 for t in kw_tokens if t in c_tokens)
                    if overlap < max(1, min(3, len(kw_tokens) // 8)):
                        continue

                v = _parse_num(cand.get("value"), cunit) or _parse_num(craw, cunit)
                if v is None:
                    continue

                vals.append(float(v))
                if len(examples) < 5:
                    examples.append({
                        "raw": craw[:32],
                        "source_url": cand.get("source_url"),
                        "context_snippet": ctx[:180]
                    })

            if len(vals) >= 2:
                vmin = min(vals)
                vmax = max(vals)
                if abs(vmax - vmin) > max(1e-9, abs(vmin) * 0.02):
                    m["value_range"] = {
                        "min": vmin,
                        "max": vmax,
                        "n": len(vals),
                        "examples": examples,
                        "method": "snapshot_candidates"
                    }
                    try:
                        unit_disp = m.get("unit") or ""
                        m["value_range_display"] = f"{vmin:g}–{vmax:g} {unit_disp}".strip()
                    except Exception:
                        pass
    # =====================================================================
    # PATCH FIX2B_RANGE2 (ADDITIVE): override legacy snapshot_candidates range with schema-unit evidence range
    # =====================================================================
    try:
        _res = analysis.get("results") if isinstance(analysis, dict) else None
        if isinstance(_res, dict):
            for _ck, _m in _res.items():
                if not isinstance(_m, dict):
                    continue
                _ev = _m.get("evidence")
                if not isinstance(_ev, list) or len(_ev) < 2:
                    continue
                _vals = []
                for _e in _ev:
                    if not isinstance(_e, dict):
                        continue
                    _v = _e.get("value_norm")
                    if _v is None:
                        _v = _e.get("value")
                    if isinstance(_v, (int, float)):
                        try:
                            _vals.append(float(_v))
                        except Exception:
                            pass
                if len(_vals) < 2:
                    continue
                _vmin = min(_vals); _vmax = max(_vals)
                if abs(_vmax - _vmin) <= max(1e-9, abs(_vmin) * 0.02):
                    continue
                _m["value_range"] = {"min": _vmin, "max": _vmax, "n": len(_vals), "method": "ph2b_schema_unit_range_v1|fix2b_range2"}
                try:
                    _unit_disp = _m.get("unit") or _m.get("unit_tag") or ""
                    _m["value_range_display"] = f"{_vmin:g}–{_vmax:g} {_unit_disp}".strip()
                except Exception:
                    pass
    except Exception:
        pass
    # =====================================================================


    # =====================================================================
    # PATCH FIX2B_RANGE3 (ADDITIVE): Schema-unit value_range rebuild for primary_metrics_canonical
    #
    # Why:
    # - We observed value_range values scaled as if converted to billions (e.g., 17.8M -> 0.0178)
    #   while still displaying "million units", causing downstream eligibility drift.
    # - FIX2B_RANGE2 only patches analysis["results"] (often empty); the dashboard-facing
    #   canonicals live under analysis["primary_response"]["primary_metrics_canonical"].
    #
    # What:
    # - Rebuild value_range from baseline_sources_cache extracted_numbers, treating candidate.value_norm
    #   as schema units (NO double scaling), constrained to the metric's chosen source_url.
    # - Pure post-processing: NO IO, NO refetch, NO hashing changes.
    # =====================================================================
    try:
        import re
        _pr = analysis.get("primary_response") if isinstance(analysis, dict) else None
        _pmc = _pr.get("primary_metrics_canonical") if isinstance(_pr, dict) else None
        _schema = (
            (analysis.get("metric_schema_frozen") if isinstance(analysis, dict) else None)
            or (_pr.get("metric_schema_frozen") if isinstance(_pr, dict) else None)
            or {}
        )
        _bsc = analysis.get("baseline_sources_cache") if isinstance(analysis, dict) else None
        if isinstance(_pmc, dict) and isinstance(_schema, dict) and isinstance(_bsc, list) and _bsc:
            # Flatten candidate universe from snapshots
            _flat = []
            for _src in _bsc:
                if not isinstance(_src, dict):
                    continue
                _nums = _src.get("extracted_numbers")
                if isinstance(_nums, list):
                    _flat.extend([n for n in _nums if isinstance(n, dict)])
            # Helper: scale evidence presence for common magnitudes
            def _ph2b_scale_token_ok(_spec_unit_tag: str, _cand: dict) -> bool:
                try:
                    sut = str(_spec_unit_tag or "").lower()
                    if not sut:
                        return True
                    # Only enforce for explicit scaled magnitudes
                    if ("million" not in sut) and ("billion" not in sut) and ("thousand" not in sut) and ("trillion" not in sut):
                        return True
                    raw = str(_cand.get("raw") or "").lower()
                    ut = str(_cand.get("unit_tag") or _cand.get("unit") or "").lower()
                    ctx = str(_cand.get("context_snippet") or "").lower()
                    blob = " ".join([raw, ut, ctx])
                    if "million" in sut:
                        return ("million" in blob) or re.search(r"\bm\b", blob) is not None or " mn" in blob
                    if "billion" in sut:
                        return ("billion" in blob) or re.search(r"\bb\b", blob) is not None or " bn" in blob
                    if "thousand" in sut:
                        return ("thousand" in blob) or re.search(r"\bk\b", blob) is not None
                    if "trillion" in sut:
                        return ("trillion" in blob) or re.search(r"\bt\b", blob) is not None
                except Exception:
                    return True

            for _ck, _m in list(_pmc.items()):
                if not isinstance(_m, dict):
                    continue
                _spec = _schema.get(_ck) if isinstance(_schema, dict) else None
                if not isinstance(_spec, dict):
                    continue
                _src_url = _m.get("source_url") or _spec.get("preferred_url") or ""
                if not _src_url:
                    continue
                _src_url_n = _ph2b_norm_url(_src_url)
                # Collect eligible vals from same source_url
                _vals = []
                _examples = []
                for _c in _flat:
                    try:
                        if _ph2b_norm_url(_c.get("source_url") or "") != _src_url_n:
                            continue
                        if _c.get("is_junk") is True:
                            continue
                        # Reuse FIX16 allowlist if present
                        try:
                            if callable(globals().get("_fix16_candidate_allowed")):
                                if not globals().get("_fix16_candidate_allowed")(_c, _spec, canonical_key=_ck):
                                    continue
                        except Exception:
                            pass
                        # Enforce scale token for scaled magnitudes
                        if not _ph2b_scale_token_ok(_spec.get("unit_tag") or _spec.get("unit") or "", _c):
                            continue
                        _v = _c.get("value_norm")
                        if _v is None:
                            _v = _c.get("value")
                        if isinstance(_v, (int, float)):
                            _vals.append(float(_v))
                            if len(_examples) < 4:
                                _examples.append({
                                    "raw": _c.get("raw"),
                                    "source_url": _c.get("source_url"),
                                    "context_snippet": (str(_c.get("context_snippet") or "")[:180])
                                })
                    except Exception:
                        pass
                        continue
                if len(_vals) < 2:
                    continue
                _vmin = min(_vals); _vmax = max(_vals)
                if abs(_vmax - _vmin) <= max(1e-9, abs(_vmin) * 0.02):
                    continue
                _m["value_range"] = {
                    "min": _vmin,
                    "max": _vmax,
                    "n": len(_vals),
                    "examples": _examples,
                    "method": "ph2b_schema_unit_range_v2|fix2b_range3"
                }
                try:
                    _unit_disp = _m.get("unit") or _m.get("unit_tag") or _spec.get("unit_tag") or ""
                    _m["value_range_display"] = f"{_vmin:g}–{_vmax:g} {_unit_disp}".strip()
                except Exception:
                    pass
    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH FIX2B_RANGE4 (ADDITIVE): late override of snapshot_candidates range builder (schema-unit semantics)
    #
    # Location:
    # - This runs AFTER the legacy "snapshot_candidates" range builder blocks above.
    #
    # Goal:
    # - Ensure value_range is computed in *schema units* (treat candidate.value_norm as schema units),
    #   avoiding any double-scaling/divide behavior.
    # - Constrain range computation to the metric's chosen current source_url when present,
    #   preventing cross-source range pollution.
    #
    # Notes:
    # - Downstream-only post-processing. Does NOT affect selection, only range/min/max display.
    # - Safe even when evidence is missing: it simply no-ops.
    # =====================================================================
    try:
        _pr = analysis.get("primary_response") if isinstance(analysis, dict) else None
        _pmc = _pr.get("primary_metrics_canonical") if isinstance(_pr, dict) else None
        _schema = (
            (analysis.get("metric_schema_frozen") if isinstance(analysis, dict) else None)
            or (_pr.get("metric_schema_frozen") if isinstance(_pr, dict) else None)
            or {}
        )
        _bsc = analysis.get("baseline_sources_cache") if isinstance(analysis, dict) else None

        if isinstance(_pmc, dict) and isinstance(_schema, dict) and isinstance(_bsc, list) and _bsc:
            # Flatten candidates once
            _flat = []
            for _src in _bsc:
                if not isinstance(_src, dict):
                    continue
                _nums = _src.get("extracted_numbers")
                if isinstance(_nums, list):
                    _flat.extend([n for n in _nums if isinstance(n, dict)])

            def _ph2b_unit_tag_norm(x: str) -> str:
                return _safe_norm_unit_tag(str(x or ""))

            for _ckey, _m in _pmc.items():
                if not isinstance(_m, dict):
                    continue

                _mdef = _schema.get(_ckey) if isinstance(_schema, dict) else None
                _mdef = _mdef if isinstance(_mdef, dict) else {}
                _exp_unit = str(_mdef.get("unit") or _m.get("unit") or _m.get("unit_tag") or "")
                _exp_tag = _ph2b_unit_tag_norm(_exp_unit)

                # If the metric already has a source_url, treat that as the range scope
                _scope_url = _m.get("cur_source_url") or _m.get("source_url") or ""
                _scope_url_n = _norm_url(_scope_url) if _scope_url else ""

                _vals = []
                for _cand in _flat:
                    if not isinstance(_cand, dict):
                        continue
                    # source scope (if known)
                    if _scope_url_n:
                        _c_url = _cand.get("source_url") or ""
                        if _norm_url(_c_url) != _scope_url_n:
                            continue
                    # unit_tag scope (only enforce when schema declares a scaled magnitude tag)
                    if _exp_tag:
                        _cand_tag = _ph2b_unit_tag_norm(_cand.get("unit_tag") or _cand.get("unit") or _cand.get("raw") or "")
                        if _exp_tag in ("K", "M", "B", "T") and _cand_tag != _exp_tag:
                            continue
                    _vn = _cand.get("value_norm")
                    if _vn is None:
                        continue
                    if isinstance(_vn, (int, float)):
                        try:
                            _vals.append(float(_vn))
                        except Exception:
                            pass

                if len(_vals) >= 2:
                    _vmin = min(_vals); _vmax = max(_vals)
                    if abs(_vmax - _vmin) > max(1e-9, abs(_vmin) * 0.02):
                        _m["value_range"] = {
                            "min": _vmin,
                            "max": _vmax,
                            "n": len(_vals),
                            "method": "ph2b_schema_unit_range_v2|fix2b_range4",
                            "scope_url": _scope_url_n or "",
                            "scope_unit_tag": _exp_tag or ""
                        }
                        try:
                            _unit_disp = _m.get("unit") or _m.get("unit_tag") or _exp_unit or ""
                            _m["value_range_display"] = f"{_vmin:g}–{_vmax:g} {_unit_disp}".strip()
                        except Exception:
                            pass
    except Exception:
        pass
    # =====================================================================



    # =====================================================================


    # =====================================================================

    # =====================================================================
    # PATCH FIX2D43 (ADD): Serialization correction — bridge primary_response fields
    # - attach_source_snapshots_to_analysis receives the *top-level* output dict,
    #   where schema/pmc live under primary_response.
    # - Prior FIX2D31/FIX2D38 baseline_schema_metrics_v1 builder reads analysis.*
    #   and therefore never ran, dropping baseline_schema_metrics_v1 from JSON.
    # - This shim mirrors metric_schema_frozen / primary_metrics_canonical / metric_anchors
    #   from primary_response into top-level analysis so the existing builder executes.
    # - Additive only: does not change selector semantics.
    # =====================================================================
    try:
        _pr = analysis.get('primary_response')
        if isinstance(_pr, dict):
            if not isinstance(analysis.get('metric_schema_frozen'), dict) and isinstance(_pr.get('metric_schema_frozen'), dict):
                analysis['metric_schema_frozen'] = _pr.get('metric_schema_frozen')
            if not isinstance(analysis.get('primary_metrics_canonical'), dict) and isinstance(_pr.get('primary_metrics_canonical'), dict):
                analysis['primary_metrics_canonical'] = _pr.get('primary_metrics_canonical')
            if not isinstance(analysis.get('metric_anchors'), dict) and isinstance(_pr.get('metric_anchors'), dict):
                analysis['metric_anchors'] = _pr.get('metric_anchors')
    except Exception:
        pass
    # =====================================================================


    # =====================================================================
    # =====================================================================
    # PATCH FIX2D65D (ADD): Ensure metric_schema_frozen is always serialized
    #
    # Problem observed:
    # - Some Analysis runs (especially narrative questions) emit no metric_schema_frozen at all.
    # - Evolution schema-only rebuild depends on prev_response.metric_schema_frozen keyspace;
    #   when missing, rebuild returns empty and Diff Panel V2 shows an empty metric change table.
    #
    # Policy:
    # - Always ensure metric_schema_frozen exists at top-level analysis output.
    # - Seed deterministically using known schema extension functions when missing/empty.
    # - No fetch, no heuristic matching.
    # =====================================================================
    try:
        _schema0 = analysis.get('metric_schema_frozen')
        if not isinstance(_schema0, dict) or not _schema0:
            _schema_seed = {}
            for _fn_name in (
                '_fix2u_extend_metric_schema_ev_chargers',
                '_fix2v_extend_metric_schema_ev_chargers_cagr',
                '_fix2ab_extend_metric_schema_global_ev_sales_ytd_2025',
            ):
                try:
                    _fn = globals().get(_fn_name)
                    if callable(_fn):
                        _schema_seed = _fn(_schema_seed) or _schema_seed
                except Exception:
                    pass
            analysis['metric_schema_frozen'] = _schema_seed
            try:
                if not isinstance(analysis.get('debug'), dict):
                    analysis['debug'] = {}
                analysis['debug']['fix2d65d_seeded_schema'] = True
                analysis['debug']['fix2d65d_schema_keys'] = int(len(_schema_seed)) if isinstance(_schema_seed, dict) else 0
            except Exception:
                pass
    except Exception:
        pass
    # =====================================================================


# PATCH FIX2D65B (ADD): Force canonical pipeline when injected URLs exist
    #
    # Problem:
    # - Some narrative / market-size queries return no primary_metrics in primary_response.
    # - When injected URLs are present, we still want schema+baseline canonicals to materialise
    #   so Evolution schema-only rebuild and Diff Panel V2 can activate.
    #
    # Policy:
    # - If injected URLs exist and metric_schema_frozen is empty/missing, seed an empty schema and
    #   apply deterministic schema extension patches. This enables FIX2D31 schema-authority rebuild
    #   to run over the baseline candidate universe without changing UI/diff logic.
    #
    # Safety:
    # - No refetch, no heuristic matching. Additive only.
    # =====================================================================
    try:
        # Collect injected URLs from UI/diagnostic fields only (plus explicit internal marker fallback)
        # to avoid misclassifying production source lists as "injected".
        _inj_urls = _yureeka_extract_injected_urls_v1(web_context)

        _has_inj = bool(_inj_urls)
        if _has_inj:
            try:
                analysis.setdefault("debug", {})
                if isinstance(analysis.get("debug"), dict):
                    analysis["debug"]["fix2d65b_forced_canonical_pipeline"] = True
                    analysis["debug"]["fix2d65b_injected_urls"] = _inj_urls[:10]
                    analysis["debug"]["injected_urls_v1"] = _inj_urls[:10]
            except Exception:
                pass

        # Ensure metric_schema_frozen exists even when LLM emitted no primary_metrics.
        # NOTE: We no longer gate this on injected URLs: Evolution schema-only rebuild
        # requires a baseline keyspace, and the deterministic schema extensions are safe.
        _schema0 = analysis.get("metric_schema_frozen")
        if not isinstance(_schema0, dict) or not _schema0:
            _schema_seed = {}

            # Apply known deterministic schema extensions (additive)
            for _fn_name in (
                "_fix2u_extend_metric_schema_ev_chargers",
                "_fix2v_extend_metric_schema_ev_chargers_cagr",
                "_fix2ab_extend_metric_schema_global_ev_sales_ytd_2025",
            ):
                try:
                    _fn = globals().get(_fn_name)
                    if callable(_fn):
                        _schema_seed = _fn(_schema_seed) or _schema_seed
                except Exception:
                    pass

            analysis["metric_schema_frozen"] = _schema_seed
    except Exception:
        pass
    # =====================================================================

    # PATCH FIX2D31 (ADD): Option A — schema-align Analysis primary_metrics_canonical
    # - When metric_schema_frozen exists in Analysis, rebuild primary_metrics_canonical
    #   by running the authoritative Analysis selector (_analysis_canonical_final_selector_v1)
    #   constrained to the frozen schema keys, over the same baseline candidate universe.
    # - This makes Analysis emit schema-aligned baseline metrics so Evolution injection can overlap
    #   and Diff Panel V2 can activate without weakening semantics.
    # =====================================================================
    try:
        _core = analysis
        schema = _core.get("metric_schema_frozen")
        pmc = _core.get("primary_metrics_canonical")
        sel = globals().get("_analysis_canonical_final_selector_v1")
        if isinstance(schema, dict) and schema and callable(sel):
            # Preserve original unconstrained PMC for diagnostics
            if isinstance(pmc, dict) and pmc and "primary_metrics_canonical_unconstrained_v0" not in analysis:
                _core["primary_metrics_canonical_unconstrained_v0"] = dict(pmc)

            # Flatten candidate universe from baseline_sources_cache + web_context.scraped_meta
            flat = []
            bsc = analysis.get("baseline_sources_cache")
            if isinstance(bsc, list):
                for src in bsc:
                    if isinstance(src, dict) and isinstance(src.get("extracted_numbers"), list):
                        flat.extend([n for n in src.get("extracted_numbers") if isinstance(n, dict)])
            if isinstance(web_context, dict):
                sm = web_context.get("scraped_meta")
                if isinstance(sm, dict) and isinstance(sm.get("extracted_numbers"), list):
                    flat.extend([n for n in sm.get("extracted_numbers") if isinstance(n, dict)])

            # Best-effort canonicalize candidates (if helper exists)
            try:
                fn_can = globals().get("canonicalize_numeric_candidate")
                if callable(fn_can):
                    flat = [fn_can(dict(n)) if isinstance(n, dict) else n for n in flat]
            except Exception:
                pass

            anchors = (_core.get("metric_anchors") or analysis.get("metric_anchors"))
            if not isinstance(anchors, dict):
                anchors = {}

            new_pmc = {}
            sel_trace = {}
            for ckey in sorted(schema.keys()):
                spec = schema.get(ckey) or {}
                if not isinstance(spec, dict):
                    continue
                try:
                    out_m, meta = sel(
                        ckey,
                        spec,
                        flat,
                        anchors=anchors,
                        prev_metric=(pmc or {}).get(ckey) if isinstance(pmc, dict) else None,
                        web_context=web_context,
                    )
                    if isinstance(out_m, dict):
                        # FIX2D33: baseline commitment for schema keys.
                        # If selector chose a winner but value_norm is missing, deterministically parse from value/raw.
                        if out_m.get("value_norm") is None:
                            try:
                                fn_parse = globals().get("_parse_num")
                                if callable(fn_parse):
                                    unit_hint = str(out_m.get("unit") or "")
                                    v_try = fn_parse(out_m.get("value"), unit_hint)
                                    if v_try is None:
                                        # fall back to evidence raw
                                        ev0 = None
                                        if isinstance(out_m.get("evidence"), list) and out_m.get("evidence"):
                                            ev0 = out_m.get("evidence")[0]
                                        if isinstance(ev0, dict):
                                            v_try = fn_parse(ev0.get("raw"), unit_hint)
                                    if v_try is not None:
                                        try:
                                            out_m["value_norm"] = float(v_try)
                                        except Exception:
                                            pass
                                            out_m["value_norm"] = v_try
                            except Exception:
                                pass
                        if out_m.get("value_norm") is not None:
                            # FIX2D39: schema-authoritative dimension/unit_family hard-binding.
                            # - If winner has a concrete (non-unknown) dimension/unit_family that CONFLICTS with schema, reject it.
                            # - Otherwise, set dimension/unit_family to schema spec (authoritative) and stamp audit flags.
                            try:
                                spec_dim = ""
                                spec_uf = ""
                                if isinstance(spec, dict):
                                    spec_dim = str(spec.get("dimension") or spec.get("dim") or "").strip()
                                    spec_uf = str(spec.get("unit_family") or spec.get("unitFamily") or "").strip()
                                prior_dim = out_m.get("dimension")
                                prior_uf = out_m.get("unit_family")
                                prior_dim_s = ("" if prior_dim is None else str(prior_dim).strip())
                                prior_uf_s = ("" if prior_uf is None else str(prior_uf).strip())

                                # Reject conflicts: non-unknown prior value disagrees with schema.
                                if spec_dim and prior_dim_s and prior_dim_s not in ("unknown",) and prior_dim_s != spec_dim:
                                    meta = dict(meta) if isinstance(meta, dict) else {}
                                    meta["fix2d39_schema_dim_conflict_v1"] = True
                                    meta["fix2d39_schema_dim_prior_v1"] = prior_dim_s
                                    meta["fix2d39_schema_dim_spec_v1"] = spec_dim
                                    sel_trace[ckey] = meta
                                    continue
                                if spec_uf and prior_uf_s and prior_uf_s not in ("unknown",) and prior_uf_s != spec_uf:
                                    meta = dict(meta) if isinstance(meta, dict) else {}
                                    meta["fix2d39_schema_uf_conflict_v1"] = True
                                    meta["fix2d39_schema_uf_prior_v1"] = prior_uf_s
                                    meta["fix2d39_schema_uf_spec_v1"] = spec_uf
                                    sel_trace[ckey] = meta
                                    continue

                                # Apply schema authority (overwrite empty/unknown, and also normalize same-values).
                                if spec_dim:
                                    if prior_dim_s != spec_dim:
                                        out_m["dimension_prior_v1"] = prior_dim
                                        out_m["dimension_coerced_from_schema_v1"] = True
                                    out_m["dimension"] = spec_dim
                                if spec_uf:
                                    if prior_uf_s != spec_uf:
                                        out_m["unit_family_prior_v1"] = prior_uf
                                        out_m["unit_family_coerced_from_schema_v1"] = True
                                    out_m["unit_family"] = spec_uf
                            except Exception:
                                pass
                            new_pmc[ckey] = out_m
                    sel_trace[ckey] = meta if isinstance(meta, dict) else {"blocked_reason": "no_meta"}
                except Exception as _e:
                    sel_trace[ckey] = {"blocked_reason": "selector_exception", "error": str(_e)[:200]}

            # FIX2D38: Always build a schema-keyed baseline map for diffing.
            # Even when no schema metric wins selection (new_pmc empty), we still emit a baseline_schema_metrics_v1
            # by filling missing schema keys from flat candidates as proxy baselines. This ensures the prev universe
            # is schema-keyed and numeric where possible, so baseline diffing can activate.
            baseline_schema = {}
            try:
                if isinstance(new_pmc, dict) and new_pmc:
                    baseline_schema.update(new_pmc)
            except Exception:
                pass
            # FIX2D40: Remap baseline metrics from generic canonical keys onto schema canonical keys (one-to-one).
            # Rationale: Analysis may emit strong baseline metrics under generic keys (e.g., "2025_global_ev_sales__unknown")
            # while Evolution/injection emits schema keys (e.g., "global_ev_sales_ytd_2025__unit_sales"). Without remap,
            # prev_value_norm stays null and diff cannot activate.
            try:
                pmc_src = analysis.get('primary_metrics_canonical')
                used_src_keys = set()
                if isinstance(schema, dict) and isinstance(pmc_src, dict):
                    def _fix2d40_score(_ck, _m, _schema_key, _spec):
                        try:
                            txt = " ".join([
                                str(_ck or ""),
                                str(_m.get('name') or ""),
                                str(_m.get('context_snippet') or ""),
                                str(_m.get('source_url') or ""),
                            ]).lower()
                        except Exception:
                            pass
                            txt = str(_ck or "").lower()
                        s = 0
                        # Basic topic cues
                        if 'sales' in _schema_key and 'sales' in txt:
                            s += 3
                        if 'chargers' in _schema_key and ('charger' in txt or 'charging' in txt):
                            s += 3
                        if 'cagr' in _schema_key and ('cagr' in txt or 'growth' in txt or 'rate' in txt):
                            s += 3
                        if 'investment' in _schema_key and ('invest' in txt or 'capex' in txt):
                            s += 3
                        # Year cues
                        if '2025' in _schema_key and '2025' in txt:
                            s += 2
                        if '2040' in _schema_key and '2040' in txt:
                            s += 2
                        if '2026' in _schema_key and '2026' in txt:
                            s += 1
                        if '2030' in _schema_key and '2030' in txt:
                            s += 1
                        # Penalize obvious mismatches
                        if 'china' in txt and 'global' in _schema_key:
                            s -= 5
                        # Unit-family compatibility quick check
                        try:
                            spec_dim = str((_spec or {}).get('dimension') or '').strip().lower()
                            spec_uf = str((_spec or {}).get('unit_family') or (_spec or {}).get('unitFamily') or '').strip().lower()
                            prior_uf = str(_m.get('unit_family') or '').strip().lower()
                            if spec_uf and prior_uf and prior_uf not in ('unknown',) and prior_uf != spec_uf:
                                s -= 4
                            prior_dim = str(_m.get('dimension') or '').strip().lower()
                            if spec_dim and prior_dim and prior_dim not in ('unknown',) and prior_dim != spec_dim:
                                s -= 4
                        except Exception:
                            pass
                        # Prefer numeric
                        if _m.get('value_norm') is not None:
                            s += 2
                        return s

                    # For each schema key missing from baseline_schema, pick best candidate from pmc_src
                    for _skey, _spec in schema.items():
                        if not _skey or _skey in baseline_schema:
                            continue
                        best_k = None
                        best_m = None
                        best_score = None
                        for _ck, _m in pmc_src.items():
                            if _ck in used_src_keys:
                                continue
                            if not isinstance(_m, dict):
                                continue
                            sc = _fix2d40_score(_ck, _m, _skey, _spec)
                            if best_score is None or sc > best_score:
                                best_score = sc
                                best_k = _ck
                                best_m = _m
                        # Require a minimum positive score to avoid accidental remaps
                        if best_m is None or (best_score is None) or (best_score < 4):
                            continue

                        out_m = dict(best_m)
                        # Mark remap audit
                        out_m['schema_remap_v1'] = True
                        out_m['schema_remap_from_ckey_v1'] = best_k
                        out_m['schema_remap_to_ckey_v1'] = _skey
                        out_m['schema_remap_score_v1'] = best_score
                        out_m['is_proxy'] = True
                        out_m['proxy_type'] = 'schema_remap'
                        out_m['proxy_reason'] = 'schema_remap'

                        # Enforce FIX2D39 hard-binding against schema (reject conflicts)
                        try:
                            spec_dim = str((_spec or {}).get('dimension') or (_spec or {}).get('dim') or '').strip()
                            spec_uf = str((_spec or {}).get('unit_family') or (_spec or {}).get('unitFamily') or '').strip()
                            prior_dim = out_m.get('dimension')
                            prior_uf = out_m.get('unit_family')
                            prior_dim_s = ('' if prior_dim is None else str(prior_dim).strip())
                            prior_uf_s = ('' if prior_uf is None else str(prior_uf).strip())
                            if spec_dim and prior_dim_s and prior_dim_s not in ('unknown',) and prior_dim_s != spec_dim:
                                continue
                            if spec_uf and prior_uf_s and prior_uf_s not in ('unknown',) and prior_uf_s != spec_uf:
                                continue
                            if spec_dim:
                                if prior_dim_s != spec_dim:
                                    out_m['dimension_prior_v1'] = prior_dim
                                    out_m['dimension_coerced_from_schema_v1'] = True
                                out_m['dimension'] = spec_dim
                            if spec_uf:
                                if prior_uf_s != spec_uf:
                                    out_m['unit_family_prior_v1'] = prior_uf
                                    out_m['unit_family_coerced_from_schema_v1'] = True
                                out_m['unit_family'] = spec_uf
                        except Exception:
                            pass

                        # Override canonical key fields if present
                        try:
                            out_m['canonical_key'] = _skey
                            out_m['ckey'] = _skey
                        except Exception:
                            pass

                        baseline_schema[_skey] = out_m
                        used_src_keys.add(best_k)
            except Exception:
                pass

            # Fill gaps with schema fallback from flat candidates (proxy baselines)
            try:
                if isinstance(schema, dict) and isinstance(flat, list):
                    for ckey, spec in schema.items():
                        if not ckey or ckey in baseline_schema:
                            continue
                        best = None
                        for cand in flat:
                            try:
                                if not isinstance(cand, dict):
                                    continue
                                ck = cand.get('canonical_key') or cand.get('ckey') or cand.get('canon_key')
                                if ck != ckey:
                                    continue
                                # Prefer numeric candidates
                                if cand.get('value_norm') is not None:
                                    best = cand
                                    break
                                if best is None:
                                    best = cand
                            except Exception:
                                pass
                                continue
                        if best is None:
                            continue
                        out_m = dict(best)
                        # Ensure normalized numeric when possible (reuse FIX2D33 parsing helper if present)
                        try:
                            if out_m.get('value_norm') is None:
                                _vraw = out_m.get('value')
                                if (_vraw is None) and isinstance(out_m.get('evidence'), list) and out_m['evidence']:
                                    _ev0 = out_m['evidence'][0]
                                    if isinstance(_ev0, dict):
                                        _vraw = _ev0.get('raw') or _ev0.get('text')
                                if _vraw is not None:
                                    try:
                                        _parsed = _fix2d33_parse_value_norm_v1(_vraw, out_m) if ' _fix2d33_parse_value_norm_v1'.strip() else None
                                    except Exception:
                                        pass
                                        _parsed = None
                                    if _parsed is not None:
                                        out_m['value_norm'] = _parsed
                            # FIX2D39: schema-authoritative dimension/unit_family hard-binding (fallback path)
                            spec_dim = ''
                            spec_uf = ''
                            if isinstance(spec, dict):
                                spec_dim = str(spec.get('dimension') or spec.get('dim') or '').strip()
                                spec_uf = str(spec.get('unit_family') or spec.get('unitFamily') or '').strip()
                            prior_dim = out_m.get('dimension')
                            prior_uf = out_m.get('unit_family')
                            prior_dim_s = ('' if prior_dim is None else str(prior_dim).strip())
                            prior_uf_s = ('' if prior_uf is None else str(prior_uf).strip())

                            # Reject conflicts: non-unknown prior value disagrees with schema.
                            if spec_dim and prior_dim_s and prior_dim_s not in ('unknown',) and prior_dim_s != spec_dim:
                                continue
                            if spec_uf and prior_uf_s and prior_uf_s not in ('unknown',) and prior_uf_s != spec_uf:
                                continue

                            # Apply schema authority.
                            if spec_dim:
                                if prior_dim_s != spec_dim:
                                    out_m['dimension_coerced_from_schema_v1'] = True
                                    out_m['dimension_prior_v1'] = prior_dim
                                out_m['dimension'] = spec_dim
                            if spec_uf:
                                if prior_uf_s != spec_uf:
                                    out_m['unit_family_coerced_from_schema_v1'] = True
                                    out_m['unit_family_prior_v1'] = prior_uf
                                out_m['unit_family'] = spec_uf
                        except Exception:
                            pass
                        # Mark as proxy baseline fallback
                        out_m['is_proxy'] = True
                        out_m['proxy_type'] = 'schema_fallback'
                        out_m['proxy_reason'] = 'schema_fallback'
                        baseline_schema[ckey] = out_m
            except Exception:
                pass

            # FIX2D71: Always commit a schema-keyed baseline PMC for downstream diffing.
            # - If selector produced schema winners (new_pmc), commit those.
            # - Otherwise, if we have a schema-keyed baseline map (baseline_schema) and PMC is empty,
            #   commit the proxy baseline map so Evolution has prev canonical metrics to diff against.
            # This does NOT reintroduce heuristics: baseline_schema is schema-keyed and fully auditable via
            # is_proxy/proxy_reason flags.
            if new_pmc:
                _core['primary_metrics_canonical'] = new_pmc
                try:
                    if not isinstance(analysis.get('results'), dict):
                        analysis['results'] = {}
                    analysis.setdefault('results', {}).setdefault('debug', {})
                    if isinstance(analysis['results'].get('debug'), dict):
                        analysis['results']['debug']['fix2d71_committed_pmc_mode'] = 'selector_winners'
                except Exception:
                    pass
            else:
                try:
                    _pmc0 = _core.get('primary_metrics_canonical')
                    _pmc0_empty = (not isinstance(_pmc0, dict)) or (not _pmc0)
                    if _pmc0_empty and isinstance(baseline_schema, dict) and baseline_schema:
                        _core['primary_metrics_canonical'] = dict(baseline_schema)
                        # Stamp audit flag at top-level (lightweight)
                        try:
                            if not isinstance(_core.get('debug'), dict):
                                _core['debug'] = {}
                            if isinstance(_core.get('debug'), dict):
                                _core['debug']['fix2d71_committed_proxy_baseline_pmc_v1'] = True
                                _core['debug']['fix2d71_proxy_baseline_pmc_count_v1'] = len(baseline_schema)
                        except Exception:
                            pass
                        try:
                            if not isinstance(analysis.get('results'), dict):
                                analysis['results'] = {}
                            analysis.setdefault('results', {}).setdefault('debug', {})
                            if isinstance(analysis['results'].get('debug'), dict):
                                analysis['results']['debug']['fix2d71_committed_pmc_mode'] = 'proxy_baseline_schema_map'
                                analysis['results']['debug']['fix2d71_proxy_baseline_pmc_count_v1'] = len(baseline_schema)
                        except Exception:
                            pass
                except Exception:
                    pass

            # FIX2D38: emit schema-keyed baseline metrics map for diffing (always, with schema fallback).
            if baseline_schema:
                try:
                    _core['baseline_schema_metrics_v1'] = dict(baseline_schema)
                except Exception:
                    pass
                    _core['baseline_schema_metrics_v1'] = baseline_schema
                try:
                    if not isinstance(analysis.get('results'), dict):
                        analysis['results'] = {}
                    analysis['results']['baseline_schema_metrics_v1'] = _core.get('baseline_schema_metrics_v1')
                    try:
                        if _core is not analysis and isinstance(_core, dict) and 'baseline_schema_metrics_v1' in _core:
                            # keep it visible under primary_response for inspection
                            pass
                    except Exception:
                        pass
                except Exception:
                    pass

            # Debug trace
            try:
                if not isinstance(analysis.get('results'), dict):
                    analysis['results'] = {}
                if not isinstance(analysis['results'].get('debug'), dict):
                    analysis['results']['debug'] = {}
                analysis['results']['debug']['fix2d38_schema_baseline_map'] = {
                    'enabled': True,
                    'schema_key_count': int(len(schema)) if isinstance(schema, dict) else 0,
                    'flat_candidate_count': int(len(flat)) if isinstance(flat, list) else 0,
                    'selected_schema_count': int(len(new_pmc)) if isinstance(new_pmc, dict) else 0,
                    'baseline_schema_count': int(len(baseline_schema)) if isinstance(baseline_schema, dict) else 0,
                }
            except Exception:
                pass
    except Exception:
        pass
    # =====================================================================
    # PATCH V1 (ADDITIVE): analysis & schema version stamping
    # - Pure metadata, NO logic impact
    # - Allows downstream drift attribution:
    #     * pipeline changes vs source changes
    # =====================================================================
    analysis.setdefault("analysis_pipeline_version", "v7_41_endstate_wip_1")
    analysis.setdefault("metric_identity_version", "canon_v2_dim_safe")
    analysis.setdefault("schema_freeze_version", 1)
    # =====================================================================

    # =========================
    # VERSION STAMP (ADDITIVE)
    # =========================
    analysis.setdefault("code_version", _yureeka_get_code_version())
    # =========================


    return analysis



def normalize_unit(unit: str) -> str:
    """Normalize unit to one of: T/B/M/%/'' (deterministic)."""
    if not unit:
        return ""
    u = unit.strip().upper().replace("USD", "").replace("$", "").replace(" ", "")
    if u in ["TRILLION", "T"]:
        return "T"
    if u in ["BILLION", "B"]:
        return "B"
    if u in ["MILLION", "M"]:
        return "M"
    if u in ["PERCENT", "%"]:
        return "%"
    if u in ["K", "THOUSAND"]:
        return "K"
    return u

def normalize_currency_prefix(raw: str) -> bool:
    """True if looks like a currency number ($/USD)."""
    if not raw:
        return False
    s = raw.strip().upper()
    return s.startswith("$") or " USD" in s or s.startswith("USD")

def is_likely_junk_context(ctx: str) -> bool:
    """
    Returns True if a context snippet strongly indicates the number is coming from
    HTML/JS/CSS/asset junk (srcset resize params, scripts, svg path data, etc.)
    rather than real narrative/tabular data.
    """
    import re

    c = (ctx or "").strip()
    if not c:
        return True

    cl = c.lower()

    # Too much binary / garbled text (common when PDF bytes leak through)
    non_print = sum(1 for ch in c if ord(ch) < 9 or (13 < ord(ch) < 32))
    if non_print > 0:
        return True

    # Lots of replacement chars / unusual glyphs → decode garbage
    bad_glyphs = c.count("\ufffd")
    if bad_glyphs >= 1:
        return True

    # Very long uninterrupted “code-ish” context
    if len(c) > 260 and ("{" in c and "}" in c) and ("function" in cl or "var " in cl or "const " in cl):
        return True

    # Hard “asset / markup / script” indicators
    hard_hints = [
        "srcset=", "resize=", "quality=", "offsc", "offscreencanvas", "createelement(\"canvas\")",
        "willreadfrequently", "function(", "webpack", "window.", "document.", "var ", "const ",
        "<script", "</script", "<style", "</style", "text/javascript", "application/javascript",
        "og:image", "twitter:image", "meta property=", "content=\"width=device-width",
        "/wp-content/", ".jpg", ".jpeg", ".png", ".svg", ".webp", ".css", ".js", ".woff", ".woff2",
        "data:image", "base64,", "viewbox", "path d=", "d=\"m", "aria-label=", "class=\""
    ]
    if any(h in cl for h in hard_hints):
        return True

    # SVG path command patterns like "h4.16v-2.56"
    if re.search(r"(?:^|[^a-z0-9])[a-z]\d+(?:\.\d+)?[a-z]-?\d", cl):
        return True

    # Image resize query param like "...jpg?resize=770%2C513..."
    if re.search(r"resize=\d+%2c\d+", cl):
        return True

    # Phone / tracking / footer junk often has lots of separators and few letters
    letters = sum(1 for ch in c if ch.isalpha())
    if len(c) >= 120 and letters / max(1, len(c)) < 0.08:
        return True

    return False


def parse_human_number(value_str: str, unit: str) -> Optional[float]:
    """
    Parse number + unit into a comparable float scale.
    - For T/B/M: returns value in billions (B) to compare apples-to-apples.
    - For %: returns numeric percent.
    """
    if value_str is None:
        return None

    s = str(value_str).strip()
    if not s:
        return None

    # remove currency symbols/commas/space
    s = s.replace("$", "").replace(",", "").strip()

    # handle parentheses for negatives e.g. (12.3)
    if s.startswith("(") and s.endswith(")"):
        s = "-" + s[1:-1].strip()

    try:
        v = float(s)
    except Exception:
        return None

    u = normalize_unit(unit)

    # Normalize magnitudes into BILLIONS for currency-like units
    if u == "T":
        return v * 1000.0
    if u == "B":
        return v
    if u == "M":
        return v / 1000.0
    if u == "K":
        return v / 1_000_000.0

    # Percent: keep as percent number
    if u == "%":
        return v

    # Unknown unit: leave as-is (still useful for ratio filtering)
    return v

def build_prev_numbers(prev_metrics: Dict) -> Dict[str, Dict]:
    """
    Build previous metric lookup keyed by metric_name string.
    Stores:
      - parsed numeric value (for matching)
      - normalized unit (for gating)
      - raw display string INCLUDING currency/magnitude (for dashboards + evolution JSON)
      - raw_value/raw_unit for debugging
    """
    def _format_raw_display(value: Any, unit: str) -> str:
        v = "" if value is None else str(value).strip()
        u = (unit or "").strip()

        if not v:
            return ""

        # Currency prefix handling (SGD/USD keywords OR symbol prefixes)
        currency = ""
        u_nospace = u.replace(" ", "")

        if u_nospace.upper().startswith("SGD"):
            currency = "S$"
            u_tail = u_nospace[3:]
        elif u_nospace.upper().startswith("USD"):
            currency = "$"
            u_tail = u_nospace[3:]
        elif u_nospace.startswith("S$"):
            currency = "S$"
            u_tail = u_nospace[2:]
        elif u_nospace.startswith("$"):
            currency = "$"
            u_tail = u_nospace[1:]
        else:
            u_tail = u_nospace

        # Percent special case
        if u_tail == "%":
            return f"{v}%"

        # Word scales
        if "billion" in u.lower():
            return f"{currency}{v} billion".strip()
        if "million" in u.lower():
            return f"{currency}{v} million".strip()

        # Compact suffix (B/M/K/T)
        if u_tail.upper() in {"T", "B", "M", "K"}:
            return f"{currency}{v}{u_tail.upper()}".strip()

        # Fallback
        return f"{currency}{v} {u}".strip()

    prev_numbers: Dict[str, Dict] = {}
    for key, metric in (prev_metrics or {}).items():
        if not isinstance(metric, dict):
            continue

        metric_name = metric.get("name", key)
        raw_value = metric.get("value", "")
        raw_unit = metric.get("unit", "")

        val = parse_human_number(str(raw_value), raw_unit)
        if val is None:
            continue

        prev_numbers[metric_name] = {
            "value": val,
            "unit": normalize_unit(raw_unit),
            "raw": _format_raw_display(raw_value, raw_unit),   # ✅ now includes currency + unit
            "raw_value": raw_value,
            "raw_unit": raw_unit,
            "keywords": extract_context_keywords(metric_name),
        }

    return prev_numbers

def _extract_baseline_cache(previous_data: dict) -> list:
    """
    Pull prior source snapshots from any known places v7.x stores them.
    Returns a list of source_result-like dicts, or [].
    """
    pd = previous_data or {}
    pr = (pd.get("primary_response") or {}) if isinstance(pd.get("primary_response"), dict) else {}

    for obj in [
        pd.get("baseline_sources_cache"),
        (pd.get("results") or {}).get("baseline_sources_cache") if isinstance(pd.get("results"), dict) else None,
        (pd.get("results") or {}).get("source_results") if isinstance(pd.get("results"), dict) else None,
        pd.get("source_results"),
        pr.get("baseline_sources_cache"),
        (pr.get("results") or {}).get("source_results") if isinstance(pr.get("results"), dict) else None,
    ]:
        if isinstance(obj, list) and obj:
            return obj

    return []


def _extract_query_from_previous(previous_data: dict) -> str:
    """
    Try to recover the original user query/topic from the saved analysis object.
    v7.27 commonly uses 'question'.
    """
    pd = previous_data or {}
    if isinstance(pd.get("question"), str) and pd["question"].strip():
        return pd["question"].strip()

    pr = pd.get("primary_response") or {}
    if isinstance(pr, dict):
        if isinstance(pr.get("question"), str) and pr["question"].strip():
            return pr["question"].strip()
        if isinstance(pr.get("query"), str) and pr["query"].strip():
            return pr["query"].strip()

    meta = pd.get("meta") or {}
    if isinstance(meta, dict) and isinstance(meta.get("question"), str) and meta["question"].strip():
        return meta["question"].strip()

    return ""

def _build_source_snapshots_from_web_context(web_context: dict) -> list:
    """
    Convert fetch_web_context() output (scraped_meta) into evolution snapshots.

    Preferred inputs:
      - web_context["scraped_meta"][url]["extracted_numbers"] (analysis-aligned)

    Safety-net hard gates (small set):
      1) homepage-like URLs downweighted + tagged
      2) nav/chrome/junk context downweighted
      3) year-only suppression (e.g., raw == "2024" and no unit/context)
      4) light topic gate (requires minimal overlap with query tokens)
    """
    import hashlib
    from datetime import datetime
    from urllib.parse import urlparse
    import re

    def _sha1(s: str) -> str:
        return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()

    def _now() -> str:
        try:
            return datetime.utcnow().isoformat() + "+00:00"
        except Exception:
            return _yureeka_now_iso_utc()

    def _is_homepage_url(u: str) -> bool:
        try:
            p = urlparse((u or "").strip())
            path = (p.path or "").strip()
            if path in ("", "/"):
                return True
            low = path.lower().rstrip("/")
            if low in ("/index", "/index.html", "/index.htm", "/home", "/default", "/default.aspx"):
                return True
            return False
        except Exception:
            return False



    def _tokenize(s: str) -> list:
        toks = re.findall(r"[a-z0-9]+", (s or "").lower())
        stop = {"the","and","or","of","in","to","for","by","from","with","on","at","as","a","an","is","are","this","that"}
        return [t for t in toks if len(t) >= 4 and t not in stop]

    def _looks_like_year_only(n: dict) -> bool:
        try:
            raw = str(n.get("raw") or "").strip()
            # =====================================================================
            # PATCH YEAR2 (ADDITIVE): handle years stored as numeric/float strings
            # - If raw is empty or looks like '2024.0', normalize to '2024' for checks.
            # =====================================================================
            try:
                if (not raw) or (raw and raw.replace('.', '', 1).isdigit() and '.' in raw):
                    vraw = n.get('value_norm') if n.get('value_norm') is not None else n.get('value')
                    iv = int(float(str(vraw).strip())) if vraw is not None else None
                    if iv is not None and 1900 <= iv <= 2105:
                        raw = str(iv)
            except Exception:
                pass
            # =====================================================================
            unit = str(n.get("unit") or "").strip()
            ctx = str(n.get("context") or n.get("context_snippet") or "").strip()
            # exactly 4 digits year and nothing else
            if re.fullmatch(r"(19|20)\d{2}", raw) and not unit:
                # if context is empty or super short, treat as junk
                if len(ctx) < 12:
                    return True
            return False
        except Exception:
            return False

    def _is_chrome_ctx(ctx: str) -> bool:
        if not ctx:
            return False
        low = ctx.lower()
        for h in globals().get("NON_DATA_CONTEXT_HINTS", []) or []:
            if h in low:
                return True
        return False

    if not isinstance(web_context, dict):
        return []

    scraped_meta = web_context.get("scraped_meta") or {}
    if not isinstance(scraped_meta, dict) or not scraped_meta:
        return []

    query = (web_context.get("query") or "")
    q_toks = set(_tokenize(query))

    out = []

    for url, meta in scraped_meta.items():
        if not isinstance(meta, dict):
            continue

        url_s = str(url or meta.get("url") or "").strip()
        if not url_s:
            continue

        extracted = meta.get("extracted_numbers") or []
        if not isinstance(extracted, list):
            extracted = []

        fp = meta.get("fingerprint") or meta.get("extract_hash") or meta.get("content_fingerprint")
        if fp and not isinstance(fp, str):
            fp = str(fp)
        if not fp and isinstance(meta.get("clean_text"), str):
            fp = _sha1(meta["clean_text"][:200000])

        status_detail = meta.get("status_detail") or meta.get("status") or ""
        fetched_ok = str(status_detail).startswith("success") or meta.get("status") == "fetched"

        is_homepage = _is_homepage_url(url_s)

        cleaned_numbers = []
        for n in extracted:
            if not isinstance(n, dict):
                continue

            # ---- Hard gate: year-only suppression ----
            if _looks_like_year_only(n):
                continue

            value = n.get("value")
            raw = n.get("raw")
            unit = n.get("unit")
            ctx = n.get("context") or n.get("context_snippet") or ""

            # normalize context
            ctx_s = ctx if isinstance(ctx, str) else ""
            ctx_s = ctx_s.strip()

            # ---- Hard gate: chrome/nav rejection (soft) ----
            chrome_ctx = _is_chrome_ctx(ctx_s)

            # ---- Light topic gate (soft): require some overlap with query tokens ----
            # This is intentionally mild: it *downweights* rather than drops everything.
            ctx_toks = set(_tokenize(ctx_s))
            tok_overlap = len(q_toks.intersection(ctx_toks)) if q_toks and ctx_toks else 0

            # quality scoring (small + interpretable)
            quality = 1.0
            reasons = []

            if is_homepage:
                quality *= 0.25
                reasons.append("homepage_like")

            if chrome_ctx:
                quality *= 0.40
                reasons.append("chrome_context")

            if q_toks and tok_overlap == 0:
                quality *= 0.55
                reasons.append("topic_miss")

            # cap/trim context snippet for JSON size
            ctx_snip = ctx_s[:240]

            cleaned_numbers.append({
                "value": value,
                "unit": unit,
                "raw": raw,
                "source_url": n.get("source_url") or url_s,
                "context_snippet": ctx_snip,
                "anchor_hash": n.get("anchor_hash") or _sha1(f"{url_s}|{ctx_snip}|{raw}|{unit}"),
                # Debug fields for tuning:
                "quality_score": round(float(quality), 3),
                "quality_reasons": reasons,
                "topic_overlap": tok_overlap,
            })

        out.append({
            "url": url_s,
            "status": "fetched_extracted" if cleaned_numbers else ("fetched" if fetched_ok else "failed"),
            "status_detail": status_detail,
            "numbers_found": len(cleaned_numbers),
            "fingerprint": fp or "",
            "fetched_at": meta.get("fetched_at") or _now(),
            "is_homepage_like": bool(is_homepage),
            "extracted_numbers": cleaned_numbers,
        })

    return out



def _build_source_snapshots_from_baseline_cache(baseline_cache: list) -> list:
    """
    Normalize prior cached source_results (from previous run) into a consistent schema.

    Tightening:
      - Detect domain-only/homepage URLs and label them (same as web_context snapshots)
      - Keep backward compatible fields; only add new fields.
    """
    from urllib.parse import urlparse

    def _is_homepage_url(u: str) -> bool:
        try:
            p = urlparse((u or "").strip())
            path = (p.path or "").strip()
            if path in ("", "/"):
                return True
            low = path.lower().rstrip("/")
            if low in ("/index", "/index.html", "/index.htm", "/home", "/default", "/default.aspx"):
                return True
            return False
        except Exception:
            return False

    out = []
    if not isinstance(baseline_cache, list):
        return out

    for sr in baseline_cache:
        if not isinstance(sr, dict):
            continue

        url = sr.get("url") or sr.get("source_url")
        if not url:
            continue
        url_s = str(url).strip()
        if not url_s:
            continue

        extracted = sr.get("extracted_numbers") or []
        if not isinstance(extracted, list):
            extracted = []

        cleaned = []
        for n in extracted:
            if not isinstance(n, dict):
                continue
            cleaned.append({
                "value": n.get("value"),
                "unit": n.get("unit"),
                "raw": n.get("raw"),
                "source_url": n.get("source_url") or url_s,
                "context": (n.get("context") or n.get("context_snippet") or "")[:220]
                if isinstance((n.get("context") or n.get("context_snippet")), str) else "",
            })

        fp = sr.get("fingerprint")
        if fp and not isinstance(fp, str):
            fp = str(fp)

        # --- homepage labeling (tightening #3) ---
        is_homepage = bool(sr.get("is_homepage")) or _is_homepage_url(url_s)
        quality_score = sr.get("quality_score")
        if quality_score is None:
            quality_score = 0.15 if is_homepage else 1.0

        skip_reason = sr.get("skip_reason") or ("homepage_url_low_signal" if is_homepage else "")

        host = sr.get("host") or ""
        path = sr.get("path") or ""
        if not host and not path:
            try:
                p = urlparse(url_s)
                host = p.netloc or ""
                path = p.path or ""
            except Exception:
                pass

        out.append({
            "url": url_s,
            "status": sr.get("status") or "",
            "status_detail": sr.get("status_detail") or "",
            "numbers_found": int(sr.get("numbers_found") or len(cleaned)),
            "fingerprint": fp,
            "fetched_at": sr.get("fetched_at"),
            "extracted_numbers": cleaned,

            # NEW debug fields (safe additions)
            "is_homepage": bool(is_homepage),
            "quality_score": float(quality_score),
            "skip_reason": skip_reason,
            "host": host,
            "path": path,
        })

    return out


def _merge_snapshots_prefer_cached_when_unchanged(current_snaps: list, cached_snaps: list) -> list:
    """
    Policy merge:
      - If current fingerprint matches cached fingerprint for same URL:
        reuse cached snapshot (even if live fetch worked)  ✅ point A
      - Else prefer current (fresh).
      - Add cached snapshots not present in current.
      - Also: if current numbers_found is 0 but cached has >0, reuse cached.
    """
    if not isinstance(current_snaps, list):
        current_snaps = []
    if not isinstance(cached_snaps, list):
        cached_snaps = []

    cached_by_url = {}
    for s in cached_snaps:
        if isinstance(s, dict) and s.get("url"):
            cached_by_url[str(s["url"])] = s

    merged = []
    seen = set()

    for cs in current_snaps:
        if not isinstance(cs, dict) or not cs.get("url"):
            continue
        url = str(cs["url"])
        seen.add(url)

        cached = cached_by_url.get(url)
        if not cached:
            merged.append(cs)
            continue

        cur_fp = cs.get("fingerprint")
        old_fp = cached.get("fingerprint")

        cur_nf = int(cs.get("numbers_found") or 0)
        old_nf = int(cached.get("numbers_found") or 0)

        # If current extraction is empty but cached had numbers, reuse cached.
        if cur_nf == 0 and old_nf > 0:
            merged.append(cached)
            continue

        # Fingerprint unchanged -> reuse cached even if live fetch worked.
        if cur_fp and old_fp and str(cur_fp) == str(old_fp):
            merged.append(cached)
        else:
            merged.append(cs)

    for url, cached in cached_by_url.items():
        if url not in seen:
            merged.append(cached)

    return merged


def _safe_parse_current_analysis(query: str, web_context: dict) -> dict:
    """
    Run the same analysis pipeline used in v7.27 to produce primary_response, but safely.
    Returns dict with at least {primary_response:{primary_metrics:{}}} or {} on failure.
    """
    import json
    qp = globals().get("query_perplexity")
    if not callable(qp):
        return {}

    try:
        raw = qp(query, web_context)
        if not raw or not isinstance(raw, str):
            return {}
        obj = json.loads(raw)
        if not isinstance(obj, dict):
            return {}
        return {"primary_response": obj}
    except Exception:
        return {}


def diff_metrics_by_name_BASE(prev_response: dict, cur_response: dict):
    """
    Canonical-first diff with:
      - HARD STOP when prev canonical_key is missing in current (no name fallback)
      - Row-level metric_definition sourced from PREVIOUS (original new analysis) schema:
          prev_response['metric_schema_frozen'][canonical_key] (preferred)
          else prev_response['primary_metrics_canonical'][canonical_key]
      - Backward compatible: still returns 'name' (non-empty) and existing fields.

    Returns:
            metric_changes, unchanged, increased, decreased, found = fn_diff(prev_response, cur_resp_for_diff)
    """
    import re

    # Defaults (used unless schema provides overrides)
    ABS_EPS = 1e-9
    REL_EPS = 0.0005

    def norm_name(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def parse_num(v, unit=""):
        fn = globals().get("parse_human_number")
        if callable(fn):
            try:
                return fn(str(v), unit)
            except Exception:
                return None
        try:
            return float(str(v).replace(",", "").strip())
        except Exception:
            return None

    # =========================================================================
    # PATCH D1 (ADDITIVE): canonical numeric extractor
    # - Prefer value_norm/base_unit when present (analysis/evolution alignment)
    # - Fall back to existing parse_num(value, unit) when canonical fields missing
    # =========================================================================
    def get_canonical_value_and_unit(m: dict):
        """
        Returns: (val: float|None, unit: str)
        Priority:
          1) value_norm (float-like) + base_unit (if present)
          2) parse_num(value, unit)
        """
        m = m if isinstance(m, dict) else {}

        # 1) canonical path
        if m.get("value_norm") is not None:
            try:
                v = float(m.get("value_norm"))
                u = str(m.get("base_unit") or m.get("unit") or "").strip()
                try:
                    if str(m.get("unit_family") or "").strip().lower() == "currency":
                        _cc = str(m.get("currency_code") or "").strip().upper()
                        if _cc:
                            u = f"{_cc}:{u}" if u else _cc
                except Exception:
                    pass
                return v, u
            except Exception:
                pass

        # 2) legacy parse path
        # =================================================================
        # PATCH V27_DISABLE_NUMERIC_INFERENCE_FOR_CURRENT (ADDITIVE)
        # When canonical-for-render is active, do NOT infer/parse numbers from
        # free-form raw strings for CURRENT-side metrics. We only trust
        # schema-canonical value_norm emitted by the rebuild layer.
        #
        # Activation: cur_response['_disable_numeric_inference_v27'] == True
        # Safety: render/diff-layer only. Does not touch fastpath/hashing/etc.
        # =================================================================
        try:
            if isinstance(cur_response, dict) and cur_response.get("_disable_numeric_inference_v27"):
                u = str(m.get("unit") or "").strip()
                return None, u
        except Exception:
            pass
        u = str(m.get("unit") or "").strip()
        v = parse_num(m.get("value"), u)
        return v, u
    # =========================================================================

    # =========================================================================
    # PATCH D0 (ADDITIVE): anchor helpers (drift=0 stability)
    # NOTE (IMPORTANT):
    # - Anchor_hash equality should NOT force "unchanged" if numeric values differ.
    #   It means "we matched the same evidence anchor" (identity/matching), not
    #   that the metric's value necessarily didn't change.
    # - This patch keeps anchor_same, but uses it only for match_confidence +
    #   diagnostics, not as a classification override.
    # =========================================================================
    def _get_anchor_hash_from_metric(m: dict):
        try:
            if isinstance(m, dict):
                ah = m.get("anchor_hash") or m.get("anchor") or m.get("anchorHash")
                return str(ah) if ah else None
        except Exception:
            return None

    def _get_prev_anchor_hash(prev_resp: dict, ckey: str, pm: dict):
        # 1) direct on metric row
        ah = _get_anchor_hash_from_metric(pm)
        if ah:
            return ah

        # 2) prev_response.metric_anchors[ckey].anchor_hash
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            return None

    def _get_cur_anchor_hash(cur_resp: dict, ckey: str, cm: dict):
        # 1) direct on metric row (evolution rebuild puts anchor_hash here)
        ah = _get_anchor_hash_from_metric(cm)
        if ah:
            return ah

        # 2) cur_response.metric_anchors[ckey].anchor_hash (if present)
        try:
            ma = (cur_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            return None
    # =========================================================================

    # =========================================================================
    # PATCH MA2 (ADDITIVE): wire metric_anchors into row fields
    # - Populate context_snippet/source_url from prev_response.metric_anchors[ckey] when available
    # - Output enrichment only
    # =========================================================================
    def _get_prev_anchor_obj(prev_resp: dict, ckey: str):
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                return a if isinstance(a, dict) else {}
        except Exception:
            return {}

    def _anchor_meta(prev_resp: dict, cur_resp: dict, ckey: str, pm: dict, cm: dict):
        """
        Returns: (source_url, context_snippet, anchor_confidence)
        Priority:
          1) prev_response.metric_anchors[ckey] (baseline anchoring is authoritative)
          2) current metric row fields (if present)
          3) prev metric row fields (if present)
        """
        a = _get_prev_anchor_obj(prev_resp, ckey)

        src = a.get("source_url") or a.get("url")
        ctx = a.get("context_snippet") or a.get("context")
        conf = a.get("anchor_confidence")

        if not src:
            try:
                src = (cm or {}).get("source_url") or (cm or {}).get("url")
            except Exception:
                pass
                src = None
        if not ctx:
            try:
                ctx = (cm or {}).get("context_snippet") or (cm or {}).get("context")
            except Exception:
                pass
                ctx = None

        if not src:
            try:
                src = (pm or {}).get("source_url") or (pm or {}).get("url")
            except Exception:
                pass
                src = None
        if not ctx:
            try:
                ctx = (pm or {}).get("context_snippet") or (pm or {}).get("context")
            except Exception:
                pass
                ctx = None

        try:
            if isinstance(ctx, str):
                ctx = ctx.strip()[:220] or None
            else:
                ctx = None
        except Exception:
            pass
            ctx = None

        try:
            conf = float(conf) if conf is not None else None
        except Exception:
            pass
            conf = None

        return src, ctx, conf
    # =========================================================================

    def prettify_ckey(ckey: str) -> str:
        ckey = str(ckey or "").strip()
        if not ckey:
            return "Unknown Metric"
        parts = ckey.split("__", 1)
        left = parts[0].replace("_", " ").strip()
        right = parts[1].replace("_", " ").strip() if len(parts) > 1 else ""
        left = " ".join(w.capitalize() for w in left.split())
        return f"{left} ({right})" if right else left

    def get_metric_definition(prev_resp: dict, ckey: str) -> dict:
        """
        Pull authoritative definition from the ORIGINAL analysis run (prev_response).
        """
        prev_resp = prev_resp if isinstance(prev_resp, dict) else {}

        # =====================================================================
        # PATCH V33_SCHEMA_PATH_TOLERANT (ADDITIVE)
        # Support both legacy top-level and analysis-shape nested schema locations.
        # =====================================================================
        schema = prev_resp.get("metric_schema_frozen")
        if not isinstance(schema, dict) or not schema:
            schema = _get_nested(prev_resp, "primary_response", "metric_schema_frozen")
        if isinstance(schema, dict):
            d = schema.get(ckey)
            if isinstance(d, dict) and d:
                out = dict(d)
                out.setdefault("canonical_key", ckey)
                return out

        prev_can = prev_resp.get("primary_metrics_canonical")
        if not isinstance(prev_can, dict) or not prev_can:
            prev_can = _get_nested(prev_resp, "primary_response", "primary_metrics_canonical")
        if isinstance(prev_can, dict):
            d = prev_can.get(ckey)
            if isinstance(d, dict) and d:
                out = {
                    "canonical_key": ckey,
                    "canonical_id": d.get("canonical_id"),
                    "dimension": d.get("dimension"),
                    "name": d.get("name") or d.get("original_name"),
                    "unit": d.get("unit"),
                    "geo_scope": d.get("geo_scope"),
                    "geo_name": d.get("geo_name"),
                    "keywords": d.get("keywords"),
                }
                return {k: v for k, v in out.items() if v not in (None, "", [], {})}

        return {"canonical_key": ckey, "name": prettify_ckey(ckey)}

    def get_display_name(prev_resp: dict, prev_can_obj: dict, cur_can_obj: dict, ckey: str) -> str:
        schema = prev_resp.get("metric_schema_frozen")
        if isinstance(schema, dict):
            sm = schema.get(ckey)
            if isinstance(sm, dict):
                v = sm.get("name")
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(prev_can_obj, dict):
            for k in ("name", "original_name"):
                v = prev_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(cur_can_obj, dict):
            for k in ("name", "original_name"):
                v = cur_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        return prettify_ckey(ckey)

    # =========================================================================
    # PATCH D3 (ADDITIVE): schema-driven tolerances (optional)
    # - If schema provides abs_eps/rel_eps use them, else default.
    # =========================================================================
    def get_eps_for_metric(prev_resp: dict, ckey: str):
        ae = ABS_EPS
        re_ = REL_EPS
        try:
            schema = (prev_resp or {}).get("metric_schema_frozen")
            if isinstance(schema, dict):
                d = schema.get(ckey)
                if isinstance(d, dict):
                    if d.get("abs_eps") is not None:
                        try:
                            ae = float(d.get("abs_eps"))
                        except Exception:
                            pass
                    if d.get("rel_eps") is not None:
                        try:
                            re_ = float(d.get("rel_eps"))
                        except Exception:
                            pass
        except Exception:
            return ae, re_
    # =========================================================================

    prev_response = prev_response if isinstance(prev_response, dict) else {}
    cur_response = cur_response if isinstance(cur_response, dict) else {}

    prev_can = prev_response.get("primary_metrics_canonical")
    cur_can = cur_response.get("primary_metrics_canonical")

    # =====================================================================
    # PATCH V33_DIFF_CANONICAL_PATHS (ADDITIVE)
    # Goal:
    # - Evolution sometimes passes the "analysis-style" response shape where
    #   canonical metrics + frozen schema live under primary_response.*
    # - Our canonical-first diff (Path A) was reading ONLY top-level keys, so
    #   cur_can frequently became {} (or missing), producing blank/N/A current.
    #
    # Fix:
    # - Resolve canonical dict + schema using a tolerant, auditable path order:
    #   1) explicit injected payloads (e.g., _canonical_for_render, canonical_for_render)
    #   2) top-level primary_metrics_canonical / metric_schema_frozen (legacy)
    #   3) nested primary_response.primary_metrics_canonical / metric_schema_frozen (analysis shape)
    #
    # Safety:
    # - Purely affects diff-panel hydration (render-only), not fastpath/hashing/etc.
    # Diagnostics:
    # - debug.diff_panel_canonical_paths_v33 (selected paths + key counts)
    # =====================================================================
    _diff_panel_paths_v33 = {
        "prev_can_path": None,
        "cur_can_path": None,
        "prev_schema_path": None,
        "cur_schema_path": None,
        "prev_can_n": 0,
        "cur_can_n": 0,
    }

    def _get_nested(d: dict, *keys):
        d = d if isinstance(d, dict) else {}
        for k in keys:
            if not isinstance(d, dict):
                return None
            d = d.get(k)
        return d

    # Prefer explicitly injected render-canonical dicts if present
    _cur_injected = cur_response.get("_canonical_for_render") or cur_response.get("canonical_for_render")
    if isinstance(_cur_injected, dict) and _cur_injected:
        cur_can = _cur_injected
        _diff_panel_paths_v33["cur_can_path"] = "_canonical_for_render|canonical_for_render"

    _prev_injected = prev_response.get("_canonical_for_render") or prev_response.get("canonical_for_render")
    if isinstance(_prev_injected, dict) and _prev_injected:
        prev_can = _prev_injected
        _diff_panel_paths_v33["prev_can_path"] = "_canonical_for_render|canonical_for_render"

    # Fall back to nested analysis-shape payloads
    if not isinstance(cur_can, dict) or not cur_can:
        _nested_cur = _get_nested(cur_response, "primary_response", "primary_metrics_canonical")
        if isinstance(_nested_cur, dict) and _nested_cur:
            cur_can = _nested_cur
            _diff_panel_paths_v33["cur_can_path"] = "primary_response.primary_metrics_canonical"

    if not isinstance(prev_can, dict) or not prev_can:
        _nested_prev = _get_nested(prev_response, "primary_response", "primary_metrics_canonical")
        if isinstance(_nested_prev, dict) and _nested_prev:
            prev_can = _nested_prev
            _diff_panel_paths_v33["prev_can_path"] = "primary_response.primary_metrics_canonical"

    # Schema paths (used for eps overrides + metric_definition)
    _prev_schema = prev_response.get("metric_schema_frozen")
    if isinstance(_prev_schema, dict) and _prev_schema:
        _diff_panel_paths_v33["prev_schema_path"] = "metric_schema_frozen"
    else:
        _prev_schema = _get_nested(prev_response, "primary_response", "metric_schema_frozen")
        if isinstance(_prev_schema, dict) and _prev_schema:
            _diff_panel_paths_v33["prev_schema_path"] = "primary_response.metric_schema_frozen"

    _cur_schema = cur_response.get("metric_schema_frozen")
    if isinstance(_cur_schema, dict) and _cur_schema:
        _diff_panel_paths_v33["cur_schema_path"] = "metric_schema_frozen"
    else:
        _cur_schema = _get_nested(cur_response, "primary_response", "metric_schema_frozen")
        if isinstance(_cur_schema, dict) and _cur_schema:
            _diff_panel_paths_v33["cur_schema_path"] = "primary_response.metric_schema_frozen"

    _diff_panel_paths_v33["prev_can_n"] = len(prev_can) if isinstance(prev_can, dict) else 0
    _diff_panel_paths_v33["cur_can_n"] = len(cur_can) if isinstance(cur_can, dict) else 0

    try:
        if isinstance(cur_response, dict):
            cur_response["_diff_panel_canonical_paths_v33"] = dict(_diff_panel_paths_v33)
    except Exception:
        pass


    # =========================
    # Path A: canonical-first
    # =========================
    if isinstance(prev_can, dict) and isinstance(cur_can, dict) and prev_can:
        metric_changes = []
        unchanged = increased = decreased = found = 0

        for ckey, pm in prev_can.items():
            pm = pm if isinstance(pm, dict) else {}
            cm = cur_can.get(ckey)
            cm = cm if isinstance(cm, dict) else {}

            display_name = get_display_name(prev_response, pm, cm, ckey)
            definition = get_metric_definition(prev_response, ckey)

            prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

            # ✅ HARD STOP: canonical key missing in current => not_found (no name fallback)
            if ckey not in cur_can or not isinstance(cur_can.get(ckey), dict):
                # PATCH MA2 (ADDITIVE): fill row fields from metric_anchors where possible
                _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, {})

                metric_changes.append({
                    "name": display_name,
                    "previous_value": prev_raw,
                    "current_value": "N/A",
                    "change_pct": None,
                    "change_type": "not_found",
                    "match_confidence": 0.0,
                    "context_snippet": _ctx,
                    "source_url": _src,
                    "anchor_used": False,  # not applicable when current metric missing
                    "canonical_key": ckey,
                    "metric_definition": definition,
                    "anchor_confidence": _aconf,
                })
                continue

            found += 1

            cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")


            # =====================================================================
            # PATCH FIX2B_EVO_CURFIELDS_V1 (ADDITIVE): enrich evolution "Current" fields with unit + range (analysis semantics)
            #
            # Why:
            # - Evolution dashboard / sheet sanitizer (_fix39_has_unit_evidence) may blank rows when "current_value"
            #   lacks unit evidence even though the canonical metric has unit/value_norm.
            # - Also ensure any attached value_range is rendered in schema units (no scaling here).
            #
            # Notes:
            # - Purely additive: does not change change_pct logic (which uses value_norm via get_canonical_value_and_unit).
            # - Does NOT re-scale value_norm; assumes value_norm is already schema-normalized (FIX16/PH2B semantics).
            # =====================================================================
            _cur_unit_tag = (cm.get("unit") or cm.get("unit_tag") or "").strip()
            _cur_value_txt = "" if cur_raw is None else str(cur_raw).strip()
            _cur_raw_display = _cur_value_txt
            try:
                if _cur_unit_tag and _cur_value_txt and (_cur_unit_tag not in _cur_value_txt):
                    _cur_raw_display = f"{_cur_value_txt} {_cur_unit_tag}".strip()
            except Exception:
                pass

            _cur_value_range = None
            _cur_value_range_display = ""
            try:
                if isinstance(cm.get("value_range"), dict):
                    _cur_value_range = dict(cm.get("value_range"))
                    if _cur_unit_tag and ("min" in _cur_value_range) and ("max" in _cur_value_range):
                        _cur_value_range_display = f"{_cur_value_range['min']:g}–{_cur_value_range['max']:g} {_cur_unit_tag}".strip()
            except Exception:
                pass
            # =====================================================================

            # =========================================================================
            # PATCH D0 (ADDITIVE): anchor identity (do NOT force unchanged)
            # =========================================================================
            prev_ah = _get_prev_anchor_hash(prev_response, ckey, pm)
            cur_ah = _get_cur_anchor_hash(cur_response, ckey, cm)
            anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))
            # =========================================================================

            # PATCH D2 (ADDITIVE): use canonical values for diff when available
            prev_val, prev_unit_cmp = get_canonical_value_and_unit(pm)
            cur_val, cur_unit_cmp = get_canonical_value_and_unit(cm)

            # PATCH D3 (ADDITIVE): metric-specific tolerances (schema overrides)
            abs_eps, rel_eps = get_eps_for_metric(prev_response, ckey)

            change_type = "unknown"
            change_pct = None

            # =========================================================================
            # PATCH D0B (ADDITIVE, REQUIRED): numeric-first classification even if anchors match
            # Why:
            # - anchor_same means "we matched the same evidence anchor"
            # - It MUST NOT short-circuit classification to "unchanged" when values differ.
            # - This fixes the exact bug you observed: prev_value_norm != cur_value_norm
            #   while change_type incorrectly says "unchanged".
            # =========================================================================
            if prev_val is not None and cur_val is not None:
                if abs(prev_val - cur_val) <= max(abs_eps, abs(prev_val) * rel_eps):
                    change_type = "unchanged"
                    change_pct = 0.0
                    unchanged += 1
                elif cur_val > prev_val:
                    change_type = "increased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    increased += 1
                else:
                    change_type = "decreased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    decreased += 1
            # If we cannot compare numerically, fall back:
            # - If anchors match, treat as unchanged ONLY as a last resort (formatting issue)
            elif anchor_same:
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            # =========================================================================

            # PATCH D4 (ADDITIVE): unit mismatch flag (debug only)
            unit_mismatch = False
            try:
                if prev_unit_cmp and cur_unit_cmp and str(prev_unit_cmp) != str(cur_unit_cmp):
                    unit_mismatch = True
            except Exception:
                pass
                unit_mismatch = False

            # PATCH MA2 (ADDITIVE): fill row fields from metric_anchors where possible
            _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, cm)

            # PATCH D0C (ADDITIVE): match_confidence reflects anchor identity
            match_conf = 92.0
            try:
                if anchor_same:
                    match_conf = 98.0
            except Exception:
                pass
                match_conf = 92.0

            metric_changes.append({
                "name": display_name,
                "previous_value": prev_raw,
                "current_value": cur_raw,
                "current_raw": _cur_raw_display,
                "current_unit_tag": _cur_unit_tag,
                "current_value_norm": cm.get("value_norm"),
                "current_value_range": _cur_value_range,
                "current_value_range_display": _cur_value_range_display,
                "change_pct": change_pct,
                "change_type": change_type,
                "match_confidence": float(match_conf),

                "context_snippet": _ctx,
                "source_url": _src,

                # anchor identity (matching), not classification
                "anchor_used": bool(anchor_same),
                "prev_anchor_hash": prev_ah,
                "cur_anchor_hash": cur_ah,

                "canonical_key": ckey,
                "metric_definition": definition,

                "anchor_confidence": _aconf,

                # expose canonical comparison basis for debugging/convergence
                "prev_value_norm": prev_val,
                "cur_value_norm": cur_val,
                "prev_unit_cmp": prev_unit_cmp,
                "cur_unit_cmp": cur_unit_cmp,
                # PATCH FIX2B_TRACE_V1 (ADDITIVE): expose chosen URL + selector trace (no behavior change)
                "cur_source_url": str((cur_can_obj or {}).get("source_url") or ""),
                "current_source_url": str((cur_can_obj or {}).get("source_url") or ""),
                "current_source_url_effective": str((cur_can_obj or {}).get("source_url") or _src or ""),
                "selector_used": str((cur_can_obj or {}).get("selector_used") or ""),
                "evo_selector_trace_v1": (dict((cur_can_obj or {}).get("analysis_selector_trace_v1") or {}) if isinstance((cur_can_obj or {}).get("analysis_selector_trace_v1"), dict) else {}),
                "winner_candidate_debug": (dict((cur_can_obj or {}).get("winner_candidate_debug") or {}) if isinstance((cur_can_obj or {}).get("winner_candidate_debug"), dict) else {}),
                "would_block_reason": str((cur_can_obj or {}).get("would_block_reason") or ""),
"unit_mismatch": bool(unit_mismatch),
                "abs_eps_used": abs_eps,
                "rel_eps_used": rel_eps,
            })

        return metric_changes, unchanged, increased, decreased, found

    # =========================
    # Path B: legacy name fallback
    # =========================
    prev_metrics = prev_response.get("primary_metrics") or {}
    cur_metrics = cur_response.get("primary_metrics") or {}
    if not isinstance(prev_metrics, dict):
        prev_metrics = {}
    if not isinstance(cur_metrics, dict):
        cur_metrics = {}

    prev_index = {}
    for k, m in prev_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            prev_index[norm_name(name)] = (name, m)

    cur_index = {}
    for k, m in cur_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            cur_index[norm_name(name)] = (name, m)

    metric_changes = []
    unchanged = increased = decreased = found = 0

    for nk, (display_name, pm) in prev_index.items():
        prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

        if nk not in cur_index:
            metric_changes.append({
                "name": display_name or "Unknown Metric",
                "previous_value": prev_raw,
                "current_value": "N/A",
                "change_pct": None,
                "change_type": "not_found",
                "match_confidence": 0.0,
                "context_snippet": None,
                "source_url": None,
                "anchor_used": False,
            })
            continue

        found += 1
        _, cm = cur_index[nk]
        cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

        prev_val, _prev_unit_cmp = get_canonical_value_and_unit(pm)
        cur_val, _cur_unit_cmp = get_canonical_value_and_unit(cm)

        prev_ah = _get_anchor_hash_from_metric(pm)
        cur_ah = _get_anchor_hash_from_metric(cm)
        anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))

        change_type = "unknown"
        change_pct = None

        # PATCH D0B mirrors canonical path: numeric-first, anchor fallback only if numeric missing
        if prev_val is not None and cur_val is not None:
            if abs(prev_val - cur_val) <= max(ABS_EPS, abs(prev_val) * REL_EPS):
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            elif cur_val > prev_val:
                change_type = "increased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                increased += 1
            else:
                change_type = "decreased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                decreased += 1
        elif anchor_same:
            change_type = "unchanged"
            change_pct = 0.0
            unchanged += 1

        metric_changes.append({
            "name": display_name or "Unknown Metric",
            "previous_value": prev_raw,
            "current_value": cur_raw,
            "change_pct": change_pct,
            "change_type": change_type,
            "match_confidence": 90.0 if anchor_same else 80.0,
            "context_snippet": None,
            "source_url": None,

            "anchor_used": bool(anchor_same),
            "prev_anchor_hash": prev_ah,
            "cur_anchor_hash": cur_ah,

            "prev_value_norm": prev_val,
            "cur_value_norm": cur_val,
        })

    return metric_changes, unchanged, increased, decreased, found







# =====================================================================
# PATCH WRAP_DIFF_METRICS_BY_NAME (ADDITIVE): preserve original as diff_metrics_by_name_BASE
# and define the patched version below.
# =====================================================================

def _yureeka_diff_metrics_by_name_wrap1(prev_response: dict, cur_response: dict):
    """
    Canonical-first diff with:
      - HARD STOP when prev canonical_key is missing in current (no name fallback)
      - Row-level metric_definition sourced from PREVIOUS (original new analysis) schema:
          prev_response['metric_schema_frozen'][canonical_key] (preferred)
          else prev_response['primary_metrics_canonical'][canonical_key]
      - Backward compatible: still returns 'name' (non-empty) and existing fields.

    Returns:
      metric_changes, unchanged, increased, decreased, found
    """
    import re

    # Defaults (used unless schema provides overrides)
    ABS_EPS = 1e-9
    REL_EPS = 0.0005

    def norm_name(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def parse_num(v, unit=""):
        fn = globals().get("parse_human_number")
        if callable(fn):
            try:
                return fn(str(v), unit)
            except Exception:
                return None
        try:
            return float(str(v).replace(",", "").strip())
        except Exception:
            return None

    # =========================================================================
    # PATCH D1 (ADDITIVE): canonical numeric extractor
    # - Prefer value_norm/base_unit when present (analysis/evolution alignment)
    # - Fall back to existing parse_num(value, unit) when canonical fields missing
    # =========================================================================
    def get_canonical_value_and_unit(m: dict):
        """
        Returns: (val: float|None, unit: str)
        Priority:
          1) value_norm (float-like) + base_unit (if present)
          2) parse_num(value, unit)
        """
        m = m if isinstance(m, dict) else {}

        # 1) canonical path
        if m.get("value_norm") is not None:
            try:
                v = float(m.get("value_norm"))
                u = str(m.get("base_unit") or m.get("unit") or "").strip()
                try:
                    if str(m.get("unit_family") or "").strip().lower() == "currency":
                        _cc = str(m.get("currency_code") or "").strip().upper()
                        if _cc:
                            u = f"{_cc}:{u}" if u else _cc
                except Exception:
                    pass
                return v, u
            except Exception:
                pass

        # 2) legacy parse path
        # =================================================================
        # PATCH V27_DISABLE_NUMERIC_INFERENCE_FOR_CURRENT (ADDITIVE)
        # When canonical-for-render is active, do NOT infer/parse numbers from
        # free-form raw strings for CURRENT-side metrics. We only trust
        # schema-canonical value_norm emitted by the rebuild layer.
        #
        # Activation: cur_response['_disable_numeric_inference_v27'] == True
        # Safety: render/diff-layer only. Does not touch fastpath/hashing/etc.
        # =================================================================
        try:
            if isinstance(cur_response, dict) and cur_response.get("_disable_numeric_inference_v27"):
                u = str(m.get("unit") or "").strip()
                return None, u
        except Exception:
            pass
        u = str(m.get("unit") or "").strip()
        v = parse_num(m.get("value"), u)
        return v, u
    # =========================================================================

    # =========================================================================
    # PATCH D0 (ADDITIVE): anchor helpers (drift=0 stability)
    # NOTE (IMPORTANT):
    # - Anchor_hash equality should NOT force "unchanged" if numeric values differ.
    #   It means "we matched the same evidence anchor" (identity/matching), not
    #   that the metric's value necessarily didn't change.
    # - This patch keeps anchor_same, but uses it only for match_confidence +
    #   diagnostics, not as a classification override.
    # =========================================================================
    def _get_anchor_hash_from_metric(m: dict):
        try:
            if isinstance(m, dict):
                ah = m.get("anchor_hash") or m.get("anchor") or m.get("anchorHash")
                return str(ah) if ah else None
        except Exception:
            return None

    def _get_prev_anchor_hash(prev_resp: dict, ckey: str, pm: dict):
        # 1) direct on metric row
        ah = _get_anchor_hash_from_metric(pm)
        if ah:
            return ah

        # 2) prev_response.metric_anchors[ckey].anchor_hash
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            return None

    def _get_cur_anchor_hash(cur_resp: dict, ckey: str, cm: dict):
        # 1) direct on metric row (evolution rebuild puts anchor_hash here)
        ah = _get_anchor_hash_from_metric(cm)
        if ah:
            return ah

        # 2) cur_response.metric_anchors[ckey].anchor_hash (if present)
        try:
            ma = (cur_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            return None
    # =========================================================================

    # =========================================================================
    # PATCH MA2 (ADDITIVE): wire metric_anchors into row fields
    # - Populate context_snippet/source_url from prev_response.metric_anchors[ckey] when available
    # - Output enrichment only
    # =========================================================================
    def _get_prev_anchor_obj(prev_resp: dict, ckey: str):
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                return a if isinstance(a, dict) else {}
        except Exception:
            return {}

    def _anchor_meta(prev_resp: dict, cur_resp: dict, ckey: str, pm: dict, cm: dict):
        """
        Returns: (source_url, context_snippet, anchor_confidence)
        Priority:
          1) prev_response.metric_anchors[ckey] (baseline anchoring is authoritative)
          2) current metric row fields (if present)
          3) prev metric row fields (if present)
        """
        a = _get_prev_anchor_obj(prev_resp, ckey)

        src = a.get("source_url") or a.get("url")
        ctx = a.get("context_snippet") or a.get("context")
        conf = a.get("anchor_confidence")

        if not src:
            try:
                src = (cm or {}).get("source_url") or (cm or {}).get("url")
            except Exception:
                pass
                src = None
        if not ctx:
            try:
                ctx = (cm or {}).get("context_snippet") or (cm or {}).get("context")
            except Exception:
                pass
                ctx = None

        if not src:
            try:
                src = (pm or {}).get("source_url") or (pm or {}).get("url")
            except Exception:
                pass
                src = None
        if not ctx:
            try:
                ctx = (pm or {}).get("context_snippet") or (pm or {}).get("context")
            except Exception:
                pass
                ctx = None

        try:
            if isinstance(ctx, str):
                ctx = ctx.strip()[:220] or None
            else:
                ctx = None
        except Exception:
            pass
            ctx = None

        try:
            conf = float(conf) if conf is not None else None
        except Exception:
            pass
            conf = None

        return src, ctx, conf
    # =========================================================================

    def prettify_ckey(ckey: str) -> str:
        ckey = str(ckey or "").strip()
        if not ckey:
            return "Unknown Metric"
        parts = ckey.split("__", 1)
        left = parts[0].replace("_", " ").strip()
        right = parts[1].replace("_", " ").strip() if len(parts) > 1 else ""
        left = " ".join(w.capitalize() for w in left.split())
        return f"{left} ({right})" if right else left

    def get_metric_definition(prev_resp: dict, ckey: str) -> dict:
        """
        Pull authoritative definition from the ORIGINAL analysis run (prev_response).
        """
        prev_resp = prev_resp if isinstance(prev_resp, dict) else {}

        schema = prev_resp.get("metric_schema_frozen")
        if isinstance(schema, dict):
            d = schema.get(ckey)
            if isinstance(d, dict) and d:
                out = dict(d)
                out.setdefault("canonical_key", ckey)
                return out

        prev_can = prev_resp.get("primary_metrics_canonical")
        if isinstance(prev_can, dict):
            d = prev_can.get(ckey)
            if isinstance(d, dict) and d:
                out = {
                    "canonical_key": ckey,
                    "canonical_id": d.get("canonical_id"),
                    "dimension": d.get("dimension"),
                    "name": d.get("name") or d.get("original_name"),
                    "unit": d.get("unit"),
                    "geo_scope": d.get("geo_scope"),
                    "geo_name": d.get("geo_name"),
                    "keywords": d.get("keywords"),
                }
                return {k: v for k, v in out.items() if v not in (None, "", [], {})}

        return {"canonical_key": ckey, "name": prettify_ckey(ckey)}

    def get_display_name(prev_resp: dict, prev_can_obj: dict, cur_can_obj: dict, ckey: str) -> str:
        schema = prev_resp.get("metric_schema_frozen")
        if isinstance(schema, dict):
            sm = schema.get(ckey)
            if isinstance(sm, dict):
                v = sm.get("name")
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(prev_can_obj, dict):
            for k in ("name", "original_name"):
                v = prev_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(cur_can_obj, dict):
            for k in ("name", "original_name"):
                v = cur_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        return prettify_ckey(ckey)

    # =========================================================================
    # PATCH D3 (ADDITIVE): schema-driven tolerances (optional)
    # - If schema provides abs_eps/rel_eps use them, else default.
    # =========================================================================
    def get_eps_for_metric(prev_resp: dict, ckey: str):
        ae = ABS_EPS
        re_ = REL_EPS
        try:
            schema = (prev_resp or {}).get("metric_schema_frozen")
            if isinstance(schema, dict):
                d = schema.get(ckey)
                if isinstance(d, dict):
                    if d.get("abs_eps") is not None:
                        try:
                            ae = float(d.get("abs_eps"))
                        except Exception:
                            pass
                    if d.get("rel_eps") is not None:
                        try:
                            re_ = float(d.get("rel_eps"))
                        except Exception:
                            pass
        except Exception:
            return ae, re_
    # =========================================================================

    prev_response = prev_response if isinstance(prev_response, dict) else {}
    cur_response = cur_response if isinstance(cur_response, dict) else {}

    # =====================================================================
    # PATCH AI_ANCHMAP1 (ADDITIVE): normalize metric_anchors shape (list -> dict)
    # Why:
    # - Some pipelines persist metric_anchors as a list of records:
    #     [{"canonical_key": ..., "anchor_hash": ..., ...}, ...]
    # - Diff expects a dict mapping canonical_key -> anchor object.
    # Determinism:
    # - Pure reshaping; no new anchors invented.
    # =====================================================================
    def _coerce_metric_anchors_to_dict(resp: dict):
        try:
            if not isinstance(resp, dict):
                return resp
            ma = resp.get("metric_anchors")
            if isinstance(ma, dict) or ma is None:
                return resp
            if isinstance(ma, list):
                out = {}
                for a in ma:
                    if not isinstance(a, dict):
                        continue
                    ck = a.get("canonical_key") or a.get("ckey") or a.get("metric_key")
                    if not ck:
                        continue
                    if ck not in out:
                        out[str(ck)] = a
                resp["metric_anchors"] = out
            return resp
        except Exception:
            return resp

    prev_response = _coerce_metric_anchors_to_dict(prev_response)
    cur_response = _coerce_metric_anchors_to_dict(cur_response)
    # =====================================================================

    prev_can = prev_response.get("primary_metrics_canonical")
    cur_can = cur_response.get("primary_metrics_canonical")

    # =========================
    # Path A: canonical-first
    # =========================
    if isinstance(prev_can, dict) and isinstance(cur_can, dict) and prev_can:
        metric_changes = []
        unchanged = increased = decreased = found = 0

        for ckey, pm in prev_can.items():
            pm = pm if isinstance(pm, dict) else {}
            cm = cur_can.get(ckey)
            cm = cm if isinstance(cm, dict) else {}

            display_name = get_display_name(prev_response, pm, cm, ckey)
            definition = get_metric_definition(prev_response, ckey)

            prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

            # ✅ HARD STOP: canonical key missing in current => not_found (no name fallback)
            if ckey not in cur_can or not isinstance(cur_can.get(ckey), dict):
                # PATCH MA2 (ADDITIVE): fill row fields from metric_anchors where possible
                _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, {})

                metric_changes.append({
                    "name": display_name,
                    "previous_value": prev_raw,
                    "current_value": "N/A",
                    "change_pct": None,
                    "change_type": "not_found",
                    "match_confidence": 0.0,
                    "context_snippet": _ctx,
                    "source_url": _src,
                    "anchor_used": False,  # not applicable when current metric missing
                    "canonical_key": ckey,
                    "metric_definition": definition,
                    "anchor_confidence": _aconf,
                })
                continue

            found += 1

            cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

            # =========================================================================
            # PATCH D0 (ADDITIVE): anchor identity (do NOT force unchanged)
            # =========================================================================
            prev_ah = _get_prev_anchor_hash(prev_response, ckey, pm)
            cur_ah = _get_cur_anchor_hash(cur_response, ckey, cm)
            anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))
            # =========================================================================

            # PATCH D2 (ADDITIVE): use canonical values for diff when available
            prev_val, prev_unit_cmp = get_canonical_value_and_unit(pm)
            cur_val, cur_unit_cmp = get_canonical_value_and_unit(cm)

            # PATCH D3 (ADDITIVE): metric-specific tolerances (schema overrides)
            abs_eps, rel_eps = get_eps_for_metric(prev_response, ckey)

            change_type = "unknown"
            change_pct = None

            # =========================================================================
            # PATCH D0B (ADDITIVE, REQUIRED): numeric-first classification even if anchors match
            # Why:
            # - anchor_same means "we matched the same evidence anchor"
            # - It MUST NOT short-circuit classification to "unchanged" when values differ.
            # - This fixes the exact bug you observed: prev_value_norm != cur_value_norm
            #   while change_type incorrectly says "unchanged".
            # =========================================================================
            if prev_val is not None and cur_val is not None:
                if abs(prev_val - cur_val) <= max(abs_eps, abs(prev_val) * rel_eps):
                    change_type = "unchanged"
                    change_pct = 0.0
                    unchanged += 1
                elif cur_val > prev_val:
                    change_type = "increased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    increased += 1
                else:
                    change_type = "decreased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    decreased += 1
            # If we cannot compare numerically, fall back:
            # - If anchors match, treat as unchanged ONLY as a last resort (formatting issue)
            elif anchor_same:
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            # =========================================================================

            # PATCH D4 (ADDITIVE): unit mismatch flag (debug only)
            unit_mismatch = False
            try:
                if prev_unit_cmp and cur_unit_cmp and str(prev_unit_cmp) != str(cur_unit_cmp):
                    unit_mismatch = True
            except Exception:
                pass
                unit_mismatch = False

            # PATCH MA2 (ADDITIVE): fill row fields from metric_anchors where possible
            _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, cm)

            # PATCH D0C (ADDITIVE): match_confidence reflects anchor identity
            match_conf = 92.0
            try:
                if anchor_same:
                    match_conf = 98.0
            except Exception:
                pass
                match_conf = 92.0

            metric_changes.append({
                "name": display_name,
                "previous_value": prev_raw,
                "current_value": cur_raw,
                "change_pct": change_pct,
                "change_type": change_type,
                "match_confidence": float(match_conf),

                "context_snippet": _ctx,
                "source_url": _src,

                # anchor identity (matching), not classification
                "anchor_used": bool(anchor_same),
                "prev_anchor_hash": prev_ah,
                "cur_anchor_hash": cur_ah,

                "canonical_key": ckey,
                "metric_definition": definition,

                "anchor_confidence": _aconf,

                # expose canonical comparison basis for debugging/convergence
                "prev_value_norm": prev_val,
                "cur_value_norm": cur_val,
                "prev_unit_cmp": prev_unit_cmp,
                "cur_unit_cmp": cur_unit_cmp,
                "unit_mismatch": bool(unit_mismatch),
                "abs_eps_used": abs_eps,
                "rel_eps_used": rel_eps,
            })

        return metric_changes, unchanged, increased, decreased, found

    # =========================
    # Path B: legacy name fallback
    # =========================
    prev_metrics = prev_response.get("primary_metrics") or {}
    cur_metrics = cur_response.get("primary_metrics") or {}
    if not isinstance(prev_metrics, dict):
        prev_metrics = {}
    if not isinstance(cur_metrics, dict):
        cur_metrics = {}

    prev_index = {}
    for k, m in prev_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            prev_index[norm_name(name)] = (name, m)

    cur_index = {}
    for k, m in cur_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            cur_index[norm_name(name)] = (name, m)

    metric_changes = []
    unchanged = increased = decreased = found = 0

    for nk, (display_name, pm) in prev_index.items():
        prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

        if nk not in cur_index:
            metric_changes.append({
                "name": display_name or "Unknown Metric",
                "previous_value": prev_raw,
                "current_value": "N/A",
                "change_pct": None,
                "change_type": "not_found",
                "match_confidence": 0.0,
                "context_snippet": None,
                "source_url": None,
                "anchor_used": False,
            })
            continue

        found += 1
        _, cm = cur_index[nk]
        cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

        prev_val, _prev_unit_cmp = get_canonical_value_and_unit(pm)
        cur_val, _cur_unit_cmp = get_canonical_value_and_unit(cm)

        prev_ah = _get_anchor_hash_from_metric(pm)
        cur_ah = _get_anchor_hash_from_metric(cm)
        anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))

        change_type = "unknown"
        change_pct = None

        # PATCH D0B mirrors canonical path: numeric-first, anchor fallback only if numeric missing
        if prev_val is not None and cur_val is not None:
            if abs(prev_val - cur_val) <= max(ABS_EPS, abs(prev_val) * REL_EPS):
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            elif cur_val > prev_val:
                change_type = "increased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                increased += 1
            else:
                change_type = "decreased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                decreased += 1
        elif anchor_same:
            change_type = "unchanged"
            change_pct = 0.0
            unchanged += 1

        metric_changes.append({
            "name": display_name or "Unknown Metric",
            "previous_value": prev_raw,
            "current_value": cur_raw,
            "change_pct": change_pct,
            "change_type": change_type,
            "match_confidence": 90.0 if anchor_same else 80.0,
            "context_snippet": None,
            "source_url": None,

            "anchor_used": bool(anchor_same),
            "prev_anchor_hash": prev_ah,
            "cur_anchor_hash": cur_ah,

            "prev_value_norm": prev_val,
            "cur_value_norm": cur_val,
        })

    return metric_changes, unchanged, increased, decreased, found

# =====================================================================
# PATCH DIFF_V2 (ADDITIVE): upgrade diff_metrics_by_name to anchor-first + value_norm-aware
# Why:
# - Enforce drift=0 when the same anchor_hash matches on both sides.
# - Prefer canonical numeric fields (value_norm/base_unit) to avoid unit/scale parsing drift.
# - Support schema-driven tolerances (abs_eps/rel_eps) when present.
# Notes:
# - Preserves prior implementation under diff_metrics_by_name_LEGACY.
# - No behavior change unless this upgraded function is called.
# =====================================================================
try:
    diff_metrics_by_name_LEGACY = _yureeka_diff_metrics_by_name_wrap1  # type: ignore
except Exception:
    pass
    diff_metrics_by_name_LEGACY = None

def _yureeka_diff_metrics_by_name_fix31(prev_response: dict, cur_response: dict):
    """
    Canonical-first diff with:
      - HARD STOP when prev canonical_key is missing in current (no name fallback)
      - Row-level metric_definition sourced from PREVIOUS (original new analysis) schema:
          prev_response['metric_schema_frozen'][canonical_key] (preferred)
          else prev_response['primary_metrics_canonical'][canonical_key]
      - Backward compatible: still returns 'name' (non-empty) and existing fields.

    Returns:
      metric_changes, unchanged, increased, decreased, found
    """
    import re

    # Defaults (used unless schema provides overrides)
    ABS_EPS = 1e-9
    REL_EPS = 0.0005

    def norm_name(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def parse_num(v, unit=""):
        fn = globals().get("parse_human_number")
        if callable(fn):
            try:
                return fn(str(v), unit)
            except Exception:
                return None
        try:
            return float(str(v).replace(",", "").strip())
        except Exception:
            return None

    # =========================================================================
    # PATCH D1 (ADDITIVE): canonical numeric extractor
    # - Prefer value_norm/base_unit when present (analysis/evolution alignment)
    # - Fall back to existing parse_num(value, unit) when canonical fields missing
    # =========================================================================
    def get_canonical_value_and_unit(m: dict):
        """
        Returns: (val: float|None, unit: str)
        Priority:
          1) value_norm (float-like) + base_unit (if present)
          2) parse_num(value, unit)
        """
        m = m if isinstance(m, dict) else {}

        # 1) canonical path
        if m.get("value_norm") is not None:
            try:
                v = float(m.get("value_norm"))
                u = str(m.get("base_unit") or m.get("unit") or "").strip()
                try:
                    if str(m.get("unit_family") or "").strip().lower() == "currency":
                        _cc = str(m.get("currency_code") or "").strip().upper()
                        if _cc:
                            u = f"{_cc}:{u}" if u else _cc
                except Exception:
                    pass
                return v, u
            except Exception:
                pass

        # 2) legacy parse path
        # =================================================================
        # PATCH V27_DISABLE_NUMERIC_INFERENCE_FOR_CURRENT (ADDITIVE)
        # When canonical-for-render is active, do NOT infer/parse numbers from
        # free-form raw strings for CURRENT-side metrics. We only trust
        # schema-canonical value_norm emitted by the rebuild layer.
        #
        # Activation: cur_response['_disable_numeric_inference_v27'] == True
        # Safety: render/diff-layer only. Does not touch fastpath/hashing/etc.
        # =================================================================
        try:
            if isinstance(cur_response, dict) and cur_response.get("_disable_numeric_inference_v27"):
                u = str(m.get("unit") or "").strip()
                return None, u
        except Exception:
            pass
        u = str(m.get("unit") or "").strip()
        v = parse_num(m.get("value"), u)
        return v, u
    # =========================================================================

    # =========================================================================
    # PATCH D0 (ADDITIVE): anchor helpers (drift=0 stability)
    # NOTE (IMPORTANT):
    # - Anchor_hash equality should NOT force "unchanged" if numeric values differ.
    #   It means "we matched the same evidence anchor" (identity/matching), not
    #   that the metric's value necessarily didn't change.
    # - This patch keeps anchor_same, but uses it only for match_confidence +
    #   diagnostics, not as a classification override.
    # =========================================================================
    def _get_anchor_hash_from_metric(m: dict):
        try:
            if isinstance(m, dict):
                ah = m.get("anchor_hash") or m.get("anchor") or m.get("anchorHash")
                return str(ah) if ah else None
        except Exception:
            return None

    def _get_prev_anchor_hash(prev_resp: dict, ckey: str, pm: dict):
        # 1) direct on metric row
        ah = _get_anchor_hash_from_metric(pm)
        if ah:
            return ah

        # 2) prev_response.metric_anchors[ckey].anchor_hash
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            return None

    def _get_cur_anchor_hash(cur_resp: dict, ckey: str, cm: dict):
        # 1) direct on metric row (evolution rebuild puts anchor_hash here)
        ah = _get_anchor_hash_from_metric(cm)
        if ah:
            return ah

        # 2) cur_response.metric_anchors[ckey].anchor_hash (if present)
        try:
            ma = (cur_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                if isinstance(a, dict):
                    ah2 = a.get("anchor_hash") or a.get("anchor")
                    if ah2:
                        return str(ah2)
        except Exception:
            return None
    # =========================================================================

    # =========================================================================
    # PATCH MA2 (ADDITIVE): wire metric_anchors into row fields
    # - Populate context_snippet/source_url from prev_response.metric_anchors[ckey] when available
    # - Output enrichment only
    # =========================================================================
    def _get_prev_anchor_obj(prev_resp: dict, ckey: str):
        try:
            ma = (prev_resp or {}).get("metric_anchors")
            if isinstance(ma, dict):
                a = ma.get(ckey)
                return a if isinstance(a, dict) else {}
        except Exception:
            return {}

    def _anchor_meta(prev_resp: dict, cur_resp: dict, ckey: str, pm: dict, cm: dict):
        """
        Returns: (source_url, context_snippet, anchor_confidence)
        Priority:
          1) prev_response.metric_anchors[ckey] (baseline anchoring is authoritative)
          2) current metric row fields (if present)
          3) prev metric row fields (if present)
        """
        a = _get_prev_anchor_obj(prev_resp, ckey)

        src = a.get("source_url") or a.get("url")
        ctx = a.get("context_snippet") or a.get("context")
        conf = a.get("anchor_confidence")

        if not src:
            try:
                src = (cm or {}).get("source_url") or (cm or {}).get("url")
            except Exception:
                pass
                src = None
        if not ctx:
            try:
                ctx = (cm or {}).get("context_snippet") or (cm or {}).get("context")
            except Exception:
                pass
                ctx = None

        if not src:
            try:
                src = (pm or {}).get("source_url") or (pm or {}).get("url")
            except Exception:
                pass
                src = None
        if not ctx:
            try:
                ctx = (pm or {}).get("context_snippet") or (pm or {}).get("context")
            except Exception:
                pass
                ctx = None

        try:
            if isinstance(ctx, str):
                ctx = ctx.strip()[:220] or None
            else:
                ctx = None
        except Exception:
            pass
            ctx = None

        try:
            conf = float(conf) if conf is not None else None
        except Exception:
            pass
            conf = None

        return src, ctx, conf
    # =========================================================================

    def prettify_ckey(ckey: str) -> str:
        ckey = str(ckey or "").strip()
        if not ckey:
            return "Unknown Metric"
        parts = ckey.split("__", 1)
        left = parts[0].replace("_", " ").strip()
        right = parts[1].replace("_", " ").strip() if len(parts) > 1 else ""
        left = " ".join(w.capitalize() for w in left.split())
        return f"{left} ({right})" if right else left

    def get_metric_definition(prev_resp: dict, ckey: str) -> dict:
        """
        Pull authoritative definition from the ORIGINAL analysis run (prev_response).
        """
        prev_resp = prev_resp if isinstance(prev_resp, dict) else {}

        schema = prev_resp.get("metric_schema_frozen")
        if isinstance(schema, dict):
            d = schema.get(ckey)
            if isinstance(d, dict) and d:
                out = dict(d)
                out.setdefault("canonical_key", ckey)
                return out

        prev_can = prev_resp.get("primary_metrics_canonical")
        if isinstance(prev_can, dict):
            d = prev_can.get(ckey)
            if isinstance(d, dict) and d:
                out = {
                    "canonical_key": ckey,
                    "canonical_id": d.get("canonical_id"),
                    "dimension": d.get("dimension"),
                    "name": d.get("name") or d.get("original_name"),
                    "unit": d.get("unit"),
                    "geo_scope": d.get("geo_scope"),
                    "geo_name": d.get("geo_name"),
                    "keywords": d.get("keywords"),
                }
                return {k: v for k, v in out.items() if v not in (None, "", [], {})}

        return {"canonical_key": ckey, "name": prettify_ckey(ckey)}

    def get_display_name(prev_resp: dict, prev_can_obj: dict, cur_can_obj: dict, ckey: str) -> str:
        schema = prev_resp.get("metric_schema_frozen")
        if isinstance(schema, dict):
            sm = schema.get(ckey)
            if isinstance(sm, dict):
                v = sm.get("name")
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(prev_can_obj, dict):
            for k in ("name", "original_name"):
                v = prev_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        if isinstance(cur_can_obj, dict):
            for k in ("name", "original_name"):
                v = cur_can_obj.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()

        return prettify_ckey(ckey)

    # =========================================================================
    # PATCH D3 (ADDITIVE): schema-driven tolerances (optional)
    # - If schema provides abs_eps/rel_eps use them, else default.
    # =========================================================================
    def get_eps_for_metric(prev_resp: dict, ckey: str):
        ae = ABS_EPS
        re_ = REL_EPS
        try:
            schema = (prev_resp or {}).get("metric_schema_frozen")
            if isinstance(schema, dict):
                d = schema.get(ckey)
                if isinstance(d, dict):
                    if d.get("abs_eps") is not None:
                        try:
                            ae = float(d.get("abs_eps"))
                        except Exception:
                            pass
                    if d.get("rel_eps") is not None:
                        try:
                            re_ = float(d.get("rel_eps"))
                        except Exception:
                            pass
        except Exception:
            return ae, re_
    # =========================================================================

    prev_response = prev_response if isinstance(prev_response, dict) else {}
    cur_response = cur_response if isinstance(cur_response, dict) else {}

    # =====================================================================
    # PATCH AI_ANCHMAP1 (ADDITIVE): normalize metric_anchors shape (list -> dict)
    # Why:
    # - Some pipelines persist metric_anchors as a list of records:
    #     [{"canonical_key": ..., "anchor_hash": ..., ...}, ...]
    # - Diff expects a dict mapping canonical_key -> anchor object.
    # Determinism:
    # - Pure reshaping; no new anchors invented.
    # =====================================================================
    def _coerce_metric_anchors_to_dict(resp: dict):
        try:
            if not isinstance(resp, dict):
                return resp
            ma = resp.get("metric_anchors")
            if isinstance(ma, dict) or ma is None:
                return resp
            if isinstance(ma, list):
                out = {}
                for a in ma:
                    if not isinstance(a, dict):
                        continue
                    ck = a.get("canonical_key") or a.get("ckey") or a.get("metric_key")
                    if not ck:
                        continue
                    if ck not in out:
                        out[str(ck)] = a
                resp["metric_anchors"] = out
            return resp
        except Exception:
            return resp

    prev_response = _coerce_metric_anchors_to_dict(prev_response)
    cur_response = _coerce_metric_anchors_to_dict(cur_response)
    # =====================================================================

    prev_can = prev_response.get("primary_metrics_canonical")
    cur_can = cur_response.get("primary_metrics_canonical")

    # =========================
    # Path A: canonical-first
    # =========================
    if isinstance(prev_can, dict) and isinstance(cur_can, dict) and prev_can:
        metric_changes = []
        unchanged = increased = decreased = found = 0

        for ckey, pm in prev_can.items():
            pm = pm if isinstance(pm, dict) else {}
            cm = cur_can.get(ckey)
            cm = cm if isinstance(cm, dict) else {}

            display_name = get_display_name(prev_response, pm, cm, ckey)
            definition = get_metric_definition(prev_response, ckey)

            prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

            # ✅ HARD STOP: canonical key missing in current => not_found (no name fallback)
            if ckey not in cur_can or not isinstance(cur_can.get(ckey), dict):
                # PATCH MA2 (ADDITIVE): fill row fields from metric_anchors where possible
                _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, {})

                metric_changes.append({
                    "name": display_name,
                    "previous_value": prev_raw,
                    "current_value": "N/A",
                    "change_pct": None,
                    "change_type": "not_found",
                    "match_confidence": 0.0,
                    "context_snippet": _ctx,
                    "source_url": _src,
                    "anchor_used": False,  # not applicable when current metric missing
                    "canonical_key": ckey,
                    "metric_definition": definition,
                    "anchor_confidence": _aconf,
                })
                continue

            found += 1

            cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

            # =========================================================================
            # PATCH D0 (ADDITIVE): anchor identity (do NOT force unchanged)
            # =========================================================================
            prev_ah = _get_prev_anchor_hash(prev_response, ckey, pm)
            cur_ah = _get_cur_anchor_hash(cur_response, ckey, cm)
            anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))
            # =========================================================================

            # PATCH D2 (ADDITIVE): use canonical values for diff when available
            prev_val, prev_unit_cmp = get_canonical_value_and_unit(pm)
            cur_val, cur_unit_cmp = get_canonical_value_and_unit(cm)

            # PATCH D3 (ADDITIVE): metric-specific tolerances (schema overrides)
            abs_eps, rel_eps = get_eps_for_metric(prev_response, ckey)

            change_type = "unknown"
            change_pct = None

            # =========================================================================
            # PATCH D0B (ADDITIVE, REQUIRED): numeric-first classification even if anchors match
            # Why:
            # - anchor_same means "we matched the same evidence anchor"
            # - It MUST NOT short-circuit classification to "unchanged" when values differ.
            # - This fixes the exact bug you observed: prev_value_norm != cur_value_norm
            #   while change_type incorrectly says "unchanged".
            # =========================================================================
            if prev_val is not None and cur_val is not None:
                if abs(prev_val - cur_val) <= max(abs_eps, abs(prev_val) * rel_eps):
                    change_type = "unchanged"
                    change_pct = 0.0
                    unchanged += 1
                elif cur_val > prev_val:
                    change_type = "increased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    increased += 1
                else:
                    change_type = "decreased"
                    change_pct = ((cur_val - prev_val) / max(abs_eps, abs(prev_val))) * 100.0
                    decreased += 1
            # If we cannot compare numerically, fall back:
            # - If anchors match, treat as unchanged ONLY as a last resort (formatting issue)
            elif anchor_same:
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            # =========================================================================

            # PATCH D4 (ADDITIVE): unit mismatch flag (debug only)
            unit_mismatch = False
            try:
                if prev_unit_cmp and cur_unit_cmp and str(prev_unit_cmp) != str(cur_unit_cmp):
                    unit_mismatch = True
            except Exception:
                pass
                unit_mismatch = False

            # PATCH MA2 (ADDITIVE): fill row fields from metric_anchors where possible
            _src, _ctx, _aconf = _anchor_meta(prev_response, cur_response, ckey, pm, cm)

            # PATCH D0C (ADDITIVE): match_confidence reflects anchor identity
            match_conf = 92.0
            try:
                if anchor_same:
                    match_conf = 98.0
            except Exception:
                pass
                match_conf = 92.0

            metric_changes.append({
                "name": display_name,
                "previous_value": prev_raw,
                "current_value": cur_raw,
                "change_pct": change_pct,
                "change_type": change_type,
                "match_confidence": float(match_conf),

                "context_snippet": _ctx,
                "source_url": _src,

                # anchor identity (matching), not classification
                "anchor_used": bool(anchor_same),
                "prev_anchor_hash": prev_ah,
                "cur_anchor_hash": cur_ah,

                "canonical_key": ckey,
                "metric_definition": definition,

                "anchor_confidence": _aconf,

                # expose canonical comparison basis for debugging/convergence
                "prev_value_norm": prev_val,
                "cur_value_norm": cur_val,
                "prev_unit_cmp": prev_unit_cmp,
                "cur_unit_cmp": cur_unit_cmp,
                "unit_mismatch": bool(unit_mismatch),
                "abs_eps_used": abs_eps,
                "rel_eps_used": rel_eps,
            })

        return metric_changes, unchanged, increased, decreased, found

    # =========================
    # Path B: legacy name fallback
    # =========================
    prev_metrics = prev_response.get("primary_metrics") or {}
    cur_metrics = cur_response.get("primary_metrics") or {}
    if not isinstance(prev_metrics, dict):
        prev_metrics = {}
    if not isinstance(cur_metrics, dict):
        cur_metrics = {}

    prev_index = {}
    for k, m in prev_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            prev_index[norm_name(name)] = (name, m)

    cur_index = {}
    for k, m in cur_metrics.items():
        if isinstance(m, dict):
            name = m.get("name") or k
            cur_index[norm_name(name)] = (name, m)

    metric_changes = []
    unchanged = increased = decreased = found = 0

    for nk, (display_name, pm) in prev_index.items():
        prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

        if nk not in cur_index:
            metric_changes.append({
                "name": display_name or "Unknown Metric",
                "previous_value": prev_raw,
                "current_value": "N/A",
                "change_pct": None,
                "change_type": "not_found",
                "match_confidence": 0.0,
                "context_snippet": None,
                "source_url": None,
                "anchor_used": False,
            })
            continue

        found += 1
        _, cm = cur_index[nk]
        cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

        prev_val, _prev_unit_cmp = get_canonical_value_and_unit(pm)
        cur_val, _cur_unit_cmp = get_canonical_value_and_unit(cm)

        prev_ah = _get_anchor_hash_from_metric(pm)
        cur_ah = _get_anchor_hash_from_metric(cm)
        anchor_same = bool(prev_ah and cur_ah and str(prev_ah) == str(cur_ah))

        change_type = "unknown"
        change_pct = None

        # PATCH D0B mirrors canonical path: numeric-first, anchor fallback only if numeric missing
        if prev_val is not None and cur_val is not None:
            if abs(prev_val - cur_val) <= max(ABS_EPS, abs(prev_val) * REL_EPS):
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            elif cur_val > prev_val:
                change_type = "increased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                increased += 1
            else:
                change_type = "decreased"
                change_pct = ((cur_val - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
                decreased += 1
        elif anchor_same:
            change_type = "unchanged"
            change_pct = 0.0
            unchanged += 1

        metric_changes.append({
            "name": display_name or "Unknown Metric",
            "previous_value": prev_raw,
            "current_value": cur_raw,
            "change_pct": change_pct,
            "change_type": change_type,
            "match_confidence": 90.0 if anchor_same else 80.0,
            "context_snippet": None,
            "source_url": None,

            "anchor_used": bool(anchor_same),
            "prev_anchor_hash": prev_ah,
            "cur_anchor_hash": cur_ah,

            "prev_value_norm": prev_val,
            "cur_value_norm": cur_val,
        })

    return metric_changes, unchanged, increased, decreased, found
# =====================================================================

def _fallback_match_from_snapshots(prev_numbers: dict, snapshots: list, anchors_by_name: dict):
    """
    When current analysis is missing, fall back to cached extracted_numbers only.
    If there is no snapshot candidate, return not_found ✅.

    Tightening implemented:
      1) Reject obvious year mismatches:
         - If metric name or prev_raw includes a year (e.g., 2024), require candidate context to contain it.
         - Also reject candidates that are a bare year if metric is not a year metric.
      2) Unit-family gating:
         - percent vs currency vs magnitude vs other (GW/TWh/tons/etc)
      3) Domain/homepage handling:
         - Downweight homepage sources heavily unless anchored (or if no non-homepage pool exists)

    Debugging enhancements:
      - Each metric row includes match_debug with:
        method, pool sizes, required years, unit families, best score, reject counts, top alternatives (small).
    """
    import re

    ABS_EPS = 1e-9
    REL_EPS = 0.0005

    def norm_unit(u: str) -> str:
        fn = globals().get("normalize_unit")
        if callable(fn):
            try:
                return fn(u)
            except Exception:
                return (u or "").strip()

    def parse_num(v, unit=""):
        fn = globals().get("parse_human_number")
        if callable(fn):
            try:
                return fn(str(v), unit)
            except Exception:
                return None
        try:
            return float(str(v).replace(",", "").strip())
        except Exception:
            return None

    def metric_tokens(name: str):
        toks = re.findall(r"[a-z0-9]+", (name or "").lower())
        stop = {"the","and","or","of","in","to","for","by","from","with","on","at","as"}
        return [t for t in toks if len(t) > 3 and t not in stop][:24]

    def unit_family(unit: str, raw: str = "", ctx: str = "") -> str:
        u = (norm_unit(unit) or "").strip().upper()
        blob = f"{raw or ''} {ctx or ''}".upper()

        # percent
        if u == "%" or "%" in blob:
            return "percent"

        # currency
        if any(x in blob for x in ["USD", "SGD", "EUR", "GBP", "S$", "$", "€", "£"]):
            return "currency"
        if any(x in u for x in ["USD", "SGD", "EUR", "GBP"]) or u.startswith("$") or u.startswith("S$"):
            return "currency"

        # magnitude suffix
        if u in ("K", "M", "B", "T") or any(x in blob for x in [" BILLION", " MILLION", " TRILLION", " BN", " MN"]):
            return "magnitude"

        # otherwise: other units like GW, TWh, tons, units, etc
        return "other"

    def required_years(metric_name: str, prev_raw: str) -> list:
        years = set()
        for s in [metric_name or "", prev_raw or ""]:
            for y in re.findall(r"\b(19\d{2}|20\d{2})\b", str(s)):
                years.add(y)
        return sorted(years)

    def year_ok(req_years: list, ctx: str) -> bool:
        if not req_years:
            return True
        c = (ctx or "").lower()
        return any(y.lower() in c for y in req_years)

    def is_bare_year(raw: str, unit: str) -> bool:
        r = (raw or "").strip()
        if unit and norm_unit(unit) not in ("", None):
            # If there is a unit, don't treat as bare year
            return False
        return bool(re.match(r"^(19\d{2}|20\d{2})$", r))

    def ctx_score(tokens, ctx: str) -> float:
        c = (ctx or "").lower()
        if not tokens:
            return 0.0
        hit = sum(1 for t in tokens if t in c)
        return hit / max(1, len(tokens))

    # Flatten candidates from snapshots ONLY, keep snapshot metadata
    candidates = []
    for sr in (snapshots or []):
        if not isinstance(sr, dict):
            continue
        url = sr.get("url")
        if not url:
            continue
        is_home = bool(sr.get("is_homepage"))
        qs = sr.get("quality_score", 1.0)
        try:
            qs = float(qs)
        except Exception:
            pass
            qs = 1.0

        for n in (sr.get("extracted_numbers") or []):
            if not isinstance(n, dict):
                continue
            candidates.append({
                "url": url,
                "value": n.get("value"),
                "unit": norm_unit(n.get("unit") or ""),
                "raw": n.get("raw") or "",
                "context": n.get("context") or "",
                "is_homepage": is_home,
                "quality_score": qs,
            })

    # Pre-split pools for tightening #3
    non_home = [c for c in candidates if not c.get("is_homepage")]
    home = [c for c in candidates if c.get("is_homepage")]

    out_changes = []
    for metric_name, prev in (prev_numbers or {}).items():
        prev_raw = prev.get("raw") or prev.get("value") or "N/A"
        prev_unit = norm_unit(prev.get("unit") or "")
        prev_val = prev.get("value")
        toks = prev.get("keywords") or metric_tokens(metric_name)

        req_years = required_years(metric_name, str(prev_raw))
        prev_fam = unit_family(prev_unit, str(prev_raw), "")

        anchor = anchors_by_name.get(metric_name) or {}
        anchor_url = anchor.get("source_url") if isinstance(anchor, dict) else None

        # Pool policy:
        # - anchored: use anchor_url pool if exists
        # - else: use non-homepage pool when available; only fall back to homepage if necessary
        pool_policy = "non_home_preferred"
        pool = non_home if non_home else candidates
        if anchor_url:
            anchored_pool = [c for c in candidates if c.get("url") == anchor_url]
            if anchored_pool:
                pool = anchored_pool
                pool_policy = "anchored_url"
            else:
                pool_policy = "anchored_url_not_present"

        reject_counts = {"year_mismatch": 0, "unit_mismatch": 0, "bare_year_reject": 0}
        best = None
        best_score = -1e9
        top_alts = []  # store a few near-misses for debugging

        for c in pool:
            ctx = c.get("context", "") or ""
            raw = c.get("raw", "") or ""
            unit = c.get("unit", "") or ""

            # (1) year gating: if required years exist, require them in context
            if not year_ok(req_years, ctx):
                reject_counts["year_mismatch"] += 1
                continue

            # reject bare-year candidates unless the metric itself is a year metric
            # (prevents "2024" being selected as a value for percent/currency/etc)
            if is_bare_year(str(raw), unit) and prev_fam != "other":
                reject_counts["bare_year_reject"] += 1
                continue

            # (2) unit-family gating
            cand_fam = unit_family(unit, raw, ctx)
            if prev_fam != cand_fam:
                reject_counts["unit_mismatch"] += 1
                continue

            score = ctx_score(toks, ctx)

            # bonus for numeric closeness
            cv = parse_num(c.get("value"), unit) or parse_num(raw, unit)
            if prev_val is not None and cv is not None:
                if abs(prev_val - cv) <= max(ABS_EPS, abs(prev_val) * REL_EPS):
                    score += 0.25

            # (3) homepage penalty unless anchored
            if c.get("is_homepage") and not anchor_url:
                score -= 0.35

            # quality_score weighting
            try:
                score *= max(0.1, min(1.0, float(c.get("quality_score", 1.0))))
            except Exception:
                pass

            # keep top alternatives for debugging
            if len(top_alts) < 5:
                top_alts.append({
                    "raw": raw[:60],
                    "unit": unit,
                    "url": c.get("url"),
                    "score": float(score),
                    "is_homepage": bool(c.get("is_homepage")),
                    "ctx": (ctx or "")[:120],
                })

            if score > best_score:
                best_score = score
                best = c

        # sort alt candidates by score desc
        try:
            top_alts.sort(key=lambda x: x.get("score", 0.0), reverse=True)
        except Exception:
            pass

        if not best:
            out_changes.append({
                "name": metric_name,
                "previous_value": prev_raw,
                "current_value": "N/A",
                "change_pct": None,
                "change_type": "not_found",
                "match_confidence": 0.0,
                "context_snippet": None,
                "source_url": None,
                "anchor_used": bool(anchor_url),

                # NEW debug payload
                "match_debug": {
                    "method": "snapshots_only",
                    "pool_policy": pool_policy,
                    "pool_size": int(len(pool)),
                    "req_years": req_years,
                    "prev_unit": prev_unit,
                    "prev_unit_family": prev_fam,
                    "reject_counts": reject_counts,
                    "top_alternatives": top_alts[:3],
                }
            })
            continue

        cur_raw = best.get("raw") or best.get("value")
        cv = parse_num(best.get("value"), best.get("unit")) or parse_num(cur_raw, best.get("unit"))

        change_type = "unknown"
        change_pct = None
        if prev_val is not None and cv is not None:
            if abs(prev_val - cv) <= max(ABS_EPS, abs(prev_val) * REL_EPS):
                change_type = "unchanged"
                change_pct = 0.0
            elif cv > prev_val:
                change_type = "increased"
                change_pct = ((cv - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0
            else:
                change_type = "decreased"
                change_pct = ((cv - prev_val) / max(ABS_EPS, abs(prev_val))) * 100.0

        conf = max(0.0, min(60.0, best_score * 60.0))

        out_changes.append({
            "name": metric_name,
            "previous_value": prev_raw,
            "current_value": cur_raw,
            "change_pct": change_pct,
            "change_type": change_type,
            "match_confidence": float(conf),
            "context_snippet": (best.get("context") or "")[:200] if isinstance(best.get("context"), str) else None,
            "source_url": best.get("url"),
            "anchor_used": bool(anchor_url),

            # NEW debug payload
            "match_debug": {
                "method": "snapshots_only",
                "pool_policy": pool_policy,
                "pool_size": int(len(pool)),
                "req_years": req_years,
                "prev_unit": prev_unit,
                "prev_unit_family": prev_fam,
                "best_unit": best.get("unit"),
                "best_unit_family": unit_family(best.get("unit") or "", best.get("raw") or "", best.get("context") or ""),
                "best_score": float(best_score),
                "best_is_homepage": bool(best.get("is_homepage")),
                "reject_counts": reject_counts,
                "top_alternatives": top_alts[:3],
            }
        })

    return out_changes


def compute_source_anchored_diff_BASE(previous_data: dict, web_context: dict = None) -> dict:
    """
    Tight source-anchored evolution:
      - Prefer snapshots from analysis (baseline_sources_cache)
      - Optionally reconstruct snapshots from web_context.scraped_meta
      - If no valid snapshots: return not_found (no heuristic junk)

    Always returns a dict.
    """
    import re
    from datetime import datetime, timezone

    # FIX2D66_PROMOTE_INJECTED_URLS_IN_ATTACH (ADDITIVE)
    try:
        _qtxt = str((analysis or {}).get('question') or (analysis or {}).get('query') or '')
        web_context = _fix2d66_promote_injected_urls(web_context or {}, question_text=_qtxt, stage='analysis_attach')
    except Exception:
        pass

    def _now():
        return datetime.now(timezone.utc).isoformat()

    def _safe_int(x, default=0):
        try:
            return int(x)
        except Exception:
            return default

    def _fingerprint(text: str):
        try:
            fn = globals().get("fingerprint_text")
            if callable(fn):
                return fn(text or "")
        except Exception:
            pass
        try:
            return fingerprint_text(text or "")
        except Exception:
            return None

    # ============================================================
    # PATCH CSR_UNWRAP1 (ADDITIVE): robust nested retrieval helpers
    # Why:
    # - Some runs store rebuild essentials under primary_response or results.primary_response
    # - Evolution may look only at top-level keys, causing schema=0 / anchors=0
    # ============================================================
    def _get_nested(d, path, default=None):
        try:
            x = d
            for k in path:
                if not isinstance(x, dict):
                    return default
                x = x.get(k)
            return x if x is not None else default
        except Exception:
            return default

    def _first_present(d, paths, default=None):
        for p in paths:
            v = _get_nested(d, p, None)
            if v is not None:
                return v
        return default
    # ============================================================

    # =====================================================================
    # PATCH HF5 (ADDITIVE): rehydrate previous_data from HistoryFull if wrapper
    # Why:
    # - Some UI/Sheets paths provide a summarized wrapper that lacks primary_response,
    #   metric_schema_frozen, metric_anchors, baseline_sources_cache, etc.
    # - If a full_store_ref pointer exists, load the full payload deterministically.
    #
    # NOTE:
    # - Do NOT write to `output` here (output not built yet). We stash flags
    #   and attach them after `output = {...}` is created.
    # =====================================================================
    _prev_rehydrated = False
    _prev_rehydrated_ref = ""

    try:
        if isinstance(previous_data, dict):
            _pr = previous_data.get("primary_response")

            # Determine if we are missing rebuild essentials
            _need = (
                (not isinstance(_pr, dict))
                or (not _pr)
                or (not isinstance(_pr.get("metric_schema_frozen"), dict))
            )

            if _need:
                # Explicit line (requested): simplest location first
                ref = previous_data.get("full_store_ref", "")  # <-- requested line

                # Then fall back to other known wrapper locations (more robust)
                _ref = (
                    ref
                    or (previous_data.get("results") or {}).get("full_store_ref")
                    or (isinstance(_pr, dict) and _pr.get("full_store_ref"))
                    or ""
                )

                # Last-ditch deterministic fallback: if wrapper carries _sheet_id
                if (not _ref) and isinstance(previous_data.get("_sheet_id"), str) and previous_data.get("_sheet_id"):
                    _ref = f"gsheet:HistoryFull:{previous_data.get('_sheet_id')}"

                if isinstance(_ref, str) and _ref.startswith("gsheet:"):
                    parts = _ref.split(":")
                    _ws_title = parts[1] if len(parts) > 1 and parts[1] else "HistoryFull"
                    _aid = parts[2] if len(parts) > 2 else ""
                    full = load_full_history_payload_from_sheet(_aid, worksheet_title=_ws_title) if _aid else {}
                    if isinstance(full, dict) and full:
                        previous_data = full
                        _prev_rehydrated = True
                        _prev_rehydrated_ref = _ref
    except Exception:
        pass
    # =====================================================================

    # ---------- Pull baseline snapshots (VALID only) ----------
    snapshot_origin = "none"
    baseline_sources_cache = []

    try:
        if isinstance(previous_data, dict):
            # 1) results.baseline_sources_cache (preferred)
            r = previous_data.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache"), list):
                baseline_sources_cache = r.get("baseline_sources_cache") or []
                if baseline_sources_cache:
                    snapshot_origin = "analysis_results_cache"

            # 2) top-level baseline_sources_cache
            if not baseline_sources_cache and isinstance(previous_data.get("baseline_sources_cache"), list):
                baseline_sources_cache = previous_data.get("baseline_sources_cache") or []
                if baseline_sources_cache:
                    snapshot_origin = "analysis_top_level_cache"
    except Exception:
        pass
        baseline_sources_cache = []

    # =====================================================================
    # PATCH ES1B (ADDITIVE): broaden snapshot discovery (legacy storage shapes)
    # =====================================================================
    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            pr = previous_data.get("primary_response") or {}
            if isinstance(pr, dict):
                # A) primary_response.results.baseline_sources_cache
                r2 = pr.get("results")
                if (not baseline_sources_cache) and isinstance(r2, dict) and isinstance(r2.get("baseline_sources_cache"), list):
                    baseline_sources_cache = r2.get("baseline_sources_cache") or []
                    if baseline_sources_cache:
                        snapshot_origin = "primary_response_results_cache"

                # B) primary_response.baseline_sources_cache
                if (not baseline_sources_cache) and isinstance(pr.get("baseline_sources_cache"), list):
                    baseline_sources_cache = pr.get("baseline_sources_cache") or []
                    if baseline_sources_cache:
                        snapshot_origin = "primary_response_top_level_cache"

                # C) primary_response.results.source_results (reconstruct minimal snapshot shape)
                if (not baseline_sources_cache) and isinstance(r2, dict) and isinstance(r2.get("source_results"), list):
                    rebuilt_sr = []
                    for sr in (r2.get("source_results") or []):
                        if not isinstance(sr, dict):
                            continue
                        u = sr.get("source_url") or sr.get("url")
                        ex = sr.get("extracted_numbers")
                        if u and isinstance(ex, list) and ex:
                            rebuilt_sr.append({
                                "source_url": u,
                                "extracted_numbers": ex,
                                "clean_text": sr.get("clean_text") or sr.get("content") or "",
                                "fingerprint": sr.get("fingerprint"),
                                "fetched_at": sr.get("fetched_at"),
                            })
                    rebuilt_sr.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))
                    if rebuilt_sr:
                        baseline_sources_cache = rebuilt_sr
                        snapshot_origin = "primary_response_source_results_rebuild"

        # D) previous_data.results.source_results fallback
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            r3 = previous_data.get("results")
            if isinstance(r3, dict) and isinstance(r3.get("source_results"), list):
                rebuilt_sr2 = []
                for sr in (r3.get("source_results") or []):
                    if not isinstance(sr, dict):
                        continue
                    u = sr.get("source_url") or sr.get("url")
                    ex = sr.get("extracted_numbers")
                    if u and isinstance(ex, list) and ex:
                        rebuilt_sr2.append({
                            "source_url": u,
                            "extracted_numbers": ex,
                            "clean_text": sr.get("clean_text") or sr.get("content") or "",
                            "fingerprint": sr.get("fingerprint"),
                            "fetched_at": sr.get("fetched_at"),
                        })
                rebuilt_sr2.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))
                if rebuilt_sr2:
                    baseline_sources_cache = rebuilt_sr2
                    snapshot_origin = "analysis_source_results_rebuild"
    except Exception:
        pass

    # =====================================================================
    # PATCH SS6C (ADDITIVE): evidence_records fallback for snapshots (evolution-time)
    # =====================================================================
    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            _er = None
            if isinstance(previous_data.get("results"), dict):
                _er = previous_data["results"].get("evidence_records")
            if _er is None:
                _er = previous_data.get("evidence_records")
            _rebuilt = build_baseline_sources_cache_from_evidence_records(_er)
            if isinstance(_rebuilt, list) and _rebuilt:
                baseline_sources_cache = _rebuilt
                snapshot_origin = "evidence_records_rebuild"
    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH ES1C (ADDITIVE): validate snapshot shape & prepare debug metadata
    # =====================================================================
    _snapshot_debug = None
    try:
        _raw_len = int(len(baseline_sources_cache)) if isinstance(baseline_sources_cache, list) else 0
        _kept = []
        if isinstance(baseline_sources_cache, list):
            for s in baseline_sources_cache:
                if not isinstance(s, dict):
                    continue
                u = s.get("source_url") or s.get("url")
                ex = s.get("extracted_numbers")
                if u and isinstance(ex, list):
                    _kept.append(s)
        _kept.sort(key=lambda d: (str(d.get("source_url") or d.get("url") or ""), str(d.get("fingerprint") or "")))
        baseline_sources_cache = _kept
        _snapshot_debug = {
            "origin": snapshot_origin,
            "raw_count": _raw_len,
            "valid_count": int(len(baseline_sources_cache)),
            "example_urls": [x.get("source_url") or x.get("url") for x in (baseline_sources_cache[:3] if isinstance(baseline_sources_cache, list) else [])],
            "prev_keys": sorted(list(previous_data.keys()))[:40] if isinstance(previous_data, dict) else [],
        }
    except Exception:
        pass
    # =====================================================================

    # 3) reconstruct from web_context.scraped_meta (if provided)
    if (not baseline_sources_cache) and isinstance(web_context, dict):
        try:
            scraped_meta = web_context.get("scraped_meta") or {}
            rebuilt = []
            if isinstance(scraped_meta, dict):
                for url, meta in scraped_meta.items():
                    if not isinstance(meta, dict):
                        continue
                    content = meta.get("clean_text") or meta.get("content") or ""
                    fp = meta.get("fingerprint") or _fingerprint(content)
                    if not fp or len(content or "") < 800:
                        continue
                    nums = meta.get("extracted_numbers") or []
                    if not isinstance(nums, list):
                        nums = []
                    rebuilt.append({
                        "url": url,
                        "status": meta.get("status") or "fetched",
                        "status_detail": meta.get("status_detail") or "",
                        "numbers_found": _safe_int(meta.get("numbers_found"), default=len(nums)),
                        "fetched_at": meta.get("fetched_at") or _now(),
                        "fingerprint": fp,
                        "content_type": meta.get("content_type") or "",
                        "extracted_numbers": [
                            {
                                "value": n.get("value"),
                                "unit": n.get("unit"),
                                "raw": n.get("raw"),
                                "context_snippet": (n.get("context_snippet") or n.get("context") or "")[:200],
                                "anchor_hash": n.get("anchor_hash"),
                                "is_junk": n.get("is_junk"),
                                "junk_reason": n.get("junk_reason"),
                                "unit_tag": n.get("unit_tag"),
                                "unit_family": n.get("unit_family"),
                                "base_unit": n.get("base_unit"),
                                "multiplier_to_base": n.get("multiplier_to_base"),
                                "value_norm": n.get("value_norm"),
                                "start_idx": n.get("start_idx"),
                                "end_idx": n.get("end_idx"),
                                "source_url": n.get("source_url") or url,
                            }
                            for n in nums if isinstance(n, dict)
                        ]
                    })
            if rebuilt:
                baseline_sources_cache = rebuilt
                snapshot_origin = "web_context_scraped_meta"
        except Exception:
            pass

    # Also count invalid snapshots for debug (if present)
    invalid_count = 0
    try:
        if isinstance(previous_data, dict):
            r = previous_data.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache_invalid"), list):
                invalid_count = len(r.get("baseline_sources_cache_invalid") or [])
    except Exception:
        pass
        invalid_count = 0

    # ---------- Prepare stable default output ----------
    output = {
        "status": "success",
        "message": "",
        "sources_checked": 0,
        "sources_fetched": 0,
        "numbers_extracted_total": 0,
        "stability_score": 0.0,
        "summary": {
            "total_metrics": 0,
            "metrics_found": 0,
            "metrics_increased": 0,
            "metrics_decreased": 0,
            "metrics_unchanged": 0,
        },
        "metric_changes": [],
        "source_results": [],
        "interpretation": "",
        "snapshot_origin": snapshot_origin,
        "valid_snapshot_count": len(baseline_sources_cache or []),
        "invalid_snapshot_count": int(invalid_count),
        "generated_at": _now(),
    }

    # Attach debug flags (rehydration + snapshot_debug)
    try:
        if _prev_rehydrated:
            output["previous_data_rehydrated"] = True
            output["previous_data_full_store_ref"] = _prev_rehydrated_ref
    except Exception:
        pass
    try:
        if isinstance(_snapshot_debug, dict) and _snapshot_debug:
            output["snapshot_debug"] = _snapshot_debug
    except Exception:
        pass

    # =====================================================================
    # PATCH SS6 (ADDITIVE, REQUIRED): last-chance snapshot rehydration
    # =====================================================================
    try:
        if not baseline_sources_cache and isinstance(previous_data, dict):
            _ref = previous_data.get("snapshot_store_ref") or (previous_data.get("results") or {}).get("snapshot_store_ref")
            _hash = previous_data.get("source_snapshot_hash") or (previous_data.get("results") or {}).get("source_snapshot_hash")

            # ============================================================
            # PATCH FIX41I_SS6_STABLE (ADDITIVE): prefer v2/stable snapshot refs & hashes
            # Why:
            # - Analysis now emits stable/v2 snapshot hashes (source_snapshot_hash_v2 / _stable) and
            #   snapshot_store_ref_v2 pointing at the same Snapshots row key.
            # - Evolution must prefer these fields to keep fastpath alignment intact.
            # ============================================================
            try:
                _ref_v2 = previous_data.get("snapshot_store_ref_v2") or previous_data.get("snapshot_store_ref_stable")
                if (not _ref_v2) and isinstance(previous_data.get("results"), dict):
                    _ref_v2 = (previous_data.get("results") or {}).get("snapshot_store_ref_v2") or (previous_data.get("results") or {}).get("snapshot_store_ref_stable")
                if isinstance(_ref_v2, str) and _ref_v2:
                    _ref = _ref_v2  # prefer stable ref
            except Exception:
                pass

            try:
                _hash_stable = previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2")
                if (not _hash_stable) and isinstance(previous_data.get("results"), dict):
                    _hash_stable = (previous_data.get("results") or {}).get("source_snapshot_hash_stable") or (previous_data.get("results") or {}).get("source_snapshot_hash_v2")
                if isinstance(_hash_stable, str) and _hash_stable:
                    _hash = _hash_stable  # prefer stable hash
            except Exception:
                pass
            # ============================================================

            if isinstance(_ref, str) and _ref.startswith("gsheet:"):
                parts = _ref.split(":")
                _ws_title = parts[1] if len(parts) > 1 and parts[1] else "Snapshots"
                _h = parts[2] if len(parts) > 2 else ""
                baseline_sources_cache = load_full_snapshots_from_sheet(_h, worksheet_title=_ws_title) if _h else []
                if baseline_sources_cache:
                    output["snapshot_origin"] = "sheet_snapshot_store_ref"

            if not baseline_sources_cache and isinstance(_hash, str) and _hash:
                baseline_sources_cache = load_full_snapshots_from_sheet(_hash, worksheet_title="Snapshots")
                if baseline_sources_cache:
                    output["snapshot_origin"] = "sheet_source_snapshot_hash"

            if not baseline_sources_cache and isinstance(_ref, str) and _ref and not _ref.startswith("gsheet:"):
                baseline_sources_cache = load_full_snapshots_local(_ref)
                if baseline_sources_cache:
                    output["snapshot_origin"] = "local_snapshot_store_ref"

            if isinstance(baseline_sources_cache, list):
                output["valid_snapshot_count"] = len(baseline_sources_cache)
    except Exception:
        pass
    # =====================================================================


    # =====================================================================
    # PATCH REFACTOR39_SNAPSHOT_STORE_FALLBACK (ADDITIVE)
    # Purpose:
    # - During HistoryFull persistence we may omit baseline_sources_cache to avoid Sheets cell limits,
    #   and instead persist snapshots in the Snapshots worksheet / local snapshot store with a ref/hash.
    # - Source-anchored evolution MUST be able to rehydrate baseline snapshots from:
    #     * snapshot_store_ref / snapshot_store_ref_v2 (gsheet:Snapshots:<hash> OR local path)
    #     * source_snapshot_hash_v2 / source_snapshot_hash
    # Behavior:
    # - If baseline_sources_cache is empty after normal discovery, attempt to load snapshots deterministically.
    # - Still strict: if we cannot load snapshots, we remain snapshot-gated (no fabricated matches).
    # =====================================================================
    _snapshot_store_debug = {}
    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            _res = previous_data.get("results") if isinstance(previous_data.get("results"), dict) else {}
            _pr = previous_data.get("primary_response") if isinstance(previous_data.get("primary_response"), dict) else {}
            _pr_res = _pr.get("results") if isinstance(_pr.get("results"), dict) else {}

            # Prefer explicit refs; fall back to hashes
            _store_ref = (
                previous_data.get("snapshot_store_ref")
                or (_res.get("snapshot_store_ref") if isinstance(_res, dict) else "")
                or (_pr.get("snapshot_store_ref") if isinstance(_pr, dict) else "")
                or (_pr_res.get("snapshot_store_ref") if isinstance(_pr_res, dict) else "")
                or ""
            )

            _store_ref_v2 = (
                previous_data.get("snapshot_store_ref_v2")
                or (_res.get("snapshot_store_ref_v2") if isinstance(_res, dict) else "")
                or (_pr.get("snapshot_store_ref_v2") if isinstance(_pr, dict) else "")
                or (_pr_res.get("snapshot_store_ref_v2") if isinstance(_pr_res, dict) else "")
                or ""
            )

            _ssh_v2 = (
                previous_data.get("source_snapshot_hash_v2")
                or (_res.get("source_snapshot_hash_v2") if isinstance(_res, dict) else "")
                or (_pr.get("source_snapshot_hash_v2") if isinstance(_pr, dict) else "")
                or (_pr_res.get("source_snapshot_hash_v2") if isinstance(_pr_res, dict) else "")
                or ""
            )

            _ssh_v1 = (
                previous_data.get("source_snapshot_hash")
                or (_res.get("source_snapshot_hash") if isinstance(_res, dict) else "")
                or (_pr.get("source_snapshot_hash") if isinstance(_pr, dict) else "")
                or (_pr_res.get("source_snapshot_hash") if isinstance(_pr_res, dict) else "")
                or ""
            )

            def _extract_hash(ref: str) -> str:
                try:
                    ref = str(ref or "")
                    if ref.startswith("gsheet:Snapshots:"):
                        return ref.split(":")[-1]
                except Exception:
                    pass
                return ""

            _hash_to_load = _extract_hash(_store_ref_v2) or _extract_hash(_store_ref) or str(_ssh_v2 or "") or str(_ssh_v1 or "")

            _snapshot_store_debug = {
                "store_ref": str(_store_ref or ""),
                "store_ref_v2": str(_store_ref_v2 or ""),
                "ssh_v2_present": bool(_ssh_v2),
                "ssh_v1_present": bool(_ssh_v1),
                "hash_to_load": str(_hash_to_load or ""),
            }

            _loaded = []
            _loaded_origin = ""

            # 1) Load by explicit gsheet ref (v2 then v1)
            if isinstance(_store_ref_v2, str) and _store_ref_v2.startswith("gsheet:Snapshots:"):
                try:
                    _loaded = load_full_snapshots_from_sheet(_store_ref_v2.split(":")[-1], worksheet_title="Snapshots")
                    _loaded_origin = "gsheet_ref_v2"
                except Exception:
                    pass

            if (not _loaded) and isinstance(_store_ref, str) and _store_ref.startswith("gsheet:Snapshots:"):
                try:
                    _loaded = load_full_snapshots_from_sheet(_store_ref.split(":")[-1], worksheet_title="Snapshots")
                    _loaded_origin = "gsheet_ref_v1"
                except Exception:
                    pass

            # 2) Load by local store ref if it looks like a path
            if (not _loaded) and isinstance(_store_ref, str) and _store_ref and (not _store_ref.startswith("gsheet:")):
                try:
                    _loaded = load_full_snapshots_local(_store_ref)
                    _loaded_origin = "local_ref"
                except Exception:
                    pass

            # 3) Load by hash (sheet first, then deterministic local path)
            if (not _loaded) and _hash_to_load:
                try:
                    _loaded = load_full_snapshots_from_sheet(str(_hash_to_load), worksheet_title="Snapshots")
                    _loaded_origin = "gsheet_hash"
                except Exception:
                    pass

            if (not _loaded) and _hash_to_load:
                try:
                    # Deterministic path used by store_full_snapshots_local
                    import os
                    _p = os.path.join(_snapshot_store_dir(), f"{str(_hash_to_load)}.json")
                    _loaded = load_full_snapshots_local(_p)
                    if _loaded:
                        _loaded_origin = "local_hash_path"
                except Exception:
                    pass

            if isinstance(_loaded, list) and _loaded:
                baseline_sources_cache = _loaded
                snapshot_origin = f"snapshot_store_fallback:{_loaded_origin}"
                _snapshot_store_debug["loaded_count"] = int(len(_loaded))
                _snapshot_store_debug["loaded_origin"] = str(_loaded_origin)
            else:
                _snapshot_store_debug["loaded_count"] = 0
                _snapshot_store_debug["loaded_origin"] = str(_loaded_origin or "none")
    except Exception:
        pass

    # Re-validate snapshot shape after fallback load (keeps strict invariants)
    try:
        if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
            _kept2 = []
            for s in baseline_sources_cache:
                if not isinstance(s, dict):
                    continue
                u = s.get("source_url") or s.get("url")
                ex = s.get("extracted_numbers")
                # Some legacy stores use "numbers" instead of "extracted_numbers"
                if ex is None and isinstance(s.get("numbers"), list):
                    try:
                        s["extracted_numbers"] = s.get("numbers") or []
                        ex = s.get("extracted_numbers")
                    except Exception:
                        pass
                if u and isinstance(ex, list):
                    _kept2.append(s)
            _kept2.sort(key=lambda d: (str(d.get("source_url") or d.get("url") or ""), str(d.get("fingerprint") or "")))
            baseline_sources_cache = _kept2
            # Update debug if available
            if isinstance(_snapshot_debug, dict):
                _snapshot_debug["origin"] = snapshot_origin
                _snapshot_debug["valid_count"] = int(len(baseline_sources_cache))
    except Exception:
        pass
    # =====================================================================

    # If no valid snapshots, return "not_found"
    if not baseline_sources_cache:
        try:
            output.setdefault("debug", {})
            if isinstance(output.get("debug"), dict):
                if isinstance(_snapshot_debug, dict):
                    output["debug"]["snapshot_debug_v1"] = _snapshot_debug
                if isinstance(_snapshot_store_debug, dict) and _snapshot_store_debug:
                    output["debug"]["snapshot_store_debug_v1"] = _snapshot_store_debug
        except Exception:
            pass
        output["status"] = "failed"
        output["message"] = "No valid snapshots available for source-anchored evolution. (Snapshot store fallback attempted; no re-fetch / no heuristic matching performed.)"
        output["interpretation"] = "Snapshot-gated: evolution refused to fabricate matches without valid cached source text."
        # ============================================================
        # PATCH START: FIX2D8_PROMOTE_NESTED_RESULTS_CALLSITE_V1
        # Purpose: Ensure results.* contains promoted canonical outputs
        # ============================================================
        try:
            _fix2d8_promote_nested_results_v1(output)
        except Exception:
            pass
        # ============================================================
        # PATCH END: FIX2D8_PROMOTE_NESTED_RESULTS_CALLSITE_V1
        # ============================================================
        # ============================================================
        # PATCH START: FIX2D10_MATERIALIZE_OUTPUT_DEBUG_CANONICAL_FOR_RENDER_CALLSITE_V1
        # Purpose: ensure output_debug.canonical_for_render_v1 is present for dashboard hydration
        # ============================================================
        try:
            _fix2d10_materialize_output_debug_canonical_for_render_v1(output)
        except Exception:
            pass
        # ============================================================
        # PATCH END: FIX2D10_MATERIALIZE_OUTPUT_DEBUG_CANONICAL_FOR_RENDER_CALLSITE_V1
        # ============================================================
        # PATCH FIX2D20 (ADD): trace year-like commits in primary_metrics_canonical
        _fix2d20_trace_year_like_commits(output, stage=str((output or {}).get('results',{}).get('debug',{}).get('stage') or 'evolution'), callsite='compute_source_anchored_diff_return')
        return output    # =====================================================================
    # PATCH FIX41AFC10 (ADDITIVE): Fetch + attach injected URL snapshots when injection delta exists
    #
    # Goal:
    # - We already bypass fastpath when injected URL delta exists.
    # - However, evolution can still be snapshot-gated and never actually fetch the injected URL.
    # - This patch performs a **targeted fetch** for injected URLs that are not already present in
    #   the baseline snapshot universe, then appends them to `baseline_sources_cache` so they can
    #   participate in downstream admission / hashing / metric rebuild deterministically.
    #
    # Non-negotiables:
    # - No change to normal (no-injection) behavior
    # - Additive only; no refactors
    # =====================================================================
    try:
        _fix41afc10_injected_norm = []
        try:
            # Robust extraction order (same intent as FIX41AFC9 lineage)
            _fix41afc10_injected_norm = _inj_diag_norm_url_list(
                (web_context or {}).get("extra_urls")
                or (web_context or {}).get("diag_extra_urls_ui")
                or []
            )
        except Exception:
            pass
            _fix41afc10_injected_norm = []

        # If UI raw textarea is present, merge its parsed URLs (newline/comma separated)
        try:
            _fix41afc10_ui_raw = str((web_context or {}).get("diag_extra_urls_ui_raw") or (web_context or {}).get("extra_urls_ui_raw") or "")
            if _fix41afc10_ui_raw.strip():
                _fix41afc10_from_raw = _inj_diag_norm_url_list(_fix41afc10_ui_raw)
                if _fix41afc10_from_raw:
                    _fix41afc10_injected_norm = _inj_diag_norm_url_list((_fix41afc10_injected_norm or []) + _fix41afc10_from_raw)
        except Exception:
            pass

        # Only run when there is any injected URL present at all (do not affect normal runs)
        if _fix41afc10_injected_norm:
            # Build baseline URL set from existing snapshots
            _fix41afc10_base_set = set()
            try:
                for _s in (baseline_sources_cache or []):
                    if isinstance(_s, dict) and _s.get("url"):
                        _fix41afc10_base_set.add(_normalize_url(_s.get("url")) or str(_s.get("url")))
            except Exception:
                pass

            _fix41afc10_missing = [u for u in _fix41afc10_injected_norm if (u and (u not in _fix41afc10_base_set))]
            if _fix41afc10_missing:
                _fix41afc10_attempted = []
                _fix41afc10_persisted = []
                for _u in _fix41afc10_missing:
                    try:
                        _fix41afc10_attempted.append(_u)

                        _txt = None
                        try:
                            _txt = fetch_url_content(_u)
                        except Exception:
                            pass
                            _txt = None

                        _status = "failed"
                        _detail = "no_text"
                        _nums = []
                        if isinstance(_txt, str) and _txt.strip():
                            _status = "success"
                            _detail = "fetched"
                            try:
                                _nums = extract_numbers_with_context(_txt, source_url=_u) or []
                            except Exception:
                                pass
                                _nums = []
                        # fingerprint (best-effort; used only for determinism / debugging)
                        _fp = ""
                        try:
                            if isinstance(_txt, str) and _txt:
                                _fp = hashlib.sha256(_txt.encode("utf-8", errors="ignore")).hexdigest()
                        except Exception:
                            pass
                            _fp = ""

                        _snap = {
                            "url": _u,
                            "status": _status,
                            "status_detail": _detail,
                            "numbers_found": len(_nums or []),
                            "fetched_at": _now(),
                            "fingerprint": _fp,
                            "extracted_numbers": _nums or [],
                        }

                        # Append snapshot (even if failed) so lifecycle is visible + deterministic
                        try:
                            if isinstance(baseline_sources_cache, list):
                                baseline_sources_cache.append(_snap)
                                _fix41afc10_persisted.append(_u)
                        except Exception:
                            pass
                    except Exception:
                        pass
                        # never break evolution on injected fetch attempts
                        pass

                # Emit explicit debug for closure visibility
                try:
                    if isinstance(output.get("debug"), dict):
                        output.setdefault("debug", {})
                        output["debug"].setdefault("fix35", {})
                        output["debug"]["fix35"]["injected_fetch_attempted_count"] = len(_fix41afc10_attempted)
                        output["debug"]["fix35"]["injected_fetch_attempted"] = _fix41afc10_attempted
                        output["debug"]["fix35"]["injected_fetch_snapshot_appended_count"] = len(_fix41afc10_persisted)
                        output["debug"]["fix35"]["injected_fetch_snapshot_appended"] = _fix41afc10_persisted
                except Exception:
                    pass
    except Exception:
        pass

    # ---------- Use your existing deterministic metric diff helper ----------
    prev_response = (previous_data or {}).get("primary_response", {}) or {}

    # =====================================================================
    # PATCH HF6 (ADDITIVE): tolerate previous_data being the primary_response itself
    # =====================================================================
    try:
        if (not isinstance(prev_response, dict) or not prev_response) and isinstance(previous_data, dict):
            if isinstance(previous_data.get("primary_metrics_canonical"), dict) or isinstance(previous_data.get("metric_schema_frozen"), dict):
                prev_response = previous_data
    except Exception:
        pass
    # =====================================================================

    prev_metrics = prev_response.get("primary_metrics_canonical") or prev_response.get("primary_metrics") or {}

    # ============================================================
    # PATCH CSR_INPUTS1 (ADDITIVE): normalize prev schema/anchors/canon
    # (safe alias for prior `prev_analysis` usage)
    # ============================================================
    prev_analysis = previous_data  # PATCH CSR_INPUTS1_ALIAS (ADDITIVE)
    try:
        prev_schema = _first_present(prev_analysis, [
            ("metric_schema_frozen",),
            ("primary_response", "metric_schema_frozen"),
            ("results", "metric_schema_frozen"),
            ("results", "primary_response", "metric_schema_frozen"),
        ], default=None)

        prev_canon = _first_present(prev_analysis, [
            ("primary_metrics_canonical",),
            ("primary_response", "primary_metrics_canonical"),
            ("results", "primary_metrics_canonical"),
            ("results", "primary_response", "primary_metrics_canonical"),
        ], default=None)

        prev_anchors = _first_present(prev_analysis, [
            ("metric_anchors",),
            ("primary_response", "metric_anchors"),
            ("results", "metric_anchors"),
            ("results", "primary_response", "metric_anchors"),
        ], default=None)

        if (not isinstance(prev_schema, dict) or not prev_schema) and isinstance(prev_canon, dict) and prev_canon:
            try:
                fn = globals().get("freeze_metric_schema")
                if callable(fn):
                    prev_schema = fn(prev_canon)
            except Exception:
                pass
    except Exception:
        pass
    # ============================================================



    # =====================================================================
    # PATCH FIX2D73 (ADDITIVE): ensure prev_response carries baseline canonical metrics
    # Why:
    # - Diff Panel V2 consumes prev_response.primary_metrics_canonical.
    # - HistoryFull rehydrate can place canonical metrics under nested containers
    #   (e.g., previous_data.results.primary_metrics_canonical), leaving prev_response empty.
    # What:
    # - If prev_canon exists, copy into prev_response.primary_metrics_canonical when missing.
    # - Also expose at top-level previous_data.primary_metrics_canonical (debug/compat).
    # - Record debug counts for closure verification.
    # =====================================================================
    try:
        if isinstance(prev_response, dict):
            if (not isinstance(prev_response.get("primary_metrics_canonical"), dict)) or (not prev_response.get("primary_metrics_canonical")):
                if isinstance(prev_canon, dict) and prev_canon:
                    prev_response["primary_metrics_canonical"] = prev_canon
        if isinstance(previous_data, dict):
            if (not isinstance(previous_data.get("primary_metrics_canonical"), dict)) or (not previous_data.get("primary_metrics_canonical")):
                if isinstance(prev_canon, dict) and prev_canon:
                    previous_data["primary_metrics_canonical"] = prev_canon
    except Exception:
        pass

    try:
        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            output["debug"].setdefault("fix2d73", {})
            if isinstance(output["debug"].get("fix2d73"), dict):
                output["debug"]["fix2d73"].update({
                    "prev_canon_count": int(len(prev_canon)) if isinstance(prev_canon, dict) else 0,
                    "prev_response_pmc_count": int(len(prev_response.get("primary_metrics_canonical") or {})) if isinstance(prev_response, dict) and isinstance(prev_response.get("primary_metrics_canonical"), dict) else 0,
                    "previous_data_top_pmc_count": int(len(previous_data.get("primary_metrics_canonical") or {})) if isinstance(previous_data, dict) and isinstance(previous_data.get("primary_metrics_canonical"), dict) else 0,
                })
    except Exception:
        pass
    # =====================================================================
    # Ensure schema/anchors are available inside prev_response (additive copies)
    try:
        if isinstance(prev_response, dict) and not isinstance(prev_response.get("metric_schema_frozen"), dict):
            if isinstance(previous_data.get("metric_schema_frozen"), dict):
                prev_response["metric_schema_frozen"] = previous_data.get("metric_schema_frozen")
    except Exception:
        pass
    try:
        if isinstance(prev_response, dict) and not isinstance(prev_response.get("metric_anchors"), dict):
            if isinstance(previous_data.get("metric_anchors"), dict):
                prev_response["metric_anchors"] = previous_data.get("metric_anchors")
    except Exception:
        pass

    # Build a minimal current metrics dict from snapshots:
    current_metrics = {}

    # Prefer metric_anchors to rebuild current_metrics (snapshot-gated)
    def _get_metric_anchors(prev: dict) -> dict:
        if not isinstance(prev, dict):
            return {}
        a = prev.get("metric_anchors")
        if isinstance(a, dict) and a:
            return a
        pr = prev.get("primary_response")
        if isinstance(pr, dict):
            a2 = pr.get("metric_anchors")
            if isinstance(a2, dict) and a2:
                return a2
        res = prev.get("results")
        if isinstance(res, dict):
            a3 = res.get("metric_anchors")
            if isinstance(a3, dict) and a3:
                return a3
        return {}

    def _canonicalize_candidate(n: dict) -> dict:
        try:
            fn = globals().get("canonicalize_numeric_candidate")
            if callable(fn):
                return fn(dict(n))
        except Exception:
            return dict(n)

    def _build_anchor_to_candidate_map(snapshots: list) -> dict:
        m = {}
        for sr in snapshots or []:
            if not isinstance(sr, dict):
                continue
            for n in (sr.get("extracted_numbers") or []):
                if not isinstance(n, dict):
                    continue
                nn = _canonicalize_candidate(n)
                ah = nn.get("anchor_hash")
                if not ah:
                    continue
                if ah not in m:
                    m[ah] = nn
        return m

    try:
        metric_anchors = _get_metric_anchors(previous_data)
        anchor_to_candidate = _build_anchor_to_candidate_map(baseline_sources_cache)

        if isinstance(metric_anchors, dict) and metric_anchors:
            for ckey, a in metric_anchors.items():
                if not isinstance(a, dict):
                    continue
                ah = a.get("anchor_hash") or a.get("anchor")
                if not ah:
                    continue
                cand = anchor_to_candidate.get(ah)
                if not isinstance(cand, dict):
                    continue

                base = prev_metrics.get(ckey) if isinstance(prev_metrics, dict) else None
                out_row = dict(base) if isinstance(base, dict) else {}
                out_row.update({
                    "canonical_key": ckey,
                    "anchor_hash": ah,
                    "anchor_used": True,
                    "anchor_confidence": a.get("anchor_confidence"),
                    "source_url": cand.get("source_url") or a.get("source_url"),
                    "raw": cand.get("raw"),
                    "value": cand.get("value"),
                    "unit": cand.get("unit"),
                    "value_norm": cand.get("value_norm"),
                    "context_snippet": cand.get("context_snippet") or cand.get("context") or "",
                    "candidate_id": cand.get("candidate_id") or a.get("candidate_id"),
                    "fix36_origin": "anchor_mapping",  # PATCH FIX36 (ADD): per-metric provenance
                })
                current_metrics[ckey] = out_row
    except Exception:
        pass

    # ============================================================
    # PATCH FIX36 (ADDITIVE): set current_metrics_origin when anchor mapping populated any metrics
    # ============================================================
    try:
        if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
            if output["debug"]["fix35"].get("current_metrics_origin") in (None, "", "unknown"):
                # If any current metric was filled via anchor mapping, stamp origin
                if isinstance(current_metrics, dict) and any(isinstance(v, dict) and v.get("anchor_used") for v in current_metrics.values()):
                    output["debug"]["fix35"]["current_metrics_origin"] = "anchor_mapping"
    except Exception:
        pass
    # ============================================================

    # Rebuild fallback only if anchors didn't produce metrics
    if not isinstance(current_metrics, dict) or not current_metrics:
        try:
            # =====================================================================
            # PATCH FIX41AFC14 (ADDITIVE): Augment baseline_sources_cache with injected URL delta BEFORE schema-only rebuild
            #
            # Problem:
            # - With fastpath bypassed, evolution may fall back to schema-only rebuild *without* running any
            #   fetch cycle. In that case, injected URLs never enter baseline_sources_cache, so rebuild cannot
            #   see them and injection remains inert (attempted=0, persisted=0).
            #
            # Goal:
            # - If injected URLs are present AND introduce a delta vs the current snapshot universe,
            #   run fetch_web_context() once (normal mode) with force_admit/force_scrape enabled,
            #   then merge any successful injected snapshots into baseline_sources_cache, and only then
            #   call the rebuild function.
            #
            # Safety:
            # - No effect when no injection or no delta.
            # - Does NOT alter hashing logic; it only ensures the snapshot pool reflects successfully fetched injected sources.
            # - Never raises; on any failure, proceeds with the original baseline_sources_cache.
            # =====================================================================
            try:
                _fx14_wc = web_context if isinstance(web_context, dict) else {}
                _fx14_extra_raw = []
                if isinstance(_fx14_wc.get("extra_urls"), (list, tuple)) and _fx14_wc.get("extra_urls"):
                    _fx14_extra_raw = list(_fx14_wc.get("extra_urls") or [])
                elif isinstance(_fx14_wc.get("diag_extra_urls_ui"), (list, tuple)) and _fx14_wc.get("diag_extra_urls_ui"):
                    _fx14_extra_raw = list(_fx14_wc.get("diag_extra_urls_ui") or [])
                elif isinstance(_fx14_wc.get("diag_extra_urls_ui_raw"), str) and (_fx14_wc.get("diag_extra_urls_ui_raw") or "").strip():
                    _raw = str(_fx14_wc.get("diag_extra_urls_ui_raw") or "")
                    _parts = []
                    for _line in _raw.splitlines():
                        _line = (_line or "").strip()
                        if not _line:
                            continue
                        for _p in _line.split(","):
                            _p = (_p or "").strip()
                            if _p:
                                _parts.append(_p)
                    _fx14_extra_raw = _parts

                _fx14_inj = _inj_diag_norm_url_list(_fx14_extra_raw) if _fx14_extra_raw else []
                _fx14_base_urls = []
                if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                    for _r in baseline_sources_cache:
                        if isinstance(_r, dict) and isinstance(_r.get("source_url"), str) and _r.get("source_url"):
                            _fx14_base_urls.append(_r.get("source_url"))
                _fx14_base_set = set(_inj_diag_norm_url_list(_fx14_base_urls)) if _fx14_base_urls else set()
                _fx14_delta = sorted(list(set(_fx14_inj) - _fx14_base_set)) if _fx14_inj else []

                if _fx14_delta:
                    _fx14_q = str((prev_response or {}).get("question") or (previous_data or {}).get("question") or "").strip()
                    _fx14_prev_snap = baseline_sources_cache
                    _fx14_fwc = fetch_web_context(
                        _fx14_q or "evolution_injection_fetch_pre_rebuild",
                        num_sources=int(min(12, max(1, len(_fx14_base_set) + len(_fx14_inj)))),
                        fallback_mode=True,
                        fallback_urls=list(_inj_diag_norm_url_list(_fx14_base_urls)),
                        existing_snapshots=_fx14_prev_snap,
                        extra_urls=list(_fx14_inj),
                        diag_run_id=str((_fx14_wc or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                        diag_extra_urls_ui_raw=(_fx14_wc or {}).get("diag_extra_urls_ui_raw"),
                        identity_only=False,
                        force_scrape_extra_urls=True,
                        force_admit_extra_urls=True,
                    ) or {}

                    try:
                        if isinstance(_fx14_wc, dict):
                            _fx14_wc["evolution_calls_fetch_web_context"] = True
                    except Exception:
                        pass

                    _fx14_sm = _fx14_fwc.get("scraped_meta")
                    _fx14_bsc_new = None
                    try:
                        _fn_bsc = globals().get("_fix24_baseline_sources_cache_from_scraped_meta")
                        if callable(_fn_bsc) and isinstance(_fx14_sm, dict):
                            _fx14_bsc_new = _fn_bsc(_fx14_sm)
                    except Exception:
                        pass
                        _fx14_bsc_new = None

                    if isinstance(_fx14_bsc_new, list) and _fx14_bsc_new:
                        # Merge: keep original order for existing snapshots, append new unique ones
                        _merged = list(baseline_sources_cache or [])
                        _seen = set(_inj_diag_norm_url_list(_fx14_base_urls))
                        _added = []
                        for _row in _fx14_bsc_new:
                            if not isinstance(_row, dict):
                                continue
                            _u = _row.get("source_url") or ""
                            _nu = (_inj_diag_norm_url(_u) if isinstance(_u, str) else "")
                            if _nu and _nu not in _seen:
                                _merged.append(_row)
                                _seen.add(_nu)
                                _added.append(_nu)
                        if _added:
                            baseline_sources_cache = _merged

                        # Debug
                        try:
                            output.setdefault("debug", {})
                            if isinstance(output.get("debug"), dict):
                                output["debug"].setdefault("fix41afc14", {})
                                if isinstance(output["debug"].get("fix41afc14"), dict):
                                    output["debug"]["fix41afc14"].update({
                                        "inj_delta_count": int(len(_fx14_delta)),
                                        "inj_delta": list(_fx14_delta),
                                        "merged_added_count": int(len(_added)),
                                        "merged_added": list(_added),
                                        "baseline_sources_cache_count_after_merge": int(len(baseline_sources_cache or [])),
                                    })
                        except Exception:
                            pass
            except Exception:
                pass
            # =====================================================================
            fn_rebuild = globals().get("rebuild_metrics_from_snapshots_schema_only") or globals().get("rebuild_metrics_from_snapshots")
            if callable(fn_rebuild):
                current_metrics = fn_rebuild(prev_response, baseline_sources_cache, web_context=web_context)
                # PATCH FIX36 (ADD): provenance for rebuild fallback
                try:
                    if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                        if output["debug"]["fix35"].get("current_metrics_origin") in (None, "", "unknown"):
                            output["debug"]["fix35"]["current_metrics_origin"] = "schema_only_rebuild"
                except Exception:
                    pass
                # =====================================================================
                # PATCH FIX41AFC18 (ADDITIVE): schema-preserve guard on rebuild when injected URLs are present
                # Intent:
                #   - Keep drift-0 for existing metrics even when an injected URL forces rebuild
                #   - Only allow a rebuilt metric to replace the previous metric if it carries
                #     evidence anchors and passes basic schema/unit sanity checks
                #   - Never changes fastpath logic; only post-rebuild metric selection safety
                # =====================================================================
                try:
                    _fix41afc18_prev = prev_response.get("primary_metrics_canonical") if isinstance(prev_response, dict) else None
                    _fix41afc18_schema = prev_response.get("metric_schema_frozen") if isinstance(prev_response, dict) else None

                    # Recover injected-delta set (normalized) from earlier patches, if available
                    _fix41afc18_inj_delta = []
                    try:
                        if isinstance(output.get("debug"), dict):
                            _d15 = output["debug"].get("fix41afc15") or output["debug"].get("fix41afc16") or output["debug"].get("fix41afc14") or {}
                            if isinstance(_d15, dict):
                                _fix41afc18_inj_delta = list(_d15.get("inj_delta") or [])
                    except Exception:
                        pass
                        _fix41afc18_inj_delta = []
                    _fix41afc18_inj_set = set()
                    try:
                        _norm_fn = globals().get("_inj_diag_norm_url_list")
                        if callable(_norm_fn) and _fix41afc18_inj_delta:
                            _fix41afc18_inj_set = set(_norm_fn(_fix41afc18_inj_delta))
                        else:
                            _fix41afc18_inj_set = set([str(u).strip() for u in (_fix41afc18_inj_delta or []) if str(u).strip()])
                    except Exception:
                        pass
                        _fix41afc18_inj_set = set([str(u).strip() for u in (_fix41afc18_inj_delta or []) if str(u).strip()])

                    def _fix41afc18_has_evidence(m: dict) -> bool:
                        if not isinstance(m, dict):
                            return False
                        ev = m.get("evidence")
                        if isinstance(ev, list) and len(ev) > 0:
                            return True
                        # fallback: anchor_hash present
                        ah = m.get("anchor_hash") or m.get("anchorHash") or m.get("anchor")
                        return bool(ah and str(ah) not in ("None", "none", ""))

                    def _fix41afc18_unit_ok(prev_m: dict, cur_m: dict) -> bool:
                        try:
                            # Prefer schema unit_family/unit_tag if available
                            ck = str(cur_m.get("canonical_key") or "")
                            schema_row = _fix41afc18_schema.get(ck) if isinstance(_fix41afc18_schema, dict) else None
                            if isinstance(schema_row, dict):
                                exp_fam = str(schema_row.get("unit_family") or "")
                                exp_tag = str(schema_row.get("unit_tag") or schema_row.get("unit") or "")
                                cur_fam = str(cur_m.get("unit_family") or "")
                                cur_tag = str(cur_m.get("unit_tag") or cur_m.get("unit") or "")
                                # If schema expects a family/tag, require match when present
                                if exp_fam and cur_fam and exp_fam != cur_fam:
                                    return False
                                if exp_tag and cur_tag and exp_tag != cur_tag:
                                    return False
                            # Fallback: if both prev and cur have unit_family, require equality
                            pf = str(prev_m.get("unit_family") or "")
                            cf = str(cur_m.get("unit_family") or "")
                            if pf and cf and pf != cf:
                                return False
                            return True
                        except Exception:
                            return True

                    def _fix41afc18_from_injected_source(cur_m: dict) -> bool:
                        try:
                            su = str(cur_m.get("source_url") or cur_m.get("source") or "")
                            if not su:
                                return False
                            # normalize by simple strip only (avoid heavy deps)
                            su = su.strip()
                            # if we have inj set, treat any exact/startswith match as injected
                            for iu in _fix41afc18_inj_set:
                                if not iu:
                                    continue
                                if su == iu or su.startswith(iu) or iu.startswith(su):
                                    return True
                            return False
                        except Exception:
                            return False

                    def _fix41afc18_value(prev_m: dict):
                        for k in ("value_norm", "value"):
                            v = prev_m.get(k) if isinstance(prev_m, dict) else None
                            if isinstance(v, (int, float)):
                                return float(v)
                        return None

                    _fix41afc18_replaced = 0
                    _fix41afc18_preserved = 0
                    _fix41afc18_added = 0
                    _fix41afc18_notes = []

                    # Only apply guard when injected delta is present (no-change case remains locked)
                    if _fix41afc18_inj_set and isinstance(_fix41afc18_prev, dict) and isinstance(current_metrics, dict):
                        for _ck, _prev_m in _fix41afc18_prev.items():
                            if not isinstance(_ck, str) or not _ck:
                                continue
                            _cur_m = current_metrics.get(_ck)
                            if not isinstance(_cur_m, dict):
                                # If rebuild dropped a metric, preserve previous
                                current_metrics[_ck] = _prev_m
                                _fix41afc18_preserved += 1
                                _fix41afc18_notes.append({"canonical_key": _ck, "action": "preserve_prev_metric_missing_in_rebuild"})
                                continue

                            # If rebuild metric lacks evidence, preserve previous (schema-driven drift lock)
                            if not _fix41afc18_has_evidence(_cur_m):
                                current_metrics[_ck] = _prev_m
                                _fix41afc18_preserved += 1
                                _fix41afc18_notes.append({"canonical_key": _ck, "action": "preserve_prev_no_evidence"})
                                continue

                            # If unit sanity fails, preserve previous
                            if not _fix41afc18_unit_ok(_prev_m, _cur_m):
                                current_metrics[_ck] = _prev_m
                                _fix41afc18_preserved += 1
                                _fix41afc18_notes.append({"canonical_key": _ck, "action": "preserve_prev_unit_mismatch"})
                                continue

                            # If the replacement comes only from injected source and is wildly different, preserve prev
                            try:
                                pv = _fix41afc18_value(_prev_m)
                                cv = _fix41afc18_value(_cur_m)
                                if pv is not None and cv is not None and pv != 0:
                                    rel = abs(cv - pv) / max(abs(pv), 1e-9)
                                    if _fix41afc18_from_injected_source(_cur_m) and rel >= 0.50:
                                        current_metrics[_ck] = _prev_m
                                        _fix41afc18_preserved += 1
                                        _fix41afc18_notes.append({"canonical_key": _ck, "action": "preserve_prev_suspicious_injected_delta", "prev": pv, "cur": cv, "rel": rel})
                                        continue
                            except Exception:
                                pass

                            # Otherwise accept rebuilt metric (explicit replace)
                            _fix41afc18_replaced += 1

                        # If rebuild produced new metrics not in schema, keep them but track (non-breaking)
                        for _ck2, _cur_m2 in list(current_metrics.items()):
                            if not isinstance(_ck2, str) or not _ck2:
                                continue
                            if _ck2 not in _fix41afc18_prev:
                                _fix41afc18_added += 1

                    # Emit debug for traceability
                    try:
                        if isinstance(output.get("debug"), dict):
                            output["debug"].setdefault("fix41afc18", {})
                            if isinstance(output["debug"].get("fix41afc18"), dict):
                                output["debug"]["fix41afc18"].update({
                                    "inj_delta_present": bool(_fix41afc18_inj_set),
                                    "inj_delta_count": int(len(_fix41afc18_inj_set)),
                                    "rebuild_metrics_replaced_count": int(_fix41afc18_replaced),
                                    "rebuild_metrics_preserved_count": int(_fix41afc18_preserved),
                                    "rebuild_metrics_added_count": int(_fix41afc18_added),
                                    "notes_sample": _fix41afc18_notes[:10],
                                })
                    except Exception:
                        pass
                except Exception:
                    pass
                # =====================================================================
        except Exception:
            pass
            current_metrics = {}
    if not isinstance(current_metrics, dict) or not current_metrics:
        # FIX2D65A: do not fail hard here; emit a warning and continue so Evolution can still output JSON.
        try:
            output.setdefault("warnings", [])
            output["warnings"].append({
                "code": "FIX2D65_REBUILD_EMPTY_WITH_SNAPSHOTS",
                "message": "Valid snapshots exist, but metric rebuild returned empty. Continuing (no refetch, no heuristic).",
                "sources_checked": int(len(baseline_sources_cache or [])),
                "sources_fetched": int(len(baseline_sources_cache or [])),
            })
        except Exception:
            pass
        output["status"] = output.get("status") or "ok_with_warnings"
        output["message"] = output.get("message") or "Valid snapshots exist, but metric rebuild returned empty. Continuing (no refetch, no heuristic)."
        # Keep current_metrics as empty dict; downstream code should handle it.
        current_metrics = {}
        # PATCH FIX2D20 (ADD): trace year-like commits in primary_metrics_canonical
        _fix2d20_trace_year_like_commits(output, stage=str((output or {}).get('results',{}).get('debug',{}).get('stage') or 'evolution'), callsite='compute_source_anchored_diff_return')
    # =====================================================================
    # PATCH FIX2D2_ANCHOR_FILL_FOR_CURRENT (ADDITIVE)
    # Purpose:
    #   When schema_frozen is missing/misaligned (e.g., wrong namespace) the render/diff
    #   rebuild can populate an unrelated keyspace, causing the dashboard "Current" column
    #   to be empty for the intended canonical keys and diff rows to be all not_found.
    #
    # Fix (render/diff layer only):
    #   - Build a deterministic anchor_hash -> candidate map from the *current* snapshot pool.
    #   - For each canonical_key that exists in prev_response.primary_metrics_canonical, if the
    #     current_metrics dict is missing that key (or has no value_norm), fill it from the
    #     anchored candidate in the current pool (when available).
    #
    # Non-negotiables:
    #   - No changes to hashing, injection lifecycle, snapshot attach, or extraction.
    #   - Anchor-hash is treated as the authority for key/value materialization.
    # =====================================================================
    try:
        def _fix2d2_safe_dict(x):
            return x if isinstance(x, dict) else {}

        def _fix2d2_unwrap_primary_metrics_canonical(resp: dict) -> dict:
            if not isinstance(resp, dict):
                return {}
            for path in (("primary_metrics_canonical",), ("primary_response", "primary_metrics_canonical"), ("results", "primary_metrics_canonical"), ("results", "primary_response", "primary_metrics_canonical")):
                cur = resp
                ok = True
                for k in path:
                    if isinstance(cur, dict) and k in cur:
                        cur = cur.get(k)
                    else:
                        ok = False
                        break
                if ok and isinstance(cur, dict) and cur:
                    return cur
            return {}

        def _fix2d2_unwrap_metric_anchors(resp: dict) -> dict:
            if not isinstance(resp, dict):
                return {}
            for path in (("metric_anchors",), ("primary_response", "metric_anchors"), ("results", "metric_anchors"), ("results", "primary_response", "metric_anchors")):
                cur = resp
                ok = True
                for k in path:
                    if isinstance(cur, dict) and k in cur:
                        cur = cur.get(k)
                    else:
                        ok = False
                        break
                if ok and isinstance(cur, dict) and cur:
                    return cur
            return {}

        _fix2d2_prev_can = _fix2d2_unwrap_primary_metrics_canonical(prev_response)
        _fix2d2_anchors = _fix2d2_unwrap_metric_anchors(prev_response)

        # Resolve the best available "current" pool for candidate lookup
        _fix2d2_pool = (
            locals().get("baseline_sources_cache_current")
            or (output.get("baseline_sources_cache_current") if isinstance(output, dict) else None)
            or (output.get("results", {}).get("baseline_sources_cache_current") if isinstance(output, dict) else None)
            or locals().get("baseline_sources_cache")
            or locals().get("baseline_sources_cache_prefetched")
            or None
        )

        _fix2d2_anchor_to_cand = {}
        _fix2d2_scanned = 0
        if isinstance(_fix2d2_pool, list) and _fix2d2_pool:
            for _src in _fix2d2_pool:
                _srcd = _fix2d2_safe_dict(_src)
                for _cand in (_srcd.get("extracted_numbers") or []):
                    _fix2d2_scanned += 1
                    if not isinstance(_cand, dict):
                        continue
                    _ah = _cand.get("anchor_hash") or _cand.get("anchor") or _cand.get("anchorHash")
                    if not _ah:
                        continue
                    # Deterministic first-wins (pool order is deterministic)
                    if str(_ah) not in _fix2d2_anchor_to_cand:
                        _fix2d2_anchor_to_cand[str(_ah)] = _cand

        _fix2d2_filled = 0
        _fix2d2_missing_anchor = 0
        _fix2d2_missing_cand = 0

        if isinstance(_fix2d2_prev_can, dict) and _fix2d2_prev_can:
            if not isinstance(current_metrics, dict):
                current_metrics = {}
            for _ck in list(_fix2d2_prev_can.keys()):
                # Only fill when current is missing / empty
                _cur_m = current_metrics.get(_ck) if isinstance(current_metrics, dict) else None
                _cur_vn = _cur_m.get("value_norm") if isinstance(_cur_m, dict) else None
                if (_cur_m is not None) and (_cur_vn is not None):
                    continue

                _a = _fix2d2_safe_dict(_fix2d2_anchors.get(_ck))
                _ah = _a.get("anchor_hash") or _a.get("anchor") or _a.get("anchorHash")
                if not _ah:
                    _fix2d2_missing_anchor += 1
                    continue
                _cand = _fix2d2_anchor_to_cand.get(str(_ah))
                if not isinstance(_cand, dict):
                    _fix2d2_missing_cand += 1
                    continue

                # Materialize a minimal metric dict for downstream diff/display
                _out = dict(_fix2d2_prev_can.get(_ck) or {})
                try:
                    _out.update({
                        "canonical_key": _ck,
                        "anchor_hash": str(_ah),
                        "anchor_used": True,
                        "source_url": _cand.get("source_url") or _a.get("source_url") or "",
                        "raw": _cand.get("raw"),
                        "value": _cand.get("value"),
                        "unit": _cand.get("unit"),
                        "unit_tag": _cand.get("unit_tag") or _cand.get("unit") or "",
                        "unit_family": _cand.get("unit_family") or "",
                        "base_unit": _cand.get("base_unit") or _cand.get("unit") or "",
                        "multiplier_to_base": _cand.get("multiplier_to_base"),
                        "value_norm": _cand.get("value_norm"),
                        "context_snippet": _cand.get("context_snippet") or _cand.get("context") or "",
                        "candidate_id": _cand.get("candidate_id") or _a.get("candidate_id"),
                    })
                except Exception:
                    pass

                current_metrics[_ck] = _out
                _fix2d2_filled += 1

        # Emit debug (non-breaking)
        try:
            if isinstance(output, dict):
                output.setdefault("debug", {})
                if isinstance(output.get("debug"), dict):
                    output["debug"].setdefault("fix2d2", {})
                    if isinstance(output["debug"].get("fix2d2"), dict):
                        output["debug"]["fix2d2"].update({
                            "anchor_fill_attempted": True,
                            "prev_can_count": int(len(_fix2d2_prev_can)) if isinstance(_fix2d2_prev_can, dict) else 0,
                            "anchor_map_candidates_scanned": int(_fix2d2_scanned),
                            "anchor_map_size": int(len(_fix2d2_anchor_to_cand)),
                            "filled_count": int(_fix2d2_filled),
                            "missing_anchor_count": int(_fix2d2_missing_anchor),
                            "missing_candidate_count": int(_fix2d2_missing_cand),
                        })
        except Exception:
            pass
    except Exception:
        pass
    # =====================================================================

# Diff using existing diff helper if present
    metric_changes = []
    try:
        fn_diff = globals().get("diff_metrics_by_name")
        if callable(fn_diff):
            # ============================================================
            # PATCH FIX38 (ADDITIVE): ensure schema is wired into diff layer + emit lookup provenance
            # - Some runs showed schema_unit_family=None in diff rows, preventing unit-required mismatch logic.
            # - We attach metric_schema_frozen to prev_response if missing, and post-process diff rows
            #   to populate schema_unit_family + schema_lookup_source.
            # ============================================================
            _fix38_schema = None
            _fix38_schema_src = ""
            try:
                # Prefer schema already on prev_response
                if isinstance(prev_response, dict) and isinstance(prev_response.get("metric_schema_frozen"), dict) and prev_response.get("metric_schema_frozen"):
                    _fix38_schema = prev_response.get("metric_schema_frozen")
                    _fix38_schema_src = "prev_response.metric_schema_frozen"
                # Else try common containers on previous_data
                elif isinstance(previous_data, dict):
                    if isinstance(previous_data.get("metric_schema_frozen"), dict) and previous_data.get("metric_schema_frozen"):
                        _fix38_schema = previous_data.get("metric_schema_frozen")
                        _fix38_schema_src = "previous_data.metric_schema_frozen"
                    elif isinstance(previous_data.get("primary_response"), dict) and isinstance(previous_data["primary_response"].get("metric_schema_frozen"), dict) and previous_data["primary_response"].get("metric_schema_frozen"):
                        _fix38_schema = previous_data["primary_response"].get("metric_schema_frozen")
                        _fix38_schema_src = "previous_data.primary_response.metric_schema_frozen"
                # Attach if missing
                if isinstance(_fix38_schema, dict) and _fix38_schema and isinstance(prev_response, dict):
                    if not (isinstance(prev_response.get("metric_schema_frozen"), dict) and prev_response.get("metric_schema_frozen")):
                        prev_response["metric_schema_frozen"] = _fix38_schema
            except Exception:
                pass

            # Record schema wiring debug
            try:
                if isinstance(output.get("debug"), dict):
                    output["debug"].setdefault("fix38", {})
                    output["debug"]["fix38"]["schema_attached"] = bool(isinstance(_fix38_schema, dict) and _fix38_schema)
                    output["debug"]["fix38"]["schema_source"] = _fix38_schema_src
            except Exception:
                pass

            cur_resp_for_diff = {"primary_metrics_canonical": current_metrics}
            # ============================================================
            # ============================================================
            # PATCH V34D (ADDITIVE): provide metric_anchors for current side so anchor_hash join can work
            # - compute_source_anchored_diff previously passed only primary_metrics_canonical to diff layer
            # - v34 join requires cur_response.metric_anchors[ckey].anchor_hash (secondary join)
            # - We synthesize metric_anchors from rebuilt canonical metrics when missing.
            # Safety:
            # - Deterministic, inference-free. Uses only anchor_hash already attached to rebuilt canonical metrics.
            # ============================================================
            try:
                if isinstance(cur_resp_for_diff, dict) and "metric_anchors" not in cur_resp_for_diff:
                    _v34d_cur_anchors = {}
                    if isinstance(current_metrics, dict):
                        for _ck_v34d, _m_v34d in current_metrics.items():
                            if not isinstance(_ck_v34d, str) or not _ck_v34d:
                                continue
                            if not isinstance(_m_v34d, dict):
                                continue
                            _ah = _m_v34d.get("anchor_hash") or _m_v34d.get("anchor") or _m_v34d.get("anchor_id") or None
                            if isinstance(_ah, str) and _ah:
                                _v34d_cur_anchors[_ck_v34d] = {"anchor_hash": _ah}
                    if _v34d_cur_anchors:
                        cur_resp_for_diff["metric_anchors"] = _v34d_cur_anchors
            except Exception:
                pass
            # ============================================================
            metric_changes, unchanged, increased, decreased, found = fn_diff(prev_response, cur_resp_for_diff)

            # ============================================================
            # PATCH V34F (ADDITIVE): force-run strict anchor-hash join diff and preserve diagnostics
            # WHY:
            # - Some deployments still route through a legacy diff function (or strip diagnostics),
            #   leaving Current blank/N/A even when a stable anchor match exists.
            # - We deterministically re-run the v34 anchor-join diff against the exact same
            #   (prev_response, cur_resp_for_diff) payload, then adopt its output if it yields
            #   any per-row v34 diagnostics.
            # SAFETY:
            # - Render/diff layer only. No fetch, no inference, no hashing changes.
            # ============================================================
            try:
                _v34f_fn = (
                    globals().get("diff_metrics_by_name_FIX41_V34C_UNWRAP")
                    or globals().get("diff_metrics_by_name_FIX41_V34_ANCHOR_JOIN")
                    or globals().get("diff_metrics_by_name_FIX41_V34_ANCHOR_JOIN".lower())
                )
                if callable(_v34f_fn):
                    _mc_v34f, _u_v34f, _i_v34f, _d_v34f, _f_v34f = _v34f_fn(prev_response, cur_resp_for_diff)
                    # Adopt only if it looks like the v34 pipeline (has diag.diff_join_trace_v1 anywhere)
                    _has_v34_diag = False
                    if isinstance(_mc_v34f, list):
                        for _r_v34f in _mc_v34f:
                            if isinstance(_r_v34f, dict) and isinstance(_r_v34f.get("diag"), dict) and _r_v34f["diag"].get("diff_join_trace_v1"):
                                _has_v34_diag = True
                                break
                    if _has_v34_diag:
                        metric_changes, unchanged, increased, decreased, found = _mc_v34f, _u_v34f, _i_v34f, _d_v34f, _f_v34f
                        # Surface top-level join summary debug (emitted on cur_resp_for_diff by v34)
                        try:
                            _v34f_dbg = None
                            if isinstance(cur_resp_for_diff, dict) and isinstance(cur_resp_for_diff.get("debug"), dict):
                                _v34f_dbg = cur_resp_for_diff["debug"].get("diff_join_anchor_v34")
                            if isinstance(_v34f_dbg, dict) and isinstance(output.get("debug"), dict):
                                output["debug"]["diff_join_anchor_v34"] = _v34f_dbg
                                output["debug"]["diff_join_anchor_v34"]["v34f_forced_rerun"] = True
                        except Exception:
                            pass
            except Exception:
                pass
            # ============================================================
            # ============================================================
            # ============================================================
            # PATCH V34D (ADDITIVE): merge diff join summary debug into evolution output debug
            # - v34 join emits debug.diff_join_anchor_v34 on the cur_response object passed to diff layer
            # - cur_resp_for_diff is local; we surface it to output.debug for auditability.
            # ============================================================
            try:
                _v34d_dbg = None
                if isinstance(cur_resp_for_diff, dict) and isinstance(cur_resp_for_diff.get("debug"), dict):
                    _v34d_dbg = cur_resp_for_diff["debug"].get("diff_join_anchor_v34")
                if isinstance(_v34d_dbg, dict) and isinstance(output.get("debug"), dict):
                    output["debug"]["diff_join_anchor_v34"] = _v34d_dbg
            except Exception:
                pass
            # ============================================================

            # PATCH FIX38 (ADDITIVE): populate schema_unit_family on diff rows (if missing)
            # ============================================================
            try:
                if isinstance(metric_changes, list) and metric_changes and isinstance(_fix38_schema, dict) and _fix38_schema:
                    bad = {}
                    for row in metric_changes:
                        if not isinstance(row, dict):
                            continue
                        ckey = row.get("canonical_key") or row.get("canonical") or row.get("key") or ""
                        if not ckey:
                            continue
                        if row.get("schema_unit_family") in (None, "", "None"):
                            md = _fix38_schema.get(ckey) if isinstance(_fix38_schema.get(ckey), dict) else None
                            uf = ""
                            if isinstance(md, dict):
                                uf = (md.get("unit_family") or md.get("unit") or "").strip()
                            if uf:
                                row["schema_unit_family"] = uf
                                row["fix38_schema_lookup"] = _fix38_schema_src or "attached_schema"
                        # Track any remaining year-like current with missing unit-family for diagnosis
                        try:
                            cv = row.get("cur_value_norm")
                            cu = (row.get("cur_unit_cmp") or "").strip()
                            if isinstance(cv, (int, float)) and 1900 <= float(cv) <= 2100 and not cu:
                                bad[ckey] = {"cur_value_norm": cv, "cur_unit_cmp": cu, "schema_unit_family": row.get("schema_unit_family")}
                        except Exception:
                            pass
                    if bad:
                        output.setdefault("debug", {}).setdefault("fix38", {})["bad_year_currents_sample"] = dict(list(bad.items())[:10])
            except Exception:
                pass
        else:
            metric_changes, unchanged, increased, decreased, found = ([], 0, 0, 0, 0)
    except Exception:
        pass
        metric_changes, unchanged, increased, decreased, found = ([], 0, 0, 0, 0)

    # ============================================================
    # PATCH FIX36 (ADDITIVE): attach per-row provenance from current_metrics
    # - Adds row['fix36_origin'] when available so we can see which path produced 'Current'
    # ============================================================
    try:
        if isinstance(metric_changes, list) and isinstance(current_metrics, dict):
            for r in metric_changes:
                if not isinstance(r, dict):
                    continue
                ck = r.get("canonical_key") or r.get("canonical") or ""
                if ck and isinstance(current_metrics.get(ck), dict):
                    if current_metrics[ck].get("fix36_origin"):
                        r["fix36_origin"] = current_metrics[ck].get("fix36_origin")
    except Exception:
        pass
    # ============================================================



    # =================================================================
    # PATCH DIFF_PANEL_V2_WIRING (ADDITIVE): Build a self-contained, deterministic
    # diff table feed from canonical outputs ONLY, and safely override render rows.
    #
    # Safety:
    # - Render layer only.
    # - No fetch, no inference, no hashing changes.
    #
    # Behavior:
    # - Always emits one row per prev.primary_metrics_canonical key.
    # - If prev empty, emits a sentinel row.
    # - Join order: canonical_key -> anchor_hash -> none.
    # =================================================================

    _mc_v2 = []
    _mc_v2_summary = None
    try:
        _fn_v2 = globals().get("build_diff_metrics_panel_v2__rows_refactor47") or globals().get("build_diff_metrics_panel_v2__rows") or globals().get("build_diff_metrics_panel_v2")
        if callable(_fn_v2):
            _mc_v2, _mc_v2_summary = _fn_v2(prev_response, cur_resp_for_diff)

        # =================================================================
        # PATCH DIFF_PANEL_V2_WIRING_FIX1 (ADDITIVE): Fallback cur_response wrapper
        # Why:
        # - Some evolution packaging paths do not preserve/shape cur_resp_for_diff as a
        #   response-like dict containing primary_metrics_canonical.
        # - We build a minimal, deterministic wrapper from current_metrics (already rebuilt)
        #   so V2 always emits rows when prev metrics exist.
        # Safety:
        # - Render-only. No inference. No legacy diff edits.
        # =================================================================
        try:
            if (
                (not isinstance(_mc_v2, list) or not _mc_v2)
                and callable(_fn_v2)
                and isinstance(current_metrics, dict)
                and current_metrics
            ):
                _cur_v2 = {"primary_metrics_canonical": current_metrics}
                # -------------------------------------------------------------
                # PATCH FIX2M_OBSERVED_ROWS_WIRING (ADDITIVE)
                # Provide observed-number pools (already fetched/extracted) to
                # the V2 diff builder so it can emit "observed" rows.
                # This is render-layer only: does NOT alter extraction/hashing.
                # -------------------------------------------------------------
                try:
                    _bsc_cur = output.get("baseline_sources_cache_current")
                    if _bsc_cur is None:
                        _bsc_cur = output.get("baseline_sources_cache")
                    if isinstance(_bsc_cur, list) and _bsc_cur:
                        _cur_v2["baseline_sources_cache_current"] = _bsc_cur
                except Exception:
                    pass
                _mc_v2, _mc_v2_summary = _fn_v2(prev_response, _cur_v2)
        except Exception:
            pass
    except Exception:
        pass
        _mc_v2, _mc_v2_summary = ([], None)

    # Write V2 rows to output for UI/debug visibility
    try:
        output["metric_changes_v2"] = _mc_v2 or []
    except Exception:
        pass

    # =================================================================
    # PATCH DIFF_PANEL_V2_WIRING_FIX2 (ADDITIVE): Persist V2 artifacts under output["results"]
    # - Some serializers/callers only retain a curated subset of top-level keys.
    # - We mirror key artifacts into output["results"]["diff_panel_v2"] for auditability.
    # =================================================================
    try:
        _r = output.setdefault("results", {})
        if isinstance(_r, dict):
            _r.setdefault("diff_panel_v2", {})
            if isinstance(_r.get("diff_panel_v2"), dict):
                _r["diff_panel_v2"]["metric_changes_v2"] = _mc_v2 or []
                if isinstance(_mc_v2_summary, dict):
                    _r["diff_panel_v2"]["summary"] = _mc_v2_summary
    except Exception:
        pass

    # Surface top-level summary under output.debug for auditability
    try:
        if isinstance(_mc_v2_summary, dict):
            _dbg = output.setdefault("debug", {})
            if isinstance(_dbg, dict):
                _dbg["diff_panel_v2_summary"] = _mc_v2_summary

        # =====================================================================
        # PATCH FIX2S_MERGE_MAPPING_DIAG_INTO_V2_SUMMARY (ADDITIVE)
        # =====================================================================
        if isinstance(web_context, dict) and isinstance(_mc_v2_summary, dict):
            _md = web_context.get("_fix2s_mapping_diag")
            if isinstance(_md, dict):
                _mc_v2_summary.setdefault("mapping_rules_version", _md.get("mapping_rules_version"))
                _mc_v2_summary.setdefault("mapping_hits", _md.get("mapping_hits"))
                _mc_v2_summary.setdefault(
                    "observed_rows_canonicalized_by_mapping",
                    _md.get("observed_rows_canonicalized_by_mapping"),
                )
        # =====================================================================
        # END PATCH FIX2S_MERGE_MAPPING_DIAG_INTO_V2_SUMMARY
        # =====================================================================

    except Exception:
        pass

    # Option B: prefer v2 rows if non-empty; otherwise keep legacy output
    try:
        if isinstance(_mc_v2, list) and _mc_v2:
            metric_changes = _mc_v2
    except Exception:
        pass
    # =====================================================================
    # PATCH FIX2G_OPTION_B_PROOF_AND_SENTINEL (ADDITIVE)
    # Objective:
    # - Make Option B activation unambiguous in serialized outputs.
    # - Ensure V2 never yields an empty table when prev canonical metrics exist.
    # - Add injection/hash diagnostics (read-only) to confirm injected URLs enter hash inputs.
    # Safety: diagnostics only; no changes to hashing/fastpath/snapshots/extraction.
    # =====================================================================
    try:
        _dbg = output.setdefault("debug", {})
        if isinstance(_dbg, dict):
            _dbg["diff_panel_v2_option_b_active"] = True
            _dbg["diff_panel_v2_code_version"] = str(globals().get("CODE_VERSION") or "")
    except Exception:
        pass

    # If prev canonical metrics exist but V2 produced zero rows, emit a sentinel row
    # and still override the UI feed so the panel never goes silently empty.
    try:
        _prev_pmc = {}
        if isinstance(prev_response, dict):
            _prev_pmc = prev_response.get("primary_metrics_canonical") if isinstance(prev_response.get("primary_metrics_canonical"), dict) else {}
        if isinstance(_prev_pmc, dict) and _prev_pmc and (not isinstance(_diff_v2_rows, list) or not _diff_v2_rows):
            _sent = {
                "metric_name": "Diff Panel V2 sentinel",
                "canonical_key": "__diff_panel_v2_sentinel__",
                "previous": None,
                "current": "N/A",
                "change_type": "v2_empty_unexpected",
                "diag": {
                    "diff_join_trace_v1": {
                        "prev_ckey": None,
                        "resolved_cur_ckey": None,
                        "method": "none",
                        "prev_anchor_hash": None,
                        "cur_anchor_hash": None,
                    },
                    "diff_current_source_trace_v1": {
                        "current_source_path_used": "none",
                        "current_value_norm": None,
                        "current_unit_tag": None,
                        "inference_disabled": (not _fix2d25_inference_enabled),
                    "inference_gate_v1": {
                        "enabled": bool(_fix2d25_inference_enabled),
                        "reason": str(_fix2d2c_inference_gate_reason or ""),
                    },
                    },
                    "reason": "prev.primary_metrics_canonical non-empty but V2 emitted 0 rows; check diff_panel_v2_error and canonical inputs",
                },
            }
            _diff_v2_rows = [_sent]
            _diff_v2_summary = {"rows_total": 1, "joined_by_ckey": 0, "joined_by_anchor_hash": 0,
            "joined_by_ckey_anchor_mismatch": 0,
        "joined_by_ckey_anchor_mismatch": 0, "not_found": 1, "sentinel": True}
            output["metric_changes_v2"] = _diff_v2_rows
            _dbg = output.setdefault("debug", {})
            if isinstance(_dbg, dict):
                _dbg["diff_panel_v2_summary"] = _diff_v2_summary
            metric_changes = _diff_v2_rows
    except Exception:
        pass

    # Injection/hash diagnostics (read-only): mirror key facts from inj_trace_v1
    try:
        _dbg = output.setdefault("debug", {})
        _inj = _dbg.get("inj_trace_v1") if isinstance(_dbg, dict) else None
        if isinstance(_dbg, dict) and isinstance(_inj, dict):
            admitted = _inj.get("admitted_norm") if isinstance(_inj.get("admitted_norm"), list) else []
            hash_in = _inj.get("hash_inputs_norm") if isinstance(_inj.get("hash_inputs_norm"), list) else []
            # Build a normalized URL set from current baseline cache (if present)
            cur_cache = output.get("baseline_sources_cache_current") if isinstance(output.get("baseline_sources_cache_current"), list) else []
            cur_urls = set()
            for sr in cur_cache:
                if isinstance(sr, dict):
                    u = (sr.get("url") or sr.get("source_url") or sr.get("source") or "").strip()
                    if u:
                        cur_urls.add(u)
            _dbg["inj_hash_diag_v1"] = {
                "admitted_norm_count": len(admitted),
                "hash_inputs_norm_count": len(hash_in),
                "admitted_norm_sample": admitted[:5],
                "hash_inputs_norm_sample": hash_in[:5],
                "admitted_present_in_cache_current": any(u in cur_urls for u in admitted),
                "hash_inputs_present_in_cache_current": any(u in cur_urls for u in hash_in),
            }
    except Exception:
        pass
    # =====================================================================
    # END PATCH FIX2G_OPTION_B_PROOF_AND_SENTINEL
    # =====================================================================



    output["metric_changes"] = metric_changes or []

    # =====================================================================
    # PATCH PH2B_S3 (ADDITIVE): Attach selector breadcrumb + current fields per diff row
    # - Makes Phase 2B runbook checks (#3/#7/#9) a fast scan.
    # - Does not alter selection; only exposes chosen current metadata.
    # =====================================================================
    try:
        if isinstance(output.get("metric_changes"), list) and isinstance(current_metrics, dict):
            # local normalizer for safe comparisons in dashboards
            def _ph2b_norm_url(_u: str) -> str:
                try:
                    _u = (_u or "").strip()
                    if not _u:
                        return ""
                    _u = _u.replace("http://", "https://")
                    # drop trailing slash
                    while _u.endswith("/") and len(_u) > 8:
                        _u = _u[:-1]
                    # remove www.
                    _u = _u.replace("://www.", "://")
                    return _u
                except Exception:
                    return (_u or "").strip()

            for _row in output["metric_changes"]:
                if not isinstance(_row, dict):
                    continue
                _ck = _row.get("canonical_key") or _row.get("canonical_id") or ""
                if not _ck:
                    continue
                _cur = current_metrics.get(_ck)
                if not isinstance(_cur, dict):
                    continue

                if "selector_used" not in _row:
                    _row["selector_used"] = _cur.get("selector_used") or ""
                if not _row.get("cur_source_url"):
                    _row["cur_source_url"] = _cur.get("source_url") or ""
                if "cur_source_url_norm" not in _row:
                    _row["cur_source_url_norm"] = _ph2b_norm_url(_row.get("cur_source_url") or "")
                if "cur_value_norm" not in _row and _cur.get("value_norm") is not None:
                    _row["cur_value_norm"] = _cur.get("value_norm")
                if "cur_unit_cmp" not in _row:
                    _row["cur_unit_cmp"] = _cur.get("unit") or _cur.get("unit_tag") or ""
                if "anchor_used" not in _row and isinstance(_cur.get("anchor_used"), bool):
                    _row["anchor_used"] = _cur.get("anchor_used")
    except Exception:
        pass
    # =====================================================================

    output["summary"]["total_metrics"] = len(output["metric_changes"])
    output["summary"]["metrics_found"] = int(found or 0)
    output["summary"]["metrics_increased"] = int(increased or 0)
    output["summary"]["metrics_decreased"] = int(decreased or 0)
    output["summary"]["metrics_unchanged"] = int(unchanged or 0)

    total = max(1, len(output["metric_changes"]))
    output["stability_score"] = (output["summary"]["metrics_unchanged"] / total) * 100.0

    # ============================================================
    # REFACTOR11 (ADD): Recompute evolution summary counters from final rows
    #
    # Motivation:
    # - Legacy counters (increased/decreased/unchanged) can be stale when the
    #   diff row list is post-processed or swapped to the canonical-first join.
    # - The UI and JSON should reflect what is actually in output["metric_changes"].
    # ============================================================
    try:
        _rf11_counts = {"increased": 0, "decreased": 0, "unchanged": 0, "added": 0, "removed": 0}
        for _r in (output.get("metric_changes") or []):
            if not isinstance(_r, dict):
                continue
            _ct = str(_r.get("change_type") or _r.get("baseline_change_type") or "").strip().lower()
            if _ct in _rf11_counts:
                _rf11_counts[_ct] += 1

        if not isinstance(output.get("summary"), dict):
            output["summary"] = {}
        _rf11_total = int(len(output.get("metric_changes") or []))
        output["summary"]["total_metrics"] = _rf11_total
        output["summary"]["metrics_found"] = _rf11_total
        output["summary"]["metrics_increased"] = int(_rf11_counts["increased"])
        output["summary"]["metrics_decreased"] = int(_rf11_counts["decreased"])
        output["summary"]["metrics_unchanged"] = int(_rf11_counts["unchanged"])
        # Extra (non-breaking) counters for more faithful reporting
        output["summary"]["metrics_added"] = int(_rf11_counts["added"])
        output["summary"]["metrics_removed"] = int(_rf11_counts["removed"])

        _rf11_den = max(1, _rf11_total)
        output["stability_score"] = (float(output["summary"]["metrics_unchanged"]) / float(_rf11_den)) * 100.0

        if not isinstance(output.get("debug"), dict):
            output["debug"] = {}

            try:
                output["debug"].setdefault("runtime_identity_v1", _yureeka_runtime_identity_v1())
            except Exception:
                pass
        output["debug"]["refactor11_summary_recompute_v1"] = {
            "counts": dict(_rf11_counts),
            "total_rows": int(_rf11_total),
            "stability_den": int(_rf11_den),
        }
    except Exception:
        pass


    output["source_results"] = baseline_sources_cache[:50]
    output["sources_checked"] = len(baseline_sources_cache)
    output["sources_fetched"] = len(baseline_sources_cache)

    try:
        total_nums = 0
        for sr in baseline_sources_cache or []:
            if isinstance(sr, dict) and isinstance(sr.get("extracted_numbers"), list):
                total_nums += len(sr.get("extracted_numbers") or [])
        output["numbers_extracted_total"] = int(total_nums)
    except Exception:
        pass

    output["message"] = "Source-anchored evolution completed (snapshot-gated, analysis-aligned)."
    output["interpretation"] = "Evolution used cached source snapshots only; no brute-force candidate harvesting."


    # =====================================================================
    # REFACTOR55: Consolidate Metric Changes outputs (single source of truth)
    # - Keep output["metric_changes"] as the canonical feed.
    # - Mirror the same list into output["metric_changes_v2"] for backward/UI compatibility.
    # - Drop metric_changes_legacy (no longer maintained).
    # =====================================================================
    try:
        _final_rows = output.get("metric_changes")
        if not isinstance(_final_rows, list) or not _final_rows:
            _final_rows = output.get("metric_changes_v2") or []
        if not isinstance(_final_rows, list):
            _final_rows = []
        output["metric_changes"] = _final_rows
        output["metric_changes_v2"] = _final_rows
        try:
            output.pop("metric_changes_legacy", None)
        except Exception:
            pass
    except Exception:
        pass

# PATCH FIX2D20 (ADD): trace year-like commits in primary_metrics_canonical

    _fix2d20_trace_year_like_commits(output, stage=str((output or {}).get('results',{}).get('debug',{}).get('stage') or 'evolution'), callsite='compute_source_anchored_diff_return')

    return output# =====================================================================
# PATCH DIFF_PANEL_V2 (ADDITIVE): Deterministic Diff Metrics Panel V2 table feed
#
# Goal:
# - Rebuild the pipeline that feeds the Diff Metrics table WITHOUT touching legacy diff internals.
# - Deterministic, auditable, inference-free.
# - Emits one row per prev.primary_metrics_canonical key (or a single sentinel row if none).
#
# Inputs:
# - prev_response, cur_response: each may be a raw response, or nested under {primary_response:...},
#   or under {results:...}. We unwrap deterministically.
#
# Join (strict):
#   1) canonical_key exact match
#   2) anchor_hash match (deterministic lexicographic min if multiple)
#   3) otherwise not_found
#
# Current value sourcing (strict):
#   - ONLY from cur.primary_metrics_canonical[resolved_cur_ckey]
#   - Never from any numeric inference / free-form pools
#
# Output:
# - rows (list[dict]) suitable for UI table
# - summary (dict) written into debug.diff_panel_v2_summary
# =====================================================================

def build_diff_metrics_panel_v2(prev_response: dict, cur_response: dict):
    """Return (rows, summary) for Diff Metrics Panel V2."""

    # =================================================================
    # PATCH FIX2D1_B (ADDITIVE): prevent UnboundLocalError on 'summary'
    # Some exception paths reference `summary` before it is assigned.
    # Initialize a safe default up-front so errors degrade gracefully
    # into "not_found" rows rather than killing the diff panel.
    # =================================================================
    rows = []
    summary = {"rows_total": 0, "joined_by_ckey": 0, "joined_by_anchor_hash": 0, "not_found": 0, "baseline_comparable": 0, "baseline_increased": 0, "baseline_decreased": 0, "baseline_unchanged": 0, "baseline_added": 0, "baseline_not_found": 0}
    # =================================================================


    def _unwrap_primary_metrics_canonical(resp: dict):
        if not isinstance(resp, dict):
            return {}
        # common containers
        for k in ("primary_metrics_canonical",):
            if isinstance(resp.get(k), dict) and resp.get(k):
                return resp.get(k) or {}
        pr = resp.get("primary_response")
        if isinstance(pr, dict):
            if isinstance(pr.get("primary_metrics_canonical"), dict) and pr.get("primary_metrics_canonical"):
                return pr.get("primary_metrics_canonical") or {}
            res = pr.get("results")
            if isinstance(res, dict) and isinstance(res.get("primary_metrics_canonical"), dict) and res.get("primary_metrics_canonical"):
                return res.get("primary_metrics_canonical") or {}
        res = resp.get("results")
        if isinstance(res, dict) and isinstance(res.get("primary_metrics_canonical"), dict) and res.get("primary_metrics_canonical"):
            return res.get("primary_metrics_canonical") or {}
        return {}

    def _unwrap_metric_anchors(resp: dict):
        if not isinstance(resp, dict):
            return {}
        for k in ("metric_anchors",):
            if isinstance(resp.get(k), dict) and resp.get(k):
                return resp.get(k) or {}
        pr = resp.get("primary_response")
        if isinstance(pr, dict) and isinstance(pr.get("metric_anchors"), dict) and pr.get("metric_anchors"):
            return pr.get("metric_anchors") or {}
        res = resp.get("results")
        if isinstance(res, dict) and isinstance(res.get("metric_anchors"), dict) and res.get("metric_anchors"):
            return res.get("metric_anchors") or {}
        return {}

    def _get_anchor_hash_for_ckey(ckey: str, m: dict, anchors: dict):
        # 1) explicit on metric
        try:
            if isinstance(m, dict):
                ah = m.get("anchor_hash") or m.get("anchor") or m.get("anchorHash")
                if ah:
                    return str(ah)
        except Exception:
            pass
        # 2) anchors map
        try:
            a = anchors.get(ckey) if isinstance(anchors, dict) else None
            if isinstance(a, dict):
                ah = a.get("anchor_hash") or a.get("anchor") or a.get("anchorHash")
                if ah:
                    return str(ah)
        except Exception:
            return None

    def _raw_display_value(m: dict):
        if not isinstance(m, dict):
            return None
        if m.get("raw") is not None:
            return m.get("raw")
        if m.get("value") is not None:
            return m.get("value")
        return None

    def _canon_value_norm(m: dict):
        if not isinstance(m, dict):
            return None
        if m.get("value_norm") is None:
            return None
        try:
            return float(m.get("value_norm"))
        except Exception:
            return None

    def _canon_unit_tag(m: dict):
        if not isinstance(m, dict):
            return ""
        return str(m.get("base_unit") or m.get("unit") or m.get("unit_tag") or "").strip()

    prev_metrics = _unwrap_primary_metrics_canonical(prev_response)
    cur_metrics = _unwrap_primary_metrics_canonical(cur_response)

    prev_anchors = _unwrap_metric_anchors(prev_response)
    cur_anchors = _unwrap_metric_anchors(cur_response)

    # =====================================================================
    # PATCH FIX2D2C_INFERENCE_GATE_V1 (ADDITIVE)
    # Ensure guarded inference gate is always defined in this active builder.
    # Default: enabled (guards already enforced by FIX2D24/FIX2D26/FIX2D27).
    # Kill-switch: set EVO_DISABLE_DIFF_INFERENCE=1 to disable.
    # =====================================================================
    _fix2d25_inference_enabled = True
    _fix2d2c_inference_gate_reason = "enabled_by_default"
    try:
        import os
        _e = str(os.environ.get("EVO_DISABLE_DIFF_INFERENCE", "")).strip().lower()
        if _e in ("1", "true", "yes", "y", "on"):
            _fix2d25_inference_enabled = False
            _fix2d2c_inference_gate_reason = "disabled_by_env"
    except Exception:
        pass
    # END PATCH FIX2D2C_INFERENCE_GATE_V1



    # Build reverse index: anchor_hash -> [ckeys]
    cur_by_anchor = {}
    try:
        if isinstance(cur_metrics, dict):
            for ck, m in cur_metrics.items():
                if not isinstance(ck, str) or not ck:
                    continue
                if not isinstance(m, dict):
                    continue
                ah = _get_anchor_hash_for_ckey(ck, m, cur_anchors)
                if ah:
                    cur_by_anchor.setdefault(str(ah), []).append(ck)
    except Exception:
        pass
        cur_by_anchor = {}

    # Deterministic resolution: lexicographic min on canonical_key
    for ah, cks in list(cur_by_anchor.items()):
        try:
            cur_by_anchor[ah] = sorted([c for c in cks if isinstance(c, str)])
        except Exception:
            pass

    rows = []
    joined_by_ckey = 0
    joined_by_anchor = 0
    not_found = 0
    sample_anchor_joins = []

    matched_cur_ckeys = set()

    # Capture injected URL set for diagnostics/tagging only (no impact on hashing/extraction)
    inj_set = set()
    # =====================================================================
    # PATCH FIX2W_INJECTED_SET_BUILD_V2 (ADDITIVE): Robust injected URL set union
    # =====================================================================
    inj_set_sources = {"inj_trace_v1": [], "diag_injected_urls": [], "diag_injected_urls_ui": []}
    try:
        _dbg0 = (cur_response or {}).get("debug", {}) if isinstance((cur_response or {}).get("debug", {}), dict) else {}
        _inj0 = _dbg0.get("inj_trace_v1", {}) if isinstance(_dbg0.get("inj_trace_v1", {}), dict) else {}
        _ad0 = _inj0.get("admitted_norm") or _inj0.get("admitted") or _inj0.get("ui_norm") or []
        if isinstance(_ad0, list):
            inj_set_sources["inj_trace_v1"] = [u for u in _ad0 if isinstance(u, str) and u]
    except Exception:
        pass
    try:
        def _unwrap_diag_inj(resp: dict):
            if not isinstance(resp, dict):
                return {}
            r = resp.get("results")
            if isinstance(r, dict):
                d = r.get("debug")
                if isinstance(d, dict) and isinstance(d.get("diag_injected_urls"), dict):
                    return d.get("diag_injected_urls") or {}
            d0 = resp.get("debug")
            if isinstance(d0, dict) and isinstance(d0.get("diag_injected_urls"), dict):
                return d0.get("diag_injected_urls") or {}
            return {}
        _di = _unwrap_diag_inj(cur_response)
        _ad = _di.get("admitted_norm") or _di.get("admitted") or []
        _ui = _di.get("ui_norm") or []
        if isinstance(_ad, list):
            inj_set_sources["diag_injected_urls"] = [u for u in _ad if isinstance(u, str) and u]
        if isinstance(_ui, list):
            inj_set_sources["diag_injected_urls_ui"] = [u for u in _ui if isinstance(u, str) and u]
    except Exception:
        pass
    # END PATCH FIX2W_INJECTED_SET_BUILD_V2

    # PATCH FIX2D3 START: Diff Panel V2 injected-set detection (cur_response may not include debug wrapper)
    try:
        # inj_trace_v1 may live in different containers depending on caller.
        _inj = None

        # (1) common: cur_response.debug.inj_trace_v1
        try:
            if isinstance(cur_response, dict):
                _dbg = cur_response.get("debug") if isinstance(cur_response.get("debug"), dict) else None
                if isinstance(_dbg, dict) and isinstance(_dbg.get("inj_trace_v1"), dict):
                    _inj = _dbg.get("inj_trace_v1")
                elif isinstance(cur_response.get("inj_trace_v1"), dict):
                    _inj = cur_response.get("inj_trace_v1")
        except Exception:
            pass
            _inj = None

        # (2) sometimes: locals/output.debug.inj_trace_v1 (cur_response passed as a partial dict)
        if not isinstance(_inj, dict) or not _inj:
            try:
                _out = locals().get("output")
                if isinstance(_out, dict):
                    _dbg2 = _out.get("debug") if isinstance(_out.get("debug"), dict) else None
                    if isinstance(_dbg2, dict) and isinstance(_dbg2.get("inj_trace_v1"), dict):
                        _inj = _dbg2.get("inj_trace_v1")
            except Exception:
                pass

        # (3) last resort: locals inj_trace_v1 variable
        if not isinstance(_inj, dict) or not _inj:
            try:
                _inj3 = locals().get("inj_trace_v1")
                if isinstance(_inj3, dict):
                    _inj = _inj3
            except Exception:
                pass

        _ad = (_inj or {}).get("admitted_norm") or (_inj or {}).get("admitted") or []
        if isinstance(_ad, list):
            inj_set = set([u for u in _ad if isinstance(u, str) and u])
    except Exception:
        pass
        inj_set = set()
    # END PATCH FIX2D3


    def _metric_source_urls(mdict: dict):
        """Best-effort extract list of source/evidence URLs from a canonical metric dict."""
        if not isinstance(mdict, dict):
            return []
        for k in ("source_urls", "sources", "evidence_urls", "urls", "source_url"):
            v = mdict.get(k)
            if isinstance(v, str) and v:
                return [v]
            if isinstance(v, list) and v:
                return [x for x in v if isinstance(x, str) and x]
        return []

    def _is_from_injected_url(mdict: dict):
        if not inj_set:
            return False
        urls = _metric_source_urls(mdict)
        return any((u in inj_set) for u in urls)


    # Sentinel behavior if no prev metrics
    if not isinstance(prev_metrics, dict) or not prev_metrics:

        rows.append({
            "name": "No previous canonical metrics",
            "canonical_key": None,
            "previous_value": "N/A",
            "current_value": "N/A",
            "change_pct": None,
            "change_type": "no_prev_metrics",
            "match_confidence": 0.0,
            "diag": {
                "diff_join_trace_v1": {
                    "prev_ckey": None,
                    "resolved_cur_ckey": None,
                    "method": "none",
                    "prev_anchor_hash": None,
                    "cur_anchor_hash": None,
                },
                "diff_current_source_trace_v1": {
                    "current_source_path_used": "none",
                    "current_value_norm": None,
                    "current_unit_tag": None,
                    "inference_disabled": False,
                },
            },
        })
        summary = {
            "rows_total": 1,
            "joined_by_ckey": 0,
            "joined_by_anchor_hash": 0,
            "not_found": 1,
            "sample_anchor_joins": [],
        }
        # =====================================================================
    # PATCH FIX2W_DIFF_PANEL_INJ_TRACE_ATTACH_V1 (ADDITIVE)
    # =====================================================================
    try:
        summary.setdefault("diag", {})
        if isinstance(summary.get("diag"), dict):
            summary["diag"].setdefault("fix2w_injected_url_set_v2", {})
            summary["diag"]["fix2w_injected_url_set_v2"]["inj_set_size"] = int(len(inj_set)) if isinstance(inj_set, set) else 0
            summary["diag"]["fix2w_injected_url_set_v2"]["inj_set_sources"] = inj_set_sources if isinstance(locals().get("inj_set_sources"), dict) else {}
            try:
                _samp = list(sorted(list(inj_set)))[:50] if isinstance(inj_set, set) else []
                summary["diag"]["fix2w_injected_url_set_v2"]["inj_set_sample"] = _samp
            except Exception:
                pass
                summary["diag"]["fix2w_injected_url_set_v2"]["inj_set_sample"] = []
    except Exception:
        pass
    # END PATCH FIX2W_DIFF_PANEL_INJ_TRACE_ATTACH_V1

    # Emit rows (deterministic ordering). Default is strict (prev-only).
    _join_mode = _fix2d6_get_diff_join_mode_v1()
    _join_mode = "union" if _join_mode in ("union", "u", "1", "true", "yes", "y") else "strict"

    # =====================================================================
    # FIX2D46_SCHEMA_CROSS_SOURCE_CURRENT (ADDITIVE)
    # Goal:
    # - Allow Evolution to use "current" metrics from NEW sources (not present in baseline),
    #   as long as they canonicalize to the SAME schema key as a baseline metric.
    # Mechanism:
    # - Re-key cur_metrics by schema_key using each metric's own canonical_key (or fallbacks)
    # - Select a deterministic winner per schema_key (highest confidence, tie-break by key)
    # - Rebuild cur_by_anchor from the re-keyed cur_metrics
    # Activation:
    # - If diff join mode is set to one of: schema, cross, schema_union, schema_cross
    #   (via _fix2d6_get_diff_join_mode_v1), this block runs.
    # Safety:
    # - Purely affects diff panel hydration/joining; does not affect hashing or extraction.
    # =====================================================================
    _fix2d46_enabled = False
    _fix2d46_mode_raw = ""
    try:
        _jm_raw = _fix2d6_get_diff_join_mode_v1()
        _fix2d46_mode_raw = str(_jm_raw or "").strip().lower()
        if _fix2d46_mode_raw in ("schema", "cross", "schema_union", "schema_cross", "schema_cross_source", "schema+union", "x"):
            _fix2d46_enabled = True
    except Exception:
        pass
        _fix2d46_enabled = False

    if _fix2d46_enabled and isinstance(cur_metrics, dict):
        try:
            _cur_rekeyed = {}
            _cur_pick_trace = {"enabled": True, "mode_raw": _fix2d46_mode_raw, "winners": 0, "dropped": 0}

            def _fix2d46_score(m: dict) -> float:
                if not isinstance(m, dict):
                    return 0.0
                for k in ("confidence", "match_confidence", "schema_match_score", "score", "rank_score"):
                    v = m.get(k)
                    try:
                        if v is not None:
                            return float(v)
                    except Exception:
                        return 0.0

            for _ck, _m in cur_metrics.items():
                if not isinstance(_m, dict):
                    _cur_pick_trace["dropped"] += 1
                    continue
                _schema_key = (
                    _m.get("canonical_key")
                    or _m.get("canonicalkey")
                    or _m.get("schema_key")
                    or _m.get("schema_canonical_key")
                    or _m.get("schema_key_v1")
                    or _ck
                )
                if not isinstance(_schema_key, str) or not _schema_key:
                    _cur_pick_trace["dropped"] += 1
                    continue

                _s = _fix2d46_score(_m)
                _prev = _cur_rekeyed.get(_schema_key)
                if _prev is None:
                    _cur_rekeyed[_schema_key] = {"__score": _s, "__src_ckey": _ck, "metric": _m}
                else:
                    # winner: higher score, tie-break lexicographically by __src_ckey for determinism
                    if (_s > float(_prev.get("__score") or 0.0)) or (
                        _s == float(_prev.get("__score") or 0.0) and str(_ck) < str(_prev.get("__src_ckey") or "")
                    ):
                        _cur_rekeyed[_schema_key] = {"__score": _s, "__src_ckey": _ck, "metric": _m}

            # Finalize: strip wrapper, leave dict keyed by schema_key
            cur_metrics = {k: v["metric"] for k, v in _cur_rekeyed.items() if isinstance(v, dict) and isinstance(v.get("metric"), dict)}
            _cur_pick_trace["winners"] = int(len(cur_metrics))

            # Rebuild anchor index from re-keyed cur_metrics
            cur_by_anchor = {}
            try:
                for ck, cm in cur_metrics.items():
                    ah = _get_anchor_hash_for_ckey(ck, cm, cur_anchors)
                    if ah:
                        cur_by_anchor.setdefault(str(ah), []).append(ck)
                for ah in list(cur_by_anchor.keys()):
                    cur_by_anchor[ah] = sorted([c for c in cur_by_anchor[ah] if isinstance(c, str)])
            except Exception:
                pass
                cur_by_anchor = {}

            try:
                summary.setdefault("diag", {})
                if isinstance(summary.get("diag"), dict):
                    summary["diag"]["fix2d46_schema_cross_source"] = dict(_cur_pick_trace)
            except Exception:
                pass
        except Exception:
            pass
    # =====================================================================
    # END FIX2D46_SCHEMA_CROSS_SOURCE_CURRENT
    # =====================================================================

    prev_keys = sorted([k for k in prev_metrics.keys() if isinstance(k, str)]) if isinstance(prev_metrics, dict) else []
    cur_keys = sorted([k for k in cur_metrics.keys() if isinstance(k, str)]) if isinstance(cur_metrics, dict) else []

    _prev_set = set(prev_keys)
    _cur_set = set(cur_keys)
    _both_count = len(_prev_set & _cur_set)
    _prev_only_count = len(_prev_set - _cur_set)
    _cur_only_count = len(_cur_set - _prev_set)

    if _join_mode == "union":
        _emit_keys = sorted(_prev_set | _cur_set)
    else:
        _emit_keys = list(prev_keys)

    for prev_ckey in _emit_keys:
        pm = prev_metrics.get(prev_ckey) if isinstance(prev_metrics, dict) else None
        pm = pm if isinstance(pm, dict) else {}

        _has_prev = prev_ckey in _prev_set
        _has_cur = prev_ckey in _cur_set

        # REFACTOR27: capture per-row source URLs for debugging/injection gating
        prev_source_url = None
        cur_source_url = None
        try:
            _purls = _metric_source_urls(pm) if isinstance(pm, dict) else []
            prev_source_url = _purls[0] if isinstance(_purls, list) and _purls else None
        except Exception:
            prev_source_url = None

        prev_raw = _raw_display_value(pm)
        prev_val_norm = _canon_value_norm(pm)
        prev_unit = _canon_unit_tag(pm)
        prev_ah = _get_anchor_hash_for_ckey(prev_ckey, pm, prev_anchors)

        # REFACTOR03: default; will be computed once current metric is resolved
        _ref03_unit_mismatch = False

        resolved_cur_ckey = None
        method = "none"

        # 1) primary join by canonical_key
        if isinstance(cur_metrics, dict) and prev_ckey in cur_metrics:
            resolved_cur_ckey = prev_ckey
            method = "ckey"
            joined_by_ckey += 1
        else:
            # 2) secondary join by anchor_hash
            if prev_ah and str(prev_ah) in cur_by_anchor and cur_by_anchor.get(str(prev_ah)):
                resolved_cur_ckey = cur_by_anchor[str(prev_ah)][0]
                method = "anchor_hash"
                joined_by_anchor += 1
                if len(sample_anchor_joins) < 10:
                    sample_anchor_joins.append({
                        "anchor_hash": str(prev_ah),
                        "prev_ckey": prev_ckey,
                        "resolved_cur_ckey": resolved_cur_ckey,
                        "candidates": cur_by_anchor.get(str(prev_ah))[:5],
                    })

        cm = None
        cur_raw = "N/A"
        cur_val_norm = None
        cur_unit = None
        cur_ah = None

        if resolved_cur_ckey and isinstance(cur_metrics, dict):
            try:
                matched_cur_ckeys.add(resolved_cur_ckey)
            except Exception:
                pass
            cm = cur_metrics.get(resolved_cur_ckey)
            cm = cm if isinstance(cm, dict) else {}
            _tmp_raw = _raw_display_value(cm)
            if _tmp_raw is not None and _tmp_raw != "":
                cur_raw = _tmp_raw
            cur_val_norm = _canon_value_norm(cm)
            cur_unit = _canon_unit_tag(cm)
            cur_ah = _get_anchor_hash_for_ckey(resolved_cur_ckey, cm, cur_anchors)

            # REFACTOR03 (ADDITIVE): detect unit family/scale mismatch for this canonical key
            try:
                _ref03_unit_mismatch = _refactor03_diff_unit_mismatch_v1(
                    prev_key=prev_ckey,
                    prev_metric=pm,
                    cur_metric=cm,
                    prev_unit=prev_unit,
                    cur_unit=cur_unit,
                )
            except Exception:
                _ref03_unit_mismatch = False

            # FIX2D2E: capture join source url for trace/UI
            try:
                _urls = _metric_source_urls(cm)
                cur_source_url = _urls[0] if _urls else None
            except Exception:
                pass
                cur_source_url = None

            try:
                _urls = _metric_source_urls(cm)
                if _urls:
                    cur_source_url = _urls[0]
            except Exception:
                pass
                cur_source_url = None
        else:
            not_found += 1

        # change_type + change_pct (purely numeric when both value_norm exist)
        change_type = "unknown"
        change_pct = None
        match_conf = 0.0

        if method == "ckey":
            match_conf = 95.0
        elif method == "anchor_hash":
            match_conf = 85.0

        if resolved_cur_ckey is None:
            change_type = "not_found"
        else:
            if isinstance(prev_val_norm, (int, float)) and isinstance(cur_val_norm, (int, float)):
                # classify without any unit family inference
                if abs(prev_val_norm - cur_val_norm) <= max(1e-9, abs(prev_val_norm) * 0.0005):
                    change_type = "unchanged"
                    change_pct = 0.0
                elif cur_val_norm > prev_val_norm:
                    change_type = "increased"
                    change_pct = ((cur_val_norm - prev_val_norm) / max(1e-9, abs(prev_val_norm))) * 100.0
                else:
                    change_type = "decreased"
                    change_pct = ((cur_val_norm - prev_val_norm) / max(1e-9, abs(prev_val_norm))) * 100.0
            else:
                # we do not infer numeric; keep classification neutral
                change_type = "unknown"

        # Union mode: expose current-only and prev-only metrics as added/removed (identity remains canonical_key).
        if _join_mode == "union":
            if _has_cur and not _has_prev:
                change_type = "added"
                change_pct = None
            elif _has_prev and not _has_cur:
                change_type = "removed"
                change_pct = None

# -----------------------------------------------------------------
        # PATCH FIX2D13_BASELINE_SEMANTICS_V1 (ADDITIVE)
        # Baseline-focused semantics for Analysis -> Evolution comparison:
        # - Only compute increased/decreased/unchanged when BOTH sides have numeric values.
        # - If baseline missing but current present => added (newly discovered vs baseline)
        # - If baseline present but current missing => not_found (missing vs baseline)
        # This is orthogonal to join_mode (strict/union) and preserves existing row fields.
        # -----------------------------------------------------------------
        baseline_prev_value = prev_raw if _has_prev else None
        baseline_cur_value = cur_raw if _has_cur else None
        baseline_delta_abs = None
        baseline_delta_pct = None
        baseline_change_type = None
        baseline_is_comparable = bool(_has_prev and _has_cur)

        if baseline_is_comparable:
            try:
                _d = float(cur_val_norm) - float(prev_val_norm)
                baseline_delta_abs = _d
                if abs(float(prev_val_norm)) > 1e-12:
                    baseline_delta_pct = (_d / float(prev_val_norm)) * 100.0
                # tolerance: treat tiny diffs as unchanged (avoid float jitter)
                if abs(_d) < 1e-9:
                    baseline_change_type = "unchanged"
                elif _d > 0:
                    baseline_change_type = "increased"
                else:
                    baseline_change_type = "decreased"
            except Exception:
                pass
                baseline_is_comparable = False
                baseline_change_type = "unknown"
        else:
            if _has_cur and not _has_prev:
                baseline_change_type = "added"
            elif _has_prev and not _has_cur:
                baseline_change_type = "not_found"
            else:
                baseline_change_type = "unknown"


        # =====================================================================
        # PATCH FIX2D41_SCHEMA_REMAP_BASELINE_DIFF_ADMISSION (ADDITIVE)
        # Option A: allow schema-remapped baselines (schema_remap_v1==True) to participate
        # in baseline diffing when BOTH sides have numeric values. Unit/dimension guards
        # remain in place; this patch only removes the final policy veto against remapped
        # baselines, while stamping explicit audit fields.
        # =====================================================================
        _fix2d41_baseline_remap_used = False
        _fix2d41_baseline_remap_score = None
        _fix2d41_baseline_comparability_mode = None
        try:
            if isinstance(pm, dict) and pm.get("schema_remap_v1") is True:
                _fix2d41_baseline_remap_used = True
                _fix2d41_baseline_remap_score = pm.get("schema_remap_score_v1")
                if isinstance(prev_val_norm, (int, float)) and isinstance(cur_val_norm, (int, float)):
                    baseline_is_comparable = True
                    _fix2d41_baseline_comparability_mode = "schema_remap_allowed"
                    try:
                        _d = float(cur_val_norm) - float(prev_val_norm)
                        baseline_delta_abs = _d
                        if abs(float(prev_val_norm)) > 1e-12:
                            baseline_delta_pct = (_d / float(prev_val_norm)) * 100.0
                        if abs(_d) < 1e-9:
                            baseline_change_type = "unchanged"
                        elif _d > 0:
                            baseline_change_type = "increased"
                        else:
                            baseline_change_type = "decreased"
                    except Exception:
                        pass
        except Exception:
            pass
            _fix2d41_baseline_remap_used = False
            _fix2d41_baseline_remap_score = None
            _fix2d41_baseline_comparability_mode = None
        # =====================================================================
        # END PATCH FIX2D41_SCHEMA_REMAP_BASELINE_DIFF_ADMISSION
        # =====================================================================
        try:
            summary["baseline_comparable"] = int(summary.get("baseline_comparable") or 0) + (1 if baseline_is_comparable else 0)
            if baseline_change_type == "increased":
                summary["baseline_increased"] = int(summary.get("baseline_increased") or 0) + 1
            elif baseline_change_type == "decreased":
                summary["baseline_decreased"] = int(summary.get("baseline_decreased") or 0) + 1
            elif baseline_change_type == "unchanged":
                summary["baseline_unchanged"] = int(summary.get("baseline_unchanged") or 0) + 1
            elif baseline_change_type == "added":
                summary["baseline_added"] = int(summary.get("baseline_added") or 0) + 1
            elif baseline_change_type == "not_found":
                summary["baseline_not_found"] = int(summary.get("baseline_not_found") or 0) + 1
        except Exception:
            pass
        # END PATCH FIX2D13_BASELINE_SEMANTICS_V1

        # =====================================================================
        # PATCH FIX2D32_ANCHOR_MISMATCH_DIFFABLE (ADDITIVE)
        # If we joined by canonical_key, anchors may legitimately differ in production
        # (new source / updated page). Treat as diffable; stamp diagnostics.
        # =====================================================================
        _fix2d32_anchor_mismatch = False
        try:
            if method_effective == "ckey" and prev_ah and cur_ah_effective and str(prev_ah) != str(cur_ah_effective):
                _fix2d32_anchor_mismatch = True
                try:
                    summary["joined_by_ckey_anchor_mismatch"] = int(summary.get("joined_by_ckey_anchor_mismatch") or 0) + 1
                except Exception:
                    pass
        except Exception:
            pass
            _fix2d32_anchor_mismatch = False
        # =====================================================================
        # END PATCH FIX2D32_ANCHOR_MISMATCH_DIFFABLE
        # =====================================================================

        # =====================================================================
        # PATCH FIX2D32_ANCHOR_MISMATCH_DIFFABLE (ADDITIVE)
        # Treat anchor_hash mismatches as still diffable when joined by canonical_key.
        # Stamp diagnostics and count mismatches for audit.
        # =====================================================================
        _fix2d32_anchor_mismatch = False
        try:
            if method_effective == "ckey" and prev_ah and cur_ah_effective and str(prev_ah) != str(cur_ah_effective):
                _fix2d32_anchor_mismatch = True
                try:
                    summary["joined_by_ckey_anchor_mismatch"] = int(summary.get("joined_by_ckey_anchor_mismatch") or 0) + 1
                except Exception:
                    pass
        except Exception:
            pass
            _fix2d32_anchor_mismatch = False
        # =====================================================================
        # END PATCH FIX2D32_ANCHOR_MISMATCH_DIFFABLE
        # =====================================================================




        # =====================================================================
        # REFACTOR03_UNIT_MISMATCH_GUARD (ADDITIVE)
        # If a unit family / scale mismatch is detected, suppress comparability and
        # classify deterministically as unit_mismatch.
        # =====================================================================
        if _ref03_unit_mismatch:
            baseline_is_comparable = False
            baseline_delta_abs = None
            baseline_delta_pct = None
            baseline_change_type = "unit_mismatch"
            change_type = "unit_mismatch"
            change_pct = None



        # =====================================================================
        # PATCH FIX2D35_PROXY_BASELINE_DIFF_ADMISSION (ADDITIVE)
        # Allow schema-mandated proxy baselines (is_proxy=True) to remain diffable
        # when numeric prev_value_norm exists. Anchors/units guards remain in place.
        # Stamp diagnostics for audit.
        # =====================================================================
        _fix2d35_baseline_proxy_used = False
        _fix2d35_baseline_proxy_type = None
        try:
            if isinstance(pm, dict) and pm.get('is_proxy'):
                _fix2d35_baseline_proxy_used = True
                _fix2d35_baseline_proxy_type = pm.get('proxy_type') or pm.get('proxy_reason') or None
                try:
                    summary['baseline_proxy_rows'] = int(summary.get('baseline_proxy_rows') or 0) + 1
                except Exception:
                    pass
        except Exception:
            pass
            _fix2d35_baseline_proxy_used = False
            _fix2d35_baseline_proxy_type = None
        # =====================================================================
        # END PATCH FIX2D35_PROXY_BASELINE_DIFF_ADMISSION
        # =====================================================================

        display_name = pm.get("name") or pm.get("display_name") or pm.get("original_name") or prev_ckey

        row = {
            "name": display_name or "Unknown Metric",
            "canonical_key": prev_ckey,
            "previous_value": prev_raw,
            "current_value": cur_raw,
            "change_pct": change_pct,
            "change_type": change_type,
            "match_confidence": match_conf,

            # PATCH FIX2D13 (ADDITIVE): baseline-focused diff fields
            "baseline_prev_value": baseline_prev_value,
            "baseline_cur_value": baseline_cur_value,
            "baseline_delta_abs": baseline_delta_abs,
            "baseline_delta_pct": baseline_delta_pct,
            "baseline_change_type": baseline_change_type,
            "baseline_is_comparable": baseline_is_comparable,

            # PATCH FIX2D35 (ADDITIVE): proxy baseline audit
            "baseline_proxy_used": bool(_fix2d35_baseline_proxy_used),
            "baseline_proxy_type": _fix2d35_baseline_proxy_type,

            # PATCH FIX2D41 (ADDITIVE): schema remap baseline audit
            "baseline_remap_used": bool(_fix2d41_baseline_remap_used),
            "baseline_remap_score": _fix2d41_baseline_remap_score,
            "baseline_comparability_mode_v1": _fix2d41_baseline_comparability_mode,


            # minimal context fields kept for UI compatibility
            "context_snippet": None,
            "source_url": (cur_source_url or prev_source_url),
            "current_source_url": cur_source_url,
            "previous_source_url": prev_source_url,

            # anchor fields for debugging/inspection
            "anchor_used": (method == "anchor_hash"),
            "prev_anchor_hash": prev_ah,
            "cur_anchor_hash": cur_ah,
            "prev_value_norm": prev_val_norm,
            "cur_value_norm": cur_val_norm,

            # FIX2D76 (ADDITIVE): expose unit evidence for FIX39 sanitizer + UI
            "prev_unit_tag": prev_unit,
            "cur_unit_cmp": cur_unit,
            "current_unit": cur_unit,
            "current_unit_tag": cur_unit,
            "unit_mismatch": bool(_ref03_unit_mismatch),

            "diag": {
                "diff_join_trace_v1": {
                    "prev_ckey": prev_ckey,
                    "resolved_cur_ckey": resolved_cur_ckey,
                    "method": method,
                    "inference_attempted": bool(_fix2d25_inference_enabled and (prev_val_norm is not None) and (resolved_cur_ckey is None or cur_val_norm is None)),
                    "inference_bound": bool(method == "inference_bound"),
                    "prev_anchor_hash": prev_ah,
                    "cur_anchor_hash": cur_ah,
                },
                "diff_current_source_trace_v1": {
                    "current_source_path_used": "primary_metrics_canonical" if resolved_cur_ckey else "none",
                    "current_value_norm": cur_val_norm,
                    "current_unit_tag": cur_unit,
                    "inference_disabled": (not _fix2d25_inference_enabled),
                    "inference_gate_v1": {
                        "enabled": bool(_fix2d25_inference_enabled),
                        "reason": str(_fix2d2c_inference_gate_reason or ""),
                    },
                },
            },
        }

        # REFACTOR27: hard veto unit-mismatch rows from emitting numeric deltas (avoid nonsense deltas)
        try:
            if bool(_ref03_unit_mismatch):
                row["unit_mismatch"] = True
                row["change_type"] = "unit_mismatch"
                row["change_pct"] = None
                row["baseline_is_comparable"] = False
                row["baseline_change_type"] = "unit_mismatch"
                row["baseline_delta_abs"] = None
                row["baseline_delta_pct"] = None
        except Exception:
            pass

        # Keep any helpful selector breadcrumb if present on current metric
        try:
            if isinstance(cm, dict) and cm.get("selector_used") and not row.get("selector_used"):
                row["selector_used"] = cm.get("selector_used")
        except Exception:
            pass

        rows.append(row)

    # -------------------------------------------------------------
    #
    # -------------------------------------------------------------
    # PATCH DIFF_PANEL_V2_OBSERVED_HELPERS (ADDITIVE)
    # -------------------------------------------------------------
    def _unwrap_inj_admitted_norm(resp: dict):
        try:
            dbg = resp.get("debug") if isinstance(resp, dict) else None
            if isinstance(dbg, dict):
                it = dbg.get("inj_trace_v1")
                if isinstance(it, dict) and isinstance(it.get("admitted_norm"), list):
                    return [x for x in it.get("admitted_norm") if isinstance(x, str)]
            it = resp.get("inj_trace_v1") if isinstance(resp, dict) else None
            if isinstance(it, dict) and isinstance(it.get("admitted_norm"), list):
                return [x for x in it.get("admitted_norm") if isinstance(x, str)]
        except Exception:
            return []

    def _unwrap_baseline_sources_cache_current(resp: dict):
        if not isinstance(resp, dict):
            return []
        # Prefer debug.baseline_sources_cache_current if present
        dbg = resp.get("debug")
        if isinstance(dbg, dict):
            b = dbg.get("baseline_sources_cache_current")
            if isinstance(b, list) and b:
                return b
        # Fallbacks (some runs attach at top-level or under results)
        b = resp.get("baseline_sources_cache_current")
        if isinstance(b, list) and b:
            return b
        res = resp.get("results")
        if isinstance(res, dict):
            b = res.get("baseline_sources_cache_current")
            if isinstance(b, list) and b:
                return b
        pr = resp.get("primary_response")
        if isinstance(pr, dict):
            dbg2 = pr.get("debug")
            if isinstance(dbg2, dict):
                b = dbg2.get("baseline_sources_cache_current")
                if isinstance(b, list) and b:
                    return b
        return []

    def _is_plausible_year_only(val):
        try:
            v = float(val)
        except Exception:
            return False
        return 1900.0 <= v <= 2100.0 and abs(v - int(v)) < 1e-9

    # NEW: Append current-only canonical metrics as additional rows.
    # These represent metrics present in current run but not matched
    # to any previous canonical metric (ckey/anchor). Deterministic,
    # inference-free, and auditable.
    # -------------------------------------------------------------
    current_only_total = 0
    current_only_injected = 0
    try:
        if isinstance(cur_metrics, dict) and cur_metrics:
            for ck, cm in cur_metrics.items():
                if not isinstance(ck, str) or not ck:
                    continue
                if ck in matched_cur_ckeys:
                    continue
                if not isinstance(cm, dict):
                    continue

                cur_raw = _raw_display_value(cm) or "N/A"
                cur_val_norm = _canon_value_norm(cm)
                cur_unit = _canon_unit_tag(cm)
                cur_ah = _anchor_hash_from_metric(cm) or None

                row = {
                    "name": cm.get("name") or cm.get("metric_name") or ck,
                    "canonical_key": ck,
                    "previous_value": "N/A",
                    "current_value": cur_raw,
                    "change_pct": None,
                    "change_type": "current_only",
                    "match_confidence": 0.0,

                    # PATCH FIX2D13 (ADDITIVE): baseline-focused diff fields
                    "baseline_prev_value": None,
                    "baseline_cur_value": cur_raw,
                    "baseline_delta_abs": None,
                    "baseline_delta_pct": None,
                    "baseline_change_type": "added",
                    "baseline_is_comparable": False,

                    "context_snippet": None,
                    "source_url": (_metric_source_urls(cm) or [None])[0],
                    "anchor_used": False,
                    "prev_anchor_hash": None,
                    "cur_anchor_hash": cur_ah,
                    "prev_value_norm": None,
                    "cur_value_norm": cur_val_norm,
                    # FIX2D76 (ADDITIVE): unit evidence fields (for FIX39 + UI)
                    "prev_unit_tag": None,
                    "cur_unit_cmp": cur_unit,
                    "current_unit": cur_unit,
                    "current_unit_tag": cur_unit,
                    "unit_mismatch": False,
                    "from_injected_url": _is_from_injected_url(cm),
                    "diag": {
                        "diff_join_trace_v1": {
                            "prev_ckey": None,
                            "resolved_cur_ckey": ck,
                            "method": "current_only",
                            "prev_anchor_hash": None,
                            "cur_anchor_hash": cur_ah,
                        },
                        "diff_current_source_trace_v1": {
                            "current_source_path_used": "primary_metrics_canonical",
                            "current_value_norm": cur_val_norm,
                            "current_unit_tag": cur_unit,
                            "inference_disabled": False,
                        },
                        "diff_current_only_trace_v1": {
                            "reason": "unmatched_current_metric",
                            "matched_cur_ckeys_count": len(matched_cur_ckeys),
                            "injected_url_match": _is_from_injected_url(cm),
                        },
                    },
                }

                # preserve selector breadcrumbs if present
                try:
                    if cm.get("selector_used"):
                        row["selector_used"] = cm.get("selector_used")
                except Exception:
                    pass

                rows.append(row)
                try:
                    summary["baseline_added"] = int(summary.get("baseline_added") or 0) + 1
                except Exception:
                    pass
                current_only_total += 1
                if row.get("from_injected_url"):
                    current_only_injected += 1
    except Exception:
        pass



    # -------------------------------------------------------------
    # PATCH DIFF_PANEL_V2_OBSERVED_ROWS (ADDITIVE)
    #
    # Emit "observed" current-only rows from deterministic, already-
    # collected extracted_numbers pools (render-only). These rows are:
    # - explicitly non-canonical (do NOT participate in joins/diff math)
    # - labeled so UI/user can distinguish them from canonical metrics
    # - de-duped and capped to avoid noise
    # -------------------------------------------------------------
    observed_rows_total = 0
    observed_rows_injected = 0
    observed_rows_filtered_yearlike = 0
    observed_rows_filtered_noninjected = 0
    observed_yearlike_samples = []

    def _fix2d23_is_yearlike_token(value_norm, unit_tag):
        try:
            if unit_tag:
                return False
            if isinstance(value_norm, bool):
                return False
            # numeric years
            if isinstance(value_norm, (int, float)):
                fv = float(value_norm)
                iv = int(round(fv))
                if abs(fv - iv) < 1e-9 and 1900 <= iv <= 2100:
                    return True
            # string years, including "2030.0"
            s = str(value_norm).strip()
            if s.endswith('.0'):
                s2 = s[:-2]
                if s2.isdigit():
                    iv = int(s2)
                    if 1900 <= iv <= 2100:
                        return True
            if s.isdigit() and len(s) == 4:
                iv = int(s)
                if 1900 <= iv <= 2100:
                    return True
        except Exception:
            return False
        return False

    try:
        import hashlib

        admitted_norm = set(_unwrap_inj_admitted_norm(cur_response))
        bsc = _unwrap_baseline_sources_cache_current(cur_response)

        # Build exclusion sets from canonical/anchor identities so we don't
        # "double report" what is already present canonically.
        prev_can = _unwrap_primary_metrics_canonical(prev_response) or {}
        cur_can = _unwrap_primary_metrics_canonical(cur_response) or {}
        prev_anchor_map = _unwrap_metric_anchors(prev_response) or {}
        cur_anchor_map = _unwrap_metric_anchors(cur_response) or {}

        existing_ckeys = set()
        for d in (prev_can, cur_can):
            if isinstance(d, dict):
                existing_ckeys.update([k for k in d.keys() if isinstance(k, str)])

        existing_anchor_hashes = set()
        for amap in (prev_anchor_map, cur_anchor_map):
            if isinstance(amap, dict):
                for _k, _v in amap.items():
                    if isinstance(_v, dict):
                        ah = _v.get("anchor_hash") or _v.get("anchor")
                        if isinstance(ah, str) and ah:
                            existing_anchor_hashes.add(ah)

        # Pull candidates from extracted_numbers lists
        candidates = []
        for src in bsc:
            if not isinstance(src, dict):
                continue
            url_norm = src.get("url_norm") or src.get("source_url_norm") or src.get("url") or src.get("source_url")
            url_norm = url_norm if isinstance(url_norm, str) else None
            extracted = src.get("extracted_numbers") or src.get("numbers") or src.get("extractions")
            if not isinstance(extracted, list) or not extracted:
                continue
            for item in extracted:
                if not isinstance(item, dict):
                    continue
                val = item.get("value_norm")
                if val is None:
                    val = item.get("value")
                if val is None:
                    continue
                # Skip pure years with no unit evidence
                unit_tag = item.get("unit_tag") or item.get("unit") or item.get("unit_norm")
                if (unit_tag is None or unit_tag == "" or unit_tag == "unknown") and _is_plausible_year_only(val):
                    continue

                year = item.get("year") or item.get("year_norm") or item.get("as_of_year")
                year = int(year) if isinstance(year, (int, float)) and 1900 <= int(year) <= 2100 else (year if isinstance(year, str) else None)

                label = item.get("metric_name") or item.get("name") or item.get("label") or item.get("context") or item.get("snippet")
                label = label if isinstance(label, str) else None

                ah = item.get("anchor_hash") or item.get("anchor")
                if not isinstance(ah, str) or not ah:
                    # Stable local fallback: hash(label|unit|year|url)
                    basis = "|".join([
                        label or "",
                        str(unit_tag or ""),
                        str(year or ""),
                        url_norm or "",
                    ])
                    ah = "obs_" + hashlib.sha1(basis.encode("utf-8")).hexdigest()[:16]

                # Exclude if already represented canonically
                if ah in existing_anchor_hashes:
                    continue

                candidates.append({
                    "value_norm": val,
                    "unit_tag": unit_tag if isinstance(unit_tag, str) and unit_tag else None,
                    "year": year,
                    "label": label,
                    "anchor_hash": ah,
                    "url_norm": url_norm,
                    "from_injected_url": bool(url_norm and url_norm in admitted_norm),
                    "context_snippet": item.get("snippet") if isinstance(item.get("snippet"), str) else None,
                })

        # De-dupe by anchor_hash and cap to keep table readable
        seen = set()
        deduped = []
        for c in candidates:
            ah = c.get("anchor_hash")
            if ah in seen:
                continue
            seen.add(ah)
            deduped.append(c)
        # deterministic order: injected first, then by (label,url,year,unit,value)
        deduped.sort(key=lambda x: (
            0 if x.get("from_injected_url") else 1,
            (x.get("label") or ""),
            (x.get("url_norm") or ""),
            str(x.get("year") or ""),
            (x.get("unit_tag") or ""),
            str(x.get("value_norm") or ""),
        ))

        MAX_OBSERVED = 25
        # =====================================================================
        # PATCH FIX2T_FILTER_OBSERVED_TO_INJECTED_ONLY (ADDITIVE)
        # ---------------------------------------------------------------------
        # Goal: Ensure Observed rows (and any downstream export derived from them)
        # contain ONLY metrics sourced from the *actual injected URL set* for
        # this run. This prevents baseline-source remainder extractions from
        # polluting the Observed export set.
        #
        # Mechanism: We rely on the existing deterministic membership flag
        # `from_injected_url`, which is computed as:
        #   from_injected_url := (url_norm in admitted_norm)
        # where `admitted_norm` is the normalized, de-duped injected URL list
        # derived from the UI / web_context injection wiring.
        #
        # NOTE: This is intentionally domain-agnostic (works for any injected
        # domain), and does not depend on heuristic domain checks.
        ONLY_INJECTED_OBSERVED = True
        observed_rows_filtered_noninjected = 0
        # =====================================================================
        for c in deduped[:MAX_OBSERVED]:
            observed_rows_total += 1
            if ONLY_INJECTED_OBSERVED and not c.get("from_injected_url"):
                observed_rows_filtered_noninjected += 1
                continue
            if c.get("from_injected_url"):
                observed_rows_injected += 1

            disp = c.get("label") or "Observed metric"
            unit_tag = c.get("unit_tag")
            year = c.get("year")
            suffix = []
            if year:
                suffix.append(str(year))
            if unit_tag:
                suffix.append(unit_tag)
            if suffix:
                disp = f"{disp} ({', '.join(suffix)})"

            rows.append({
                "name": f"[Observed] {disp}",
                "canonical_key": f"observed__{c.get('anchor_hash')}",
                "previous_value": "N/A",
                "current_value": c.get("value_norm"),
                "change_pct": None,
                "change_type": "current_only_observed",
                "match_confidence": 0.0,
                "context_snippet": c.get("context_snippet"),
                "source_url": c.get("url_norm"),
                "unit_mismatch": False,
                "diag": {
                    "diff_join_trace_v1": {
                        "prev_ckey": None,
                        "resolved_cur_ckey": None,
                        "method": "current_only_observed",
                        "prev_anchor_hash": None,
                        "cur_anchor_hash": c.get("anchor_hash"),
                    },
                    "diff_current_source_trace_v1": {
                        "current_source_path_used": "baseline_sources_cache_current.extracted_numbers",
                        "current_value_norm": c.get("value_norm"),
                        "current_unit_tag": unit_tag,
                        "inference_disabled": False,
                    },
                    "diff_observed_trace_v1": {
                        "from_injected_url": bool(c.get("from_injected_url")),
                        "url_norm": c.get("url_norm"),
                        "anchor_hash": c.get("anchor_hash"),
                        "excluded_if_anchor_overlaps_canonical": True,
                    },
                }
            })

    except Exception as _e:
        try:
            summary.setdefault("diag_errors", [])
            summary["diag_errors"].append({"observed_rows_error": str(_e)})
        except Exception:
            pass


    # -------------------------------------------------------------
    # PATCH FIX2N_DIFF_PANEL_V2_RENDER_OBSERVED_ADAPTER (ADDITIVE)
    #
    # Promote extracted numbers into UI-renderable rows, sourced from:
    #   cur_response.results.source_results[*].extracted_numbers
    #
    # Strict rules:
    # - is_junk == False
    # - not a pure year (e.g., 2024/2030) when unit_tag missing
    # - numeric value present
    # - has at least one of: unit / unit_tag / measure_kind
    # - deterministic dedup by (anchor_hash, value_norm, unit_tag, source_url)
    # - does NOT join / does NOT affect diff math
    # -------------------------------------------------------------
    observed_rows_promoted_from_source_results = 0
    try:
        import hashlib

        admitted_norm = set(_unwrap_inj_admitted_norm(cur_response))

        def _fix2n_unwrap_source_results(resp: dict):
            if not isinstance(resp, dict):
                return []
            r = resp.get("results")
            if isinstance(r, dict) and isinstance(r.get("source_results"), list) and r.get("source_results"):
                return r.get("source_results") or []
            pr = resp.get("primary_response")
            if isinstance(pr, dict):
                r2 = pr.get("results")
                if isinstance(r2, dict) and isinstance(r2.get("source_results"), list) and r2.get("source_results"):
                    return r2.get("source_results") or []
            return []

        def _fix2n_has_unit_evidence(item: dict) -> bool:
            if not isinstance(item, dict):
                return False
            u = item.get("unit")
            ut = item.get("unit_tag") or item.get("unit_norm") or item.get("base_unit")
            mk = item.get("measure_kind")
            if isinstance(u, str) and u.strip():
                return True
            if isinstance(ut, str) and ut.strip():
                return True
            if isinstance(mk, str) and mk.strip():
                return True
            return False

        def _fix2n_best_value_norm(item: dict):
            v = item.get("value_norm")
            if v is None:
                v = item.get("value")
            return v

        def _fix2n_best_unit_tag(item: dict):
            ut = item.get("unit_tag") or item.get("unit_norm") or item.get("base_unit") or item.get("unit")
            return ut.strip() if isinstance(ut, str) else ""

        def _fix2n_best_source_url(sr: dict, item: dict):
            su = item.get("source_url") if isinstance(item, dict) else None
            if isinstance(su, str) and su:
                return su
            su = sr.get("source_url") or sr.get("url") or sr.get("source") or sr.get("url_norm")
            return su if isinstance(su, str) else ""

        # exclusion: anchor_hash already represented canonically
        existing_anchor_hashes = set()
        for amap in (_unwrap_metric_anchors(prev_response) or {}, _unwrap_metric_anchors(cur_response) or {}):
            if isinstance(amap, dict):
                for _k, _v in amap.items():
                    if isinstance(_v, dict):
                        ah = _v.get("anchor_hash") or _v.get("anchor")
                        if isinstance(ah, str) and ah:
                            existing_anchor_hashes.add(ah)

        # dedup against any existing observed rows already appended earlier in this function
        existing_row_keys = set()
        try:
            for rr in (rows or []):
                if isinstance(rr, dict):
                    existing_row_keys.add((
                        str(rr.get("canonical_key") or ""),
                        str(rr.get("current_value") or ""),
                        str(rr.get("source_url") or ""),
                    ))
        except Exception:
            pass
            existing_row_keys = set()

        candidates = []
        for sr in (_fix2n_unwrap_source_results(cur_response) or []):
            if not isinstance(sr, dict):
                continue
            ex = sr.get("extracted_numbers")
            if not isinstance(ex, list) or not ex:
                continue
            for item in ex:
                if not isinstance(item, dict):
                    continue
                if item.get("is_junk") is True:
                    continue
                v = _fix2n_best_value_norm(item)
                if v is None:
                    continue
                if not _fix2n_has_unit_evidence(item):
                    continue

                unit_tag = _fix2n_best_unit_tag(item)
                if (not unit_tag) and _is_plausible_year_only(v):
                    continue

                src_url = _fix2n_best_source_url(sr, item)

                # =====================================================================
                # PATCH FIX2W_INJECTED_MEMBERSHIP_TRACE_V1 (ADDITIVE): raw vs normalized injected membership
                # =====================================================================
                src_url_norm = None
                from_inj_norm = False
                try:
                    if isinstance(src_url, str) and src_url:
                        _tmp = _inj_diag_norm_url_list([src_url])
                        if isinstance(_tmp, list) and _tmp:
                            src_url_norm = _tmp[0]
                            from_inj_norm = bool(src_url_norm in admitted_norm)
                except Exception:
                    pass

                from_inj = bool(src_url and (src_url in admitted_norm))
                # END PATCH FIX2W_INJECTED_MEMBERSHIP_TRACE_V1

                ah = item.get("anchor_hash") or item.get("anchor")
                if not isinstance(ah, str) or not ah:
                    basis = "|".join([
                        str(item.get("context_snippet") or item.get("snippet") or item.get("context") or ""),
                        unit_tag or "",
                        str(v),
                        src_url or "",
                    ])
                    ah = "obs_" + hashlib.sha1(basis.encode("utf-8")).hexdigest()[:16]

                if ah in existing_anchor_hashes:
                    continue

                candidates.append({
                    "anchor_hash": ah,
                    "value_norm": v,
                    "unit_tag": unit_tag,
                    "measure_kind": item.get("measure_kind") if isinstance(item.get("measure_kind"), str) else "",
                    "source_url": src_url or None,
                    "context_snippet": item.get("context_snippet") or item.get("snippet") or item.get("context") or "",
                    "from_injected_url": from_inj,
                    "url_norm": (src_url_norm if isinstance(src_url_norm, str) else None),
                    "from_injected_url_norm": bool(from_inj_norm),
                })

        seen_keys = set()
        deduped = []
        for c in candidates:
            k = (
                str(c.get("anchor_hash") or ""),
                str(c.get("value_norm")),
                str(c.get("unit_tag") or ""),
                str(c.get("source_url") or ""),
            )
            if k in seen_keys:
                continue
            seen_keys.add(k)
            deduped.append(c)

        deduped.sort(key=lambda x: (
            0 if x.get("from_injected_url") else 1,
            str(x.get("source_url") or ""),
            str(x.get("unit_tag") or ""),
            str(x.get("value_norm") or ""),
            str(x.get("anchor_hash") or ""),
        ))

        MAX_OBSERVED_SR = 50
        for c in deduped[:MAX_OBSERVED_SR]:
            # build row-key to avoid duplicates with any earlier observed rows
            row_key = (
                f"observed__{c.get('anchor_hash')}",
                str(c.get("value_norm") or ""),
                str(c.get("source_url") or ""),
            )
            if row_key in existing_row_keys:
                continue
            existing_row_keys.add(row_key)

            observed_rows_total += 1
            observed_rows_promoted_from_source_results += 1
            if c.get("from_injected_url"):
                observed_rows_injected += 1

            unit_tag = c.get("unit_tag") or ""
            mk = c.get("measure_kind") or ""

            # FIX2D23: Never promote unitless bare years as Observed current values

            _fix2d24_disable_fix2d23 = True
            if not _fix2d24_disable_fix2d23:
                if _fix2d23_is_yearlike_token(c.get("value_norm"), unit_tag):
                    observed_rows_filtered_yearlike += 1
                    if len(observed_yearlike_samples) < 20:
                        observed_yearlike_samples.append({
                            "value_norm": c.get("value_norm"),
                            "raw": c.get("raw"),
                            "source_url": c.get("source_url"),
                            "context_snippet": str(c.get("context_snippet") or "")[:120],
                            "anchor_hash": c.get("anchor_hash"),
                        })
                    continue

                disp_bits = [b for b in [mk, unit_tag] if b]
                disp_suffix = f" ({', '.join(disp_bits)})" if disp_bits else ""
                snippet = str(c.get("context_snippet") or "").strip().replace("\n", " ")
                snippet = snippet[:80] if snippet else "extracted_number"

                rows.append({
                    "name": f"[Observed] {snippet}{disp_suffix}",
                    "canonical_key": f"observed__{c.get('anchor_hash')}",
                    "previous_value": "N/A",
                    "current_value": c.get("value_norm"),
                    "change_pct": None,
                    "change_type": "observed_new",
                    "match_confidence": 0.0,
                    "context_snippet": c.get("context_snippet"),
                    "source_url": c.get("source_url"),
                    "anchor_used": False,
                    "prev_anchor_hash": None,
                    "cur_anchor_hash": c.get("anchor_hash"),
                    "prev_value_norm": None,
                    "cur_value_norm": (float(c.get("value_norm")) if isinstance(c.get("value_norm"), (int, float)) else None),
                    "unit_mismatch": False,
                    "from_injected_url": bool(c.get("from_injected_url")),
                    "diag": {
                        "observed_source": "source_results.extracted_numbers",
                        "from_injected_url": bool(c.get("from_injected_url")),
                        "anchor_hash": c.get("anchor_hash"),
                        "promotion_reason": "no_canonical_match",
                        "diff_current_source_trace_v1": {
                            "current_source_path_used": "results.source_results[*].extracted_numbers",
                            "current_value_norm": c.get("value_norm"),
                            "current_unit_tag": unit_tag,
                            "inference_disabled": False,
                        },
                    },
                })
    except Exception as _e:
        try:
                summary.setdefault("diag_errors", [])
                summary["diag_errors"].append({"fix2n_observed_adapter_error": str(_e)})
        except Exception:
                pass



    # -------------------------------------------------------------
    # PATCH FIX2P_DIFF_PANEL_V2_CANARY_ROW (ADDITIVE)
    #
    # Deterministic render-path probe:
    # When enabled via cur_response.results.debug.force_canary_observed_row == True,
    # inject a single obvious [Observed][CANARY] row to prove the Evolution
    # dashboard is hydrating from results.metric_changes and that the
    # Current column can display injected values.
    #
    # This does NOT touch extraction/hashing/canonicalisation.
    # -------------------------------------------------------------
    _fix2p_canary_injected = False
    try:
        _dbg = None
        if isinstance(cur_response, dict):
                _r = cur_response.get('results')
                if isinstance(_r, dict):
                    _dbg = _r.get('debug')
                if _dbg is None:
                    _dbg = cur_response.get('debug')
        _force_canary = bool(isinstance(_dbg, dict) and _dbg.get('force_canary_observed_row') is True)
        if _force_canary:
                rows.append({
                    'name': '[Observed][CANARY] render-path probe',
                    'canonical_key': 'observed__canary_render_probe',
                    'previous_value': 'N/A',
                    'current_value': '12345.67',
                    'unit_tag': 'canary_unit',
                    'unit': 'canary_unit',
                    'change_pct': None,
                    'change_type': 'observed_new',
                    'match_confidence': 0.0,
                    'context_snippet': 'CANARY_ROW',
                    'source_url': 'canary://render_probe',
                    'anchor_used': False,
                    'prev_anchor_hash': None,
                    'cur_anchor_hash': 'canary_render_probe',
                    'prev_value_norm': None,
                    'cur_value_norm': 12345.67,
                    'unit_mismatch': False,
                    'from_injected_url': False,
                    'diag': {
                        'observed_source': 'canary',
                        'promotion_reason': 'render_path_probe',
                        'note': 'If this row does not render, the UI is not reading results.metric_changes.',
                    },
                })
                _fix2p_canary_injected = True
    except Exception:
        pass
        _fix2p_canary_injected = False
    summary = {
        "rows_total": len(rows),
        "join_mode": _join_mode, "prev_only_count": int(_prev_only_count), "cur_only_count": int(_cur_only_count), "both_count": int(_both_count),
        "joined_by_ckey": int(joined_by_ckey),
        "joined_by_anchor_hash": int(joined_by_anchor),
        "not_found": int(not_found),
        "sample_anchor_joins": sample_anchor_joins,
        "current_only_rows": current_only_total,
        "current_only_injected_rows": current_only_injected,
        "observed_rows": int(observed_rows_total),
        "observed_injected_rows": int(observed_rows_injected),
        "observed_rows_filtered_noninjected": int(observed_rows_filtered_noninjected),
        "observed_rows_filtered_yearlike": int(observed_rows_filtered_yearlike),
        "observed_yearlike_samples": observed_yearlike_samples,
        "observed_rows_promoted_from_source_results": int(observed_rows_promoted_from_source_results),
        "canary_row_injected": bool(_fix2p_canary_injected),
    }

    return rows, summary

# =====================================================================
# END PATCH DIFF_PANEL_V2
# =====================================================================
# ===================== PATCH RMS_CORE1 (ADDITIVE) =====================




# =====================================================================
# PATCH WRAP_COMPUTE_SOURCE_ANCHORED_DIFF (ADDITIVE): preserve original as compute_source_anchored_diff_BASE
# and define the patched version below.
# =====================================================================


# =====================================================================
# PATCH FIX2D1 (ADDITIVE): Early aliases for canonical-for-render rebuild
# ---------------------------------------------------------------------
# Problem:
#   compute_source_anchored_diff() performs a late "display rebuild" and
#   looks up these callables via globals():
#     - rebuild_metrics_from_snapshots_analysis_canonical_v1
#     - rebuild_metrics_from_snapshots_schema_only_fix16
#   In Streamlit, the evolution run can invoke compute_source_anchored_diff()
#   BEFORE the later parts of this file finish executing/defining those
#   functions, leading to:
#     skip_reason_v19 = "fn_missing"
#   Which then leaves Current blank and produces no diff rows.
#
# Fix (render-layer only):
#   - Provide *early* safe aliases so the callables always exist at lookup time.
#   - Each alias does a best-effort late-binding to the final implementations
#     (if/when they are defined later), else falls back to the earliest safe
#     schema-only rebuild if available.
#
# Safety:
#   - Additive-only. Does NOT touch hashing, injection, snapshot attach, fetch,
#     extraction, or Analysis. Only prevents "fn_missing" gating.
# =====================================================================

# ===================== FIX2D84: Percent-year token sanitizer =====================

def _fix2dXX_hotfix_percent_year_token_sanitize_pmc_v1(
    pmc: dict,
    metric_schema_frozen: dict,
    debug: dict,
    label: str,
) -> dict:
    """
    Hotfix: for __percent keys, reject bare year tokens (1900-2100) unless the raw token itself
    contains strong percent evidence (%/percent/pct). Do NOT trust unit_tag alone.
    """
    if not isinstance(pmc, dict):
        return pmc

    def _is_percent_key(k: str) -> bool:
        if isinstance(k, str) and k.endswith("__percent"):
            return True
        sch = metric_schema_frozen.get(k) if isinstance(metric_schema_frozen, dict) else None
        if isinstance(sch, dict):
            dim = str(sch.get("dimension") or sch.get("unit_kind") or "").lower()
            if dim == "percent":
                return True
        return False

    def _yearlike_raw_token(raw: str) -> bool:
        if not raw:
            return False
        t = raw.strip()
        if len(t) == 4 and t.isdigit():
            y = int(t)
            return 1900 <= y <= 2100
        return False

    def _strong_percent_evidence_in_token(raw: str) -> bool:
        if not raw:
            return False
        s = raw.lower()
        return ("%" in s) or ("percent" in s) or ("pct" in s)

    rejected = []
    out = {}

    for k, v in pmc.items():
        if not _is_percent_key(k):
            out[k] = v
            continue

        raw = ""
        if isinstance(v, dict):
            ev = v.get("evidence")
            if isinstance(ev, dict):
                raw = str(ev.get("raw") or ev.get("raw_text") or ev.get("token") or "")
            if not raw:
                raw = str(v.get("raw") or "")

        if _yearlike_raw_token(raw) and not _strong_percent_evidence_in_token(raw):
            rej = {"canonical_key": k, "raw": raw}
            if isinstance(v, dict):
                rej["value_norm"] = v.get("value_norm")
                rej["anchor_hash"] = v.get("anchor_hash")
                rej["source_url"] = v.get("source_url")
                rej["method"] = v.get("method")
            rejected.append(rej)
            # DROP the key (so Evolution shows it as 'added' instead of prev=2040)
            continue

        out[k] = v

    dbg = debug.setdefault("fix2dXX_percent_year_token_sanitize_v1", {})
    dbg["label"] = label
    dbg["pmc_in_count"] = len(pmc)
    dbg["pmc_out_count"] = len(out)
    dbg["rejected_count"] = len(rejected)
    dbg["rejected_samples"] = rejected[:5]

    return out



def _fix2d86_sanitize_pmc_percent_year_tokens_v1(pmc: dict, metric_schema_frozen: dict, label: str):
    """
    FIX2D86 HOTFIX:
    For __percent keys, drop bindings where the chosen evidence is a bare year token (1900-2100),
    e.g. raw="2040" and value_norm=2040.0. This prevents prev=2040 for CAGR percent keys.
    """
    dbg = {
        "label": label,
        "pmc_in_count": len(pmc) if isinstance(pmc, dict) else 0,
        "pmc_out_count": 0,
        "dropped_count": 0,
        "dropped_samples": [],
    }
    if not isinstance(pmc, dict):
        return pmc, dbg

    def _is_percent_key(k: str) -> bool:
        if isinstance(k, str) and k.endswith("__percent"):
            return True
        sch = metric_schema_frozen.get(k) if isinstance(metric_schema_frozen, dict) else None
        if isinstance(sch, dict):
            dim = str(sch.get("dimension") or sch.get("unit_kind") or "").lower()
            return dim == "percent"
        return False

    def _yearlike_num(v) -> bool:
        try:
            x = float(v)
        except Exception:
            return False
        if abs(x - round(x)) > 1e-9:
            return False
        y = int(round(x))
        return 1900 <= y <= 2100

    def _extract_raw_token(vdict: dict) -> str:
        if not isinstance(vdict, dict):
            return ""
        ev = vdict.get("evidence")

        # evidence can be dict or list of dicts in your codebase
        if isinstance(ev, dict):
            return str(ev.get("raw") or ev.get("raw_text") or ev.get("token") or "")
        if isinstance(ev, list) and ev:
            for item in ev:
                if isinstance(item, dict):
                    raw = str(item.get("raw") or item.get("raw_text") or item.get("token") or "")
                    if raw:
                        return raw
        return str(vdict.get("raw") or "")

    def _raw_is_bare_year(raw: str) -> bool:
        if not raw:
            return False
        t = raw.strip().lower()
        # Must be "2040" or "2040.0" style, and MUST NOT contain percent markers
        if ("%" in t) or ("percent" in t) or ("pct" in t):
            return False
        # accept 4-digit integer, or integer with .0
        if t.isdigit() and len(t) == 4:
            y = int(t)
            return 1900 <= y <= 2100
        if t.endswith(".0"):
            base = t[:-2]
            if base.isdigit() and len(base) == 4:
                y = int(base)
                return 1900 <= y <= 2100
        return False

    out = {}
    for k, v in pmc.items():
        if not _is_percent_key(k):
            out[k] = v
            continue

        if isinstance(v, dict):
            val_norm = v.get("value_norm")
            raw = _extract_raw_token(v)
            if _yearlike_num(val_norm) and _raw_is_bare_year(raw):
                dbg["dropped_count"] += 1
                if len(dbg["dropped_samples"]) < 5:
                    dbg["dropped_samples"].append({
                        "canonical_key": k,
                        "value_norm": val_norm,
                        "raw": raw,
                        "source_url": v.get("source_url"),
                        "method": v.get("method"),
                        "anchor_hash": v.get("anchor_hash"),
                    })
                continue

        out[k] = v

    dbg["pmc_out_count"] = len(out)
    return out, dbg




def rebuild_metrics_from_snapshots_schema_only_fix16(prev_response: dict, snapshot_pool: list, web_context: dict = None):  # noqa: F811
    """
    FIX2Dxx hotfix:
    - Always run schema-only rebuild (late-binding alias) and then sanitize the output so that
      __percent keys can NEVER bind bare year tokens like "2040" unless the token itself contains
      strong percent evidence (%/percent/pct).
    - Emits debug counters under: prev_response['debug']['fix2dXX_percent_year_token_sanitize_v1']
    """
    pmc = {}

    # Prefer a later concrete implementation if it already exists (late-binding)
    try:
        fn = globals().get("_rebuild_metrics_from_snapshots_schema_only_fix16_impl")
        if callable(fn):
            pmc = fn(prev_response, snapshot_pool, web_context=web_context)
        else:
            fn = globals().get("rebuild_metrics_from_snapshots_schema_only")
            if callable(fn):
                try:
                    pmc = fn(prev_response, snapshot_pool, web_context=web_context)
                except TypeError:
                    pmc = fn(prev_response, snapshot_pool)
    except Exception:
        pmc = {}

    # ---- HOTFIX: percent keys must not bind bare year tokens (e.g., 2040) ----
    try:
        # Locate schema (prev_response can be full results dict or nested under results)
        metric_schema_frozen = {}
        if isinstance(prev_response, dict):
            metric_schema_frozen = (
                prev_response.get("metric_schema_frozen")
                or (prev_response.get("results", {}) if isinstance(prev_response.get("results"), dict) else {}).get("metric_schema_frozen")
                or {}
            )

        # Attach debug to prev_response (safe place that will be persisted in Analysis JSON)
        debug = prev_response.setdefault("debug", {}) if isinstance(prev_response, dict) else {}

        # Sanitize schema-only rebuild output
        pmc = _fix2dXX_hotfix_percent_year_token_sanitize_pmc_v1(
            pmc=pmc,
            metric_schema_frozen=metric_schema_frozen,
            debug=debug,
            label="schema_only_rebuild_fix16_alias",
        )
    except Exception:
        pass

    return pmc


def rebuild_metrics_from_snapshots_analysis_canonical_v1(prev_response: dict, snapshot_pool: list, web_context: dict = None):  # noqa: F811
    """Early alias: prefer the analysis-canonical rebuild if later-defined; else schema-only."""
    # Prefer a later concrete implementation if it already exists (late-binding)
    try:
        fn = globals().get("_rebuild_metrics_from_snapshots_analysis_canonical_v1_impl")
        if callable(fn):
            return fn(prev_response, snapshot_pool, web_context=web_context)
    except Exception:
        pass

    # Otherwise, fall back deterministically to schema-only
    try:
        fn = globals().get("rebuild_metrics_from_snapshots_schema_only_fix16")
        if callable(fn):
            return fn(prev_response, snapshot_pool, web_context=web_context)
    except Exception:
        return {}


def compute_source_anchored_diff(previous_data: dict, web_context: dict = None) -> dict:
    """
    Tight source-anchored evolution:
      - Prefer snapshots from analysis (baseline_sources_cache)
      - Optionally reconstruct snapshots from web_context.scraped_meta
      - If no valid snapshots: return not_found (no heuristic junk)

    Always returns a dict.
    """
    import re
    from datetime import datetime, timezone

    # FIX2D66_PROMOTE_INJECTED_URLS_IN_ATTACH (ADDITIVE)
    try:
        _qtxt = str((analysis or {}).get('question') or (analysis or {}).get('query') or '')
        web_context = _fix2d66_promote_injected_urls(web_context or {}, question_text=_qtxt, stage='analysis_attach')
    except Exception:
        pass

    def _now():
        return datetime.now(timezone.utc).isoformat()

    def _safe_int(x, default=0):
        try:
            return int(x)
        except Exception:
            return default

    def _fingerprint(text: str):
        try:
            fn = globals().get("fingerprint_text")
            if callable(fn):
                return fn(text or "")
        except Exception:
            pass
        try:
            return fingerprint_text(text or "")
        except Exception:
            return None

    # ============================================================
    # PATCH CSR_UNWRAP1 (ADDITIVE): robust nested retrieval helpers
    # Why:
    # - Some runs store rebuild essentials under primary_response or results.primary_response
    # - Evolution may look only at top-level keys, causing schema=0 / anchors=0
    # ============================================================
    def _get_nested(d, path, default=None):
        try:
            x = d
            for k in path:
                if not isinstance(x, dict):
                    return default
                x = x.get(k)
            return x if x is not None else default
        except Exception:
            return default

    def _first_present(d, paths, default=None):
        for p in paths:
            v = _get_nested(d, p, None)
            if v is not None:
                return v
        return default
    # ============================================================

    # =====================================================================
    # PATCH FIX41U (ADDITIVE): Evolution-side injected-URL diag prewire + replay visibility
    # Objective:
    # - Ensure compute_source_anchored_diff can ALWAYS populate web_context.diag_injected_urls
    #   even when the caller only supplies:
    #     * web_context["extra_urls"]
    #     * web_context["diag_extra_urls_ui_raw"]
    #     * web_context["diag_run_id"]
    # - Additionally, if the evolution UI did NOT supply extra_urls, record what the baseline
    #   analysis run had (if any) as "replayed_from_analysis_norm" for diagnostics only.
    # Safety:
    # - Purely additive diagnostics. Does NOT alter fastpath logic or hashing inputs.
    # =====================================================================
    def _fix41u_extract_injected_from_prev(prev: dict) -> dict:
        try:
            if not isinstance(prev, dict):
                return {}
            cand_paths = [
                ["results","debug","inj_trace_v1"],
                ["primary_response","results","debug","inj_trace_v1"],
                ["results","primary_response","results","debug","inj_trace_v1"],
                ["debug","inj_trace_v1"],
            ]
            for p in cand_paths:
                v = _get_nested(prev, p, None)
                if isinstance(v, dict) and v:
                    return v
        except Exception:
            return {}

    try:
        if web_context is None or not isinstance(web_context, dict):
            web_context = {}

        # Pull what the caller provided (Evolution UI should pass these)
        _fix41u_ui_raw = ""
        try:
            _fix41u_ui_raw = str(web_context.get("diag_extra_urls_ui_raw") or web_context.get("extra_urls_ui_raw") or "")
        except Exception:
            pass
            _fix41u_ui_raw = ""

        _fix41u_extra_urls = []
        try:
            _fix41u_extra_urls = _inj_diag_norm_url_list(web_context.get("extra_urls") or [])
        except Exception:
            pass
            _fix41u_extra_urls = []

        # If Evolution UI did not supply extras, capture what baseline analysis had (diagnostic only)
        _fix41u_prev_trace = _fix41u_extract_injected_from_prev(previous_data)
        _fix41u_replayed = []
        try:
            if not _fix41u_extra_urls and isinstance(_fix41u_prev_trace, dict):
                _fix41u_replayed = _inj_diag_norm_url_list(_fix41u_prev_trace.get("ui_norm") or [])
        except Exception:
            pass
            _fix41u_replayed = []

        # Ensure diag container exists
        web_context.setdefault("diag_injected_urls", {})
        if isinstance(web_context.get("diag_injected_urls"), dict):
            _d = web_context["diag_injected_urls"]
            # run_id continuity
            try:
                _d.setdefault("run_id", str(web_context.get("diag_run_id") or ""))
            except Exception:
                pass
            # UI-provided inputs (preferred truth)
            _d.setdefault("ui_raw", _fix41u_ui_raw)
            _d.setdefault("ui_norm", list(_fix41u_extra_urls))
            _d.setdefault("intake_norm", list(_fix41u_extra_urls))
            # Replay visibility (diagnostic only; NOT used for hashing unless caller also provided extras)
            if _fix41u_replayed:
                _d.setdefault("replayed_from_analysis_norm", list(_fix41u_replayed))
    except Exception:
        pass
    # =====================================================================
    # END PATCH FIX41U
    # =====================================================================


    # =====================================================================
    # PATCH HF5 (ADDITIVE): rehydrate previous_data from HistoryFull if wrapper
    # Why:
    # - Some UI/Sheets paths provide a summarized wrapper that lacks primary_response,
    #   metric_schema_frozen, metric_anchors, baseline_sources_cache, etc.
    # - If a full_store_ref pointer exists, load the full payload deterministically.
    #
    # NOTE:
    # - Do NOT write to `output` here (output not built yet). We stash flags
    #   and attach them after `output = {...}` is created.
    # =====================================================================
    _prev_rehydrated = False
    _prev_rehydrated_ref = ""

    try:
        if isinstance(previous_data, dict):
            _pr = previous_data.get("primary_response")

            # Determine if we are missing rebuild essentials
            _need = (
                (not isinstance(_pr, dict))
                or (not _pr)
                or (not isinstance(_pr.get("metric_schema_frozen"), dict))
            )

            if _need:
                # Explicit line (requested): simplest location first
                ref = previous_data.get("full_store_ref", "")  # <-- requested line

                # Then fall back to other known wrapper locations (more robust)
                _ref = (
                    ref
                    or (previous_data.get("results") or {}).get("full_store_ref")
                    or (isinstance(_pr, dict) and _pr.get("full_store_ref"))
                    or ""
                )

                # Last-ditch deterministic fallback: if wrapper carries _sheet_id
                if (not _ref) and isinstance(previous_data.get("_sheet_id"), str) and previous_data.get("_sheet_id"):
                    _ref = f"gsheet:HistoryFull:{previous_data.get('_sheet_id')}"

                if isinstance(_ref, str) and _ref.startswith("gsheet:"):
                    parts = _ref.split(":")
                    _ws_title = parts[1] if len(parts) > 1 and parts[1] else "HistoryFull"
                    _aid = parts[2] if len(parts) > 2 else ""
                    full = load_full_history_payload_from_sheet(_aid, worksheet_title=_ws_title) if _aid else {}
                    if isinstance(full, dict) and full:
                        previous_data = full
                        _prev_rehydrated = True
                        _prev_rehydrated_ref = _ref
    except Exception:
        pass
    # =====================================================================

    # ---------- Pull baseline snapshots (VALID only) ----------
    snapshot_origin = "none"
    baseline_sources_cache = []

    try:
        if isinstance(previous_data, dict):
            # 1) results.baseline_sources_cache (preferred)
            r = previous_data.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache"), list):
                baseline_sources_cache = r.get("baseline_sources_cache") or []
                if baseline_sources_cache:
                    snapshot_origin = "analysis_results_cache"

            # 2) top-level baseline_sources_cache
            if not baseline_sources_cache and isinstance(previous_data.get("baseline_sources_cache"), list):
                baseline_sources_cache = previous_data.get("baseline_sources_cache") or []
                if baseline_sources_cache:
                    snapshot_origin = "analysis_top_level_cache"
    except Exception:
        pass
        baseline_sources_cache = []

    # =====================================================================
    # PATCH ES1B (ADDITIVE): broaden snapshot discovery (legacy storage shapes)
    # =====================================================================
    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            pr = previous_data.get("primary_response") or {}
            if isinstance(pr, dict):
                # A) primary_response.results.baseline_sources_cache
                r2 = pr.get("results")
                if (not baseline_sources_cache) and isinstance(r2, dict) and isinstance(r2.get("baseline_sources_cache"), list):
                    baseline_sources_cache = r2.get("baseline_sources_cache") or []
                    if baseline_sources_cache:
                        snapshot_origin = "primary_response_results_cache"

                # B) primary_response.baseline_sources_cache
                if (not baseline_sources_cache) and isinstance(pr.get("baseline_sources_cache"), list):
                    baseline_sources_cache = pr.get("baseline_sources_cache") or []
                    if baseline_sources_cache:
                        snapshot_origin = "primary_response_top_level_cache"

                # C) primary_response.results.source_results (reconstruct minimal snapshot shape)
                if (not baseline_sources_cache) and isinstance(r2, dict) and isinstance(r2.get("source_results"), list):
                    rebuilt_sr = []
                    for sr in (r2.get("source_results") or []):
                        if not isinstance(sr, dict):
                            continue
                        u = sr.get("source_url") or sr.get("url")
                        ex = sr.get("extracted_numbers")
                        if u and isinstance(ex, list) and ex:
                            rebuilt_sr.append({
                                "source_url": u,
                                "extracted_numbers": ex,
                                "clean_text": sr.get("clean_text") or sr.get("content") or "",
                                "fingerprint": sr.get("fingerprint"),
                                "fetched_at": sr.get("fetched_at"),
                            })
                    rebuilt_sr.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))
                    if rebuilt_sr:
                        baseline_sources_cache = rebuilt_sr
                        snapshot_origin = "primary_response_source_results_rebuild"

        # D) previous_data.results.source_results fallback
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            r3 = previous_data.get("results")
            if isinstance(r3, dict) and isinstance(r3.get("source_results"), list):
                rebuilt_sr2 = []
                for sr in (r3.get("source_results") or []):
                    if not isinstance(sr, dict):
                        continue
                    u = sr.get("source_url") or sr.get("url")
                    ex = sr.get("extracted_numbers")
                    if u and isinstance(ex, list) and ex:
                        rebuilt_sr2.append({
                            "source_url": u,
                            "extracted_numbers": ex,
                            "clean_text": sr.get("clean_text") or sr.get("content") or "",
                            "fingerprint": sr.get("fingerprint"),
                            "fetched_at": sr.get("fetched_at"),
                        })
                rebuilt_sr2.sort(key=lambda d: (str(d.get("source_url") or ""), str(d.get("fingerprint") or "")))
                if rebuilt_sr2:
                    baseline_sources_cache = rebuilt_sr2
                    snapshot_origin = "analysis_source_results_rebuild"
    except Exception:
        pass

    # =====================================================================
    # PATCH SS6C (ADDITIVE): evidence_records fallback for snapshots (evolution-time)
    # =====================================================================
    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            _er = None
            if isinstance(previous_data.get("results"), dict):
                _er = previous_data["results"].get("evidence_records")
            if _er is None:
                _er = previous_data.get("evidence_records")
            _rebuilt = build_baseline_sources_cache_from_evidence_records(_er)
            if isinstance(_rebuilt, list) and _rebuilt:
                baseline_sources_cache = _rebuilt
                snapshot_origin = "evidence_records_rebuild"
    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH ES1C (ADDITIVE): validate snapshot shape & prepare debug metadata
    # =====================================================================
    _snapshot_debug = None
    try:
        _raw_len = int(len(baseline_sources_cache)) if isinstance(baseline_sources_cache, list) else 0
        _kept = []
        if isinstance(baseline_sources_cache, list):
            for s in baseline_sources_cache:
                if not isinstance(s, dict):
                    continue
                u = s.get("source_url") or s.get("url")
                ex = s.get("extracted_numbers")
                if u and isinstance(ex, list):
                    _kept.append(s)
        _kept.sort(key=lambda d: (str(d.get("source_url") or d.get("url") or ""), str(d.get("fingerprint") or "")))
        baseline_sources_cache = _kept
        _snapshot_debug = {
            "origin": snapshot_origin,
            "raw_count": _raw_len,
            "valid_count": int(len(baseline_sources_cache)),
            "example_urls": [x.get("source_url") or x.get("url") for x in (baseline_sources_cache[:3] if isinstance(baseline_sources_cache, list) else [])],
            "prev_keys": sorted(list(previous_data.keys()))[:40] if isinstance(previous_data, dict) else [],
        }
    except Exception:
        pass
    # =====================================================================

    # 3) reconstruct from web_context.scraped_meta (if provided)
    if (not baseline_sources_cache) and isinstance(web_context, dict):
        try:
            scraped_meta = web_context.get("scraped_meta") or {}
            rebuilt = []
            if isinstance(scraped_meta, dict):
                for url, meta in scraped_meta.items():
                    if not isinstance(meta, dict):
                        continue
                    content = meta.get("clean_text") or meta.get("content") or ""
                    fp = meta.get("fingerprint") or _fingerprint(content)
                    if not fp or len(content or "") < 800:
                        continue
                    nums = meta.get("extracted_numbers") or []
                    if not isinstance(nums, list):
                        nums = []
                    rebuilt.append({
                        "url": url,
                        "status": meta.get("status") or "fetched",
                        "status_detail": meta.get("status_detail") or "",
                        "numbers_found": _safe_int(meta.get("numbers_found"), default=len(nums)),
                        "fetched_at": meta.get("fetched_at") or _now(),
                        "fingerprint": fp,
                        "content_type": meta.get("content_type") or "",
                        "extracted_numbers": [
                            {
                                "value": n.get("value"),
                                "unit": n.get("unit"),
                                "raw": n.get("raw"),
                                "context_snippet": (n.get("context_snippet") or n.get("context") or "")[:200],
                                "anchor_hash": n.get("anchor_hash"),
                                "is_junk": n.get("is_junk"),
                                "junk_reason": n.get("junk_reason"),
                                "unit_tag": n.get("unit_tag"),
                                "unit_family": n.get("unit_family"),
                                "base_unit": n.get("base_unit"),
                                "multiplier_to_base": n.get("multiplier_to_base"),
                                "value_norm": n.get("value_norm"),
                                "start_idx": n.get("start_idx"),
                                "end_idx": n.get("end_idx"),
                                "source_url": n.get("source_url") or url,
                            }
                            for n in nums if isinstance(n, dict)
                        ]
                    })
            if rebuilt:
                baseline_sources_cache = rebuilt
                snapshot_origin = "web_context_scraped_meta"
        except Exception:
            pass

    # Also count invalid snapshots for debug (if present)
    invalid_count = 0
    try:
        if isinstance(previous_data, dict):
            r = previous_data.get("results")
            if isinstance(r, dict) and isinstance(r.get("baseline_sources_cache_invalid"), list):
                invalid_count = len(r.get("baseline_sources_cache_invalid") or [])
    except Exception:
        pass
        invalid_count = 0

    # ---------- Prepare stable default output ----------
    output = {
        "status": "success",
        "message": "",
        "sources_checked": 0,
        "sources_fetched": 0,
        "numbers_extracted_total": 0,
        "stability_score": 0.0,
        "summary": {
            "total_metrics": 0,
            "metrics_found": 0,
            "metrics_increased": 0,
            "metrics_decreased": 0,
            "metrics_unchanged": 0,
        },
        "metric_changes": [],
        "source_results": [],
        "interpretation": "",
        "snapshot_origin": snapshot_origin,
        "valid_snapshot_count": len(baseline_sources_cache or []),
        "invalid_snapshot_count": int(invalid_count),
        "generated_at": _now(),
    }

    # =====================================================================
    # PATCH FIX35 (ADDITIVE): emit origin + hash debugging for process-of-elimination
    # - Always stamp CODE_VERSION into output
    # - Create output['debug'] container (non-breaking)
    # - Track fastpath eligibility + reason in a deterministic way
    # =====================================================================
    try:
        output["code_version"] = _yureeka_get_code_version()
    except Exception:
        pass
    try:
        _yureeka_lock_version_globals_v1()
        _yureeka_ensure_final_bindings_v1()
        if not isinstance(output.get("debug"), dict):
            output["debug"] = {}

        # REFACTOR17: Binding manifest should reflect the *actual* Diff Panel V2 entrypoint.
        _fn_v2 = None
        _bf_v2 = ""
        for _cand_name in ["build_diff_metrics_panel_v2__rows", "build_diff_metrics_panel_v2"]:
            try:
                _cand = globals().get(_cand_name)
            except Exception:
                _cand = None
            if callable(_cand):
                _fn_v2 = _cand
                _bf_v2 = _cand_name
                break
        try:
            if _bf_v2:
                globals()["_YUREEKA_DIFF_PANEL_V2_ENTRYPOINT_BOUND_FROM"] = _bf_v2
        except Exception:
            pass

        # Legacy diff entrypoint (may be absent during early Streamlit execution).
        _fn = None
        _bf = ""
        try:
            _cand = globals().get("diff_metrics_by_name")
        except Exception:
            _cand = None
        if callable(_cand):
            _fn = _cand
            _bf = "diff_metrics_by_name"
        if not callable(_fn):
            for _cand_name in [
                "diff_metrics_by_name",
                "_yureeka_diff_metrics_by_name_v24",
                "diff_metrics_by_name_V24_BASE",
                "_refactor09_diff_metrics_by_name",
                "diff_metrics_by_name_FIX41_V34C_UNWRAP",
                "diff_metrics_by_name_FIX41_V34_ANCHOR_JOIN",
                "diff_metrics_by_name_FIX40_V32_PREFER_PMC",
                "diff_metrics_by_name_FIX34_V24_STRICT",
                "diff_metrics_by_name_FIX33_V23_CANONICAL_CLEAR",
                "diff_metrics_by_name_FIX2D34",
            ]:
                try:
                    _cand = globals().get(_cand_name)
                except Exception:
                    _cand = None
                if callable(_cand):
                    _fn = _cand
                    _bf = _cand_name
                    try:
                        globals()["diff_metrics_by_name"] = _cand
                    except Exception:
                        pass
                    break

        # Last resort: scan globals for any callable that looks like a diff_metrics_by_name variant.
        if not callable(_fn):
            try:
                _hits = []
                for _k, _v in list(globals().items()):
                    if "diff_metrics_by_name" in str(_k) and callable(_v):
                        _hits.append((_k, _v))
                if _hits:
                    _hits.sort(key=lambda t: str(t[0]))
                    _bf, _fn = _hits[-1]
            except Exception:
                pass

        try:
            if _bf:
                globals()["_YUREEKA_DIFF_METRICS_BY_NAME_BOUND_FROM"] = _bf
        except Exception:
            pass

        _auth_tag = ""
        _auth_src = ""
        _auth_set_ok = False
        try:
            if callable(_fn):
                _auth_set_ok = bool(_yureeka_set_authoritative_binding_v1(_fn, _yureeka_get_code_version()))
        except Exception:
            _auth_set_ok = False

        try:
            _auth_tag = str(getattr(_fn, "__YUREEKA_AUTHORITATIVE_BINDING__", "") or "")
            if _auth_tag:
                _auth_src = "attr"
        except Exception:
            _auth_tag = ""
        if not _auth_tag:
            try:
                _auth_tag = str(globals().get("_YUREEKA_DIFF_METRICS_BY_NAME_AUTHORITATIVE_TAG") or "")
                if _auth_tag:
                    _auth_src = "globals"
            except Exception:
                _auth_tag = ""

        _man = {
            "code_version": _yureeka_get_code_version(),
            "authority_manifest_v1": _yureeka_authority_manifest_v1(),
            "final_bindings_version": str(globals().get("_YUREEKA_FINAL_BINDINGS_VERSION") or ""),

            # Diff Panel V2 (authoritative for metric_changes_v2)
            "diff_panel_v2_entrypoint_name": str(getattr(_fn_v2, "__name__", "") or ""),
            "diff_panel_v2_entrypoint_qualname": str(getattr(_fn_v2, "__qualname__", "") or ""),
            "diff_panel_v2_entrypoint_module": str(getattr(_fn_v2, "__module__", "") or ""),
            "diff_panel_v2_entrypoint_id": str(id(_fn_v2)) if _fn_v2 is not None else "",
            "diff_panel_v2_entrypoint_bound_from": str(_bf_v2 or ""),

            # Legacy diff (best-effort)
            "diff_metrics_by_name_authoritative": str(_auth_tag or ""),
            "diff_metrics_by_name_authoritative_source": str(_auth_src or ""),
            "diff_metrics_by_name_authoritative_set_ok": bool(_auth_set_ok),
            "diff_metrics_by_name_name": str(getattr(_fn, "__name__", "") or ""),
            "diff_metrics_by_name_qualname": str(getattr(_fn, "__qualname__", "") or ""),
            "diff_metrics_by_name_module": str(getattr(_fn, "__module__", "") or ""),
            "diff_metrics_by_name_id": str(id(_fn)) if _fn is not None else "",
            "diff_metrics_by_name_bound_from": str(_bf or ""),
        }

        if not _man.get("diff_panel_v2_entrypoint_name"):
            _man["warning_v2_missing"] = "Diff Panel V2 entrypoint not resolved at manifest time (possible early Streamlit trigger before later defs)."
        if not _man.get("diff_metrics_by_name_name"):
            _man["note_legacy_missing"] = "Legacy diff_metrics_by_name not resolved at manifest time; Diff Panel V2 is authoritative for metric_changes_v2."

        output["debug"]["binding_manifest_v1"] = _man
    except Exception:
        pass



    try:
        if not isinstance(output.get("debug"), dict):
            output["debug"] = {}
        output["debug"].setdefault("fix35", {})
        output["debug"]["fix35"]["current_metrics_origin"] = "unknown"
        output["debug"]["fix35"]["fastpath_eligible"] = False
        output["debug"]["fix35"]["fastpath_reason"] = ""
    except Exception:
        pass

    # Attach debug flags (rehydration + snapshot_debug)
    try:
        if _prev_rehydrated:
            output["previous_data_rehydrated"] = True
            output["previous_data_full_store_ref"] = _prev_rehydrated_ref
    except Exception:
        pass
    try:
        if isinstance(_snapshot_debug, dict) and _snapshot_debug:
            output["snapshot_debug"] = _snapshot_debug
    except Exception:
        pass

    # =====================================================================
    # PATCH SS6 (ADDITIVE, REQUIRED): last-chance snapshot rehydration
    # =====================================================================
    try:
        if not baseline_sources_cache and isinstance(previous_data, dict):
            _ref = previous_data.get("snapshot_store_ref") or (previous_data.get("results") or {}).get("snapshot_store_ref")
            _hash = previous_data.get("source_snapshot_hash") or (previous_data.get("results") or {}).get("source_snapshot_hash")

            # ============================================================
            # PATCH FIX41I_SS6_STABLE (ADDITIVE): prefer v2/stable snapshot refs & hashes
            # Why:
            # - Analysis now emits stable/v2 snapshot hashes (source_snapshot_hash_v2 / _stable) and
            #   snapshot_store_ref_v2 pointing at the same Snapshots row key.
            # - Evolution must prefer these fields to keep fastpath alignment intact.
            # ============================================================
            try:
                _ref_v2 = previous_data.get("snapshot_store_ref_v2") or previous_data.get("snapshot_store_ref_stable")
                if (not _ref_v2) and isinstance(previous_data.get("results"), dict):
                    _ref_v2 = (previous_data.get("results") or {}).get("snapshot_store_ref_v2") or (previous_data.get("results") or {}).get("snapshot_store_ref_stable")
                if isinstance(_ref_v2, str) and _ref_v2:
                    _ref = _ref_v2  # prefer stable ref
            except Exception:
                pass

            try:
                _hash_stable = previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2")
                if (not _hash_stable) and isinstance(previous_data.get("results"), dict):
                    _hash_stable = (previous_data.get("results") or {}).get("source_snapshot_hash_stable") or (previous_data.get("results") or {}).get("source_snapshot_hash_v2")
                if isinstance(_hash_stable, str) and _hash_stable:
                    _hash = _hash_stable  # prefer stable hash
            except Exception:
                pass
            # ============================================================

            if isinstance(_ref, str) and _ref.startswith("gsheet:"):
                parts = _ref.split(":")
                _ws_title = parts[1] if len(parts) > 1 and parts[1] else "Snapshots"
                _h = parts[2] if len(parts) > 2 else ""
                baseline_sources_cache = load_full_snapshots_from_sheet(_h, worksheet_title=_ws_title) if _h else []
                if baseline_sources_cache:
                    output["snapshot_origin"] = "sheet_snapshot_store_ref"

            if not baseline_sources_cache and isinstance(_hash, str) and _hash:
                baseline_sources_cache = load_full_snapshots_from_sheet(_hash, worksheet_title="Snapshots")
                if baseline_sources_cache:
                    output["snapshot_origin"] = "sheet_source_snapshot_hash"

            if not baseline_sources_cache and isinstance(_ref, str) and _ref and not _ref.startswith("gsheet:"):
                baseline_sources_cache = load_full_snapshots_local(_ref)
                if baseline_sources_cache:
                    output["snapshot_origin"] = "local_snapshot_store_ref"

            if isinstance(baseline_sources_cache, list):
                output["valid_snapshot_count"] = len(baseline_sources_cache)
    except Exception:
        pass
    # =====================================================================


    # =====================================================================
    # PATCH REFACTOR39_SNAPSHOT_STORE_FALLBACK (ADDITIVE)
    # Purpose:
    # - During HistoryFull persistence we may omit baseline_sources_cache to avoid Sheets cell limits,
    #   and instead persist snapshots in the Snapshots worksheet / local snapshot store with a ref/hash.
    # - Source-anchored evolution MUST be able to rehydrate baseline snapshots from:
    #     * snapshot_store_ref / snapshot_store_ref_v2 (gsheet:Snapshots:<hash> OR local path)
    #     * source_snapshot_hash_v2 / source_snapshot_hash
    # Behavior:
    # - If baseline_sources_cache is empty after normal discovery, attempt to load snapshots deterministically.
    # - Still strict: if we cannot load snapshots, we remain snapshot-gated (no fabricated matches).
    # =====================================================================
    _snapshot_store_debug = {}
    try:
        if (not baseline_sources_cache) and isinstance(previous_data, dict):
            _res = previous_data.get("results") if isinstance(previous_data.get("results"), dict) else {}
            _pr = previous_data.get("primary_response") if isinstance(previous_data.get("primary_response"), dict) else {}
            _pr_res = _pr.get("results") if isinstance(_pr.get("results"), dict) else {}

            # Prefer explicit refs; fall back to hashes
            _store_ref = (
                previous_data.get("snapshot_store_ref")
                or (_res.get("snapshot_store_ref") if isinstance(_res, dict) else "")
                or (_pr.get("snapshot_store_ref") if isinstance(_pr, dict) else "")
                or (_pr_res.get("snapshot_store_ref") if isinstance(_pr_res, dict) else "")
                or ""
            )

            _store_ref_v2 = (
                previous_data.get("snapshot_store_ref_v2")
                or (_res.get("snapshot_store_ref_v2") if isinstance(_res, dict) else "")
                or (_pr.get("snapshot_store_ref_v2") if isinstance(_pr, dict) else "")
                or (_pr_res.get("snapshot_store_ref_v2") if isinstance(_pr_res, dict) else "")
                or ""
            )

            _ssh_v2 = (
                previous_data.get("source_snapshot_hash_v2")
                or (_res.get("source_snapshot_hash_v2") if isinstance(_res, dict) else "")
                or (_pr.get("source_snapshot_hash_v2") if isinstance(_pr, dict) else "")
                or (_pr_res.get("source_snapshot_hash_v2") if isinstance(_pr_res, dict) else "")
                or ""
            )

            _ssh_v1 = (
                previous_data.get("source_snapshot_hash")
                or (_res.get("source_snapshot_hash") if isinstance(_res, dict) else "")
                or (_pr.get("source_snapshot_hash") if isinstance(_pr, dict) else "")
                or (_pr_res.get("source_snapshot_hash") if isinstance(_pr_res, dict) else "")
                or ""
            )

            def _extract_hash(ref: str) -> str:
                try:
                    ref = str(ref or "")
                    if ref.startswith("gsheet:Snapshots:"):
                        return ref.split(":")[-1]
                except Exception:
                    pass
                return ""

            _hash_to_load = _extract_hash(_store_ref_v2) or _extract_hash(_store_ref) or str(_ssh_v2 or "") or str(_ssh_v1 or "")

            _snapshot_store_debug = {
                "store_ref": str(_store_ref or ""),
                "store_ref_v2": str(_store_ref_v2 or ""),
                "ssh_v2_present": bool(_ssh_v2),
                "ssh_v1_present": bool(_ssh_v1),
                "hash_to_load": str(_hash_to_load or ""),
            }

            _loaded = []
            _loaded_origin = ""

            # 1) Load by explicit gsheet ref (v2 then v1)
            if isinstance(_store_ref_v2, str) and _store_ref_v2.startswith("gsheet:Snapshots:"):
                try:
                    _loaded = load_full_snapshots_from_sheet(_store_ref_v2.split(":")[-1], worksheet_title="Snapshots")
                    _loaded_origin = "gsheet_ref_v2"
                except Exception:
                    pass

            if (not _loaded) and isinstance(_store_ref, str) and _store_ref.startswith("gsheet:Snapshots:"):
                try:
                    _loaded = load_full_snapshots_from_sheet(_store_ref.split(":")[-1], worksheet_title="Snapshots")
                    _loaded_origin = "gsheet_ref_v1"
                except Exception:
                    pass

            # 2) Load by local store ref if it looks like a path
            if (not _loaded) and isinstance(_store_ref, str) and _store_ref and (not _store_ref.startswith("gsheet:")):
                try:
                    _loaded = load_full_snapshots_local(_store_ref)
                    _loaded_origin = "local_ref"
                except Exception:
                    pass

            # 3) Load by hash (sheet first, then deterministic local path)
            if (not _loaded) and _hash_to_load:
                try:
                    _loaded = load_full_snapshots_from_sheet(str(_hash_to_load), worksheet_title="Snapshots")
                    _loaded_origin = "gsheet_hash"
                except Exception:
                    pass

            if (not _loaded) and _hash_to_load:
                try:
                    # Deterministic path used by store_full_snapshots_local
                    import os
                    _p = os.path.join(_snapshot_store_dir(), f"{str(_hash_to_load)}.json")
                    _loaded = load_full_snapshots_local(_p)
                    if _loaded:
                        _loaded_origin = "local_hash_path"
                except Exception:
                    pass

            if isinstance(_loaded, list) and _loaded:
                baseline_sources_cache = _loaded
                snapshot_origin = f"snapshot_store_fallback:{_loaded_origin}"
                _snapshot_store_debug["loaded_count"] = int(len(_loaded))
                _snapshot_store_debug["loaded_origin"] = str(_loaded_origin)
            else:
                _snapshot_store_debug["loaded_count"] = 0
                _snapshot_store_debug["loaded_origin"] = str(_loaded_origin or "none")
    except Exception:
        pass

    # Re-validate snapshot shape after fallback load (keeps strict invariants)
    try:
        if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
            _kept2 = []
            for s in baseline_sources_cache:
                if not isinstance(s, dict):
                    continue
                u = s.get("source_url") or s.get("url")
                ex = s.get("extracted_numbers")
                # Some legacy stores use "numbers" instead of "extracted_numbers"
                if ex is None and isinstance(s.get("numbers"), list):
                    try:
                        s["extracted_numbers"] = s.get("numbers") or []
                        ex = s.get("extracted_numbers")
                    except Exception:
                        pass
                if u and isinstance(ex, list):
                    _kept2.append(s)
            _kept2.sort(key=lambda d: (str(d.get("source_url") or d.get("url") or ""), str(d.get("fingerprint") or "")))
            baseline_sources_cache = _kept2
            # Update debug if available
            if isinstance(_snapshot_debug, dict):
                _snapshot_debug["origin"] = snapshot_origin
                _snapshot_debug["valid_count"] = int(len(baseline_sources_cache))
    except Exception:
        pass
    # =====================================================================

    # If no valid snapshots, return "not_found"
    if not baseline_sources_cache:
        try:
            output.setdefault("debug", {})
            if isinstance(output.get("debug"), dict):
                if isinstance(_snapshot_debug, dict):
                    output["debug"]["snapshot_debug_v1"] = _snapshot_debug
                if isinstance(_snapshot_store_debug, dict) and _snapshot_store_debug:
                    output["debug"]["snapshot_store_debug_v1"] = _snapshot_store_debug
        except Exception:
            pass
        output["status"] = "failed"
        output["message"] = "No valid snapshots available for source-anchored evolution. (Snapshot store fallback attempted; no re-fetch / no heuristic matching performed.)"
        output["interpretation"] = "Snapshot-gated: evolution refused to fabricate matches without valid cached source text."
        # PATCH FIX2D20 (ADD): trace year-like commits in primary_metrics_canonical
        _fix2d20_trace_year_like_commits(output, stage=str((output or {}).get('results',{}).get('debug',{}).get('stage') or 'evolution'), callsite='compute_source_anchored_diff_return')
        return output    # ---------- Use your existing deterministic metric diff helper ----------
    prev_response = (previous_data or {}).get("primary_response", {}) or {}

    # =====================================================================
    # PATCH HF6 (ADDITIVE): tolerate previous_data being the primary_response itself
    # =====================================================================
    try:
        if (not isinstance(prev_response, dict) or not prev_response) and isinstance(previous_data, dict):
            if isinstance(previous_data.get("primary_metrics_canonical"), dict) or isinstance(previous_data.get("metric_schema_frozen"), dict):
                prev_response = previous_data
    except Exception:
        pass
    # =====================================================================

    prev_metrics = prev_response.get("primary_metrics_canonical") or prev_response.get("primary_metrics") or {}

    # ============================================================
    # PATCH CSR_INPUTS1 (ADDITIVE): normalize prev schema/anchors/canon
    # (safe alias for prior `prev_analysis` usage)
    # ============================================================
    prev_analysis = previous_data  # PATCH CSR_INPUTS1_ALIAS (ADDITIVE)
    try:
        prev_schema = _first_present(prev_analysis, [
            ("metric_schema_frozen",),
            ("primary_response", "metric_schema_frozen"),
            ("results", "metric_schema_frozen"),
            ("results", "primary_response", "metric_schema_frozen"),
        ], default=None)

        prev_canon = _first_present(prev_analysis, [
            ("primary_metrics_canonical",),
            ("primary_response", "primary_metrics_canonical"),
            ("results", "primary_metrics_canonical"),
            ("results", "primary_response", "primary_metrics_canonical"),
        ], default=None)

        prev_anchors = _first_present(prev_analysis, [
            ("metric_anchors",),
            ("primary_response", "metric_anchors"),
            ("results", "metric_anchors"),
            ("results", "primary_response", "metric_anchors"),
        ], default=None)

        if (not isinstance(prev_schema, dict) or not prev_schema) and isinstance(prev_canon, dict) and prev_canon:
            try:
                fn = globals().get("freeze_metric_schema")
                if callable(fn):
                    prev_schema = fn(prev_canon)
            except Exception:
                pass
    except Exception:
        pass
    # ============================================================



    # =====================================================================
    # PATCH FIX2D73 (ADDITIVE): ensure prev_response carries baseline canonical metrics
    # Why:
    # - Diff Panel V2 consumes prev_response.primary_metrics_canonical.
    # - HistoryFull rehydrate can place canonical metrics under nested containers
    #   (e.g., previous_data.results.primary_metrics_canonical), leaving prev_response empty.
    # What:
    # - If prev_canon exists, copy into prev_response.primary_metrics_canonical when missing.
    # - Also expose at top-level previous_data.primary_metrics_canonical (debug/compat).
    # - Record debug counts for closure verification.
    # =====================================================================
    try:
        if isinstance(prev_response, dict):
            if (not isinstance(prev_response.get("primary_metrics_canonical"), dict)) or (not prev_response.get("primary_metrics_canonical")):
                if isinstance(prev_canon, dict) and prev_canon:
                    prev_response["primary_metrics_canonical"] = prev_canon
        if isinstance(previous_data, dict):
            if (not isinstance(previous_data.get("primary_metrics_canonical"), dict)) or (not previous_data.get("primary_metrics_canonical")):
                if isinstance(prev_canon, dict) and prev_canon:
                    previous_data["primary_metrics_canonical"] = prev_canon
    except Exception:
        pass

    try:
        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            output["debug"].setdefault("fix2d73", {})
            if isinstance(output["debug"].get("fix2d73"), dict):
                output["debug"]["fix2d73"].update({
                    "prev_canon_count": int(len(prev_canon)) if isinstance(prev_canon, dict) else 0,
                    "prev_response_pmc_count": int(len(prev_response.get("primary_metrics_canonical") or {})) if isinstance(prev_response, dict) and isinstance(prev_response.get("primary_metrics_canonical"), dict) else 0,
                    "previous_data_top_pmc_count": int(len(previous_data.get("primary_metrics_canonical") or {})) if isinstance(previous_data, dict) and isinstance(previous_data.get("primary_metrics_canonical"), dict) else 0,
                })
    except Exception:
        pass
    # =====================================================================
    # Ensure schema/anchors are available inside prev_response (additive copies)
    try:
        if isinstance(prev_response, dict) and not isinstance(prev_response.get("metric_schema_frozen"), dict):
            if isinstance(previous_data.get("metric_schema_frozen"), dict):
                prev_response["metric_schema_frozen"] = previous_data.get("metric_schema_frozen")
    except Exception:
        pass
    try:
        if isinstance(prev_response, dict) and not isinstance(prev_response.get("metric_anchors"), dict):
            if isinstance(previous_data.get("metric_anchors"), dict):
                prev_response["metric_anchors"] = previous_data.get("metric_anchors")
    except Exception:
        pass

    # =====================================================================
    # PATCH FIX31 (ADDITIVE): authoritative fast-path when sources + data unchanged
    #
    # Principle:
    #   If the source snapshot inputs are proven unchanged, do NOT perform any
    #   anchor-based selection or rebuild "gymnastics". Reuse the already
    #   processed + schema-gated metrics from the previous analysis payload and
    #   publish directly.
    #
    # Implementation notes:
    #   - We compute a stable hash from baseline_sources_cache[*].extracted_numbers
    #     using a reduced, order-independent projection.
    #   - If it matches previous_data/source_snapshot_hash AND a prior processed
    #     canonical metrics dict exists, we set current_metrics to prev_metrics
    #     and force anchors to be ignored by short-circuiting _get_metric_anchors().
    #   - This is purely additive and does not remove legacy paths.
    # =====================================================================
    _fix31_authoritative_reuse = False
    try:
        import json as _fix31_json
        import hashlib as _fix31_hashlib

        def _fix31_stable_dumps(obj):
            try:
                return _fix31_json.dumps(obj, sort_keys=True, ensure_ascii=False, separators=(",", ":"))
            except Exception:
                pass
                # last resort
                return str(obj)

        def _fix31_snapshot_fingerprint(bsc):
            # Reduced projection: stable across benign field additions/ordering
            rows = []
            for sr in (bsc or []):
                if not isinstance(sr, dict):
                    continue
                u = sr.get("source_url") or sr.get("url") or ""
                nums = []
                for n in (sr.get("extracted_numbers") or []):
                    if not isinstance(n, dict):
                        continue
                    nums.append({
                        "anchor_hash": n.get("anchor_hash") or "",
                        "value_norm": n.get("value_norm"),
                        "unit_tag": n.get("unit_tag") or "",
                        "unit": n.get("unit") or n.get("unit_norm") or "",
                        "currency": n.get("currency") or n.get("currency_symbol") or "",
                        "is_percent": bool(n.get("is_percent") or n.get("has_percent")),
                        "is_junk": bool(n.get("is_junk")),
                    })
                # order-independent for candidates
                nums = sorted(nums, key=lambda x: (_fix31_stable_dumps(x)))
                rows.append({"source_url": u, "extracted_numbers": nums})
            rows = sorted(rows, key=lambda r: r.get("source_url") or "")
            payload = _fix31_stable_dumps(rows).encode("utf-8", errors="ignore")
            return _fix31_hashlib.sha256(payload).hexdigest()

        # =========================
        # PATCH FIX37 (ADD): stable snapshot hash for fastpath alignment
        # - Use the SAME hash function as analysis (compute_source_snapshot_hash_v2) whenever possible.
        # - Falls back to legacy compute_source_snapshot_hash, then to the reduced fingerprint.
        # =========================
        def _fix37_snapshot_hash_stable(bsc):
            try:
                if isinstance(bsc, list) and bsc:
                    try:
                        _h2 = compute_source_snapshot_hash_v2(bsc)
                        if _h2:
                            return str(_h2)
                    except Exception:
                        pass
                    try:
                        _h1 = compute_source_snapshot_hash(bsc)
                        if _h1:
                            return str(_h1)
                    except Exception:
                        pass
            except Exception:
                return _fix31_snapshot_fingerprint(bsc)

        _prev_hash = None
        _prev_hash_stable = None
        if isinstance(previous_data, dict):
            # =========================
            # PATCH FIX37 (ADD): prefer stable hash keys when available
            # =========================
            _prev_hash_stable = previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2")
            try:
                if not _prev_hash_stable and isinstance(previous_data.get("results"), dict):
                    _prev_hash_stable = (previous_data.get("results") or {}).get("source_snapshot_hash_stable") or (previous_data.get("results") or {}).get("source_snapshot_hash_v2")
            except Exception:
                pass
            _prev_hash = _prev_hash_stable or previous_data.get("source_snapshot_hash")
            # PATCH FIX41I_FASTPATH_PREF (ADDITIVE): explicit preferred hash (stable/v2 first)
            _prev_hash_pref = _prev_hash_stable or previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2") or _prev_hash

            try:
                if not _prev_hash and isinstance(previous_data.get("results"), dict):
                    _prev_hash = (previous_data.get("results") or {}).get("source_snapshot_hash")
            except Exception:
                pass

# PATCH FIX36 (ADDITIVE): populate explicit fastpath ineligibility reasons
        # - Record current/previous hashes even on mismatch
        # - Explain which prerequisite failed (no_prev_hash / no_prev_metrics / no_snapshots / hash_mismatch)
        # ============================================================
        _fix36_cur_hash = None
        _fix36_reason = ""

        # =====================================================================
        # PATCH FIX41AFC15 (ADDITIVE): Pre-hash merge of injected URL delta into baseline_sources_cache
        #
        # Why:
        # - In your latest evolution JSON, fastpath was correctly bypassed due to injected delta,
        #   but the current snapshot universe (baseline_sources_cache) still did NOT include the
        #   injected URL, so the stable hash still matched and downstream logic treated the run
        #   as "no delta" (no fetch, no rebuild, no injected lifecycle).
        #
        # Goal:
        # - BEFORE computing the current stable hash / fastpath eligibility, deterministically
        #   append placeholder snapshot rows for any injected URLs missing from the current
        #   baseline_sources_cache universe. This makes the hash differ (as it should when the
        #   source universe changes), forcing the normal rebuild/fetch pathways without changing
        #   the hashing algorithm itself.
        #
        # Safety:
        # - Purely additive.
        # - No effect when no injected URLs are present OR all injected URLs already exist in
        #   baseline_sources_cache.
        # =====================================================================
        try:
            _fx15_wc = web_context if isinstance(web_context, dict) else {}
            _fx15_extra = []
            # Prefer already-wired list fields
            if isinstance(_fx15_wc.get("extra_urls"), (list, tuple)):
                _fx15_extra = list(_fx15_wc.get("extra_urls") or [])
            elif isinstance(_fx15_wc.get("diag_extra_urls_ui"), (list, tuple)) and _fx15_wc.get("diag_extra_urls_ui"):
                _fx15_extra = list(_fx15_wc.get("diag_extra_urls_ui") or [])
            elif isinstance(_fx15_wc.get("diag_extra_urls_ui_raw"), str) and (_fx15_wc.get("diag_extra_urls_ui_raw") or "").strip():
                _raw = str(_fx15_wc.get("diag_extra_urls_ui_raw") or "")
                _parts = []
                for _line in _raw.splitlines():
                    _line = (_line or "").strip()
                    if not _line:
                        continue
                    for _p in _line.split(","):
                        _p = (_p or "").strip()
                        if _p:
                            _parts.append(_p)
                _fx15_extra = _parts

            _fx15_inj_norm = _inj_diag_norm_url_list(_fx15_extra) if _fx15_extra else []
            if _fx15_inj_norm and isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                _fx15_base_urls = []
                for _r in (baseline_sources_cache or []):
                    if not isinstance(_r, dict):
                        continue
                    _u = _r.get("source_url") or _r.get("url") or ""
                    if isinstance(_u, str) and _u:
                        _fx15_base_urls.append(_u)
                _fx15_base_set = set(_inj_diag_norm_url_list(_fx15_base_urls)) if _fx15_base_urls else set()
                _fx15_delta = [u for u in _fx15_inj_norm if u and u not in _fx15_base_set]
                if _fx15_delta:
                    # Append stable placeholders so the snapshot hash changes deterministically.
                    for _u in _fx15_delta:
                        baseline_sources_cache.append({
                            "source_url": _u,
                            "url": _u,
                            "status": "injected_pending",
                            "status_detail": "injected_url_placeholder_pre_hash",
                            "snapshot_text": "",
                            "extracted_numbers": [],
                            "numbers_found": 0,
                            "injected": True,
                            "injected_reason": "prehash_placeholder",
                        })
                    # Also ensure downstream sees a consistent universe via web_context["extra_urls"].
                    try:
                        if isinstance(_fx15_wc, dict):
                            _fx15_wc.setdefault("extra_urls", [])
                            if isinstance(_fx15_wc.get("extra_urls"), list):
                                # keep original order; append unique normalized
                                _seen = set(_inj_diag_norm_url_list(_fx15_wc.get("extra_urls") or []))
                                for _u in _fx15_delta:
                                    if _u not in _seen:
                                        _fx15_wc["extra_urls"].append(_u)
                                        _seen.add(_u)
                    except Exception:
                        pass
                    # Debug
                    try:
                        output.setdefault("debug", {})
                        if isinstance(output.get("debug"), dict):
                            output["debug"].setdefault("fix41afc15", {})
                            if isinstance(output["debug"].get("fix41afc15"), dict):
                                output["debug"]["fix41afc15"].update({
                                    "inj_norm_count": int(len(_fx15_inj_norm)),
                                    "inj_norm": list(_fx15_inj_norm),
                                    "inj_delta_count": int(len(_fx15_delta)),
                                    "inj_delta": list(_fx15_delta),
                                    "baseline_sources_cache_count_after_placeholder": int(len(baseline_sources_cache or [])),
                                })
                    except Exception:
                        pass
        except Exception:
            pass

        # =====================================================================
        # PATCH FIX41AFC16 (ADDITIVE): If injected URL placeholders exist, actually fetch+extract them
        #
        # Observed gap (from evolution JSON):
        #   - Injected URLs were present in ui/intake/hash_inputs, but remained:
        #       status = "injected_pending" / status_detail = "injected_url_placeholder_pre_hash"
        #   - So they never produced snapshot_text / extracted_numbers, and thus could not
        #     influence metric rebuild beyond a "hash universe" delta.
        #
        # Goal:
        #   - When injection is present, attempt to fetch+extract the injected URLs (delta-only),
        #     and update their baseline_sources_cache rows in-place so downstream rebuild sees them
        #     like normal fetched sources (or explicit failed reasons).
        #
        # Safety:
        #   - Purely additive.
        #   - No effect when no injected URLs are present.
        #   - Only touches rows that are injected placeholders (status == injected_pending) OR
        #     URLs that are injected_delta (not already in baseline).
        # =====================================================================
        try:
            _fx16_wc = web_context if isinstance(web_context, dict) else {}
            _fx16_extra = []
            if isinstance(_fx16_wc.get("extra_urls"), (list, tuple)) and _fx16_wc.get("extra_urls"):
                _fx16_extra = list(_fx16_wc.get("extra_urls") or [])
            elif isinstance(_fx16_wc.get("diag_extra_urls_ui"), (list, tuple)) and _fx16_wc.get("diag_extra_urls_ui"):
                _fx16_extra = list(_fx16_wc.get("diag_extra_urls_ui") or [])
            elif isinstance(_fx16_wc.get("diag_extra_urls_ui_raw"), str) and (_fx16_wc.get("diag_extra_urls_ui_raw") or "").strip():
                _raw = str(_fx16_wc.get("diag_extra_urls_ui_raw") or "")
                _parts = []
                for _line in _raw.splitlines():
                    _line = (_line or "").strip()
                    if not _line:
                        continue
                    for _p in _line.split(","):
                        _p = (_p or "").strip()
                        if _p:
                            _parts.append(_p)
                _fx16_extra = _parts

            _fx16_inj_norm = _inj_diag_norm_url_list(_fx16_extra) if _fx16_extra else []
            _fx16_base_urls = []
            if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                for _r in (baseline_sources_cache or []):
                    if not isinstance(_r, dict):
                        continue
                    _u = _r.get("source_url") or _r.get("url") or ""
                    if isinstance(_u, str) and _u:
                        _fx16_base_urls.append(_u)
            _fx16_base_set = set(_inj_diag_norm_url_list(_fx16_base_urls)) if _fx16_base_urls else set()
            _fx16_delta = [u for u in _fx16_inj_norm if u and u not in _fx16_base_set]

            # Identify placeholder rows that should be fetched
            _fx16_targets = []
            if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                for _r in (baseline_sources_cache or []):
                    if not isinstance(_r, dict):
                        continue
                    _u = _r.get("source_url") or _r.get("url") or ""
                    _u_norm = _inj_diag_norm_url_list([_u])[0] if isinstance(_u, str) and _u else ""
                    if not _u_norm:
                        continue
                    if _r.get("status") == "injected_pending":
                        _fx16_targets.append((_u_norm, _r, "placeholder_row"))
                    elif _u_norm in _fx16_delta:
                        _fx16_targets.append((_u_norm, _r, "delta_row"))

            # Also cover the case where placeholders were not appended (defensive)
            for _u in (_fx16_delta or []):
                if not isinstance(baseline_sources_cache, list):
                    continue
                if any((_inj_diag_norm_url_list([(_r.get("source_url") or _r.get("url") or "")])[0] if isinstance(_r, dict) else "") == _u for _r in (baseline_sources_cache or [])):
                    continue
                baseline_sources_cache.append({
                    "source_url": _u,
                    "url": _u,
                    "status": "injected_pending",
                    "status_detail": "injected_url_placeholder_pre_hash",
                    "snapshot_text": "",
                    "extracted_numbers": [],
                    "numbers_found": 0,
                    "injected": True,
                    "injected_reason": "fx16_defensive_placeholder",
                })
                _fx16_targets.append((_u, baseline_sources_cache[-1], "defensive_placeholder"))

            # Fetch+extract for targets (best-effort)
            _fx16_fetched = []
            _fx16_failed = []
            if _fx16_targets:
                for (_u_norm, _row, _why) in _fx16_targets:
                    # Skip if row already has text/numbers (idempotent)
                    try:
                        if isinstance(_row.get("snapshot_text"), str) and _row.get("snapshot_text").strip():
                            continue
                        if isinstance(_row.get("extracted_numbers"), list) and len(_row.get("extracted_numbers") or []) > 0:
                            continue
                    except Exception:
                        pass

                    _txt = None
                    _detail = ""
                    try:
                        _txt, _detail = fetch_url_content_with_status(_u_norm, timeout=25)
                    except Exception as _e:
                        _txt, _detail = None, f"exception:{type(_e).__name__}"

                    if _txt and isinstance(_txt, str) and len(_txt.strip()) >= 200:
                        # FIX2D69A: ensure numeric extraction runs on snapshot_text, with HTML->text fallback and auditable debug
                        _extract_text = _txt
                        try:
                            _t0 = str(_txt or '')
                            _looks_html = ('<' in _t0 and '>' in _t0 and ('</' in _t0 or '<html' in _t0.lower() or '<body' in _t0.lower()))
                            if _looks_html:
                                try:
                                    from bs4 import BeautifulSoup  # type: ignore
                                    _extract_text = BeautifulSoup(_t0, 'html.parser').get_text(' ')
                                except Exception:
                                    _extract_text = re.sub(r'<[^>]+>', ' ', _t0)
                            if not isinstance(_extract_text, str):
                                _extract_text = str(_extract_text or '')
                            _extract_text = re.sub(r'\s+', ' ', _extract_text).strip()
                            if not _extract_text:
                                _extract_text = _t0
                        except Exception:
                            _extract_text = _txt

                        _nums = []
                        _fx69_errors = []
                        _callable = bool(callable(extract_numbers_with_context))
                        try:
                            _row['fix2d68_extract_attempted'] = bool(_callable)
                            _row['fix2d68_extract_input_len'] = int(len(_extract_text) if isinstance(_extract_text, str) else 0)
                            _row['fix2d68_extract_input_head'] = (_extract_text[:200] if isinstance(_extract_text, str) else '')
                        except Exception:
                            pass
                        if _callable:
                            for _mode in ('source_url', 'url', 'plain'):
                                try:
                                    _tmp = None
                                    if _mode == 'source_url':
                                        _tmp = extract_numbers_with_context(_extract_text, source_url=_u_norm)
                                    elif _mode == 'url':
                                        _tmp = extract_numbers_with_context(_extract_text, url=_u_norm)
                                    else:
                                        _tmp = extract_numbers_with_context(_extract_text)
                                    # normalize extractor return
                                    if _tmp is None:
                                        _nums = []
                                    elif isinstance(_tmp, list):
                                        _nums = _tmp
                                    elif isinstance(_tmp, tuple) and len(_tmp) >= 1 and isinstance(_tmp[0], list):
                                        _nums = _tmp[0]
                                    elif isinstance(_tmp, dict) and isinstance(_tmp.get('extracted_numbers'), list):
                                        _nums = _tmp.get('extracted_numbers') or []
                                    else:
                                        _nums = []
                                    _row['fix2d68_extract_call_mode'] = _mode
                                    break
                                except Exception as _e:
                                    _fx69_errors.append({'mode': _mode, 'error': repr(_e)})
                                    _nums = []
                        if _fx69_errors:
                            _row['fix2d68_extract_errors'] = _fx69_errors

                        _row.update({
                            'status': 'fetched',
                            'status_detail': (_detail or 'success'),
                            'snapshot_text': _txt[:7000],
                            'extracted_numbers': _nums,
                            'numbers_found': int(len(_nums or [])),
                            'injected': True,
                            'injected_reason': _row.get('injected_reason') or 'fx16_fetch_and_extract',
                        })
                        _fx16_fetched.append({
                            'url': _u_norm, 'why': _why, 'numbers_found': int(len(_nums or [])), 'status_detail': (_detail or 'success')
                        })
                    else:
                        _row.update({
                            'status': 'failed',
                            'status_detail': (_detail or 'failed:no_text'),
                            'snapshot_text': '',
                            'extracted_numbers': [],
                            'numbers_found': 0,
                            'injected': True,
                            'injected_reason': _row.get('injected_reason') or 'fx16_fetch_failed',
                        })
                        _fx16_failed.append({
                            'url': _u_norm, 'why': _why, 'status_detail': (_detail or 'failed:no_text')
                        })

            # Emit debug
            try:
                output.setdefault("debug", {})
                if isinstance(output.get("debug"), dict):
                    output["debug"].setdefault("fix41afc16", {})
                    if isinstance(output["debug"].get("fix41afc16"), dict):
                        output["debug"]["fix41afc16"].update({
                            "inj_norm_count": int(len(_fx16_inj_norm or [])),
                            "inj_delta_count": int(len(_fx16_delta or [])),
                            "fetch_target_count": int(len(_fx16_targets or [])),
                            "fetched_count": int(len(_fx16_fetched or [])),
                            "failed_count": int(len(_fx16_failed or [])),
                            "fetched": list(_fx16_fetched or []),
                            "failed": list(_fx16_failed or []),
                        })
            except Exception:
                pass
        except Exception:
            pass


        # =====================================================================
        # PATCH FIX41AFC17 (ADDITIVE): Pin fetched injected snapshots into canonical snapshot plumbing
        #
        # Observed gap (from evolution JSON after FIX41AFC16):
        #   - Injected URL reaches intake/admitted/attempted/hash_inputs, but snapshot_debug remains empty
        #     (origin none / raw_count 0), and downstream consumers appear to miss the fetched snapshot_text.
        #
        # Goal:
        #   - Ensure the same snapshot-carrier fields used by New Analysis are populated for Evolution,
        #     so that attach_source_snapshots_to_analysis() (and any downstream rebuild plumbing) can
        #     “see” the injected (and other) snapshots deterministically.
        #
        # Safety:
        #   - Purely additive wiring.
        #   - No effect if baseline_sources_cache is missing.
        #   - Does not alter hashing logic; only ensures snapshots are attached consistently.
        # =====================================================================
        try:
            if isinstance(baseline_sources_cache, list) and baseline_sources_cache and isinstance(web_context, dict):
                # Provide canonical aliases for current pool (additive; downstream may read any of these)
                web_context.setdefault("current_baseline_sources_cache", baseline_sources_cache)
                web_context.setdefault("baseline_sources_cache_current", baseline_sources_cache)
                web_context.setdefault("current_source_results", baseline_sources_cache)

                # Mirror into output for downstream consumers/debug (additive)
                try:
                    output.setdefault("baseline_sources_cache_current", baseline_sources_cache)
                except Exception:
                    pass
                try:
                    output.setdefault("baseline_sources_cache", baseline_sources_cache)
                except Exception:
                    pass
                try:
                    output.setdefault("results", {})
                    if isinstance(output.get("results"), dict):
                        output["results"].setdefault("baseline_sources_cache_current", baseline_sources_cache)
                        output["results"].setdefault("baseline_sources_cache", baseline_sources_cache)
                except Exception:
                    pass

                # Call the same snapshot attach helper used by analysis if present (best-effort)
                try:
                    _att_fn = globals().get("attach_source_snapshots_to_analysis")
                    if callable(_att_fn):
                        _att_fn(output, web_context)
                except Exception:
                    pass

                # Small debug breadcrumb
                try:
                    output.setdefault("debug", {})
                    if isinstance(output.get("debug"), dict):
                        output["debug"].setdefault("fix41afc17", {})
                        if isinstance(output["debug"].get("fix41afc17"), dict):
                            output["debug"]["fix41afc17"].update({
                                "attached_pool_count": int(len(baseline_sources_cache)),
                                "attach_called": bool(callable(globals().get("attach_source_snapshots_to_analysis"))),
                            })
                except Exception:
                    pass
        except Exception:
            pass
        # =====================================================================

        try:
            if not (isinstance(baseline_sources_cache, list) and baseline_sources_cache):
                _fix36_reason = "no_snapshots"
            elif not (isinstance(prev_metrics, dict) and prev_metrics):
                _fix36_reason = "no_prev_metrics"
            else:
                _fix36_cur_hash = _fix37_snapshot_hash_stable(baseline_sources_cache)
                if not (isinstance(_prev_hash, str) and _prev_hash):
                    _fix36_reason = "no_prev_hash"
                elif _fix36_cur_hash != _prev_hash:
                    _fix36_reason = "hash_mismatch"
                else:
                    _fix36_reason = "hash_match_and_prev_metrics_present"

            # =====================================================================
            # PATCH EVO_FASTPATH_BYPASS_ON_INJECTED_URL_DELTA_V1 (ADDITIVE)
            # Intent:
            #   If the Evolution UI supplies injected URLs that are NOT already part of the
            #   baseline source universe, bypass fastpath eligibility even when hashes match.
            #   This does NOT weaken fastpath for the locked/no-injection case; it only prevents
            #   "observed but inert" injections from being ignored when they materially change
            #   the intended source universe.
            #
            #   Key rule:
            #     - If injected_delta (normalized_injected_urls - normalized_baseline_urls) is non-empty
            #       and fastpath would otherwise be eligible, force _fix36_reason to a bypass reason so
            #       fastpath_eligible becomes False and rebuild path can run.
            # =====================================================================
            try:
                _evo_wc = web_context if isinstance(web_context, dict) else {}
                _evo_diag = _evo_wc.get("diag_injected_urls") if isinstance(_evo_wc.get("diag_injected_urls"), dict) else {}
                _evo_extra_urls = []
                # Prefer already-normalized intake/ui lists if present
                for _k in ("intake", "ui_norm", "ui", "extra_urls"):
                    _v = _evo_diag.get(_k)
                    if isinstance(_v, (list, tuple)) and _v:
                        _evo_extra_urls = list(_v)
                        break
                # Fall back to raw web_context extras if diag not populated
                if not _evo_extra_urls:
                    _v2 = _evo_wc.get("extra_urls")
                    if isinstance(_v2, (list, tuple)) and _v2:
                        _evo_extra_urls = list(_v2)


                # =====================================================================
                # PATCH EVO_FASTPATH_BYPASS_INJ_DELTA_V2 (ADDITIVE):
                #   Robustly recover injected/extra URLs for bypass detection even when
                #   diag_injected_urls is not populated yet (common on replay/fastpath).
                #   Sources (in order):
                #     - web_context["extra_urls"] if list
                #     - web_context["diag_extra_urls_ui"] if list
                #     - web_context["diag_extra_urls_ui_raw"] if str (newline/space separated)
                #   This is diagnostic-only: we ONLY use this to decide whether to bypass
                #   fastpath when hashes otherwise match, so injected URLs can be admitted
                #   via the rebuild path and become first-class inputs.
                # =====================================================================
                try:
                    if not _evo_extra_urls:
                        _v3 = _evo_wc.get("diag_extra_urls_ui")
                        if isinstance(_v3, (list, tuple)) and _v3:
                            _evo_extra_urls = list(_v3)
                    if not _evo_extra_urls:
                        _raw = _evo_wc.get("diag_extra_urls_ui_raw")
                        if isinstance(_raw, str) and _raw.strip():
                            # Split on newlines first; also allow commas/spaces as separators
                            _parts = []
                            for _line in _raw.splitlines():
                                _line = (_line or "").strip()
                                if not _line:
                                    continue
                                # allow comma-separated within a line
                                for _p in _line.split(","):
                                    _p = (_p or "").strip()
                                    if _p:
                                        _parts.append(_p)
                            if _parts:
                                _evo_extra_urls = _parts
                except Exception:
                    pass

                # =====================================================================
                # PATCH FIX41AFC2 (ADDITIVE): Ensure rebuild/fetch path receives injected URLs
                #   If we recovered injected URLs from Streamlit diagnostic fields (e.g.,
                #   diag_extra_urls_ui_raw) and web_context["extra_urls"] is empty, wire the
                #   recovered list into web_context["extra_urls"] so downstream admission/
                #   fetch/persist logic can see the same universe deterministically.
                #   No effect on no-injection runs.
                # =====================================================================
                try:
                    if isinstance(_evo_wc, dict):
                        _wc_extra = _evo_wc.get("extra_urls")
                        if (not isinstance(_wc_extra, (list, tuple)) or not _wc_extra) and isinstance(_evo_extra_urls, list) and _evo_extra_urls:
                            _evo_wc["extra_urls"] = list(_evo_extra_urls)
                            try:
                                _evo_wc["__yureeka_extra_urls_are_injection_v1"] = True
                                _evo_wc["__yureeka_injected_urls_v1"] = list(_evo_extra_urls)
                            except Exception:
                                pass

                except Exception:
                    pass

                _evo_inj_set = set(_inj_diag_norm_url_list(_evo_extra_urls)) if _evo_extra_urls else set()

                # Baseline universe = urls present in baseline_sources_cache (the same object used for hashing)
                _evo_base_urls = []
                if isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                    for _row in baseline_sources_cache:
                        if isinstance(_row, dict) and isinstance(_row.get("source_url"), str) and _row.get("source_url"):
                            _evo_base_urls.append(_row.get("source_url"))
                _evo_base_set = set(_inj_diag_norm_url_list(_evo_base_urls)) if _evo_base_urls else set()

                _evo_inj_delta = sorted(list(_evo_inj_set - _evo_base_set)) if _evo_inj_set else []
                # =====================================================================
                # PATCH FIX41AFC2 (ADDITIVE): Latch bypass decision for later fastpath checks
                #   We persist a simple boolean flag in locals so the downstream FIX31
                #   authoritative reuse check can be disabled without refactoring.
                # =====================================================================
                _fix41af_inj_delta_present = bool(_evo_inj_delta)



                # Only bypass when hashes match and we would otherwise take fastpath
                if _evo_inj_delta and _fix36_reason == "hash_match_and_prev_metrics_present":
                    _fix36_reason = "hash_match_but_injected_urls_present_bypass_fastpath"
                    try:
                        if isinstance(output.get("debug"), dict) and isinstance(output.get("debug", {}).get("fix35"), dict):
                            output["debug"]["fix35"]["fastpath_bypass_injected_delta"] = _evo_inj_delta
                            output["debug"]["fix35"]["fastpath_bypass_injected_delta_count"] = len(_evo_inj_delta)
                    except Exception:
                        pass
            except Exception:
                pass
                # Never break evolution on diagnostics / bypass checks
                pass
            if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                # Preserve any earlier reason, but fill if empty
                if not output["debug"]["fix35"].get("fastpath_reason"):
                    output["debug"]["fix35"]["fastpath_reason"] = _fix36_reason
                output["debug"]["fix35"]["fastpath_eligible"] = bool(_fix36_reason == "hash_match_and_prev_metrics_present")
                if _fix36_cur_hash:
                    output["debug"]["fix35"]["source_snapshot_hash_current"] = _fix36_cur_hash
                    output["debug"]["fix35"]["source_snapshot_hash_current_alg"] = "fix37_stable_v2_preferred"
                if isinstance(_prev_hash, str) and _prev_hash:
                    output["debug"]["fix35"]["source_snapshot_hash_previous"] = (_prev_hash_pref if isinstance(locals().get("_prev_hash_pref"), str) and locals().get("_prev_hash_pref") else _prev_hash)
                # PATCH FIX37 (ADD): also expose stable-hash candidate if available
                try:
                    if isinstance(_prev_hash_stable, str) and _prev_hash_stable:
                        output["debug"]["fix35"]["source_snapshot_hash_previous_stable"] = _prev_hash_stable
                except Exception:
                    pass
        except Exception:
            pass
        # ============================================================

        # Only attempt fast-path if we have snapshots AND prior canonical metrics to reuse
        if isinstance(baseline_sources_cache, list) and baseline_sources_cache and isinstance(prev_metrics, dict) and prev_metrics:
            # ============================================================
            # PATCH FIX38 (ADDITIVE): align FIX31 authoritative reuse with FIX37 stable hash
            # - Previously FIX31 compared a v1 fingerprint against prev source_snapshot_hash,
            #   which could mismatch even when data was unchanged.
            # - We now prefer the same stable/v2 hash used by analysis & FIX37 debug.
            # ============================================================
            _cur_hash_v1 = _fix31_snapshot_fingerprint(baseline_sources_cache)
            try:
                _cur_hash = _fix37_snapshot_hash_stable(baseline_sources_cache)
            except Exception:
                pass
                _cur_hash = _cur_hash_v1

            # Prefer stable/v2 previous hash if present
            _prev_hash_pref = previous_data.get("source_snapshot_hash_stable") or previous_data.get("source_snapshot_hash_v2") or _prev_hash

            # =====================================================================
            # PATCH FIX42 (ADDITIVE): prefer "current" snapshot pool when provided
            #
            # Goal:
            #   When hashes are unequal, rebuild should run on the SAME snapshot pool
            #   that "new analysis" just produced, not on the stale baseline cache
            #   embedded in the previous analysis payload.
            #
            # Where it comes from:
            #   - web_context["current_baseline_sources_cache"]  (preferred)
            #   - web_context["baseline_sources_cache_current"]
            #   - web_context["current_source_results"]         (fallback alias)
            #
            # Policy (additive, fastpath-safe):
            #   - If force_rebuild is asserted by UI/web_context, always use current pool.
            #   - Else, only switch to current pool if its stable hash != previous hash.
            #   - If hashes match, we keep existing behavior (but either pool is equivalent).
            # =====================================================================
            _fix42_used_current_pool = False
            _fix42_reason = ""
            _fix42_cur_pool_hash = None
            try:
                if isinstance(web_context, dict):
                    _cur_pool = (
                        web_context.get("current_baseline_sources_cache")
                        or web_context.get("baseline_sources_cache_current")
                        or web_context.get("current_source_results")
                        or web_context.get("current_source_results_cache")
                        or None
                    )
                    if isinstance(_cur_pool, list) and _cur_pool:
                        _force = bool(
                            web_context.get("force_rebuild")
                            or web_context.get("forced_rebuild")
                            or web_context.get("force_full_rebuild")
                            or web_context.get("force_metric_rebuild")
                        )
                        try:
                            _fix42_cur_pool_hash = _fix37_snapshot_hash_stable(_cur_pool)
                        except Exception:
                            pass
                            _fix42_cur_pool_hash = None

                        _prev_hash_for_compare = _prev_hash_pref if (isinstance(_prev_hash_pref, str) and _prev_hash_pref) else _prev_hash
                        _hash_mismatch = bool(
                            (isinstance(_prev_hash_for_compare, str) and _prev_hash_for_compare and isinstance(_fix42_cur_pool_hash, str) and _fix42_cur_pool_hash)
                            and (_fix42_cur_pool_hash != _prev_hash_for_compare)
                        )

                        if _force or _hash_mismatch:
                            baseline_sources_cache = _cur_pool
                            snapshot_origin = (snapshot_origin or "analysis_cache") + "|fix42_current_pool"
                            _fix42_used_current_pool = True
                            _fix42_reason = "forced_rebuild" if _force else "hash_mismatch_use_current_pool"
            except Exception:
                pass

            # Attach FIX42 diagnostics (non-breaking)
            try:
                if _fix42_used_current_pool:
                    output["snapshot_origin"] = snapshot_origin
                if isinstance(output.get("debug"), dict):
                    output["debug"].setdefault("fix42", {})
                    output["debug"]["fix42"]["used_current_pool"] = bool(_fix42_used_current_pool)
                    output["debug"]["fix42"]["reason"] = _fix42_reason
                    if isinstance(_fix42_cur_pool_hash, str) and _fix42_cur_pool_hash:
                        output["debug"]["fix42"]["current_pool_hash_stable"] = _fix42_cur_pool_hash
            except Exception:
                pass
            # =====================================================================


            # =====================================================================
            # PATCH FIX41AFC2 (ADDITIVE): Enforce fastpath bypass on injected URL delta
            #   If an injected URL delta exists, we MUST NOT take FIX31 authoritative
            #   reuse (fastpath replay), even if hashes match. We do this additively by
            #   temporarily blanking _prev_hash_pref so the existing hash-match check
            #   remains unchanged for normal runs.
            # =====================================================================
            _fix41af_prev_hash_pref_saved = None
            try:
                if bool(locals().get("_fix41af_inj_delta_present")):
                    _fix41af_prev_hash_pref_saved = _prev_hash_pref
                    _prev_hash_pref = ""
                    try:
                        if isinstance(output.get("debug"), dict) and isinstance(output.get("debug", {}).get("fix35"), dict):
                            output["debug"]["fix35"]["fastpath_reason"] = "hash_match_but_injected_urls_present_bypass_fastpath"
                            output["debug"]["fix35"]["fastpath_eligible"] = False
                    except Exception:
                        pass
            except Exception:
                pass

            if isinstance(_prev_hash_pref, str) and _prev_hash_pref and _cur_hash == _prev_hash_pref:
                _fix31_authoritative_reuse = True
                # =====================================================================
                # PATCH FIX41AFC2 (ADDITIVE): Restore _prev_hash_pref after bypass guard
                # =====================================================================
                try:
                    if _fix41af_prev_hash_pref_saved is not None:
                        _prev_hash_pref = _fix41af_prev_hash_pref_saved
                except Exception:
                    pass

                try:
                    output["rebuild_skipped"] = True
                    output["rebuild_skipped_reason"] = "fix31_sources_unchanged_reuse_prev_metrics"
                    output["source_snapshot_hash_current"] = _cur_hash
                    output["source_snapshot_hash_previous"] = (_prev_hash_cmp if " _prev_hash_cmp" in locals() else _prev_hash)
                    try:
                        if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                            output["debug"]["fix35"]["fastpath_eligible"] = True
                            output["debug"]["fix35"]["fastpath_reason"] = "hash_match_and_prev_metrics_present"
                            output["debug"]["fix35"]["source_snapshot_hash_current"] = _cur_hash
                            output["debug"]["fix35"]["source_snapshot_hash_previous"] = (_prev_hash_pref if isinstance(locals().get("_prev_hash_pref"), str) and locals().get("_prev_hash_pref") else _prev_hash)
                            output["debug"]["fix35"]["current_metrics_origin"] = "reuse_processed_metrics_fastpath"
                    except Exception:
                        pass
                except Exception:
                    pass
    except Exception:
        pass
        _fix31_authoritative_reuse = False
        try:
            if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                if not output["debug"]["fix35"].get("fastpath_reason"):
                    output["debug"]["fix35"]["fastpath_reason"] = "fastpath_not_taken_or_exception"
        except Exception:
            pass
    # =====================================================================

    # Build a minimal current metrics dict from snapshots:
    current_metrics = {}
    # ============================================================
    # PATCH FIX31 (ADDITIVE): assign authoritative reused metrics now
    # ============================================================
    try:

        # =====================================================================
        # PATCH FIX41AFC2 (ADDITIVE): Ensure _prev_hash_pref restored if bypass guard blanked it
        #   (covers the case where hash-match condition was false and the inline restore
        #   inside the if-body did not execute).
        # =====================================================================
        try:
            if locals().get("_fix41af_prev_hash_pref_saved") is not None and not _prev_hash_pref:
                _prev_hash_pref = locals().get("_fix41af_prev_hash_pref_saved")
        except Exception:
            pass

        if _fix31_authoritative_reuse and isinstance(prev_metrics, dict) and prev_metrics:
            current_metrics = dict(prev_metrics)
            try:
                output["snapshot_origin"] = (output.get("snapshot_origin") or "") + "|fix31_reuse_prev_metrics"
            except Exception:
                pass
    except Exception:
        pass
    # ============================================================


    # Prefer metric_anchors to rebuild current_metrics (snapshot-gated)
    def _get_metric_anchors(prev: dict) -> dict:
        # ============================================================
        # PATCH FIX31 (ADDITIVE): if authoritative reuse is active, ignore anchors entirely
        # so the reused, schema-gated metrics remain untouched.
        # ============================================================
        try:
            if _fix31_authoritative_reuse:
                return {}
        except Exception:
            pass
        # ============================================================

        if not isinstance(prev, dict):
            return {}
        a = prev.get("metric_anchors")
        if isinstance(a, dict) and a:
            return a
        pr = prev.get("primary_response")
        if isinstance(pr, dict):
            a2 = pr.get("metric_anchors")
            if isinstance(a2, dict) and a2:
                return a2
        res = prev.get("results")
        if isinstance(res, dict):
            a3 = res.get("metric_anchors")
            if isinstance(a3, dict) and a3:
                return a3
        return {}

    def _canonicalize_candidate(n: dict) -> dict:
        try:
            fn = globals().get("canonicalize_numeric_candidate")
            if callable(fn):
                return fn(dict(n))
        except Exception:
            return dict(n)

    def _build_anchor_to_candidate_map(snapshots: list) -> dict:
        m = {}
        for sr in snapshots or []:
            if not isinstance(sr, dict):
                continue
            for n in (sr.get("extracted_numbers") or []):
                if not isinstance(n, dict):
                    continue
                nn = _canonicalize_candidate(n)
                ah = nn.get("anchor_hash")
                if not ah:
                    continue
                if ah not in m:
                    m[ah] = nn
        return m

    try:
        metric_anchors = _get_metric_anchors(previous_data)
        anchor_to_candidate = _build_anchor_to_candidate_map(baseline_sources_cache)

        if isinstance(metric_anchors, dict) and metric_anchors:
            for ckey, a in metric_anchors.items():
                if not isinstance(a, dict):
                    continue
                ah = a.get("anchor_hash") or a.get("anchor")
                if not ah:
                    continue
                cand = anchor_to_candidate.get(ah)
                if not isinstance(cand, dict):
                    continue

                base = prev_metrics.get(ckey) if isinstance(prev_metrics, dict) else None
                out_row = dict(base) if isinstance(base, dict) else {}
                out_row.update({
                    "canonical_key": ckey,
                    "anchor_hash": ah,
                    "anchor_used": True,
                    "anchor_confidence": a.get("anchor_confidence"),
                    "source_url": cand.get("source_url") or a.get("source_url"),
                    "raw": cand.get("raw"),
                    "value": cand.get("value"),
                    "unit": cand.get("unit"),
                    "value_norm": cand.get("value_norm"),
                    "context_snippet": cand.get("context_snippet") or cand.get("context") or "",
                    "candidate_id": cand.get("candidate_id") or a.get("candidate_id"),
                })
                current_metrics[ckey] = out_row
    except Exception:
        pass

    # Rebuild fallback only if anchors didn't produce metrics
    if not isinstance(current_metrics, dict) or not current_metrics:
        try:
            fn_rebuild = globals().get("rebuild_metrics_from_snapshots_schema_only") or globals().get("rebuild_metrics_from_snapshots")
            if callable(fn_rebuild):
                current_metrics = fn_rebuild(prev_response, baseline_sources_cache, web_context=web_context)
        except Exception:
            pass
            current_metrics = {}
    if not isinstance(current_metrics, dict) or not current_metrics:
        # FIX2D65A: do not fail hard here; emit a warning and continue so Evolution can still output JSON.
        try:
            output.setdefault("warnings", [])
            output["warnings"].append({
                "code": "FIX2D65_REBUILD_EMPTY_WITH_SNAPSHOTS",
                "message": "Valid snapshots exist, but metric rebuild returned empty. Continuing (no refetch, no heuristic).",
                "sources_checked": int(len(baseline_sources_cache or [])),
                "sources_fetched": int(len(baseline_sources_cache or [])),
            })
        except Exception:
            pass
        output["status"] = output.get("status") or "ok_with_warnings"
        output["message"] = output.get("message") or "Valid snapshots exist, but metric rebuild returned empty. Continuing (no refetch, no heuristic)."
        # Keep current_metrics as empty dict; downstream code should handle it.
        current_metrics = {}
        # PATCH FIX2D20 (ADD): trace year-like commits in primary_metrics_canonical
        _fix2d20_trace_year_like_commits(output, stage=str((output or {}).get('results',{}).get('debug',{}).get('stage') or 'evolution'), callsite='compute_source_anchored_diff_return')
    # =====================================================================
    # PATCH FIX41AFC19 (ADDITIVE): Anchor-first FIX16 rebuild override (schema parity)
    #
    # Why:
    # - Latest evo JSON shows current metrics can be selected from non-matching units
    #   (e.g., unit_sales metric receiving a unitless/negative number; percent metric
    #   receiving a magnitude unit like 'B'). This leads the dashboard "Current" column
    #   to display the wrong metric values even though injection plumbing is progressing.
    # - The new analysis pipeline already relies on FIX16-style hard eligibility gates +
    #   anchor_hash deterministic rebuild. Evolution must use the same selection rules
    #   when fastpath is NOT taken (hash mismatch or injection-triggered rebuild).
    #
    # What:
    # - Right before diffing, attempt an anchor-first rebuild using:
    #     rebuild_metrics_from_snapshots_schema_only_fix16(prev_response, pool, web_context)
    #   when available.
    # - If it returns a non-empty dict, it *overrides* the previously computed
    #   current_metrics (additive override only when rebuild succeeded).
    # - Emits explicit debug fields for traceability.
    #
    # Non-negotiables:
    # - Does NOT alter fastpath logic.
    # - Only activates when fastpath is not taken (i.e., not authoritative reuse).
    # =====================================================================
    try:
        _fix41afc19_applied = False
        _fix41afc19_reason = ""
        _fix41afc19_fn_name = ""
        _fix41afc19_rebuilt_count = 0

        # Only consider override when fastpath is not active
        if not bool(locals().get("_fix31_authoritative_reuse")):
            # Attempt to locate the "current" snapshot pool (post-injection merge/attach)
            _fix41afc19_pool = (
                locals().get("baseline_sources_cache_current")
                or (output.get("baseline_sources_cache_current") if isinstance(output, dict) else None)
                or (output.get("results", {}).get("baseline_sources_cache_current") if isinstance(output, dict) else None)
                or locals().get("baseline_sources_cache")
                or locals().get("baseline_sources_cache_prefetched")
                or None
            )

            # =====================================================================
            # PATCH PH2B_S2 (ADDITIVE): Robust pool resolution for canonical rebuild
            # - Some pipelines store the post-attach merged pool under different locals()
            #   names (or only inside nested objects). If the pool is missed, FIX41AFC19
            #   appears "not applied" even on rebuild runs.
            # - We search locals() for any list-like baseline_sources_cache* variants and
            #   choose the largest plausible pool as a safe fallback.
            # =====================================================================
            if _fix41afc19_pool is None:
                try:
                    _cand_pools = []
                    for _k, _v in (locals() or {}).items():
                        if not isinstance(_k, str):
                            continue
                        if "baseline_sources_cache" in _k and isinstance(_v, list) and _v:
                            _cand_pools.append((_k, _v))
                    # Choose the largest pool (most likely post-attach merged universe)
                    if _cand_pools:
                        _cand_pools.sort(key=lambda kv: len(kv[1] or []), reverse=True)
                        _fix41afc19_pool = _cand_pools[0][1]
                        _fix41afc19_reason = (_fix41afc19_reason or "") + "|ph2b_s2_pool_fallback:" + str(_cand_pools[0][0])
                except Exception:
                    pass
            # =====================================================================

            # Prefer Analysis-canonical rebuild (Phase 2B hard-wire) when present; else fall back to FIX16 schema-only rebuild
            _fix41afc19_fn = globals().get("rebuild_metrics_from_snapshots_analysis_canonical_v1")
            if callable(_fix41afc19_fn):
                _fix41afc19_fn_name = "rebuild_metrics_from_snapshots_analysis_canonical_v1"
            else:
                _fix41afc19_fn = globals().get("rebuild_metrics_from_snapshots_schema_only_fix16")
                if callable(_fix41afc19_fn):
                    _fix41afc19_fn_name = "rebuild_metrics_from_snapshots_schema_only_fix16"
                else:
                    _fix41afc19_fn = None

            if callable(_fix41afc19_fn) and _fix41afc19_pool is not None:
                try:
                    _fix41afc19_rebuilt = _fix41afc19_fn(prev_response, _fix41afc19_pool, web_context=web_context)
                except TypeError:
                    # Backward-compat: older signature without web_context
                    _fix41afc19_rebuilt = _fix41afc19_fn(prev_response, _fix41afc19_pool)

                if isinstance(_fix41afc19_rebuilt, dict) and _fix41afc19_rebuilt:
                    current_metrics = dict(_fix41afc19_rebuilt)
                    _fix41afc19_applied = True
                    _fix41afc19_rebuilt_count = len(current_metrics)
                    _fix41afc19_reason = "override_current_metrics_with_fix16_anchor_rebuild"
                else:
                    _fix41afc19_reason = (_fix41afc19_reason or "") + "|rebuilt_empty_or_non_dict"
    except Exception:
        pass

    # Emit debug for FIX41AFC19 (non-breaking)
    try:
        if isinstance(output.get("debug"), dict):
            output["debug"].setdefault("fix41afc19", {})
            output["debug"]["fix41afc19"]["applied"] = bool(locals().get("_fix41afc19_applied"))
            output["debug"]["fix41afc19"]["reason"] = locals().get("_fix41afc19_reason") or ""
            output["debug"]["fix41afc19"]["fn"] = locals().get("_fix41afc19_fn_name") or ""
            output["debug"]["fix41afc19"]["rebuilt_count"] = int(locals().get("_fix41afc19_rebuilt_count") or 0)
    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH V19_HARDWIRE_EVO_CANONICAL (ADDITIVE)
    # Objective:
    # - Ensure Evolution diff "current" side is built from the same canonical semantics as Analysis
    #   by forcing a best-effort canonical rebuild for DISPLAY/DIFF, even when earlier logic
    #   skipped due to authoritative reuse or pool-resolution drift.
    # - Adds two minimal diagnostics:
    #   (2) debug.fix41afc19 truth table (attempted/applied/skip_reason + keys_sample + pool_count)
    #   (3) debug.evo_winner_trace_v1 for key EV metrics (winner provenance + top3 candidate glimpse)
    # Safety:
    # - Additive-only. Does not modify hashing inputs or snapshot attach. Affects only what diff renders.
    # =====================================================================
    _fix41afc19_attempted_v19 = False
    _fix41afc19_skip_reason_v19 = ""
    _fix41afc19_pool_count_v19 = 0
    _fix41afc19_keys_sample_v19 = []
    _fix41afc19_winner_trace_v19 = {}

    try:
        # Start with whatever earlier stage produced
        current_metrics_for_display = locals().get("current_metrics") if isinstance(locals().get("current_metrics"), dict) else {}

        # Force a display rebuild if earlier FIX41AFC19 did not apply or rebuilt_count==0
        _already_applied = bool(locals().get("_fix41afc19_applied"))
        _already_count = int(locals().get("_fix41afc19_rebuilt_count") or 0)

        if (not _already_applied) or (_already_count <= 0):
            _fix41afc19_attempted_v19 = True

            # PATCH FIX2D3 START: Harden FIX41AFC19 v19 pool resolution + callable lookup
            # Resolve the best available snapshot pool (post-attach merged universe)
            # NOTE: Different callers store the "current" snapshot pool under different names.
            # We intentionally scan a wide set of candidate keys and pick the largest non-empty list.
            _pool = None
            _pool_key_used = None
            try:
                _cand = []

                # Common/expected keys (current run)
                _cand.append(("baseline_sources_cache_current", locals().get("baseline_sources_cache_current")))
                _cand.append(("baseline_sources_cache", locals().get("baseline_sources_cache")))
                _cand.append(("baseline_sources_cache_prefetched", locals().get("baseline_sources_cache_prefetched")))

                # Attachment / merge outputs (varies by branch)
                _cand.append(("baseline_sources_cache_attached", locals().get("baseline_sources_cache_attached")))
                _cand.append(("baseline_sources_cache_merged", locals().get("baseline_sources_cache_merged")))
                _cand.append(("attached_pool", locals().get("attached_pool")))
                _cand.append(("current_pool", locals().get("current_pool")))
                _cand.append(("source_snapshots_current", locals().get("source_snapshots_current")))
                _cand.append(("snapshots_current", locals().get("snapshots_current")))

                # Sometimes carried on output dict
                _out = (locals().get("output") if isinstance(locals().get("output"), dict) else None)
                if isinstance(_out, dict):
                    _cand.append(("output.baseline_sources_cache_current", _out.get("baseline_sources_cache_current")))
                    _cand.append(("output.baseline_sources_cache", _out.get("baseline_sources_cache")))
                    _res = _out.get("results") if isinstance(_out.get("results"), dict) else None
                    if isinstance(_res, dict):
                        _cand.append(("output.results.baseline_sources_cache_current", _res.get("baseline_sources_cache_current")))
                        _cand.append(("output.results.baseline_sources_cache", _res.get("baseline_sources_cache")))
                        _cand.append(("output.results.attached_pool", _res.get("attached_pool")))
                        _cand.append(("output.results.current_pool", _res.get("current_pool")))

                # Final sweep: anything in locals containing baseline_sources_cache*
                for _k, _v in (locals() or {}).items():
                    if not isinstance(_k, str):
                        continue
                    if "baseline_sources_cache" in _k and isinstance(_v, list) and _v:
                        _cand.append((_k, _v))

                # Choose best candidate (largest list wins)
                _best = None
                for _k, _v in _cand:
                    if isinstance(_v, list) and _v:
                        if _best is None or len(_v) > len(_best[1] or []):
                            _best = (_k, _v)
                if _best is not None:
                    _pool_key_used, _pool = _best[0], _best[1]
            except Exception:
                pass
                _pool = None
                _pool_key_used = None


            if _pool is None:
                try:
                    _cand_pools = []
                    for _k, _v in (locals() or {}).items():
                        if not isinstance(_k, str):
                            continue
                        if "baseline_sources_cache" in _k and isinstance(_v, list) and _v:
                            _cand_pools.append((_k, _v))
                    if _cand_pools:
                        _cand_pools.sort(key=lambda kv: len(kv[1] or []), reverse=True)
                        _pool = _cand_pools[0][1]
                except Exception:
                    pass
                    _pool = None

            if isinstance(_pool, list):
                _fix41afc19_pool_count_v19 = len(_pool or [])
                _fix41afc19_pool_key_used_v19 = _pool_key_used

            # Pick best available rebuild fn
            def _fix2d3_resolve_callable(_name: str):
                try:
                    fn = globals().get(_name)
                    if callable(fn):
                        return fn
                except Exception:
                    pass
                try:
                    fn = locals().get(_name)
                    if callable(fn):
                        return fn
                except Exception:
                    pass
                try:
                    import sys as _sys
                    _mod = _sys.modules.get(__name__)
                    fn = getattr(_mod, _name, None) if _mod else None
                    if callable(fn):
                        return fn
                except Exception:
                    return None

            # END PATCH FIX2D3

            _fn = _fix2d3_resolve_callable("rebuild_metrics_from_snapshots_analysis_canonical_v1")
            _fn_name = "rebuild_metrics_from_snapshots_analysis_canonical_v1"
            if not callable(_fn):
                _fn = _fix2d3_resolve_callable("rebuild_metrics_from_snapshots_schema_only_fix16")
                _fn_name = "rebuild_metrics_from_snapshots_schema_only_fix16"


            # PATCH FIX2D2_FALLBACK_REBUILD_FN_NAMES (ADDITIVE):
            # Some branches expose only the legacy names. Accept them as safe fallbacks
            # so 'fn_missing' doesn't mask a usable rebuild implementation.
            if not callable(_fn):
                _fn = globals().get("rebuild_metrics_from_snapshots_schema_only")
                _fn_name = "rebuild_metrics_from_snapshots_schema_only"
            if not callable(_fn):
                _fn = globals().get("rebuild_metrics_from_snapshots_with_anchors")
                _fn_name = "rebuild_metrics_from_snapshots_with_anchors"
            if not callable(_fn):
                _fn = globals().get("rebuild_metrics_from_snapshots")
                _fn_name = "rebuild_metrics_from_snapshots"

            if not callable(_fn):
                _fix41afc19_skip_reason_v19 = "fn_missing"
            elif _pool is None:
                _fix41afc19_skip_reason_v19 = "pool_missing"
            elif not isinstance(_pool, list) or not _pool:
                _fix41afc19_skip_reason_v19 = "pool_empty"
            else:
                try:
                    try:
                        _rebuilt = _fn(prev_response, _pool, web_context=web_context)
                    except TypeError:
                        _rebuilt = _fn(prev_response, _pool)
                except Exception as _e:
                    _rebuilt = None
                    _fix41afc19_skip_reason_v19 = "rebuild_exception:" + str(type(_e).__name__)

                # REFACTOR35: guard against schema-only rebuilds leaking debug keys into PMC
                # Keep only keys that exist in the frozen schema.
                try:
                    _schema_keys = set((analysis.get('metric_schema_frozen') or {}).keys()) if isinstance(analysis.get('metric_schema_frozen'), dict) else set()
                    if isinstance(_rebuilt, dict) and _schema_keys:
                        _rebuilt = {k: v for (k, v) in _rebuilt.items() if isinstance(k, str) and k in _schema_keys and isinstance(v, dict)}
                except Exception:
                    pass

                if isinstance(_rebuilt, dict) and _rebuilt:
                    current_metrics_for_display = dict(_rebuilt)
                    _fix41afc19_skip_reason_v19 = "applied_v19_display_rebuild:" + str(_fn_name)
                    try:
                        _fix41afc19_keys_sample_v19 = list(current_metrics_for_display.keys())[:10]
                    except Exception:
                        pass
                        _fix41afc19_keys_sample_v19 = []
                else:
                    _fix41afc19_skip_reason_v19 = _fix41afc19_skip_reason_v19 or "rebuilt_empty_or_non_dict"

        # Apply display override (used by diff below)
        locals()["current_metrics"] = current_metrics_for_display  # keep variable name used by downstream diff
        # ============================================================
        # PATCH START: FIX2D9_SCHEMA_ANCHORED_REBUILD_CALLSITE_V1
        # Purpose: Override current_metrics_for_display with schema-anchored rebuild
        # ============================================================
        try:
            _fix2d9_over, _fix2d9_diag = _fix2d9_schema_anchored_rebuild_current_metrics_v1(
                prev_response=prev_response,
                pool=_pool,
                web_context=web_context,
            )
            try:
                output.setdefault('results', {}).setdefault('debug', {})
                output['results']['debug']['fix2d9_schema_anchored_rebuild_v1'] = _fix2d9_diag
            except Exception:
                pass
            if isinstance(_fix2d9_over, dict) and _fix2d9_over:
                current_metrics_for_display = dict(_fix2d9_over)
                locals()['current_metrics'] = current_metrics_for_display
                try:
                    output.setdefault('results', {})['primary_metrics_canonical'] = current_metrics_for_display
                except Exception:
                    pass
        except Exception:
            pass
        # ============================================================
        # PATCH END: FIX2D9_SCHEMA_ANCHORED_REBUILD_CALLSITE_V1
        # ============================================================
        # ============================================================
        # PATCH START: FIX2D7_EXEC_STAMP_AND_PROPAGATE_V1
        # Purpose:
        #   - Stamp exec code version + join mode into results.debug
        #   - Propagate current_metrics_for_display into results.primary_metrics_canonical
        #     so downstream render/diff can populate the 'Current' column.
        # ============================================================
        try:
            output.setdefault('results', {}).setdefault('debug', {})
            output['results']['debug']['__exec_code_version'] = globals().get('CODE_VERSION')
            try:
                output['results']['debug']['__exec_join_mode'] = _fix2d6_get_diff_join_mode_v1()
            except Exception:
                pass
        except Exception:
            pass
        try:
            if isinstance(current_metrics_for_display, dict) and current_metrics_for_display:
                output.setdefault('results', {})
                output['results']['primary_metrics_canonical'] = dict(current_metrics_for_display)
                try:
                    output['results'].setdefault('primary_response', {})
                    if isinstance(output['results']['primary_response'], dict):
                        output['results']['primary_response']['primary_metrics_canonical'] = dict(current_metrics_for_display)
                except Exception:
                    pass
                output.setdefault('debug', {})
                if isinstance(output['debug'], dict):
                    output['debug']['fix2d7_propagate_current_canon_v1'] = {
                        'applied': True,
                        'count': int(len(current_metrics_for_display)),
                        'keys_sample': list(current_metrics_for_display.keys())[:10],
                    }
        except Exception:
            pass
        # ============================================================
        # PATCH END: FIX2D7_EXEC_STAMP_AND_PROPAGATE_V1
        # ============================================================

        try:
            _fix41afc19_keys_sample_v19 = _fix41afc19_keys_sample_v19 or (list(current_metrics_for_display.keys())[:10] if isinstance(current_metrics_for_display, dict) else [])
        except Exception:
            pass

        # ---------------- Diagnostic (3): winner provenance trace for key EV metrics ----------------
        # Heuristic: pick the known canonical keys if present, else infer from schema/keys.
        _key_candidates = [
            "units_sold_2024__unit_sales",
            "market_share_2024__percent",
            "projected_market_share_2026__percent",
            "projected_market_share_2030__percent",
            "yoy_growth_rate_2024__percent",
            "cagr_2024__percent",
        ]

        try:
            prev_canon = (prev_response or {}).get("primary_metrics_canonical") if isinstance(prev_response, dict) else {}
            if not isinstance(prev_canon, dict):
                prev_canon = {}

            cur_canon = current_metrics_for_display if isinstance(current_metrics_for_display, dict) else {}

            # PATCH FIX2D4 (ADD): explicit key overlap diagnostics (prev vs current)
            try:
                if isinstance(output.get("debug"), dict):
                    output["debug"]["key_overlap_v1"] = _emit_key_overlap_debug_v1(
                        prev_metrics=prev_canon,
                        cur_metrics=cur_canon,
                        target_key="global_ev_sales_ytd_2025__unit_sales"
                    )
            except Exception:
                pass

            # Infer projected-share keys if the exact ones are not present
            if isinstance(prev_canon, dict) and isinstance(cur_canon, dict):
                all_keys = set(list(prev_canon.keys()) + list(cur_canon.keys()))
                for k in sorted(all_keys):
                    lk = k.lower()
                    if ("project" in lk or "proj" in lk) and ("share" in lk or "market_share" in lk) and ("2026" in lk or "2030" in lk) and k not in _key_candidates:
                        _key_candidates.append(k)

            # Flatten snapshot candidates once (for top3 glimpse)
            flat = []
            try:
                _pool_for_flat = locals().get("baseline_sources_cache") if isinstance(locals().get("baseline_sources_cache"), list) else []
                for sr in _pool_for_flat or []:
                    if isinstance(sr, dict):
                        for c in (sr.get("extracted_numbers") or []):
                            if isinstance(c, dict):
                                flat.append(c)
            except Exception:
                pass
                flat = []

            def _cand_has_unit_evidence(c: dict) -> bool:
                try:
                    if (c.get("unit_tag") or c.get("unit") or c.get("unit_norm") or c.get("unit_raw") or "").strip():
                        return True
                    if (c.get("currency") or c.get("currency_symbol") or "").strip():
                        return True
                    if c.get("is_percent") or c.get("has_percent"):
                        return True
                    if (c.get("base_unit") or "").strip():
                        return True
                    if (c.get("unit_family") or "").strip():
                        return True
                    if isinstance(c.get("unit_tokens"), list) and c.get("unit_tokens"):
                        return True
                except Exception:
                    return False
                return False

            # Schema lookup (if available)
            schema = {}
            try:
                if isinstance(previous_data, dict):
                    pr = previous_data.get("primary_response") if isinstance(previous_data.get("primary_response"), dict) else previous_data
                    schema = (pr.get("metric_schema_frozen") or {}) if isinstance(pr, dict) else {}
            except Exception:
                pass
                schema = {}

            def _score_candidate_for_key(c: dict, ckey: str) -> int:
                try:
                    md = schema.get(ckey) if isinstance(schema, dict) else {}
                    kws = []
                    if isinstance(md, dict):
                        kws = md.get("keywords") or md.get("keyword_hints") or []
                    kws = [str(k).lower() for k in kws if str(k).strip()]
                    ctx = (c.get("context") or c.get("window") or c.get("context_window") or "").lower()
                    s = 0
                    for k in kws[:25]:
                        if k and k in ctx:
                            s += 1
                    if _cand_has_unit_evidence(c):
                        s += 5
                    # Prefer anchor-matching if both present
                    try:
                        if (c.get("anchor_hash") or "") and isinstance(md, dict) and (md.get("anchor_hash") or ""):
                            if str(c.get("anchor_hash")) == str(md.get("anchor_hash")):
                                s += 10
                    except Exception:
                        return int(s)
                except Exception:
                    return 0

            # Build traces for up to 5 keys that exist in either prev/cur
            _keys_for_trace = []
            for k in _key_candidates:
                if k in (prev_canon or {}) or k in (cur_canon or {}):
                    _keys_for_trace.append(k)
                if len(_keys_for_trace) >= 5:
                    break
            if not _keys_for_trace:
                # fallback: first 5 keys from prev canonical
                _keys_for_trace = list(prev_canon.keys())[:5]

            for k in _keys_for_trace:
                pv = (prev_canon.get(k) or {}) if isinstance(prev_canon, dict) else {}
                cv = (cur_canon.get(k) or {}) if isinstance(cur_canon, dict) else {}

                winner_src = "unknown"
                if _fix41afc19_attempted_v19 and "applied_v19_display_rebuild" in (_fix41afc19_skip_reason_v19 or ""):
                    winner_src = "analysis_canonical_rebuild_v19"
                elif bool(locals().get("_fix41afc19_applied")):
                    winner_src = "analysis_canonical_rebuild"
                else:
                    winner_src = "fallback_snapshot_selector"

                # Top3 glimpse from flat pool
                top3 = []
                try:
                    scored = sorted(flat, key=lambda c: _score_candidate_for_key(c, k), reverse=True)[:3]
                    for t in scored:
                        top3.append({
                            "raw": t.get("raw"),
                            "value_norm": t.get("value_norm"),
                            "unit_tag": t.get("unit_tag") or t.get("unit") or "",
                            "has_unit_evidence": bool(_cand_has_unit_evidence(t)),
                            "anchor_hash": t.get("anchor_hash"),
                        })
                except Exception:
                    pass
                    top3 = []

                _fix41afc19_winner_trace_v19[k] = {
                    "winner_source": winner_src,
                    "prev_value_norm": pv.get("value_norm"),
                    "prev_unit": pv.get("unit") or pv.get("unit_tag") or "",
                    "cur_value_norm": cv.get("value_norm"),
                    "cur_unit": cv.get("unit") or cv.get("unit_tag") or "",
                    "cur_has_unit_evidence": bool((cv.get("unit") or cv.get("unit_tag") or "").strip()),
                    "top3_candidates_glimpse": top3,
                }
        except Exception:
            pass

    except Exception:
        pass

    # ---------------- Diagnostic (2): FIX41AFC19 truth table ----------------
    try:
        if isinstance(output.get("debug"), dict):
            output["debug"].setdefault("fix41afc19", {})
            output["debug"]["fix41afc19"]["attempted"] = bool(_fix41afc19_attempted_v19)
            # Keep the pre-existing 'applied' field as-is, but add applied_v19_display_override
            output["debug"]["fix41afc19"]["applied_v19_display_override"] = bool(_fix41afc19_attempted_v19 and ("applied_v19_display_rebuild" in (_fix41afc19_skip_reason_v19 or "")))
            # Never allow blank reason: prefer v19 skip_reason when earlier reason is empty
            _prev_reason = str(output["debug"]["fix41afc19"].get("reason") or "")
            if (not _prev_reason.strip()) and (_fix41afc19_skip_reason_v19 or "").strip():
                output["debug"]["fix41afc19"]["reason"] = str(_fix41afc19_skip_reason_v19)
            output["debug"]["fix41afc19"]["skip_reason_v19"] = str(_fix41afc19_skip_reason_v19 or "")
            output["debug"]["fix41afc19"]["pool_count_v19"] = int(_fix41afc19_pool_count_v19 or 0)
            output["debug"]["fix41afc19"]["pool_key_used_v19"] = str(locals().get("_fix41afc19_pool_key_used_v19") or locals().get("_pool_key_used") or "")
            output["debug"]["fix41afc19"]["rebuilt_keys_sample_v19"] = list(_fix41afc19_keys_sample_v19 or [])
            if _fix41afc19_winner_trace_v19:
                output["debug"]["evo_winner_trace_v1"] = _fix41afc19_winner_trace_v19
    except Exception:
        pass
    # =====================================================================
    # =====================================================================
    # PATCH V20_CANONICAL_FOR_RENDER (ADDITIVE): make Evolution dashboard derive
    # "Current" from a canonical-for-render payload (analysis-aligned) WITHOUT
    # touching fastpath/hashing/snapshot-attach.
    #
    # Why:
    # - Evolution UI renders from diff rows (metric_changes), not analysis key-metrics.
    # - If diff rows source "Current" from raw extracted pools, unitless survivors
    #   (e.g., 170, 2) can win.
    # - We compute a late, render-only canonical dict from the frozen snapshot pool
    #   using the same rebuild semantics as analysis (best effort), then force the
    #   diff + row hydration to use it.
    #
    # Safety:
    # - Purely post-snapshot, post-hash: affects ONLY dashboard/diff rendering.
    # - Does NOT alter source selection, hashing inputs, injection lifecycle, fastpath.
    #
    # Diagnostics:
    # - output.debug.canonical_for_render_v1
    # - output.debug.canonical_for_render_row_audit_v1
    # =====================================================================
    _canonical_for_render_applied = False
    _canonical_for_render_reason = ""
    _canonical_for_render_fn = ""
    _canonical_for_render_count = 0
    _canonical_for_render_keys_sample = []
    _canonical_for_render_replaced_current_metrics = False

    canonical_for_render = {}
    try:
        # Default: use whatever current_metrics we already have
        canonical_for_render = current_metrics if isinstance(current_metrics, dict) else {}
        # REFACTOR15: if refactors nested the current PMC under output['results'], recover it so diffing can't go empty.
        if (not isinstance(canonical_for_render, dict)) or (isinstance(canonical_for_render, dict) and not canonical_for_render):
            try:
                if isinstance(output.get("primary_metrics_canonical"), dict) and output.get("primary_metrics_canonical"):
                    canonical_for_render = output.get("primary_metrics_canonical")
            except Exception:
                pass
        if (not isinstance(canonical_for_render, dict)) or (isinstance(canonical_for_render, dict) and not canonical_for_render):
            try:
                _pr = output.get("primary_response") if isinstance(output.get("primary_response"), dict) else None
                if isinstance(_pr, dict) and isinstance(_pr.get("primary_metrics_canonical"), dict) and _pr.get("primary_metrics_canonical"):
                    canonical_for_render = _pr.get("primary_metrics_canonical")
            except Exception:
                pass
        if (not isinstance(canonical_for_render, dict)) or (isinstance(canonical_for_render, dict) and not canonical_for_render):
            try:
                _r = output.get("results") if isinstance(output.get("results"), dict) else None
                if isinstance(_r, dict) and isinstance(_r.get("primary_metrics_canonical"), dict) and _r.get("primary_metrics_canonical"):
                    canonical_for_render = _r.get("primary_metrics_canonical")
            except Exception:
                pass
        # Also publish for downstream consumers (UI + v2 diff builder)
        try:
            if isinstance(canonical_for_render, dict):
                output["primary_metrics_canonical"] = canonical_for_render
                if isinstance(output.get("primary_response"), dict):
                    output["primary_response"]["primary_metrics_canonical"] = canonical_for_render
        except Exception:
            pass

        # REFACTOR26: hydrate source_url for canonical metrics (in-place, schema-preserving)
        try:
            _pmc_tmp = output.get("primary_metrics_canonical") if isinstance(output, dict) else None
            if isinstance(_pmc_tmp, dict):
                _refactor26_hydrate_primary_metrics_canonical_source_urls_v1(_pmc_tmp)
        except Exception:
            pass


        # =====================================================================
        # PATCH V30_CANONICAL_FOR_RENDER_SEED_DISABLE (ADDITIVE)
        # Goal:
        # - Stop seeding canonical_for_render from current_metrics because current_metrics may already
        #   contain year-like / unitless / junk winners (e.g., "2.0 B", "-6441").
        # - Force the downstream rebuild path (which is intended to be analysis-aligned) to run,
        #   while keeping fastpath/hashing/injection/snapshot attach untouched.
        #
        # Mechanism:
        # - If prev_response carries a frozen schema, disable the seed by default.
        # - Allow opt-out via env var EVO_CANONICAL_FOR_RENDER_ALLOW_SEED=1.
        # - Emit a small trace later via _canonical_for_render_reason tag.
        # =====================================================================
        try:
            _allow_seed = str(os.getenv("EVO_CANONICAL_FOR_RENDER_ALLOW_SEED", "") or "").strip() in ("1", "true", "True", "yes", "YES")
            _has_schema = isinstance(prev_response, dict) and isinstance(prev_response.get("metric_schema_frozen") or {}, dict) and bool(prev_response.get("metric_schema_frozen"))
            if _has_schema and not _allow_seed:
                canonical_for_render = {}
                _canonical_for_render_reason = "v30_seed_disabled_force_rebuild"
        except Exception:
            pass

        # PATCH V21_CANONICAL_FOR_RENDER_SUSPICION (ADDITIVE):
        # Even when current_metrics has "enough" keys, it can still be junk (year-like/unitless winners).
        # Detect suspicious existing canonical dict and force a render-only rebuild in that case.
        def _v21_yearlike(x):
            try:
                if x is None:
                    return False
                fx = float(x)
                if abs(fx - round(fx)) < 1e-9:
                    ix = int(round(fx))
                    return 1900 <= ix <= 2105
                return False
            except Exception:
                return False

        def _v21_metric_suspicious(m):
            try:
                if not isinstance(m, dict):
                    return True
                u = (m.get("unit") or m.get("unit_tag") or "").strip()
                vn = m.get("value_norm")
                if (not u) and (_v21_yearlike(vn) or vn is None):
                    return True
                return False
            except Exception:
                return True

        _suspicious_existing = False
        try:
            if isinstance(canonical_for_render, dict) and canonical_for_render:
                _keys_sample_chk = list(sorted(list(canonical_for_render.keys())))[:25]
                _sus = 0
                _tot = 0
                for _k in _keys_sample_chk:
                    _tot += 1
                    if _v21_metric_suspicious(canonical_for_render.get(_k)):
                        _sus += 1
                if _tot > 0:
                    _suspicious_existing = (_sus / float(_tot)) >= 0.30
        except Exception:
            pass
            _suspicious_existing = False

        # Best-effort: rebuild canonical-for-render from frozen snapshots using analysis-aligned builder.
        # Apply when current canonical is missing/suspiciously small.
        _need_render_rebuild = (not isinstance(canonical_for_render, dict)) or (len(canonical_for_render) < 3) or bool(_suspicious_existing)
        if _need_render_rebuild and isinstance(baseline_sources_cache, list) and baseline_sources_cache:
            # Resolve schema/anchors/canon from prev_response (analysis baseline)
            _schema = {}
            _anchors = {}
            _prev_canon = {}
            try:
                if isinstance(prev_response, dict):
                    _schema = prev_response.get("metric_schema_frozen") or {}
                    _anchors = prev_response.get("metric_anchors") or {}
                    _prev_canon = prev_response.get("primary_metrics_canonical") or {}
            except Exception:
                pass
                _schema, _anchors, _prev_canon = {}, {}, {}

            # Choose the best available analysis-aligned rebuild function
            _fn = globals().get("rebuild_metrics_from_snapshots_analysis_canonical_v1")
            if callable(_fn):
                try:
                    canonical_for_render = _fn(baseline_sources_cache, _schema, _anchors, _prev_canon)
                    _canonical_for_render_fn = "rebuild_metrics_from_snapshots_analysis_canonical_v1"
                except Exception:
                    pass
                    canonical_for_render = {}
            if (not canonical_for_render) and callable(globals().get("rebuild_metrics_from_snapshots_with_anchors_fix16")):
                try:
                    _fn2 = globals().get("rebuild_metrics_from_snapshots_with_anchors_fix16")
                    canonical_for_render = _fn2(baseline_sources_cache, _schema, _anchors, _prev_canon)
                    _canonical_for_render_fn = "rebuild_metrics_from_snapshots_with_anchors_fix16"
                except Exception:
                    pass
                    canonical_for_render = {}
            if (not canonical_for_render) and callable(globals().get("rebuild_metrics_from_snapshots_schema_only")):
                try:
                    _fn3 = globals().get("rebuild_metrics_from_snapshots_schema_only")
                    canonical_for_render = _fn3(baseline_sources_cache, _schema)
                    _canonical_for_render_fn = "rebuild_metrics_from_snapshots_schema_only"
                except Exception:
                    pass
                    canonical_for_render = {}

            if isinstance(canonical_for_render, dict) and canonical_for_render:
                _canonical_for_render_applied = True
                _canonical_for_render_reason = "applied_render_only_rebuild" if not bool(_suspicious_existing) else "applied_render_only_rebuild_forced_suspicious"
                _canonical_for_render_count = int(len(canonical_for_render))
                _canonical_for_render_keys_sample = list(sorted(list(canonical_for_render.keys())))[:12]
                _canonical_for_render_replaced_current_metrics = True
                # PATCH V22_CANONICAL_FOR_RENDER_FN_GUARD (ADDITIVE): ensure fn label is never empty when rebuild succeeded
                try:
                    if not str(_canonical_for_render_fn or "").strip():
                        _canonical_for_render_fn = "unknown_rebuild_fn"
                except Exception:
                    pass
            else:
                _canonical_for_render_reason = "render_rebuild_failed_or_empty"
        else:
            _canonical_for_render_reason = "used_existing_current_metrics" if not bool(_suspicious_existing) else "forced_render_rebuild_due_to_suspicious_existing_failed"
            _canonical_for_render_count = int(len(canonical_for_render)) if isinstance(canonical_for_render, dict) else 0
            _canonical_for_render_keys_sample = list(sorted(list(canonical_for_render.keys())))[:12] if isinstance(canonical_for_render, dict) else []
    except Exception:
        pass
        canonical_for_render = current_metrics if isinstance(current_metrics, dict) else {}
        _canonical_for_render_reason = "exception_fallback_existing"


    # =====================================================================
    # PATCH V28_FORCE_ANCHOR_PICK_FOR_RENDER (ADDITIVE)
    # Problem observed:
    # - canonical_for_render rebuild may select junk numbers from the frozen pool
    #   (e.g., GlobeNewswire footer "2B" or email fragments "-6441") when anchors
    #   are not strictly enforced.
    #
    # Fix:
    # - If prev_response provides metric_anchors for a canonical_key, forcibly
    #   resolve the exact anchored candidate from baseline_sources_cache by matching
    #   anchor_hash and use that to populate canonical_for_render for that key.
    #
    # Scope / Safety:
    # - Render-only: affects ONLY canonical_for_render (dashboard "Current")
    # - Does NOT touch hashing, fastpath, injection lifecycle, or snapshot attach.
    #
    # Diagnostics:
    # - output.debug.canonical_for_render_anchor_enforce_v28 (summary)
    # - per-metric cm["diag"]["v28_anchor_enforced"] (when applied)
    # =====================================================================
    _v28_anchor_enforce = {
        "attempted": False,
        "schema_keys": 0,
        "anchors_keys": 0,
        "hits": 0,
        "misses": 0,
        "hit_keys_sample": [],
        "miss_keys_sample": [],
        "note": "render-only anchor enforcement by anchor_hash against frozen extracted_numbers pool",
    }

    def _v28_iter_numbers_from_sources_cache(_sources_cache):
        try:
            for _src in (_sources_cache or []):
                if not isinstance(_src, dict):
                    continue
                _url = _src.get("url") or _src.get("source_url") or ""
                nums = _src.get("extracted_numbers") or []
                if isinstance(nums, list):
                    for _n in nums:
                        if isinstance(_n, dict):
                            yield _url, _n
        except Exception:
            return

    def _v28_pick_by_anchor_hash(_sources_cache, _anchor_hash: str):
        try:
            ah = str(_anchor_hash or "").strip()
            if not ah or ah == "None":
                return None
            for _url, _n in _v28_iter_numbers_from_sources_cache(_sources_cache):
                try:
                    if str(_n.get("anchor_hash") or "").strip() == ah:
                        out = dict(_n)
                        if _url and (not out.get("source_url")):
                            out["source_url"] = _url
                        return out
                except Exception:
                    pass
                    continue
            return None
        except Exception:
            return None

    def _v28_schema_unit_label(_schema_row: dict) -> str:
        try:
            if not isinstance(_schema_row, dict):
                return ""
            # Prefer schema unit_tag (human-friendly) then unit
            u = (_schema_row.get("unit_tag") or _schema_row.get("unit") or "").strip()
            # Small convenience mapping
            if u == "M":
                return "million units"
            if u == "B":
                return "billion"
            return u
        except Exception:
            return ""

    try:
        if isinstance(canonical_for_render, dict) and isinstance(baseline_sources_cache, list) and baseline_sources_cache:
            _v28_anchor_enforce["attempted"] = True
            _schema = {}
            _anchors = {}
            try:
                if isinstance(prev_response, dict):
                    _schema = prev_response.get("metric_schema_frozen") or {}
                    _anchors = prev_response.get("metric_anchors") or {}
            except Exception:
                pass
                _schema, _anchors = {}, {}
            _v28_anchor_enforce["schema_keys"] = int(len(_schema)) if isinstance(_schema, dict) else 0
            _v28_anchor_enforce["anchors_keys"] = int(len(_anchors)) if isinstance(_anchors, dict) else 0

            if isinstance(_anchors, dict) and _anchors:
                for _ckey, _ainfo in list(_anchors.items()):
                    try:
                        if not _ckey:
                            continue
                        if not isinstance(_ainfo, dict):
                            continue
                        _ah = _ainfo.get("anchor_hash") or _ainfo.get("anchor") or ""
                        if not str(_ah or "").strip() or str(_ah) == "None":
                            continue

                        cand = _v28_pick_by_anchor_hash(baseline_sources_cache, _ah)
                        if not isinstance(cand, dict):
                            _v28_anchor_enforce["misses"] += 1
                            if len(_v28_anchor_enforce["miss_keys_sample"]) < 12:
                                _v28_anchor_enforce["miss_keys_sample"].append(str(_ckey))
                            continue

                        # Build minimal schema-aligned canonical metric
                        srow = _schema.get(_ckey) if isinstance(_schema, dict) else None
                        unit_lbl = _v28_schema_unit_label(srow if isinstance(srow, dict) else {})
                        vnorm = cand.get("value_norm")
                        if vnorm is None:
                            vnorm = cand.get("value")
                        raw = (cand.get("raw") or "").strip()
                        if not raw:
                            try:
                                if vnorm is not None and unit_lbl:
                                    raw = f"{vnorm} {unit_lbl}".strip()
                                elif vnorm is not None:
                                    raw = str(vnorm)
                            except Exception:
                                pass
                                raw = ""

                        cm = canonical_for_render.get(_ckey) if isinstance(canonical_for_render, dict) else None
                        if not isinstance(cm, dict):
                            cm = {}
                        cm["value_norm"] = vnorm
                        cm["unit"] = unit_lbl
                        cm["unit_tag"] = unit_lbl
                        if raw:
                            cm["raw"] = raw
                        # Provide evidence and source hint
                        cm["source_url"] = cand.get("source_url") or cand.get("url") or _ainfo.get("source_url") or ""
                        cm["context_snippet"] = cand.get("context_snippet") or cand.get("context") or _ainfo.get("context_snippet") or ""
                        cm["evidence"] = [cand]
                        cm.setdefault("diag", {})
                        if isinstance(cm.get("diag"), dict):
                            cm["diag"]["v28_anchor_enforced"] = True
                            cm["diag"]["v28_anchor_hash"] = str(_ah)
                            cm["diag"]["v28_anchor_candidate_raw"] = cand.get("raw")
                            cm["diag"]["v28_anchor_candidate_unit"] = cand.get("unit") or cand.get("unit_tag") or ""
                            cm["diag"]["v28_anchor_candidate_value_norm"] = cand.get("value_norm")

                        canonical_for_render[_ckey] = cm
                        _v28_anchor_enforce["hits"] += 1
                        if len(_v28_anchor_enforce["hit_keys_sample"]) < 12:
                            _v28_anchor_enforce["hit_keys_sample"].append(str(_ckey))
                    except Exception:
                        pass
                        continue
# ============================================================
# ============================================================
# ============================================================
    # ============================================================


    except Exception:
        pass


    # ============================================================
    # PATCH START: FIX2D11_RENDER_GATE_FALLBACK_UNANCHORED_V2B
    # Purpose:
    #   Render-gate fallback (UNION / demo mode only).
    #   If V28 anchor enforcement was attempted but produced zero hits,
    #   populate canonical_for_render from current primary_metrics_canonical,
    #   and label metrics as unanchored-for-render.
    #
    # Safety:
    #   - Render-only (does not affect canonicalisation, hashing, snapshots, fastpath)
    #   - UNION mode only
    #   - No new try/except blocks inside compute_source_anchored_diff
    # ============================================================

    _fix2d11_join_mode = None
    if "_fix2d6_get_diff_join_mode_v1" in globals():
        _fix2d11_join_mode = _fix2d6_get_diff_join_mode_v1()

    if (
        isinstance(locals().get("_v28_anchor_enforce"), dict)
        and _v28_anchor_enforce.get("attempted")
        and int(_v28_anchor_enforce.get("hits") or 0) == 0
        and str(_fix2d11_join_mode or "").lower() == "union"
    ):
        _pmc = None
        if isinstance(output.get("results"), dict):
            _pmc = output["results"].get("primary_metrics_canonical")

        if isinstance(_pmc, dict) and _pmc:
            canonical_for_render = dict(_pmc)
            for _k, _cm in canonical_for_render.items():
                if isinstance(_cm, dict):
                    _cm.setdefault("diag", {})
                    if isinstance(_cm.get("diag"), dict):
                        _cm["diag"]["render_origin"] = "current_unanchored"

            _v28_anchor_enforce["fix2d11_fallback_applied"] = True
            _v28_anchor_enforce["fix2d11_fallback_count"] = len(canonical_for_render)
        else:
            _v28_anchor_enforce["fix2d11_fallback_applied"] = False
            _v28_anchor_enforce["fix2d11_fallback_reason"] = "no_current_primary_metrics_canonical"

    # ============================================================
    # PATCH END: FIX2D11_RENDER_GATE_FALLBACK_UNANCHORED_V2B
    # ============================================================

    try:
        if isinstance(output.get("debug"), dict):
            output["debug"]["canonical_for_render_anchor_enforce_v28"] = _v28_anchor_enforce
    except Exception:
        pass
    # =====================================================================
    # END PATCH V28_FORCE_ANCHOR_PICK_FOR_RENDER
    # =====================================================================

    # =====================================================================
    # PATCH V29_CANONICAL_FOR_RENDER_SCHEMA_GATE_AND_JUNK_REJECT (ADDITIVE)
    # =====================================================================
    # Problem:
    # - canonical_for_render can still select "junk" numerics (e.g., footer phone
    #   fragments like -6441 or marketing magnitudes like 2B) as Current, because
    #   the frozen extracted_numbers pool is noisy and some late selection paths
    #   lack strict schema gating.
    #
    # Fix (render-only):
    # - Apply a strict schema-compatibility gate for canonical_for_render values.
    # - Hard-reject phone/contact/email/footer-like contexts.
    # - If the existing canonical_for_render metric is suspicious (unitless for
    #   unit-required dimensions, or unit-incompatible like "B" for percent),
    #   attempt to replace it with the best compatible candidate from the frozen
    #   extracted_numbers pool.
    #
    # Safety:
    # - Render-only: affects ONLY canonical_for_render (dashboard Current).
    # - Does NOT touch fastpath replay, hashing universe, injection lifecycle,
    #   snapshot attach, or extraction.
    #
    # Diagnostics:
    # - output.debug.canonical_for_render_schema_gate_v29 (summary)
    # - per-metric cm["diag"]["v29_schema_gate_*"] flags (when applied)
    # =====================================================================
    _v29_schema_gate = {
        "attempted": False,
        "canonical_keys": 0,
        "suspicious": 0,
        "replaced": 0,
        "kept": 0,
        "candidates_checked": 0,
        "replaced_keys_sample": [],
        "suspicious_keys_sample": [],
        "note": "render-only schema gate + junk reject on canonical_for_render",
    }

    def _v29_s(_x):
        try:
            return str(_x or "")
        except Exception:
            return ""

    def _v29_lower(_x):
        try:
            return _v29_s(_x).lower()
        except Exception:
            return ""

    def _v29_get_text_blob(*parts):
        try:
            out = []
            for p in parts:
                if not p:
                    continue
                if isinstance(p, (list, tuple)):
                    out.extend([_v29_s(z) for z in p if z])
                else:
                    out.append(_v29_s(p))
            return " ".join([z for z in out if z]).strip()
        except Exception:
            return ""

    def _v29_phoneish(text):
        # Catch common phone patterns including "+1-888-600-6441" and fragments.
        try:
            import re
            t = _v29_s(text)
            if not t:
                return False
            if re.search(r"\+\d[\d\-\s]{7,}\d", t):
                return True
            if re.search(r"\b\d{3}[-\s]\d{3}[-\s]\d{4}\b", t):
                return True
            if re.search(r"\bext\.?\s*\d+\b", t, re.I):
                return True
            return False
        except Exception:
            return False

    def _v29_junk_context(text):
        try:
            t = _v29_lower(text)
            if not t:
                return False
            junk_terms = [
                "contact", "email", "phone", "tel", "telephone", "fax", "call us",
                "press release", "copyright", "all rights reserved", "subscribe",
                "unsubscribe", "privacy policy", "terms of use", "cookie", "newsletter",
                "about us", "follow us", "for media", "media contact"
            ]
            if any(w in t for w in junk_terms):
                return True
            if _v29_phoneish(text):
                return True
            # Many PR footers include an email address
            if "@" in t and "." in t:
                return True
            return False
        except Exception:
            return False

    def _v29_unit_str(obj):
        try:
            if isinstance(obj, dict):
                return _v29_s(obj.get("unit") or obj.get("unit_tag") or obj.get("unit_cmp") or obj.get("cur_unit_cmp") or "")
            return ""
        except Exception:
            return ""

    def _v29_value_norm(obj):
        try:
            if isinstance(obj, dict):
                v = obj.get("value_norm")
                if v is None:
                    v = obj.get("cur_value_norm")
                if v is None:
                    v = obj.get("value")
                return v
            return None
        except Exception:
            return None

    def _v29_has_unit_evidence(obj):
        try:
            # Conservative: unit evidence if unit string non-empty OR raw contains %/$/€ etc.
            if not isinstance(obj, dict):
                return False
            u = _v29_unit_str(obj).strip()
            if u:
                return True
            raw = _v29_s(obj.get("raw") or "")
            if "%" in raw or "$" in raw or "€" in raw or "£" in raw:
                return True
            return False
        except Exception:
            return False

    def _v29_expected_dimension(schema_row):
        try:
            if isinstance(schema_row, dict):
                # Prefer FIX16 helper if present
                if "_fix16_expected_dimension" in globals():
                    return _fix16_expected_dimension(schema_row)
                return schema_row.get("dimension") or schema_row.get("unit_family") or ""
            return ""
        except Exception:
            return ""

    def _v29_schema_requires_unit(schema_row):
        try:
            dim = _v29_lower(_v29_expected_dimension(schema_row))
            if dim in ("currency", "percent", "rate", "ratio"):
                return True
            uf = _v29_lower(_v29_s(schema_row.get("unit_family") if isinstance(schema_row, dict) else ""))
            if uf in ("currency", "percent", "rate", "ratio"):
                return True
            # unit_sales / unit counts should have some "unit-ness"
            if "unit" in dim or "unit" in uf:
                return True
            return False
        except Exception:
            return False

    def _v29_unit_compatible(schema_row, cand_obj):
        try:
            if isinstance(schema_row, dict) and isinstance(cand_obj, dict):
                if "_fix16_unit_compatible" in globals():
                    return bool(_fix16_unit_compatible(schema_row, cand_obj))
            # fallback heuristic
            dim = _v29_lower(_v29_expected_dimension(schema_row))
            u = _v29_lower(_v29_unit_str(cand_obj))
            raw = _v29_lower(_v29_s(cand_obj.get("raw") or ""))
            if dim == "percent":
                return ("%"
                        in u) or ("percent" in u) or ("%" in raw)
            if dim == "currency":
                return any(sym in raw for sym in ["$", "€", "£"]) or ("usd" in u) or ("eur" in u) or ("sgd" in u) or ("currency" in u)
            if "unit" in dim:
                # must not be percent/currency-like
                if "%" in raw or "%" in u:
                    return False
                if any(sym in raw for sym in ["$", "€", "£"]):
                    return False
                # prefer explicit unit words
                if "unit" in u or "vehicle" in u or "car" in u or "sales" in u:
                    return True
                # allow million/billion with implied units only if raw/context says units/sales
                ctx = _v29_lower(_v29_get_text_blob(cand_obj.get("context_snippet"), cand_obj.get("context")))
                if ("unit" in ctx) or ("sales" in ctx) or ("vehicle" in ctx):
                    return True
                return False
            return True
        except Exception:
            return False

    def _v29_is_suspicious_current(schema_row, cm):
        try:
            if not isinstance(cm, dict):
                return True
            dim = _v29_lower(_v29_expected_dimension(schema_row))
            requires_unit = _v29_schema_requires_unit(schema_row)
            u = _v29_lower(_v29_unit_str(cm))
            v = _v29_value_norm(cm)
            blob = _v29_get_text_blob(cm.get("raw"), cm.get("context_snippet"), cm.get("source_url"))
            if _v29_junk_context(blob):
                return True
            if requires_unit and not _v29_has_unit_evidence(cm):
                return True
            # Percent metrics must not carry magnitude units like B/M
            if dim == "percent" and ("b" in u or "m" in u) and "%" not in u:
                return True
            if dim == "percent" and isinstance(v, (int, float)) and abs(float(v)) > 1000:
                return True
            # Unit sales must not be negative (phone fragments, ids)
            if "unit" in dim and isinstance(v, (int, float)) and float(v) < 0:
                return True
            return not _v29_unit_compatible(schema_row, cm)
        except Exception:
            return True

    def _v29_keywords(schema_row):
        try:
            import re
            if not isinstance(schema_row, dict):
                return []
            nm = _v29_lower(schema_row.get("name") or schema_row.get("label") or schema_row.get("display_name") or "")
            toks = [t for t in re.split(r"[^a-z0-9]+", nm) if t and len(t) >= 4]
            # prune common filler
            bad = set(["global", "projected", "market", "share", "sales", "volume", "units", "unit", "year"])
            toks = [t for t in toks if t not in bad]
            return toks[:10]
        except Exception:
            return []

    def _v29_score_candidate(schema_row, cand_obj):
        try:
            score = 0
            u = _v29_lower(_v29_unit_str(cand_obj))
            raw = _v29_lower(_v29_s(cand_obj.get("raw") or ""))
            ctx = _v29_lower(_v29_get_text_blob(cand_obj.get("context_snippet"), cand_obj.get("context")))
            dim = _v29_lower(_v29_expected_dimension(schema_row))
            if dim == "percent":
                if "%" in raw or "%" in u or "percent" in u:
                    score += 5
                if "b" in u or "m" in u:
                    score -= 4
            if "unit" in dim:
                if "unit" in ctx or "sales" in ctx or "vehicle" in ctx:
                    score += 3
                if "%" in raw or "$" in raw:
                    score -= 3
            if not _v29_junk_context(ctx + " " + raw):
                score += 1
            for kw in _v29_keywords(schema_row):
                if kw and kw in ctx:
                    score += 1
            return score
        except Exception:
            return 0

    try:
        if isinstance(canonical_for_render, dict) and isinstance(prev_response, dict) and isinstance(baseline_sources_cache, list) and baseline_sources_cache:
            _v29_schema_gate["attempted"] = True
            _schema = prev_response.get("metric_schema_frozen") or {}
            _v29_schema_gate["canonical_keys"] = int(len(canonical_for_render)) if isinstance(canonical_for_render, dict) else 0

            for _ckey, _cm in list(canonical_for_render.items()):
                try:
                    if not _ckey:
                        continue
                    srow = _schema.get(_ckey) if isinstance(_schema, dict) else None
                    if not isinstance(srow, dict):
                        # without schema, we cannot safely gate; keep
                        _v29_schema_gate["kept"] += 1
                        continue

                    if _v29_is_suspicious_current(srow, _cm):
                        _v29_schema_gate["suspicious"] += 1
                        if len(_v29_schema_gate["suspicious_keys_sample"]) < 12:
                            _v29_schema_gate["suspicious_keys_sample"].append(str(_ckey))

                        best = None
                        best_score = -10**9
                        # search frozen pool for compatible candidates
                        for cand in _v28_iter_numbers_from_sources_cache(baseline_sources_cache):
                            _v29_schema_gate["candidates_checked"] += 1
                            if not isinstance(cand, dict):
                                continue
                            blob = _v29_get_text_blob(cand.get("raw"), cand.get("context_snippet"), cand.get("context"), cand.get("source_url") or cand.get("url"))
                            if _v29_junk_context(blob):
                                continue
                            # normalize candidate
                            cobj = dict(cand)
                            # ensure value_norm present if possible
                            if cobj.get("value_norm") is None and cobj.get("value") is not None:
                                cobj["value_norm"] = cobj.get("value")
                            if not _v29_unit_compatible(srow, cobj):
                                continue
                            if _v29_schema_requires_unit(srow) and not _v29_has_unit_evidence(cobj):
                                continue
                            sc = _v29_score_candidate(srow, cobj)
                            if sc > best_score:
                                best_score = sc
                                best = cobj
                        if isinstance(best, dict):
                            prior_v = _v29_value_norm(_cm if isinstance(_cm, dict) else {})
                            prior_u = _v29_unit_str(_cm if isinstance(_cm, dict) else {})
                            # overwrite with best candidate
                            if not isinstance(_cm, dict):
                                _cm = {}
                            _cm["value_norm"] = best.get("value_norm")
                            _cm["unit"] = _v28_schema_unit_label(srow) or (_v29_unit_str(best) or _v28_schema_unit_label(srow))
                            _cm["unit_tag"] = _cm.get("unit")
                            if best.get("raw"):
                                _cm["raw"] = best.get("raw")
                            _cm["source_url"] = best.get("source_url") or best.get("url") or _cm.get("source_url") or ""
                            _cm["context_snippet"] = best.get("context_snippet") or best.get("context") or _cm.get("context_snippet") or ""
                            _cm["evidence"] = [best]
                            _cm.setdefault("diag", {})
                            if isinstance(_cm.get("diag"), dict):
                                _cm["diag"]["v29_schema_gate_replaced"] = True
                                _cm["diag"]["v29_schema_gate_prior_value_norm"] = prior_v
                                _cm["diag"]["v29_schema_gate_prior_unit"] = prior_u
                                _cm["diag"]["v29_schema_gate_best_score"] = best_score
                                _cm["diag"]["v29_schema_gate_best_raw"] = best.get("raw")
                                _cm["diag"]["v29_schema_gate_best_unit"] = best.get("unit") or best.get("unit_tag") or ""
                            canonical_for_render[_ckey] = _cm
                            _v29_schema_gate["replaced"] += 1
                            if len(_v29_schema_gate["replaced_keys_sample"]) < 12:
                                _v29_schema_gate["replaced_keys_sample"].append(str(_ckey))
                        else:
                            _v29_schema_gate["kept"] += 1
                    else:
                        _v29_schema_gate["kept"] += 1
                except Exception:
                    pass
                    continue
    except Exception:
        pass

    try:
        if isinstance(output.get("debug"), dict):
            output["debug"]["canonical_for_render_schema_gate_v29"] = _v29_schema_gate
    except Exception:
        pass
    # =====================================================================
    # END PATCH V29_CANONICAL_FOR_RENDER_SCHEMA_GATE_AND_JUNK_REJECT
    # =====================================================================


# PATCH V22_CANONICAL_FOR_RENDER_NORMALIZE (ADDITIVE): normalize canonical_for_render metric dicts so that
    # downstream row hydration does not overwrite Current with blanks when the rebuilt dict uses alternate fields.
    # - Derives value_norm/unit/raw from common alternate keys and evidence entries.
    # - Purely render-layer enrichment; does NOT alter selection/hashing.
    def _v22_extract_numeric(v):
        try:
            if v is None:
                return None
            if isinstance(v, (int, float)):
                return float(v)
            s = str(v).strip()
            if not s:
                return None
            # strip commas
            s2 = s.replace(",", "")
            return float(s2)
        except Exception:
            return None

    def _v22_norm_metric(cm: dict) -> dict:
        try:
            if not isinstance(cm, dict):
                return cm
            # value_norm
            vn = cm.get("value_norm")
            if vn is None:
                for k in ("value", "value_num", "value_float", "norm_value", "canonical_value_norm"):
                    if k in cm and cm.get(k) is not None:
                        vn = cm.get(k)
                        break
            # evidence-derived
            if vn is None and isinstance(cm.get("evidence"), list) and cm.get("evidence"):
                try:
                    ev0 = cm.get("evidence")[0]
                    if isinstance(ev0, dict):
                        vn = ev0.get("value_norm") if ev0.get("value_norm") is not None else ev0.get("value")
                except Exception:
                    pass
            vn_f = _v22_extract_numeric(vn)
            if vn_f is not None:
                cm["value_norm"] = vn_f

            # unit
            unit = (cm.get("unit") or cm.get("unit_tag") or cm.get("unit_label") or "").strip()
            if (not unit) and isinstance(cm.get("evidence"), list) and cm.get("evidence"):
                try:
                    ev0 = cm.get("evidence")[0]
                    if isinstance(ev0, dict):
                        unit = (ev0.get("unit") or ev0.get("unit_tag") or "").strip()
                except Exception:
                    pass
            if unit:
                cm["unit"] = unit

            # raw/display
            raw = (cm.get("raw") or cm.get("value_raw") or cm.get("raw_value") or cm.get("display") or "").strip()
            if not raw:
                try:
                    if cm.get("value_norm") is not None and (cm.get("unit") or ""):
                        raw = f"{cm.get('value_norm')} {cm.get('unit')}".strip()
                    elif cm.get("value_norm") is not None:
                        raw = str(cm.get("value_norm"))
                except Exception:
                    pass
                    raw = ""
            if raw:
                cm["raw"] = raw
            cm.setdefault("diag", {})
            if isinstance(cm.get("diag"), dict):
                cm["diag"].setdefault("v22_norm", True)
            return cm
        except Exception:
            return cm

    try:
        if isinstance(canonical_for_render, dict) and canonical_for_render:
            for _k, _m in list(canonical_for_render.items()):
                if isinstance(_m, dict):
                    canonical_for_render[_k] = _v22_norm_metric(_m)
    except Exception:
        pass


    # =====================================================================
    # PATCH V30_STRICT_SCHEMA_UNIT_GATE (ADDITIVE)
    # Goal:
    # - Apply an analysis-like schema/unit compatibility gate at render-time.
    # - Explicitly reject obviously incompatible unit evidence (e.g. "2.0 B" for a % metric,
    #   or unitless negatives like "-6441" for a unit_sales metric).
    #
    # Notes:
    # - Render-only: does not affect extraction, snapshot attach, hashing, or fastpath replay.
    # - Best-effort: only runs if FIX16 helpers are present.
    # =====================================================================
    _v30_strict_gate = {"attempted": False, "dropped": 0, "dropped_keys_sample": []}
    try:
        _v30_strict_gate["attempted"] = True
        _schema = {}
        try:
            if isinstance(prev_response, dict):
                _schema = prev_response.get("metric_schema_frozen") or {}
        except Exception:
            pass
            _schema = {}

        _fix16_exp = globals().get("_fix16_expected_dimension")
        _fix16_comp = globals().get("_fix16_unit_compatible")
        _fix16_has_unit = globals().get("_fix16_candidate_has_any_unit")
        if callable(_fix16_exp) and callable(_fix16_comp) and isinstance(_schema, dict) and _schema:
            _dropped = []
            for _ck, _m in list((canonical_for_render or {}).items()):
                if not isinstance(_m, dict):
                    continue
                _defn = _schema.get(_ck) or {}
                try:
                    _expected = _fix16_exp(_defn)
                except Exception:
                    pass
                    _expected = None

                # pull unit evidence in the same style analysis expects
                _unit_tag = str(_m.get("unit_tag") or _m.get("unit") or "").strip()
                _raw = str(_m.get("raw") or _m.get("raw_value") or "").strip()
                _has_any = False
                try:
                    if callable(_fix16_has_unit):
                        _has_any = bool(_fix16_has_unit(_m))
                except Exception:
                    pass
                    _has_any = bool(_unit_tag) or ("% " in (_raw + " ") or "$" in _raw)

                _ok = True
                try:
                    _ok = bool(_fix16_comp(_expected, _unit_tag, _has_any))
                except Exception:
                    pass
                    _ok = True

                if not _ok:
                    _dropped.append(_ck)
                    try:
                        canonical_for_render.pop(_ck, None)
                    except Exception:
                        pass

            _v30_strict_gate["dropped"] = len(_dropped)
            _v30_strict_gate["dropped_keys_sample"] = list(_dropped[:12])
    except Exception:
        pass

    # Diff using existing diff helper if present, but FORCE cur_response to canonical-for-render.
    metric_changes = []
    cur_resp_for_diff = None
    try:
        fn_diff = globals().get("diff_metrics_by_name")
        if callable(fn_diff):
            cur_resp_for_diff = {"primary_metrics_canonical": canonical_for_render}
            # =====================================================================
            # PATCH V34E_DIFF_METRIC_CHANGES_ANCHOR_INPUT (ADDITIVE)
            # Ensure the diff layer receives metric_anchors for CURRENT so that
            # v34 anchor-hash secondary join can resolve drifting canonical_keys.
            # - Deterministic, inference-free: only uses anchor_hash already present
            #   on canonical_for_render entries (if any).
            # =====================================================================
            try:
                _cur_metric_anchors = {}
                if isinstance(canonical_for_render, dict):
                    for _ckey, _m in canonical_for_render.items():
                        if not isinstance(_ckey, str):
                            continue
                        if not isinstance(_m, dict):
                            continue
                        _ah = _m.get("anchor_hash")
                        if not _ah and isinstance(_m.get("anchor"), dict):
                            _ah = _m.get("anchor", {}).get("anchor_hash")
                        if not _ah and isinstance(_m.get("anchor_meta"), dict):
                            _ah = _m.get("anchor_meta", {}).get("anchor_hash")
                        # Only emit when present and non-null-ish
                        if _ah and str(_ah).lower() not in ("none", "null", ""):
                            _cur_metric_anchors[_ckey] = {"anchor_hash": str(_ah)}
                if _cur_metric_anchors:
                    cur_resp_for_diff["metric_anchors"] = _cur_metric_anchors
            except Exception:
                pass
            # =====================================================================
            # END PATCH V34E_DIFF_METRIC_CHANGES_ANCHOR_INPUT
            # =====================================================================


            # =====================================================================
            # PATCH V27_DISABLE_NUMERIC_INFERENCE_FLAG (ADDITIVE)
            # Signal to diff layer: when canonical-for-render is active, do NOT
            # infer/parse numeric values for CURRENT from free-form strings.
            # =====================================================================
            try:
                cur_resp_for_diff["_disable_numeric_inference_v27"] = True
            except Exception:
                pass
            # =====================================================================
            # END PATCH V27_DISABLE_NUMERIC_INFERENCE_FLAG
            # =====================================================================
            # =====================================================================
            # PATCH V24_STRICT_CKEY_FLAG (ADDITIVE)
            # When using canonical-for-render, force strict canonical_key identity matching in diff layer.
            # This prevents cross-metric substitution (e.g., 2.0 B / 170.0 / year values) from fallback matchers.
            # =====================================================================
            try:
                cur_resp_for_diff["_ph2b_strict_ckey_v24"] = True
            except Exception:
                pass
            metric_changes, unchanged, increased, decreased, found = fn_diff(prev_response, cur_resp_for_diff)
            # =====================================================================
            # PATCH V34E_DIFF_JOIN_SUMMARY_SURFACE (ADDITIVE)
            # Surface v34 join summary (if produced by diff layer) into top-level debug
            # so it appears in the Evolution JSON for audit.
            # =====================================================================
            try:
                _dj = None
                if isinstance(cur_resp_for_diff, dict):
                    _dj = (cur_resp_for_diff.get("debug") or {}).get("diff_join_anchor_v34")
                if isinstance(_dj, dict):
                    try:
                        if "debug" not in output or not isinstance(output.get("debug"), dict):
                            output["debug"] = {}
                    except Exception:
                        pass
                        output["debug"] = {}
                    try:
                        output["debug"]["diff_join_anchor_v34"] = _dj
                    except Exception:
                        pass
            except Exception:
                pass
            # =====================================================================
            # END PATCH V34E_DIFF_JOIN_SUMMARY_SURFACE
            # =====================================================================


        else:
            metric_changes, unchanged, increased, decreased, found = ([], 0, 0, 0, 0)
    except Exception:
        pass
        metric_changes, unchanged, increased, decreased, found = ([], 0, 0, 0, 0)

    # Post-process diff rows: ensure "Current" fields are derived from canonical-for-render.
    _row_audit = {
        "rows_total": int(len(metric_changes or [])),
        "rows_hydrated": 0,
        "rows_missing_canonical": 0,
        "rows_skipped_missing_fields": 0,
        "rows_with_prior_current_overridden": 0,
    }
    try:
        if isinstance(metric_changes, list) and isinstance(canonical_for_render, dict):
            for row in metric_changes:
                if not isinstance(row, dict):
                    continue
                ckey = row.get("canonical_key") or row.get("canonical") or row.get("canonical_id") or ""
                if not ckey:
                    continue
                cm = canonical_for_render.get(ckey)
                if not isinstance(cm, dict):
                    _row_audit["rows_missing_canonical"] += 1
                    continue

                # PATCH V22_ROW_HYDRATE_GUARD (ADDITIVE): only override if canonical metric has usable fields
                # Prevents overwriting a previously non-empty current with blanks when canon metric is sparse.
                _cm_vn = cm.get("value_norm")
                _cm_unit = (cm.get("unit") or cm.get("unit_tag") or "").strip()
                _cm_raw = (cm.get("raw") or cm.get("value_raw") or cm.get("raw_value") or "").strip()
                if _cm_vn is None and (not _cm_raw):
                    # no usable canonical payload to hydrate from
                    _row_audit["rows_skipped_missing_fields"] += 1
                    row.setdefault("diag", {})
                    if isinstance(row.get("diag"), dict):
                        row["diag"].setdefault("canonical_for_render_v1", {})
                        row["diag"]["canonical_for_render_v1"]["applied"] = False
                        row["diag"]["canonical_for_render_v1"]["reason"] = "skipped_missing_canonical_fields"
                        row["diag"]["canonical_for_render_v1"]["fn"] = _canonical_for_render_fn
                    continue

                # Capture prior values for audit if we are overriding
                prior = {
                    "current_value": row.get("current_value"),
                    "current_value_norm": row.get("current_value_norm"),
                    "cur_value_norm": row.get("cur_value_norm"),
                    "cur_unit_cmp": row.get("cur_unit_cmp"),
                    "current_unit": row.get("current_unit"),
                }

                # Hydrate from canonical metric
                vnorm = cm.get("value_norm")
                unit = (cm.get("unit") or cm.get("unit_tag") or "").strip()
                raw = (cm.get("raw") or cm.get("value_raw") or cm.get("raw_value") or "").strip()
                if not raw:
                    # Build a lightweight display string when raw isn't available
                    try:
                        if vnorm is not None and unit:
                            raw = f"{vnorm} {unit}".strip()
                        elif vnorm is not None:
                            raw = str(vnorm)
                    except Exception:
                        pass
                        raw = row.get("current_value") or ""

                # Apply canonical-for-render to diff row
                row["current_value_norm"] = vnorm
                row["cur_value_norm"] = vnorm
                row["current_unit"] = unit
                row["cur_unit_cmp"] = unit
                row["current_value"] = raw

                # PATCH V22_CLEAR_UNIT_MISMATCH_ON_CANON (ADDITIVE): if canonical-for-render provides
                # a schema-aligned unit+value, clear any prior unit_mismatch that came from raw/fallback.
                try:
                    if (vnorm is not None) and str(unit or "").strip():
                        if row.get("unit_mismatch") is True:
                            row["unit_mismatch"] = False
                        if row.get("change_type") in ("unit_mismatch", "invalid_current"):
                            pv = row.get("previous_value")
                            pvf = _v22_extract_numeric(pv)
                            cvf = _v22_extract_numeric(vnorm)
                            if pvf is not None and cvf is not None:
                                if abs(cvf - pvf) < 1e-9:
                                    row["change_type"] = "unchanged"
                                elif cvf > pvf:
                                    row["change_type"] = "increased"
                                else:
                                    row["change_type"] = "decreased"
                except Exception:
                    pass

                # Range fields (if present in canonical)
                if isinstance(cm.get("value_range"), dict):
                    row["current_value_range"] = cm.get("value_range")
                if (cm.get("value_range_display") or "").strip():
                    row["current_value_range_display"] = cm.get("value_range_display")

                # Audit stamp
                row.setdefault("diag", {})
                if isinstance(row.get("diag"), dict):
                    row["diag"].setdefault("canonical_for_render_v1", {})
                    row["diag"]["canonical_for_render_v1"]["applied"] = True
                    row["diag"]["canonical_for_render_v1"]["fn"] = _canonical_for_render_fn
                    row["diag"]["canonical_for_render_v1"]["reason"] = _canonical_for_render_reason
                    row["diag"]["canonical_for_render_v1"]["prior_current"] = prior

                # =====================================================================
                # PATCH V26_LOCK_CANONICAL_CURRENT_FIELDS (ADDITIVE)
                # Goal: Once a row is hydrated from canonical-for-render, lock the "Current" fields
                #       so later post-processing (heuristics/sanitizers) cannot overwrite them.
                # This is render-only and does NOT affect hashing/fastpath/snapshot attach.
                # =====================================================================
                try:
                    row["_lock_current_v26"] = True
                    # ensure nested dict exists
                    row.setdefault("diag", {})
                    if isinstance(row.get("diag"), dict):
                        row["diag"].setdefault("canonical_for_render_v1", {})
                        if isinstance(row["diag"].get("canonical_for_render_v1"), dict):
                            row["diag"]["canonical_for_render_v1"]["locked_current_v26"] = {
                                "current_value": row.get("current_value"),
                                "current_value_norm": row.get("current_value_norm"),
                                "cur_value_norm": row.get("cur_value_norm"),
                                "cur_unit_cmp": row.get("cur_unit_cmp"),
                                "current_unit": row.get("current_unit"),
                                "current_value_range": row.get("current_value_range"),
                                "current_value_range_display": row.get("current_value_range_display"),
                            }
                except Exception:
                    pass
                # =====================================================================
                # END PATCH V26_LOCK_CANONICAL_CURRENT_FIELDS
                # =====================================================================




                # Determine if we actually changed the row
                if prior.get("cur_value_norm") != vnorm or str(prior.get("cur_unit_cmp") or "") != unit or str(prior.get("current_value") or "") != raw:
                    _row_audit["rows_with_prior_current_overridden"] += 1
                _row_audit["rows_hydrated"] += 1
    except Exception:
        pass

    # Attach diagnostics for auditability
    try:
        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            output["debug"]["canonical_for_render_v1"] = {
                "applied": bool(_canonical_for_render_applied),
                "reason": str(_canonical_for_render_reason or ""),
                "fn": str(_canonical_for_render_fn or ""),
                "rebuilt_count": int(_canonical_for_render_count or 0),
                "keys_sample": list(_canonical_for_render_keys_sample or []),
                # =====================================================================
                # PATCH FIX2W_CANONICAL_FOR_RENDER_DIAG_EXT_V1 (ADDITIVE)
                # =====================================================================
                "diag_ext": (lambda: {
                    "current_metrics_count": int(len(current_metrics)) if isinstance(current_metrics, dict) else 0,
                    "baseline_sources_cache_current_rows": int(len(baseline_sources_cache_current)) if isinstance(baseline_sources_cache_current, list) else 0,
                    "baseline_sources_cache_prev_rows": int(len(baseline_sources_cache_prev)) if isinstance(baseline_sources_cache_prev, list) else 0,
                    "has_web_context": bool(isinstance(web_context, dict)),
                    "web_context_extra_urls_count": int(len(web_context.get("extra_urls") or [])) if isinstance(web_context, dict) and isinstance(web_context.get("extra_urls"), list) else 0,
                    "web_context_fix2v_binding": (web_context.get("fix2v_candidate_binding_v1") or {}) if isinstance(web_context, dict) else {},
                    "reason_is_empty_rebuild": bool(str(_canonical_for_render_reason or "") in ("render_rebuild_failed_or_empty", "forced_render_rebuild_due_to_suspicious_existing_failed")),
                })(),
                # END PATCH FIX2W_CANONICAL_FOR_RENDER_DIAG_EXT_V1
                "replaced_current_metrics_for_render": bool(_canonical_for_render_replaced_current_metrics),
            }
            # ============================================================
            # PATCH START: FIX2D5_MIRROR_CANONICAL_FOR_RENDER_V1
            # Purpose:
            #   Diff/dashboard diagnostics consume results.debug.*, but canonical_for_render_v1
            #   was attached to output.debug only. Mirror into output.results.debug to prevent
            #   false "missing_output_debug.canonical_for_render_v1" diagnoses.
            #   (Additive; no behavior change)
            # ============================================================
            try:
                if isinstance(output.get("results"), dict):
                    output["results"].setdefault("debug", {})
                    if isinstance(output["results"].get("debug"), dict):
                        output["results"]["debug"]["canonical_for_render_v1"] = dict(output["debug"].get("canonical_for_render_v1") or {})
            except Exception:
                pass
            # ============================================================
            # PATCH END: FIX2D5_MIRROR_CANONICAL_FOR_RENDER_V1
            # ============================================================


            output["debug"]["canonical_for_render_row_audit_v1"] = _row_audit
    except Exception:
        pass
    # =====================================================================
    # END PATCH V20_CANONICAL_FOR_RENDER
    # =====================================================================


    # =====================================================================
    # REFACTOR56: Diff Panel V2 lastmile (downsizing)
    # Objective:
    # - Keep Diff Panel V2 as the authoritative producer of the metric changes table feed.
    # - Do not emit metric_changes_legacy (removed).
    # Safety:
    # - Render-only. No inference. No changes to hashing/fastpath/snapshots/extraction.
    # =====================================================================
    _diff_v2_rows = []
    _diff_v2_summary = None
    # =====================================================================
    # REFACTOR45: Diff Panel V2 hardening (RecursionError-safe)
    # - Always pass minimal, acyclic wrappers into the V2 builder.
    # - Capture traceback for any V2 builder failure.
    # - If V2 fails, DO NOT overwrite previously computed rows; otherwise
    #   synthesize strict canonical-join rows so the panel never goes empty
    #   when prev/cur canonical maps are present.
    # =====================================================================
    try:
        import traceback as _tb
    except Exception:
        _tb = None

    _prev_can_v2 = {}
    _cur_can_v2 = {}

    try:
        _fn_v2 = (globals().get("build_diff_metrics_panel_v2__rows_refactor47")
                 or globals().get("build_diff_metrics_panel_v2__rows")
                 or globals().get("build_diff_metrics_panel_v2"))
        if callable(_fn_v2):
            # Previous (baseline) canonical map
            try:
                _prev_can_v2 = _diffpanel_v2__unwrap_primary_metrics_canonical(prev_response)
            except Exception:
                _prev_can_v2 = {}
            if not isinstance(_prev_can_v2, dict):
                try:
                    _prev_can_v2 = prev_response.get("primary_metrics_canonical") if isinstance(prev_response, dict) else {}
                except Exception:
                    _prev_can_v2 = {}
            if not isinstance(_prev_can_v2, dict):
                _prev_can_v2 = {}

            # Current canonical map (prefer already rebuilt current_metrics)
            try:
                if isinstance(current_metrics, dict) and current_metrics:
                    _cur_can_v2 = current_metrics
                else:
                    _cur_can_v2 = _diffpanel_v2__unwrap_primary_metrics_canonical(cur_resp_for_diff) if isinstance(cur_resp_for_diff, dict) else {}
            except Exception:
                _cur_can_v2 = {}
            if not isinstance(_cur_can_v2, dict):
                _cur_can_v2 = {}

            # Minimal wrappers (avoid passing the full evolution output dict)
            _prev_for_v2 = {"primary_metrics_canonical": dict(_prev_can_v2)}
            _cur_for_v2 = {"primary_metrics_canonical": dict(_cur_can_v2)}

            # Carry schema freeze map if present (helps unit-family expectations)
            try:
                _msf = None
                if isinstance(prev_response, dict):
                    _msf = prev_response.get("metric_schema_frozen") or (prev_response.get("results") or {}).get("metric_schema_frozen")
                if _msf is None and isinstance(output, dict):
                    _msf = output.get("metric_schema_frozen") or (output.get("results") or {}).get("metric_schema_frozen")
                if isinstance(_msf, dict) and _msf:
                    _prev_for_v2["metric_schema_frozen"] = _msf
                    _cur_for_v2["metric_schema_frozen"] = _msf
            except Exception:
                pass

            # Attach observed-number pools (source_results) for V2 "observed" promotion
            try:
                _sr = None
                if isinstance(baseline_sources_cache_current, list) and baseline_sources_cache_current:
                    _sr = baseline_sources_cache_current
                elif isinstance(baseline_sources_cache, list) and baseline_sources_cache:
                    _sr = baseline_sources_cache
                elif isinstance(output, dict):
                    _sr = output.get("source_results") or ((output.get("results") or {}).get("source_results"))
                if isinstance(_sr, list) and _sr:
                    _cur_for_v2.setdefault("results", {})
                    if isinstance(_cur_for_v2.get("results"), dict) and (not isinstance(_cur_for_v2["results"].get("source_results"), list) or not _cur_for_v2["results"].get("source_results")):
                        _cur_for_v2["results"]["source_results"] = _sr
            except Exception:
                pass

            _diff_v2_rows, _diff_v2_summary = _fn_v2(_prev_for_v2, _cur_for_v2 or {})
    except Exception as _e:
        try:
            _dbg = output.setdefault("debug", {})
            if isinstance(_dbg, dict):
                # REFACTOR57: suppress diff_panel_v2_error emission; fallback builder handles rows.
                try:
                    if _tb is not None:
                        _dbg["diff_panel_v2_traceback"] = _tb.format_exc()
                except Exception:
                    pass
        except Exception:
            pass

        # If rows were already computed earlier in this function, preserve them.
        try:
            _existing = output.get("metric_changes_v2")
            if isinstance(_existing, list) and _existing:
                _diff_v2_rows = _existing
                _diff_v2_summary = None
            else:
                # Strict canonical-key join fallback (no inference, no heuristics)
                _diff_v2_rows = []
                _prev_map = _prev_can_v2 if isinstance(_prev_can_v2, dict) else {}
                _cur_map = _cur_can_v2 if isinstance(_cur_can_v2, dict) else {}

                def _vn(_m):
                    try:
                        if isinstance(_m, dict):
                            v = _m.get("value_norm")
                            if v is None:
                                v = _m.get("value")
                            return float(v) if v is not None else None
                    except Exception:
                        return None
                    return None

                def _unit(_m):
                    try:
                        if isinstance(_m, dict):
                            return (_m.get("unit_tag") or _m.get("unit") or None)
                    except Exception:
                        return None
                    return None

                for _ck in sorted([k for k in _prev_map.keys() if isinstance(k, str) and k]):
                    _pm = _prev_map.get(_ck) if isinstance(_prev_map, dict) else None
                    _cm = _cur_map.get(_ck) if isinstance(_cur_map, dict) else None

                    _pv = _vn(_pm)
                    _cv = _vn(_cm)
                    _pu = _unit(_pm)
                    _cu = _unit(_cm)

                    _is_comp = (_pv is not None and _cv is not None and str(_pu or "") == str(_cu or ""))
                    _d = None
                    _pct = None
                    _ctype = "unknown"
                    if _is_comp:
                        _d = float(_cv) - float(_pv)
                        if abs(float(_pv)) > 1e-12:
                            _pct = (_d / float(_pv)) * 100.0
                        if abs(_d) < 1e-9:
                            _ctype = "unchanged"
                        elif _d > 0:
                            _ctype = "increased"
                        else:
                            _ctype = "decreased"
                    else:
                        if (_pm is None) and (_cm is not None):
                            _ctype = "added"
                        elif _cm is None:
                            _ctype = "not_found"

                    _name = None
                    try:
                        _name = ((_pm or {}).get("name") if isinstance(_pm, dict) else None) or ((_cm or {}).get("name") if isinstance(_cm, dict) else None) or _ck
                    except Exception:
                        _name = _ck

                    _diff_v2_rows.append({
                        "canonical_key": _ck,
                        "name": _name,
                        "previous_value": _pv,
                        "current_value": (_cv if _cm is not None else "N/A"),
                        "previous_unit": _pu,
                        "current_unit": (_cu if _cm is not None else None),
                        "prev_value_norm": _pv,
                        "cur_value_norm": (_cv if _cm is not None else None),
                        "delta_abs": _d,
                        "delta_pct": _pct,
                        "change_type": _ctype,
                        "baseline_is_comparable": bool(_is_comp),
                        "current_method": "strict_fallback_v2",
                        "source_url": None,
                    })
                _diff_v2_summary = {"rows_total": int(len(_diff_v2_rows)), "builder_id": "REFACTOR45_STRICT_FALLBACK"}

        except Exception:
            _diff_v2_rows, _diff_v2_summary = ([], None)

    # Persist V2 artifacts for auditability
    try:
        output["metric_changes_v2"] = _diff_v2_rows or []
    except Exception:
        pass
    try:
        if isinstance(_diff_v2_summary, dict):
            _dbg = output.setdefault("debug", {})
            if isinstance(_dbg, dict):
                _dbg["diff_panel_v2_summary"] = _diff_v2_summary
    except Exception:
        pass

    # Option B: override UI feed if V2 emitted any rows
    try:
        if isinstance(_diff_v2_rows, list) and _diff_v2_rows:
            metric_changes = _diff_v2_rows
            try:
                found = int(_diff_v2_summary.get("rows_total", found)) if isinstance(_diff_v2_summary, dict) else found
            except Exception:
                pass
    except Exception:
        pass

    output["metric_changes"] = metric_changes or []
    output["summary"]["total_metrics"] = len(output["metric_changes"])
    output["summary"]["metrics_found"] = int(found or 0)
    output["summary"]["metrics_increased"] = int(increased or 0)
    output["summary"]["metrics_decreased"] = int(decreased or 0)
    output["summary"]["metrics_unchanged"] = int(unchanged or 0)

    total = max(1, len(output["metric_changes"]))
    output["stability_score"] = (output["summary"]["metrics_unchanged"] / total) * 100.0

    output["source_results"] = baseline_sources_cache[:50]
    output["sources_checked"] = len(baseline_sources_cache)
    output["sources_fetched"] = len(baseline_sources_cache)

    try:
        total_nums = 0
        for sr in baseline_sources_cache or []:
            if isinstance(sr, dict) and isinstance(sr.get("extracted_numbers"), list):
                total_nums += len(sr.get("extracted_numbers") or [])
        output["numbers_extracted_total"] = int(total_nums)
    except Exception:
        pass

    output["message"] = "Source-anchored evolution completed (snapshot-gated, analysis-aligned)."
    output["interpretation"] = "Evolution used cached source snapshots only; no brute-force candidate harvesting."

    # =====================================================================
    # PATCH FIX35 (ADDITIVE): attach bad-current traces for unit-required metrics
    # - If a diff row shows a year-like integer as current for a unit-required metric,
    #   emit a compact trace: origin, schema unit_family, current fields, and top candidates.
    # =====================================================================
    try:
        if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
            bad_traces = {}
            # Build a flattened candidate pool once (from snapshots only)
            flat = []
            for sr in baseline_sources_cache or []:
                if isinstance(sr, dict):
                    for c in (sr.get("extracted_numbers") or []):
                        if isinstance(c, dict):
                            flat.append(c)

            def _is_yearlike(v):
                try:
                    iv = int(float(v))
                    return 1900 <= iv <= 2100
                except Exception:
                    return False

            def _schema_unit_required(md: dict, ckey: str = "") -> bool:
                uf = ((md or {}).get("unit_family") or (md or {}).get("unit") or "").strip().lower()
                if uf in {"currency", "percent", "rate", "ratio"}:
                    return True
                ck = (ckey or "").lower().strip()
                return ck.endswith("__currency") or ck.endswith("__percent") or ck.endswith("__rate") or ck.endswith("__ratio")

            def _cand_unit_evidence(c: dict) -> bool:
                if not isinstance(c, dict):
                    return False
                if (c.get("unit_tag") or c.get("unit") or c.get("unit_norm") or c.get("unit_raw") or "").strip():
                    return True
                if (c.get("currency") or c.get("currency_symbol") or "").strip():
                    return True
                if c.get("is_percent") or c.get("has_percent"):
                    return True
                if (c.get("base_unit") or "").strip():
                    return True
                if (c.get("unit_family") or "").strip():
                    return True
                if isinstance(c.get("unit_tokens"), list) and c.get("unit_tokens"):
                    return True
                return False

            schema = {}
            try:
                if isinstance(previous_data, dict):
                    pr = previous_data.get("primary_response") if isinstance(previous_data.get("primary_response"), dict) else previous_data
                    schema = (pr.get("metric_schema_frozen") or {}) if isinstance(pr, dict) else {}
            except Exception:
                pass
                schema = {}

            for row in output.get("metric_changes") or []:
                try:
                    ckey = row.get("canonical_key") or row.get("canonical") or ""
                    md = schema.get(ckey) if isinstance(schema, dict) else None
                    if not _schema_unit_required(md or {}, ckey):
                        continue

                    cur_val = row.get("current_value_norm")
                    cur_unit = (row.get("cur_unit_cmp") or row.get("current_unit") or "").strip()
                    if cur_val is None:
                        continue
                    if not _is_yearlike(cur_val):
                        continue
                    if cur_unit:
                        continue

                    kws = []
                    if isinstance(md, dict):
                        kws = md.get("keywords") or md.get("keyword_hints") or []
                    kws = [str(k).lower() for k in kws if str(k).strip()]

                    def _hit_score(c):
                        ctx = (c.get("context") or c.get("window") or c.get("context_window") or "").lower()
                        score = 0
                        for k in kws[:25]:
                            if k and k in ctx:
                                score += 1
                        if _cand_unit_evidence(c):
                            score += 5
                        if _is_yearlike(c.get("value_norm")) and not _cand_unit_evidence(c):
                            score -= 5
                        return score

                    top = sorted(flat, key=_hit_score, reverse=True)[:10]
                    bad_traces[ckey or row.get("name") or "unknown_metric"] = {
                        "current_value_norm": cur_val,
                        "cur_unit_cmp": cur_unit,
                        "schema_unit_family": (md or {}).get("unit_family") if isinstance(md, dict) else "",
                        "origin": output["debug"]["fix35"].get("current_metrics_origin", "unknown"),
                        "top_candidates": [
                            {
                                "raw": t.get("raw"),
                                "value_norm": t.get("value_norm"),
                                "unit_tag": t.get("unit_tag"),
                                "unit_family": t.get("unit_family"),
                                "base_unit": t.get("base_unit"),
                                "has_unit_evidence": bool(_cand_unit_evidence(t)),
                                "anchor_hash": t.get("anchor_hash"),
                            }
                            for t in top
                        ],
                    }
                except Exception:
                    pass
                    continue

            if bad_traces:
                output["debug"]["fix35"]["bad_current_traces"] = bad_traces
                output["debug"]["fix35"]["bad_current_trace_count"] = len(bad_traces)
    except Exception:
        pass

    # =====================================================================
    # PATCH INJ_TRACE_V1_EMIT_EVOLUTION (ADDITIVE): always emit canonical trace
    # - Mirrors to output.results.debug.inj_trace_v1 for a fixed location across modes
    # - Does NOT affect fastpath decisioning
    # =====================================================================
    try:
        _wc_diag = {}
        if isinstance(web_context, dict):
            _wc_diag = web_context.get("diag_injected_urls") or {}
        _hash_inputs = _inj_diag_hash_inputs_from_bsc(baseline_sources_cache)

        # Determine path from existing fix35 origin stamp
        _path = ""
        try:
            origin = ""
            if isinstance(output.get("debug"), dict) and isinstance(output["debug"].get("fix35"), dict):
                origin = str(output["debug"]["fix35"].get("current_metrics_origin") or "")
            if "fastpath" in origin:
                _path = "fastpath"
            elif "rebuild" in origin:
                _path = "rebuild"
            else:
                _path = "unknown"
        except Exception:
            pass
            _path = "unknown"

        # Rebuild "selected" URLs: unique source_url from current metrics (if present)
        _selected = []
        try:
            cm = output.get("current_metrics")
            if isinstance(cm, dict):
                for v in cm.values():
                    if isinstance(v, dict):
                        u = (v.get("source_url") or "").strip()
                        if u:
                            _selected.append(u)
            _selected = sorted(set(_selected))
        except Exception:
            pass
            _selected = []

        # For evolution, rebuild_pool is effectively the hash input URL universe available via snapshots
        # =====================================================================
        # PATCH INJ_HASH_V1_EVO (ADDITIVE): compute per-URL hash exclusion reasons in evolution
        # - If injected URLs exist but are not in hash_inputs, we record the most likely reason:
        #     * excluded_by_flag_default_off  (when inclusion switch is OFF)
        #     * missing_from_hash_inputs      (when switch ON but still absent)
        # =====================================================================
        _evo_hash_reasons = {}
        try:
            _evo_persisted = []
            if isinstance(_wc_diag, dict):
                _evo_persisted = _inj_diag_norm_url_list(_wc_diag.get("persisted_norm") or _wc_diag.get("persisted") or [])
            _evo_inj = _inj_diag_norm_url_list(
                (_wc_diag.get("intake_norm") if isinstance(_wc_diag, dict) else []) or
                (_wc_diag.get("ui_norm") if isinstance(_wc_diag, dict) else []) or
                []
            )
            _evo_targets = _evo_persisted or _evo_inj
            _hashset = set(_inj_diag_norm_url_list(_hash_inputs or []))
            _incl = _inj_hash_policy_should_include(_evo_targets)
            for u in _evo_targets:
                if u in _hashset:
                    _evo_hash_reasons[u] = "present_in_hash_inputs"
                else:
                    _evo_hash_reasons[u] = ("excluded_by_policy_disable" if _inj_hash_policy_explicit_disable() else ("excluded_by_legacy_switch_default_off" if not _incl else "missing_from_hash_inputs"))
        except Exception:
            pass
            _evo_hash_reasons = {}
        # =====================================================================

        # =====================================================================
        # PATCH INJ_TRACE_V1_EVO_ADMISSION_ALIGN_V1 (ADDITIVE)
        # Goal:
        # - Evolution often bypasses fetch_web_context(), so "admitted" may be unset even
        #   when URLs are actually in the current scrape/hash universe.
        # - This patch makes inj_trace_v1 "admitted_norm" reflect the same practical
        #   universe used for scraping/hashing (without changing any control flow).
        #
        # Policy (diagnostics only):
        # - If diag.admitted is empty but hash_inputs are present, treat hash_inputs as
        #   admitted for trace purposes.
        # - Prefer any explicit FIX24 evo merge set if present (urls_after_merge_norm).
        # =====================================================================
        try:
            if isinstance(_wc_diag, dict):
                _ad = _inj_diag_norm_url_list(_wc_diag.get("admitted") or _wc_diag.get("extra_urls_admitted") or [])
                if not _ad:
                    _pref = []
                    try:
                        _pref = _inj_diag_norm_url_list(_wc_diag.get("urls_after_merge_norm") or [])
                    except Exception:
                        pass
                        _pref = []
                    if not _pref:
                        _pref = _inj_diag_norm_url_list(_hash_inputs or [])
                    if _pref:
                        _wc_diag["admitted"] = list(_pref)
                        _wc_diag.setdefault("admission_reason", "trace_fallback_to_hash_inputs_or_urls_after_merge")
        except Exception:
            pass
        # =====================================================================

        # =====================================================================
        # PATCH INJ_TRACE_V1_EVO_REBUILD_SELECTED_FALLBACK_V1 (ADDITIVE)
        # Goal:
        # - In fastpath/replay or when current_metrics lacks source_url fields,
        #   rebuild_selected_norm can be empty, creating misleading pool_minus_selected.
        #
        # Diagnostics-only fallback:
        # - If rebuild_selected is empty but rebuild_pool/hash_inputs exists, treat
        #   selected as the full pool for trace purposes.
        # =====================================================================
        try:
            if (not _selected) and _hash_inputs:
                _selected = list(_inj_diag_norm_url_list(_hash_inputs))
                if isinstance(_wc_diag, dict):
                    _wc_diag.setdefault("rebuild_selected_reason", "trace_fallback_to_hash_inputs_no_current_metric_sources")
        except Exception:
            pass
        # =====================================================================


        # =====================================================================
        # PATCH FIX41AFC12 (ADDITIVE): Admission-gate override for injected URLs + post-fetch trace
        #
        # Why:
        # - inj_trace_v1 shows injected URLs at intake but missing from admitted (unknown_rejected_pre_admission).
        # - We must "pin" injection at the admission boundary for evolution (delta-only), and emit a post-fetch trace
        #   because inj_trace_v1 may be emitted before the fetch/persist stage completes.
        #
        # What:
        # - If injected URLs are present (from web_context.extra_urls OR diag ui fields) we force-add them into
        #   _wc_diag["admitted"] so the trace reflects admission override deterministically (delta-only).
        # - Additionally, emit inj_trace_v2_postfetch using best-effort enrichment from scraped_meta / cur_bsc if available.
        #
        # Safety:
        # - Purely additive; never raises; does not modify fastpath rules or hashing.
        # =====================================================================
        try:
            _fx12_wc = web_context if isinstance(web_context, dict) else {}
            _fx12_diag = _wc_diag if isinstance(locals().get("_wc_diag"), dict) else (_fx12_wc.get("diag_injected_urls") if isinstance(_fx12_wc.get("diag_injected_urls"), dict) else {})
            _fx12_inj_raw = []
            try:
                _fx12_inj_raw = list(_fx12_wc.get("extra_urls") or [])
            except Exception:
                pass
                _fx12_inj_raw = []
            if not _fx12_inj_raw and isinstance(_fx12_diag, dict):
                try:
                    _fx12_inj_raw = list(_fx12_diag.get("ui_norm") or _fx12_diag.get("intake_norm") or [])
                except Exception:
                    pass
                    _fx12_inj_raw = []
            _fx12_inj = _inj_diag_norm_url_list(_fx12_inj_raw or [])
            if _fx12_inj and isinstance(_wc_diag, dict):
                _fx12_prev_ad = _inj_diag_norm_url_list(_wc_diag.get("admitted") or [])
                _fx12_forced = sorted(list(set(_fx12_inj) - set(_fx12_prev_ad)))
                if _fx12_forced:
                    _wc_diag["admitted"] = list(_inj_diag_stable_dedupe_order((_fx12_prev_ad or []) + _fx12_forced))
                    _wc_diag.setdefault("forced_admit_reasons", {})
                    if isinstance(_wc_diag.get("forced_admit_reasons"), dict):
                        for _u in _fx12_forced:
                            _wc_diag["forced_admit_reasons"][_u] = "forced_admit_injected_url_override"
                    try:
                        output.setdefault("debug", {})
                        if isinstance(output.get("debug"), dict):
                            output["debug"].setdefault("fix41afc12", {})
                            if isinstance(output["debug"].get("fix41afc12"), dict):
                                output["debug"]["fix41afc12"].update({
                                    "forced_admit_injected_count": int(len(_fx12_forced)),
                                    "forced_admit_injected_urls": list(_fx12_forced),
                                })
                    except Exception:
                        pass
        except Exception:
            pass
        # =====================================================================
        _trace = _inj_trace_v1_build(
            diag_injected_urls=_wc_diag if isinstance(_wc_diag, dict) else {},
            hash_inputs=_hash_inputs,
            stage="evolution",
            path=_path,
            rebuild_pool=_hash_inputs,
            rebuild_selected=_selected,
            hash_exclusion_reasons=_evo_hash_reasons,
        )

        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            output["debug"]["inj_trace_v1"] = _trace

        # Fixed location mirror: results.debug.inj_trace_v1
        output.setdefault("results", {})
        if isinstance(output.get("results"), dict):
            output["results"].setdefault("debug", {})
            if isinstance(output["results"].get("debug"), dict):
                output["results"]["debug"]["inj_trace_v1"] = _trace

                # =====================================================================
                # PATCH FIX41AFC12_POSTFETCH (ADDITIVE): inj_trace_v2_postfetch
                #
                # Emit a second trace after best-effort enrichment from scraped_meta / baseline cache so that
                # attempted/persisted deltas reflect the true post-fetch state (inj_trace_v1 may be earlier).
                # =====================================================================
                try:
                    _fx12_diag2 = dict(_wc_diag) if isinstance(_wc_diag, dict) else {}
                    try:
                        _sm = locals().get("scraped_meta")
                        if isinstance(_sm, dict):
                            _fx12_diag2 = _inj_trace_v1_enrich_diag_from_scraped_meta(_fx12_diag2, _sm, (_inj_extra_urls or []))
                    except Exception:
                        pass
                    try:
                        _cb = locals().get("cur_bsc") or locals().get("baseline_sources_cache") or locals().get("baseline_sources_cache_current")
                        if isinstance(_cb, list):
                            _fx12_diag2 = _inj_trace_v1_enrich_diag_from_bsc(_fx12_diag2, _cb)
                    except Exception:
                        pass
                    _trace2 = _inj_trace_v1_build(
                        diag_injected_urls=_fx12_diag2 if isinstance(_fx12_diag2, dict) else {},
                        hash_inputs=_hash_inputs,
                        stage="evolution",
                        path=str(_path or "evolution") + "_postfetch",
                        rebuild_pool=_hash_inputs,
                        rebuild_selected=_selected,
                        hash_exclusion_reasons=_evo_hash_reasons,
                    )
                    output["debug"]["inj_trace_v2_postfetch"] = _trace2
                    if isinstance(output.get("results"), dict) and isinstance(output["results"].get("debug"), dict):
                        output["results"]["debug"]["inj_trace_v2_postfetch"] = _trace2
                except Exception:
                    pass


    # =====================================================================
    except Exception:
        pass

    # PATCH FIX41AFC19_V25 (ADDITIVE): Dashboard "Current" source audit + row sample
    #
    # Why:
    # - Conclusively identify which structure the Evolution dashboard reads for
    #   the "Current" column, and whether upstream patches are modifying that
    #   exact structure.
    #
    # What:
    # - Emit a compact debug payload that:
    #     * states the dashboard read-path ("results.metric_changes[].current_value")
    #     * samples the first N metric_changes rows (canonical_key, current_value, unit hints, diag keys)
    # - Purely additive: no selection, hashing, or fastpath behavior changes.
    # =====================================================================
    try:
        if isinstance(output, dict):
            _dbg = output.get("debug") if isinstance(output.get("debug"), dict) else {}
            if not isinstance(_dbg, dict):
                _dbg = {}
            rows = output.get("metric_changes") or []
            # =====================================================================
            # PATCH FIX2D24 (ADDITIVE): Last-mile yearlike guard for dashboard "Current"
            #
            # Why:
            # - Even after schema-only rebuild hardening, dashboard "Current" may still
            #   be hydrated from metric_changes rows that pull values from multiple
            #   paths (canonical, observed, legacy). We must prevent unitless bare
            #   years (e.g., 2024, 2030, including 2030.0) from appearing as metric
            #   values for non-year metrics.
            #
            # What:
            # - Post-process metric_changes in-place before the dashboard consumes it.
            # - If current value is yearlike AND unit is empty AND metric is not a
            #   year-as-value metric, blank it (N/A) and attach diagnostics.
            # - Keep a compact trace in output.debug.fix2d24_yearlike_current_trace_v1.
            # =====================================================================
            def _fix2d24_is_yearlike(v, raw=None):
                try:
                    if v is None:
                        return False
                    fv = float(v)
                    if fv < 1900.0 or fv > 2100.0:
                        return False
                    if abs(fv - round(fv)) > 1e-9:
                        return False
                    rs = str(raw if raw is not None else v).strip()
                    rs = rs.replace(",", "")
                    if re.match(r"^\d{4}(?:\.0+)?$", rs):
                        return True
                    # allow e.g. '2030.0' in string form
                    if re.match(r"^\d{4}\.0+$", rs):
                        return True
                    return True
                except Exception:
                    return False

            def _fix2d24_metric_expects_year(ckey, name=None):
                try:
                    ck = str(ckey or "").lower()
                    nm = str(name or "").lower()
                    if ck.endswith("__year"):
                        return True
                    if "year" in ck and ("__" in ck):
                        # conservative: treat explicit year unit tags as year metrics
                        if "__year" in ck:
                            return True
                    if nm.strip() == "year":
                        return True
                    return False
                except Exception:
                    return False

            _fix2d24_filtered = 0
            _fix2d24_samples = []
            if isinstance(rows, list):
                for _r in rows:
                    if not isinstance(_r, dict):
                        continue
                    _ckey = _r.get("canonical_key")
                    _nm = _r.get("metric") or _r.get("name")
                    _unit = _r.get("cur_unit_cmp")
                    if _unit is None:
                        _unit = _r.get("current_unit")
                    if _unit is None:
                        _unit = _r.get("unit")
                    _unit = str(_unit or "").strip()
                    _cvn = _r.get("current_value_norm")
                    if _cvn is None:
                        _cvn = _r.get("cur_value_norm")
                    _raw = _r.get("current_value")
                    if _raw is None:
                        _raw = _r.get("cur_raw")

                    if _fix2d24_is_yearlike(_cvn, _raw) and (not _unit) and (not _fix2d24_metric_expects_year(_ckey, _nm)):
                        _fix2d24_filtered += 1
                        if len(_fix2d24_samples) < 25:
                            _fix2d24_samples.append({
                                "canonical_key": _ckey,
                                "name": _nm,
                                "blocked_value_norm": _cvn,
                                "blocked_raw": _raw,
                                "source_url": _r.get("source_url") or _r.get("cur_source_url"),
                            })
                        _r.setdefault("diag", {})
                        if isinstance(_r.get("diag"), dict):
                            _r["diag"]["fix2d24_yearlike_current_blocked"] = {
                                "blocked": True,
                                "value_norm": _cvn,
                                "raw": _raw,
                                "unit": _unit,
                            }
                        # blank current for dashboard consumption
                        _r["current_value"] = "N/A"
                        _r["current_value_norm"] = None
                        _r["cur_value_norm"] = None
                        # =====================================================================
                        # PATCH FIX2D2L (AUTHORITATIVE): yearlike-current blocked => inference fallback + commit
                        # Why:
                        # - FIX2D24 correctly blocks unitless yearlike current values (e.g. 2024/2030)
                        #   but previously left Current as N/A. This patch immediately falls back to the
                        #   guarded inference pool and commits a binding current value into metric_changes.
                        # What:
                        # - Build a shared extracted-number pool from baseline_sources_cache(_current)
                        # - Score candidates with unit-family + keyword/context hints
                        # - Commit into current_value/current_value_norm/current_source/current_method
                        # - Attach explicit trace yearlike_current_blocked_then_inferred_v1
                        # =====================================================================
                        try:
                            _r_diag = _r.get('diag') if isinstance(_r.get('diag'), dict) else {}
                            _blocked_vn = _cvn
                            _blocked_raw = _raw
                            # lazily build pool once
                            if '_fix2d2l_pool_v1' not in locals():
                                _fix2d2l_pool_v1 = []
                                def _fix2d2l_add_from_bsc(_bsc_list):
                                    try:
                                        if not isinstance(_bsc_list, list):
                                            return
                                        for _src in _bsc_list:
                                            if not isinstance(_src, dict):
                                                continue
                                            _surl = _src.get('source_url') or _src.get('url')
                                            _nums = _src.get('extracted_numbers')
                                            if not isinstance(_nums, list):
                                                continue
                                            for _n in _nums:
                                                if not isinstance(_n, dict):
                                                    continue
                                                _vn = _n.get('value_norm')
                                                _rw = _n.get('raw')
                                                _ut = str(_n.get('unit_tag') or '').strip()
                                                _uf = str(_n.get('unit_family') or '').strip()
                                                _ctx = _n.get('context_snippet') or _n.get('context') or ''
                                                # derive minimal unit_family if missing
                                                if not _uf:
                                                    if _ut == '%':
                                                        _uf = 'percent'
                                                    elif _ut in {'USD','US$','$','EUR','€','GBP','£','SGD','S$','JPY','¥','CNY','RMB','CN¥'}:
                                                        _uf = 'currency'
                                                    elif _ut in {'M','B','T','K'}:
                                                        _uf = 'magnitude'
                                                _fix2d2l_pool_v1.append({
                                                    'value_norm': _vn,
                                                    'raw': _rw,
                                                    'unit_tag': _ut,
                                                    'unit_family': _uf,
                                                    'source_url': (_n.get('source_url') or _surl),
                                                    'context_snippet': _ctx,
                                                })
                                    except Exception:
                                        return
                                # collect from common locations
                                _fix2d2l_add_from_bsc(output.get('baseline_sources_cache_current'))
                                _fix2d2l_add_from_bsc(output.get('baseline_sources_cache'))
                                if isinstance(output.get('results'), dict):
                                    _fix2d2l_add_from_bsc(output['results'].get('baseline_sources_cache_current'))
                                    _fix2d2l_add_from_bsc(output['results'].get('baseline_sources_cache'))
                            _pool = locals().get('_fix2d2l_pool_v1') or []
                            # determine desired unit_family from row hints
                            _desired_uf = ''
                            try:
                                _hint_u = str(_r.get('prev_unit_cmp') or _r.get('unit') or _r.get('cur_unit_cmp') or '').strip().lower()
                                _hint_name = str(_nm or '').lower()
                                if '%' in _hint_u or 'percent' in _hint_u or 'share' in _hint_name:
                                    _desired_uf = 'percent'
                                elif any(x in _hint_u for x in ['usd','us$','$','eur','€','gbp','£','sgd','s$','jpy','¥','cny','rmb']):
                                    _desired_uf = 'currency'
                                elif any(x in _hint_name for x in ['sales','units','deliver','ship','vehicle']):
                                    _desired_uf = 'magnitude'
                            except Exception:
                                pass
                                _desired_uf = ''
                            # keyword set for context scoring
                            _kw = set()
                            try:
                                for tok in re.split(r"[^a-z0-9]+", str(_nm or _ckey or '').lower()):
                                    if len(tok) >= 4 and tok not in {'global','total','market','share','sales','units','value','year'}:
                                        _kw.add(tok)
                            except Exception:
                                pass
                                _kw = set()
                            # injected-first preference (FIX2D2M)
                            _inj_urls = set()
                            try:
                                # Prefer explicit injected sources when present in caches
                                def _fix2d2m_scan_injected(bsc):
                                    if not isinstance(bsc, list):
                                        return
                                    for it in bsc:
                                        try:
                                            if not isinstance(it, dict):
                                                continue
                                            if it.get('injected') is True:
                                                su = str(it.get('source_url') or '').strip()
                                                if su:
                                                    _inj_urls.add(su)
                                        except Exception:
                                            pass
                                            continue
                                _fix2d2m_scan_injected(output.get('baseline_sources_cache_current'))
                                _fix2d2m_scan_injected(output.get('baseline_sources_cache'))
                                if isinstance(output.get('results'), dict):
                                    _fix2d2m_scan_injected(output['results'].get('baseline_sources_cache_current'))
                                    _fix2d2m_scan_injected(output['results'].get('baseline_sources_cache'))
                            except Exception:
                                pass
                                _inj_urls = set()

                            def _fix2d2m_score_pool(_pool_in):
                                _sc = []
                                for _cand in _pool_in:
                                    try:
                                        if not isinstance(_cand, dict):
                                            continue
                                        _vn = _cand.get('value_norm')
                                        _rw = _cand.get('raw')
                                        if _fix2d24_is_yearlike(_vn, _rw):
                                            continue
                                        _uf = str(_cand.get('unit_family') or '').strip()
                                        _ut = str(_cand.get('unit_tag') or '').strip()
                                        _ctx = str(_cand.get('context_snippet') or '').lower()
                                        _score = 0.0
                                        if _desired_uf and _uf == _desired_uf:
                                            _score += 10.0
                                        if _desired_uf == 'percent' and ('%' in _ctx or _ut == '%'):
                                            _score += 3.0
                                        if _desired_uf == 'magnitude' and ('million' in _ctx or 'units' in _ctx):
                                            _score += 2.0
                                        if _desired_uf == 'currency' and any(x in _ctx for x in ['$', 'us$', 'usd', 'eur', '€', 'gbp', '£']):
                                            _score += 2.0
                                        _row_surl = str(_r.get('source_url') or _r.get('cur_source_url') or '').strip()
                                        if _row_surl and str(_cand.get('source_url') or '').strip() == _row_surl:
                                            _score += 2.0
                                        if _kw:
                                            _hits = sum(1 for k in _kw if k in _ctx)
                                            _score += min(6.0, 1.5 * _hits)
                                        _sc.append((_score, _cand))
                                    except Exception:
                                        pass
                                        continue
                                _sc.sort(key=lambda t: (t[0] if t and len(t)>0 else 0), reverse=True)
                                return _sc

                            # two-pass selection: injected-only then global
                            _pool_injected = []
                            if _inj_urls:
                                _pool_injected = [c for c in _pool if isinstance(c, dict) and str(c.get('source_url') or '').strip() in _inj_urls]
                            _scored_inj = _fix2d2m_score_pool(_pool_injected) if _pool_injected else []
                            _sel = _scored_inj[0][1] if _scored_inj and _scored_inj[0][0] >= 3.0 else None
                            _used_injected_pass = bool(_sel is not None)
                            _scored = _scored_inj
                            if _sel is None:
                                _scored = _fix2d2m_score_pool(_pool)
                                _sel = _scored[0][1] if _scored and _scored[0][0] >= 3.0 else None
                            _committed = False
                            _reason = 'no_eligible_candidates' if not _scored else ('score_below_threshold' if _sel is None else 'selected')
                            if isinstance(_sel, dict):
                                # commit into metric_changes row (UI-read fields)
                                _r['current_value_norm'] = _sel.get('value_norm')
                                _r['cur_value_norm'] = _sel.get('value_norm')
                                _r['current_value'] = (_sel.get('raw') if _sel.get('raw') not in (None, '') else str(_sel.get('value_norm')))
                                _r['current_source'] = _sel.get('source_url')
                                _r['current_method'] = 'yearlike_blocked_then_inferred'
                                # comparable only if prev exists
                                _pv = _r.get('previous_value_norm')
                                if _pv is None:
                                    _pv = _r.get('prev_value_norm')
                                _r['baseline_is_comparable'] = bool(_pv is not None and _r.get('current_value_norm') is not None)
                                _committed = True
                            # trace
                            _r.setdefault('diag', {})
                            if isinstance(_r.get('diag'), dict):
                                _r['diag']['yearlike_current_blocked_then_inferred_v1'] = {
                                    'blocked_value_norm': _blocked_vn,
                                    'blocked_raw': _blocked_raw,
                                    'desired_unit_family': _desired_uf or None,
                                    'pool_size': int(len(_pool)) if isinstance(_pool, list) else None,
                                    'eligible_scored': int(len(_scored)),
                                    'selected': bool(_sel is not None),
                                    'selected_value_norm': (_sel.get('value_norm') if isinstance(_sel, dict) else None),
                                    'selected_raw': (_sel.get('raw') if isinstance(_sel, dict) else None),
                                    'selected_unit_family': (_sel.get('unit_family') if isinstance(_sel, dict) else None),
                                    'selected_unit_tag': (_sel.get('unit_tag') if isinstance(_sel, dict) else None),
                                    'selected_source_url': (_sel.get('source_url') if isinstance(_sel, dict) else None),
                                    'committed': bool(_committed),
                                    'reason': _reason,
                                    'top3': [
                                        {
                                            'score': float(sc),
                                            'value_norm': c.get('value_norm') if isinstance(c, dict) else None,
                                            'raw': c.get('raw') if isinstance(c, dict) else None,
                                            'unit_family': c.get('unit_family') if isinstance(c, dict) else None,
                                            'unit_tag': c.get('unit_tag') if isinstance(c, dict) else None,
                                            'source_url': c.get('source_url') if isinstance(c, dict) else None,
                                        }
                                        for sc, c in (_scored[:3] if isinstance(_scored, list) else [])
                                    ],
                                }
                            # injected preference trace
                            try:
                                _r['diag'].setdefault('injected_source_preference_trace_v1', {})
                                _r['diag']['injected_source_preference_trace_v1'] = {
                                    'injected_present': bool(_inj_urls),
                                    'injected_urls': sorted(list(_inj_urls))[:3] if _inj_urls else [],
                                    'pass1_injected_pool_size': int(len(_pool_injected)) if isinstance(_pool_injected, list) else 0,
                                    'pass1_selected': bool(_used_injected_pass),
                                    'fallback_used': bool((not _used_injected_pass) and (_sel is not None)),
                                    'selected_source_url': (_sel.get('source_url') if isinstance(_sel, dict) else None),
                                }
                            except Exception:
                                pass

                            # aggregate
                            try:
                                _dbg.setdefault('fix2d2l_yearlike_block_fallback_trace_v1', {})
                                _agg = _dbg.get('fix2d2l_yearlike_block_fallback_trace_v1')
                                if not isinstance(_agg, dict):
                                    _agg = {}
                                _agg['fallback_attempts'] = int(_agg.get('fallback_attempts', 0)) + 1
                                if _committed:
                                    _agg['fallback_commits'] = int(_agg.get('fallback_commits', 0)) + 1
                                _agg.setdefault('samples', [])
                                if isinstance(_agg.get('samples'), list) and len(_agg['samples']) < 10:
                                    _agg['samples'].append({
                                        'canonical_key': _ckey,
                                        'blocked_value_norm': _blocked_vn,
                                        'committed': bool(_committed),
                                        'selected_value_norm': (_sel.get('value_norm') if isinstance(_sel, dict) else None),
                                        'selected_source_url': (_sel.get('source_url') if isinstance(_sel, dict) else None),
                                        'reason': _reason,
                                    })
                                _dbg['fix2d2l_yearlike_block_fallback_trace_v1'] = _agg
                                output['debug'] = _dbg
                            except Exception:
                                pass
                        except Exception:
                            pass

            # attach trace
            try:
                _dbg = output.get("debug") if isinstance(output.get("debug"), dict) else {}
                if not isinstance(_dbg, dict):
                    _dbg = {}
                _dbg.setdefault("fix2d24_yearlike_current_trace_v1", {})
                _dbg["fix2d24_yearlike_current_trace_v1"] = {
                    "filtered_count": int(_fix2d24_filtered),
                    "samples": _fix2d24_samples,
                    "note": "Blocks unitless yearlike current values at metric_changes hydration",
                }
                output["debug"] = _dbg
            except Exception:
                pass

            _sample = []
            if isinstance(rows, list):
                for _r in rows[:25]:
                    if not isinstance(_r, dict):
                        continue
                    _diag = _r.get("diag") if isinstance(_r.get("diag"), dict) else {}
                    _sample.append({
                        "canonical_key": _r.get("canonical_key"),
                        "name": _r.get("metric") or _r.get("name"),
                        "current_value": _r.get("current_value"),
                        "current_value_norm": (_r.get("current_value_norm") if _r.get("current_value_norm") is not None else _r.get("cur_value_norm")),
                        "cur_unit_cmp": (_r.get("cur_unit_cmp") if _r.get("cur_unit_cmp") is not None else _r.get("current_unit")),
                        "anchor_used": _r.get("anchor_used"),
                        "unit_mismatch": _r.get("unit_mismatch"),
                        "diag_keys": (list(_diag.keys()) if isinstance(_diag, dict) else []),
                    })
            _dbg["dashboard_current_source_v25"] = {
                "dashboard_reads": "results.metric_changes[].current_value",
                "rows_sample_n": len(_sample),
                "rows_sample": _sample,
            }
            # REFACTOR12: canonical_for_render debug block must be present (avoid stale "missing" diagnostics)
            try:
                _cfr = _dbg.get("canonical_for_render_v1")
                if (not isinstance(_cfr, dict)) or (not _cfr):
                    _dbg["canonical_for_render_v1"] = {
                        "present": True,
                        "code_version": _yureeka_get_code_version(),
            "authority_manifest_v1": _yureeka_authority_manifest_v1(),
                        "note": "REFACTOR12 standardized debug block (was intermittently missing)",
                    }
            except Exception:
                pass
            _dbg["canonical_for_render_present_v25"] = bool(_dbg.get("canonical_for_render_v1"))

            # =====================================================================
            # PATCH FIX2AC_CANONICAL_FOR_RENDER_DIAGNOSE_V1 (ADDITIVE)
            # Why:
            # - We sometimes observe canonical_for_render_present_v25 == False and/or
            #   missing bound canonical entities in the dashboard, even when injection
            #   extracted numbers exist.
            #
            # What:
            # - Emit a single compact debug object that explains, deterministically,
            #   why canonical_for_render did not run / did not apply / did not produce
            #   candidates / did not change any rows.
            # - Purely additive; does not change selection, hashing, or rendering.
            # =====================================================================
            try:
                _cfr_dbg = _dbg.get("canonical_for_render_v1") if isinstance(_dbg.get("canonical_for_render_v1"), dict) else None

                # Row-level diag aggregation (if present)
                _rows = None
                try:
                    _rows = output.get("metric_changes") if isinstance(output, dict) else None
                except Exception:
                    pass
                    _rows = None

                _row_diag_present = 0
                _row_diag_applied_true = 0
                _row_diag_applied_false = 0
                _row_diag_reasons = {}
                _row_diag_reason_samples = []

                if isinstance(_rows, list):
                    for _r in _rows[:250]:
                        if not isinstance(_r, dict):
                            continue
                        _d = _r.get("diag") if isinstance(_r.get("diag"), dict) else {}
                        _c = _d.get("canonical_for_render_v1") if isinstance(_d.get("canonical_for_render_v1"), dict) else None
                        if not isinstance(_c, dict):
                            continue
                        _row_diag_present += 1
                        _ap = _c.get("applied")
                        if _ap is True:
                            _row_diag_applied_true += 1
                        elif _ap is False:
                            _row_diag_applied_false += 1
                        _rsn = str(_c.get("reason") or "")
                        if _rsn:
                            _row_diag_reasons[_rsn] = int(_row_diag_reasons.get(_rsn, 0)) + 1
                            if len(_row_diag_reason_samples) < 12:
                                _row_diag_reason_samples.append({
                                    "canonical_key": _r.get("canonical_key"),
                                    "reason": _rsn,
                                    "fn": str(_c.get("fn") or ""),
                                })

                # Determine the most precise reason we can provide
                _reason = ""
                _reason_detail = {}

                if not isinstance(_cfr_dbg, dict):
                    _reason = "missing_output_debug.canonical_for_render_v1"
                    _reason_detail = {
                        "has_output_debug": bool(isinstance(_dbg, dict)),
                        "row_diag_present": int(_row_diag_present),
                        "row_diag_applied_true": int(_row_diag_applied_true),
                        "row_diag_applied_false": int(_row_diag_applied_false),
                    }
                else:
                    _applied = bool(_cfr_dbg.get("applied"))
                    _rb = int(_cfr_dbg.get("rebuilt_count") or 0)
                    _rsn = str(_cfr_dbg.get("reason") or "")
                    if not _applied:
                        _reason = "canonical_for_render_not_applied"
                        _reason_detail = {
                            "canonical_for_render_reason": _rsn,
                            "rebuilt_count": _rb,
                        }
                    elif _applied and _rb <= 0:
                        _reason = "canonical_for_render_applied_but_empty"
                        _reason_detail = {
                            "canonical_for_render_reason": _rsn,
                            "rebuilt_count": _rb,
                        }
                    elif _applied and _rb > 0 and _row_diag_applied_true == 0:
                        _reason = "canonical_for_render_rebuilt_but_no_rows_hydrated"
                        _reason_detail = {
                            "canonical_for_render_reason": _rsn,
                            "rebuilt_count": _rb,
                        }
                    else:
                        _reason = "canonical_for_render_present"
                        _reason_detail = {
                            "canonical_for_render_reason": _rsn,
                            "rebuilt_count": _rb,
                        }

                _dbg["canonical_for_render_diagnosis_fix2ac_v1"] = {
                    "reason": _reason,
                    "reason_detail": _reason_detail,
                    "canonical_for_render_present_v25": bool(_dbg.get("canonical_for_render_v25") or _dbg.get("canonical_for_render_present_v25")),
                    "row_diag_present": int(_row_diag_present),
                    "row_diag_applied_true": int(_row_diag_applied_true),
                    "row_diag_applied_false": int(_row_diag_applied_false),
                    "row_diag_reason_counts": _row_diag_reasons,
                    "row_diag_reason_samples": _row_diag_reason_samples,
                    "has_canonical_for_render_v1": bool(isinstance(_cfr_dbg, dict)),
                    "canonical_for_render_v1_summary": (
                        {
                            "applied": bool(_cfr_dbg.get("applied")),
                            "reason": str(_cfr_dbg.get("reason") or ""),
                            "fn": str(_cfr_dbg.get("fn") or ""),
                            "rebuilt_count": int(_cfr_dbg.get("rebuilt_count") or 0),
                            "keys_sample": list(_cfr_dbg.get("keys_sample") or [])[:10],
                        }
                        if isinstance(_cfr_dbg, dict) else {}
                    ),
                }
            except Exception:
                pass
            # =====================================================================
            # END PATCH FIX2AC_CANONICAL_FOR_RENDER_DIAGNOSE_V1
            # =====================================================================

            # =====================================================================
            # PATCH V33_DIFF_PANEL_CANONICAL_PATHS_EMIT (ADDITIVE)
            # Surface which response-shape path diff used to hydrate "current".
            # =====================================================================
            try:
                _paths = None
                try:
                    _paths = cur_response.get("_diff_panel_canonical_paths_v33") if isinstance(cur_response, dict) else None
                except Exception:
                    pass
                    _paths = None
                if isinstance(_paths, dict) and _paths:
                    _dbg["diff_panel_canonical_paths_v33"] = _paths
            except Exception:
                pass

            output["debug"] = _dbg
    except Exception:
        pass
    # =====================================================================
    # END PATCH FIX41AFC19_V25
    # =====================================================================


    # =====================================================================
    # PATCH V26_RESTORE_LOCKED_CURRENT_FIELDS (ADDITIVE)
    # If any later code overwrote current_* fields, restore the locked canonical-for-render
    # fields right before returning the evolution output.
    # =====================================================================
    try:
        _lock_dbg = {"rows_total": 0, "rows_locked": 0, "rows_restored": 0, "rows_missing_lock": 0, "restored_keys_sample": []}
        for _r in (output.get("metric_changes") or []):
            if not isinstance(_r, dict):
                continue
            _lock_dbg["rows_total"] += 1
            if not _r.get("_lock_current_v26"):
                continue
            _lock_dbg["rows_locked"] += 1
            _d = _r.get("diag") if isinstance(_r.get("diag"), dict) else {}
            _lc = None
            try:
                if isinstance(_d, dict):
                    _cfr = _d.get("canonical_for_render_v1") if isinstance(_d.get("canonical_for_render_v1"), dict) else {}
                    _lc = _cfr.get("locked_current_v26") if isinstance(_cfr, dict) else None
            except Exception:
                pass
                _lc = None
            if not isinstance(_lc, dict):
                _lock_dbg["rows_missing_lock"] += 1
                continue
            # If current fields differ from locked, restore
            _changed = False
            for _k in ("current_value", "current_value_norm", "cur_value_norm", "cur_unit_cmp", "current_unit",
                       "current_value_range", "current_value_range_display"):
                if _k in _lc:
                    if _r.get(_k) != _lc.get(_k):
                        _r[_k] = _lc.get(_k)
                        _changed = True
            if _changed:
                _lock_dbg["rows_restored"] += 1
                ck = _r.get("canonical_key") or _r.get("canonical") or ""
                if ck and len(_lock_dbg["restored_keys_sample"]) < 20:
                    _lock_dbg["restored_keys_sample"].append(str(ck))
        output.setdefault("debug", {})
        if isinstance(output.get("debug"), dict):
            output["debug"]["lock_current_v26"] = _lock_dbg
    except Exception:
        pass
    # =====================================================================
    # END PATCH V26_RESTORE_LOCKED_CURRENT_FIELDS
    # =====================================================================


    # PATCH FIX2D20 (ADD): trace year-like commits in primary_metrics_canonical



    # =====================================================================
    # REFACTOR56: enforce removal of legacy metric_changes output (safety rail)
    # =====================================================================
    try:
        output.pop("metric_changes_legacy", None)
    except Exception:
        pass

    _fix2d20_trace_year_like_commits(output, stage=str((output or {}).get('results',{}).get('debug',{}).get('stage') or 'evolution'), callsite='compute_source_anchored_diff_return')


    return output

# =================== END PATCH RMS_CORE1 (ADDITIVE) ===================





def extract_context_keywords(metric_name: str) -> List[str]:
    """
    General-purpose keyword extraction for matching metric names to page contexts.

    Goals:
    - Work for ANY topic (not tourism-specific)
    - Keep deterministic behavior
    - Extract years/quarters, key financial/stat terms, and meaningful tokens
    """
    if not metric_name:
        return []

    name = str(metric_name)
    n = name.lower()

    keywords: List[str] = []

    # Years (e.g., 2019, 2024)
    years = re.findall(r"\b(19\d{2}|20\d{2})\b", name)
    keywords.extend(years)

    # Quarters / time buckets
    q = re.findall(r"\bq[1-4]\b", n)
    keywords.extend([x.upper() for x in q])

    # Common metric concepts (broad, cross-industry)
    concept_phrases = [
        "market size", "revenue", "sales", "turnover", "profit", "operating profit",
        "ebit", "ebitda", "net income", "gross margin", "margin",
        "growth", "yoy", "cagr", "share", "penetration",
        "forecast", "projected", "projection", "estimate", "expected",
        "actual", "baseline", "target",
        "volume", "units", "shipments", "users", "subscribers", "visitors",
        "price", "asp", "arpu", "aov",
        "inflation", "gdp", "unemployment", "interest rate"
    ]
    for p in concept_phrases:
        if p in n:
            keywords.append(p)

    # Units / scales that help matching
    unit_hints = ["trillion", "billion", "million", "thousand", "%", "percent"]
    for u in unit_hints:
        if u in n:
            keywords.append(u)

    # Tokenize remaining meaningful words
    tokens = re.findall(r"[a-z0-9]+", n)
    stop = {
        "the","and","or","of","in","to","for","by","from","with","on","at","as",
        "total","overall","average","avg","number","rate","value","amount",
        "annual","year","years","monthly","month","daily","day","quarter","quarters"
    }
    for t in tokens:
        if t in stop:
            continue
        if len(t) <= 2:
            continue
        keywords.append(t)

    # De-dup, keep stable ordering
    seen = set()
    out = []
    for k in keywords:
        if k and k not in seen:
            seen.add(k)
            out.append(k)

    return out[:30]

def extract_numbers_with_context(text, source_url: str = "", max_results: int = 350):
    """
    Extract numeric candidates with context windows (analysis-aligned, hardened).

    Fixes / tightening:
    - ALWAYS returns a list (never None)  ✅ critical for snapshots & evolution
    - Strips HTML tags/scripts/styles if HTML-like
    - Nav/chrome/junk rejection (analytics, cookie banners, menus, footers, etc.)
    - Suppress year-only candidates (e.g., "2024") unless clearly a metric
    - Suppress ID-like long integers, phone-like patterns, DOI/ISBN-like contexts
    - Captures currency + scale + percent + common magnitude suffixes
    - Adds anchor_hash for stable matching
    """
    import re
    import hashlib

    if not text or not str(text).strip():
        return []

    raw = str(text)

    # ---------- helpers ----------
    def _sha1(s: str) -> str:
        return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()

    def _normalize_unit(u: str) -> str:
        u = (u or "").strip()
        if not u:
            return ""
        ul = u.lower().replace(" ", "")

        # Energy units (must come before magnitude)
        if "twh" in ul:
            return "TWh"
        if "gwh" in ul:
            return "GWh"
        if "mwh" in ul:
            return "MWh"
        if "kwh" in ul:
            return "kWh"
        if ul == "wh":
            return "Wh"

        # Magnitudes (case-insensitive; fix: accept single-letter suffixes)
        if ul in ("bn", "billion", "b"):
            return "B"
        if ul in ("mn", "mio", "million", "m"):
            return "M"
        if ul in ("k", "thousand", "000"):
            return "K"
        if ul in ("trillion", "tn", "t"):
            return "T"

        if ul in ("pct", "percent", "%"):
            return "%"

        return u

    def _looks_html(s: str) -> bool:
        sl = s.lower()
        return ("<html" in sl) or ("<div" in sl) or ("<p" in sl) or ("<script" in sl) or ("</" in sl)

    def _html_to_text(s: str) -> str:
        # Prefer BeautifulSoup if available
        try:
            from bs4 import BeautifulSoup  # type: ignore
            soup = BeautifulSoup(s, "html.parser")
            for tag in soup(["script", "style", "noscript", "svg", "canvas", "iframe", "header", "footer", "nav", "form"]):
                try:
                    tag.decompose()
                except Exception:
                    pass
            txt = soup.get_text(separator=" ", strip=True)
            txt = re.sub(r"\s+", " ", txt).strip()
            return txt
        except Exception:
            pass
            # fallback: cheap strip
            s2 = re.sub(r"(?is)<(script|style|noscript).*?>.*?</\1>", " ", s)
            s2 = re.sub(r"(?is)<[^>]+>", " ", s2)
            s2 = re.sub(r"\s+", " ", s2).strip()
            return s2

    def _is_phone_like(ctx: str, rawnum: str) -> bool:
        # strict phone pattern or phone keywords nearby
        if re.search(r"\b\d{3}-\d{3}-\d{4}\b", rawnum):
            return True
        c = (ctx or "").lower()
        if any(k in c for k in ["call", "phone", "tel:", "telephone", "contact us", "whatsapp"]):
            if re.search(r"\b\d{7,}\b", rawnum):
                return True
        return False

    def _is_id_like(val_str: str, ctx: str) -> bool:
        # very long digit strings typically IDs, unless explicitly monetary with symbols
        digits = re.sub(r"\D", "", val_str or "")
        if len(digits) >= 13:
            c = (ctx or "").lower()
            if any(k in c for k in ["isbn", "doi", "issn", "arxiv", "repec", "id:", "order", "invoice", "reference"]):
                return True
            # generic ID-like (too many digits)
            return True
        return False

    def _chrome_junk(ctx: str) -> bool:
        c = (ctx or "").lower()
        # common site chrome / analytics / cookie / nav junk
        bad = [
            "googleanalyticsobject", "gtag(", "googletagmanager", "analytics", "doubleclick",
            "cookie", "consent", "privacy", "terms", "copyright", "all rights reserved",
            "subscribe", "newsletter", "sign in", "login", "menu", "search", "breadcrumb",
            "share this", "follow us", "social media", "footer", "header", "nav", "sitemap"
        ]
        if any(b in c for b in bad):
            return True
        # css/js-like
        if any(b in c for b in ["function(", "var ", "const ", "let ", "webpack", "sourcemappingurl", ".css", "{", "};"]):
            return True
        # low alpha ratio
        if len(c) > 80:
            letters = sum(ch.isalpha() for ch in c)
            if letters / max(1, len(c)) < 0.18:
                return True
        return False

    def _year_only_suppression(num: float, unit: str, rawnum: str, ctx: str) -> bool:
        # suppress standalone 4-digit years like 2024 with no unit/currency
        if unit:
            return False
        s = (rawnum or "").strip()
        if re.fullmatch(r"\d{4}", s):
            year = int(s)
            if 1900 <= year <= 2099:
                c = (ctx or "").lower()
                allow_kw = ["cagr", "growth", "inflation", "gdp", "revenue", "market", "sales", "shipments", "capacity"]
                if not any(k in c for k in allow_kw):
                    return True
        return False

    # -------------------------------------------------------------------------
    # ADDITIVE (Patch A1): fix common "split year" artifact (e.g., "202 5" -> "2025")
    # Do this AFTER HTML->text and BEFORE regex extraction.
    # -------------------------------------------------------------------------

    # ---------- normalize to visible text ----------
    if _looks_html(raw):
        raw = _html_to_text(raw)

    # cap huge pages
    raw = raw[:250_000]

    # ---- ADDITIVE: fix common "split year" artifact (e.g., "202 5" -> "2025") ----
    raw = re.sub(r"\b((?:19|20)\d)\s+(\d)\b", r"\1\2", raw)
    # -----------------------------------------------------------------------------

    # -------------------------------------------------------------------------
    # PATCH A3 (ADDITIVE): year-range detector (tag-only, does NOT drop candidates)
    # -------------------------------------------------------------------------
    def _is_year_range_context(ctx: str) -> bool:
        return bool(re.search(r"\b(19|20)\d{2}\s*(?:-|–|—|to)\s*(19|20)\d{2}\b", ctx or "", flags=re.I))

    # -------------------------------------------------------------------------
    # ADDITIVE (Patch A2): non-destructive junk tagger
    # - We DO NOT filter here; we tag and downstream excludes by default.
    # -------------------------------------------------------------------------
    def _junk_tag(value: float, unit: str, raw_disp: str, ctx: str):
        """
        Non-destructive junk classifier.
        Returns (is_junk: bool, reason: str).
        """
        c = (ctx or "").lower()
        u = (unit or "").strip()

        # =========================
        # PATCH A3 (TAG ONLY): year-range endpoints are usually timeline metadata
        # =========================
        try:
            iv = int(float(value))
            if u == "" and 1900 <= iv <= 2099 and _is_year_range_context(ctx):
                return True, "year_range"
        except Exception:
            pass

        nav_hits = [
            "skip to content", "menu", "search", "login", "sign in", "sign up",
            "subscribe", "newsletter", "cookie", "privacy", "terms", "copyright",
            "all rights reserved", "back to top", "next", "previous", "page ",
            "home", "about", "contact", "sitemap", "breadcrumb"
        ]
        if any(h in c for h in nav_hits):
            try:
                if u == "" and abs(float(value)) <= 20:
                    return True, "nav_small_int"
            except Exception:
                pass

        if u == "":
            try:
                if abs(float(value)) <= 12:
                    if any(h in c for h in ["•", "–", "step", "chapter", "section", "item", "no."]):
                        return True, "enumeration_small_int"
            except Exception:
                pass

        if u == "":
            try:
                iv = int(abs(float(value)))
                if 190 <= iv <= 209:
                    if any(x in (raw_disp or "") for x in ["202", "203", "204", "205", "206", "207", "208", "209"]):
                        return True, "year_fragment_3digit"
            except Exception:
                return False, ""

    # -------------------------------------------------------------------------
    # PATCH M1 (ADDITIVE): semantic classifier for associations like "share" vs "units"
    # NOTE: moved OUTSIDE the loop for determinism + speed (no behavioral change).
    # Also emits a "measure_assoc" label that downstream can display easily.
    # -------------------------------------------------------------------------
    def _classify_measure(unit_tag: str, ctx: str):
        """
        Returns (measure_kind, measure_assoc):
          - measure_kind: stable internal tag (share_pct / growth_pct / count_units / money / etc.)
          - measure_assoc: human-meaning label ("share", "growth", "units", "money", "energy", etc.)
        """
        c = (ctx or "").lower()
        ut = (unit_tag or "").strip()

        if ut == "%":
            if any(k in c for k in ["market share", "share of", "share", "penetration", "portion", "contribution"]):
                return "share_pct", "share"
            if any(k in c for k in ["growth", "cagr", "increase", "decrease", "yoy", "mom", "qoq", "rate"]):
                return "growth_pct", "growth"
            return "percent_other", "percent"

        if ut in ("K", "M", "B", "T", ""):
            if any(k in c for k in ["units", "unit", "vehicles", "cars", "sold", "sales volume", "shipments", "deliveries", "registrations"]):
                return "count_units", "units"
            if any(k in c for k in ["revenue", "sales ($", "usd", "$", "market size", "valuation", "turnover"]):
                return "money", "money"
            return "magnitude_other", "magnitude"

        if ut in ("TWh", "GWh", "MWh", "kWh", "Wh"):
            return "energy", "energy"

        return "other", "other"
    # -------------------------------------------------------------------------

    # ---------- extraction pattern ----------
    # =========================
    # PATCH N1 (ADDITIVE, BUGFIX): currency tokens
    # - Fix US$ being parsed as S$ by matching US\$ first.
    # - Also accept "US$" as a single token (case-insensitive).
    # =========================
    pat = re.compile(
        r"(US\$|US\$(?!\w)|S\$|\$|USD|SGD|EUR|€|GBP|£)?\s*"
        # =========================
        # PATCH N2 (ADDITIVE, BUGFIX): avoid capturing negative year from year-range
        # - We'll still allow negatives generally, but we'll tag the special "2025-2030" case below.
        # (No behavior change for real negatives like -1.2% etc.)
        # =========================
        r"(-?\d{1,3}(?:,\d{3})*(?:\.\d+)?|-?\d+(?:\.\d+)?)(?!\d)\s*"
        # =========================
        # PATCH N3 (ADDITIVE, BUGFIX): capture 'tn' magnitude explicitly
        # - Keep your A5 safeguard: single-letter magnitudes only match if NOT followed by a letter.
        # =========================
        r"(TWh|GWh|MWh|kWh|Wh|tn|(?:T|B|M|K)(?![A-Za-z])|trillion|billion|million|bn|mn|%|percent)?",
        flags=re.I
    )

    out = []
    for m in pat.finditer(raw):
        cur = (m.group(1) or "").strip()
        num_s = (m.group(2) or "").strip()
        unit_s = (m.group(3) or "").strip()

        if not num_s:
            continue

        start = max(0, m.start() - 160)
        end = min(len(raw), m.end() + 160)
        ctx = raw[start:end].replace("\n", " ")
        ctx = re.sub(r"\s+", " ", ctx).strip()
        ctx_store = ctx[:240]

        # numeric parse
        try:
            val = float(num_s.replace(",", ""))
        except Exception:
            pass
            continue

        # normalize unit
        unit = _normalize_unit(unit_s)

        raw_disp = f"{cur} {num_s}{unit_s}".strip()
        raw_num_only = (cur + num_s).strip()

        if _chrome_junk(ctx_store):
            continue
        if _is_phone_like(ctx_store, raw_disp):
            continue
        if _is_id_like(raw_disp, ctx_store):
            continue
        if _year_only_suppression(val, unit, num_s, ctx_store):
            continue

        # =========================
        # PATCH N2b (ADDITIVE, BUGFIX): tag the "negative year from range" case as junk
        # Example: "CAGR 2025-2030" producing "-2030"
        # - Do NOT drop here (keep non-destructive policy); just tag.
        # =========================
        neg_from_hyphen_range = False
        neg_year_from_range = False
        try:
            if num_s.startswith("-") and m.start() > 0 and raw[m.start() - 1].isdigit():
                neg_from_hyphen_range = True
                iv = int(abs(float(val)))
                if 1900 <= iv <= 2099:
                    neg_year_from_range = True
        except Exception:
            pass
            neg_from_hyphen_range = False
            neg_year_from_range = False
# =========================

        anchor_hash = _sha1(f"{source_url}|{raw_disp}|{ctx_store}")
        # FIX2D69B: defensive tuple normalization (prevent unpack None)
        try:
            _jt = _junk_tag(val, unit, raw_disp, ctx_store)
            if isinstance(_jt, tuple) and len(_jt) == 2:
                is_junk, junk_reason = _jt
            else:
                is_junk, junk_reason = (False, "")
        except Exception:
            is_junk, junk_reason = (False, "")

        # =========================
        # PATCH N2c (ADDITIVE): override junk tagging reason when we confidently detect this bug
        # =========================
        if neg_year_from_range:
            is_junk = True
            junk_reason = "year_range_negative_endpoint"
        elif neg_from_hyphen_range:
            is_junk = True
            junk_reason = "hyphen_range_negative_endpoint"
        # =========================


            # =================================================================
            # PATCH YEAR_ONLY_V2 (ADDITIVE): suppress standalone years as datapoints
            # Why:
            # - Years (e.g., 2025) frequently appear in headings/ranges and should not
            #   compete with real metric values (currency, %, volumes) in evolution.
            # - We keep years only if there is strong metric context nearby.
            # Rules:
            # - If value is an integer-like 4-digit year in [1900..2100],
            #   unit is empty, and context lacks currency/%/magnitude cues => mark junk.
            # =================================================================
            try:
                if (not is_junk) and (not str(unit or "").strip()):
                    _v_int = None
                    try:
                        _v_int = int(float(val)) if val is not None else None
                    except Exception:
                        pass
                        _v_int = None

                    if _v_int is not None and 1900 <= _v_int <= 2100:
                        # Treat 4-digit years as non-metric tokens; keep but mark as junk.
                        is_junk = True
                        if not junk_reason:
                            junk_reason = "year_token"
            except Exception:
                pass
            # =================================================================
# semantic association tags
        # FIX2D69B: defensive tuple normalization (prevent unpack None)
        try:
            _cm = _classify_measure(unit, ctx_store)
            if isinstance(_cm, tuple) and len(_cm) == 2:
                measure_kind, measure_assoc = _cm
            else:
                measure_kind, measure_assoc = ("other", "other")
        except Exception:
            measure_kind, measure_assoc = ("other", "other")

        out.append({
            "value": val,
            "unit": unit,
            "raw": raw_disp,
            "source_url": source_url,
            "context": ctx_store,
            "context_snippet": ctx_store,
            "anchor_hash": anchor_hash,

            "is_junk": bool(is_junk),
            "junk_reason": junk_reason,
            "start_idx": int(m.start()),
            "end_idx": int(m.end()),

            "measure_kind": measure_kind,
            "measure_assoc": measure_assoc,
        })

        if len(out) >= int(max_results or 350):
            break

    # =========================
    # PATCH FIX2D2J (ADDITIVE): enforce unit_family/measure_kind normalization on extractor output
    # - Ensures out entries include unit_tag, unit_family, and corrected currency measure_kind/assoc
    # - Adds compact unit_measure_classifier_trace_v1 for audit
    # =========================
    try:
        fn_can = globals().get("canonicalize_numeric_candidate")
        if not callable(fn_can):
            fn_can = canonicalize_numeric_candidate
        out2 = []
        for _c in out:
            if isinstance(_c, dict):
                try:
                    _c2 = fn_can(dict(_c)) or dict(_c)
                except Exception:
                    pass
                    _c2 = dict(_c)
                out2.append(_c2)
            else:
                out2.append(_c)
        out = out2
    except Exception:
        pass
    # =========================

    return out



def extract_numbers_with_context_pdf(text):
    """
    PDF-specialized extractor wrapper.

    Tightening changes (v7.29+):
    - Inherit the year-only rejection from extract_numbers_with_context().
    - Keep boilerplate filters; prefer metric/table-like contexts.
    """
    import re

    if not text:
        return []

    base = extract_numbers_with_context(text) or []

    def _bad_pdf_context(ctx):
        c = (ctx or "").lower()
        bad = [
            "issn", "isbn", "doi", "catalogue", "legal notice",
            "all rights reserved", "reproduction is authorised",
            "printed by", "manuscript completed", "©", "copyright",
            "table of contents"
        ]
        return any(b in c for b in bad)

    def _good_pdf_context(ctx):
        c = (ctx or "").lower()
        # Lightweight heuristic: "table-ish" or "metric-ish"
        good = [
            "market", "revenue", "sales", "capacity", "generation", "growth",
            "cagr", "forecast", "projection", "increase", "decrease",
            "percent", "%", "billion", "million", "trillion", "usd", "eur", "gbp", "sgd"
        ]
        return any(g in c for g in good)

    filtered = []
    for n in base:
        if not isinstance(n, dict):
            continue
        ctx = n.get("context") or ""
        if _bad_pdf_context(ctx):
            continue
        filtered.append(n)

    # Prefer contexts that look "metric-like"
    preferred = [n for n in filtered if _good_pdf_context(n.get("context") or "")]

    # If we filtered too aggressively, fall back safely
    if preferred:
        return preferred
    if filtered:
        return filtered
    return base


def calculate_context_match(keywords: List[str], context: str) -> float:
    """Calculate how well keywords match the context (deterministic)."""
    if not context:
        return 0.0

    context_lower = context.lower()

    # If no keywords, give a small baseline (we'll rely more on value_score)
    if not keywords:
        return 0.25

    # Year keywords MUST match if present
    year_keywords = [kw for kw in keywords if re.fullmatch(r"20\d{2}", kw)]
    if year_keywords:
        if not any(y in context_lower for y in year_keywords):
            return 0.0

    matches = sum(1 for kw in keywords if kw.lower() in context_lower)

    # Instead of hard "matches < 2 = reject", scale smoothly:
    match_ratio = matches / max(len(keywords), 1)

    # If nothing matches, reject
    if matches == 0:
        return 0.0

    # Score between 0.35 and 1.0 depending on ratio
    return 0.35 + (match_ratio * 0.65)


def render_source_anchored_results(results, query: str):
    """Render source-anchored evolution results (guarded + backward compatible + tuned debug UI)."""
    import math
    import re
    from collections import Counter, defaultdict

    st.header("📈 Source-Anchored Evolution Analysis")
    st.markdown(f"**Query:** {query}")

    if not isinstance(results, dict):
        st.error("❌ Evolution returned an invalid result payload (not a dict).")
        st.write(results)
        return

    status = (results.get("status") or "").strip().lower()
    message = results.get("message") or ""

    def _safe_int(x, default=0) -> int:
        try:
            if x is None:
                return default
            return int(x)
        except Exception:
            return default

    def _safe_float(x, default=0.0) -> float:
        try:
            if x is None:
                return default
            return float(x)
        except Exception:
            return default

    def _fmt_pct(x, default="—") -> str:
        try:
            if x is None:
                return default
            v = float(x)
            if math.isnan(v):
                return default
            return f"{v:.0f}%"
        except Exception:
            return default

    def _fmt_change_pct(x) -> str:
        try:
            if x is None:
                return "-"
            v = float(x)
            if math.isnan(v):
                return "-"
            return f"{v:+.1f}%"
        except Exception:
            return "-"

    def _short(u: str, n: int = 95) -> str:
        if not u:
            return ""
        return (u[:n] + "…") if len(u) > n else u

    if status != "success":
        st.error(f"❌ {message or 'Evolution failed'}")
        sr = results.get("source_results") or []
        if isinstance(sr, list) and sr:
            st.subheader("🔗 Source Verification")
            for src in sr:
                if not isinstance(src, dict):
                    continue
                u = _short((src.get("url") or ""), 90)
                st.error(f"❌ {u} - {src.get('status_detail', 'Unknown error')}")
        return

    sources_checked = _safe_int(results.get("sources_checked"), 0)
    sources_fetched = _safe_int(results.get("sources_fetched"), 0)
    stability = _safe_float(results.get("stability_score"), 0.0)
    summary = results.get("summary") or {}
    if not isinstance(summary, dict):
        summary = {}

    metrics_inc = _safe_int(summary.get("metrics_increased"), 0)
    metrics_dec = _safe_int(summary.get("metrics_decreased"), 0)
    metrics_unch = _safe_int(summary.get("metrics_unchanged"), 0)

    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Sources Checked", sources_checked)
    col2.metric("Sources Fetched", sources_fetched)
    col3.metric("Stability", _fmt_pct(stability))
    if metrics_inc > metrics_dec:
        col4.success("📈 Trending Up")
    elif metrics_dec > metrics_inc:
        col4.error("📉 Trending Down")
    else:
        col4.info("➡️ Stable")

    if message:
        st.caption(message)

    st.markdown("---")

    # -------------------------
    # Source status
    # -------------------------
    st.subheader("🔗 Source Verification")
    src_results = results.get("source_results") or []
    if not isinstance(src_results, list):
        src_results = []

    # If everything failed, show breakdown
    if sources_checked > 0 and sources_fetched == 0 and src_results:
        reasons = []
        for s in src_results:
            if isinstance(s, dict):
                reasons.append((s.get("status_detail") or "unknown").split(":")[0])
        top = Counter(reasons).most_common(6)
        if top:
            st.warning("No sources were fetched successfully. Top failure types:")
            st.write({k: v for k, v in top})

    for src in src_results:
        if not isinstance(src, dict):
            continue
        url = src.get("url") or ""
        sstatus = src.get("status") or ""
        detail = src.get("status_detail") or ""
        ctype = src.get("content_type") or ""
        nfound = _safe_int(src.get("numbers_found"), 0)

        short = _short(url, 95)

        # show extra debug flags if present
        flags = []
        if src.get("snapshot_origin"):
            flags.append(f"origin={src.get('snapshot_origin')}")
        if src.get("is_homepage"):
            flags.append("homepage")
        if src.get("skip_reason"):
            flags.append(f"skip={src.get('skip_reason')}")
        if src.get("quality_score") is not None:
            try:
                flags.append(f"q={float(src.get('quality_score')):.2f}")
            except Exception:
                pass
                flags.append(f"q={src.get('quality_score')}")

        flag_txt = f" • {' • '.join(flags)}" if flags else ""

        if str(sstatus).startswith("fetched"):
            extra = f" ({nfound} nums)"
            if ctype:
                extra += f" • {ctype}"
            st.success(f"✅ {short}{extra}{flag_txt}")
        else:
            extra = f" - {detail}" if detail else ""
            if ctype:
                extra += f" • {ctype}"
            st.error(f"❌ {short}{extra}{flag_txt}")

    st.markdown("---")

    # -------------------------
    # Metric changes table
    # -------------------------
    st.subheader("💰 Metric Changes")

    # Prefer the V2 schema if present; fall back to legacy key for older snapshots.
    rows = results.get("metric_changes_v2") or results.get("metric_changes") or []
    if not isinstance(rows, list) or not rows:
        st.info("No metric changes to display.")
        return

    def _is_v2_row(_r: dict) -> bool:
        try:
            return isinstance(_r, dict) and (
                ("delta_pct" in _r) or ("prev_value_norm" in _r) or ("cur_value_norm" in _r)
            )
        except Exception:
            return False

    is_v2 = any(_is_v2_row(r) for r in rows)

    # Only show Δt column if at least one row has a non-empty value.
    show_delta = any(
        isinstance(r, dict) and str(r.get("analysis_evolution_delta_human") or "").strip()
        for r in rows
    )

    def _fmt_vu(v, u: str) -> str:
        """Format value + unit compactly for tables."""
        u = (u or "").strip()
        if v is None or v == "":
            return ""
        try:
            vv = float(v)
            # Use general format; keep it compact.
            s = f"{vv:g}"
        except Exception:
            s = str(v)
        return f"{s} {u}".strip()

    def _fmt_delta_pct_v2(v) -> str:
        if v is None or v == "":
            return ""
        try:
            return f"{_safe_float(v, 0.0):.2f}%"
        except Exception:
            return str(v)

    table_rows = []
    for r in rows:
        if not isinstance(r, dict):
            continue

        if is_v2:
            prev_u = r.get("previous_unit") or ""
            cur_u = r.get("current_unit") or prev_u or ""

            out_row = {
                "Metric": (r.get("name") or r.get("metric") or ""),
                "Canonical Key": (r.get("canonical_key") or ""),
                "Previous": _fmt_vu(r.get("previous_value"), prev_u),
                "Current": _fmt_vu(r.get("current_value"), cur_u),
                "Δ": ("" if r.get("delta_abs") is None else f"{_safe_float(r.get('delta_abs'), 0.0):g}"),
                "Δ%": _fmt_delta_pct_v2(r.get("delta_pct")),
                "Status": (r.get("change_type") or r.get("status") or ""),
                "Comparable": ("✅" if r.get("baseline_is_comparable") else "⚠"),
                "Method": (r.get("current_method") or ""),
            }
            if show_delta:
                out_row["Δt (A→E)"] = r.get("analysis_evolution_delta_human") or ""
            table_rows.append(out_row)
        else:
            metric_label = r.get("metric") or r.get("name") or ""
            status_label = r.get("status") or r.get("change_type") or ""

            out_row = {
                "Metric": metric_label,
                "Canonical Key": r.get("canonical_key", "") or "",
                "Match Stage": r.get("match_stage", "") or "",
                "Previous": r.get("previous_value", "") or "",
                "Current": r.get("current_value", "") or "",
                "Δ%": _fmt_change_pct(r.get("change_pct") if r.get("change_pct") is not None else r.get("delta_pct")),
                "Status": status_label,
                "Match": _fmt_pct(r.get("match_confidence")),
                "Score": ("" if r.get("match_score") is None else f"{_safe_float(r.get('match_score'), 0.0):.2f}"),
                "Anchor": "✅" if r.get("anchor_used") else "",
            }
            if show_delta:
                out_row["Δt (A→E)"] = r.get("analysis_evolution_delta_human") or ""
            table_rows.append(out_row)

    st.dataframe(table_rows, use_container_width=True)


    # -------------------------
    # Debug / tuning views
    # -------------------------
    # Aggregate rejection reasons across all metrics (quick tuning signal)
    agg_rej = Counter()
    for r in rows:
        if isinstance(r, dict) and isinstance(r.get("rejected_reason_counts"), dict):
            for k, v in r["rejected_reason_counts"].items():
                try:
                    agg_rej[k] += int(v or 0)
                except Exception:
                    pass

    if agg_rej:
        with st.expander("🧰 Tuning Summary (aggregate rejects across all metrics)"):
            st.write(dict(agg_rej.most_common(20)))

    # Full per-metric debug
    with st.expander("🧾 Per-metric match details (debug)"):
        for i, r in enumerate(rows, 1):
            if not isinstance(r, dict):
                continue

            metric_label = r.get("metric") or r.get("name") or f"metric_{i}"
            status_label = r.get("status") or r.get("change_type") or "unknown"

            canonical_key = r.get("canonical_key", "") or ""
            stage = r.get("match_stage", "") or ""
            conf = r.get("match_confidence", None)
            score = r.get("match_score", None)

            header = f"{i}. {metric_label} — {status_label}"
            meta_bits = []
            if canonical_key:
                meta_bits.append(f"ck={canonical_key}")
            if stage:
                meta_bits.append(f"stage={stage}")
            if conf is not None:
                meta_bits.append(f"conf={_fmt_pct(conf)}")
            if score is not None:
                try:
                    meta_bits.append(f"score={float(score):.2f}")
                except Exception:
                    pass
                    meta_bits.append(f"score={score}")

            if meta_bits:
                header += f"  ({' • '.join(meta_bits)})"

            with st.expander(header):
                # Values
                st.write({
                    "previous_value": r.get("previous_value"),
                    "current_value": r.get("current_value"),
                    "change_pct": r.get("change_pct"),
                })

                # Candidate considered / rejects
                st.write("Candidates considered:", _safe_int(r.get("candidates_considered_count"), 0))

                rej = r.get("rejected_reason_counts")
                if isinstance(rej, dict) and rej:
                    # sort largest first
                    try:
                        rej_sorted = dict(sorted(((k, int(v or 0)) for k, v in rej.items()), key=lambda x: x[1], reverse=True))
                    except Exception:
                        pass
                        rej_sorted = rej
                    st.write("Rejected reason counts:", rej_sorted)

                # Score breakdown (if present)
                sb = r.get("score_breakdown")
                if isinstance(sb, dict) and sb:
                    st.write("Score breakdown:", sb)

                # Matched candidate (new)
                mc = r.get("matched_candidate")
                if isinstance(mc, dict) and mc:
                    st.markdown("**Matched candidate**")
                    st.write({
                        "raw": mc.get("raw"),
                        "value": mc.get("value"),
                        "unit": mc.get("unit"),
                        "source_url": mc.get("source_url"),
                        "anchor_hash": mc.get("anchor_hash"),
                        "is_homepage": mc.get("is_homepage"),
                        "skip_reason": mc.get("skip_reason"),
                        "quality_score": mc.get("quality_score"),
                    })
                    ctx = mc.get("context_snippet")
                    if ctx:
                        st.write("Context:")
                        st.code(str(ctx))
                else:
                    # Backward-compatible fields
                    src = r.get("matched_source") or r.get("source_url")
                    ctx = r.get("matched_context") or r.get("context_snippet")
                    if src:
                        st.write("Source:", src)
                    if ctx:
                        st.write("Context:")
                        st.code(str(ctx))

                # Additional anchor hash compatibility
                if r.get("matched_anchor_hash"):
                    st.write("Matched Anchor Hash:", r.get("matched_anchor_hash"))

    st.markdown("---")


# =========================================================
# 9. DASHBOARD RENDERING
# =========================================================

def detect_x_label_dynamic(labels: list) -> str:
    """Enhanced X-axis detection with better region matching"""
    if not labels:
        return "Category"

    # Convert to lowercase for comparison
    label_texts = [str(l).lower().strip() for l in labels]
    all_text = ' '.join(label_texts)

    # 1. GEOGRAPHIC REGIONS (PRIORITY 1)
    region_keywords = [
        'north america', 'asia pacific', 'asia-pacific', 'apac', 'europe', 'emea',
        'latin america', 'latam', 'middle east', 'africa', 'oceania',
        'rest of world', 'row', 'china', 'usa', 'india', 'japan', 'germany'
    ]

    # Count how many labels contain region keywords
    region_matches = sum(
        1 for label in label_texts
        if any(keyword in label for keyword in region_keywords)
    )

    # If 40%+ of labels are regions → "Regions"
    if region_matches / len(labels) >= 0.4:
        return "Regions"

    # 2. YEARS (e.g., 2023, 2024, 2025)
    year_pattern = r'\b(19|20)\d{2}\b'
    year_count = sum(1 for label in label_texts if re.search(year_pattern, label))
    if year_count / len(labels) > 0.5:
        return "Years"

    # 3. QUARTERS (Q1, Q2, Q3, Q4)
    quarter_pattern = r'\bq[1-4]\b'
    quarter_count = sum(1 for label in label_texts if re.search(quarter_pattern, label, re.IGNORECASE))
    if quarter_count >= 2:
        return "Quarters"

    # 4. MONTHS
    months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']
    month_count = sum(1 for label in label_texts if any(month in label for month in months))
    if month_count >= 3:
        return "Months"

    # 5. COMPANIES (common suffixes)
    company_keywords = ['inc', 'corp', 'ltd', 'llc', 'gmbh', 'ag', 'sa', 'plc']
    company_count = sum(1 for label in label_texts if any(kw in label for kw in company_keywords))
    if company_count >= 2:
        return "Companies"

    # 6. PRODUCTS/SEGMENTS (if contains "segment", "product", "category")
    if any(word in all_text for word in ['segment', 'product line', 'category', 'type']):
        return "Segments"

    # Default
    return "Categories"

def detect_y_label_dynamic(values: list) -> str:
    """Fully dynamic Y-axis label based on magnitude + context"""
    if not values:
        return "Value"

    numeric_values = []
    for v in values:
        try:
            numeric_values.append(abs(float(v)))
        except (ValueError, TypeError):
            continue

    if not numeric_values:
        return "Value"

    avg_mag = np.mean(numeric_values)
    max_mag = max(numeric_values)

    # Non-overlapping ranges with clear boundaries
    # 1. BILLIONS (large market sizes)
    if max_mag > 100 or avg_mag > 50:
        return "USD B"

    # 2. MILLIONS (medium values)
    elif max_mag > 10 or avg_mag > 5:
        return "USD M"

    # 3. PERCENTAGES (typical 0-100 range, but also small decimals)
    elif max_mag <= 100 and avg_mag <= 50:
        # Check if values look like percentages (mostly 0-100)
        if all(0 <= v <= 100 for v in numeric_values):
            return "Percent %"
        else:
            return "USD K"

    # 4. Default
    else:
        return "Units"

# =========================================================
# 3A. QUESTION CATEGORIZATION + SIGNALS (DETERMINISTIC)
# =========================================================

def categorize_question_signals(query: str, qs: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Build a question_profile used for structured reporting.

    IMPORTANT:
      - category must follow query_structure if provided (single source of truth).
      - signals can be rich, but must not contradict the chosen category.
    """
    qs = qs or {}
    q = (query or "").strip()

    # Prefer category/main/side from query_structure when available
    category = (qs.get("category") or "").strip() or "unknown"
    main_q = (qs.get("main") or "").strip() or q
    side_qs = qs.get("side") if isinstance(qs.get("side"), list) else []

    # Deterministic signals (richer classifier)
    base = classify_question_signals(q) or {}

    # Force category + expected_metric_ids to match query_structure category
    # (but preserve other extracted info like years/regions/intents)
    signals: Dict[str, Any] = {}
    signals["category"] = category

    # Carry over extracted fields
    signals["years"] = base.get("years", []) or []
    signals["regions"] = base.get("regions", []) or []
    signals["intents"] = base.get("intents", []) or []

    # Keep raw signals for debugging
    raw_hits = list(base.get("signals") or [])
    signals["raw_signals"] = raw_hits

    def _signal_consistent_with_category(sig: str, cat: str) -> bool:
        s = (sig or "").lower()
        c = (cat or "").lower()
        if not s:
            return False

        # If final category is country, drop industry/company category-rule strings
        if c == "country":
            if "industry_keywords" in s or "mixed_signals_default_to_industry" in s or "company_keywords" in s:
                return False

        # If final category is industry, drop explicit country-rule strings
        if c == "industry":
            if "macro_outlook_bias_country" in s or "country_keywords" in s:
                return False

        return True

    signals["signals"] = [s for s in raw_hits if _signal_consistent_with_category(s, category)]

    # Expected metric IDs: always determined by the final category, then lightly enriched by intents (optional)
    expected_metric_ids: List[str] = []
    try:
        expected_metric_ids = get_expected_metric_ids_for_category(category) or []
    except Exception:
        pass
        expected_metric_ids = []

    # Optional: enrich with intent-based suggestions (won't remove anything)
    intent_metric_suggestions = {
        "market_size": ["market_size", "market_size_2024", "market_size_2025"],
        "growth_forecast": ["cagr", "market_size_2030"],
        "competitive_landscape": ["market_share", "top_players"],
        "pricing": ["avg_price", "asp"],
        "consumer_demand": ["users", "penetration", "arpu"],
        "supply_chain": ["capacity", "shipments"],
        "investment": ["capex", "profit", "ebitda"],
        "macro_outlook": ["gdp", "inflation", "interest_rate", "exchange_rate"],
    }

    intents = signals.get("intents") or []
    for intent in intents:
        for mid in intent_metric_suggestions.get(intent, []):
            if mid not in expected_metric_ids:
                expected_metric_ids.append(mid)

    signals["expected_metric_ids"] = expected_metric_ids

    profile: Dict[str, Any] = {
        "category": category,
        "signals": signals,
        "main_question": main_q,
        "side_questions": side_qs,
    }

    # Keep debug for traceability
    if qs.get("debug") is not None:
        profile["debug_query_structure"] = qs.get("debug")

    return profile




def render_dashboard(
    primary_json: str,
    final_conf: float,
    web_context: Dict,
    base_conf: float,
    user_question: str,
    veracity_scores: Optional[Dict] = None,
    source_reliability: Optional[List[str]] = None,
):
    """Render the analysis dashboard"""

    # -------------------------
    # Parse primary response
    # -------------------------

    # =========================
    # PATCH RD1 (ADDITIVE): safe preview helper
    # - Prevents slice errors when primary_json is dict/list/etc.
    # - Keeps original behavior for strings
    # =========================
    def _preview(x, limit: int = 1000) -> str:
        try:
            if isinstance(x, (dict, list)):
                s = json.dumps(x, ensure_ascii=False, indent=2, default=str)
            else:
                s = str(x)
        except Exception:
            pass
            s = repr(x)
        return s[:limit]
    # =========================

    try:
        # =========================
        # PATCH RD2 (ADDITIVE): accept dict/list directly
        # - If caller passes dict (primary_data), just use it
        # - If caller passes list, wrap it (keeps downstream dict access safe)
        # - Else try json.loads on string
        # =========================
        if isinstance(primary_json, dict):
            data = primary_json
        elif isinstance(primary_json, list):
            data = {"_list": primary_json}
        else:
            data = json.loads(primary_json)
        # =========================

    except Exception as e:
        st.error(f"❌ Cannot render dashboard: {e}")
        # =========================
        # PATCH RD1 (ADDITIVE): safe preview (no slicing crash)
        # =========================
        st.code(_preview(primary_json))
        # =========================
        return

    # -------------------------
    # Helper: metric value formatting (currency + compact units) + RANGE SUPPORT
    # -------------------------
    def _format_metric_value(m: Any) -> str:
        """
        Format metric values cleanly, with RANGE SUPPORT:
        - If value_range exists (min/max), show min–max using the same currency/unit rules
        - Otherwise show the point value as before
        """
        if not isinstance(m, dict):
            if m is None:
                return "N/A"
            return str(m)

        # -------------------------
        # Helper: format a single numeric endpoint (val+unit)
        # -------------------------
        def _format_point(val: Any, unit: str) -> str:
            if val is None or val == "":
                return "N/A"

            unit = (unit or "").strip()
            raw_val = str(val).strip()

            # Try parse numeric
            try:
                num = float(raw_val.replace(",", ""))
            except Exception:
                pass
                # If we can't parse as float, just glue value+unit neatly
                return f"{raw_val}{unit}".strip() if unit else raw_val

            # Normalize unit spacing
            unit = unit.replace(" ", "")
            currency_prefix = ""
            u_upper = unit.upper()

            # Common patterns: "S$B", "SGDB", "USD B", "$B"
            if u_upper.startswith("S$"):
                currency_prefix = "S$"
                unit = unit[2:]
            elif u_upper.startswith("SGD"):
                currency_prefix = "S$"
                unit = unit[3:]
            elif u_upper.startswith("USD"):
                currency_prefix = "$"
                unit = unit[3:]
            elif u_upper.startswith("$"):
                currency_prefix = "$"
                unit = unit[1:]

            unit = unit.strip()

            # Percent
            if unit == "%":
                return f"{num:.1f}%"

            # Compact units
            unit_upper = unit.upper()
            if unit_upper in ("B", "BILLION"):
                formatted = f"{num:.2f}".rstrip("0").rstrip(".") + "B"
                return f"{currency_prefix}{formatted}".strip()
            if unit_upper in ("M", "MILLION"):
                formatted = f"{num:.2f}".rstrip("0").rstrip(".") + "M"
                return f"{currency_prefix}{formatted}".strip()
            if unit_upper in ("K", "THOUSAND"):
                formatted = f"{num:.2f}".rstrip("0").rstrip(".") + "K"
                return f"{currency_prefix}{formatted}".strip()

            # Plain number formatting
            if abs(num) >= 1000:
                if float(num).is_integer():
                    formatted = f"{int(num):,}"
                else:
                    formatted = f"{num:,.2f}".rstrip("0").rstrip(".")
            else:
                formatted = f"{num:g}"

            # Unit glue
            if unit:
                formatted = f"{formatted} {unit}".strip()

            return f"{currency_prefix}{formatted}".strip()

        # -------------------------
        # RANGE: prefer value_range if present and meaningful
        # -------------------------
        unit = (m.get("unit") or "").strip()
        vr = m.get("value_range")

        if isinstance(vr, dict):
            vmin = vr.get("min")
            vmax = vr.get("max")
            if vmin is not None and vmax is not None:
                left = _format_point(vmin, unit)
                right = _format_point(vmax, unit)
                if left != "N/A" and right != "N/A" and left != right:
                    return f"{left}–{right}"

        # Precomputed range display (optional)
        vr_disp = m.get("value_range_display")
        if isinstance(vr_disp, str) and vr_disp.strip():
            return vr_disp.strip()

        # -------------------------
        # POINT VALUE fallback
        # -------------------------
        val = m.get("value")
        if val is None or val == "":
            return "N/A"

        return _format_point(val, unit)

    # -------------------------
    # Header + confidence row
    # -------------------------
    st.header("📊 Yureeka Market Report")
    st.markdown(f"**Question:** {user_question}")

    col1, col2, col3 = st.columns(3)
    col1.metric("Final Confidence", f"{float(final_conf):.1f}%")
    col2.metric("Base Model", f"{float(base_conf):.1f}%")
    if isinstance(veracity_scores, dict):
        col3.metric("Evidence", f"{float(veracity_scores.get('overall', 0) or 0):.1f}%")
    else:
        col3.metric("Evidence", "N/A")

    st.markdown("---")

    # -------------------------
    # Executive Summary
    # -------------------------
    st.subheader("📋 Executive Summary")
    st.markdown(f"**{data.get('executive_summary', 'No summary available')}**")

    # Optional: expand summary if side-questions exist
    side_questions = data.get("side_questions") or (data.get("question_profile", {}) or {}).get("side_questions", [])
    if side_questions:
        st.markdown("")
        st.markdown("**Also addressed:**")
        for sq in side_questions[:6]:
            if sq:
                st.markdown(f"- {sq}")

    st.markdown("---")

    # -------------------------
    # Key Metrics
    # -------------------------
    st.subheader("💰 Key Metrics")
    metrics = data.get("primary_metrics", {}) or {}

    question_category = data.get("question_category") or (data.get("question_profile", {}) or {}).get("category")
    question_signals = data.get("question_signals") or (data.get("question_profile", {}) or {}).get("signals", {})
    expected_ids = data.get("expected_metric_ids") or ((data.get("question_signals") or {}).get("expected_metric_ids") or [])

    metric_rows: List[Dict[str, str]] = []

    if question_category:
        metric_rows.append({"Metric": "Question Category", "Value": str(question_category)})
    if isinstance(question_signals, dict) and question_signals:
        metric_rows.append({"Metric": "Signals", "Value": ", ".join([str(x) for x in (question_signals.get("signals") or [])][:10])})
    if expected_ids:
        metric_rows.append({"Metric": "Expected Metrics", "Value": ", ".join([str(x) for x in expected_ids][:10])})

    # Render primary metrics
    if isinstance(metrics, dict) and metrics:
        for _, m in metrics.items():
            if isinstance(m, dict):
                name = m.get("name") or "Metric"
                metric_rows.append({"Metric": str(name), "Value": _format_metric_value(m)})

    # Display metrics table
    if metric_rows:
        try:
            import pandas as pd  # optional dependency in your environment
            df_metrics = pd.DataFrame(metric_rows)
            st.dataframe(df_metrics, use_container_width=True, hide_index=True)
        except Exception:
            pass
            for r in metric_rows:
                st.write(f"**{r.get('Metric','')}**: {r.get('Value','')}")

    st.markdown("---")

    # -------------------------
    # Key Findings
    # -------------------------
    st.subheader("🧠 Key Findings")
    kf = data.get("key_findings") or []
    if isinstance(kf, list) and kf:
        for item in kf[:12]:
            if item:
                st.markdown(f"- {item}")
    else:
        st.info("No key findings available.")

    st.markdown("---")

    # -------------------------
    # Trends / Forecast
    # -------------------------
    st.subheader("📈 Trends & Forecast")
    tf = data.get("trends_forecast") or []
    if isinstance(tf, list) and tf:
        for t in tf[:12]:
            if isinstance(t, dict):
                trend = t.get("trend") or ""
                direction = t.get("direction") or ""
                timeline = t.get("timeline") or ""
                st.markdown(f"- **{trend}** {direction} ({timeline})")
            elif t:
                st.markdown(f"- {t}")
    else:
        st.info("No trends forecast available.")

    st.markdown("---")

    # -------------------------
    # Sources / Web Context summary
    # -------------------------
    st.subheader("🔎 Sources & Evidence")
    sources = data.get("sources") or data.get("web_sources") or []
    if isinstance(sources, list) and sources:
        with st.expander(f"Show sources ({len(sources)})"):
            for s in sources[:50]:
                if s:
                    st.markdown(f"- {s}")
            if len(sources) > 50:
                st.markdown(f"... (+{len(sources)-50} more)")

    # Web context debug counters if present
    if isinstance(web_context, dict):
        dbg = web_context.get("debug_counts") or {}
        if isinstance(dbg, dict) and dbg:
            with st.expander("Collector debug counts"):
                st.json(dbg)

    # =====================================================================
    # PATCH UI_EXTRA_URLS_TRACE2 (ADDITIVE): show injected extra-URL trace (if any)
    # =====================================================================
    try:
        exdbg = {}
        if isinstance(web_context, dict):
            exdbg = web_context.get("extra_urls_debug") or {}
            # Back-compat: allow nested placement under debug_counts
            if (not exdbg) and isinstance(web_context.get("debug_counts"), dict):
                exdbg = (web_context.get("debug_counts") or {}).get("extra_urls_debug") or {}
        if isinstance(exdbg, dict) and exdbg:
            with st.expander("Extra URLs trace (injected sources)"):
                st.json(exdbg)
    except Exception:
        pass
    # =====================================================================

    # Source reliability badges (if provided)
    if isinstance(source_reliability, list) and source_reliability:
        with st.expander("Source reliability"):
            for line in source_reliability[:80]:
                st.write(line)



def render_native_comparison(baseline: Dict, compare: Dict):
    """Render a clean comparison between two analyses"""

    st.header("📊 Analysis Comparison")

    # Time info
    baseline_time = baseline.get('timestamp', '')
    compare_time = compare.get('timestamp', '')

    try:
        baseline_dt = datetime.fromisoformat(baseline_time.replace('Z', '+00:00'))
        compare_dt = datetime.fromisoformat(compare_time.replace('Z', '+00:00'))
        delta = compare_dt - baseline_dt
        if delta.days > 0:
            delta_str = f"{delta.days}d {delta.seconds // 3600}h"
        else:
            delta_str = f"{delta.seconds // 3600}h {(delta.seconds % 3600) // 60}m"
    except:
        delta_str = "Unknown"

    # Overview row
    col1, col2, col3 = st.columns(3)
    col1.metric("Baseline", baseline_time[:16] if baseline_time else "N/A")
    col2.metric("Current", compare_time[:16] if compare_time else "N/A")
    col3.metric("Time Delta", delta_str)

    st.markdown("---")

    # Extract metrics
    baseline_metrics = baseline.get('primary_response', {}).get('primary_metrics', {})
    compare_metrics = compare.get('primary_response', {}).get('primary_metrics', {})

    # Build metric diff table
    st.subheader("💰 Metric Changes")

    diff_rows = []
    stability_count = 0
    total_count = 0

    # Canonicalize metrics for stable matching
    baseline_canonical = canonicalize_metrics(baseline_metrics)
    compare_canonical = canonicalize_metrics(compare_metrics)

    # Build lookup by canonical ID
    baseline_by_id = {}
    compare_by_id = {}

    for cid, m in baseline_canonical.items():
        baseline_by_id[cid] = m

    for cid, m in compare_canonical.items():
        compare_by_id[cid] = m

    all_ids = set(baseline_by_id.keys()).intersection(compare_by_id.keys())

    for cid in sorted(all_ids):
        baseline_m = baseline_by_id.get(cid)
        compare_m = compare_by_id.get(cid)

        # Use canonical name for display, fallback to original
        display_name = cid
        if baseline_m and baseline_m.get('name'):
            display_name = baseline_m['name']


        if baseline_m and compare_m:
            old_val = baseline_m.get('value', 'N/A')
            new_val = compare_m.get('value', 'N/A')
            unit = compare_m.get('unit', baseline_m.get('unit', ''))

            old_num = parse_to_float(old_val)
            new_num = parse_to_float(new_val)

            if old_num is not None and new_num is not None and old_num != 0:
                change_pct = ((new_num - old_num) / abs(old_num)) * 100

                if abs(change_pct) < 1:
                    icon, reason = "➡️", "No change"
                    stability_count += 1
                elif abs(change_pct) < 5:
                    icon, reason = "➡️", "Minor change"
                    stability_count += 1
                elif change_pct > 0:
                    icon, reason = "📈", "Increased"
                else:
                    icon, reason = "📉", "Decreased"

                delta_str = f"{change_pct:+.1f}%"
            else:
                icon, delta_str, reason = "➡️", "-", "Non-numeric"
                stability_count += 1

            diff_rows.append({
                '': icon,
                'Metric': display_name,
                'Old': _fmt_currency_first(str(old_val), str(unit)),
                'New': _fmt_currency_first(str(new_val), str(unit)),
                'Δ': delta_str,
                'Reason': reason
            })
            total_count += 1

        elif baseline_m:
            old_val = baseline_m.get('value', 'N/A')
            unit = baseline_m.get('unit', '')
            diff_rows.append({
                '': '❌',
                'Metric': display_name,
                'Old': f"{old_val} {unit}".strip(),
                'New': '-',
                'Δ': '-',
                'Reason': 'Removed'
            })
            total_count += 1
        else:
            new_val = compare_m.get('value', 'N/A')
            unit = compare_m.get('unit', '')
            diff_rows.append({
                '': '🆕',
                'Metric': display_name,
                'Old': '-',
                'New': f"{new_val} {unit}".strip(),
                'Δ': '-',
                'Reason': 'New'
            })
            total_count += 1

    if diff_rows:
        st.dataframe(pd.DataFrame(diff_rows), hide_index=True, use_container_width=True)

        # Show canonical ID mapping for debugging
        with st.expander("🔧 Canonical ID Mapping (Debug)"):
            st.write("**How metrics were matched:**")

            baseline_canonical = canonicalize_metrics(baseline_metrics)
            compare_canonical = canonicalize_metrics(compare_metrics)

            col1, col2 = st.columns(2)

            with col1:
                st.write("**Baseline Metrics:**")
                for cid, m in baseline_canonical.items():
                    original = m.get('original_name', 'N/A')
                    canonical = m.get('name', 'N/A')
                    st.caption(f"`{cid}`")
                    st.write(f"  {original} → {canonical}")

            with col2:
                st.write("**Current Metrics:**")
                for cid, m in compare_canonical.items():
                    original = m.get('original_name', 'N/A')
                    canonical = m.get('name', 'N/A')
                    st.caption(f"`{cid}`")
                    st.write(f"  {original} → {canonical}")
    else:
        st.info("No metrics to compare")

    # Stability score
    stability_pct = (stability_count / total_count * 100) if total_count > 0 else 100

    st.markdown("---")
    st.subheader("📊 Stability Score")

    col1, col2, col3 = st.columns(3)
    col1.metric("Stable Metrics", f"{stability_count}/{total_count}")
    col2.metric("Stability", f"{stability_pct:.0f}%")

    if stability_pct >= 80:
        col3.success("🟢 Highly Stable")
    elif stability_pct >= 60:
        col3.warning("🟡 Moderate Changes")
    else:
        col3.error("🔴 Significant Drift")

    # Confidence comparison
    st.markdown("---")
    st.subheader("🎯 Confidence Change")

    col1, col2, col3 = st.columns(3)
    baseline_conf = baseline.get('final_confidence', 0)
    compare_conf = compare.get('final_confidence', 0)
    conf_change = compare_conf - baseline_conf if isinstance(baseline_conf, (int, float)) and isinstance(compare_conf, (int, float)) else 0

    col1.metric("Baseline", f"{baseline_conf:.1f}%" if isinstance(baseline_conf, (int, float)) else "N/A")
    col2.metric("Current", f"{compare_conf:.1f}%" if isinstance(compare_conf, (int, float)) else "N/A")
    col3.metric("Change", f"{conf_change:+.1f}%")

    # Download comparison
    st.markdown("---")
    comparison_output = {
        "comparison_timestamp": _yureeka_now_iso_utc(),
        "baseline": baseline,
        "current": compare,
        "stability_score": stability_pct,
        "metrics_compared": total_count,
        "metrics_stable": stability_count
    }

    st.download_button(
        label="💾 Download Comparison Report",
        data=json.dumps(comparison_output, indent=2, ensure_ascii=False).encode('utf-8'),
        file_name=f"yureeka_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        mime="application/json"
    )

# =========================================================
# 10. MAIN APPLICATION
# =========================================================

# ==============================================================================
# PATCH FIX39 (ADDITIVE): Final publish/render unit-required hard gate
#
# Why:
# - Even if upstream selection is tightened, some paths (UI render, sheet publish,
#   legacy mappings) can still surface unit-less year-like integers (e.g., 2024/2025)
#   in the "Current" column for unit-required metrics (currency/percent/rate/ratio).
# - FIX39 enforces the invariant at the last mile: right before rendering/publishing.
#
# Behavior:
# - For each metric change row (dict-form) and each EvolutionDiff metric entry (object-form),
#   if schema indicates unit required (via unit_family or canonical_key suffix) AND
#   current value lacks token-level unit evidence, then:
#     * blank out Current/new_raw
#     * set unit_mismatch flag / change_type to "unit_mismatch" where possible
# - Purely additive; does not refactor upstream pipelines.
# ==============================================================================

def _fix39_schema_unit_required(metric_def: dict, canonical_key: str = "") -> bool:
    try:
        uf = str((metric_def or {}).get("unit_family") or (metric_def or {}).get("unit") or "").strip().lower()
        if uf in {"currency", "percent", "rate", "ratio"}:
            return True
    except Exception:
        pass
    ck = (canonical_key or "").strip().lower()
    if ck.endswith("__currency") or ck.endswith("__percent") or ck.endswith("__rate") or ck.endswith("__ratio"):
        return True
    # unit_tag explicit
    try:
        ut = str((metric_def or {}).get("unit_tag") or "").strip()
        if ut:
            # if schema explicitly wants a unit token, treat as required
            return True
    except Exception:
        return False

def _fix39_has_unit_evidence(metric_like: dict) -> bool:
    """Token-level unit evidence check (tolerant across shapes)."""
    try:
        m = metric_like if isinstance(metric_like, dict) else {}
        for k in ("unit", "unit_tag", "base_unit", "unit_family", "currency", "currency_symbol"):
            if str(m.get(k) or "").strip():
                return True
        if bool(m.get("is_percent") or m.get("has_percent")):
            return True
        # Some rows store comparator field
        if str(m.get("cur_unit_cmp") or "").strip():
            return True
        raw = str(m.get("raw") or m.get("value") or m.get("new_raw") or "")
        if raw and any(sym in raw for sym in ("$", "€", "£", "¥", "%")):
            return True
    except Exception:
        return False

def _fix39_sanitize_metric_change_rows(results_dict: dict) -> None:
    """Sanitize dict-based evolution results before publishing/rendering."""
    if not isinstance(results_dict, dict):
        return
    try:
        schema = results_dict.get("metric_schema_frozen") or results_dict.get("schema") or {}
        metric_changes = None
        # common nesting patterns
        if isinstance(results_dict.get("results"), dict) and isinstance(results_dict["results"].get("metric_changes"), list):
            metric_changes = results_dict["results"]["metric_changes"]
        elif isinstance(results_dict.get("metric_changes"), list):
            metric_changes = results_dict.get("metric_changes")

        if not isinstance(metric_changes, list):
            return

        bad = []
        for row in metric_changes:
            if not isinstance(row, dict):
                continue
            ck = row.get("canonical_key") or row.get("canonical") or row.get("key") or ""
            md = {}
            try:
                if isinstance(schema, dict) and ck in schema:
                    md = schema.get(ck) or {}
            except Exception:
                pass
                md = {}
            if _fix39_schema_unit_required(md, ck):
                # current fields may be in different keys
                cur_like = {
                    "unit": row.get("current_unit") or row.get("cur_unit") or row.get("unit") or "",
                    "unit_tag": row.get("current_unit_tag") or row.get("unit_tag") or "",
                    "unit_family": row.get("schema_unit_family") or "",
                    "cur_unit_cmp": row.get("cur_unit_cmp") or "",
                    "raw": row.get("current_value") or row.get("current_raw") or row.get("Current") or "",
                    "new_raw": row.get("new_raw") or "",
                    "currency_symbol": row.get("currency_symbol") or "",
                    "is_percent": row.get("is_percent") or False,
                }
                if not _fix39_has_unit_evidence(cur_like):
                    # FIX2D76: v2 rows often keep unit evidence under diag.diff_current_source_trace_v1
                    try:
                        diag = row.get("diag") if isinstance(row.get("diag"), dict) else {}
                        dcs = diag.get("diff_current_source_trace_v1") if isinstance(diag.get("diff_current_source_trace_v1"), dict) else {}
                        ut = str(dcs.get("current_unit_tag") or dcs.get("current_unit_tag_norm") or "").strip()
                        if ut:
                            cur_like["unit"] = cur_like.get("unit") or ut
                            cur_like["unit_tag"] = cur_like.get("unit_tag") or ut
                            cur_like["cur_unit_cmp"] = cur_like.get("cur_unit_cmp") or ut
                    except Exception:
                        pass
                if not _fix39_has_unit_evidence(cur_like):
                    # invalidate
                    row["unit_mismatch"] = True
                    # prefer explicit fields if present
                    for k in ("current_value", "current_raw", "new_raw", "Current"):
                        if k in row:
                            row[k] = ""
                    if "current_value_norm" in row:
                        row["current_value_norm"] = None
                    if "cur_value_norm" in row:
                        row["cur_value_norm"] = None
                    # normalize change_type
                    if row.get("change_type") not in ("unit_mismatch", "invalid_current"):
                        row["change_type"] = "unit_mismatch"
                    bad.append(str(ck))
        # small debug marker
        dbg = results_dict.setdefault("debug", {})
        f39 = dbg.setdefault("fix39", {})
        f39["invalidated_count"] = len(bad)
        if bad:
            f39["invalidated_keys_sample"] = bad[:20]
    except Exception:
        return

def _fix39_sanitize_evolutiondiff_object(diff_obj, metric_schema_frozen: dict = None):
    """Sanitize object-based EvolutionDiff (used by Streamlit renderer)."""
    try:
        schema = metric_schema_frozen or {}
        mdiffs = getattr(diff_obj, "metric_diffs", None)
        if not mdiffs:
            return diff_obj
        bad = []
        for m in mdiffs:
            try:
                ck = getattr(m, "canonical_key", "") or getattr(m, "canonical", "") or ""
                md = schema.get(ck) if isinstance(schema, dict) else {}
                if _fix39_schema_unit_required(md or {}, ck):
                    unit = getattr(m, "unit", "") or ""
                    new_raw = getattr(m, "new_raw", None)
                    # basic evidence check: unit or symbol in new_raw
                    has_e = bool(str(unit).strip())
                    if not has_e:
                        s = str(new_raw or "")
                        if any(sym in s for sym in ("$", "€", "£", "¥", "%")):
                            has_e = True
                    if not has_e:
                        # invalidate
                        try: setattr(m, "new_raw", "")
                        except Exception: pass
                        try: setattr(m, "new_value", None)
                        except Exception: pass
                        try: setattr(m, "change_type", "unit_mismatch")
                        except Exception: pass
                        bad.append(str(ck))
            except Exception:
                pass
                continue
        try:
            dbg = getattr(diff_obj, "debug", None)
            if isinstance(dbg, dict):
                dbg.setdefault("fix39", {})["invalidated_count"] = len(bad)
        except Exception:
            return diff_obj
    except Exception:
        return diff_obj

def main():
    st.set_page_config(
        page_title="Yureeka Market Report",
        page_icon="💹",
        layout="wide"
    )

    st.title("💹 Yureeka Market Intelligence")
    _yureeka_show_debug_playbook_in_streamlit_v1()

    # Info section
    col_info, col_status = st.columns([3, 1])
    with col_info:
        st.markdown("""
        **Yureeka** provides AI-powered market research and analysis for finance,
        economics, and business questions.
        Powered by evidence-based verification and real-time web search.

        *Currently in prototype stage.*
        """)

    # Create tabs
    tab1, tab2 = st.tabs(["🔍 New Analysis", "📈 Evolution Analysis"])

    # =====================
    # TAB 1: NEW ANALYSIS
    # =====================
    with tab1:
        query = st.text_input(
            "Enter your question about markets, industries, finance, or economics:",
            placeholder="e.g., What is the size of the global EV battery market?"
        )

        col_opt1, col_opt2 = st.columns(2)
        with col_opt1:
            use_web = st.checkbox(
                "Enable web search (recommended)",
                value=bool(SERPAPI_KEY),
                disabled=not SERPAPI_KEY
            )


            # ============================================================

            # PATCH UI_EXTRA_SOURCES_TAB1 (ADDITIVE)

            # - Add extra URL injection UI directly to TAB 1 (New Analysis)

            # - Does NOT alter behavior unless user supplies URLs

            # ============================================================

            extra_sources_text_tab1 = st.text_area(

                "Extra source URLs (optional, one per line)",

                placeholder="https://example.com/report\nhttps://another-source.com/page",

                help="Add these URLs to the admitted source list for this analysis run (useful for hash-mismatch tests).",

                height=90,

                key="ui_extra_sources_tab1",

            )

            # ============================================================



        if st.button("🔍 Analyze", type="primary") and query:
            if len(query.strip()) < 5:
                st.error("❌ Please enter a question with at least 5 characters")
                return

            query = query.strip()[:500]

            query_structure = extract_query_structure(query) or {}
            question_profile = categorize_question_signals(query, qs=query_structure)
            question_signals = question_profile.get("signals", {}) or {}

            web_context = {}
            if use_web:
                with st.spinner("🌐 Searching the web..."):

                    # ---- ADDITIVE: pass existing snapshots for reuse (Change #3 wiring) ----
                    existing_snapshots = None

                    # If you have an analysis dict already in scope, reuse its cache
                    try:
                        if isinstance(locals().get("analysis"), dict):
                            existing_snapshots = (
                                analysis.get("baseline_sources_cache")
                                or (analysis.get("results", {}) or {}).get("baseline_sources_cache")
                                or (analysis.get("results", {}) or {}).get("source_results")
                            )
                    except Exception:
                        pass
                        existing_snapshots = None

                    # Optional: if you keep a prior analysis in session_state, reuse it
                    try:
                        prev = st.session_state.get("last_analysis")
                        if existing_snapshots is None and isinstance(prev, dict):
                            existing_snapshots = (
                                prev.get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("source_results")
                            )
                    except Exception:
                        pass

                                        # ============================================================
                    # PATCH UI_EXTRA_SOURCES2 (ADDITIVE): parse extra source URLs
                    # ============================================================
                    extra_urls = []
                    try:
                        for _l in str(extra_sources_text_tab1 or "").splitlines():
                            _u = _l.strip()
                            if not _u:
                                continue
                            if _u.startswith("http://") or _u.startswith("https://"):
                                extra_urls.append(_u)
                    except Exception:
                        pass
                        extra_urls = []


                    # ============================================================
                    # PATCH INJ_DIAG_TAB1_CALL (ADDITIVE): correlate UI extra-URL input into fetch_web_context diagnostics
                    # ============================================================
                    _analysis_run_id = _inj_diag_make_run_id("analysis")
                    # ============================================================

                    web_context = fetch_web_context(
                        query,
                        num_sources=3,
                        existing_snapshots=existing_snapshots,
                        extra_urls=extra_urls,
                        diag_run_id=_analysis_run_id,
                        diag_extra_urls_ui_raw=(extra_sources_text_tab1 or ""),
                    )
                    # ----------------------------------------------------------------------

            if not web_context or not web_context.get("search_results"):
                st.info("💡 Using AI knowledge without web search")
                web_context = {
                    "search_results": [],
                    "scraped_content": {},
                    "summary": "",
                    "sources": [],
                    "source_reliability": []
                }

            with st.spinner("🤖 Analyzing query..."):
                primary_response = query_perplexity(query, web_context, query_structure=query_structure)

            if not primary_response:
                st.error("❌ Primary model failed to respond")
                return

            try:
                primary_data = json.loads(primary_response)
            except Exception as e:
                st.error(f"❌ Failed to parse primary response: {e}")
                st.code(primary_response[:1000])
                return

            with st.spinner("✅ Verifying evidence quality..."):
                veracity_scores = evidence_based_veracity(primary_data, web_context)

            base_conf = float(primary_data.get("confidence", 75))
            final_conf = calculate_final_confidence(base_conf, veracity_scores.get("overall", 0))

            # Optional: canonicalize + attribution + schema freeze (only if your codebase defines these)
            try:
                # 1) canonicalize (unchanged)
                if primary_data.get("primary_metrics"):
                    _pmc_raw = canonicalize_metrics(
                        primary_data.get("primary_metrics", {}),
                        merge_duplicates_to_range=True,
                        question_text=query,
                        category_hint=str(primary_data.get("question_category", ""))
                    )
                    _pmc_ok, _pmc_prov = _fix2d58b_split_primary_metrics_canonical(_pmc_raw)
                    primary_data["primary_metrics_canonical"] = _pmc_ok
                    if _pmc_prov:
                        primary_data["primary_metrics_provisional"] = _pmc_prov

                # 2) freeze schema FIRST ✅ (so attribution can be schema-first)
                if primary_data.get("primary_metrics_canonical"):
                    primary_data["metric_schema_frozen"] = freeze_metric_schema(
                        primary_data["primary_metrics_canonical"]
                    )


                # PATCH FIX2U_EV_CHARGERS_SCHEMA_APPLY_V1 (ADDITIVE)
                try:
                    fn_fix2u = globals().get("_fix2u_extend_metric_schema_ev_chargers")
                    if callable(fn_fix2u):
                        primary_data["metric_schema_frozen"] = fn_fix2u(primary_data.get("metric_schema_frozen") or {})
                except Exception:
                    pass
                # END PATCH FIX2U_EV_CHARGERS_SCHEMA_APPLY_V1

                # =========================================================
                # PATCH FIX2V_EV_CHARGERS_CAGR_SCHEMA_APPLY_V1 (ADDITIVE)
                try:
                    fn_fix2v = globals().get("_fix2v_extend_metric_schema_ev_chargers_cagr")
                    if callable(fn_fix2v):
                        primary_data["metric_schema_frozen"] = fn_fix2v(primary_data.get("metric_schema_frozen") or {})
                except Exception:
                    pass
                # END PATCH FIX2V_EV_CHARGERS_CAGR_SCHEMA_APPLY_V1

                # =========================================================
                # PATCH FIX2AB_GLOBAL_EV_SALES_YTD_2025_SCHEMA_APPLY_V1 (ADDITIVE)
                try:
                    fn_fix2ab = globals().get("_fix2ab_extend_metric_schema_global_ev_sales_ytd_2025")
                    if callable(fn_fix2ab):
                        primary_data["metric_schema_frozen"] = fn_fix2ab(primary_data.get("metric_schema_frozen") or {})
                except Exception:
                    pass
                # END PATCH FIX2AB_GLOBAL_EV_SALES_YTD_2025_SCHEMA_APPLY_V1

                # =========================================================
                # PATCH FIX2D61 (ADDITIVE): Option A schema extension from provisional metrics
                # - Build schema proposals from primary_metrics_provisional using freeze_metric_schema.
                # - Auto-promote into metric_schema_frozen (governance can later restrict via allowlist).
                # - Record proposals/promotions for audit.
                # =========================================================
                try:
                    _prov = primary_data.get("primary_metrics_provisional")
                    _schema = primary_data.get("metric_schema_frozen")
                    if isinstance(_prov, dict) and _prov and isinstance(_schema, dict):
                        _prov_schema = freeze_metric_schema(_prov)
                        if isinstance(_prov_schema, dict) and _prov_schema:
                            primary_data["schema_promotion_proposals_v1"] = sorted([str(k) for k in _prov_schema.keys()])
                            # Auto-promote: merge proposals into frozen schema
                            for _k, _spec in _prov_schema.items():
                                if _k not in _schema:
                                    _schema[_k] = _spec
                            primary_data["metric_schema_frozen"] = _schema
                            primary_data["schema_promoted_v1"] = sorted([str(k) for k in _prov_schema.keys() if str(k) in _schema])
                except Exception:
                    pass
                # END PATCH FIX2D61
                # =========================================================


                # =========================================================


                # 2.B) FIX2D59: schema-first canonical identity rekey (Analysis)
                # - Routes existing pmc keys through the resolver using the frozen schema (after schema extension patches).
                try:
                    if isinstance(primary_data.get('primary_metrics_canonical'), dict) and isinstance(primary_data.get('metric_schema_frozen'), dict):
                        primary_data['primary_metrics_canonical'] = rekey_metrics_via_identity_resolver_v1(
                            primary_data.get('primary_metrics_canonical') or {},
                            primary_data.get('metric_schema_frozen') or {},
                        )
                except Exception:
                    pass
                try:
                    if isinstance(primary_data.get('primary_metrics_provisional'), dict) and isinstance(primary_data.get('metric_schema_frozen'), dict):
                        primary_data['primary_metrics_provisional'] = rekey_metrics_via_identity_resolver_v1(
                            primary_data.get('primary_metrics_provisional') or {},
                            primary_data.get('metric_schema_frozen') or {},
                        )
                except Exception:
                    pass

                # =========================================================

                # =========================================================
                # PATCH FIX2D61 (ADDITIVE): feed provisional into canonical before schema-only enforcement
                # - After schema promotion + rekey, merge provisional into canonical so bound rows can be retained.
                # =========================================================
                try:
                    _prov = primary_data.get("primary_metrics_provisional")
                    if isinstance(_prov, dict) and _prov:
                        _can = primary_data.get("primary_metrics_canonical")
                        if not isinstance(_can, dict):
                            _can = {}
                        for _k, _v in _prov.items():
                            _can[_k] = _v
                        primary_data["primary_metrics_canonical"] = _can
                        primary_data["primary_metrics_provisional"] = {}
                except Exception:
                    pass
                # END PATCH FIX2D61 MERGE

                # PATCH FIX2D60 (ADDITIVE): schema-only canonical enforcement
                # - After rekeying, keep ONLY schema-bound keys in primary_metrics_canonical.
                # - Move everything else into primary_metrics_provisional (quarantined for audit).
                # =========================================================
                try:
                    _pmc_bound, _pmc_not_bound = _fix2d60_split_schema_bound_only(primary_data.get('primary_metrics_canonical') or {})
                    if isinstance(_pmc_bound, dict):
                        primary_data['primary_metrics_canonical'] = _pmc_bound
                    if isinstance(_pmc_not_bound, dict) and _pmc_not_bound:
                        _prov = primary_data.get('primary_metrics_provisional')
                        if not isinstance(_prov, dict):
                            _prov = {}
                        # merge (schema-bound rule is stronger than any earlier provisional split)
                        for _k, _v in _pmc_not_bound.items():
                            _prov[_k] = _v
                        primary_data['primary_metrics_provisional'] = _prov
                except Exception:
                    pass

                # 3) attribution using frozen schema ✅
                if primary_data.get("primary_metrics_canonical"):
                    primary_data["primary_metrics_canonical"] = add_range_and_source_attribution_to_canonical_metrics(
                        primary_data.get("primary_metrics_canonical", {}),
                        web_context,
                        metric_schema=(primary_data.get("metric_schema_frozen") or {}),
                    )

                # PATCH SV1/EG1 (ADDITIVE): validate frozen schema + enforce evidence gating (analysis-side)
                try:
                    fn = globals().get("apply_schema_validation_and_evidence_gating")
                    if callable(fn):
                        primary_data = fn(primary_data)
                except Exception:
                    pass

            except Exception:
                pass

            # Hash key findings (optional)
            try:
                if primary_data.get("key_findings"):
                    findings_with_hash = []
                    for finding in primary_data.get("key_findings", []):
                        if finding:
                            findings_with_hash.append({
                                "text": finding,
                                "semantic_hash": compute_semantic_hash(finding)
                            })
                    primary_data["key_findings_hashed"] = findings_with_hash
            except Exception:
                pass


            # Save baseline numeric cache if available (existing behavior)

            # Build output
            output = {
                "question": query,
                "question_profile": question_profile,
                "question_category": question_profile.get("category"),
                "question_signals": question_signals,
                "side_questions": question_profile.get("side_questions", []),
                "timestamp": _yureeka_now_iso_utc(),
                "primary_response": primary_data,
                "final_confidence": final_conf,
                "veracity_scores": veracity_scores,
                "web_sources": web_context.get("sources", []),
                "code_version": _yureeka_get_code_version(),
            "authority_manifest_v1": _yureeka_authority_manifest_v1(),
                }


            # REFACTOR31 (ADDITIVE): runtime identity stamp for diagnosing stale-version runs
            try:
                output.setdefault("debug", {})
                if isinstance(output.get("debug"), dict):
                    output["debug"].setdefault("runtime_identity_v1", _yureeka_runtime_identity_v1())
            except Exception:
                pass


            try:
                if isinstance(output.get("primary_response"), dict):
                    output["primary_response"]["code_version"] = _yureeka_get_code_version()
            except Exception:
                pass


            # ✅ NEW: attach analysis-aligned snapshots (from scraped_meta)
            # This is the stable cache evolution should reuse.
            try:
                output = attach_source_snapshots_to_analysis(output, web_context)
            except Exception:
                pass

            # =====================================================================
            # PATCH FIX2D72 (REQUIRED): Materialize & persist schema-keyed baseline canonical metrics
            # Why:
            # - Evolution diffing requires previous_data.primary_metrics_canonical to exist and be schema-keyed.
            # - Some analysis paths seed metric_schema_frozen but do not emit primary_metrics_canonical into the
            #   persisted payload. This causes Evolution to report no_prev_metrics even when keys are stable.
            # What:
            # - If primary_metrics_canonical is missing/empty, rebuild it deterministically from the frozen schema
            #   using the same authoritative rebuild helper used by Evolution.
            # - Write into BOTH top-level and results for maximum persistence compatibility.
            # =====================================================================
            try:
                _pmc0 = output.get('primary_metrics_canonical')
                _pmc0_empty = (not isinstance(_pmc0, dict)) or (not _pmc0)
                if _pmc0_empty:
                    fn_rebuild = globals().get('rebuild_metrics_from_snapshots_analysis_canonical_v1')
                    if callable(fn_rebuild):
                        _rebuilt = fn_rebuild(output.get('primary_response') or {}, output.get('baseline_sources_cache') or [], web_context=web_context)
                        if isinstance(_rebuilt, dict) and _rebuilt:
                            output['primary_metrics_canonical'] = _rebuilt
                            output.setdefault('results', {})
                            if isinstance(output.get('results'), dict):
                                output['results']['primary_metrics_canonical'] = _rebuilt
                            # lightweight debug stamp
                            try:
                                output.setdefault('debug', {})
                                if isinstance(output.get('debug'), dict):
                                    output['debug']['fix2d72_analysis_pmc_written'] = True
                                    output['debug']['fix2d72_pmc_key_count'] = len(_rebuilt)
                            except Exception:
                                pass
            except Exception:
                pass
            # =====================================================================

            with st.spinner("💾 Saving to history..."):
                if add_to_history(output):
                    st.success("✅ Analysis saved to Google Sheets")
                else:
                    st.warning("⚠️ Saved to session only (Google Sheets unavailable)")

            json_bytes = json.dumps(output, indent=2, ensure_ascii=False).encode("utf-8")
            filename = f"yureeka_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

            st.download_button(
                label="💾 Download Analysis JSON",
                data=json_bytes,
                file_name=filename,
                mime="application/json"
            )

            render_dashboard(
            primary_data,
            final_conf,
            web_context,
            base_conf,
            query,
            veracity_scores,
            web_context.get("source_reliability", [])
            )


            with st.expander("🔧 Debug Information"):
                st.write("**Confidence Breakdown:**")
                st.json({
                    "base_confidence": base_conf,
                    "evidence_score": veracity_scores.get("overall", 0),
                    "final_confidence": final_conf,
                    "veracity_breakdown": veracity_scores
                })
                st.write("**Primary Model Response:**")
                st.json(primary_data)

    # =====================
    # TAB 2: EVOLUTION ANALYSIS
    # =====================
    with tab2:
        st.markdown("""
        ### 📈 Track the evolution of key metrics over time using **deterministic source-anchored analysis**.

        **How it works:**
        - Select a baseline from your history (stored in Google Sheets)
        - Re-fetches the **exact same sources** from that analysis
        - Extracts current numbers using regex (no LLM variance)
        - Computes deterministic diffs with context-aware matching
        """)

        with st.sidebar:
            st.subheader("📚 History")

            if st.button("🔄 Refresh"):
                st.cache_resource.clear()
                st.rerun()

            sheet = get_google_sheet()
            if sheet:
                st.success("✅ Google Sheets connected")
            else:
                st.warning("⚠️ Using session storage")

            # =====================================================================
            # PATCH FIX40 (ADDITIVE): Scenario B control — Force rebuild toggle
            # - Streamlit Cloud UI has no free-text question editing (dropdown-only).
            # - This toggle lets you intentionally bypass the unchanged fastpath so you
            #   can validate the rebuild path + FIX39 publish invariants.
            # - Pure UI flag; no logic changes unless explicitly enabled.
            # =====================================================================
            force_rebuild = st.checkbox(
                "🧪 Force rebuild (ignore snapshot fastpath)",
                value=False,
                key="fix41_force_rebuild_toggle",
                help="Debug only: forces evolution to rebuild even if sources+data are unchanged."
            )
            # =====================================================================

        # ✅ FIX: your codebase uses get_history(), not load_history()
        history = get_history()

        if not history:
            st.info("📭 No previous analyses found. Run an analysis in the 'New Analysis' tab first.")
            return

        baseline_options = [
            f"{i+1}. {h.get('question', 'N/A')}  ({h.get('timestamp', '')})"
            for i, h in enumerate(history)
        ]
        baseline_choice = st.selectbox("Select baseline analysis:", baseline_options)
        baseline_idx = int(baseline_choice.split(".")[0]) - 1
        baseline_data = history[baseline_idx]

        compare_method = st.selectbox(
            "Comparison method:",
            [
                "source-anchored evolution (re-fetch same sources)",
                "another saved analysis (deterministic)",
                "fresh analysis (volatile)"
            ]
        )

        # ============================================================
        # PATCH UI_EXTRA_SOURCES1 (ADDITIVE)
        # ============================================================
        extra_sources_text = st.text_area(
            "Extra source URLs (optional, one per line)",
            placeholder="https://example.com/report\nhttps://another-source.com/page",
            help="Adds these URLs to the admitted source list for this run. Useful to test hash-mismatch rebuilds.",
            height=110,
        )

        compare_data = None
        if "another saved analysis" in compare_method:
            compare_options = [
                f"{i+1}. {h.get('question', 'N/A')}  ({h.get('timestamp', '')})"
                for i, h in enumerate(history) if i != baseline_idx
            ]
            if compare_options:
                compare_choice = st.selectbox("Select comparison analysis:", compare_options)
                compare_idx = int(compare_choice.split(".")[0]) - 1
                compare_data = history[compare_idx]
            else:
                st.warning("No other saved analyses to compare with.")

        st.markdown("---")

        if st.button("🧬 Run Evolution Analysis", type="primary"):

            if "source-anchored evolution" in compare_method:
                evolution_query = baseline_data.get("question", "")
                if not evolution_query:
                    st.error("❌ No question found in baseline.")
                    return

                with st.spinner("🧬 Running source-anchored evolution..."):

                    try:


                        # ============================================================

                        # PATCH INJ_DIAG_EVO_UI (ADDITIVE): pass extra injected URLs + run_id into evolution

                        # ============================================================

                        _evo_run_id = _inj_diag_make_run_id("evo")

                        _extra_urls_evo = []

                        try:

                            for _l in str(extra_sources_text or "").splitlines():

                                _u = _l.strip()

                                if not _u:

                                    continue

                                if _u.startswith("http://") or _u.startswith("https://"):

                                    _extra_urls_evo.append(_u)

                        except Exception:
                            pass

                            _extra_urls_evo = []


                        results = run_source_anchored_evolution(

                            baseline_data,

                            web_context={

                                "force_rebuild": bool(force_rebuild),

                                "extra_urls": _extra_urls_evo,

                                "diag_run_id": _evo_run_id,

                                "diag_extra_urls_ui_raw": (extra_sources_text or ""),

                            },

                        )

                        # ============================================================


                    except Exception as e:

                        st.error(f"❌ Evolution failed: {e}")

                        return


                interpretation = ""
                try:
                    if results and isinstance(results, dict):
                        interpretation = results.get("interpretation", "") or ""
                except Exception:
                    pass
                    interpretation = ""



                # =====================================================================
                # REFACTOR25: Analysis→Evolution timing delta (production only)
                # - Standardize timestamps to UTC with offset (+00:00)
                # - Compute/stamp run_timing_v1 in Evolution results
                # - Attach per-row delta fields; blank when current metric is injected-sourced
                # =====================================================================
                _analysis_ts_raw = None
                _analysis_ts_norm = None
                _evo_ts = _yureeka_now_iso_utc()

                try:
                    _analysis_ts_raw = (baseline_data or {}).get("timestamp")
                    _dt_a = _parse_iso_dt(_analysis_ts_raw) if _analysis_ts_raw else None
                    _analysis_ts_norm = _dt_a.isoformat() if _dt_a else (_analysis_ts_raw or None)
                except Exception:
                    _analysis_ts_norm = _analysis_ts_raw or None

                _delta_seconds = None
                _delta_human = ""
                _delta_warnings = []
                try:
                    _dt_a2 = _parse_iso_dt(_analysis_ts_norm) if _analysis_ts_norm else None
                    _dt_e2 = _parse_iso_dt(_evo_ts) if _evo_ts else None
                    if _dt_a2 and _dt_e2:
                        _ds = (_dt_e2 - _dt_a2).total_seconds()
                        if _ds < 0:
                            _delta_warnings.append("delta_negative_clamped_to_zero")
                            _ds = 0.0
                        _delta_seconds = float(_ds)
                        _delta_human = _yureeka_humanize_seconds_v1(_delta_seconds)
                    else:
                        _delta_warnings.append("delta_uncomputed_missing_timestamp")
                except Exception:
                    _delta_warnings.append("delta_uncomputed_exception")

                # Attach run timing to Evolution results (debug + non-debug copy)
                try:
                    if isinstance(results, dict):
                        _dbg = results.get("debug")
                        if not isinstance(_dbg, dict):
                            _dbg = {}
                            results["debug"] = _dbg
                        _dbg["run_timing_v1"] = {
                            "analysis_timestamp": _analysis_ts_norm,
                            "evolution_timestamp": _evo_ts,
                            "delta_seconds": _delta_seconds,
                            "delta_human": _delta_human,
                            "warnings": list(_delta_warnings),
                        }
                        results["run_delta_seconds"] = _delta_seconds
                        results["run_delta_human"] = _delta_human
                        if isinstance(results.get("results"), dict):
                            results["results"]["run_delta_seconds"] = _delta_seconds
                            results["results"]["run_delta_human"] = _delta_human
                            _dbg2 = results["results"].get("debug")
                            if not isinstance(_dbg2, dict):
                                _dbg2 = {}
                                results["results"]["debug"] = _dbg2
                            _dbg2["run_timing_v1"] = dict(_dbg["run_timing_v1"])
                except Exception:
                    pass

                # Add per-row delta fields (production only; blank if injected)
                try:
                    _inj_urls = []
                    if isinstance(results, dict):
                        _dbg3 = results.get("debug") if isinstance(results.get("debug"), dict) else {}
                        _inj_urls = _dbg3.get("fix2d65b_injected_urls") or []
                        if not isinstance(_inj_urls, list):
                            _inj_urls = []

                    _inj_norm = None
                    try:
                        _inj_norm = _inj_diag_norm_url_list(_inj_urls)
                    except Exception:
                        _inj_norm = None

                    _inj_set = set([str(u).strip() for u in (_inj_norm or _inj_urls) if isinstance(u, str) and u.strip()])

                    def _refactor25_extract_metric_source_url(_m: dict):
                        try:
                            return _refactor26_extract_metric_source_url_v1(_m)
                        except Exception:
                            return None

                    _pmc = {}
                    if isinstance(results, dict):
                        _pmc = results.get("primary_metrics_canonical") or {}
                        if (not isinstance(_pmc, dict)) and isinstance(results.get("results"), dict):
                            _pmc = results["results"].get("primary_metrics_canonical") or {}
                    if not isinstance(_pmc, dict):
                        _pmc = {}

                                        # REFACTOR29: collect per-row gating stats for injection/production delta display
                    _row_delta_gating = {
                        "inj_set_size": len(_inj_set),
                        "rows_total": 0,
                        "injected_rows_total": 0,
                        "injected_rows_blank_delta": 0,
                        "production_rows_total": 0,
                        "production_rows_with_delta": 0,
                        "rows_with_source_url": 0,
                        "rows_missing_source_url": 0,
                        "rows_suppressed_by_injection": 0,
                        "unattributed_rows": 0,
                        "duplicate_rows_skipped": 0,
                    }

                    _seen_ck = set()

                    def _apply_delta_to_rows(_rows: list):
                        if not isinstance(_rows, list):
                            return
                        for _r in _rows:
                            if not isinstance(_r, dict):
                                continue

                            _ckey_for_count = None
                            try:
                                _ckey_for_count = _r.get("canonical_key")
                            except Exception:
                                _ckey_for_count = None

                            _uniq_key = _ckey_for_count if (isinstance(_ckey_for_count, str) and _ckey_for_count) else ("__row_%s" % (id(_r),))
                            _count_row = True
                            try:
                                if _uniq_key in _seen_ck:
                                    _count_row = False
                                    try:
                                        _row_delta_gating["duplicate_rows_skipped"] += 1
                                    except Exception:
                                        pass
                                else:
                                    _seen_ck.add(_uniq_key)
                            except Exception:
                                pass

                            if _count_row:
                                try:
                                    _row_delta_gating["rows_total"] += 1
                                except Exception:
                                    pass

                            is_injected = False
                            if _inj_set:
                                _ckey = _r.get("canonical_key")
                                # REFACTOR26: prefer row-attributed current URL when available
                                _su = None
                                try:
                                    _su = _refactor26_extract_row_current_source_url_v1(_r)
                                except Exception:
                                    _su = None
                                if _su is None:
                                    _cm = _pmc.get(_ckey) if isinstance(_ckey, str) else None
                                    _su = _refactor25_extract_metric_source_url(_cm) if isinstance(_cm, dict) else None
                                if _su is None:
                                    # REFACTOR53: cannot attribute -> treat as production (do not suppress)
                                    is_injected = False
                                    if _count_row:
                                        try:
                                            _row_delta_gating["unattributed_rows"] += 1
                                            _row_delta_gating["rows_missing_source_url"] += 1
                                        except Exception:
                                            pass
                                else:
                                    if _count_row:
                                        try:
                                            _row_delta_gating["rows_with_source_url"] += 1
                                        except Exception:
                                            pass
                                    _su_norm = _su
                                    try:
                                        _tmp = _inj_diag_norm_url_list([_su])
                                        if isinstance(_tmp, list) and _tmp:
                                            _su_norm = str(_tmp[0] or _su).strip()
                                    except Exception:
                                        _su_norm = _su
                                    is_injected = (_su_norm in _inj_set)

                            if _count_row:
                                try:
                                    if is_injected:
                                        _row_delta_gating["injected_rows_total"] += 1
                                        _row_delta_gating["rows_suppressed_by_injection"] += 1
                                    else:
                                        _row_delta_gating["production_rows_total"] += 1
                                except Exception:
                                    pass

                            if (not is_injected) and (_delta_human or _delta_seconds is not None):
                                _r["analysis_evolution_delta_human"] = _delta_human
                                _r["analysis_evolution_delta_seconds"] = _delta_seconds
                                try:
                                    _row_delta_gating["production_rows_with_delta"] += 1
                                except Exception:
                                    pass
                            else:
                                _r["analysis_evolution_delta_human"] = ""
                                _r["analysis_evolution_delta_seconds"] = None
                                if is_injected and _count_row:
                                    try:
                                        _row_delta_gating["injected_rows_blank_delta"] += 1
                                    except Exception:
                                        pass

                    # REFACTOR30: avoid double-counting by applying delta stamping once, then propagating
                    _rows_a = results.get("metric_changes")
                    _rows_b = results.get("metric_changes_v2")
                    _primary_rows = _rows_a if isinstance(_rows_a, list) and _rows_a else (_rows_b if isinstance(_rows_b, list) else None)

                    # If metric_changes is missing/empty but v2 exists, alias metric_changes to v2 (UI reads metric_changes).
                    try:
                        if isinstance(results, dict) and isinstance(_primary_rows, list):
                            if (not isinstance(_rows_a, list)) or (len(_rows_a) == 0):
                                results["metric_changes"] = _primary_rows
                            if (not isinstance(_rows_b, list)) or (len(_rows_b) == 0):
                                results["metric_changes_v2"] = _primary_rows
                    except Exception:
                        pass

                    if isinstance(_primary_rows, list):
                        _apply_delta_to_rows(_primary_rows)

                    # Propagate delta fields to the other list (best-effort), keyed by canonical_key
                    try:
                        _rows_a2 = results.get("metric_changes")
                        _rows_b2 = results.get("metric_changes_v2")
                        if isinstance(_rows_a2, list) and isinstance(_rows_b2, list) and (_rows_a2 is not _rows_b2) and isinstance(_primary_rows, list):
                            _map = {}
                            for _pr in _primary_rows:
                                if isinstance(_pr, dict):
                                    _ck = _pr.get("canonical_key")
                                    if isinstance(_ck, str) and _ck and _ck not in _map:
                                        _map[_ck] = _pr
                            _secondary = _rows_b2 if _primary_rows is _rows_a2 else _rows_a2
                            for _sr in _secondary:
                                if not isinstance(_sr, dict):
                                    continue
                                _ck2 = _sr.get("canonical_key")
                                _src = _map.get(_ck2) if isinstance(_ck2, str) else None
                                if isinstance(_src, dict):
                                    _sr["analysis_evolution_delta_human"] = _src.get("analysis_evolution_delta_human") or ""
                                    _sr["analysis_evolution_delta_seconds"] = _src.get("analysis_evolution_delta_seconds")
                                else:
                                    _sr.setdefault("analysis_evolution_delta_human", "")
                                    _sr.setdefault("analysis_evolution_delta_seconds", None)
                    except Exception:
                        pass


                    # Harness / invariants (soft assertions + diagnostics)
                    try:
                        if isinstance(results, dict) and isinstance(results.get("debug"), dict):
                            rt = results["debug"].get("run_timing_v1")
                            if isinstance(rt, dict):
                                rt.setdefault("assertions", {})
                                rt["row_delta_gating_v1"] = dict(_row_delta_gating)
                                if isinstance(rt.get("assertions"), dict):
                                    if (not _inj_set) and _analysis_ts_norm:
                                        rt["assertions"]["delta_computed_non_negative"] = bool((_delta_seconds is not None) and (float(_delta_seconds) >= 0))
                                    if _inj_set:
                                        rt["assertions"]["injected_rows_have_blank_delta"] = bool(
                                            _row_delta_gating.get("injected_rows_blank_delta", 0) == _row_delta_gating.get("injected_rows_total", 0)
                                        )
                                    if (_delta_seconds is not None) and (_row_delta_gating.get("production_rows_total", 0) > 0):
                                        rt["assertions"]["production_rows_have_delta"] = bool(
                                            _row_delta_gating.get("production_rows_with_delta", 0) == _row_delta_gating.get("production_rows_total", 0)
                                        )

                        # Keep nested results copy aligned (best-effort)
                        if isinstance(results, dict) and isinstance(results.get("results"), dict):
                            _dbg_nested = results["results"].get("debug")
                            if not isinstance(_dbg_nested, dict):
                                _dbg_nested = {}
                                results["results"]["debug"] = _dbg_nested
                            if isinstance(results.get("debug"), dict) and isinstance(results["debug"].get("run_timing_v1"), dict):
                                _dbg_nested["run_timing_v1"] = dict(results["debug"]["run_timing_v1"])
                    except Exception:
                        pass
                except Exception:
                    pass
                evolution_output = {
                    "question": evolution_query,
                    "timestamp": _evo_ts,
                    "analysis_type": "source_anchored",
                    "previous_timestamp": _analysis_ts_norm,
                    "results": results,
                    "interpretation": {
                        "text": interpretation,
                        "authoritative": False,
                        "source": "llm_optional"
                    }
                }

                st.download_button(
                    label="💾 Download Evolution Report",
                    data=json.dumps(evolution_output, indent=2, ensure_ascii=False).encode("utf-8"),
                    file_name=f"yureeka_evolution_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json"
                )

                # ✅ FIX: guarded renderer to avoid stability_score=None formatting crashes
                render_source_anchored_results(results, evolution_query)

            elif "another saved analysis" in compare_method:
                if compare_data:
                    st.success("✅ Comparing two saved analyses (deterministic)")
                    render_native_comparison(baseline_data, compare_data)
                else:
                    st.error("❌ Please select a comparison analysis")

            else:
                st.warning("⚠️ Running fresh analysis - results may vary")

                query = baseline_data.get("question", "")
                if not query:
                    st.error("❌ No query found")
                    return

                with st.spinner("🌐 Fetching current data..."):
                    # ---- ADDITIVE: pass existing snapshots for reuse (Change #3 wiring) ----
                    existing_snapshots = None

                    try:
                        prev = st.session_state.get("last_analysis")
                        if isinstance(prev, dict):
                            existing_snapshots = (
                                prev.get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("baseline_sources_cache")
                                or (prev.get("results", {}) or {}).get("source_results")
                            )
                    except Exception:
                        pass
                        existing_snapshots = None

                    web_context = fetch_web_context(
                        query,
                        num_sources=3,
                        existing_snapshots=existing_snapshots,
                    )
                    # ----------------------------------------------------------------------


                if not web_context:
                    web_context = {
                        "search_results": [],
                        "scraped_content": {},
                        "summary": "",
                        "sources": [],
                        "source_reliability": []
                    }

                with st.spinner("🤖 Running analysis..."):
                    new_response = query_perplexity(query, web_context)

                if new_response:
                    try:
                        new_parsed = json.loads(new_response)
                        veracity = evidence_based_veracity(new_parsed, web_context)
                        base_conf = float(new_parsed.get("confidence", 75))
                        final_conf = calculate_final_confidence(base_conf, veracity.get("overall", 0))

                        compare_data = {
                            "question": query,
                            "timestamp": _yureeka_now_iso_utc(),
                            "primary_response": new_parsed,
                            "final_confidence": final_conf,
                            "veracity_scores": veracity,
                            "web_sources": web_context.get("sources", [])
                        }

                        add_to_history(compare_data)
                        st.success("✅ Saved to history")

                        render_native_comparison(baseline_data, compare_data)
                    except Exception as e:
                        st.error(f"❌ Failed: {e}")
                else:
                    st.error("❌ Analysis failed")


# ======================================================================
# PATCH SV1/EG1 (ADDITIVE): Schema validation + Evidence gating (analysis)
# - Additive only: does not remove or refactor existing code.
# - Only applied in TAB 1 (New Analysis) via a small post-pass hook.
# - Does NOT alter evolution behavior (no changes to evolution functions).
# ======================================================================

def validate_metric_schema_frozen(metric_schema_frozen: dict) -> dict:
    """
    Validate frozen metric schema for internal consistency.
    Returns: {"ok": bool, "errors": [...], "warnings": [...], "by_key": {...}}
    """
    issues = {"ok": True, "errors": [], "warnings": [], "by_key": {}}

    def _add(kind: str, canonical_key: str, msg: str):
        issues["ok"] = issues["ok"] and (kind != "errors")
        issues[kind].append({"canonical_key": canonical_key, "message": msg})
        issues["by_key"].setdefault(canonical_key, {"errors": [], "warnings": []})
        issues["by_key"][canonical_key][kind].append(msg)

    if not isinstance(metric_schema_frozen, dict):
        _add("errors", "__schema__", "metric_schema_frozen missing or not a dict")
        return issues

    for canonical_key, spec in metric_schema_frozen.items():
        if not isinstance(spec, dict):
            _add("errors", canonical_key, "schema entry not a dict")
            continue

        dim = (spec.get("dimension") or spec.get("measure_kind") or "").lower().strip()
        unit = (spec.get("unit") or spec.get("unit_tag") or "").strip()
        unit_family = (spec.get("unit_family") or spec.get("unit_family_tag") or "").lower().strip()
        name = (spec.get("name") or canonical_key or "").lower()

        # Hard conflict: currency + percent
        if dim in ("currency", "revenue", "market_value", "value") and unit in ("%", "percent", "percentage"):
            _add("errors", canonical_key, "dimension=currency but unit is percent (%)")

        # Soft checks for percent metrics without percent unit
        if ("cagr" in name or dim in ("percent", "percentage", "growth_rate")) and unit and unit not in ("%", "percent", "percentage"):
            _add("warnings", canonical_key, f"percent-like metric but unit='{unit}' (expected '%')")

        # Common drift hazard: CAGR schema includes 'share'
        kw = " ".join([str(x) for x in (spec.get("keywords") or [])]).lower()
        if "cagr" in name and "share" in kw:
            _add("warnings", canonical_key, "CAGR schema keywords include 'share' (risk of mapping share% to CAGR)")

        # Unit family conflicts
        if dim == "currency" and unit_family and unit_family not in ("currency", "money"):
            _add("warnings", canonical_key, f"dimension=currency but unit_family='{unit_family}'")

    return issues


def _metric_evidence_list(metric: dict):
    ev = metric.get("evidence")
    if isinstance(ev, list):
        return ev
    return []


def _synthesize_evidence_from_examples(metric: dict, max_items: int = 5) -> list:
    """
    If metric has value_range.examples (from attribution pass), synthesize evidence records.
    This keeps JSON stable and makes evolution rebuild auditing possible.
    """
    examples = None
    vr = metric.get("value_range")
    if isinstance(vr, dict):
        examples = vr.get("examples")
    if not isinstance(examples, list) or not examples:
        return []

    # Try to use an existing anchor hash function if present
    anchor_fn = globals().get("compute_anchor_hash")
    out = []
    for ex in examples[:max_items]:
        if not isinstance(ex, dict):
            continue
        url = ex.get("source_url") or ex.get("url") or ""
        raw = ex.get("raw") or ""
        ctx = ex.get("context") or ex.get("context_window") or ex.get("snippet") or ""
        ah = ex.get("anchor_hash") or ""
        if not ah and callable(anchor_fn):
            try:
                ah = anchor_fn(url, ctx)
            except Exception:
                pass
                ah = ""
        out.append({
            "source_url": url,
            "raw": raw,
            "context_snippet": ctx[:500] if isinstance(ctx, str) else "",
            "anchor_hash": ah,
            "method": "value_range_examples",
        })
    return out


def ensure_metric_has_evidence(metric: dict) -> dict:
    """
    Evidence gating for a single metric:
    - If evidence already exists -> no change
    - Else synthesize from value_range.examples if available
    - Else mark as proxy (do not delete or zero the metric)
    """
    if not isinstance(metric, dict):
        return metric

    ev = _metric_evidence_list(metric)
    if ev:
        return metric

    synth = _synthesize_evidence_from_examples(metric)
    if synth:
        metric["evidence"] = synth
        return metric

    # No evidence at all: mark proxy (do not alter numeric payload)
    metric.setdefault("evidence", [])
    metric["is_proxy"] = True
    metric["proxy_type"] = "evidence_missing"
    metric["proxy_reason"] = "no_evidence_anchors_available"
    metric["proxy_confidence"] = float(metric.get("proxy_confidence") or 0.2)
    return metric


def enforce_evidence_gating(primary_metrics_canonical: dict) -> dict:
    """
    Apply evidence gating across canonical metrics.
    Returns the (mutated) dict for compatibility.
    """
    if not isinstance(primary_metrics_canonical, dict):
        return primary_metrics_canonical

    for k, m in list(primary_metrics_canonical.items()):
        if isinstance(m, dict):
            primary_metrics_canonical[k] = ensure_metric_has_evidence(m)

    return primary_metrics_canonical


def apply_schema_validation_and_evidence_gating(primary_data: dict) -> dict:
    """
    New Analysis post-pass hook:
    - validates metric_schema_frozen
    - evidence-gates primary_metrics_canonical
    - marks schema-conflict metrics as proxy (does not remove anything)
    """
    if not isinstance(primary_data, dict):
        return primary_data

    # Where schema is stored
    schema = (
        primary_data.get("metric_schema_frozen")
        or (primary_data.get("primary_response") or {}).get("metric_schema_frozen")
        or (primary_data.get("results") or {}).get("metric_schema_frozen")
        or {}
    )

    validation = validate_metric_schema_frozen(schema)
    primary_response = primary_data.setdefault("primary_response", {})
    primary_response["schema_validation"] = validation

    # Mark schema-conflict metrics as proxy (additive)
    pmc = primary_data.get("primary_metrics_canonical")
    if isinstance(pmc, dict) and validation.get("by_key"):
        for ck, iss in validation["by_key"].items():
            if ck in pmc and isinstance(pmc[ck], dict) and iss.get("errors"):
                pmc[ck]["is_proxy"] = True
                pmc[ck]["proxy_type"] = "schema_conflict"
                pmc[ck]["proxy_reason"] = "schema_validation_error"
                pmc[ck]["proxy_confidence"] = float(pmc[ck].get("proxy_confidence") or 0.15)
                pmc[ck]["schema_issues"] = {"errors": iss.get("errors", []), "warnings": iss.get("warnings", [])}

    # Evidence gating
    pmc2 = primary_data.get("primary_metrics_canonical")
    if isinstance(pmc2, dict):
        before = sum(1 for v in pmc2.values() if isinstance(v, dict) and _metric_evidence_list(v))
        enforce_evidence_gating(pmc2)
        after = sum(1 for v in pmc2.values() if isinstance(v, dict) and _metric_evidence_list(v))
        prox = sum(1 for v in pmc2.values() if isinstance(v, dict) and v.get("is_proxy"))
        primary_response["evidence_gating_summary"] = {
            "total_metrics": len(pmc2),
            "metrics_with_evidence_before": before,
            "metrics_with_evidence_after": after,
            "metrics_marked_proxy": prox,
        }

    return primary_data



# ===================== PATCH RMS_UNWRAP1 (ADDITIVE) =====================
def _normalize_prev_response_for_rebuild(previous_data):
    """Best-effort normalization of the loaded baseline object for rebuild dispatch.
    - If previous_data is a JSON string, parse it.
    - If it contains nested 'primary_response' as JSON string, parse it.
    - If it contains a top-level wrapper with 'data' or 'results', keep as dict.
    This is additive and only affects rebuild dispatch input normalization.
    """
    import json
    try:
        pd = previous_data
        if isinstance(pd, str) and pd.strip().startswith(("{","[")):
            try:
                pd = json.loads(pd)
            except Exception:
                pass
                pd = previous_data
        if isinstance(pd, dict):
            pr = pd.get("primary_response")
            if isinstance(pr, str) and pr.strip().startswith(("{","[")):
                try:
                    pd["primary_response"] = json.loads(pr)
                except Exception:
                    pass
            # Some callers store the main payload under 'data'
            d = pd.get("data")
            if isinstance(d, str) and d.strip().startswith(("{","[")):
                try:
                    pd["data"] = json.loads(d)
                except Exception:
                    return pd
    except Exception:
        return previous_data
# =================== END PATCH RMS_UNWRAP1 (ADDITIVE) ===================


if __name__ == "__main__":
    # REFACTOR35: main() invocation moved to end-of-file so all late refactor defs/overrides are loaded before any runs.
    pass


# ===================== PATCH RMS_DISPATCH2 (ADDITIVE) =====================
def _get_metric_anchors_any(prev_response: dict) -> dict:
    """Best-effort retrieval of metric_anchors from any plausible location (additive helper)."""
    try:
        if not isinstance(prev_response, dict):
            return {}
        for path in (
            ("metric_anchors",),
            ("results", "metric_anchors"),
            ("primary_response", "metric_anchors"),
            ("primary_response", "results", "metric_anchors"),
        ):
            cur = prev_response
            ok = True
            for k in path:
                if isinstance(cur, dict) and k in cur:
                    cur = cur[k]
                else:
                    ok = False
                    break
            if ok and isinstance(cur, dict) and cur:
                return cur
        return {}
    except Exception:
        return {}

def _coerce_prev_response_any(previous_data):
    """Normalize previous_data into a dict-shaped 'prev_response' for rebuild dispatch (additive helper)."""
    try:
        return previous_data if isinstance(previous_data, dict) else {}
    except Exception:
        return {}
# =================== END PATCH RMS_DISPATCH2 (ADDITIVE) ===================

# =====================================================================
# PATCH FIX16 (ADDITIVE): close analysis↔evolution metric lock-down gaps
# Goals (deterministic, no re-architecture):
#   1) De-year schema keyword scoring for non-year metrics
#   2) Hard unit expectation gating (unitless years can't win currency/percent)
#   3) Absolute anchor priority when anchors exist
# Notes:
#   - Additive only: we define FIX16 rebuild functions and re-wire dispatch
#   - No refetch, no heuristics beyond schema/unit/anchors
# =====================================================================

def _fix16_is_year_token(s: str) -> bool:
    try:
        s2 = str(s or "").strip()
        return bool(re.fullmatch(r"(19\d{2}|20\d{2})", s2))
    except Exception:
        return False


def _fix16_metric_is_year_like(metric_spec: dict, canonical_key: str = "") -> bool:
    """Deterministic allow-list for metrics whose value is genuinely a year."""
    try:
        spec = metric_spec or {}
        blob = " ".join([
            str(canonical_key or ""),
            str(spec.get("name") or ""),
            str(spec.get("canonical_key") or spec.get("canonical_id") or ""),
            " ".join([str(x) for x in (spec.get("keywords") or spec.get("keyword_hints") or []) if x]),
            str(spec.get("dimension") or ""),
        ]).lower()
        # year-ish intents
        return any(k in blob for k in (" year", "year_", "founded", "since", "established", "launch year", "model year"))
    except Exception:
        return False


def _fix16_prune_year_keywords(keywords: list, metric_is_year_like: bool) -> list:
    """Remove YYYY tokens from keyword scoring unless the metric is year-like."""
    try:
        if metric_is_year_like:
            return list(keywords or [])
        out = []
        for k in (keywords or []):
            if _fix16_is_year_token(k):
                continue
            out.append(k)
        return out
    except Exception:
        return list(keywords or [])


def _fix16_expected_dimension(metric_spec: dict) -> str:
    try:
        spec = metric_spec or {}
        dim = (spec.get("dimension") or spec.get("unit_family") or spec.get("expected_unit_family") or "").strip().lower()
        return dim
    except Exception:
        return ""


def _fix16_infer_dimension_from_canonical_key(canonical_key: str) -> str:
    """Infer an expected dimension when schema row is missing dimension/unit_family.
    Keeps REFACTOR02 behavior deterministic and blocks cross-dimension leakage.
    """
    try:
        ck = str(canonical_key or "").strip().lower()
        if not ck:
            return ""
        if "__percent" in ck or ck.endswith("_percent"):
            return "percent"
        if "__currency" in ck or ck.endswith("_currency"):
            return "currency"
        if "__unit_" in ck:
            return "magnitude"
        return ""
    except Exception:
        return ""


def _fix16_candidate_has_any_unit(c: dict) -> bool:
    try:
        if not isinstance(c, dict):
            return False
        for k in ("base_unit", "unit", "unit_tag", "unit_family"):
            if str(c.get(k) or "").strip():
                return True
        # raw sometimes carries $ or % even if unit field blank
        raw = str(c.get("raw") or "")
        if "$" in raw or "%" in raw:
            return True
        return False
    except Exception:
        return False


def _fix16_unit_compatible(c: dict, expected_dim: str) -> bool:
    """Hard gate: if schema expects a unit family/dimension, candidate must be compatible.

    Backward-compatible:
      - Some legacy call sites pass (metric_spec_dict, candidate_dict). In that case we swap.
    """
    try:
        # ---- Back-compat: called as (metric_spec, candidate)
        if isinstance(expected_dim, dict) and isinstance(c, dict):
            spec_like = any(k in c for k in ("dimension", "unit_family", "expected_unit_family", "canonical_key", "name"))
            cand_like = any(k in expected_dim for k in ("raw", "value", "value_norm", "unit", "unit_tag", "unit_family", "base_unit"))
            if spec_like and cand_like:
                _candidate = expected_dim
                expected_dim = _fix16_expected_dimension(c)
                c = _candidate

        if not expected_dim:
            return True
        if not isinstance(c, dict):
            return False

        dim = str(expected_dim).strip().lower()

        # Normalize common synonyms
        if dim in ("magnitude", "count", "quantity", "units", "unit_count", "unit_sales", "number", "numbers", "volume"):
            dim = "magnitude"
        if dim in ("pct", "percentage"):
            dim = "percent"
        if dim in ("money",):
            dim = "currency"

        raw = str(c.get("raw") or "").lower()
        u = (c.get("base_unit") or c.get("unit") or c.get("unit_tag") or "").strip().lower()
        cand_fam = (c.get("unit_family") or "").strip().lower()

        def _has_percent_marker() -> bool:
            try:
                return ("%" in raw) or ("percent" in raw) or ("%" in u) or ("percent" in u)
            except Exception:
                return False

        def _has_currency_marker() -> bool:
            try:
                return (
                    ("$" in raw) or ("us$" in raw) or ("usd" in raw) or ("sgd" in raw) or ("eur" in raw) or ("gbp" in raw)
                    or ("aud" in raw) or ("cny" in raw) or ("jpy" in raw) or ("€" in raw) or ("£" in raw) or ("¥" in raw)
                    or ("$" in u) or ("usd" in u) or ("sgd" in u) or ("eur" in u) or ("gbp" in u) or ("€" in u) or ("£" in u) or ("¥" in u)
                )
            except Exception:
                return False

        # ---- Percent: require explicit percent marker; reject bare year tokens mis-tagged as percent
        if dim == "percent":
            if not _has_percent_marker():
                return False
            try:
                v = c.get("value") if c.get("value") is not None else c.get("value_norm")
                if isinstance(v, (int, float)):
                    iv = int(v)
                    if 1900 <= iv <= 2100 and abs(float(v) - float(iv)) < 1e-9:
                        # if it really is a percent, raw should contain an explicit '%'
                        if "%" not in raw:
                            return False
            except Exception:
                pass
            if cand_fam:
                return cand_fam == "percent"
            return True

        # ---- Currency: require explicit currency marker (raw or unit); block magnitude-only tokens without currency context
        if dim == "currency":
            if not _has_currency_marker():
                return False
            if cand_fam:
                return cand_fam == "currency"
            return True

        # ---- Magnitude / count-like: must NOT look like currency or percent
        if dim == "magnitude":
            if cand_fam in ("currency", "percent", "rate", "ratio"):
                return False
            if _has_currency_marker() or _has_percent_marker():
                return False
            return True

        # ---- Other dims: keep legacy behavior (soft), but enforce unit presence when truly required
        requires_unit = dim in ("rate", "ratio")
        if requires_unit and not _fix16_candidate_has_any_unit(c):
            return False
        if cand_fam and dim in ("rate", "ratio"):
            return cand_fam == dim

        return True
    except Exception:
        return True



def _fix16_candidate_allowed(c: dict, metric_spec: dict, canonical_key: str = "") -> bool:
    """Compose fix15 exclusion + fix16 hard unit gate + year-token guard."""
    try:
        if not isinstance(c, dict):
            return False

        # Respect fix15 junk/year-only exclusion if present
        fn = globals().get("_candidate_disallowed_for_metric")
        if callable(fn):
            if fn(c, dict(metric_spec or {}, canonical_key=canonical_key)):
                return False
        expected_dim = _fix16_expected_dimension(metric_spec)
        if not expected_dim:
            expected_dim = _fix16_infer_dimension_from_canonical_key(canonical_key)
        if not _fix16_unit_compatible(c, expected_dim):
            return False

        # Extra deterministic guard: unitless year-like numbers should never compete
        # for non-year metrics even if upstream tagging missed them.
        if not _fix16_metric_is_year_like(metric_spec, canonical_key=canonical_key):
            # PATCH FIX2B_RANGE_SCHEMA_V1 (ADDITIVE): range uses schema-unit value when available
            v = c.get("value") if c.get("value") is not None else c.get("value_norm")
            u = (c.get("base_unit") or c.get("unit") or "").strip()
            if u == "" and isinstance(v, (int, float)):
                iv = int(v)
                if 1900 <= iv <= 2100:
                    return False

        return True
    except Exception:
        return True


def rebuild_metrics_from_snapshots_with_anchors_fix16(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    FIX16 anchor-aware rebuild:
      - Absolute anchor priority when anchor_hash exists in prev_response.metric_anchors
      - Hard disallow junk/year-like unitless candidates for non-year metrics
      - Hard unit expectation gating for currency/percent/rate/ratio dimensions
    """
    import re

    if not isinstance(prev_response, dict):
        return {}

    metric_anchors = (
        prev_response.get("metric_anchors")
        or (prev_response.get("primary_response") or {}).get("metric_anchors")
        or (prev_response.get("results") or {}).get("metric_anchors")
        or {}
    )
    if not isinstance(metric_anchors, dict) or not metric_anchors:
        return {}

    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )

    # Build deterministic candidate index (anchor_hash -> best candidate)
    fn_idx = globals().get("_es_build_candidate_index_deterministic")
    cand_index = fn_idx(baseline_sources_cache) if callable(fn_idx) else {}

    rebuilt = {}

    for canonical_key, a in (metric_anchors or {}).items():
        if not isinstance(a, dict):
            continue
        ah = a.get("anchor_hash") or a.get("anchor") or ""
        if not ah:
            continue

        spec = (metric_schema.get(canonical_key) if isinstance(metric_schema, dict) else None) or {}
        spec = dict(spec)
        spec.setdefault("name", a.get("name") or canonical_key)
        spec.setdefault("canonical_key", canonical_key)

        c = cand_index.get(ah)
        if not isinstance(c, dict):
            continue

        # FIX16 eligibility hard-gates
        if not _fix16_candidate_allowed(c, spec, canonical_key=canonical_key):
            continue

            # PATCH FIX2D2U: shared semantic gate (local snippet required tokens)
            try:
                _ok_u, _why_u = _fix2d2u_semantic_eligible_global(c, spec, str(canonical_key))
                if not _ok_u:
                    continue
            except Exception:
                pass

        rebuilt[canonical_key] = {
            "canonical_key": canonical_key,
            "name": spec.get("name") or canonical_key,
            "value": c.get("value"),
            "unit": c.get("unit") or "",
            "value_norm": c.get("value_norm"),
            "source_url": c.get("source_url") or "",
            "anchor_hash": c.get("anchor_hash") or ah,
            "evidence": [{
                "source_url": c.get("source_url") or "",
                "raw": c.get("raw") or "",
                "context_snippet": (c.get("context_snippet") or c.get("context") or c.get("context_window") or "")[:400],
                "anchor_hash": c.get("anchor_hash") or ah,
                "method": "anchor_hash_rebuild_fix16",
            }],
            "anchor_used": True,
        }

    return rebuilt



# =====================================================================
# PATCH FIX2S_OBSERVED_TO_CANONICAL_RULES_V1 (ADDITIVE)
# Objective:
# - Deterministically map a selected set of injected/observed extractions into
#   existing Analysis canonical_keys so they participate in diffing.
# - Rules are explicit and auditable: domain + year + exact context substrings,
#   with deterministic unit_tag/measure_kind tagging.
# - Canonical authority remains single-sourced via _analysis_canonical_final_selector_v1
#   (this mapping only influences candidate eligibility for a target canonical_key).
# =====================================================================

def _fix2s_extract_year_from_candidate(c: dict) -> int:
    """Deterministically extract a 4-digit year from candidate fields or text."""
    try:
        if not isinstance(c, dict):
            return 0
        # Preferred explicit fields
        for k in ("year", "year_int", "target_year"):
            v = c.get(k)
            try:
                iv = int(v)
                if 1900 <= iv <= 2100:
                    return iv
            except Exception:
                pass
        # Fallback: strict regex scan of available text fields
        blob = " ".join([
            str(c.get("context_snippet") or ""),
            str(c.get("label") or ""),
            str(c.get("raw") or ""),
            str(c.get("text") or ""),
        ])
        m = re.search(r"\b(19\d{2}|20\d{2})\b", blob)
        if m:
            iv = int(m.group(1))
            if 1900 <= iv <= 2100:
                return iv
    except Exception:
        return 0


def _fix2s_apply_observed_to_canonical_rules_v1(candidates: list, metric_schema: dict, web_context=None) -> dict:
    """
    Apply deterministic mapping rules to candidate dicts.

    Returns a diagnostics dict:
      {
        "mapping_rules_version": str,
        "mapping_hits": [ {anchor_hash, target_key, year, source_url} ... ],
        "observed_rows_canonicalized_by_mapping": int,
        "mapping_misses": [ {anchor_hash, reason, source_url} ... ]   # optional
      }
    """
    diag = {
        "mapping_rules_version": "fix2s_rules_v1",
        "mapping_hits": [],
        "observed_rows_canonicalized_by_mapping": 0,
        "mapping_misses": [],
    }

    if not isinstance(candidates, list) or not candidates:
        return diag
    if not isinstance(metric_schema, dict) or not metric_schema:
        return diag

    # ---- Rule table (selected observed keys -> target canonical keys) ----
    # NOTE: target_key must already exist in Analysis' canonical namespace (schema).
    # We only promote if target_key exists in metric_schema.
    rule_table = [
        # Market share (%)
        {
            "anchor_hash": "f6cb33abc9aff96c8280223ee5f62cfe7be064f5",
            "target_key": "global_2025_market_share",
            "year": 2025,
            "kind": "percent",
            "domain_substr": "ev-volumes.com",
            "context_substr_any": ["market share", "ev share", "%", "share of"],
        },
        {
            "anchor_hash": "eedc7b779f3d87c41943da9d82d04b3f2b9a02e5",
            "target_key": "global_2026_market_share",
            "year": 2026,
            "kind": "percent",
            "domain_substr": "ev-volumes.com",
            "context_substr_any": ["market share", "ev share", "%", "share of"],
        },
        {
            "anchor_hash": "2fa90308784a5827b2e8c63b0a3992c5846e8e22",
            "target_key": "global_2030_market_share",
            "year": 2030,
            "kind": "percent",
            "domain_substr": "ev-volumes.com",
            "context_substr_any": ["market share", "ev share", "%", "share of"],
        },
        # Units sold (unit_sales)
        {
            "anchor_hash": "47da6bcf38afcc47c8b36819b69d88dffc772618",
            "target_key": "global_2024_units_sold",
            "year": 2024,
            "kind": "unit_sales",
            "domain_substr": "ev-volumes.com",
            "context_substr_any": ["ev sales", "sales", "sold", "registrations", "million"],
        },
        {
            "anchor_hash": "5679e35e4abefef7f6a3842414eceed7c3020508",
            "target_key": "global_2025_units_sold",
            "year": 2025,
            "kind": "unit_sales",
            "domain_substr": "ev-volumes.com",
            "context_substr_any": ["ev sales", "sales", "sold", "registrations", "million"],
        },
        {
            "anchor_hash": "37d6d391458630e5bbd4d34564c7490792dfbaf0",
            "target_key": "global_2040_units_sold",
            "year": 2040,
            "kind": "unit_sales",
            "domain_substr": "ev-volumes.com",
            "context_substr_any": ["ev sales", "sales", "sold", "registrations", "million"],
        },
    ]

    rules_by_anchor = {r.get("anchor_hash"): r for r in rule_table if isinstance(r, dict) and r.get("anchor_hash")}

    def _norm(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    for c in candidates:
        if not isinstance(c, dict):
            continue
        ah = str(c.get("anchor_hash") or "")
        if not ah or ah not in rules_by_anchor:
            continue

        r = rules_by_anchor.get(ah) or {}
        tgt = r.get("target_key") or ""
        if not tgt:
            continue

        # Guard: only map into existing schema keys
        if tgt not in metric_schema:
            diag["mapping_misses"].append({"anchor_hash": ah, "reason": "target_not_in_schema", "source_url": c.get("source_url")})
            continue

        url = str(c.get("source_url") or "")
        if r.get("domain_substr") and r["domain_substr"] not in url:
            diag["mapping_misses"].append({"anchor_hash": ah, "reason": "domain_mismatch", "source_url": url})
            continue

        yr = _fix2s_extract_year_from_candidate(c)
        if r.get("year") and yr != int(r["year"]):
            diag["mapping_misses"].append({"anchor_hash": ah, "reason": "year_mismatch", "source_url": url, "year": yr})
            continue

        # Exact substring allowlist (deterministic, no fuzzy matching)
        blob = " ".join([
            str(c.get("context_snippet") or ""),
            str(c.get("label") or ""),
            str(c.get("raw") or ""),
            str(c.get("text") or ""),
        ])
        blob_n = _norm(blob)
        allow = r.get("context_substr_any") or []
        if allow:
            ok = False
            for lit in allow:
                if _norm(str(lit)) and _norm(str(lit)) in blob_n:
                    ok = True
                    break
            if not ok:
                diag["mapping_misses"].append({"anchor_hash": ah, "reason": "context_mismatch", "source_url": url})
                continue

        # Deterministic unit tagging to satisfy schema/unit gates
        kind = r.get("kind")
        if kind == "percent":
            c.setdefault("unit_tag", "percent")
            if not c.get("unit"):
                c["unit"] = "%"
            c.setdefault("measure_kind", "percent")
        elif kind == "unit_sales":
            c.setdefault("unit_tag", "unit_sales")
            if not c.get("unit"):
                c["unit"] = "unit_sales"
            c.setdefault("measure_kind", "unit_sales")

        # Force competition only for this target_key (prevents cross-metric leakage)
        c["fix2s_force_canonical_key"] = tgt

        # Provide canonical key hint (selector remains authority)
        c["canonical_key"] = tgt

        diag["mapping_hits"].append({"anchor_hash": ah, "target_key": tgt, "year": yr, "source_url": url})
        diag["observed_rows_canonicalized_by_mapping"] += 1

    # Attach to web_context for downstream summary merge (additive)
    try:
        if isinstance(web_context, dict):
            web_context["_fix2s_mapping_diag"] = diag
    except Exception:
        return diag

# =====================================================================
# END PATCH FIX2S_OBSERVED_TO_CANONICAL_RULES_V1
# =====================================================================

def rebuild_metrics_from_snapshots_schema_only_fix16(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    FIX16 schema-only rebuild:
      - Removes YYYY tokens from keyword scoring for non-year metrics
      - Hard unit expectation gating
      - Applies fix15 junk/year exclusion + fix16 extra year-token disallow
      - Deterministic selection/tie-breaks
    """
    import re

    if not isinstance(prev_response, dict):
        return {}

    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )
    if not isinstance(metric_schema, dict) or not metric_schema:
        return {}

    # Flatten snapshot candidates (no re-fetch)
    if isinstance(baseline_sources_cache, dict) and isinstance(baseline_sources_cache.get("snapshots"), list):
        sources = baseline_sources_cache.get("snapshots", [])
    elif isinstance(baseline_sources_cache, list):
        sources = baseline_sources_cache
    else:
        sources = []

    # =========================================================
    # PATCH FIX2AA_INJECTED_SNAPSHOT_ADMISSION_V1 (ADDITIVE)
    # Purpose:
    #   The Analysis-parity rebuild consumes a "snapshot pool" shaped like:
    #     [{source_url, extracted_numbers, ...}, ...]
    #   In several evolution runs, injected URLs were fetched + extracted (source_results),
    #   but their extracted_numbers were NOT present in baseline_sources_cache["snapshots"],
    #   so rebuild never saw them.
    #
    # This patch additively admits injected fetched sources into the snapshot pool by:
    #   - reading baseline_sources_cache["source_results"] when present
    #   - filtering strictly to injected URLs (normalized membership)
    #   - appending a snapshot-shaped dict into `sources`
    # Non-negotiables:
    #   - additive only; no behavior change for non-injected sources
    #   - no domain hardcoding; uses diag_injected_urls admitted/ui list
    # =========================================================
    _fix2aa_diag = {
        "enabled": True,
        "source_results_seen": 0,
        "injected_norm_set_size": 0,
        "admitted_to_snapshot_pool": 0,
        "admitted_urls": [],
        "skipped_already_present": 0,
        "skipped_not_injected": 0,
        "skipped_no_extracted_numbers": 0,
        "skipped_bad_shape": 0,
    }
    try:
        if isinstance(web_context, dict) and isinstance(baseline_sources_cache, dict):
            # Build injected normalized set
            _d = web_context.get("diag_injected_urls") or web_context.get("extra_urls_debug") or {}
            _inj_norm = set()
            if isinstance(_d, dict):
                _ad = _inj_diag_norm_url_list(_d.get("admitted") or _d.get("extra_urls_admitted") or [])
                _ui = _inj_diag_norm_url_list(_d.get("ui_norm") or _d.get("extra_urls_ui_norm") or _d.get("extra_urls_normalized") or [])
                _inj_norm = set(_ad or _ui or [])
            _fix2aa_diag["injected_norm_set_size"] = int(len(_inj_norm))

            # Build set of already-present source URLs in snapshot pool (normalized)
            _present = set()
            try:
                for _s in (sources or []):
                    if not isinstance(_s, dict):
                        continue
                    _u0 = _s.get("source_url") or _s.get("url") or ""
                    _present.add((_canonicalize_injected_url(_u0) if callable(globals().get("_canonicalize_injected_url")) else str(_u0 or "")).strip())
            except Exception:
                pass
                _present = set()

            _sr_list = baseline_sources_cache.get("source_results")
            if isinstance(_sr_list, list) and _sr_list:
                _fix2aa_diag["source_results_seen"] = int(len(_sr_list))
                for _sr in _sr_list:
                    if not isinstance(_sr, dict):
                        _fix2aa_diag["skipped_bad_shape"] += 1
                        continue
                    _u = _sr.get("source_url") or _sr.get("url") or ""
                    _un = (_canonicalize_injected_url(_u) if callable(globals().get("_canonicalize_injected_url")) else str(_u or "")).strip()
                    if not _un or (_un not in _inj_norm):
                        _fix2aa_diag["skipped_not_injected"] += 1
                        continue
                    if _un in _present:
                        _fix2aa_diag["skipped_already_present"] += 1
                        continue
                    _xs = _sr.get("extracted_numbers")
                    if not isinstance(_xs, list) or not _xs:
                        _fix2aa_diag["skipped_no_extracted_numbers"] += 1
                        continue

                    # Admit as snapshot-shaped dict
                    sources.append({
                        "source_url": _u,
                        "extracted_numbers": _xs,
                        "fix2aa_admitted_from": "source_results",
                    })
                    _present.add(_un)
                    _fix2aa_diag["admitted_to_snapshot_pool"] += 1
                    if len(_fix2aa_diag["admitted_urls"]) < 50:
                        _fix2aa_diag["admitted_urls"].append(_u)
    except Exception:
        pass
        # diagnostics only; never block rebuild
        pass
    try:
        if isinstance(web_context, dict):
            web_context["fix2aa_injected_snapshot_admission_v1"] = _fix2aa_diag
    except Exception:
        pass
    # END PATCH FIX2AA_INJECTED_SNAPSHOT_ADMISSION_V1

    candidates = []
    for s in sources:
        if not isinstance(s, dict):
            continue
        url = s.get("source_url") or s.get("url") or ""
        xs = s.get("extracted_numbers")
        if isinstance(xs, list) and xs:
            for c in xs:
                if not isinstance(c, dict):
                    continue
                c2 = dict(c)
                c2.setdefault("source_url", url)
                candidates.append(c2)

    # =========================================================
    # PATCH FIX2V_INJECTED_CANDIDATE_BINDING_V1 (ADDITIVE)
    #   - Only binds candidates originating from injected URLs (admitted/ui list)
    #   - Uses exact substring + explicit year guards (no fuzzy matching)
    #   - Adds a per-candidate force key so only the intended schema slot competes
    # =========================================================
    _fix2v_injected_norm_set = set()
    try:
        if isinstance(web_context, dict):
            _d = web_context.get("diag_injected_urls") or web_context.get("extra_urls_debug") or {}
            if isinstance(_d, dict):
                _ad = _inj_diag_norm_url_list(_d.get("admitted") or _d.get("extra_urls_admitted") or [])
                _ui = _inj_diag_norm_url_list(_d.get("ui_norm") or _d.get("extra_urls_ui_norm") or _d.get("extra_urls_normalized") or [])
                _fix2v_injected_norm_set = set(_ad or _ui or [])
    except Exception:
        pass
        _fix2v_injected_norm_set = set()

    def _fix2v_norm_url(u: str) -> str:
        try:
            cu = _canonicalize_injected_url(u) if callable(globals().get("_canonicalize_injected_url")) else None
            return (cu or str(u or "")).strip()
        except Exception:
            return str(u or "").strip()

    def _fix2v_extract_years_from_blob(blob: str) -> set:
        try:
            ys = set()
            for mm in re.findall(r"\b(19\d{2}|20\d{2})\b", str(blob or "")):
                try:
                    ys.add(int(mm))
                except Exception:
                    return ys
        except Exception:
            return set()

    _fix2v_bind_hits = []
    for _c in candidates:
        try:
            if not isinstance(_c, dict):
                continue
            _u = _fix2v_norm_url(_c.get("source_url") or "")
            _c["_fix2v_source_is_injected"] = bool(_u and (_u in _fix2v_injected_norm_set))
            if not _c.get("_fix2v_source_is_injected"):
                continue

            _blob = " ".join([
                str(_c.get("context_snippet") or ""),
                str(_c.get("label") or ""),
                str(_c.get("raw") or ""),
            ])
            _blob_n = _norm(_blob)
            _years = _fix2v_extract_years_from_blob(_blob)

            # --- Bind: 2040 charger infrastructure count (e.g., 206.6 million worldwide) ---
            if (2040 in _years) and (("charging infrastructure" in _blob_n) or ("ev charging" in _blob_n) or ("chargers" in _blob_n) or ("charger" in _blob_n)):
                if ("worldwide" in _blob_n) or ("global" in _blob_n):
                    # If upstream left unit empty but raw contains 'million', use unit_tag M for magnitude
                    if (not str(_c.get("unit_tag") or _c.get("unit") or "").strip()) and ("million" in str(_c.get("raw") or "").lower()):
                        _c["unit_tag"] = "M"
                    _c["fix2v_force_canonical_key"] = "global_ev_chargers_2040__unit_count"
                    _fix2v_bind_hits.append(dict(kind="count_2040", anchor_hash=_c.get("anchor_hash"), value=_c.get("value_norm") or _c.get("value"), source_url=_c.get("source_url")))
                    continue

            # --- Bind: CAGR 12.3% from 2026 to 2040 ---
            if (2026 in _years) and (2040 in _years):
                if ("cagr" in _blob_n) or ("compound annual growth rate" in _blob_n):
                    raw_l = str(_c.get("raw") or "").lower()
                    u_l = str(_c.get("unit") or _c.get("unit_tag") or "").lower()
                    if ("%" in raw_l) or ("%" in u_l) or ("percent" in u_l) or ("percent" in raw_l):
                        _c["fix2v_force_canonical_key"] = "global_ev_chargers_cagr_2026_2040__percent"
                        _fix2v_bind_hits.append(dict(kind="cagr_2026_2040", anchor_hash=_c.get("anchor_hash"), value=_c.get("value_norm") or _c.get("value"), source_url=_c.get("source_url")))
                        continue
        except Exception:
            pass
            continue

    try:
        if isinstance(web_context, dict):
            web_context.setdefault("fix2v_candidate_binding_v1", {})
            web_context["fix2v_candidate_binding_v1"]["injected_norm_set_size"] = int(len(_fix2v_injected_norm_set))
            web_context["fix2v_candidate_binding_v1"]["binding_hits"] = _fix2v_bind_hits[:200]
            web_context["fix2v_candidate_binding_v1"]["binding_hit_count"] = int(len(_fix2v_bind_hits))
    except Exception:
        pass

    # =====================================================================
    # PATCH FIX2Y_CANDIDATE_AUTOPSY_V1 (ADDITIVE)
    # Purpose:
    #   Provide a deterministic "why not canonical?" autopsy for targeted keys.
    #   Records:
    #     - candidate pool size
    #     - eligible vs rejected counts + first-N reject reasons
    #     - top-scoring candidates (pre-selection) + winner summary
    # Non-negotiables:
    #   - additive only; no behavior changes to selection
    #   - uses existing FIX16 gates; does not bypass selector
    # Scope:
    #   Only for the new EV-charger keys introduced in FIX2U/FIX2V.
    # =====================================================================
    _fix2y_targets = set([
        "global_ev_chargers_2040__unit_count",
        "global_ev_chargers_cagr_2026_2040__percent",
    ])
    _fix2y_autopsy = {}
    try:
        # Make FIX2W eval sampler safe: some branches refer to extracted_candidates.
        extracted_candidates = candidates  # noqa: F841
    except Exception:
        pass

    def _fix2y_gate_reason(_c: dict, _spec: dict, _ck: str) -> str:
        try:
            if not isinstance(_c, dict):
                return "non_dict_candidate"
            # fix15 junk/year-only exclusion (if present)
            _fn = globals().get("_candidate_disallowed_for_metric")
            if callable(_fn):
                try:
                    if _fn(_c, dict(_spec or {}, canonical_key=_ck)):
                        return "fix15_disallowed"
                except Exception:
                    return "fix15_disallowed_err"
            try:
                _expected_dim = _fix16_expected_dimension(_spec)
            except Exception:
                pass
                _expected_dim = ""
            try:
                if not _fix16_unit_compatible(_c, _expected_dim):
                    return "unit_incompatible"
            except Exception:
                pass
                # if unit compatibility check fails, treat as incompatible for diagnosis only
                return "unit_incompatible_err"
            # year-token guard (unitless year-like numerics)
            try:
                if not _fix16_metric_is_year_like(_spec, canonical_key=_ck):
                    _v = _c.get("value") if _c.get("value") is not None else _c.get("value_norm")
                    _u = (str(_c.get("base_unit") or _c.get("unit") or "")).strip()
                    if _u == "" and isinstance(_v, (int, float)):
                        _iv = int(_v)
                        if 1900 <= _iv <= 2100:
                            return "unitless_year_guard"
            except Exception:
                return "ok"
        except Exception:
            return "gate_reason_err"

    def _fix2y_score_hits(_c: dict, _kw_norm: list) -> int:
        try:
            ctx = _norm(_c.get("context_snippet") or _c.get("context") or _c.get("context_window") or "")
            raw = _norm(_c.get("raw") or "")
            _hits = 0
            for _k in (_kw_norm or []):
                if _k and (_k in ctx or _k in raw):
                    _hits += 1
            return int(_hits)
        except Exception:
            return 0

    # We'll populate _fix2y_autopsy inside the schema loop when the key matches.
    # =====================================================================
    # END PATCH FIX2Y_CANDIDATE_AUTOPSY_V1
    # =====================================================================

# =====================================================================
    # PATCH FIX2W_BINDING_RULE_EVAL_SAMPLES_V1 (ADDITIVE)
    # =====================================================================
    try:
        _eval_samples = []
        _eligible_but_unbound = 0
        if isinstance(extracted_candidates, list):
            for _c in extracted_candidates:
                if not isinstance(_c, dict):
                    continue
                _is_inj = False
                try:
                    _is_inj = bool(_ph2b_norm_url(_c.get("source_url") or "") in _fix2v_injected_norm_set)
                except Exception:
                    pass
                    _is_inj = False
                if not _is_inj:
                    continue

                _raw = str(_c.get("context_snippet") or _c.get("snippet") or _c.get("raw") or "")
                _raw_l = _raw.lower()
                if ("charg" not in _raw_l) and ("cagr" not in _raw_l) and ("compound annual" not in _raw_l) and ("2040" not in _raw_l):
                    continue

                has_charging = (("charging infrastructure" in _raw_l) or ("ev charging" in _raw_l) or ("charger" in _raw_l) or ("chargers" in _raw_l))
                has_global = (("global" in _raw_l) or ("worldwide" in _raw_l))
                has_2040 = ("2040" in _raw_l) or (str(_c.get("year") or "") == "2040")
                has_2026 = ("2026" in _raw_l) or (str(_c.get("year") or "") == "2026")
                has_cagr = (("cagr" in _raw_l) or ("compound annual growth rate" in _raw_l))
                u_l = str(_c.get("unit") or _c.get("unit_tag") or _c.get("unit_norm") or "").lower()
                has_percent = ("%" in _raw_l) or ("%" in u_l) or ("percent" in u_l) or ("percent" in _raw_l)
                has_magnitude_m = (" million" in _raw_l) or (u_l.strip() in ("m", "mn", "million", "mio")) or (str(_c.get("unit") or "").strip() == "M")

                predicted = "none"
                if has_charging and has_global and has_2040 and (not has_percent) and has_magnitude_m:
                    predicted = "count_2040"
                if has_cagr and has_2026 and has_2040 and has_percent:
                    predicted = "cagr_2026_2040"

                bound_key = str(_c.get("fix2v_force_canonical_key") or "")
                if predicted != "none" and not bound_key:
                    _eligible_but_unbound += 1

                _eval_samples.append({
                    "source_url": _c.get("source_url"),
                    "source_url_norm": _ph2b_norm_url(_c.get("source_url") or ""),
                    "value_norm": _c.get("value_norm") if isinstance(_c.get("value_norm"), (int, float)) else _c.get("value"),
                    "unit": _c.get("unit") or _c.get("unit_tag"),
                    "has_charging": bool(has_charging),
                    "has_global": bool(has_global),
                    "has_2040": bool(has_2040),
                    "has_2026": bool(has_2026),
                    "has_cagr": bool(has_cagr),
                    "has_percent": bool(has_percent),
                    "has_magnitude_m": bool(has_magnitude_m),
                    "predicted_rule": predicted,
                    "bound_key": bound_key or None,
                })
                if len(_eval_samples) >= 200:
                    break

        if isinstance(web_context, dict):
            web_context.setdefault("fix2v_candidate_binding_v1", {})
            web_context["fix2v_candidate_binding_v1"]["rule_eval_samples"] = _eval_samples
            web_context["fix2v_candidate_binding_v1"]["eligible_but_unbound_count"] = int(_eligible_but_unbound)
    except Exception:
        pass
    # END PATCH FIX2W_BINDING_RULE_EVAL_SAMPLES_V1
    # END PATCH FIX2V_INJECTED_CANDIDATE_BINDING_V1

    # =========================================================
    # PATCH FIX2Z_SCHEMA_BINDING_ADMISSION_V1 (ADDITIVE)
    #   - Deterministically synthesize schema-bound candidates from extracted_numbers
    #   - Injected-only (domain-agnostic): only candidates proven from injected URL set
    #   - No fuzzy matching: exact substring keyword hits + unit-family compatibility
    #   - Does NOT bypass the Analysis canonical selector; it only increases eligible pool
    # =========================================================
    try:
        _fix2z_hits = []
        _fix2z_added = 0
        _fix2z_seen = 0

        # Build list of schema keys grouped by unit_family for quick scan
        _fix2z_schema_percent = []
        _fix2z_schema_magnitude = []
        for _k, _spec in (metric_schema or {}).items():
            if not isinstance(_spec, dict):
                continue
            _uf = str(_spec.get("unit_family") or "").lower().strip()
            if _uf == "percent":
                _fix2z_schema_percent.append((_k, _spec))
            elif _uf == "magnitude":
                _fix2z_schema_magnitude.append((_k, _spec))

        def _fix2z_blob(c: dict) -> str:
            return " ".join([
                str(c.get("context_snippet") or ""),
                str(c.get("label") or ""),
                str(c.get("raw") or ""),
                str(c.get("context") or ""),
            ]).strip()

        def _fix2z_is_injected(c: dict) -> bool:
            if bool(c.get("_fix2v_source_is_injected")):
                return True
            try:
                u = str(c.get("source_url") or "")
                u_norm = (_inj_diag_norm_url_list([u])[0] if u else "")
                return (u_norm in _fix2v_injected_norm_set)
            except Exception:
                return False

        def _fix2z_has_percent(c: dict, blob_l: str) -> bool:
            u = str(c.get("unit_tag") or c.get("unit") or "").lower()
            if ("%" in u) or ("percent" in u):
                return True
            if "%" in (str(c.get("raw") or "")):
                return True
            if ("%" in blob_l) or (" percent" in blob_l):
                return True
            mk = str(c.get("measure_kind") or "").lower()
            if "pct" in mk or "percent" in mk:
                return True
            return False

        def _fix2z_has_magnitude_m(c: dict, blob_l: str) -> bool:
            u = str(c.get("unit_tag") or c.get("unit") or "").lower()
            if u == "m" or "million" in u:
                return True
            if " million" in blob_l:
                return True
            if str(c.get("unit_tag") or "") == "M":
                return True
            return False

        def _fix2z_money_context(blob_l: str) -> bool:
            # Avoid contaminating count metrics with spend/currency context
            money_terms = ["$", " usd", "usd ", "billion", "bn", "spend", "investment", "capex", "revenue", "worth"]
            return any(t in blob_l for t in money_terms)

        def _fix2z_keyword_hits(blob_l: str, keywords) -> int:
            hits = 0
            if not isinstance(keywords, list):
                return 0
            for kw in keywords:
                kw_s = str(kw or "").strip().lower()
                if not kw_s:
                    continue
                if kw_s.isdigit():
                    # year tokens must be present literally
                    if kw_s in blob_l:
                        hits += 1
                    continue
                if kw_s in blob_l:
                    hits += 1
            return hits

        # Scan unbound injected candidates and synthesize schema-bound copies
        for _c in candidates:
            if not isinstance(_c, dict):
                continue
            if _c.get("fix2v_force_canonical_key"):
                continue  # already bound by FIX2V
            if not _fix2z_is_injected(_c):
                continue  # injected-only admission
            _fix2z_seen += 1

            _blob = _fix2z_blob(_c)
            _blob_l = _blob.lower()

            # Quick classify candidate type
            _is_pct = _fix2z_has_percent(_c, _blob_l)
            _is_mag = _fix2z_has_percent(_c, _blob_l) is False and _fix2z_has_magnitude_m(_c, _blob_l)

            _best = None
            _best_hits = -1
            _best_spec = None

            if _is_pct:
                for _k, _spec in _fix2z_schema_percent:
                    _hits = _fix2z_keyword_hits(_blob_l, _spec.get("keywords"))
                    # Require both boundary years if schema implies a window
                    if ("2026" in [str(x) for x in (_spec.get("keywords") or [])]) and ("2026" not in _blob_l):
                        continue
                    if ("2040" in [str(x) for x in (_spec.get("keywords") or [])]) and ("2040" not in _blob_l):
                        continue
                    if _hits > _best_hits:
                        _best_hits = _hits
                        _best = _k
                        _best_spec = _spec
            elif _is_mag:
                for _k, _spec in _fix2z_schema_magnitude:
                    # Do not bind magnitude count keys if money context detected
                    if str(_spec.get("dimension") or "").lower().strip() == "count":
                        if _fix2z_money_context(_blob_l):
                            continue
                    _hits = _fix2z_keyword_hits(_blob_l, _spec.get("keywords"))
                    # Ensure 2040 is present if schema expects it
                    if ("2040" in [str(x) for x in (_spec.get("keywords") or [])]) and ("2040" not in _blob_l):
                        continue
                    if _hits > _best_hits:
                        _best_hits = _hits
                        _best = _k
                        _best_spec = _spec

            # Deterministic threshold: require a minimum keyword-hit support
            if _best and _best_spec and _best_hits >= 4:
                _c2 = dict(_c)
                _c2["fix2v_force_canonical_key"] = _best
                _c2["fix2z_bound_by"] = "schema_keywords"
                _c2["fix2z_keyword_hits"] = int(_best_hits)
                # Encourage unit tagging consistency (do not overwrite existing unit tags)
                try:
                    if not str(_c2.get("unit_tag") or _c2.get("unit") or "").strip():
                        if str(_best_spec.get("unit_tag") or "").strip():
                            _c2["unit_tag"] = str(_best_spec.get("unit_tag"))
                    if str(_best_spec.get("dimension") or "").strip() and not str(_c2.get("dimension") or "").strip():
                        _c2["dimension"] = str(_best_spec.get("dimension"))
                    if str(_best_spec.get("unit_family") or "").strip() and not str(_c2.get("unit_family") or "").strip():
                        _c2["unit_family"] = str(_best_spec.get("unit_family"))
                except Exception:
                    pass
                candidates.append(_c2)
                _fix2z_added += 1
                if len(_fix2z_hits) < 200:
                    _fix2z_hits.append(dict(
                        bound_key=_best,
                        keyword_hits=int(_best_hits),
                        value=_c.get("value_norm") or _c.get("value"),
                        unit=_c.get("unit_tag") or _c.get("unit"),
                        source_url=_c.get("source_url"),
                        anchor_hash=_c.get("anchor_hash"),
                    ))

        if isinstance(web_context, dict):
            web_context.setdefault("fix2z_schema_binding_admission_v1", {})
            web_context["fix2z_schema_binding_admission_v1"]["seen_injected_unbound"] = int(_fix2z_seen)
            web_context["fix2z_schema_binding_admission_v1"]["synth_candidates_added"] = int(_fix2z_added)
            web_context["fix2z_schema_binding_admission_v1"]["hits"] = _fix2z_hits
    except Exception:
        pass
    # END PATCH FIX2Z_SCHEMA_BINDING_ADMISSION_V1
    # =========================================================
    # =========================================================

    def _norm(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def _cand_sort_key(c: dict):
        try:
            return (
                str(c.get("anchor_hash") or ""),
                str(c.get("source_url") or ""),
                int(c.get("start_idx") or 0),
                str(c.get("raw") or ""),
                str(c.get("unit") or ""),
                float(c.get("value_norm") or 0.0),
            )
        except Exception:
            return ("", "", 0, "", "", 0.0)

    # Deterministic global ordering of candidates
    candidates.sort(key=_cand_sort_key)

    rebuilt = {}

    for canonical_key, sch in metric_schema.items():
        if not isinstance(sch, dict):
            continue

        spec = dict(sch)
        spec.setdefault("canonical_key", canonical_key)
        spec.setdefault("name", sch.get("name") or canonical_key)

        metric_is_year_like = _fix16_metric_is_year_like(spec, canonical_key=canonical_key)

        keywords = sch.get("keywords") or sch.get("keyword_hints") or []
        if isinstance(keywords, str):
            keywords = [keywords]
        keywords = _fix16_prune_year_keywords(list(keywords), metric_is_year_like)
        kw_norm = [_norm(k) for k in keywords if k]

        expected_dim = _fix16_expected_dimension(spec)


        # =====================================================================
        # PATCH FIX2Y_CANDIDATE_AUTOPSY_V1 (ADDITIVE): per-key autopsy snapshot
        # =====================================================================
        if canonical_key in _fix2y_targets:
            try:
                _a = {
                    "canonical_key": canonical_key,
                    "schema_expected_dim": expected_dim,
                    "keywords": list(keywords) if isinstance(keywords, list) else [],
                    "kw_norm_count": int(len(kw_norm)) if isinstance(kw_norm, list) else 0,
                    "pool_total": int(len(candidates)) if isinstance(candidates, list) else 0,
                    "pool_force_filtered": 0,
                    "eligible_count": 0,
                    "rejected_count": 0,
                    "reject_reasons": {},
                    "reject_samples": [],
                    "top_candidates": [],
                    "winner": {},
                }

                _force_pool = []
                for _c0 in (candidates or []):
                    if not isinstance(_c0, dict):
                        continue
                    _fk0 = _c0.get("fix2v_force_canonical_key")
                    if _fk0 and _fk0 != canonical_key:
                        continue
                    _force_pool.append(_c0)
                _a["pool_force_filtered"] = int(len(_force_pool))

                _eligible = []
                for _c1 in _force_pool:
                    _gr = _fix2y_gate_reason(_c1, spec, canonical_key)
                    if _gr == "ok" and _fix16_candidate_allowed(_c1, spec, canonical_key=canonical_key):
                        _eligible.append(_c1)
                    else:
                        _a["rejected_count"] += 1
                        _a["reject_reasons"][_gr] = int(_a["reject_reasons"].get(_gr, 0)) + 1
                        if len(_a["reject_samples"]) < 25:
                            _a["reject_samples"].append({
                                "reason": _gr,
                                "value_norm": _c1.get("value_norm") if isinstance(_c1.get("value_norm"), (int, float)) else _c1.get("value"),
                                "unit": _c1.get("unit") or _c1.get("unit_tag") or "",
                                "source_url": _c1.get("source_url") or "",
                                "anchor_hash": _c1.get("anchor_hash") or "",
                                "raw_head": (str(_c1.get("raw") or "")[:120]),
                            })

                _a["eligible_count"] = int(len(_eligible))

                # Rank preview: top candidates by keyword hits then FIX16 sort key
                _ranked = []
                for _c2 in _eligible:
                    _hits2 = _fix2y_score_hits(_c2, kw_norm)
                    _ranked.append((_hits2, _cand_sort_key(_c2), _c2))
                _ranked.sort(key=lambda t: (-int(t[0]), t[1]))
                for _hits2, _sk2, _c2 in _ranked[:10]:
                    _a["top_candidates"].append({
                        "hits": int(_hits2),
                        "value_norm": _c2.get("value_norm") if isinstance(_c2.get("value_norm"), (int, float)) else _c2.get("value"),
                        "unit": _c2.get("unit") or _c2.get("unit_tag") or "",
                        "source_url": _c2.get("source_url") or "",
                        "anchor_hash": _c2.get("anchor_hash") or "",
                        "raw_head": (str(_c2.get("raw") or "")[:160]),
                    })

                _fix2y_autopsy[canonical_key] = _a
            except Exception:
                pass
        # =====================================================================
        # END PATCH FIX2Y_CANDIDATE_AUTOPSY_V1
        # =====================================================================

        best = None
        best_tie = None

        for c in candidates:
            # FIX2V: if candidate is force-bound to a specific canonical slot, only allow it to compete there
            _fk = c.get("fix2v_force_canonical_key")
            if _fk and _fk != canonical_key:
                continue

            # FIX16 hard eligibility gates
            if not _fix16_candidate_allowed(c, spec, canonical_key=canonical_key):
                continue

            # PATCH FIX2D2U: semantic eligibility gate (parity with evolution schema rebuild)
            try:
                _ok_u, _why_u = _fix2d2u_semantic_eligible(c, spec, canonical_key)
                if not _ok_u:
                    continue
            except Exception:
                pass

            # PATCH FIX2D2U: enforce shared semantic eligibility (parity with Evolution)
            try:
                _ok_u, _why_u = _fix2d2u_semantic_eligible(c, spec, canonical_key)
                if not _ok_u:
                    continue
            except Exception:
                pass

            # FIX2D2R: forbid bare-year tokens when a better sibling exists nearby (parity guard)
            try:
                _ek = 'other'
                _ed = (expected_dim or '').lower().strip()
                if canonical_key.endswith('__percent') or _ed == 'percent':
                    _ek = 'percent'
                elif canonical_key.endswith('__currency') or _ed == 'currency':
                    _ek = 'currency'
                elif canonical_key.endswith('__unit_sales') or 'unit' in canonical_key.lower() or 'sales' in canonical_key.lower():
                    _ek = 'unit'
                elif canonical_key.endswith('__year') or 'year' in _ed:
                    _ek = 'year'

                if _ek != 'year' and _fix2d2r_is_bare_year_cand(c) and not str(c.get('unit') or c.get('unit_tag') or '').strip():
                    if _fix2d2r_has_better_sibling(c, candidates, _ek):
                        continue
            except Exception:
                pass

            # keyword relevance
            ctx = _norm(c.get("context_snippet") or c.get("context") or c.get("context_window") or "")
            raw = _norm(c.get("raw") or "")

            hits = 0
            for k in kw_norm:
                if k and (k in ctx or k in raw):
                    hits += 1

            # If there are no keyword hits at all, keep as weak fallback only if unit family matches strongly
            # but do not select zero-hit candidates over hit candidates.
            tie = (-hits,) + _cand_sort_key(c)
            if best is None or tie < best_tie:
                best = c
                best_tie = tie

        if not isinstance(best, dict):
            continue

        # Require at least one keyword hit unless the schema has no keywords
        if kw_norm:
            if best_tie is not None and isinstance(best_tie, tuple):
                try:
                    if (-best_tie[0]) <= 0:
                        continue
                except Exception:
                    pass

        rebuilt[canonical_key] = {
            "canonical_key": canonical_key,
            "name": spec.get("name") or canonical_key,
            "value": best.get("value"),
            "unit": best.get("unit") or "",
            "value_norm": best.get("value_norm"),
            "source_url": best.get("source_url") or "",
            "anchor_hash": best.get("anchor_hash") or "",
            "evidence": [{
                "source_url": best.get("source_url") or "",
                "raw": best.get("raw") or "",
                "context_snippet": (best.get("context_snippet") or best.get("context") or best.get("context_window") or "")[:400],
                "anchor_hash": best.get("anchor_hash") or "",
                "method": "schema_only_rebuild_fix16",
            }],
            "anchor_used": False,
        }

    return rebuilt


# =====================================================================
# PATCH FIX16 (ADDITIVE): wire FIX16 rebuilds into the existing dispatch
# - Keep names identical so evolution uses these as the LAST definitions
# - We expose both functions while preserving older ones for reference
# =====================================================================




# =====================================================================
# END PATCH FIX16
# =====================================================================


# =====================================================================
# PATCH PH2B_S1 (ADDITIVE): Extract a PURE analysis-canonical final selector (v1)
# Goal:
#   - Provide exactly ONE authoritative selector for dashboard-facing "Current"
#   - Treat candidate.value_norm as schema units (no base-unit assumption)
#   - Deterministic tie-breaks; NO IO; NO re-fetch; NO hashing changes
#
# Notes:
#   - This is a single-metric selector. Batch rebuild helpers may call it.
#   - Reuses FIX16 hard eligibility gates (_fix16_candidate_allowed) + FIX16 scoring.
#   - Adds optional preferred/anchor lock when anchors are present (stays in preferred source).
# =====================================================================

def _ph2b_norm_url(url: str) -> str:
    try:
        fn = globals().get("_normalize_url")
        if callable(fn):
            return str(fn(url or ""))
    except Exception:
        return str((url or "").strip())



# =====================================================================
# PATCH FIX2D2U (ADD): Shared semantic eligibility gate (Analysis parity)
# - Uses LOCAL context_snippet (tight window) instead of page-wide context_window.
# - Prevents cross-metric pollution in schema-only rebuild paths.
# - Applied in BOTH Analysis selector and Evolution schema-only rebuild(s).
# =====================================================================

def _fix2d2u_norm_text(s: str) -> str:
    try:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()
    except Exception:
        return ""


def _fix2d2u_local_text(cand: dict) -> str:
    """Prefer the tightest snippet around the number."""
    try:
        if not isinstance(cand, dict):
            return ""
        for k in ("context_snippet", "context", "context_window", "context_window_raw", "context_window_text"):
            v = cand.get(k)
            if isinstance(v, str) and v.strip():
                return v
        return str(cand.get("raw") or cand.get("value") or "")
    except Exception:
        return ""


def _fix2d2u_required_token_groups(canonical_key: str, spec: dict) -> list:
    """Return OR-groups; every group must have at least one hit in local snippet."""
    try:
        ck = str(canonical_key or "").lower()
        nm = str((spec or {}).get("name") or (spec or {}).get("metric_name") or "").lower()
        base = (ck + " " + nm).strip()

        groups = []

        # Geo cues
        if "china" in base:
            groups.append(["china", "chinese"])

        # Time cues: require explicit 4-digit years present in key/name
        years = re.findall(r"\b(19\d{2}|20\d{2})\b", base)
        for y in years[:3]:
            groups.append([y])

        # Metric family cues
        if ("charger" in base) or ("charging" in base):
            groups.append(["charger", "chargers", "charging", "station", "stations", "infrastructure"])
        if ("investment" in base) or ("capex" in base) or ("spend" in base) or (str((spec or {}).get("unit_family") or "").lower() == "currency"):
            groups.append(["investment", "invest", "capex", "spend", "spending", "cost", "expenditure"])
        if ("share" in base) or (str((spec or {}).get("unit_family") or "").lower() == "percent"):
            groups.append(["market share", "share", "%", "percent"])
        if ("sale" in base) or ("sales" in base) or ("unit_sales" in base):
            groups.append(["sales", "sold", "deliveries", "registrations", "units"])
        if "ytd" in base:
            groups.append(["ytd", "year to date"])

        # De-dupe
        out=[]
        for g in groups:
            gg=[]
            for t in g:
                tt=str(t or "").strip().lower()
                if tt and tt not in gg:
                    gg.append(tt)
            if gg:
                out.append(gg)
        return out
    except Exception:
        return []


def _fix2d2u_semantic_eligible_global(cand: dict, spec: dict, canonical_key: str) -> tuple:
    """(ok, reason) gate; no-op when no required groups."""
    try:
        groups = _fix2d2u_required_token_groups(canonical_key, spec or {})
        if not groups:
            return True, ""
        txt = _fix2d2u_local_text(cand)
        blob = " ".join([_fix2d2u_norm_text(txt), _fix2d2u_norm_text(str(cand.get("raw") or ""))]).strip()
        for g in groups:
            hit=False
            for tok in g:
                tn=_fix2d2u_norm_text(tok)
                if tn and tn in blob:
                    hit=True
                    break
            if not hit:
                return False, "missing_required_tokens:" + "|".join(g[:3])
        return True, ""
    except Exception:
        return True, ""



# =====================================================================
# PATCH FIX2D2V (ADDITIVE): enforce semantic gates at schema_only_rebuild
# commit point using a tight local window around the numeric token.
# - Prevents cross-metric pollution where a China sales snippet populates
#   chargers/investment/CAGR schema keys.
# - Requires canonical-key year tokens to appear locally (if present).
# - Logs reject counts into web_context.debug.fix2d2v_schema_commit_rejects.
# =====================================================================

def _fix2d2v_tight_window(cand: dict, width: int = 120) -> str:
    try:
        txt = str(cand.get('context_snippet') or cand.get('context') or cand.get('context_window') or '')
        if not txt:
            return ''
        raw = str(cand.get('raw') or cand.get('value') or '')
        if raw:
            i = txt.find(raw)
            if i >= 0:
                a = max(0, i - width)
                b = min(len(txt), i + len(raw) + width)
                return txt[a:b]
        # fallback: just trim
        return txt[: (2*width)]
    except Exception:
        return ''


def _fix2d2v_years_from_key(canonical_key: str):
    try:
        import re
        return re.findall(r"\b(19\d{2}|20\d{2})\b", str(canonical_key or ''))
    except Exception:
        return []


def _fix2d2v_semantic_eligible_commit(cand: dict, spec: dict, canonical_key: str, web_context=None) -> tuple:
    """Stricter eligibility used immediately before committing schema_only_rebuild outputs."""
    try:
        if not bool(globals().get("_FIX2D2U_ENABLE", True)):
            return True, ''
        # 1) Start with FIX2D2U required-token groups, but only check within a tight window
        groups = _fix2d2u_required_token_groups(canonical_key, spec)
        win = _fix2d2u_norm(_fix2d2v_tight_window(cand) + ' ' + str(cand.get('raw') or ''))
        for g in groups or []:
            hit = False
            for tok in g:
                tn = _fix2d2u_norm(tok)
                if tn and tn in win:
                    hit = True
                    break
            if not hit:
                return False, 'missing_required_tokens_tight'
        # 2) If the canonical key contains explicit years, require them locally as well
        years = _fix2d2v_years_from_key(canonical_key)
        if years:
            for y in years:
                if y and (_fix2d2u_norm(y) not in win):
                    return False, 'missing_required_year_tight'
        return True, ''
    finally:
        pass

# =====================================================================
# END PATCH FIX2D2V
# =====================================================================
# =====================================================================
# END PATCH FIX2D2U
# =====================================================================

# =====================================================================
# PATCH FIX2D63 (ADDITIVE): harden schema_only_rebuild_fix17 selection
# against injected-year pollution for unit/count metrics.
#
# Motivation:
#   Even with downstream yearlike blocking, schema_only_rebuild_fix17 can
#   still select a nearby year token (e.g., 2024/2025) as the VALUE for a
#   unit_sales metric when the context includes headings like "YTD 2025".
#   This patch moves the rejection upstream, at candidate-eligibility time,
#   and fixes a variable typo that could silently disable FIX2D2U gating.
#
# Policy (for unit/count-like schema keys):
#   - Reject yearlike numeric candidates unless they carry unit evidence.
#   - Prefer candidates with unit evidence (unit_tag/unit/unit_family/context).
#   - Keep existing bare-year token guards as a last-mile safety net.
# =====================================================================

def _fix2d63_is_yearlike_value(cand: dict) -> bool:
    try:
        v = cand.get('value_norm')
        if v is None:
            try:
                v = float(cand.get('value') or 0.0)
            except Exception:
                pass
                v = None
        if v is None:
            return False
        iv = int(float(v))
        # Conservative year window
        return (1900 <= iv <= 2100) and abs(float(v) - float(iv)) < 1e-9
    except Exception:
        return False


def _fix2d63_has_unit_evidence(cand: dict) -> bool:
    try:
        ut = str(cand.get('unit_tag') or cand.get('unit') or '').strip()
        uf = str(cand.get('unit_family') or '').strip().lower()
        mk = str(cand.get('measure_kind') or '').strip().lower()
        ma = str(cand.get('measure_assoc') or '').strip().lower()
        if ut:
            return True
        if uf in ('magnitude', 'percent', 'currency', 'energy', 'index'):
            return True
        if mk in ('count_units', 'count', 'quantity'):
            return True
        if ma in ('units', 'unit_sales', 'sales'):
            return True
        ctx = (str(cand.get('context_snippet') or cand.get('context') or cand.get('context_window') or '') + ' ' + str(cand.get('raw') or '')).lower()
        # Contextual unit hints
        if any(w in ctx for w in ['million', 'billion', 'thousand', 'trillion', 'units', 'unit', 'vehicles', '%', 'percent', 'usd', 'sgd', 'eur', '$', '€', '£', '¥']):
            return True
        return False
    except Exception:
        return False


def _fix2d63_schema_expects_unit_or_count(canonical_key: str, spec: dict) -> bool:
    try:
        ck = str(canonical_key or '')
        dim = str((spec or {}).get('dimension') or '').strip().lower()
        uf = str((spec or {}).get('unit_family') or '').strip().lower()
        # Suffix-based (most reliable given existing schema patterns)
        if ck.endswith('__unit_sales') or ck.endswith('__units') or ck.endswith('__unit'):
            return True
        # Schema hints
        if dim in ('unit_sales', 'count', 'quantity'):
            return True
        if uf == 'magnitude' and dim:
            # magnitude metrics are generally non-year values unless explicitly year metrics
            return True
        return False
    except Exception:
        return False

# =====================================================================
# END PATCH FIX2D63
# =====================================================================

def _analysis_canonical_final_selector_v1(
    canonical_key: str,
    schema_frozen: dict,
    candidates: list,
    anchors: dict = None,
    prev_metric: dict = None,
    web_context: dict = None,
) -> tuple:
    """Pure selector: returns (best_metric_or_None, meta_dict)."""
    import re

    meta = {
        "selector_used": "analysis_canonical_v1",
        "canonical_key": canonical_key or "",
        "anchor_used": False,
        "blocked_reason": "",
        "preferred_url": "",
        "chosen_source_url": "",
        "tie_break": "",
        "eligible_count": 0,
        "range_method": "",
    }

    spec = schema_frozen or {}
    if not isinstance(spec, dict) or not spec:
        meta["blocked_reason"] = "missing_schema"
        return None, meta

    # Determine preferred URL from anchors (strongest) or schema if present
    preferred_url = ""
    anchor = None
    if isinstance(anchors, dict) and canonical_key in anchors and isinstance(anchors.get(canonical_key), dict):
        anchor = anchors.get(canonical_key) or {}
        preferred_url = anchor.get("source_url") or ""
    preferred_url = preferred_url or (spec.get("preferred_url") or spec.get("source_url") or "")
    if preferred_url:
        meta["preferred_url"] = _ph2b_norm_url(preferred_url)

    # Prepare FIX16 keyword scoring (same as rebuild_metrics_from_snapshots_schema_only_fix16)
    def _norm(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def _cand_sort_key(c: dict):
        try:
            return (
                str(c.get("anchor_hash") or ""),
                str(c.get("source_url") or ""),
                int(c.get("start_idx") or 0),
                str(c.get("raw") or ""),
                str(c.get("unit") or ""),
                float(c.get("value_norm") or 0.0),
            )
        except Exception:
            return ("", "", 0, "", "", 0.0)

    # FIX16 keyword pruning helper if present; otherwise keep keywords as-is
    metric_is_year_like = False
    try:
        fn_year = globals().get("_fix16_metric_is_year_like")
        if callable(fn_year):
            metric_is_year_like = bool(fn_year(spec, canonical_key=canonical_key))
    except Exception:
        pass
        metric_is_year_like = False

    keywords = spec.get("keywords") or spec.get("keyword_hints") or []
    if isinstance(keywords, str):
        keywords = [keywords]
    try:
        fn_prune = globals().get("_fix16_prune_year_keywords")
        if callable(fn_prune):
            keywords = fn_prune(list(keywords), metric_is_year_like)
    except Exception:
        pass
        keywords = list(keywords) if isinstance(keywords, list) else []

    kw_norm = [_norm(k) for k in (keywords or []) if k]

    # Candidate filtering
    cands = [c for c in (candidates or []) if isinstance(c, dict)]
    # PATCH FIX2B_TRACE_V1 (ADDITIVE): candidate counts for trace
    try:
        meta["candidate_count_in"] = int(len(cands))
    except Exception:
        pass
    # Enforce preferred source lock when available (prevents cross-source hijack)
    if meta["preferred_url"]:
        pref = meta["preferred_url"]
        cands_pref = []
        for c in cands:
            cu = _ph2b_norm_url(c.get("source_url") or "")
            if cu and cu == pref:
                cands_pref.append(c)
        # If preferred exists but yields zero candidates, we keep empty (hard lock).
        cands = cands_pref
        # PATCH FIX2B_TRACE_V1 (ADDITIVE): preferred-locked candidate count
        try:
            meta["candidate_count_pref"] = int(len(cands))
        except Exception:
            pass

    eligible = []
    for c in cands:
        try:
            # =====================================================================
            # PATCH PH2B_UF1 (ADDITIVE): Fill missing unit_family/unit_cmp deterministically
            # Many snapshot candidates omit unit_family even when unit_tag/raw clearly indicates
            # magnitude/percent/currency. The analysis selector treats unit_family as authoritative
            # for schema gating; leaving it blank causes false ineligibility (empty Current).
            # =====================================================================
            try:
                if isinstance(c, dict) and not str(c.get("unit_family") or "").strip():
                    _raw = str(c.get("raw") or "")
                    _ut = str(c.get("unit_tag") or c.get("unit") or "")
                    _ctx = str(c.get("context_snippet") or "")
                    _blob = (" ".join([_raw, _ut, _ctx])).lower()
                    uf = ""
                    if "%" in _blob or "percent" in _blob or "percentage" in _blob:
                        uf = "percent"
                    elif any(tok in _blob for tok in ["usd", "sgd", "eur", "gbp", "$", "€", "£", "¥", "aud", "cad", "inr", "cny", "rmb"]):
                        uf = "currency"
                    else:
                        # Magnitude / counts (incl. unit sales)
                        if any(w in _blob for w in ["million", "billion", "thousand", "trillion"]) or re.search(r"[mbkt]", _blob):
                            uf = "magnitude"
                        elif str(c.get("measure_kind") or "").lower() in ("count_units", "count", "quantity"):
                            uf = "magnitude"
                        elif str(c.get("measure_assoc") or "").lower() in ("units", "unit_sales", "sales"):
                            uf = "magnitude"
                    if uf:
                        c["unit_family"] = uf
                        # unit_cmp hint (best-effort; used only for display/debug)
                        if not str(c.get("unit_cmp") or "").strip():
                            if uf == "percent":
                                c["unit_cmp"] = "%"
                            elif uf == "currency":
                                c["unit_cmp"] = "currency"
                            else:
                                c["unit_cmp"] = (_ut or "").strip()
            except Exception:
                pass

            if not _fix16_candidate_allowed(c, spec, canonical_key=canonical_key):
                continue

            # FIX2D2R: forbid bare-year tokens when a better sibling exists nearby (parity guard)
            try:
                _ek = 'other'
                _ed = (expected_dim or '').lower().strip()
                if canonical_key.endswith('__percent') or _ed == 'percent':
                    _ek = 'percent'
                elif canonical_key.endswith('__currency') or _ed == 'currency':
                    _ek = 'currency'
                elif canonical_key.endswith('__unit_sales') or 'unit' in canonical_key.lower() or 'sales' in canonical_key.lower():
                    _ek = 'unit'
                elif canonical_key.endswith('__year') or 'year' in _ed:
                    _ek = 'year'

                if _ek != 'year' and _fix2d2r_is_bare_year_cand(c) and not str(c.get('unit') or c.get('unit_tag') or '').strip():
                    if _fix2d2r_has_better_sibling(c, candidates, _ek):
                        continue
            except Exception:
                pass

            # =====================================================================
            # PATCH FIX2B_SEL23 (ADDITIVE): unit-family hard gating + year suppression + scaled-magnitude unit evidence
            # - Rejects percent/currency evidence when schema expects magnitude (prevents % hijacks).
            # - Suppresses unitless bare years (e.g., 2030) as candidates.
            # - Enforces unit evidence for scaled magnitude schemas (million/billion/etc.).
            # =====================================================================
            try:
                _raw0 = str(c.get("raw") or "").strip()
                _raw0_l = _raw0.lower()
                _v0 = c.get("value_norm", None)
                if _v0 is None:
                    _v0 = c.get("value", None)

                _cand_family = str(c.get("unit_family") or "").strip().lower()
                _cand_ucmp = str(c.get("unit_cmp") or c.get("unit_tag") or "").strip().lower()
                _spec_family = str(spec.get("unit_family") or "").strip().lower()
                # =====================================================================
                # PATCH FIX2B_SCHEMAFAM_INFER_V1 (ADDITIVE):
                # If schema.unit_family is missing/blank, infer expected family deterministically
                # from schema.dimension / canonical_key suffixes using existing FIX17 helper.
                # This prevents silent bypass of percent/currency hard-gates.
                # =====================================================================
                try:
                    if not _spec_family:
                        _fn_exp = globals().get("_fix17_expected_dimension")
                        if callable(_fn_exp):
                            _exp = str(_fn_exp(spec, canonical_key=canonical_key) or "").strip().lower()
                            if _exp in ("percent", "currency", "magnitude", "rate", "ratio"):
                                _spec_family = _exp
                except Exception:
                    pass
                _spec_ut = str(spec.get("unit_tag") or spec.get("unit") or "").strip().lower()

                # evidence flags
                _has_pct = ("%" in _raw0) or ("percent" in _raw0_l) or ("%" in _cand_ucmp) or (_cand_family == "percent")
                _has_ccy = any(sym in _raw0 for sym in ("$", "€", "£", "¥")) or any(tok in _raw0_l for tok in ("usd", "eur", "sgd", "gbp", "jpy")) or (_cand_family == "currency")

                _has_unit_ev0 = False
                try:
                    for _k in ("base_unit", "unit", "unit_tag", "unit_family"):
                        if str(c.get(_k) or "").strip():
                            _has_unit_ev0 = True
                            break
                    if not _has_unit_ev0:
                        if any(tok in _raw0_l for tok in ("million", "billion", "trillion", "mn", "bn", "thousand")):
                            _has_unit_ev0 = True
                        if _has_pct or _has_ccy:
                            _has_unit_ev0 = True
                except Exception:
                    pass

                # unit-family hard gating (schema-driven)
                try:
                    if _spec_family == "percent":
                        if not _has_pct:
                            meta["blocked_reason"] = "percent_evidence_missing_hard_block"
                            continue
                    elif _spec_family == "currency":
                        if not _has_ccy:
                            meta["blocked_reason"] = "currency_evidence_missing_hard_block"
                            continue
                    elif _spec_family == "magnitude":
                        # reject percent/currency candidates outright
                        if _has_pct or _has_ccy:
                            meta["blocked_reason"] = "unit_mismatch_hard_block"
                            continue
                except Exception:
                    pass

                # year-only candidate suppression (strict): only when unit evidence is missing
                try:
                    if (not _has_unit_ev0) and isinstance(_v0, (int, float)):
                        _iv0 = int(float(_v0))
                        if 1900 <= _iv0 <= 2100:
                            import re as _re
                            if _re.fullmatch(r"(19\d{2}|20\d{2})", _raw0 or str(_iv0)):
                                continue
                except Exception:
                    pass

                # scaled magnitude requires unit evidence (schema implies million/billion/etc.)
                try:
                    _spec_nm = str(spec.get("name") or "").lower()
                    _scaled = any(t in _spec_ut for t in ("million", "billion", "trillion", "thousand")) or (_spec_ut in ("m", "b", "t", "k"))
                    _countish = any(t in _spec_nm for t in ("unit", "units", "sales", "deliveries", "shipments", "registrations", "volume"))
                    if _spec_family == "magnitude" and _scaled and _countish and (not _has_unit_ev0):
                        meta["blocked_reason"] = "unit_evidence_missing_hard_block"
                        continue
                except Exception:
                    pass

                # =====================================================================
                # PATCH FIX2B_SCALE_EV_V2 (2026-01-10)
                # Purpose: Fix broken scale/countish gating where earlier patch strings were truncated
                #          (e.g., "milli..." / "uni...") and therefore never matched.
                #          Enforce: if schema implies scaled magnitude (million/billion/etc.), unit evidence
                #          must exist in the candidate (unit_tag/unit_cmp/raw/context), else hard-block.
                # Safety: additive-only; does not change fastpath/hashing/injection/snapshot attach.
                # =====================================================================
                try:
                    _spec_nm2 = str(spec.get("name") or spec.get("label") or "").lower()
                    _spec_ut2 = str(_spec_ut or "").lower()

                    def _ph2b_has_scale_token(_s: str) -> bool:
                        try:
                            _s = str(_s or "").lower()
                        except Exception:
                            pass
                            _s = ""
                        if not _s:
                            return False
                        toks = [
                            "million", "mn", "m", "millions",
                            "billion", "bn", "b", "billions",
                            "trillion", "tn", "t", "trillions",
                            "thousand", "k", "000",
                        ]
                        # require word-boundary-ish for single-letter tokens
                        if "million" in _s or "millions" in _s or "billion" in _s or "billions" in _s or "trillion" in _s or "trillions" in _s or "thousand" in _s:
                            return True
                        import re as _re2
                        if _re2.search(r"\b(mn|bn|tn|k)\b", _s):
                            return True
                        # "m" / "b" / "t" are too ambiguous; only accept when adjacent to "usd/units/sales" etc.
                        if _re2.search(r"\b(m|b|t)\b", _s) and _re2.search(r"(units?|sales|deliveries|shipments|vehicles|usd|eur|sgd|gbp|jpy|cny|aud|cad)", _s):
                            return True
                        return False

                    def _ph2b_has_unit_evidence_candidate(_c: dict) -> bool:
                        if not isinstance(_c, dict):
                            return False
                        if str(_c.get("unit_tag") or "").strip():
                            return True
                        if str(_c.get("unit_cmp") or "").strip():
                            return True
                        if str(_c.get("unit_family") or "").strip():
                            return True
                        # raw/context tokens
                        if _ph2b_has_scale_token(_c.get("raw")):
                            return True
                        if _ph2b_has_scale_token(_c.get("context_snippet") or _c.get("context")):
                            return True
                        return False

                    _schema_scaled = _ph2b_has_scale_token(_spec_ut2)
                    # If schema is scaled, we hard-require candidate unit evidence regardless of name.
                    if _spec_family == "magnitude" and _schema_scaled and (not _ph2b_has_unit_evidence_candidate(c0)):
                        meta["blocked_reason"] = "unit_evidence_missing_hard_block"
                        continue

                    # Extra guard: suppress obvious document-structure numbers (pages/figures) when schema is scaled.
                    try:
                        if _spec_family == "magnitude" and _schema_scaled:
                            _ctx = str((c0 or {}).get("context_snippet") or (c0 or {}).get("context") or "").lower()
                            if ("pages" in _ctx) or ("figures" in _ctx) or ("page " in _ctx):
                                # treat as ineligible rather than junking globally
                                continue
                    except Exception:
                        pass
                except Exception:
                    pass
                # =====================================================================
                # PATCH FIX2B_SCALE_EV_V2 END
                # =====================================================================

                # =====================================================================
                # PATCH FIX2B_SCALE_EV_V1 (ADDITIVE): require *scale* evidence for scaled magnitude schemas
                # Why:
                # - Earlier gating treated inferred unit_family='magnitude' as "unit evidence", allowing unitless
                #   integers (e.g., 170) to pass for schemas like "million units".
                # - For scaled schemas, we require explicit scale evidence in raw/unit_tag/unit_cmp (million/billion/etc.).
                # Safety:
                # - Only applies when schema implies a scale (million/billion/thousand/trillion or M/B/K/T).
                # - Does NOT change fastpath/hashing/injection/snapshot attach.
                # =====================================================================
                try:
                    _scaled2 = False
                    try:
                        _scaled2 = any(t in _spec_ut for t in ("million", "billion", "trillion", "thousand")) or (_spec_ut in ("m", "b", "t", "k"))
                    except Exception:
                        pass
                        _scaled2 = False
                    if _spec_family == "magnitude" and _scaled2:
                        _blob = (" ".join([
                            str(c.get("raw") or ""),
                            str(c.get("unit_cmp") or ""),
                            str(c.get("unit_tag") or c.get("unit") or ""),
                            str(c.get("context_snippet") or c.get("context") or ""),
                        ])).lower()
                        _has_scale_ev = any(tok in _blob for tok in ("million", "billion", "trillion", "thousand", "mn", "bn")) or bool(re.search(r"\b[mbkt]\b", _blob))
                        # =================================================================
                        # PATCH FIX2B_SCALE_MATCH_V1 (ADDITIVE):
                        # For scaled schemas, require scale to *match* the schema (e.g., million vs billion).
                        # Prevents wrong-scale candidates from surviving for 'million units' schemas.
                        # =================================================================
                        try:
                            _schema_scale = ""
                            if "million" in _spec_ut or _spec_ut == "m":
                                _schema_scale = "m"
                            elif "billion" in _spec_ut or _spec_ut == "b":
                                _schema_scale = "b"
                            elif "thousand" in _spec_ut or _spec_ut == "k":
                                _schema_scale = "k"
                            elif "trillion" in _spec_ut or _spec_ut == "t":
                                _schema_scale = "t"

                            _cand_scale = ""
                            if any(t in _blob for t in ("million", "mn")) or bool(re.search(r"m", _blob)):
                                _cand_scale = "m"
                            elif any(t in _blob for t in ("billion", "bn")) or bool(re.search(r"b", _blob)):
                                _cand_scale = "b"
                            elif "thousand" in _blob or bool(re.search(r"k", _blob)):
                                _cand_scale = "k"
                            elif "trillion" in _blob or bool(re.search(r"t", _blob)):
                                _cand_scale = "t"

                            if _schema_scale and _cand_scale and _schema_scale != _cand_scale:
                                meta["blocked_reason"] = "scale_mismatch_hard_block"
                                continue
                        except Exception:
                            pass
                        if not _has_scale_ev:
                            meta["blocked_reason"] = "scale_evidence_missing_hard_block"
                            continue
                except Exception:
                    pass
            except Exception:
                pass
            # =====================================================================
        except Exception:
            pass
            continue
        # =====================================================================
        # PATCH FIX2D17 (ADDITIVE): Reject bare-year tokens and enforce domain keyword overlap
        # Prevents year tokens like 2030 from being committed as metric values for non-year metrics,
        # and prevents unrelated snippets (e.g., "By 2030 ... sales ...") from satisfying chargers/investment metrics.
        # =====================================================================
        try:
            _fix2d17_spec = spec if isinstance(spec, dict) else {}
            _fix2d17_ckey = str(canonical_key or "")
            _fix2d17_dim = str(_fix2d17_spec.get("dimension") or "")
            _fix2d17_unitfam = str(_fix2d17_spec.get("unit_family") or "")
            _fix2d17_ut = str(_fix2d17_spec.get("unit_tag") or _fix2d17_spec.get("unit") or "")
            _fix2d17_year_metric = (
                ("year" in _fix2d17_dim.lower())
                or ("year" in _fix2d17_unitfam.lower())
                or (_fix2d17_ut.lower() in ["year", "yr", "years"])
                or (re.search(r"(?:^|_)(?:19|20)\d{2}(?:_|$)", _fix2d17_ckey) is not None)
            )

            _raw = str(c.get("raw") or "")
            try:
                _vf = float(c.get("value_norm") or 0.0)
            except Exception:
                pass
                _vf = 0.0
            _raw_digits = re.sub(r"[^0-9]", "", _raw or "")
            _looks_year = (1900.0 <= _vf <= 2100.0) and (len(_raw_digits) == 4 and _raw_digits == str(int(_vf)))

            _cand_unit = str(c.get("unit") or c.get("unit_tag") or "").strip()
            _cand_uf = str(c.get("unit_family") or "").strip().lower()

            if _looks_year and not _fix2d17_year_metric:
                # Allow only if this "year" token is clearly attached to a real unit (rare); otherwise reject.
                if (not _cand_unit) and (_cand_uf in ["", "unknown", "none"]):
                    meta["fix2d17_reject_bare_year"] = int(meta.get("fix2d17_reject_bare_year") or 0) + 1
                    continue

            # Domain keyword overlap enforcement for certain metrics to prevent cross-metric pollution.
            ctx_blob = (" " + str(c.get("context_snippet") or c.get("context") or "") + " " + _raw + " ").lower()
            # Normalize to token-ish spaces
            ctx_tok = " " + re.sub(r"[^a-z0-9]+", " ", ctx_blob) + " "

            domain_tokens = {
                "charger", "chargers", "charging", "station", "stations", "infrastructure",
                "investment", "invest", "capex", "spend", "spending",
                "sales", "sold", "market", "share", "revenue", "turnover",
                "units", "deliveries", "registrations",
            }

            kw = _fix2d17_spec.get("keywords") or []
            if isinstance(kw, (list, tuple)):
                kw_l = [str(x).lower().strip() for x in kw if str(x).strip()]
            else:
                kw_l = []
            required = [k for k in kw_l if k in domain_tokens]

            # If schema explicitly includes domain tokens, require at least one to appear in candidate context/raw.
            if required:
                if not any((" " + t + " ") in ctx_tok for t in required):
                    meta["fix2d17_reject_domain_mismatch"] = int(meta.get("fix2d17_reject_domain_mismatch") or 0) + 1
                    continue
        except Exception:
            pass

        eligible.append(c)

    meta["eligible_count"] = int(len(eligible) or 0)
    # PATCH FIX2B_TRACE_V1 (ADDITIVE): eligible candidate count
    try:
        meta["candidate_count_eligible"] = int(len(eligible))
    except Exception:
        pass

    if not eligible:
        meta["blocked_reason"] = "no_eligible_candidates_in_preferred_source" if meta["preferred_url"] else "no_eligible_candidates"
        return None, meta

    # Deterministic scoring: keyword hits + stable tie-break
    best = None
    best_tie = None
    for c in sorted(eligible, key=_cand_sort_key):
        ctx = _norm(c.get("context_snippet") or c.get("context") or c.get("context_window") or "")
        raw = _norm(c.get("raw") or "")
        hits = 0
        for k in kw_norm:
            if not k:
                continue
            if k in ctx:
                hits += 2
            if k in raw:
                hits += 1

        # Prefer anchored candidate_id/anchor_hash when anchors exist
        anchor_bonus = 0
        if isinstance(anchor, dict) and anchor:
            ah = str(anchor.get("anchor_hash") or "")
            cid = str(anchor.get("candidate_id") or "")
            if ah and str(c.get("anchor_hash") or "") == ah:
                anchor_bonus += 10
            if cid and str(c.get("candidate_id") or "") == cid:
                anchor_bonus += 10

        score = hits + anchor_bonus

        # Tie-break: higher score, then earlier occurrence, then stable sort key
        tie = (int(score), -int(c.get("start_idx") or 0), _cand_sort_key(c))
        if best is None or tie > best_tie:
            best = c
            best_tie = tie

    if best is None:
        meta["blocked_reason"] = "no_winner_after_scoring"
        return None, meta

    # =====================================================================
    # PATCH FIX2D17 (ADDITIVE): Last-mile reject if winner is a bare-year token for non-year metrics
    # =====================================================================
    try:
        _raw = str(best.get("raw") or "")
        try:
            _vf = float(best.get("value_norm") or 0.0)
        except Exception:
            pass
            _vf = 0.0
        _raw_digits = re.sub(r"[^0-9]", "", _raw or "")
        _looks_year = (1900.0 <= _vf <= 2100.0) and (len(_raw_digits) == 4 and _raw_digits == str(int(_vf)))
        _dim = str(spec.get("dimension") or "")
        _uf = str(spec.get("unit_family") or "")
        _ut = str(spec.get("unit_tag") or spec.get("unit") or "")
        _year_metric = (
            ("year" in _dim.lower()) or ("year" in _uf.lower()) or (_ut.lower() in ["year", "yr", "years"])
            or (re.search(r"(?:^|_)(?:19|20)\\d{2}(?:_|$)", str(canonical_key or "")) is not None)
        )
        _cand_unit = str(best.get("unit") or best.get("unit_tag") or "").strip()
        _cand_uf = str(best.get("unit_family") or "").strip().lower()
        if _looks_year and (not _year_metric) and (not _cand_unit) and (_cand_uf in ["", "unknown", "none"]):
            meta["blocked_reason"] = "fix2d17_reject_bare_year_best"
            meta["fix2d17_bare_year_best"] = True
            return None, meta
    except Exception:
        pass

    # Build value_range in schema units (NO double divide)
    try:
        vals = []
        for c in eligible:
            v = c.get("value_norm")
            if v is None:
                # fallback to parsing 'value'/'raw' in the unit of the candidate/spec
                try:
                    fn_parse = globals().get("_parse_num")
                    if callable(fn_parse):
                        v = fn_parse(c.get("value"), c.get("unit") or "") or fn_parse(c.get("raw"), c.get("unit") or "")
                except Exception:
                    pass
                    v = None
            if v is None:
                continue
            try:
                vals.append(float(v))
            except Exception:
                pass
        if len(vals) >= 2:
            vmin = min(vals); vmax = max(vals)
            meta["value_range"] = {"min": vmin, "max": vmax, "n": len(vals), "method": "ph2b_schema_unit_range_v2|fix2b_range4"}
            meta["range_method"] = "ph2b_schema_unit_range_v2|fix2b_range4"
    except Exception:
        pass

    out = {
        "name": spec.get("name") or spec.get("label") or canonical_key,
        "canonical_key": canonical_key,
        "value": best.get("value"),
        "unit": best.get("unit") or spec.get("unit_tag") or "",
        "value_norm": best.get("value_norm"),
        "source_url": best.get("source_url") or "",
        "anchor_hash": best.get("anchor_hash") or "",
        "candidate_id": best.get("candidate_id") or "",
        "context_snippet": best.get("context_snippet") or best.get("context") or "",
        "anchor_used": bool(isinstance(anchor, dict) and anchor and (
            (anchor.get("anchor_hash") and str(best.get("anchor_hash") or "") == str(anchor.get("anchor_hash")))
            or (anchor.get("candidate_id") and str(best.get("candidate_id") or "") == str(anchor.get("candidate_id")))
        )),
        "evidence": [{
            "source_url": best.get("source_url") or "",
            "raw": best.get("raw") or "",
            "context_snippet": (best.get("context_snippet") or best.get("context") or best.get("context_window") or "")[:400],
            "anchor_hash": best.get("anchor_hash") or "",
            "method": "analysis_canonical_selector_v1",
        }],
    }

    meta["anchor_used"] = bool(out.get("anchor_used"))
    meta["chosen_source_url"] = _ph2b_norm_url(out.get("source_url") or "")
    # =====================================================================

    # =====================================================================
    # PATCH FIX2B_TRACE_V2 (ADDITIVE): richer trace fields (NO behavior change)
    # - Adds winner_candidate_debug + would_block_reason for scaled schemas.
    # =====================================================================
    try:
        _winner = out if isinstance(out, dict) else {}
        meta["winner_candidate_debug"] = {
        "canonical_key": str(canonical_key),
        "source_url": str(_winner.get("source_url") or ""),
        "source_url_norm": _ph2b_norm_url(_winner.get("source_url") or ""),
        "candidate_id": str(_winner.get("candidate_id") or _winner.get("id") or _winner.get("anchor_hash") or ""),
        "value_norm": _winner.get("value_norm"),
        "raw": str(_winner.get("raw") or _winner.get("value") or ""),
        "unit_cmp": str(_winner.get("unit_cmp") or ""),
        "unit_family": str(_winner.get("unit_family") or ""),
        "unit_tag": str(_winner.get("unit_tag") or ""),
        "context_snippet": str(_winner.get("context_snippet") or _winner.get("context") or ""),
        }

    # "Would block" diagnostic: scaled magnitude schema but chosen candidate lacks unit evidence.
        _spec_unit = str((schema_frozen or {}).get("unit_tag") or (schema_frozen or {}).get("unit") or "")
        _spec_unit_l = _spec_unit.lower()
        _is_scaled = any(tok in _spec_unit_l for tok in ["million", "billion", "trillion", "thousand", "mn", "bn", "m ", "b ", "k "]) or (_spec_unit.strip() in ["M", "B", "K", "T"])
        _winner_unit_cmp = str(_winner.get("unit_cmp") or "")
        _winner_unit_tag = str(_winner.get("unit_tag") or "")
        _winner_raw = str(_winner.get("raw") or _winner.get("value") or "")
        _winner_has_scale = any(tok in (_winner_raw.lower()) for tok in ["million", "billion", "trillion", "thousand"]) or (_winner_unit_tag.strip().upper() in ["M", "B", "K", "T"]) or (_winner_unit_cmp.strip().upper() in ["M", "B", "K", "T", "%"])
        if _is_scaled and not _winner_has_scale and not _winner_unit_cmp:
            meta["would_block_reason"] = "unit_evidence_missing_for_scaled_schema"
        else:
            meta["would_block_reason"] = ""
    except Exception:
        pass

    # PATCH FIX2B_TRACE_V1 (ADDITIVE): emit selector trace payload
    # - Does NOT change selection; purely diagnostic.
    # =====================================================================
    try:
        meta["analysis_selector_trace_v1"] = {
            "selector_used": meta.get("selector_used"),
            "preferred_url": meta.get("preferred_url"),
            "chosen_source_url": meta.get("chosen_source_url"),
            "n_candidates_in": int(meta.get("candidate_count_in") or 0),
            "n_candidates_pref": int(meta.get("candidate_count_pref") or 0),
            "n_candidates_eligible": int(meta.get("candidate_count_eligible") or 0),
            "blocked_reason": meta.get("blocked_reason") or "",
            "anchor_used": bool(meta.get("anchor_used")),
            "would_block_reason": meta.get("would_block_reason") or "",
            "winner_candidate_debug": dict(meta.get("winner_candidate_debug") or {}) if isinstance(meta.get("winner_candidate_debug"), dict) else {},
        }
    except Exception:
        return out, meta


def rebuild_metrics_from_snapshots_analysis_canonical_v1(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """Batch rebuild using the extracted canonical selector (pure, deterministic)."""
    if not isinstance(prev_response, dict):
        return {}
    metric_schema = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )
    if not isinstance(metric_schema, dict) or not metric_schema:
        return {}

    metric_anchors = (
        prev_response.get("metric_anchors")
        or (prev_response.get("results") or {}).get("metric_anchors")
        or {}
    )
    if not isinstance(metric_anchors, dict):
        metric_anchors = {}

    # Flatten candidates from snapshots
    if isinstance(baseline_sources_cache, dict) and isinstance(baseline_sources_cache.get("snapshots"), list):
        sources = baseline_sources_cache.get("snapshots", [])
    elif isinstance(baseline_sources_cache, list):
        sources = baseline_sources_cache
    else:
        sources = []

    candidates = []
    for s in sources:
        if not isinstance(s, dict):
            continue
        url = s.get("source_url") or s.get("url") or ""
        xs = s.get("extracted_numbers")
        if isinstance(xs, list) and xs:
            for c in xs:
                if not isinstance(c, dict):
                    continue
                c2 = dict(c)
                if url and not c2.get("source_url"):
                    c2["source_url"] = url
                candidates.append(c2)

    # =====================================================================
    # PATCH FIX2S_APPLY_RULES_IN_ANALYSIS_CANONICAL_REBUILD (ADDITIVE)
    # =====================================================================
    try:
        _fix2s_apply_observed_to_canonical_rules_v1(candidates, metric_schema, web_context=web_context)
    except Exception:
        pass
    # =====================================================================
    # END PATCH FIX2S_APPLY_RULES_IN_ANALYSIS_CANONICAL_REBUILD
    # =====================================================================
    def _norm(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def _cand_sort_key(c: dict):
        try:
            return (
                str(c.get("anchor_hash") or ""),
                str(c.get("source_url") or ""),
                int(c.get("start_idx") or 0),
                str(c.get("raw") or ""),
                str(c.get("unit") or ""),
                float(c.get("value_norm") or 0.0),
            )
        except Exception:
            return ("", "", 0, "", "", 0.0)

    # =====================================================================
    # PATCH FIX2D16 (ADD): Disable FIX2D15 gating; rely on last-mile bare-year reject instead
    _fix2d16_disable_fix2d15 = False
    # PATCH FIX2D17: Deprecate FIX2D16 soft-match/year guards
    _fix2d17_disable_fix2d16 = True

    # (Legacy block retained for context but disabled)
    # PATCH FIX2D15 (ADD): Harden schema-only rebuild candidate eligibility
    # Goals:
    #   1) Prevent bare-year tokens (e.g., 2030) from being selected as metric values
    #      when the metric does NOT explicitly expect a year-as-value.
    #   2) Prevent cross-metric pollution where a sales/market snippet satisfies
    #      unrelated schema metrics (e.g., chargers, charging investment).
    #   3) Enforce light unit-family expectations (percent / currency).
    # Notes:
    #   - This patch intentionally supersedes (and replaces) FIX2D15's earlier guard.
    #   - Keep changes local to schema_only_rebuild_fix17 selection.
    # =====================================================================

    def _fix2d15_expects_year_value(spec: dict, canonical_key: str) -> bool:
        try:
            unit_hint = str(spec.get("unit_tag") or spec.get("unit") or "").lower()
            dim_hint = str(spec.get("dimension") or spec.get("value_type") or "").lower()
            if dim_hint == "year":
                return True
            if "year" in unit_hint:
                return True
            ck = str(canonical_key or "")
            if ck.endswith("__year") or ("_year_" in ck):
                return True
            return False
        except Exception:
            return False

    def _fix2d15_is_bare_year_token(cand: dict) -> bool:
        try:
            raw = str(cand.get("raw") or cand.get("value") or "").strip()
            v = cand.get("value_norm")
            try:
                vf = float(v)
            except Exception:
                pass
                try:
                    vf = float(re.sub(r"[^0-9\.\-]+", "", raw or ""))
                except Exception:
                    return False

            if vf < 1900 or vf > 2100:
                return False

            # Accept raw like "2030" and "2030.0" (and "2030.00") as year tokens
            looks_year = False
            try:
                if re.fullmatch(r"\d{4}", raw or ""):
                    looks_year = True
                elif re.fullmatch(r"\d{4}\.0+", raw or ""):
                    looks_year = True
            except Exception:
                pass
                looks_year = False

            if not looks_year:
                try:
                    iv = int(vf)
                    looks_year = (abs(vf - iv) < 1e-6) and (1900 <= iv <= 2100)
                except Exception:
                    pass
                    looks_year = False

            if not looks_year:
                return False

            unit = str(cand.get("unit") or cand.get("unit_tag") or "").strip()
            if unit:
                return False
            if "%" in raw or "$" in raw or "€" in raw or "£" in raw:
                return False

            return True
        except Exception:
            return False


    def _fix2d15_metric_domain_tokens(canonical_key: str, spec: dict) -> list:
        """Derive a small set of domain tokens from canonical_key/name."""
        try:
            ck = _norm(str(canonical_key or ""))
            nm = _norm(str(spec.get("name") or spec.get("metric_name") or ""))
            base = " ".join([ck, nm]).strip()
            toks = [w for w in base.split() if w and not re.fullmatch(r"\d{4}", w)]
            stop = set(["global","world","worldwide","total","overall","ev","electric","vehicle","vehicles","metric","market"])
            out = []
            for w in toks:
                if w in stop:
                    continue
                if len(w) < 4:
                    continue
                if w not in out:
                    out.append(w)
            return out[:6]
        except Exception:
            return []

    # =====================================================================
    # PATCH FIX2D19 (ADD): Required domain token binding
    # - Prevent generic keyword hits (e.g., 'global', 'market') from allowing
    #   unrelated candidates (e.g., year tokens) to satisfy domain-specific
    #   metrics like chargers/investment.
    # - For certain metrics, require at least one strong domain token to
    #   appear in candidate context/raw.
    # =====================================================================
    def _fix2d19_required_domain_tokens(canonical_key: str, spec: dict) -> list:
        try:
            if bool(globals().get("_FIX2D20_DISABLE_FIX2D19", False)):
                return []
            ck = str(canonical_key or '').lower()
            dim = str(spec.get('dimension') or spec.get('value_type') or '').lower()
            uf = str(spec.get('unit_family') or '').lower()
            req = []
            if 'charger' in ck or 'charging' in ck:
                req += ['charger', 'charging', 'station', 'infrastructure']
            if 'investment' in ck or 'capex' in ck or dim == 'currency' or uf == 'currency':
                # still allow currency cues to satisfy, but require at least one semantic token
                req += ['invest', 'investment', 'capex', 'spend', 'spending', 'cost']
            if 'share' in ck or dim == 'percent' or uf == 'percent':
                req += ['share', 'market share', 'ev share']
            if 'sale' in ck or 'sales' in ck or dim in ('unit_sales','sales','units') or uf in ('unit_sales','sales','units'):
                req += ['sales', 'sold', 'deliveries', 'deliver']
            # de-dupe while keeping order
            out=[]
            for r in req:
                r=str(r).strip().lower()
                if r and r not in out:
                    out.append(r)
            return out[:8]
        except Exception:
            return []




    # =====================================================================
    # PATCH FIX2D22 (ADD): Eligibility-before-scoring gate for schema-only rebuild
    # - This is the decisive fix: reject invalid candidates *before* ranking.
    # - Prevents bare year tokens (e.g., 2024/2030/2030.0) from ever being
    #   eligible evidence for non-year metrics.
    # - Enforces unit-family requirements and required domain-token binding.
    # =====================================================================
    def _fix2d22_candidate_eligible(cand: dict, spec: dict, canonical_key: str, kw_norm: list) -> (bool, str):
        try:
            # 1) Hard bare-year rejection unless metric explicitly expects year-as-value
            if _fix2d15_is_bare_year_token(cand) and not _fix2d15_expects_year_value(spec, canonical_key):
                return False, 'bare_year_token'

            # 2) Unit-family enforcement (percent/currency/unit_sales)
            if not _fix2d15_unit_family_ok(cand, spec):
                return False, 'unit_family_mismatch'

            # 3) Required domain token binding (strong)
            ctx = _norm(cand.get('context_snippet') or cand.get('context') or cand.get('context_window') or '')
            rawn = _norm(cand.get('raw') or '')

            # PATCH FIX2D2V (ADD): require explicit year tokens from canonical_key to appear locally
            try:
                _yrs = re.findall(r"\b(19\d{2}|20\d{2})\b", str(canonical_key or ""))
                if _yrs:
                    _hit_all = True
                    for _y in _yrs:
                        if _y and (_y not in ctx) and (_y not in rawn):
                            _hit_all = False
                            break
                    if not _hit_all:
                        return False, 'missing_required_year_token'
            except Exception:
                pass


            req_dom = _fix2d19_required_domain_tokens(canonical_key, spec)
            if req_dom:
                hit = 0
                for r in req_dom:
                    rr = _norm(r)
                    if rr and (rr in ctx or rr in rawn):
                        hit += 1
                        break
                if hit <= 0:
                    return False, 'missing_required_domain_token'

            # 4) If schema provides keywords/domain hints, require at least one hit
            dom = _fix2d15_metric_domain_tokens(canonical_key, spec)
            if (kw_norm or dom):
                hit_kw = 0
                for k in (kw_norm or []):
                    if k and (k in ctx or k in rawn):
                        hit_kw += 1
                        break
                hit_dom = 0
                for d in (dom or []):
                    if d and (d in ctx or d in rawn):
                        hit_dom += 1
                        break
                if hit_kw <= 0 and hit_dom <= 0:
                    return False, 'no_keyword_or_domain_hits'

            return True, ''
        except Exception:
            return True, ''

    def _fix2d15_unit_family_ok(cand: dict, spec: dict) -> bool:
        try:
            dim = str(spec.get("dimension") or "").lower()
            uf = str(spec.get("unit_family") or "").lower()
            raw = str(cand.get("raw") or "")
            unit = str(cand.get("unit") or cand.get("unit_tag") or "").lower()

            s = (raw + " " + unit).lower()
            has_currency = ("$" in raw) or ("us$" in s) or ("usd" in s) or ("eur" in s) or ("gbp" in s) or ("€" in raw) or ("£" in raw)
            has_percent = ("%" in raw) or ("percent" in s) or ("pct" in s)

            if dim == "percent" or uf == "percent":
                return has_percent
            if dim == "currency" or uf == "currency":
                return has_currency

            # FIX2D18: unit-sales expectations (prevents bare years like 2030 from winning)
            if not bool(globals().get("_FIX2D20_DISABLE_FIX2D18", False)):
                if dim in ("unit_sales","units","sales") or uf in ("unit_sales","units","sales"):
                    # require some unit cue
                    has_units = ("unit" in s) or ("units" in s) or ("million" in s) or ("mn" in s) or ("m " in s) or (" m" in s)
                    if not has_units:
                        return False

            return True
        except Exception:
            return True

    def _fix2d15_candidate_ok(cand: dict, spec: dict, canonical_key: str, kw_norm: list) -> (bool, str):
        try:
            if _fix2d15_is_bare_year_token(cand) and not _fix2d15_expects_year_value(spec, canonical_key):
                return False, "bare_year_token"

            if not _fix2d15_unit_family_ok(cand, spec):
                return False, "unit_family_mismatch"

            ctx = _norm(cand.get("context_snippet") or cand.get("context") or cand.get("context_window") or "")
            rawn = _norm(cand.get("raw") or "")
            dom = _fix2d15_metric_domain_tokens(canonical_key, spec)

            # FIX2D19: strong required domain-token binding
            req_dom = _fix2d19_required_domain_tokens(canonical_key, spec)
            if req_dom:
                _req_hit = 0
                for r in req_dom:
                    if not r:
                        continue
                    rr = _norm(r)
                    if rr and (rr in ctx or rr in rawn):
                        _req_hit += 1
                if _req_hit <= 0:
                    return False, 'missing_required_domain_token'


            hit_kw = 0
            for k in (kw_norm or []):
                if k and (k in ctx or k in rawn):
                    hit_kw += 1

            hit_dom = 0
            for d in (dom or []):
                if d and (d in ctx or d in rawn):
                    hit_dom += 1

            if (kw_norm or dom) and (hit_kw <= 0 and hit_dom <= 0):
                return False, "no_keyword_or_domain_hits"

            return True, ""
        except Exception:
            return True, ""

    candidates.sort(key=_cand_sort_key)

    # Debug sink
    dbg = prev_response.setdefault("_evolution_rebuild_debug", {})
    dbg.setdefault("schema_only_zero_hit_metrics_fix17", [])
    # PATCH FIX2D15 (ADD): diagnostics for schema-only gating
    dbg.setdefault("fix2d15_reject_reasons", {})
    dbg.setdefault("fix2d22_reject_reasons", {})
    dbg.setdefault("fix2d22_year_reject_samples", [])
    dbg.setdefault("fix2d15_year_reject_samples", [])

    rebuilt = {}

    for canonical_key, sch in metric_schema.items():
        if not isinstance(sch, dict):
            continue

        spec = dict(sch)
        spec.setdefault("canonical_key", canonical_key)
        spec.setdefault("name", sch.get("name") or canonical_key)

        # Use fix16 year-like & keyword pruning if available
        metric_is_year_like = _fix17_metric_is_year_like(spec, canonical_key=canonical_key)

        keywords = sch.get("keywords") or sch.get("keyword_hints") or []
        if isinstance(keywords, str):
            keywords = [keywords]

        fn_prune = globals().get("_fix16_prune_year_keywords")
        if callable(fn_prune):
            keywords2 = fn_prune(list(keywords), metric_is_year_like)
        else:
            keywords2 = list(keywords)

        kw_norm = [_norm(k) for k in (keywords2 or []) if k]

        best = None
        best_tie = None
        best_hits = 0

        # =====================================================================
        # PATCH FIX2D2S (ADD): Prefer non-year candidates when available
        # Motivation:
        #   schema_only_rebuild can still end up selecting a bare year token (e.g. 2024/2026)
        #   when that token appears in the same snippet as the real metric value.
        # Policy:
        #   If the schema does NOT expect a year-as-value and there exists at least one eligible
        #   non-year candidate in the pool for this canonical_key, then exclude bare-year tokens
        #   from competition for this canonical_key.
        # Notes:
        #   - This is a local, deterministic filter applied BEFORE keyword scoring.
        #   - It does not weaken year-blocking; it strengthens selection parity.
        # =====================================================================
        _fix2d2s_expect_year = False
        try:
            _fix2d2s_expect_year = bool(_fix2d15_expects_year_value(spec, str(canonical_key)))
        except Exception:
            pass
            _fix2d2s_expect_year = False

        _fix2d2s_eligible = []
        _fix2d2s_has_non_year = False
        for _c in (candidates or []):
            ok, _reason = _fix17_candidate_allowed_with_reason(_c, spec, canonical_key=canonical_key)
            if not ok:
                continue
            # PATCH FIX2D2U: shared semantic eligibility gate (local required tokens)
            try:
                # FIX2D63: bugfix - use the correct candidate variable (_c), not the outer loop's c.
                _ok_u, _why_u = _fix2d2u_semantic_eligible(_c, spec, canonical_key=str(canonical_key))
                if not _ok_u:
                    continue
            except Exception:
                pass

            # FIX2D63: upstream reject yearlike numeric tokens for unit/count metrics
            # unless the candidate carries unit evidence (prevents YTD 2025 headings from hijacking values).
            try:
                if _fix2d63_schema_expects_unit_or_count(str(canonical_key), spec):
                    if _fix2d63_is_yearlike_value(_c) and (not _fix2d63_has_unit_evidence(_c)):
                        dbg.setdefault('fix2d63_reject_yearlike_no_unit_evidence', 0)
                        dbg['fix2d63_reject_yearlike_no_unit_evidence'] = int(dbg.get('fix2d63_reject_yearlike_no_unit_evidence') or 0) + 1
                        continue
            except Exception:
                pass
            try:
                _ok2, _why2 = _fix2d22_candidate_eligible(_c, spec, canonical_key=str(canonical_key), kw_norm=kw_norm)
                if not _ok2:
                    dbg.setdefault('fix2d22_reject_reasons', {})
                    dbg['fix2d22_reject_reasons'][_why2] = int(dbg['fix2d22_reject_reasons'].get(_why2) or 0) + 1
                    if _why2 == 'bare_year_token':
                        dbg.setdefault('fix2d22_year_reject_samples', [])
                        if len(dbg.get('fix2d22_year_reject_samples') or []) < 20:
                            dbg['fix2d22_year_reject_samples'].append({
                                'canonical_key': str(canonical_key),
                                'raw': str(_c.get('raw') or _c.get('value') or '')[:80],
                                'value_norm': _c.get('value_norm'),
                                'unit': str(_c.get('unit') or ''),
                                'source_url': str(_c.get('source_url') or '')[:120],
                            })
                    continue
            except Exception:
                pass

            _fix2d2s_eligible.append(_c)
            try:
                if (not _fix2d2s_expect_year) and (not _fix2d15_is_bare_year_token(_c)):
                    _fix2d2s_has_non_year = True
            except Exception:
                pass

        if _fix2d2s_eligible and _fix2d2s_has_non_year and (not _fix2d2s_expect_year):
            try:
                dbg.setdefault('fix2d2s_filtered_bare_year_tokens', 0)
                _before = len(_fix2d2s_eligible)
                _fix2d2s_eligible = [x for x in _fix2d2s_eligible if not _fix2d15_is_bare_year_token(x)]
                dbg['fix2d2s_filtered_bare_year_tokens'] = int(dbg.get('fix2d2s_filtered_bare_year_tokens') or 0) + int(max(0, _before - len(_fix2d2s_eligible)))
            except Exception:
                pass
        # =====================================================================
        # END PATCH FIX2D2S
        # =====================================================================

        for c in (_fix2d2s_eligible or []):
            ok, _reason = _fix17_candidate_allowed_with_reason(c, spec, canonical_key=canonical_key)
            if not ok:
                continue

            # PATCH FIX2D2U: semantic eligibility gate (local snippet required tokens)
            try:
                _ok_u, _why_u = _fix2d2u_semantic_eligible(c, spec, str(canonical_key))
                if not _ok_u:
                    dbg.setdefault('fix2d2u_reject_reasons_fix17', {})
                    dbg['fix2d2u_reject_reasons_fix17'][_why_u] = int(dbg['fix2d2u_reject_reasons_fix17'].get(_why_u) or 0) + 1
                    continue
            except Exception:
                pass            # PATCH FIX2D22 (ADD): eligibility-before-scoring gate
            try:
                _ok2, _why2 = _fix2d22_candidate_eligible(c, spec, canonical_key=str(canonical_key), kw_norm=kw_norm)
                if not _ok2:
                    dbg.setdefault("fix2d22_reject_reasons", {})
                    dbg["fix2d22_reject_reasons"][_why2] = int(dbg["fix2d22_reject_reasons"].get(_why2) or 0) + 1
                    if _why2 == 'bare_year_token':
                        dbg.setdefault("fix2d22_year_reject_samples", [])
                        if len(dbg.get("fix2d22_year_reject_samples") or []) < 20:
                            dbg["fix2d22_year_reject_samples"].append({
                                'canonical_key': str(canonical_key),
                                'raw': str(c.get('raw') or c.get('value') or '')[:80],
                                'value_norm': c.get('value_norm'),
                                'unit': str(c.get('unit') or ''),
                                'source_url': str(c.get('source_url') or '')[:120],
                            })
                    continue
            except Exception:
                pass

            ctx = _norm(c.get("context_snippet") or c.get("context") or c.get("context_window") or "")
            raw = _norm(c.get("raw") or "")

            hits = 0
            for k in kw_norm:
                if k and (k in ctx or k in raw):
                    hits += 1

            tie = (-hits,) + _cand_sort_key(c)
            if best is None or tie < best_tie:
                best = c
                best_tie = tie
                best_hits = hits

        if not isinstance(best, dict):
            continue

        # If schema has keywords, require at least one hit.
        if kw_norm and best_hits <= 0:
            dbg["schema_only_zero_hit_metrics_fix17"].append({"canonical_key": canonical_key, "reason": "no_keyword_hits"})
            continue

        # =====================================================================
        # PATCH FIX2D16 (ADD): Last-mile bare-year rejection for schema-only promotions
        # Even if earlier eligibility gates miss it, prevent committing a pure year
        # (e.g., 2030) as the metric value for non-year metrics.
        # =====================================================================
        try:
            if _fix2d15_is_bare_year_token(best) and not _fix2d15_expects_year_value(spec, str(canonical_key)):
                dbg.setdefault("fix2d16_rejected_bare_year_commit", 0)
                dbg["fix2d16_rejected_bare_year_commit"] = int(dbg.get("fix2d16_rejected_bare_year_commit") or 0) + 1
                dbg.setdefault("fix2d16_rejected_bare_year_samples", [])
                if len(dbg.get("fix2d16_rejected_bare_year_samples") or []) < 20:
                    dbg["fix2d16_rejected_bare_year_samples"].append({
                        "canonical_key": str(canonical_key),
                        "raw": str(best.get("raw") or best.get("value") or "")[:80],
                        "value_norm": best.get("value_norm"),
                        "source_url": str(best.get("source_url") or "")[:120],
                    })
                continue
        except Exception:
            pass
        # =====================================================================


        rebuilt[canonical_key] = {
            "canonical_key": canonical_key,
            "name": spec.get("name") or canonical_key,
            "value": best.get("value"),
            "unit": best.get("unit") or "",
            "value_norm": best.get("value_norm"),
            "source_url": best.get("source_url") or "",
            "anchor_hash": best.get("anchor_hash") or "",
            "evidence": [{
                "source_url": best.get("source_url") or "",
                "raw": best.get("raw") or "",
                "context_snippet": (best.get("context_snippet") or best.get("context") or best.get("context_window") or "")[:400],
                "anchor_hash": best.get("anchor_hash") or "",
                "method": "schema_only_rebuild_fix17",
            }],
            "anchor_used": False,
        }

    return rebuilt


# =====================================================================
# PATCH FIX2D2U (ADD): Shared semantic eligibility gate (Analysis parity)
# - Uses LOCAL context_snippet (not page-wide context_window)
# - Prevents cross-metric pollution in schema-only rebuild and other projections
# - Called by BOTH Analysis selector and Evolution rebuild paths
# =====================================================================

_FIX2D2U_ENABLE = True

def _fix2d2u_norm(s: str) -> str:
    try:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()
    except Exception:
        return ""

def _fix2d2u_local_text(cand: dict) -> str:
    try:
        if not isinstance(cand, dict):
            return ""
        for k in ("context_snippet", "context", "context_window", "context_window_raw", "context_window_text"):
            v=cand.get(k)
            if isinstance(v, str) and v.strip():
                return v
        return str(cand.get("raw") or cand.get("value") or "")
    except Exception:
        return ""

def _fix2d2u_required_token_groups(canonical_key: str, spec: dict) -> list:
    """Return a list of OR-groups; each group must have at least one hit in local text."""
    try:
        ck = str(canonical_key or "").lower()
        nm = str((spec or {}).get("name") or (spec or {}).get("metric_name") or "").lower()
        uf = str((spec or {}).get("unit_family") or "").lower()
        dim = str((spec or {}).get("dimension") or (spec or {}).get("value_type") or "").lower()
        base = ck + " " + nm

        groups = []

        # Geo cues
        if "china" in base or re.search(r"\bcn\b", base):
            groups.append(["china", "chinese"])

        # Time cues: if canonical key/name includes a year, require it locally
        years = re.findall(r"\b(19\d{2}|20\d{2})\b", base)
        for y in years[:2]:
            groups.append([y])

        # Metric family cues
        if ("charger" in base) or ("charging" in base):
            groups.append(["charger", "chargers", "charging", "station", "stations", "infrastructure"])
        if ("investment" in base) or ("capex" in base) or ("spend" in base) or (uf == "currency") or (dim == "currency"):
            groups.append(["investment", "invest", "capex", "spend", "spending", "cost", "expenditure"])
        if ("share" in base) or (uf == "percent") or (dim == "percent"):
            groups.append(["market share", "share", "%", "percent"])
        if ("sale" in base) or ("sales" in base) or (uf in ("unit_sales", "sales", "units")) or (dim in ("unit_sales", "sales", "units")):
            groups.append(["sales", "sold", "deliveries", "registrations", "units"])
        if "ytd" in base:
            groups.append(["ytd", "year to date"])

        # De-dupe groups
        out=[]
        for g in groups:
            gg=[]
            for t in g:
                tt=str(t or "").strip().lower()
                if tt and tt not in gg:
                    gg.append(tt)
            if gg:
                out.append(gg)
        return out
    except Exception:
        return []

def _fix2d2u_semantic_eligible(cand: dict, spec: dict, canonical_key: str) -> tuple:
    try:
        if not bool(globals().get("_FIX2D2U_ENABLE", True)):
            return True, ""
        groups = _fix2d2u_required_token_groups(canonical_key, spec)
        if not groups:
            return True, ""
        txt = _fix2d2u_local_text(cand)
        blob = _fix2d2u_norm(txt + " " + str(cand.get('raw') or ''))
        for g in groups:
            hit=False
            for tok in g:
                tn=_fix2d2u_norm(tok)
                if tn and tn in blob:
                    hit=True
                    break
            if not hit:
                return False, "missing_required_tokens"
        return True, ""
    except Exception:
        return True, ""

# =====================================================================
# END PATCH FIX2D2U
# =====================================================================


# =====================================================================
# PATCH FIX17 (ADDITIVE): wire FIX17 rebuilds into the existing dispatch
# - Keep names identical so evolution uses these as the LAST definitions
# =====================================================================

def rebuild_metrics_from_snapshots_with_anchors(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:  # noqa: F811
    return rebuild_metrics_from_snapshots_with_anchors_fix17(prev_response, baseline_sources_cache, web_context=web_context)



# =====================================================================
# END PATCH FIX17
# =====================================================================



# =====================================================================
# PATCH FIX18 (ADDITIVE): Anchor-authoritative rebuild (no schema fallback)
# Goal:
#   - If analysis emitted an anchor for a canonical metric, evolution MUST NOT
#     fall back to schema-only selection when the anchor cannot be used.
#   - This closes the final loophole where unitless year tokens win schema-only
#     scoring after an anchor rejection.
# Implementation:
#   - Provide a FIX18 schema-only rebuild that:
#       (1) rebuilds anchored metrics via rebuild_metrics_from_snapshots_with_anchors_fix17
#       (2) rebuilds ONLY unanchored metrics via rebuild_metrics_from_snapshots_schema_only_fix17
#       (3) merges results deterministically (anchored first)
#   - Re-wire the public rebuild_metrics_from_snapshots_schema_only to FIX18.
# Notes:
#   - Fully deterministic; no refetch; no LLM.
#   - Additive only: leaves FIX17 implementations intact.
# =====================================================================

def rebuild_metrics_from_snapshots_schema_only_fix18(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:
    """
    FIX18 rebuild:
      - Anchored metrics: ONLY from anchor-aware rebuild (fix17), or skipped if rejected.
      - Unanchored metrics: schema-only rebuild (fix17) as before.
      - Never allows schema-only fallback to fill an anchored canonical_key.

    FIX2D21 add:
      - In Evolution baseline-compare mode, derive the schema keyspace from the Analysis baseline
        (previous_data.results.primary_metrics_canonical) so schema-only rebuild targets baseline keys.
    """
    if not isinstance(prev_response, dict):
        return {}

    # =====================================================================
    # PATCH FIX2D21 (ADD): derive metric_schema_frozen from analysis baseline keys when available
    # - This makes Evolution naturally produce current values for the baseline keyspace, enabling
    #   Analysis -> Evolution deltas without requiring injected URLs during Analysis.
    # =====================================================================
    try:
        _fix2d21_prev_pmc = None
        if isinstance(prev_response.get("results"), dict):
            _fix2d21_prev_pmc = prev_response.get("results", {}).get("primary_metrics_canonical")
        if not isinstance(_fix2d21_prev_pmc, dict):
            _fix2d21_prev_pmc = prev_response.get("primary_metrics_canonical")

        if isinstance(_fix2d21_prev_pmc, dict) and _fix2d21_prev_pmc:
            ms_from_baseline = {}
            for ck, mo in _fix2d21_prev_pmc.items():
                if not isinstance(ck, str) or not ck:
                    continue
                spec = {}
                if isinstance(mo, dict):
                    # carry minimal hints to help selection
                    ut = mo.get("unit_tag") or mo.get("unit") or ""
                    uf = mo.get("unit_family") or ""
                    dim = mo.get("dimension") or mo.get("value_type") or ""
                    if not uf and isinstance(ut, str) and "%" in ut:
                        uf = "percent"
                    if not dim and uf:
                        dim = uf
                    spec = {
                        "name": mo.get("name") or mo.get("metric_name") or ck,
                        "unit_tag": ut,
                        "unit_family": uf,
                        "dimension": dim,
                    }
                ms_from_baseline[ck] = spec

            # only override if we got something non-trivial
            if ms_from_baseline:
                prev_response = dict(prev_response)
                prev_response["metric_schema_frozen"] = ms_from_baseline
    except Exception:
        pass
    # =====================================================================
    # END PATCH FIX2D21
    # =====================================================================
# Anchored part (authoritative when present)
    fn_anchor = globals().get("rebuild_metrics_from_snapshots_with_anchors_fix17")
    anchored = fn_anchor(prev_response, baseline_sources_cache, web_context=web_context) if callable(fn_anchor) else {}

    # Identify anchored keys (only those with a concrete anchor_hash)
    metric_anchors_any = globals().get("_get_metric_anchors_any")
    metric_anchors = metric_anchors_any(prev_response) if callable(metric_anchors_any) else (
        prev_response.get("metric_anchors") or {}
    )
    anchored_keys = set()
    try:
        for k, a in (metric_anchors or {}).items():
            if isinstance(a, dict) and (a.get("anchor_hash") or a.get("anchor")):
                anchored_keys.add(k)
    except Exception:
        pass
        anchored_keys = set()

    # Build a shallow prev_response copy where schema-only sees ONLY unanchored metrics
    pr2 = dict(prev_response)
    ms = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or {}
    )
    if isinstance(ms, dict) and anchored_keys:
        ms2 = {k: v for k, v in ms.items() if k not in anchored_keys}
        pr2["metric_schema_frozen"] = ms2

    # Unanchored part
    fn_schema = globals().get("rebuild_metrics_from_snapshots_schema_only_fix17")
    unanchored = fn_schema(pr2, baseline_sources_cache, web_context=web_context) if callable(fn_schema) else {}

    # Deterministic merge: anchored first, then unanchored
    rebuilt = {}
    if isinstance(anchored, dict):
        rebuilt.update(anchored)
    if isinstance(unanchored, dict):
        for k, v in unanchored.items():
            if k not in rebuilt:
                rebuilt[k] = v
    return rebuilt


# Re-wire schema-only entrypoint to FIX18 (keep names identical for evolution dispatch)
def rebuild_metrics_from_snapshots_schema_only(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:  # noqa: F811
    return rebuild_metrics_from_snapshots_schema_only_fix18(prev_response, baseline_sources_cache, web_context=web_context)

# =====================================================================
# END PATCH FIX18
# =====================================================================


# ==============================================================================
# PATCH FIX24 (ADDITIVE): Sheets-first replay for unchanged sources+data, and
# scrape+hash gate for evolution to prevent any rebuild/picking when unchanged.
#
# Goals:
#   1) Evolution ALWAYS performs a current scrape/fetch pass to compute a "current"
#      snapshot hash (v2 preferred).
#   2) If current hash == prior analysis hash (v2 preferred), evolution stops and
#      replays the prior FULL analysis payload rehydrated from Google Sheets,
#      publishing metrics to the dashboard WITHOUT any metric rebuild/selection.
#   3) If hashes differ, evolution proceeds via the existing deterministic path
#      (same rebuild/anchors logic as used elsewhere in this codebase).
#
# This patch is purely additive:
#   - Preserves original run_source_anchored_evolution as run_source_anchored_evolution_BASE
#   - Adds helper functions prefixed _fix24_*
#   - Overrides run_source_anchored_evolution by re-defining it below
# ==============================================================================

try:
    run_source_anchored_evolution_BASE = run_source_anchored_evolution  # type: ignore
except Exception:
    pass
    run_source_anchored_evolution_BASE = None  # type: ignore


def _fix24_get_prev_full_payload(previous_data: dict) -> dict:
    """
    Load the FULL prior analysis payload from Google Sheets if possible.
    Falls back to previous_data if already full.
    """
    try:
        if not isinstance(previous_data, dict):
            return {}
        # If it already looks like a full payload (contains canonical metrics), return as-is
        if isinstance(previous_data.get("primary_metrics_canonical"), dict) and previous_data["primary_metrics_canonical"]:
            try:
                previous_data = _fix2d73_promote_rehydrated_prevdata_v1(previous_data)
            except Exception:
                pass
            return previous_data

        # Preferred: explicit snapshot_store_ref / full_store_ref
        ref = previous_data.get("full_store_ref") or previous_data.get("snapshot_store_ref") or ""
        # Fallback: sheet id
        if (not ref) and isinstance(previous_data.get("_sheet_id"), str) and previous_data.get("_sheet_id"):
            # Assume HistoryFull
            ref = f"gsheet:HistoryFull:{previous_data.get('_sheet_id')}"

        if isinstance(ref, str) and ref.startswith("gsheet:"):
            parts = ref.split(":")
            ws_title = parts[1] if len(parts) > 1 and parts[1] else "HistoryFull"
            aid = parts[2] if len(parts) > 2 else ""
            if aid:
                fn = globals().get("load_full_history_payload_from_sheet")
                if callable(fn):
                    full = fn(aid, worksheet_title=ws_title)
                    if isinstance(full, dict) and full:
                        try:
                            full = _fix2d73_promote_rehydrated_prevdata_v1(full)
                        except Exception:
                            pass
                        return full
    except Exception:
        return previous_data if isinstance(previous_data, dict) else {}

    return previous_data if isinstance(previous_data, dict) else {}


def _fix24_extract_source_urls(prev_full: dict) -> list:
    """
    Determine the URL list to fetch for current-hash computation.
    Uses analysis 'sources' if available, else URLs from baseline_sources_cache.
    """
    urls = []
    try:
        if isinstance(prev_full, dict):
            s = prev_full.get("sources")
            if isinstance(s, list) and s:
                urls = [str(u) for u in s if isinstance(u, str) and u.strip()]
            if not urls:
                # Try results.source_results urls
                r = prev_full.get("results") if isinstance(prev_full.get("results"), dict) else {}
                sr = r.get("source_results") if isinstance(r, dict) else None
                if isinstance(sr, list):
                    for item in sr:
                        if isinstance(item, dict):
                            u = item.get("url") or item.get("source_url")
                            if u:
                                urls.append(str(u))
            if not urls:
                # Try baseline_sources_cache urls
                r = prev_full.get("results") if isinstance(prev_full.get("results"), dict) else {}
                bsc = None
                if isinstance(r, dict):
                    bsc = r.get("baseline_sources_cache")
                if not isinstance(bsc, list):
                    bsc = prev_full.get("baseline_sources_cache")
                if isinstance(bsc, list):
                    for item in bsc:
                        if isinstance(item, dict):
                            u = item.get("source_url") or item.get("url")
                            if u:
                                urls.append(str(u))
    except Exception:
        pass

    # Stable de-dupe order
    seen = set()
    out = []
    for u in urls:
        uu = (u or "").strip()
        if not uu or uu in seen:
            continue
        seen.add(uu)
        out.append(uu)
    return out[:25]


def _fix24_build_scraped_meta(urls: list, max_chars_per_source: int = 180000) -> dict:
    """
    Fetch each URL (deterministically) and return scraped_meta in the same shape
    attach_source_snapshots_to_analysis expects: {url: {"status":..., "text":..., "extracted_numbers":[...]}}
    """
    # PATCH FIX2AF_URL_SHAPE_NORMALIZER_AND_SCRAPE_LEDGER_V1 (ADDITIVE)
    _fix2af_ledger = globals().get("_fix2af_last_scrape_ledger")
    try:
        _fix2af_norm_urls, _fix2af_norm_diag = _fix2af_normalize_url_items(urls)
        urls = _fix2af_norm_urls
        if isinstance(_fix2af_ledger, dict):
            _fix2af_ledger["__fix2af_url_normalize_diag__"] = _fix2af_norm_diag
    except Exception:
        pass
    # END PATCH FIX2AF_URL_SHAPE_NORMALIZER_AND_SCRAPE_LEDGER_V1

    scraped_meta = {}
    fetch_fn = globals().get("fetch_url_content_with_status") or globals().get("fetch_url_content")
    extract_fn = globals().get("extract_numbers_with_context")

    for u in urls or []:
        url = str(u or "").strip()
        # PATCH FIX2AF_SCRAPE_LEDGER_ATTEMPTED_V1 (ADDITIVE)
        try:
            _fix2af_ledger_put(_fix2af_ledger, url, stage="attempted", reason="entered_loop")
        except Exception:
            pass
        # END PATCH FIX2AF_SCRAPE_LEDGER_ATTEMPTED_V1
        if not url:
            continue
        try:
            if callable(fetch_fn) and fetch_fn.__name__.endswith("_with_status"):
                text, status = fetch_fn(url)
            elif callable(fetch_fn):
                text = fetch_fn(url)
                status = "success_direct" if (text and str(text).strip()) else "empty"
            else:
                text, status = (None, "no_fetch_fn")

            # PATCH FIX2AF_SCRAPED_TEXT_ACCESSOR_V1 (ADDITIVE)
            txt = _fix2af_scraped_text_accessor(text)
            # END PATCH FIX2AF_SCRAPED_TEXT_ACCESSOR_V1
            if max_chars_per_source and len(txt) > int(max_chars_per_source):
                txt = txt[: int(max_chars_per_source)]

            # PATCH FIX2AF_FETCH_FAILURE_VISIBILITY_V1 (ADDITIVE)
            try:
                _fix2af_fail_class = _fix2af_classify_fetch_failure(status, txt)
                _fix2af_ledger_put(_fix2af_ledger, url, stage="fetched", reason=_fix2af_fail_class, extra={"status": status, "text_len": len(txt or "")})
            except Exception:
                pass
            # END PATCH FIX2AF_FETCH_FAILURE_VISIBILITY_V1

            nums = []
            if callable(extract_fn) and txt.strip():
                try:
                    nums = extract_fn(txt, source_url=url)
                    if nums is None:
                        nums = []
                except Exception:
                    pass
                    nums = []

            # PATCH FIX2AF_SCRAPE_LEDGER_EXTRACTED_V1 (ADDITIVE)
            try:
                _fix2af_ledger_put(_fix2af_ledger, url, stage="extracted", reason="ok" if (isinstance(nums, list) and len(nums)>0) else "no_numbers", extra={"numbers_count": (len(nums) if isinstance(nums, list) else -1)})
            except Exception:
                pass
            # END PATCH FIX2AF_SCRAPE_LEDGER_EXTRACTED_V1

            scraped_meta[url] = {
                "status": status,
                "text": txt,
                "extracted_numbers": nums if isinstance(nums, list) else [],
                # PATCH FIX2AF_PER_URL_FETCH_DIAG_V1 (ADDITIVE)
                "fix2af_fetch_diag": {
                    "url_norm": _fix2af_norm_url(url),
                    "status": status,
                    "text_len": len(txt or ""),
                    "failure_class": _fix2af_classify_fetch_failure(status, txt),
                    "numbers_count": (len(nums) if isinstance(nums, list) else -1),
                },
                # END PATCH FIX2AF_PER_URL_FETCH_DIAG_V1
            }
        except Exception as e:
            # PATCH FIX2AF_SCRAPE_LEDGER_EXCEPTION_V1 (ADDITIVE)
            try:
                _fix2af_ledger_put(_fix2af_ledger, url, stage="exception", reason=type(e).__name__, extra={"msg": str(e)[:300]})
            except Exception:
                pass
            # END PATCH FIX2AF_SCRAPE_LEDGER_EXCEPTION_V1
            scraped_meta[url] = {"status": f"exception:{type(e).__name__}", "text": "", "extracted_numbers": [], "fix2af_fetch_diag": {"url_norm": _fix2af_norm_url(url), "status": f"exception:{type(e).__name__}", "text_len": 0, "failure_class": type(e).__name__, "numbers_count": 0}}

    return scraped_meta


def _fix24_baseline_sources_cache_from_scraped_meta(scraped_meta: dict) -> list:
    """
    Use attach_source_snapshots_to_analysis (existing deterministic normalizer) to produce
    baseline_sources_cache from scraped_meta, ensuring value_norm/unit_tag fields are present.
    """
    try:
        fn = globals().get("attach_source_snapshots_to_analysis")
        if not callable(fn):
            return []
        dummy = {"results": {}}
        web_context = {"scraped_meta": scraped_meta or {}}
        fn(dummy, web_context)
        r = dummy.get("results") if isinstance(dummy.get("results"), dict) else {}
        bsc = r.get("baseline_sources_cache") if isinstance(r, dict) else None
        return bsc if isinstance(bsc, list) else []
    except Exception:
        return []


def _fix24_get_prev_hashes(prev_full: dict) -> dict:
    """
    Extract prior snapshot hashes (v2 preferred) from a full analysis payload.
    """
    out = {"v2": "", "v1": ""}
    try:
        if not isinstance(prev_full, dict):
            return out
        out["v2"] = str(prev_full.get("source_snapshot_hash_v2") or "")
        out["v1"] = str(prev_full.get("source_snapshot_hash") or "")
        r = prev_full.get("results") if isinstance(prev_full.get("results"), dict) else {}
        if isinstance(r, dict):
            out["v2"] = out["v2"] or str(r.get("source_snapshot_hash_v2") or "")
            out["v1"] = out["v1"] or str(r.get("source_snapshot_hash") or "")
    except Exception:
        return out

    return out


def _fix24_compute_current_hashes(baseline_sources_cache: list) -> dict:
    """
    Compute current snapshot hashes (v2 preferred).
    """
    out = {"v2": "", "v1": ""}
    try:
        fn1 = globals().get("compute_source_snapshot_hash")
        fn2 = globals().get("compute_source_snapshot_hash_v2")
        if callable(fn2):
            out["v2"] = str(fn2(baseline_sources_cache) or "")
        if callable(fn1):
            out["v1"] = str(fn1(baseline_sources_cache) or "")
    except Exception:
        return out

    return out


def _fix24_make_replay_output(prev_full: dict, hashes: dict) -> dict:
    """
    Build a minimal evolution payload for the dashboard that reflects the prior analysis
    payload verbatim (no rebuild). This avoids the diff panel showing years by ensuring
    the "evolution column" is sourced from stored canonical metrics.
    """
    pmc = prev_full.get("primary_metrics_canonical") if isinstance(prev_full, dict) else {}
    pmc = pmc if isinstance(pmc, dict) else {}

    # Build a deterministic "no-change" metric_changes list WITHOUT re-selecting metrics.
    metric_changes = []
    try:
        for ckey in sorted(pmc.keys()):
            m = pmc.get(ckey) if isinstance(pmc.get(ckey), dict) else {}
            name = str(m.get("name") or m.get("metric_name") or ckey)
            v = m.get("value_norm", m.get("value"))
            unit = m.get("base_unit") or m.get("unit_tag") or m.get("unit") or ""
            metric_changes.append({
                "canonical_key": ckey,
                "name": name,
                "previous_value": v,
                "current_value": v,
                "previous_unit": unit,
                "current_unit": unit,
                "change_type": "unchanged",
                "confidence": 1.0,
            })
    except Exception:
        pass
        metric_changes = []

    return {
        "status": "ok",
        "mode": "replay_unchanged_fix24",
        "message": "Sources + data unchanged (hash match). Replaying prior analysis snapshot from Sheets.",
        "sources_checked": int(len(prev_full.get("sources") or [])) if isinstance(prev_full, dict) else 0,
        "sources_fetched": 0,
        "sources_failed": 0,
        "sources_skipped": 0,
        "source_results": [],
        "metric_changes": metric_changes,
        "change_stats": {
            "metrics_increased": 0,
            "metrics_decreased": 0,
            "metrics_unchanged": len(metric_changes),
            "metrics_total": len(metric_changes),
        },
        "debug": {
            "fix24": True,
            "prev_source_snapshot_hash_v2": hashes.get("prev_v2",""),
            "cur_source_snapshot_hash_v2": hashes.get("cur_v2",""),
            "prev_source_snapshot_hash": hashes.get("prev_v1",""),
            "cur_source_snapshot_hash": hashes.get("cur_v1",""),
            "hash_equal_v2": bool(hashes.get("prev_v2") and hashes.get("cur_v2") and hashes.get("prev_v2")==hashes.get("cur_v2")),
            "hash_equal_v1": bool(hashes.get("prev_v1") and hashes.get("cur_v1") and hashes.get("prev_v1")==hashes.get("cur_v1")),
        },
        # Provide the replay payload so the dashboard can render canonical metrics directly if desired
        "replay_analysis_payload": prev_full,
    }


# PATCH FIX41G: removed misplaced top-level web_context normalization block (was causing NameError)


def run_source_anchored_evolution(previous_data: dict, web_context: dict = None) -> dict:
    """
    PATCH FIX24 (ADDITIVE): Evolution flow is:
      1) Rehydrate prior full analysis payload from Sheets (HistoryFull)
      2) Scrape/fetch current sources to build scraped_meta + baseline_sources_cache_current
      3) Compute current snapshot hash (v2 preferred)
      4) If hash matches prior analysis: STOP and replay from Sheets (no rebuild/selection)
      5) If changed: proceed with the existing deterministic evolution path, but ensure
         it routes through the same snapshot/anchor deterministic plumbing used elsewhere.

    Note: This does NOT refactor existing evolution code; it wraps it.
    """
    # Step 1: Rehydrate prior payload
    prev_full = _fix24_get_prev_full_payload(previous_data or {})
    prev_hashes = _fix24_get_prev_hashes(prev_full)

    # Step 2: Build current scraped_meta by fetching the same URLs used previously
    urls = _fix24_extract_source_urls(prev_full)
    # =====================================================================
    # PATCH FIX41AFC3 (ADDITIVE): Recover Evolution injected URLs into web_context['extra_urls']
    #
    # Purpose:
    # - Streamlit may provide injected URLs only via diagnostic fields
    #   (diag_extra_urls_ui / diag_extra_urls_ui_raw).
    # - Downstream evolution admission & fetch logic keys off web_context['extra_urls'].
    #
    # Behavior:
    # - If web_context['extra_urls'] is empty/missing, recover from (in order):
    #     1) web_context['diag_extra_urls_ui']     (list)
    #     2) web_context['diag_extra_urls_ui_raw'] (str; newline/comma separated)
    # - Normalize/canonicalize via _inj_diag_norm_url_list (tracking params stripped).
    #
    # Safety:
    # - Purely additive wiring; no effect when no injection is present.
    # - Never raises; falls back silently.
    # =====================================================================
    try:
        if isinstance(web_context, dict):
            _wc_extra0 = web_context.get('extra_urls')
            _needs = (not isinstance(_wc_extra0, (list, tuple)) or not _wc_extra0)
            if _needs:
                _recovered = []
                # FIX2D66: also recover from extra_urls_ui_raw and question text
                try:
                    _qtxt = str((prev_full or {}).get('question') or (previous_data or {}).get('question') or '')
                    _more = _fix2d66_collect_injected_urls(web_context or {}, question_text=_qtxt)
                    if _more:
                        _recovered.extend(_more)
                except Exception:
                    pass
                _v_list = web_context.get('diag_extra_urls_ui')
                if isinstance(_v_list, (list, tuple)) and _v_list:
                    _recovered = list(_v_list)
                if not _recovered:
                    _raw = web_context.get('diag_extra_urls_ui_raw') or web_context.get('extra_urls_ui_raw')
                    if isinstance(_raw, str) and _raw.strip():
                        _parts = []
                        for _line in _raw.splitlines():
                            _line = (_line or '').strip()
                            if not _line:
                                continue
                            for _p in _line.split(','):
                                _p = (_p or '').strip()
                                if _p:
                                    _parts.append(_p)
                        if _parts:
                            _recovered = _parts
                if _recovered:
                    _recovered_norm = _inj_diag_norm_url_list(_recovered)
                    if _recovered_norm:
                        web_context['extra_urls'] = list(_recovered_norm)
                        # Also consider URLs embedded in the question text (last resort)

                        web_context.setdefault('debug', {})
                        if isinstance(web_context.get('debug'), dict):
                            web_context['debug'].setdefault('fix41afc3', {})
                            if isinstance(web_context['debug'].get('fix41afc3'), dict):
                                web_context['debug']['fix41afc3'].update({
                                    'extra_urls_recovered': True,
                                    'extra_urls_recovered_count': int(len(_recovered_norm)),
                                })
    except Exception:
        pass
    # =====================================================================


    # =====================================================================
    # PATCH EVO_ROUTE_INJECTED_URLS_THROUGH_FWC_V1 (ADDITIVE)
    #
    # Goal:
    # - When Evolution UI provides injected URLs, route them through the SAME
    #   admission/normalization/dedupe logic used by analysis (fetch_web_context),
    #   but in IDENTITY-ONLY mode (no scraping).
    #
    # Why:
    # - Previously, injected URLs could appear in ui_norm/intake_norm but never
    #   reach the admission gate, yielding empty admission_decisions.
    #
    # Behavior:
    # - Only active when web_context['extra_urls'] is non-empty.
    # - Does NOT change fastpath logic directly; it only defines the "current URL
    #   universe" inputs (urls) used for hashing/scrape_meta building.
    #
    # Safety:
    # - identity_only=True prevents any network scrape inside fetch_web_context.
    # - Purely additive; if anything fails, it falls back to existing urls list.
    # =====================================================================
    try:
        _evo_extra_urls_raw = (web_context or {}).get("extra_urls") or []
        _evo_extra_urls_norm = _inj_diag_norm_url_list(_evo_extra_urls_raw)
        if _evo_extra_urls_norm:
            _baseline_urls_for_fwc = _fix24_extract_source_urls(prev_full) or []
            _baseline_urls_for_fwc_norm = _inj_diag_norm_url_list(_baseline_urls_for_fwc)
            _q_for_fwc = str((prev_full or {}).get("question") or (previous_data or {}).get("question") or "").strip()
            _fwc = fetch_web_context(
                _q_for_fwc or "evolution_identity_only",
                num_sources=int(min(12, max(1, len(_baseline_urls_for_fwc_norm) + len(_evo_extra_urls_norm)))),
                fallback_mode=True,
                fallback_urls=_baseline_urls_for_fwc_norm,
                existing_snapshots=(prev_full or {}).get("baseline_sources_cache") or (prev_full or {}).get("baseline_sources_cache_v2") or None,
                extra_urls=_evo_extra_urls_norm,
                diag_run_id=str((web_context or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                diag_extra_urls_ui_raw=(web_context or {}).get("diag_extra_urls_ui_raw"),
                identity_only=True,
            ) or {}
            _fwc_admitted = _fwc.get("web_sources") or _fwc.get("sources") or []
            if isinstance(_fwc_admitted, list) and _fwc_admitted:
                urls = list(_fwc_admitted)
            # Attach the admission decisions to web_context for unified inj_trace reporting
            if isinstance(web_context, dict):
                if isinstance(_fwc.get("diag_injected_urls"), dict):
                    web_context.setdefault("diag_injected_urls", {})
                    if isinstance(web_context.get("diag_injected_urls"), dict):
                        # do not clobber if already present
                        for _k, _v in _fwc.get("diag_injected_urls").items():
                            web_context["diag_injected_urls"].setdefault(_k, _v)
                web_context.setdefault("debug", {})
                if isinstance(web_context.get("debug"), dict):
                    web_context["debug"].setdefault("evo_fwc_identity_only", {})
                    if isinstance(web_context["debug"].get("evo_fwc_identity_only"), dict):
                        web_context["debug"]["evo_fwc_identity_only"].update({
                            "called": True,
                            "baseline_urls_count": int(len(_baseline_urls_for_fwc_norm)),
                            "extra_urls_count": int(len(_evo_extra_urls_norm)),
                            "admitted_count": int(len(urls or [])),
                            "admitted_set_hash": _inj_diag_set_hash(_inj_diag_norm_url_list(urls or [])),
                        })
    except Exception:
        pass
    # =====================================================================
    # =====================================================================


    # =====================================================================
        # =====================================================================
    # PATCH FIX41AFC9 (ADDITIVE): Merge injected URLs into `urls` universe BEFORE scrape_meta build
    #
    # Problem observed (inj_trace_v1):
    # - Injected URL shows up in ui_norm/intake_norm, but can still vanish from admitted_norm/hash_inputs_norm.
    # - Root cause: the identity-only fetch_web_context() step may replace `urls` with an admitted list
    #   that excludes injected URLs, and later injected logic may read from a different variable/path.
    #
    # Goal:
    # - If injected URLs are present in web_context['extra_urls'], ensure they are ALWAYS merged into the
    #   local `urls` list used by _fix24_build_scraped_meta(), so the injected URLs are at least
    #   attempted (scraped_meta populated) and can become part of current hash identity when successful.
    #
    # Safety:
    # - Only active when injection is present.
    # - Purely additive: does not change hashing algorithm or fastpath rules; it only ensures the URL
    #   universe includes the injected URLs when the user provided them.
    # - Never raises.
    # =====================================================================
    try:
        _fx9_wc = web_context if isinstance(web_context, dict) else {}
        _fx9_inj = _inj_diag_norm_url_list((_fx9_wc or {}).get("extra_urls") or [])
        if _fx9_inj and isinstance(urls, list):
            _fx9_seen = set(_inj_diag_norm_url_list([(_d.get("url") if isinstance(_d, dict) else _d) for _d in urls]))
            _fx9_added = []
            for _u in _fx9_inj:
                if _u in _fx9_seen:
                    continue
                _fx9_seen.add(_u)
                urls.append(_u)
                _fx9_added.append(_u)

            if isinstance(_fx9_wc, dict):
                _fx9_wc.setdefault("debug", {})
                if isinstance(_fx9_wc.get("debug"), dict):
                    _fx9_wc["debug"].setdefault("fix41afc9", {})
                    if isinstance(_fx9_wc["debug"].get("fix41afc9"), dict):
                        _fx9_wc["debug"]["fix41afc9"].update({
                            "merged_into_urls_universe": True,
                            "injected_urls_count": int(len(_fx9_inj)),
                            "added_to_urls_count": int(len(_fx9_added)),
                            "added_to_urls": list(_fx9_added),
                            "urls_count_after_merge": int(len(urls)),
                        })
    except Exception:
        pass
    # =====================================================================

# PATCH FIX41AFC6 (ADDITIVE): When injected URL delta exists, actually FETCH it
    #
    # Observation (from inj_trace_v1 in evolution JSON):
    # - Injected URL appears in ui_norm/intake_norm, but attempted/persisted remain empty,
    #   and the injected URL never reaches hash_inputs because evolution never performs a
    #   fetch cycle for the injected delta (it only replays cached snapshots).
    #
    # Goal:
    # - ONLY when injected URLs introduce a true delta vs the baseline source universe,
    #   run fetch_web_context() in normal mode (identity_only=False) so the injected URL
    #   is actually attempted/persisted and can become a first-class current source
    #   (and thus can affect downstream identity/hash inputs).
    #
    # Safety:
    # - No effect on no-injection runs.
    # - No effect when injection is empty or introduces no delta.
    # - Uses existing_snapshots to avoid re-fetching baseline sources.
    # - Never raises; falls back to existing behavior.
    # =====================================================================
    try:
        _fix41afc6_wc = web_context if isinstance(web_context, dict) else {}
        _fix41afc6_inj = _inj_diag_norm_url_list((_fix41afc6_wc or {}).get("extra_urls") or [])
        _fix41afc6_base = _inj_diag_norm_url_list(_fix24_extract_source_urls(prev_full) or [])
        _fix41afc6_delta = sorted(list(set(_fix41afc6_inj) - set(_fix41afc6_base))) if _fix41afc6_inj else []
        if _fix41afc6_inj:
            _fix41afc6_q = str((prev_full or {}).get("question") or (previous_data or {}).get("question") or "").strip()
            _fix41afc6_prev_snap = (prev_full or {}).get("baseline_sources_cache") or (prev_full or {}).get("baseline_sources_cache_v2") or None

            _fix41afc6_fwc = fetch_web_context(
                _fix41afc6_q or "evolution_injection_fetch",
                num_sources=int(min(12, max(1, len(_fix41afc6_base) + len(_fix41afc6_inj)))),
                fallback_mode=True,
                fallback_urls=_fix41afc6_base,
                existing_snapshots=_fix41afc6_prev_snap,
                extra_urls=_fix41afc6_inj,
                diag_run_id=str((_fix41afc6_wc or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                diag_extra_urls_ui_raw=(_fix41afc6_wc or {}).get("diag_extra_urls_ui_raw"),
                # PATCH FIX41AFC8 (ADDITIVE): force scrape injected extras even if not admitted
                force_scrape_extra_urls=True,
                force_admit_extra_urls=True,
                identity_only=False,
            ) or {}

            # Prefer the admitted list from fetch_web_context (it includes injected URLs that pass admission)
            _fix41afc6_admitted = _fix41afc6_fwc.get("web_sources") or _fix41afc6_fwc.get("sources") or []
            if isinstance(_fix41afc6_admitted, list) and _fix41afc6_admitted:
                urls = list(_fix41afc6_admitted)

            # Bubble up a small marker so inj_trace_v1 can report whether evolution actually called FWC
            if isinstance(web_context, dict):
                web_context["evolution_calls_fetch_web_context"] = True
                try:
                    # FIX2D56: record that injection fetch is allowed/enabled
                    web_context.setdefault("debug", {})
                    if isinstance(web_context.get("debug"), dict):
                        web_context["debug"].setdefault("fix2d56", {})
                        if isinstance(web_context["debug"].get("fix2d56"), dict):
                            web_context["debug"]["fix2d56"].update({
                                "force_fetch_injection": True,
                                "injected_urls_count": int(len(_fix41afc6_inj or [])),
                            })
                except Exception:
                    pass
                web_context.setdefault("debug", {})
                if isinstance(web_context.get("debug"), dict):
                    web_context["debug"].setdefault("fix41afc6", {})
                    if isinstance(web_context["debug"].get("fix41afc6"), dict):
                        web_context["debug"]["fix41afc6"].update({
                            "called_fetch_web_context": True,
                            "injected_delta_count": int(len(_fix41afc6_delta)),
                            "injected_delta": list(_fix41afc6_delta),
                            "admitted_count": int(len(_fix41afc6_admitted or [])) if isinstance(_fix41afc6_admitted, list) else 0,
                        })

                    # =====================================================================
                    # PATCH FIX41AFC8 (ADDITIVE): Emit forced-fetch diagnostics for injected delta
                    # =====================================================================
                    try:
                        web_context.setdefault("debug", {})
                        if isinstance(web_context.get("debug"), dict):
                            web_context["debug"].setdefault("fix41afc8", {})
                            if isinstance(web_context["debug"].get("fix41afc8"), dict):
                                # delta URLs are what we intend to force-attempt
                                _fx8_delta_urls = list(_fix41afc6_delta or [])
                                # attempted/persist outcomes can be inferred from fetch_web_context scraped_meta
                                _fx8_results = {}
                                try:
                                    _fx8_sm = _fix41afc6_fwc.get("scraped_meta") or {}
                                    if isinstance(_fx8_sm, dict):
                                        for _u in _fx8_delta_urls:
                                            meta = _fx8_sm.get(_u) or {}
                                            if isinstance(meta, dict) and meta:
                                                _fx8_results[_u] = meta.get("status") or meta.get("fetch_status") or meta.get("reason") or "attempted"
                                            else:
                                                _fx8_results[_u] = "not_in_scraped_meta"
                                except Exception:
                                    pass
                                web_context["debug"]["fix41afc8"].update({
                                    "forced_fetch_reason": "injected_delta_present_force_fetch_even_if_not_admitted",
                                    "forced_fetch_urls": _fx8_delta_urls,
                                    "forced_fetch_count": int(len(_fx8_delta_urls)),
                                    "forced_fetch_results": _fx8_results,
                                })
                    except Exception:
                        pass

    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH FIX41AFC7 (ADDITIVE): Early recovery + latching of injected URLs into web_context["extra_urls"]
    #
    # Problem observed in evolution JSON:
    # - ui_norm/intake_norm contains the injected URL (from Streamlit textarea),
    #   but web_context["extra_urls"] can still be empty at evolution core, which
    #   causes downstream "fetch injected delta" and "forced admit" patches to see
    #   an empty injected set and skip.
    #
    # Goal:
    # - If web_context["extra_urls"] is empty, recover injected URLs from the same
    #   Streamlit diagnostic fields used at intake:
    #     1) web_context["diag_extra_urls_ui"] (list)
    #     2) web_context["diag_extra_urls_ui_raw"] (string, newline/comma separated)
    # - Normalize/canonicalize deterministically via _inj_diag_norm_url_list().
    # - Latch the recovered list back into web_context["extra_urls"] so ALL later
    #   injected URL logic (fetch + forced admit + hash identity) sees the same set.
    #
    # Safety:
    # - Purely additive. No effect when extra_urls already present.
    # - Never raises; falls back silently.
    # =====================================================================
    try:
        if isinstance(web_context, dict):
            _fix41afc7_norm = []
            _existing = web_context.get("extra_urls")
            _need = (not isinstance(_existing, (list, tuple)) or not list(_existing))
            if _need:
                _raw = []
                _v_list = web_context.get("diag_extra_urls_ui")
                if isinstance(_v_list, (list, tuple)) and _v_list:
                    _raw = list(_v_list)
                if not _raw:
                    _v_raw = web_context.get("diag_extra_urls_ui_raw")
                    if isinstance(_v_raw, str) and _v_raw.strip():
                        _parts = []
                        for _line in _v_raw.splitlines():
                            _line = (_line or "").strip()
                            if not _line:
                                continue
                            for _p in _line.split(","):
                                _p = (_p or "").strip()
                                if _p:
                                    _parts.append(_p)
                        if _parts:
                            _raw = _parts

                _fix41afc7_norm = _inj_diag_norm_url_list(_raw)
                if _fix41afc7_norm:
                    web_context["extra_urls"] = list(_fix41afc7_norm)

            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].setdefault("fix41afc7", {})
                if isinstance(web_context["debug"].get("fix41afc7"), dict):
                    web_context["debug"]["fix41afc7"].update({
                        "recovery_needed": bool(_need),
                        "recovered_extra_urls_count": int(len(_fix41afc7_norm or [])),
                        "recovered_extra_urls": list(_fix41afc7_norm or [])[:20],
                        "extra_urls_present_after_recovery": bool(isinstance(web_context.get("extra_urls"), (list, tuple)) and list(web_context.get("extra_urls") or [])),
                    })
    except Exception:
        pass
    # =====================================================================

# PATCH INJ_DIAG_EVO_CORE (ADDITIVE): allow optional injected URLs (Scenario B)
    # - Only active if caller provides web_context['extra_urls']
    # - Does NOT affect default fastpath behavior.
    # =====================================================================
    _inj_diag_run_id = ""
    _inj_extra_urls = []
    try:
        _inj_diag_run_id = str((web_context or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo")
        _inj_extra_urls = _inj_diag_norm_url_list((web_context or {}).get("extra_urls") or [])
    except Exception:
        pass
        _inj_diag_run_id = _inj_diag_make_run_id("evo")
        _inj_extra_urls = []

    try:
        if _inj_extra_urls:
            _u_seen = set([str(u or "").strip() for u in (urls or []) if str(u or "").strip()])
            for _u in _inj_extra_urls:
                if _u not in _u_seen:
                    _u_seen.add(_u)
                    urls.append(_u)
    except Exception:
        pass
    # =====================================================================


    # =====================================================================

    # =====================================================================
    # PATCH FIX41AFC4 (ADDITIVE): Force-admit injected URL deltas into evolution URL universe
    #
    # Problem (observed in evolution JSON):
    # - Injected URL appears in ui_norm/intake_norm but is missing from admitted_norm,
    #   so it never reaches attempted/persisted/hash_inputs.
    # - This typically happens when admission/allowlist logic rejects injected URLs
    #   before the fetch loop, leaving attempted empty.
    #
    # Goal:
    # - ONLY when injection is present AND it introduces a true delta vs the baseline
    #   source universe, ensure the injected URLs are included in `urls` (the universe
    #   FIX24 uses for scrape_meta building).
    #
    # Safety:
    # - Purely additive; no effect when no injection or no delta.
    # - Does not modify fastpath logic/hashing; it only ensures injected URLs are
    #   present in the post-intake universe when delta exists.
    # - Never raises; falls back silently.
    # =====================================================================
    try:
        _fix41afc4_inj_norm = _inj_diag_norm_url_list(_inj_extra_urls or [])
        _fix41afc4_base_norm = _inj_diag_norm_url_list(_fix24_extract_source_urls(prev_full) or [])
        _fix41afc4_delta = sorted(list(set(_fix41afc4_inj_norm) - set(_fix41afc4_base_norm))) if _fix41afc4_inj_norm else []
        _fix41afc4_applied = False

        if _fix41afc4_delta:
            # Determine expected URL container shape (strings vs dicts)
            _urls_list = urls if isinstance(urls, list) else []
            _urls_are_dicts = bool(_urls_list) and isinstance(_urls_list[0], dict)

            # Build a normalized "seen" set from existing urls
            if _urls_are_dicts:
                _seen_norm = set(_inj_diag_norm_url_list([(_d.get("url") if isinstance(_d, dict) else "") for _d in _urls_list]))
            else:
                _seen_norm = set(_inj_diag_norm_url_list(_urls_list))

            for _u in _fix41afc4_delta:
                if _u in _seen_norm:
                    continue
                _seen_norm.add(_u)
                if _urls_are_dicts:
                    _urls_list.append({"url": _u})
                else:
                    _urls_list.append(_u)
                _fix41afc4_applied = True

            urls = _urls_list  # rebind defensively

        if isinstance(web_context, dict):
            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].setdefault("fix41afc4", {})
                if isinstance(web_context["debug"].get("fix41afc4"), dict):
                    web_context["debug"]["fix41afc4"].update({
                        "forced_admit_applied": bool(_fix41afc4_applied),
                        "forced_admit_injected_urls_count": int(len(_fix41afc4_delta)),
                        "forced_admit_injected_urls": list(_fix41afc4_delta),
                        "baseline_urls_count": int(len(_fix41afc4_base_norm)),
                        "urls_after_forced_admit_count": int(len(_inj_diag_norm_url_list([(_d.get("url") if isinstance(_d, dict) else _d) for _d in (urls or [])] if isinstance(urls, list) else []))),
                    })
    except Exception:
        pass
    # =====================================================================


# PATCH EVO_INJ_ADMISSION_TRACE_V1 (ADDITIVE): pinpoint where injected URLs are dropped
    #
    # Why:
    # - When a URL appears in ui_norm/intake_norm but not in admitted_norm, we need
    #   an explicit reason before we change any behavior.
    #
    # What this records (debug only):
    # - Whether evolution is using fetch_web_context (it is not in FIX24 path)
    # - The pre- and post-injection URL universe
    # - Per-injected-URL admission decision + reason codes
    #
    # Safety:
    # - Does NOT alter control flow, fastpath eligibility, scraping, hashing, or selection.
    # =====================================================================
    try:
        _urls_prev_full = _fix24_extract_source_urls(prev_full)
        _urls_prev_full_norm = _inj_diag_norm_url_list(_urls_prev_full or [])
        _urls_after_merge_norm = _inj_diag_norm_url_list(urls or [])

        _admission_decisions = {}
        for _u in (_inj_extra_urls or []):
            if _u in set(_urls_after_merge_norm):
                _admission_decisions[_u] = {
                    "decision": "admitted",
                    "reason_code": "merged_into_urls_for_scrape",
                }
            else:
                _admission_decisions[_u] = {
                    "decision": "rejected",
                    "reason_code": "not_present_in_urls_after_merge",
                }

        if isinstance(web_context, dict):
            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].setdefault("evo_injection_trace", {})
                if isinstance(web_context["debug"].get("evo_injection_trace"), dict):
                    web_context["debug"]["evo_injection_trace"].update({
                        "uses_fetch_web_context": False,
                        "urls_prev_full_count": int(len(_urls_prev_full_norm)),
                        "urls_prev_full_set_hash": _inj_diag_set_hash(_urls_prev_full_norm),
                        "urls_after_merge_count": int(len(_urls_after_merge_norm)),
                        "urls_after_merge_set_hash": _inj_diag_set_hash(_urls_after_merge_norm),
                        "inj_extra_urls_norm": list(_inj_extra_urls or []),
                        "inj_merge_applied": bool(_inj_extra_urls),
                        "inj_admission_decisions": _admission_decisions,
                    })

            # Also attach to diag_injected_urls for unified downstream reporting
            web_context.setdefault("diag_injected_urls", {})
            if isinstance(web_context.get("diag_injected_urls"), dict):
                web_context["diag_injected_urls"].setdefault("admission_decisions", {})
                if isinstance(web_context["diag_injected_urls"].get("admission_decisions"), dict):
                    web_context["diag_injected_urls"]["admission_decisions"].update(_admission_decisions)
                web_context["diag_injected_urls"].setdefault("urls_prev_full_norm", _urls_prev_full_norm)
                web_context["diag_injected_urls"].setdefault("urls_after_merge_norm", _urls_after_merge_norm)
    except Exception:
        pass
    # =====================================================================

    # =====================================================================
    # PATCH FIX41AFC11 (ADDITIVE): Injection admission override + must-fetch lane (delta-only)
    #
    # Problem:
    # - Injected URLs can appear in UI intake but get dropped pre-admission, resulting in:
    #     attempted=0, persisted_norm=0, hash_inputs_norm unchanged.
    #
    # Goal:
    # - When injected URL DELTA exists (vs current urls baseline), deterministically:
    #     1) Force-admit injected delta into local `urls` universe (so downstream meta sees it)
    #     2) Force-fetch injected delta via fetch_web_context(force_scrape_extra_urls=True),
    #        so we get attempted/persisted entries or an explicit failure reason.
    #
    # Safety:
    # - No effect when no injection / no delta.
    # - Does not weaken normal fastpath logic (already bypassed upstream when delta exists).
    # =====================================================================
    try:
        _fix41afc11_wc = web_context if isinstance(web_context, dict) else {}
        # Robust recovery (order required)
        _fix41afc11_extra = []
        if isinstance(_fix41afc11_wc.get("extra_urls"), (list, tuple)) and _fix41afc11_wc.get("extra_urls"):
            _fix41afc11_extra = list(_fix41afc11_wc.get("extra_urls") or [])
        elif isinstance(_fix41afc11_wc.get("diag_extra_urls_ui"), (list, tuple)) and _fix41afc11_wc.get("diag_extra_urls_ui"):
            _fix41afc11_extra = list(_fix41afc11_wc.get("diag_extra_urls_ui") or [])
        elif isinstance(_fix41afc11_wc.get("diag_extra_urls_ui_raw"), str) and (_fix41afc11_wc.get("diag_extra_urls_ui_raw") or "").strip():
            _raw = str(_fix41afc11_wc.get("diag_extra_urls_ui_raw") or "")
            _parts = []
            for _line in _raw.splitlines():
                _line = (_line or "").strip()
                if not _line:
                    continue
                for _p in _line.split(","):
                    _p = (_p or "").strip()
                    if _p:
                        _parts.append(_p)
            _fix41afc11_extra = _parts

        _fix41afc11_inj_norm = _inj_diag_norm_url_list(_fix41afc11_extra) if _fix41afc11_extra else []
        _fix41afc11_urls_norm = _inj_diag_norm_url_list(urls) if urls else []
        _fix41afc11_inj_set = set(_fix41afc11_inj_norm or [])
        _fix41afc11_base_set = set(_fix41afc11_urls_norm or [])
        _fix41afc11_delta = sorted(list(_fix41afc11_inj_set - _fix41afc11_base_set)) if _fix41afc11_inj_set else []

        if _fix41afc11_delta:
            # (1) Force-admit delta into local urls universe deterministically
            _added = []
            if urls and isinstance(urls[0], dict):
                _seen = set(_fix41afc11_urls_norm or [])
                for _u in _fix41afc11_delta:
                    if _u in _seen:
                        continue
                    urls.append({"url": _u, "source": "injected_force_admit", "is_injected": True})
                    _seen.add(_u)
                    _added.append(_u)
            else:
                _seen = set(_fix41afc11_urls_norm or [])
                for _u in _fix41afc11_delta:
                    if _u in _seen:
                        continue
                    urls.append(_u)
                    _seen.add(_u)
                    _added.append(_u)

            # (2) Must-fetch lane: force scrape extra urls even if not admitted by normal filter
            _q = str((prev_full or {}).get("question") or (previous_data or {}).get("question") or "evolution_injection_force_fetch").strip()
            _prev_snap = (prev_full or {}).get("baseline_sources_cache") or (prev_full or {}).get("baseline_sources_cache_v2") or None
            try:
                _fwc = fetch_web_context(
                    _q or "evolution_injection_force_fetch",
                    num_sources=int(min(12, max(1, len(_fix41afc11_urls_norm or []) + len(_fix41afc11_inj_norm or [])))),
                    fallback_mode=True,
                    fallback_urls=list(_fix41afc11_urls_norm or []),
                    existing_snapshots=_prev_snap,
                    extra_urls=list(_fix41afc11_inj_norm or []),
                    diag_run_id=str((_fix41afc11_wc or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                    diag_extra_urls_ui_raw=(_fix41afc11_wc or {}).get("diag_extra_urls_ui_raw"),
                    identity_only=False,
                    force_scrape_extra_urls=True,
                force_admit_extra_urls=True,
                ) or {}
            except TypeError:
                # Backward-compat: older fetch_web_context without force_scrape_extra_urls
                _fwc = fetch_web_context(
                    _q or "evolution_injection_force_fetch",
                    num_sources=int(min(12, max(1, len(_fix41afc11_urls_norm or []) + len(_fix41afc11_inj_norm or [])))),
                    fallback_mode=True,
                    fallback_urls=list(_fix41afc11_urls_norm or []),
                    existing_snapshots=_prev_snap,
                    extra_urls=list(_fix41afc11_inj_norm or []),
                    diag_run_id=str((_fix41afc11_wc or {}).get("diag_run_id") or "") or _inj_diag_make_run_id("evo"),
                    diag_extra_urls_ui_raw=(_fix41afc11_wc or {}).get("diag_extra_urls_ui_raw"),
                    identity_only=False,
                ) or {}

            # If fetch_web_context returns a concrete web_sources list, prefer it for downstream scraped_meta
            _fwc_sources = _fwc.get("web_sources") or _fwc.get("sources") or None
            if isinstance(_fwc_sources, list) and _fwc_sources:
                urls = list(_fwc_sources)

            # Emit explicit debug fields
            if isinstance(web_context, dict):
                web_context.setdefault("debug", {})
                if isinstance(web_context.get("debug"), dict):
                    web_context["debug"].setdefault("fix41afc11", {})
                    if isinstance(web_context["debug"].get("fix41afc11"), dict):
                        web_context["debug"]["fix41afc11"].update({
                            "inj_force_admit_applied": True,
                            "inj_force_admit_count": int(len(_added)),
                            "inj_force_admit_urls": list(_added),
                            "inj_delta_count": int(len(_fix41afc11_delta)),
                            "inj_delta": list(_fix41afc11_delta),
                            "inj_must_fetch_called": True,
                            "inj_must_fetch_sources_count": int(len(_fwc_sources or [])) if isinstance(_fwc_sources, list) else 0,
                        })
    except Exception:
        pass
    # =====================================================================
    # =====================================================================
    # PATCH FIX2AE_INJECTED_FETCH_URLS_MERGE_V1 (ADDITIVE)
    # Goal:
    #   Ensure injected URLs are merged into the scrape/fetch URL universe in a shape-aware way.
    #   If urls is a list of dicts, append {"url": u}; otherwise append the string u.
    # Diagnostics:
    #   web_context.debug.fix2ae reports counts and added URLs.
    # Safety:
    #   Additive only. Never removes or reorders existing entries.
    # =====================================================================
    try:
        _fx2ae_inj_raw = list(_inj_extra_urls or [])
        _fx2ae_inj_norm = _inj_diag_norm_url_list(_fx2ae_inj_raw) if _fx2ae_inj_raw else []
        _fx2ae_added = []
        _fx2ae_shape = "unknown"
        _fx2ae_urls_before = []
        _fx2ae_urls_after = []
        if isinstance(urls, list):
            _fx2ae_urls_before = [(_d.get("url") if isinstance(_d, dict) else _d) for _d in (urls or [])]
            _urls_are_dicts = bool(urls) and isinstance(urls[0], dict)
            _fx2ae_shape = "dicts" if _urls_are_dicts else "strings"
            _seen_norm = set(_inj_diag_norm_url_list(_fx2ae_urls_before))
            for _u in (_fx2ae_inj_norm or []):
                if not _u:
                    continue
                if _u in _seen_norm:
                    continue
                _seen_norm.add(_u)
                if _urls_are_dicts:
                    urls.append({"url": _u})
                else:
                    urls.append(_u)
                _fx2ae_added.append(_u)
            _fx2ae_urls_after = [(_d.get("url") if isinstance(_d, dict) else _d) for _d in (urls or [])]
        try:
            if isinstance(web_context, dict):
                web_context.setdefault("debug", {})
                if isinstance(web_context.get("debug"), dict):
                    web_context["debug"].setdefault("fix2ae", {})
                    if isinstance(web_context["debug"].get("fix2ae"), dict):
                        web_context["debug"]["fix2ae"].update({
                            "inj_count_raw": int(len(_fx2ae_inj_raw or [])),
                            "inj_count_norm": int(len(_fx2ae_inj_norm or [])),
                            "urls_shape": _fx2ae_shape,
                            "urls_before_count": int(len(_inj_diag_norm_url_list(_fx2ae_urls_before) or [])),
                            "urls_after_count": int(len(_inj_diag_norm_url_list(_fx2ae_urls_after) or [])),
                            "added_count": int(len(_fx2ae_added or [])),
                            "added_urls": list(_fx2ae_added or [])[:50],
                        })
        except Exception:
            pass
    except Exception:
        pass
    # =====================================================================





    scraped_meta = _fix24_build_scraped_meta(urls)

    # PATCH FIX2AF_ATTACH_SCRAPE_LEDGER_TO_WEB_CONTEXT_V1 (ADDITIVE)
    try:
        _fix2af_led = globals().get("_fix2af_last_scrape_ledger")
        if isinstance(web_context, dict) and isinstance(_fix2af_led, dict):
            web_context["fix2af_scrape_ledger_v1"] = _fix2af_led
    except Exception:
        pass
    # END PATCH FIX2AF_ATTACH_SCRAPE_LEDGER_TO_WEB_CONTEXT_V1

    # Step 3: Normalize into baseline_sources_cache and hash
    cur_bsc = _fix24_baseline_sources_cache_from_scraped_meta(scraped_meta)

    # =====================================================================
    # PATCH EVO_INJECTED_URLS_AS_CURRENT_SOURCES_V1 (ADDITIVE):
    # Policy + wiring alignment for Evolution UI injected URLs
    #
    # Goal:
    # - Treat Evolution-tab injected URLs as part of the *current source universe*
    #   for hash identity *when they fetch successfully*, consistent with baseline
    #   sources (successful snapshots contribute to identity).
    #
    # Behavior (safe):
    # - If an injected URL was provided (web_context['extra_urls']) and its scrape
    #   status is success, it must appear in cur_bsc so that hash inputs can include it.
    # - If success-but-missing occurs (unexpected), we add a synthetic url-only entry
    #   tagged for hash identity (debug only; no numbers).
    # - If fetch failed, we do NOT force hash mismatch (consistent with policy),
    #   but we record explicit attempted status + reason into diagnostics.
    #
    # Safety:
    # - Does NOT alter fastpath eligibility logic directly; it only ensures that
    #   the identity inputs reflect the actual successfully fetched current sources.
    # - Purely additive; never removes or refactors existing logic.
    # =====================================================================
    try:
        _inj_sm = scraped_meta if isinstance(scraped_meta, dict) else {}
        _inj_attempted_rows = []
        _inj_success_urls = set()
        for _u in (_inj_extra_urls or []):
            _m = _inj_sm.get(_u) if isinstance(_inj_sm.get(_u), dict) else {}
            _st = str(_m.get('status') or _m.get('fetch_status') or '')
            _reason = str(_m.get('status_detail') or _m.get('fail_reason') or '')
            _clen = _m.get('clean_text_len') or _m.get('content_len') or 0
            _inj_attempted_rows.append({
                'url': _u,
                'status': _st or 'unknown',
                'reason': _reason,
                'content_len': int(_clen) if str(_clen).isdigit() else 0,
            })
            if (_st or '').lower() in ('success','ok','fetched'):
                _inj_success_urls.add(_u)

        # Ensure success injected urls are represented in cur_bsc (hash identity)
        if _inj_success_urls and isinstance(cur_bsc, list):
            _bsc_urls = set()
            for _row in cur_bsc:
                if isinstance(_row, dict):
                    _bu = str(_row.get('url') or _row.get('source_url') or '').strip()
                    if _bu:
                        _bsc_urls.add(_bu)
            _missing_success = sorted(list(_inj_success_urls - _bsc_urls))
            for _u in _missing_success:
                cur_bsc.append({
                    'url': _u,
                    'status': 'success',
                    'status_detail': 'synthetic_success_missing_in_bsc',
                    'clean_text': '',
                    'clean_text_len': 0,
                    'extracted_numbers': [],
                    'numbers_found': 0,
                    'fingerprint': 'synthetic_url_only_for_hash',
                    'is_synthetic_for_hash': True,
                })

        # Write attempted status into web_context diagnostics for transparency
        if isinstance(web_context, dict):
            web_context.setdefault('diag_injected_urls', {})
            if isinstance(web_context.get('diag_injected_urls'), dict):
                web_context['diag_injected_urls'].setdefault('attempted', [])
                # Only overwrite if empty to avoid clobbering richer traces
                if not web_context['diag_injected_urls'].get('attempted'):
                    web_context['diag_injected_urls']['attempted'] = _inj_attempted_rows
                web_context['diag_injected_urls']['success_urls'] = sorted(list(_inj_success_urls))
    except Exception:
        pass
    # =====================================================================

    cur_hashes = _fix24_compute_current_hashes(cur_bsc)

    # =====================================================================

    # PATCH INJ_DIAG_EVO_DEBUG (ADDITIVE): record injected URL lifecycle (B1)
    # =====================================================================
    try:
        _hash_inputs = _inj_diag_hash_inputs_from_bsc(cur_bsc)
        if isinstance(web_context, dict):
            web_context.setdefault("diag_injected_urls", {})
            if isinstance(web_context.get("diag_injected_urls"), dict):
                web_context["diag_injected_urls"].update({
                    "run_id": _inj_diag_run_id,
                    "ui_raw": (web_context or {}).get("diag_extra_urls_ui_raw") or "",
                    "ui_norm": _inj_extra_urls,
                    "intake_norm": _inj_extra_urls,
                    "admitted": list(urls or []),
                    "hash_inputs": _hash_inputs,
                    "injected_in_hash_inputs": sorted(list(set(_inj_extra_urls) & set(_hash_inputs))),
                    "set_hashes": {
                        "hash_inputs": _inj_diag_set_hash(_hash_inputs),
                        "admitted": _inj_diag_set_hash(list(urls or [])),
                    }
                })
    except Exception:
        pass
    # =====================================================================



    # Step 4: Compare (v2 preferred)
    equal_v2 = bool(prev_hashes.get("v2") and cur_hashes.get("v2") and prev_hashes["v2"] == cur_hashes["v2"])
    equal_v1 = bool(prev_hashes.get("v1") and cur_hashes.get("v1") and prev_hashes["v1"] == cur_hashes["v1"])
    unchanged = equal_v2 or (not prev_hashes.get("v2") and equal_v1)

    # =====================================================================
    # PATCH FIX40 (ADDITIVE): Force rebuild override (Scenario B)
    # If the UI (or caller) requests force_rebuild, we intentionally bypass
    # the unchanged fastpath even if hashes match, to exercise rebuild logic.
    # =====================================================================
    _force_rebuild = False
    try:
        _force_rebuild = bool((web_context or {}).get("force_rebuild"))
    except Exception:
        pass
        _force_rebuild = False
    if _force_rebuild:
        unchanged = False
        _fix41_force_rebuild_honored = True
    else:
        _fix41_force_rebuild_honored = False
    # =====================================================================

    if unchanged:
        hashes = {
            "prev_v2": prev_hashes.get("v2",""),
            "cur_v2": cur_hashes.get("v2",""),
            "prev_v1": prev_hashes.get("v1",""),
            "cur_v1": cur_hashes.get("v1",""),
        }
        out_replay = _fix24_make_replay_output(prev_full, hashes)
        # =====================================================================
        # PATCH FIX41 (ADDITIVE): Attach force-rebuild debug to replay output
        # =====================================================================
        try:
            if isinstance(out_replay, dict):
                out_replay.setdefault("code_version", _yureeka_get_code_version())
                out_replay.setdefault("debug", {}).setdefault("fix41", {})
                out_replay["debug"]["fix41"].update({
                    "force_rebuild_seen": bool(_fix41_force_rebuild_seen),
                    "force_rebuild_honored": bool(locals().get("_fix41_force_rebuild_honored", False)),
                    "path": "replay_unchanged",
                })
        except Exception:
            pass

        # =====================================================================
        # PATCH EVO_INJ_TRACE_REPLAY1 (ADDITIVE): emit inj_trace_v1 even on replay fastpath
        # Why:
        # - The FIX24 replay path returns early (skipping compute_source_anchored_diff),
        #   which previously meant results.debug.inj_trace_v1 might be missing.
        # - We need injected-URL lifecycle visibility even when hashes match (fastpath/replay)
        #   to validate UI wiring and to explain why a mismatch did/did not occur.
        # Safety:
        # - Pure debug emission only; does NOT affect hash logic, scraping, or fastpath decisions.
        # =====================================================================
        try:
            _wc = web_context if isinstance(web_context, dict) else {}
            _diag = _wc.get("diag_injected_urls") if isinstance(_wc.get("diag_injected_urls"), dict) else {}

            # =====================================================================
            # PATCH INJ_TRACE_V1_ENRICH_EVOLUTION_REPLAY_ARTIFACTS (ADDITIVE)
            # Populate attempted/persisted for injected URLs from scraped_meta/cur_bsc
            # even when replay fastpath returns early.
            # Also attach an explicit reason when injected URLs are present but not
            # admitted/hashed due to replay semantics.
            # =====================================================================
            try:
                if isinstance(_diag, dict):
                    # Enrich from scraped_meta (injected only) and from BSC (all)
                    _diag = _inj_trace_v1_enrich_diag_from_scraped_meta(_diag, scraped_meta, (_inj_extra_urls or []))
                    _diag = _inj_trace_v1_enrich_diag_from_bsc(_diag, cur_bsc if isinstance(cur_bsc, list) else [])
                    # Explain replay semantics when UI extras exist
                    _ui_norm = _inj_diag_norm_url_list(_diag.get("ui_norm") or [])
                    if _ui_norm:
                        _diag.setdefault("admission_reason", "fastpath_replay_no_rebuild_no_admission")
                        _diag.setdefault("injection_effective", False)
            except Exception:
                pass
            # =====================================================================

            _hash_inputs_replay = _inj_diag_hash_inputs_from_bsc(cur_bsc)
            _trace_replay = _inj_trace_v1_build(
                diag_injected_urls=_diag,
                hash_inputs=_hash_inputs_replay,
                stage="evolution",
                path="fastpath_replay",
                rebuild_pool=None,
                rebuild_selected=None,
            )
            out_replay.setdefault("results", {})
            if isinstance(out_replay.get("results"), dict):
                out_replay["results"].setdefault("debug", {})
                if isinstance(out_replay["results"].get("debug"), dict):
                    out_replay["results"]["debug"]["inj_trace_v1"] = _trace_replay
        except Exception:
            pass
        # =====================================================================
        # END PATCH EVO_INJ_TRACE_REPLAY1
        # =====================================================================
        # PATCH FIX2D20 (ADD): trace year-like commits on evolution replay output

        _fix2d20_trace_year_like_commits(out_replay, stage='evolution', callsite='run_source_anchored_evolution_replay')

        return out_replay
    # Step 5: Changed -> run deterministic evolution diff using existing machinery.
    # Provide web_context with scraped_meta so compute_source_anchored_diff can reconstruct snapshots deterministically.
    wc = {"scraped_meta": scraped_meta}
    # =====================================================================
    # PATCH FIX40 (ADDITIVE): Preserve caller flags (e.g., force_rebuild) into web_context
    # so downstream diff/rebuild logic can record provenance if needed.
    # =====================================================================
    try:
        if isinstance(web_context, dict):
            wc.update({k: v for k, v in web_context.items() if k != "scraped_meta"})
    except Exception:
        pass
    # =====================================================================



    if callable(run_source_anchored_evolution_BASE):
        try:
            out = run_source_anchored_evolution_BASE(prev_full, web_context=wc)
            if isinstance(out, dict):
                out.setdefault("debug", {})
                if isinstance(out["debug"], dict):
                    out["debug"]["fix24"] = True
                    out["debug"]["fix24_mode"] = "recompute_changed"
                    # =====================================================================
                    # PATCH FIX40 (ADDITIVE): record Scenario B override
                    # =====================================================================
                    out["debug"]["fix40_force_rebuild"] = bool(_force_rebuild)
                    # =====================================================================
            # =====================================================================
            # =====================================================================
            # =====================================================================
            # =====================================================================
                    out["debug"]["prev_source_snapshot_hash_v2"] = prev_hashes.get("v2","")
                    out["debug"]["cur_source_snapshot_hash_v2"] = cur_hashes.get("v2","")
                    out["debug"]["prev_source_snapshot_hash"] = prev_hashes.get("v1","")
                    out["debug"]["cur_source_snapshot_hash"] = cur_hashes.get("v1","")
            return out
        except Exception as e:
            # Fall through to original behavior if anything unexpected
            pass

    # Ultimate fallback: call compute_source_anchored_diff directly if base runner not available
    fn = globals().get("compute_source_anchored_diff")
    if callable(fn):
        try:
            # FIX2D55: lift prev_full onto schema keys BEFORE diff computation

            _fix2d55_apply_prev_lift(prev_full, wc)

            out_changed = fn(prev_full, web_context=wc)
            # =====================================================================
            # PATCH FIX41 (ADDITIVE): Attach force-rebuild debug to changed output
            # =====================================================================
            try:
                if isinstance(out_changed, dict):
                    out_changed.setdefault("code_version", _yureeka_get_code_version())
                    out_changed.setdefault("debug", {}).setdefault("fix41", {})
                    out_changed["debug"]["fix41"].update({
                        "force_rebuild_seen": bool(_fix41_force_rebuild_seen),
                        "force_rebuild_honored": bool(locals().get("_fix41_force_rebuild_honored", False)) or bool(_fix41_force_rebuild_seen),
                        "path": "changed_compute_source_anchored_diff",
                    })
            except Exception:
                pass
            # PATCH FIX2D20 (ADD): trace year-like commits on evolution changed output

            _fix2d20_trace_year_like_commits(out_changed, stage='evolution', callsite='run_source_anchored_evolution_changed')

            return out_changed
        except Exception:
            return {
        "status": "failed",
        "message": "FIX24: Evolution recompute failed (no callable base evolution runner).",
        "sources_checked": len(urls),
        "sources_fetched": len(urls),
        "metric_changes": [],
        "debug": {"fix24": True, "fix24_mode": "recompute_failed"},
    }


# ==============================================================================
# FIX32 (ADDITIVE): Unit-required hard gate in evolution diff rendering
#
# Problem:
# - Evolution diff table can show unit-less / year-like integers (e.g. 2024, 2033, 1500)
#   in the "Current" column for metrics whose schema requires currency/percent/rate/ratio.
# - This happens when upstream selection or LLM delta yields a numeric candidate that lacks
#   token-level unit evidence (unit_tag/unit_family/base_unit empty), and the diff layer
#   currently treats it as a valid numeric current value.
#
# Target invariant:
# - For any metric with unit_family in {currency, percent, rate, ratio} (or with a non-empty
#   unit_tag/unit), a unit-less candidate must be treated as ineligible and must not be
#   rendered as a valid "Current" value in the evolution table/output.
#
# Approach (purely additive):
# - Preserve existing diff implementation as diff_metrics_by_name_FIX31_BASE.
# - Override diff_metrics_by_name with a wrapper that post-processes each row:
#     * if schema says unit is required AND current unit evidence is missing,
#       then mark unit_mismatch=True and render current_value="N/A" (and cur_value_norm=None).
# - No refactors; does not change upstream extraction/building logic, only prevents
#   unit-less values from appearing as "valid" in evolution output.
# ==============================================================================
try:
    diff_metrics_by_name_FIX31_BASE = _yureeka_diff_metrics_by_name_fix31  # type: ignore
except Exception:
    pass
    diff_metrics_by_name_FIX31_BASE = None  # type: ignore

def _fix32_metric_requires_unit(metric_def: dict) -> bool:
    """Return True if schema implies this metric requires unit evidence."""
    try:
        spec = metric_def if isinstance(metric_def, dict) else {}
        uf = str(spec.get("unit_family") or "").strip().lower()
        ut = str(spec.get("unit_tag") or spec.get("unit") or "").strip()
        # Core families we treat as unit-required
        if uf in ("currency", "percent", "rate", "ratio"):
            return True
        # If schema explicitly declares a unit tag/unit, treat as required (except year/time-ish)
        if ut:
            # Avoid requiring "year" units
            blob = (uf + " " + ut + " " + str(spec.get("name") or "")).lower()
            if "year" in blob or "time" in blob:
                return False
            return True
    except Exception:
        return False

def _fix32_has_token_unit_evidence(metric_row: dict) -> bool:
    """
    Token-level unit evidence heuristic:
    - Prefer structured fields: base_unit/unit/unit_tag/unit_family
    - Fall back to raw token containing '$' or '%' or currency code immediately adjacent.
    Deterministic; does NOT attempt any NLP.
    """
    try:
        m = metric_row if isinstance(metric_row, dict) else {}
        for k in ("base_unit", "unit", "unit_tag", "unit_family"):
            if str(m.get(k) or "").strip():
                return True

        raw = str(m.get("raw") or m.get("value") or "")
        if not raw:
            return False
        r = raw.strip()
        rl = r.lower()

        # direct symbol evidence on the token
        if any(sym in r for sym in ("$", "€", "£", "¥", "%")):
            return True

        # compact currency codes adjacent to the token
        # NOTE: keep conservative (requires code in same token string)
        if any(code in rl for code in ("usd", "sgd", "eur", "gbp", "aud", "cad", "jpy", "cny")):
            return True

    except Exception:
        return False
    return False

def _yureeka_diff_metrics_by_name_v24(prev_response: dict, cur_response: dict):
    """
    FIX32 wrapper: calls existing diff, then enforces the unit-required hard gate at render time.
    """
    if not callable(diff_metrics_by_name_FIX31_BASE):
        # Fallback: nothing we can do
        return ([], 0, 0, 0, 0)

    metric_changes, unchanged, increased, decreased, found = diff_metrics_by_name_FIX31_BASE(prev_response, cur_response)

    # =====================================================================
    # PATCH FIX2B_EVO_CURFIELDS_V2 (ADDITIVE): disabled due to indentation corruption; FIX2B_EVO_CURFIELDS_V1 handles canonical-first enrichment
    # =====================================================================


    try:
        if not isinstance(metric_changes, list):
            return metric_changes, unchanged, increased, decreased, found

        for row in metric_changes:
            if not isinstance(row, dict):
                continue

            md = row.get("metric_definition") or {}
            if not _fix32_metric_requires_unit(md):
                continue

            cur_unit_cmp = str(row.get("cur_unit_cmp") or "").strip()
            # If diff already says mismatch, keep it; we only add the missing-unit mismatch
            if cur_unit_cmp:
                continue

            # Determine whether the current-side metric row shows any unit evidence at all
            # We use the current canonical row when present; else fall back to row fields.
            cur_metrics = (cur_response or {}).get("primary_metrics_canonical") or {}
            ck = row.get("canonical_key") or ""
            cm = cur_metrics.get(ck) if isinstance(cur_metrics, dict) else None

            has_ev = _fix32_has_token_unit_evidence(cm or {})
            if not has_ev:
                # Enforce: missing unit where required => mismatch + no current value rendered
                row["unit_mismatch"] = True
                row["cur_unit_cmp"] = ""
                row["current_value"] = "N/A"
                row["cur_value_norm"] = None

                # Make the reason machine-detectable (non-breaking extra field)
                row.setdefault("guardrail_reason", "unit_required_missing_current_unit")

                # Re-classify change as unit_mismatch (keeps deterministic reporting)
                row["change_type"] = "unit_mismatch"
                row["change_pct"] = None

    except Exception:
        return metric_changes, unchanged, increased, decreased, found

# ==============================================================================
# END FIX32
# ==============================================================================
# =====================================================================
# PATCH V23_CANONICAL_FOR_RENDER_FORCE_CLEAR (ADDITIVE)
# Goal: When the current evolution canonical dict is "present but junk" (year-like / unitless winners),
# force the canonical-for-render rebuild path to actually run.
#
# Root cause (observed in v22 outputs):
# - canonical_for_render is initially assigned from current_metrics (non-empty)
# - _need_render_rebuild can become True due to suspiciousness
# - BUT rebuild attempts are guarded by `if (not canonical_for_render)` so they never execute
#   when canonical_for_render is already non-empty.
#
# Fix (render-only):
# - Wrap diff_metrics_by_name so that when we detect a suspicious current canonical dict,
#   we pass an empty current canonical map into the base diff function.
# - This triggers the existing v21/v22 rebuild ladder to actually run.
# - Fully auditable: adds diag markers at row-level and (optionally) at top-level.
# =====================================================================
try:
    diff_metrics_by_name_FIX32_V22_BASE
except Exception:
    pass
    diff_metrics_by_name_FIX32_V22_BASE = None

try:
    diff_metrics_by_name_FIX31_BASE
except Exception:
    pass
    diff_metrics_by_name_FIX31_BASE = None


def _ph2b_v23_yearlike(_x):
    try:
        if _x is None:
            return False
        fx = float(_x)
        if abs(fx - round(fx)) < 1e-9:
            ix = int(round(fx))
            return 1900 <= ix <= 2105
        return False
    except Exception:
        return False


def _ph2b_v23_metric_suspicious(_m):
    try:
        if not isinstance(_m, dict):
            return True
        u = (str(_m.get('unit') or _m.get('unit_tag') or '').strip())
        vn = _m.get('value_norm')
        # Most problematic pattern: unitless + yearlike/None
        if (not u) and (_ph2b_v23_yearlike(vn) or vn is None):
            return True
        # Also treat empty raw+unit as suspicious when vn is present but unitless
        raw = (str(_m.get('raw') or '').strip())
        if vn is not None and (not u) and (not raw):
            return True
        return False
    except Exception:
        return True


def _ph2b_v23_current_metrics_suspicious(_cm_map):
    try:
        if not isinstance(_cm_map, dict) or not _cm_map:
            return True
        ks = list(sorted(list(_cm_map.keys())))[:25]
        if not ks:
            return True
        sus = 0
        tot = 0
        for k in ks:
            tot += 1
            if _ph2b_v23_metric_suspicious(_cm_map.get(k)):
                sus += 1
        if tot <= 0:
            return True
        return (sus / float(tot)) >= 0.30
    except Exception:
        return False


def diff_metrics_by_name_FIX33_V23_CANONICAL_CLEAR(prev_response, cur_response, *args, **kwargs):
    """Wrapper around the existing diff implementation to force render-only rebuild when current canonical is suspicious."""
    if not callable(diff_metrics_by_name_FIX31_BASE):
        # If we can't find the base function, fall back to any existing diff impl
        if callable(diff_metrics_by_name_FIX32_V22_BASE):
            return diff_metrics_by_name_FIX32_V22_BASE(prev_response, cur_response, *args, **kwargs)
        return []

    _cur = cur_response
    _forced_clear = False
    _sus = False
    try:
        cm = None
        if isinstance(cur_response, dict):
            cm = cur_response.get('primary_metrics_canonical')
        _sus = bool(_ph2b_v23_current_metrics_suspicious(cm))
        if _sus:
            _forced_clear = True
            # Shallow-copy cur_response and clear primary_metrics_canonical only.
            _cur = dict(cur_response) if isinstance(cur_response, dict) else cur_response
            if isinstance(_cur, dict):
                # Preserve original for audit
                _cur.setdefault('diag', {})
                if isinstance(_cur.get('diag'), dict):
                    _cur['diag'].setdefault('ph2b_v23_force_clear_current_metrics', True)
                    try:
                        _cur['diag'].setdefault('ph2b_v23_force_clear_reason', 'suspicious_current_metrics_triggered_render_rebuild')
                    except Exception:
                        pass
                # Force empty so v21/v22 rebuild ladder actually runs
                _cur['primary_metrics_canonical'] = {}
    except Exception:
        pass
        _cur = cur_response
        _forced_clear = False

    rows = diff_metrics_by_name_FIX31_BASE(prev_response, _cur, *args, **kwargs)

    # PATCH V23_ROW_DIAG (ADDITIVE): mark every returned row so we can confirm v23 wrapper ran
    try:
        if _forced_clear and isinstance(rows, list):
            for r in rows:
                if isinstance(r, dict):
                    r.setdefault('diag', {})
                    if isinstance(r.get('diag'), dict):
                        r['diag'].setdefault('ph2b_v23_force_clear_applied', True)
                        r['diag'].setdefault('ph2b_v23_force_clear_suspicious', bool(_sus))
    except Exception:
        return rows


# PATCH V23_WIRE (ADDITIVE): Replace the public diff entrypoint used by evolution with the v23 wrapper.
try:
    if callable(diff_metrics_by_name_FIX33_V23_CANONICAL_CLEAR):
        diff_metrics_by_name = diff_metrics_by_name_FIX33_V23_CANONICAL_CLEAR
except Exception:
    pass

# PATCH V23_VERSION_BUMP (ADDITIVE): bump CODE_VERSION for audit
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass



# =====================================================================
# PATCH V24_STRICT_CANONICAL_KEY_MATCH (ADDITIVE)
# Goal: When canonical-for-render is active, enforce strict canonical_key identity for current-side lookup.
# This blocks all name/heuristic fallback matching that can substitute unrelated canon metrics (e.g., 2.0 B, 170.0, 2030.0).
#
# Activation: cur_response contains _ph2b_strict_ckey_v24 == True.
#
# Safety: render/diff-layer only. Does not touch fastpath/hashing/injection/snapshot attach.
# =====================================================================

# ==============================================================================
# DIFF ENGINE CONSOLIDATION (REFACTOR14): single public diff entrypoint
# - Previous override chain has been renamed into *_wrap1 / *_fix31 / *_v24 impls.
# - Public diff_metrics_by_name is now a stable wrapper around the v24 impl.
# ==============================================================================
def diff_metrics_by_name(prev_response: dict, cur_response: dict):
    return _yureeka_diff_metrics_by_name_v24(prev_response, cur_response)

try:
    diff_metrics_by_name_V24_BASE = _yureeka_diff_metrics_by_name_v24  # type: ignore
except Exception:
    pass
    diff_metrics_by_name_V24_BASE = None  # type: ignore

def _v24_num(x):
    try:
        if x is None:
            return None
        if isinstance(x, (int, float)):
            return float(x)
        s = str(x).strip()
        if not s:
            return None
        # remove common commas
        s = s.replace(",", "")
        return float(s)
    except Exception:
        return None

def diff_metrics_by_name_FIX34_V24_STRICT(prev_response: dict, cur_response: dict):
    """Strict diff: current metric is looked up ONLY by canonical_key when v24 strict flag is set."""
    # If not in strict mode, fall back to existing diff implementation.
    try:
        if not (isinstance(cur_response, dict) and cur_response.get("_ph2b_strict_ckey_v24")):
            if callable(diff_metrics_by_name_V24_BASE):
                return diff_metrics_by_name_V24_BASE(prev_response, cur_response)
            return ([], 0, 0, 0, 0)
    except Exception:
        pass
        if callable(diff_metrics_by_name_V24_BASE):
            return diff_metrics_by_name_V24_BASE(prev_response, cur_response)
        return ([], 0, 0, 0, 0)

    prev_can = (prev_response or {}).get("primary_metrics_canonical") or {}
    cur_can = (cur_response or {}).get("primary_metrics_canonical") or {}

    metric_changes = []
    unchanged = increased = decreased = found = 0

    try:
        if not isinstance(prev_can, dict):
            prev_can = {}
        if not isinstance(cur_can, dict):
            cur_can = {}

        # Use prev keys as the authoritative set for diffing (matches Analysis behavior).
        for ckey in prev_can.keys():
            pm = prev_can.get(ckey) if isinstance(prev_can.get(ckey), dict) else (prev_can.get(ckey) or {})
            cm = cur_can.get(ckey) if isinstance(cur_can.get(ckey), dict) else (cur_can.get(ckey) or None)

            row = {
                "canonical_key": ckey,
                "metric_name": (pm.get("name") if isinstance(pm, dict) else "") or ckey,
                "previous_value": (pm.get("value_norm") if isinstance(pm, dict) else None),
                "current_value": None,
                "prev_unit_cmp": (pm.get("unit") if isinstance(pm, dict) else "") or "",
                "cur_unit_cmp": "",
                "prev_value_norm": (pm.get("value_norm") if isinstance(pm, dict) else None),
                "cur_value_norm": None,
                "unit_mismatch": False,
                "change_type": "not_found",
                "confidence": pm.get("confidence") if isinstance(pm, dict) else None,
                "metric_definition": pm.get("metric_definition") if isinstance(pm, dict) else None,
            }

            if isinstance(cm, dict) and cm:
                found += 1
                cvn = cm.get("value_norm")
                cunit = (cm.get("unit") or cm.get("unit_tag") or "").strip()
                craw = (cm.get("raw") or "").strip()
                if not craw:
                    try:
                        if cvn is not None and cunit:
                            craw = f"{cvn} {cunit}".strip()
                        elif cvn is not None:
                            craw = str(cvn)
                    except Exception:
                        pass
                        craw = ""
                row["current_value"] = craw
                row["cur_value_norm"] = cvn
                row["current_value_norm"] = cvn
                row["cur_unit_cmp"] = cunit
                row["current_unit"] = cunit

                pv = _v24_num(row.get("previous_value"))
                cv = _v24_num(cvn)
                if pv is not None and cv is not None:
                    if abs(cv - pv) < 1e-9:
                        row["change_type"] = "unchanged"
                        unchanged += 1
                    elif cv > pv:
                        row["change_type"] = "increased"
                        increased += 1
                    else:
                        row["change_type"] = "decreased"
                        decreased += 1
                else:
                    # We found a row but cannot compare numerically
                    row["change_type"] = "found"

                # Attach v24 audit
                row.setdefault("diag", {})
                if isinstance(row.get("diag"), dict):
                    row["diag"].setdefault("canonical_key_strict_v1", {})
                    row["diag"]["canonical_key_strict_v1"]["enabled"] = True
                    row["diag"]["canonical_key_strict_v1"]["used_key"] = ckey
                    row["diag"]["canonical_key_strict_v1"]["fallback_blocked"] = True
            else:
                # Not found: keep blank. Still attach audit.
                row.setdefault("diag", {})
                if isinstance(row.get("diag"), dict):
                    row["diag"].setdefault("canonical_key_strict_v1", {})
                    row["diag"]["canonical_key_strict_v1"]["enabled"] = True
                    row["diag"]["canonical_key_strict_v1"]["used_key"] = ckey
                    row["diag"]["canonical_key_strict_v1"]["fallback_blocked"] = True
                    row["diag"]["canonical_key_strict_v1"]["not_found"] = True

            metric_changes.append(row)

    except Exception:
        pass
        # On failure, revert to existing diff
        if callable(diff_metrics_by_name_V24_BASE):
            return diff_metrics_by_name_V24_BASE(prev_response, cur_response)
        return ([], 0, 0, 0, 0)

    return metric_changes, unchanged, increased, decreased, found

# PATCH V24_WIRE (ADDITIVE): override diff_metrics_by_name entrypoint with v24 strict-aware wrapper.
try:
    if callable(diff_metrics_by_name_FIX34_V24_STRICT):
        diff_metrics_by_name = diff_metrics_by_name_FIX34_V24_STRICT  # type: ignore
except Exception:
    pass

# PATCH V24_VERSION_BUMP (ADDITIVE)
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass


# =====================================================================
# PATCH FIX41AFC19_V25 (ADDITIVE): CODE_VERSION bump (audit)
# =====================================================================
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass
# =====================================================================
# END PATCH FIX41AFC19_V25
# =====================================================================


# =====================================================================
# PATCH CODE_VERSION_V26 (ADDITIVE)
# =====================================================================
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
# =====================================================================
# END PATCH CODE_VERSION_V26
# =====================================================================

# =====================================================================
# PATCH V27_VERSION_BUMP (ADDITIVE)
# =====================================================================
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass
# =====================================================================
# END PATCH V27_VERSION_BUMP
# =====================================================================

# =====================================================================
# PATCH V28_VERSION_BUMP (ADDITIVE): bump CODE_VERSION for audit
# =====================================================================
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass
# =====================================================================
# END PATCH V28_VERSION_BUMP
# =====================================================================


# =====================================================================
# PATCH V29_CODE_VERSION_BUMP (ADDITIVE)
# =====================================================================
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
# =====================================================================
# END PATCH V29_CODE_VERSION_BUMP
# =====================================================================


# =====================================================================
# PATCH V32_PREFER_CUR_PRIMARY_METRICS_CANONICAL (ADDITIVE)
# Goal: For Evolution diff panel, prefer cur_response['primary_metrics_canonical'][canonical_key] as the source of CURRENT
#       when present. This avoids render-only snapshot rebuild empties and eliminates raw numeric pool fallbacks.
# Safety: diff/render-layer only. Does not touch fastpath replay / hashing universe / injection lifecycle / snapshot attach / extraction.
#
# Adds per-row audit:
#   row['diag']['v32_current_source_v1'] = {
#       'cur_has_pmc': bool,
#       'used_current_source': 'cur_primary_metrics_canonical' | 'base_diff' | 'none',
#       'ckey': canonical_key,
#   }
# Also adds a tiny debug counter into cur_response (harmless):
#   cur_response['_debug_v32_pmc_used_count'] (int)
# =====================================================================

try:
    diff_metrics_by_name_V32_BASE = diff_metrics_by_name  # type: ignore
except Exception:
    pass
    diff_metrics_by_name_V32_BASE = None  # type: ignore

def _v32_safe_str(x):
    try:
        if x is None:
            return ""
        return str(x)
    except Exception:
        return ""

def _v32_pick_canon_metric(cur_pmc: dict, ckey: str):
    try:
        if not isinstance(cur_pmc, dict):
            return None
        m = cur_pmc.get(ckey)
        return m if isinstance(m, dict) else None
    except Exception:
        return None

def _v32_extract_value_unit_raw(cm: dict):
    '''
    Extract (value_norm, unit, raw_display) without numeric inference.
    Uses a few known canonical fields and then evidence[0] as a last resort.
    '''
    try:
        if not isinstance(cm, dict):
            return (None, "", "")
        cvn = cm.get("value_norm", None)
        unit = (cm.get("unit") or cm.get("unit_tag") or cm.get("unit_cmp") or "").strip()
        raw = (cm.get("value_display") or cm.get("value_range_display") or cm.get("raw") or "").strip()
        if (cvn is None or unit == "") and isinstance(cm.get("evidence"), list) and cm["evidence"]:
            ev0 = cm["evidence"][0] if isinstance(cm["evidence"][0], dict) else None
            if isinstance(ev0, dict):
                if cvn is None:
                    cvn = ev0.get("value_norm", ev0.get("value", None))
                if unit == "":
                    unit = (ev0.get("unit") or ev0.get("unit_tag") or ev0.get("unit_cmp") or "").strip()
                if not raw:
                    raw = (ev0.get("raw") or "").strip()
        if not raw:
            try:
                if cvn is not None and unit:
                    raw = f"{cvn} {unit}".strip()
                elif cvn is not None:
                    raw = str(cvn)
            except Exception:
                pass
                raw = ""
        return (cvn, unit, raw)
    except Exception:
        return (None, "", "")

def diff_metrics_by_name_FIX40_V32_PREFER_PMC(prev_response: dict, cur_response: dict):
    '''
    Wrapper around existing diff_metrics_by_name:
      - Computes base metric_changes using existing logic
      - Then, if cur_response['primary_metrics_canonical'] contains an entry for the same canonical_key,
        overwrites CURRENT fields from that canonical metric (NO numeric inference).
      - Emits per-row audit under row['diag']['v32_current_source_v1'].
    '''
    # Run base implementation first
    if callable(diff_metrics_by_name_V32_BASE):
        metric_changes, unchanged, increased, decreased, found = diff_metrics_by_name_V32_BASE(prev_response, cur_response)
    else:
        return ([], 0, 0, 0, 0)

    try:
        cur_pmc = (cur_response or {}).get("primary_metrics_canonical") if isinstance(cur_response, dict) else None
        cur_has_pmc = isinstance(cur_pmc, dict) and len(cur_pmc) > 0
        used_count = 0

        if isinstance(metric_changes, list):
            for row in metric_changes:
                try:
                    if not isinstance(row, dict):
                        continue
                    ckey = row.get("canonical_key") or row.get("ckey") or ""
                    ckey = _v32_safe_str(ckey).strip()
                    row.setdefault("diag", {})
                    if isinstance(row.get("diag"), dict):
                        row["diag"].setdefault("v32_current_source_v1", {})
                        row["diag"]["v32_current_source_v1"]["cur_has_pmc"] = bool(cur_has_pmc)
                        row["diag"]["v32_current_source_v1"]["ckey"] = ckey

                    if not (cur_has_pmc and ckey):
                        if isinstance(row.get("diag"), dict):
                            row["diag"]["v32_current_source_v1"]["used_current_source"] = "base_diff"
                        continue

                    cm = _v32_pick_canon_metric(cur_pmc, ckey)
                    if not isinstance(cm, dict) or not cm:
                        if isinstance(row.get("diag"), dict):
                            row["diag"]["v32_current_source_v1"]["used_current_source"] = "base_diff"
                        continue

                    cvn, unit, raw = _v32_extract_value_unit_raw(cm)
                    if cvn is None and not raw:
                        if isinstance(row.get("diag"), dict):
                            row["diag"]["v32_current_source_v1"]["used_current_source"] = "base_diff"
                        continue

                    # Override CURRENT fields
                    row["current_value"] = raw
                    row["cur_value_norm"] = cvn
                    row["current_value_norm"] = cvn
                    if unit:
                        row["cur_unit_cmp"] = unit
                        row["current_unit"] = unit

                    # Soft-clear unit_mismatch if both indicate percent
                    if unit and isinstance(row.get("prev_unit_cmp"), str) and row.get("prev_unit_cmp").strip() == "%" and unit.strip() == "%":
                        row["unit_mismatch"] = False

                    # Update change_type if numeric comparable (no inference)
                    try:
                        pv = row.get("prev_value_norm", row.get("previous_value", None))
                        pv_num = float(pv) if isinstance(pv, (int, float)) else None
                        cv_num = float(cvn) if isinstance(cvn, (int, float)) else None
                        if pv_num is not None and cv_num is not None:
                            if abs(cv_num - pv_num) < 1e-9:
                                row["change_type"] = "unchanged"
                            elif cv_num > pv_num:
                                row["change_type"] = "increased"
                            else:
                                row["change_type"] = "decreased"
                    except Exception:
                        pass

                    if isinstance(row.get("diag"), dict):
                        row["diag"]["v32_current_source_v1"]["used_current_source"] = "cur_primary_metrics_canonical"
                    used_count += 1
                except Exception:
                    pass
                    continue

        # attach a tiny debug counter into cur_response for audit (harmless)
        try:
            if isinstance(cur_response, dict):
                cur_response["_debug_v32_pmc_used_count"] = used_count
        except Exception:
            pass

    except Exception:
        return metric_changes, unchanged, increased, decreased, found

# PATCH V32_WIRE (ADDITIVE): override diff_metrics_by_name entrypoint with v32 wrapper.
try:
    if callable(diff_metrics_by_name_FIX40_V32_PREFER_PMC):
        diff_metrics_by_name = diff_metrics_by_name_FIX40_V32_PREFER_PMC  # type: ignore
except Exception:
    pass

# PATCH V32_VERSION_BUMP (ADDITIVE)
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass

# =====================================================================
# END PATCH V32_PREFER_CUR_PRIMARY_METRICS_CANONICAL
# =====================================================================


# =====================================================================
# PATCH V34_EVOLUTION_DIFF_ANCHOR_JOIN (ADDITIVE)
# Goal: Fix Evolution Diff Metrics panel by adding a STRICT, ORDERED secondary join on anchor_hash.
#       - Primary join remains canonical_key equality (unchanged).
#       - Secondary join (NEW): if current missing, map prev_anchor_hash -> current canonical_key via cur metric_anchors.
#       - Current value sourcing: ONLY from cur_response['primary_metrics_canonical'][resolved_cur_ckey].
#       - No other fallback: no name similarity, no unit-family matching, no numeric inference, no raw numeric pools.
# Safety: Evolution render/diff layer ONLY. Does not touch fastpath replay / hashing universe / injection lifecycle /
#         snapshot attach / extraction / Analysis rebuild logic.
#
# Adds per-row diagnostics (exact requested shape):
#   row['diag']['diff_join_trace_v1']
#   row['diag']['diff_current_source_trace_v1']
# Adds top-level debug summary:
#   cur_response['debug']['diff_join_anchor_v34']
# =====================================================================

try:
    diff_metrics_by_name_V34_BASE = diff_metrics_by_name  # type: ignore
except Exception:
    pass
    diff_metrics_by_name_V34_BASE = None  # type: ignore


def _v34_safe_str(x):
    try:
        if x is None:
            return ""
        return str(x)
    except Exception:
        return ""


def _v34_get_anchor_hash(metric_anchors: dict, ckey: str):
    try:
        if not isinstance(metric_anchors, dict) or not ckey:
            return None
        a = metric_anchors.get(ckey)
        if not isinstance(a, dict):
            return None
        ah = a.get("anchor_hash")
        ahs = _v34_safe_str(ah).strip()
        if not ahs or ahs.lower() == "none":
            return None
        return ahs
    except Exception:
        return None


def _v34_build_cur_anchor_index(cur_metric_anchors: dict):
    """Return map anchor_hash -> deterministic canonical_key (lexicographically smallest)"""
    idx = {}
    try:
        if not isinstance(cur_metric_anchors, dict):
            return idx
        for ckey, a in cur_metric_anchors.items():
            if not isinstance(ckey, str):
                ckey = _v34_safe_str(ckey)
            ckey_s = ckey.strip()
            if not ckey_s:
                continue
            if not isinstance(a, dict):
                continue
            ah = _v34_safe_str(a.get("anchor_hash")).strip()
            if not ah or ah.lower() == "none":
                continue
            prev = idx.get(ah)
            if prev is None or ckey_s < prev:
                idx[ah] = ckey_s
    except Exception:
        return idx


def diff_metrics_by_name_FIX41_V34_ANCHOR_JOIN(prev_response: dict, cur_response: dict):
    """
    Evolution Diff Metrics join fix:
      1) Run base diff to get row set (keeps primary ckey join behavior).
      2) For rows that are missing CURRENT due to canonical_key drift, attempt strict anchor_hash join.
      3) If resolved_cur_ckey found, source CURRENT ONLY from cur primary_metrics_canonical[resolved_cur_ckey].
      4) Emit requested diagnostics and top-level debug summary.
    """
    if not callable(diff_metrics_by_name_V34_BASE):
        return ([], 0, 0, 0, 0)

    metric_changes, unchanged, increased, decreased, found = diff_metrics_by_name_V34_BASE(prev_response, cur_response)

    # Build anchor indices
    prev_ma = (prev_response or {}).get("metric_anchors") if isinstance(prev_response, dict) else None
    cur_ma = (cur_response or {}).get("metric_anchors") if isinstance(cur_response, dict) else None
    cur_pmc = (cur_response or {}).get("primary_metrics_canonical") if isinstance(cur_response, dict) else None

    prev_ma = prev_ma if isinstance(prev_ma, dict) else {}
    cur_ma = cur_ma if isinstance(cur_ma, dict) else {}
    cur_pmc = cur_pmc if isinstance(cur_pmc, dict) else {}

    cur_anchor_idx = _v34_build_cur_anchor_index(cur_ma)

    joined_by_ckey = 0
    joined_by_anchor = 0
    not_found = 0
    sample_anchor_joins = []

    def _is_missing_current(row: dict):
        try:
            # Treat "" / None / "N/A" as missing.
            v = row.get("current_value")
            if v is None:
                return True
            vs = _v34_safe_str(v).strip()
            if not vs or vs.upper() == "N/A":
                return True
            # Some codepaths may store numeric current_value_norm only.
            cvn = row.get("cur_value_norm", row.get("current_value_norm", None))
            if (cvn is None) and (not vs):
                return True
            return False
        except Exception:
            return True

    try:
        if isinstance(metric_changes, list):
            for row in metric_changes:
                if not isinstance(row, dict):
                    continue

                row.setdefault("diag", {})
                if not isinstance(row.get("diag"), dict):
                    row["diag"] = {}

                # Determine prev_ckey (what the diff row is keyed on)
                prev_ckey = row.get("canonical_key") or row.get("ckey") or row.get("canonical") or ""
                prev_ckey = _v34_safe_str(prev_ckey).strip()

                # Primary join (ckey): if base diff already has a non-missing CURRENT, call that "ckey".
                resolved_cur_ckey = prev_ckey if prev_ckey else None
                method = "none"
                prev_anchor_hash = _v34_get_anchor_hash(prev_ma, prev_ckey) if prev_ckey else None
                cur_anchor_hash = None

                if prev_ckey and (not _is_missing_current(row)):
                    method = "ckey"
                    joined_by_ckey += 1
                    # cur_anchor_hash is best-effort for trace
                    cur_anchor_hash = _v34_get_anchor_hash(cur_ma, prev_ckey)
                else:
                    # Secondary join (anchor_hash)
                    if prev_anchor_hash and prev_anchor_hash in cur_anchor_idx:
                        resolved_cur_ckey = cur_anchor_idx.get(prev_anchor_hash)
                        if resolved_cur_ckey:
                            method = "anchor_hash"
                            joined_by_anchor += 1
                            cur_anchor_hash = _v34_get_anchor_hash(cur_ma, resolved_cur_ckey) or prev_anchor_hash
                            # Capture a small sample for debug
                            if len(sample_anchor_joins) < 6:
                                sample_anchor_joins.append({
                                    "prev_ckey": prev_ckey,
                                    "resolved_cur_ckey": resolved_cur_ckey,
                                    "anchor_hash": prev_anchor_hash,
                                })
                    else:
                        resolved_cur_ckey = None
                        method = "none"
                        not_found += 1

                # Per-row diagnostics (exact requested keys)
                try:
                    row["diag"]["diff_join_trace_v1"] = {
                        "prev_ckey": prev_ckey or None,
                        "resolved_cur_ckey": resolved_cur_ckey if resolved_cur_ckey else None,
                        "method": method,
                    "inference_bound": bool(method == "inference_bound"),
                        "prev_anchor_hash": prev_anchor_hash,
                        "cur_anchor_hash": cur_anchor_hash,
                    }
                except Exception:
                    pass

                # Current sourcing (strict)
                used_path = "none"
                cur_value_norm = None
                cur_unit_tag = None

                if method in ("ckey", "anchor_hash") and resolved_cur_ckey:
                    cm = cur_pmc.get(resolved_cur_ckey)
                    if isinstance(cm, dict) and cm:
                        # Prefer normalized values directly from canonical metric
                        cur_value_norm = cm.get("value_norm", cm.get("value", None))
                        cur_unit_tag = (cm.get("unit_tag") or cm.get("unit") or cm.get("unit_cmp") or "")
                        cur_unit_tag = _v34_safe_str(cur_unit_tag).strip() or None

                        # Update row CURRENT fields ONLY if we have something concrete
                        try:
                            if cur_value_norm is not None:
                                # build display similar to v32 helper but without inference
                                if cur_unit_tag:
                                    row["current_value"] = f"{cur_value_norm} {cur_unit_tag}".strip()
                                    row["current_unit"] = cur_unit_tag
                                    row["cur_unit_cmp"] = cur_unit_tag
                                else:
                                    row["current_value"] = _v34_safe_str(cur_value_norm)
                                row["cur_value_norm"] = cur_value_norm
                                row["current_value_norm"] = cur_value_norm
                                used_path = "primary_metrics_canonical"
                        except Exception:
                            pass

                # If still missing after strict sourcing, do NOT substitute.
                if used_path != "primary_metrics_canonical" and _is_missing_current(row):
                    # Ensure canonical not_found semantics stay as blank/N/A
                    used_path = "none"

                try:
                    row["diag"]["diff_current_source_trace_v1"] = {
                        "current_source_path_used": used_path,
                        "current_value_norm": cur_value_norm if used_path == "primary_metrics_canonical" else None,
                        "current_unit_tag": cur_unit_tag if used_path == "primary_metrics_canonical" else None,
                        "inference_disabled": False,
                    }
                except Exception:
                    pass

    except Exception:
        pass

    # Top-level debug summary (on cur_response)
    try:
        if isinstance(cur_response, dict):
            cur_response.setdefault("debug", {})
            if isinstance(cur_response.get("debug"), dict):
                cur_response["debug"]["diff_join_anchor_v34"] = {
                    "rows_total": len(metric_changes) if isinstance(metric_changes, list) else 0,
                    "joined_by_ckey": joined_by_ckey,
                    "joined_by_anchor_hash": joined_by_anchor,
                    "not_found": not_found,
                    "sample_anchor_joins": sample_anchor_joins,
                }
    except Exception:
        return metric_changes, unchanged, increased, decreased, found


# PATCH V34_WIRE (ADDITIVE): override diff_metrics_by_name entrypoint with v34 anchor join wrapper.
try:
    if callable(diff_metrics_by_name_FIX41_V34_ANCHOR_JOIN):
        diff_metrics_by_name = diff_metrics_by_name_FIX41_V34_ANCHOR_JOIN  # type: ignore
except Exception:
    pass

# PATCH V34_VERSION_BUMP (ADDITIVE)
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass

# =====================================================================
# END PATCH V34_EVOLUTION_DIFF_ANCHOR_JOIN
# =====================================================================


# =====================================================================
# PATCH V34_REBUILD_FN_ALIAS (ADDITIVE):
# Evolution compute_source_anchored_diff expects these function names:
#   - rebuild_metrics_from_snapshots_analysis_canonical_v1
#   - rebuild_metrics_from_snapshots_schema_only_fix16
# Some branches only define rebuild_metrics_from_snapshots_schema_only.
# Provide safe aliases so display-rebuild can run and populate Current values,
# without touching hashing/extraction/fastpath.
# =====================================================================

try:
    _v34_base_rebuild = globals().get("rebuild_metrics_from_snapshots_schema_only")
except Exception:
    pass
    _v34_base_rebuild = None

def rebuild_metrics_from_snapshots_schema_only_fix16(prev_response: dict, snapshot_pool: list, web_context: dict = None):  # noqa: F811
    """Alias wrapper for evolution display rebuild (FIX16 semantics live in base)."""
    try:
        fn = _v34_base_rebuild
        if callable(fn):
            try:
                return fn(prev_response, snapshot_pool, web_context=web_context)
            except TypeError:
                return fn(prev_response, snapshot_pool)
    except Exception:
        return {}

def rebuild_metrics_from_snapshots_analysis_canonical_v1(prev_response: dict, snapshot_pool: list, web_context: dict = None):  # noqa: F811
    """Alias wrapper; prefer schema-only rebuild to preserve deterministic behavior."""
    try:
        fn = globals().get("rebuild_metrics_from_snapshots_schema_only_fix16")
        if callable(fn):
            return fn(prev_response, snapshot_pool, web_context=web_context)
    except Exception:
        return {}

# PATCH V34_VERSION_BUMP (ADDITIVE)
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass

# =====================================================================
# PATCH V34C_DIFF_RESPONSE_UNWRAP (ADDITIVE)
# Purpose:
#   Evolution diff panel sometimes receives wrapper objects rather than the
#   direct payload containing primary_metrics_canonical / metric_anchors.
#   This patch adds a deterministic unwrapping shim and wires a v34c wrapper
#   that runs the existing v34 anchor-hash join logic against the unwrapped
#   payloads, while preserving strict no-fallback semantics.
# Non-negotiables:
#   - Evolution render/diff layer only
#   - No changes to fastpath, hashing, injection, snapshot attach, extraction, Analysis
# =====================================================================

def _v34c_unwrap_for_diff(resp):
    """
    Deterministically unwrap common wrapper shapes to obtain the payload dict that
    actually contains 'primary_metrics_canonical' and 'metric_anchors'.

    Returns: (payload_dict, path_str)
      - payload_dict is always a dict (may be the original dict)
      - path_str indicates the unwrap path used (for debugging only)
    """
    try:
        if not isinstance(resp, dict):
            return ({}, "non_dict")

        # If it already looks like the payload, stop.
        if isinstance(resp.get("primary_metrics_canonical"), dict) or isinstance(resp.get("metric_anchors"), dict):
            return (resp, "self")

        # Candidate unwrap paths (ordered, deterministic)
        candidates = [
            ("primary_response", ["primary_response"]),
            ("results.primary_response", ["results", "primary_response"]),
            ("results.response", ["results", "response"]),
            ("results.payload", ["results", "payload"]),
            ("payload", ["payload"]),
            ("response", ["response"]),
            ("data", ["data"]),
            ("result", ["result"]),
        ]

        for label, path in candidates:
            cur = resp
            ok = True
            for k in path:
                if isinstance(cur, dict) and (k in cur):
                    cur = cur.get(k)
                else:
                    ok = False
                    break
            if not ok:
                continue
            if isinstance(cur, dict):
                if isinstance(cur.get("primary_metrics_canonical"), dict) or isinstance(cur.get("metric_anchors"), dict):
                    return (cur, label)

        # Last resort: one-level scan for a dict that looks like a payload
        try:
            for k, v in list(resp.items()):
                if isinstance(v, dict) and (isinstance(v.get("primary_metrics_canonical"), dict) or isinstance(v.get("metric_anchors"), dict)):
                    return (v, f"scan.{k}")
        except Exception:
            return (resp, "self_no_payload_keys")
    except Exception:
        return ({}, "error")


def diff_metrics_by_name_FIX41_V34C_UNWRAP(prev_response: dict, cur_response: dict):
    """
    v34c wrapper:
      - unwrap prev/cur to the canonical payload dicts
      - run the existing v34 anchor-hash join wrapper logic against unwrapped payloads
      - write top-level debug summary onto the *outer* cur_response (and inner payload too, if different)
    """
    # Ensure we can call the v34 wrapper (wired in v34).
    if not callable(diff_metrics_by_name_FIX41_V34_ANCHOR_JOIN):
        # Fallback to base if present (keeps prior behavior)
        if callable(diff_metrics_by_name_V34_BASE):
            return diff_metrics_by_name_V34_BASE(prev_response, cur_response)
        return ([], 0, 0, 0, 0)

    prev_payload, prev_path = _v34c_unwrap_for_diff(prev_response)
    cur_payload, cur_path = _v34c_unwrap_for_diff(cur_response)

    # Run v34 join on the unwrapped payloads.
    out = diff_metrics_by_name_FIX41_V34_ANCHOR_JOIN(prev_payload, cur_payload)

    # Attach an additional small debug note on the outer response (does not alter required v34 key)
    try:
        if isinstance(cur_response, dict):
            cur_response.setdefault("debug", {})
            if isinstance(cur_response.get("debug"), dict):
                cur_response["debug"]["diff_join_anchor_v34c_unwrap"] = {
                    "prev_unwrap_path": prev_path,
                    "cur_unwrap_path": cur_path,
                    "prev_payload_has_pmc": bool(isinstance(prev_payload, dict) and isinstance(prev_payload.get("primary_metrics_canonical"), dict)),
                    "cur_payload_has_pmc": bool(isinstance(cur_payload, dict) and isinstance(cur_payload.get("primary_metrics_canonical"), dict)),
                    "prev_payload_has_metric_anchors": bool(isinstance(prev_payload, dict) and isinstance(prev_payload.get("metric_anchors"), dict)),
                    "cur_payload_has_metric_anchors": bool(isinstance(cur_payload, dict) and isinstance(cur_payload.get("metric_anchors"), dict)),
                }
    except Exception:
        pass

    # If payload is a different dict, mirror the same note for convenience (no harm if same).
    try:
        if isinstance(cur_payload, dict) and (cur_payload is not cur_response):
            cur_payload.setdefault("debug", {})
            if isinstance(cur_payload.get("debug"), dict):
                cur_payload["debug"]["diff_join_anchor_v34c_unwrap"] = {
                    "prev_unwrap_path": prev_path,
                    "cur_unwrap_path": cur_path,
                    "prev_payload_has_pmc": bool(isinstance(prev_payload, dict) and isinstance(prev_payload.get("primary_metrics_canonical"), dict)),
                    "cur_payload_has_pmc": bool(isinstance(cur_payload, dict) and isinstance(cur_payload.get("primary_metrics_canonical"), dict)),
                    "prev_payload_has_metric_anchors": bool(isinstance(prev_payload, dict) and isinstance(prev_payload.get("metric_anchors"), dict)),
                    "cur_payload_has_metric_anchors": bool(isinstance(cur_payload, dict) and isinstance(cur_payload.get("metric_anchors"), dict)),
                }
    except Exception:
        return out


# PATCH V34C_WIRE (ADDITIVE): override diff_metrics_by_name entrypoint with v34c unwrap wrapper.
try:
    if callable(diff_metrics_by_name_FIX41_V34C_UNWRAP):
        diff_metrics_by_name = diff_metrics_by_name_FIX41_V34C_UNWRAP  # type: ignore
except Exception:
    pass

# PATCH V34C_VERSION_BUMP (ADDITIVE)
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass

# =====================================================================
# END PATCH V34C_DIFF_RESPONSE_UNWRAP
# =====================================================================


# =====================================================================
# PATCH V34F_VERSION_BUMP (ADDITIVE)
# =====================================================================
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass



# ==============================================================================
# PATCH FIX2E_DIFFPANEL_V2_LASTMILE (ADDITIVE)
#
# Objective:
# - Close the final gap by ensuring the Evolution "metrics change" table feed
#   (results.metric_changes) is deterministically built from canonical outputs.
# - Implement Option B: if V2 rows exist, override the returned metric_changes list,
#   Legacy audit key removed; Diff Panel V2 is authoritative for metric changes.
#
# Safety:
# - Additive only. No refactors. No changes to fastpath/hashing/snapshots/extraction.
# - No fuzzy matching or inference. Only canonical_key then anchor_hash.
# ==============================================================================

def _diffpanel_v2__unwrap_primary_metrics_canonical(_resp):
    """Best-effort unwrap of a response-like dict to {canonical_key: metric_dict}.

    FIX2D37: prefer schema-keyed baseline map (baseline_schema_metrics_v1) when present.
    This keeps the diff universe explicitly schema-keyed and stable.
    """
    try:
        if isinstance(_resp, dict):
            # FIX2D37: schema baseline override (prev-side)
            try:
                res = _resp.get("results")
                if isinstance(res, dict) and isinstance(res.get("baseline_schema_metrics_v1"), dict):
                    return res.get("baseline_schema_metrics_v1")
            except Exception:
                pass
            bsm = _resp.get("baseline_schema_metrics_v1")
            if isinstance(bsm, dict):
                return bsm

            pmc = _resp.get("primary_metrics_canonical")
            if isinstance(pmc, dict):
                return pmc
            # common nestings
            pr = _resp.get("primary_response")
            if isinstance(pr, dict) and isinstance(pr.get("primary_metrics_canonical"), dict):
                return pr.get("primary_metrics_canonical")
            res = _resp.get("results")
            if isinstance(res, dict) and isinstance(res.get("primary_metrics_canonical"), dict):
                return res.get("primary_metrics_canonical")
        return {}
    except Exception:
        return {}


def _diffpanel_v2__extract_anchor_hash(_m: dict):
    try:
        if not isinstance(_m, dict):
            return None
        for k in ("anchor_hash", "metric_anchor_hash", "anchor"):
            v = _m.get(k)
            if isinstance(v, str) and v.strip():
                return v.strip()
        # sometimes nested
        a = _m.get("metric_anchor")
        if isinstance(a, dict):
            v = a.get("anchor_hash")
            if isinstance(v, str) and v.strip():
                return v.strip()
    except Exception:
        return None


def _diffpanel_v2__extract_value_norm_and_unit(_m: dict):
    """Strict canonical sourcing only (no inference)."""
    try:
        if not isinstance(_m, dict):
            return (None, None)
        v = _m.get("value_norm")
        if v is None:
            v = _m.get("value")  # still canonical field in some payloads
        unit = _m.get("base_unit") or _m.get("unit_tag") or _m.get("unit")
        return (v, unit)
    except Exception:
        return (None, None)


def build_diff_metrics_panel_v2__rows(prev_response: dict, cur_response: dict):
    """Return (rows, summary_dict).

    FIX2D26: enable *unit-first, context-bound inference* for baseline keys when strict joins miss.
    Inference is deterministic and unit-family constrained; it never promotes
    unitless year-like tokens.
    """
    prev_can = _diffpanel_v2__unwrap_primary_metrics_canonical(prev_response)
    cur_can = _diffpanel_v2__unwrap_primary_metrics_canonical(cur_response)


    # PATCH FIX2D2Q_PROVENANCE (ADDITIVE): capture admitted injected URLs (if any) so
    # we can stamp row-level provenance for Current values.
    _fix2d2q_inj_set = set()
    try:
        _d0 = cur_response.get('debug') if isinstance(cur_response, dict) else None
        _it = _d0.get('inj_trace_v1') if isinstance(_d0, dict) else None
        if not (_it and isinstance(_it, dict)):
            _it = cur_response.get('inj_trace_v1') if isinstance(cur_response, dict) else None
        if isinstance(_it, dict):
            _an = _it.get('admitted_norm')
            if isinstance(_an, list):
                _fix2d2q_inj_set = set([u for u in _an if isinstance(u, str) and u])
    except Exception:
        pass
        _fix2d2q_inj_set = set()


    # =====================================================================
    # PATCH FIX2D2O_BASELINE_KEYED_CURRENT (ADDITIVE)
    # Problem:
    # - Evolution can surface a different set of canonical keys than the Analysis baseline.
    # - Diff joins are baseline-keyed; if cur_can lacks the baseline keys, Current stays N/A.
    #
    # Fix:
    # - Synthesize a baseline-keyed current canonical map by inferring a best current
    #   candidate for EACH baseline ckey from already-extracted_numbers pools.
    # - Uses injected-first two-pass policy (injected-only, then global fallback).
    # - Unit-family gates + yearlike blocking preserved.
    # - Render-only: does NOT change canonical key generation; it only creates
    #   a current mapping for diff/join purposes.
    # =====================================================================
    try:
        if isinstance(prev_can, dict) and prev_can and isinstance(cur_can, dict):
            _prev_keys_set = set([k for k in prev_can.keys() if isinstance(k, str) and k])
            _cur_keys_set = set([k for k in cur_can.keys() if isinstance(k, str) and k])

            # Only engage if we observe a parity gap (missing baseline keys in current)
            _missing = sorted(list(_prev_keys_set - _cur_keys_set))
            if _missing:

                def _fx2d2n_is_yearlike(v, unit_tag=None):
                    try:
                        if unit_tag is not None and str(unit_tag).strip():
                            return False
                        fv = float(v)
                        if fv.is_integer():
                            iv = int(fv)
                            return 1900 <= iv <= 2100
                    except Exception:
                        return False
                    return False

                def _fx2d2n_unwrap_pool(resp: dict):
                    pool = []
                    if not isinstance(resp, dict):
                        return pool

                    def _add_from_sources(sources):
                        if not isinstance(sources, list):
                            return
                        for s in sources:
                            if not isinstance(s, dict):
                                continue
                            su = s.get('source_url') or s.get('url')
                            nums = s.get('extracted_numbers')
                            if not isinstance(nums, list):
                                continue
                            for n in nums:
                                if not isinstance(n, dict):
                                    continue
                                vn = n.get('value_norm')
                                if vn is None:
                                    vn = n.get('value')
                                try:
                                    fvn = float(vn)
                                except Exception:
                                    pass
                                    continue
                                ut = str(n.get('unit_tag') or n.get('unit') or n.get('base_unit') or '').strip()
                                raw = str(n.get('raw') or n.get('display') or n.get('value') or '').strip()
                                ctx = str(n.get('context_snippet') or n.get('context') or n.get('context_window') or '')
                                if _fx2d2n_is_yearlike(fvn, ut):
                                    continue
                                pool.append({
                                    'value_norm': fvn,
                                    'unit_tag': ut,
                                    'raw': raw,
                                    'context_snippet': ctx,
                                    'source_url': str(su).strip() if su else None,
                                })

                    # Direct keys
                    _add_from_sources(resp.get('baseline_sources_cache_current'))
                    if not pool:
                        _add_from_sources(resp.get('baseline_sources_cache'))

                    # Nested results
                    res = resp.get('results')
                    if isinstance(res, dict):
                        if not pool:
                            _add_from_sources(res.get('baseline_sources_cache_current'))
                        if not pool:
                            _add_from_sources(res.get('baseline_sources_cache'))
                        if not pool:
                            _add_from_sources(res.get('source_results'))
                    return pool

                def _fx2d2n_injected_url_set(resp: dict):
                    try:
                        dbg = resp.get('debug') if isinstance(resp, dict) else None
                        if isinstance(dbg, dict):
                            it = dbg.get('inj_trace_v1')
                            if isinstance(it, dict) and isinstance(it.get('admitted_norm'), list):
                                return set([u for u in it.get('admitted_norm') if isinstance(u, str) and u])
                        it = resp.get('inj_trace_v1') if isinstance(resp, dict) else None
                        if isinstance(it, dict) and isinstance(it.get('admitted_norm'), list):
                            return set([u for u in it.get('admitted_norm') if isinstance(u, str) and u])
                    except Exception:
                        return set()
                    return set()

                _pool_all = _fx2d2n_unwrap_pool(cur_response)
                _inj_set = _fx2d2n_injected_url_set(cur_response)

                # PATCH FIX2D2Q_POLICY (ADDITIVE):
                # In baseline-keyed diffing, injection simulates newer content discovered
                # by Evolution. We prefer injected candidates first, but we may optionally
                # fall back to the base/global pool when injected candidates are missing
                # or fail safety/semantic gates. To avoid confusion, we stamp provenance
                # on each synthesized current metric.
                _inj_strict = False
                try:
                    _dbg0 = cur_response.get('debug') if isinstance(cur_response, dict) else None
                    if isinstance(_dbg0, dict):
                        _inj_strict = bool(_dbg0.get('injection_strict_for_baseline_diff') or False)
                except Exception:
                    pass
                    _inj_strict = False

                def _expected_family(ckey: str, prev_unit: str):
                    ck = str(ckey or '').lower()
                    pu = str(prev_unit or '').lower()
                    if 'percent' in ck or pu in ('%', 'percent') or '%' in pu:
                        return 'percent'
                    if 'currency' in ck or 'usd' in pu or '$' in pu or 's$' in pu:
                        return 'currency'
                    if 'ev_sales' in ck or 'unit_sales' in ck or 'sales' in ck:
                        return 'unit_sales'
                    if 'unit_count' in ck or 'charg' in ck or 'station' in ck or 'count' in ck:
                        return 'unit_count'
                    return 'unknown'

                def _unit_family_ok(expected: str, cand: dict):
                    try:
                        u = str(cand.get('unit_tag') or '').lower()
                        c = str(cand.get('context_snippet') or '').lower()
                        r = str(cand.get('raw') or '').lower()
                        if expected == 'percent':
                            return ('%' in u) or ('percent' in u) or ('%' in r) or ('percent' in r) or ('%' in c) or (' percent' in c)
                        if expected == 'currency':
                            return ('$' in u) or ('usd' in u) or ('currency' in u) or ('$' in r) or ('usd' in r) or ('$' in c) or (' usd' in c) or ('billion' in c) or ('bn' in c)
                        if expected == 'unit_sales':
                            return ('m' == u) or (u in ('mn','m')) or ('unit' in u) or ('million' in u) or ('units' in u) or ('million' in c) or ('units' in c)
                        if expected == 'unit_count':
                            return ('count' in u) or ('unit' in u) or ('count' in c) or ('units' in c)
                        return True
                    except Exception:
                        return False

                def _score(prev_ckey: str, prev_name: str, prev_val_norm, cand: dict):
                    try:
                        if not isinstance(cand, dict):
                            return -1e9
                        vn = cand.get('value_norm')
                        if vn is None:
                            return -1e9
                        try:
                            vn = float(vn)
                        except Exception:
                            return -1e9
                        ut = str(cand.get('unit_tag') or '').strip()
                        if _fx2d2n_is_yearlike(vn, ut):
                            return -1e9
                        ctx = str(cand.get('context_snippet') or '').lower()
                        nm = str(prev_name or '').lower()
                        ck = str(prev_ckey or '').lower()
                        expected = _expected_family(prev_ckey, prev_can.get(prev_ckey, {}).get('unit_tag') if isinstance(prev_can.get(prev_ckey), dict) else '')
                        if expected != 'unknown' and not _unit_family_ok(expected, cand):
                            return -1e9

                        sc = 0.0
                        # bind key terms
                        for kw in ('sales','share','market','revenue','charg'):
                            if kw in ck and kw in ctx:
                                sc += 1.0
                            if kw in nm and kw in ctx:
                                sc += 0.8
                        # prefer same year in context
                        m = re.search(r'(19\d{2}|20\d{2})', ck)
                        if m and m.group(1) in ctx:
                            sc += 2.5
                        # mild magnitude sanity
                        if isinstance(prev_val_norm, (int, float)):
                            try:
                                sc -= min(5.0, abs(float(vn) - float(prev_val_norm)) / max(1e-9, abs(float(prev_val_norm))))
                            except Exception:
                                return sc
                    except Exception:
                        return -1e9

                def _select_for(prev_ckey: str, pm: dict):
                    prev_name = (pm.get('name') or pm.get('metric_name') or prev_ckey) if isinstance(pm, dict) else prev_ckey
                    prev_val = pm.get('value_norm') if isinstance(pm, dict) else None

                    # Pass 1: injected-only (preferred)
                    best = None
                    bests = -1e9
                    if _inj_set:
                        for cand in _pool_all:
                            if not isinstance(cand, dict):
                                continue
                            su = cand.get('source_url')
                            if not su or su not in _inj_set:
                                continue
                            sc = _score(prev_ckey, prev_name, prev_val, cand)
                            if sc > bests:
                                bests = sc
                                best = cand
                        if best is not None and bests > -1e8:
                            return best, True, float(bests), 'injected_first', 'injected'

                        # If strict injection demo mode is enabled, do NOT fall back to base.
                        if _inj_strict:
                            return None, None, None, 'injected_strict_no_match', None

                    # Pass 2: global/base fallback (allowed when not strict)
                    best = None
                    bests = -1e9
                    for cand in _pool_all:
                        sc = _score(prev_ckey, prev_name, prev_val, cand)
                        if sc > bests:
                            bests = sc
                            best = cand
                    if best is not None and bests > -1e8:
                        mode = 'base_only' if not _inj_set else 'base_fallback'
                        return best, False, float(bests), mode, 'base'
                    return None, None, None, 'no_candidate', None

# Synthesize missing baseline keys into cur_can
                _synth = {}
                for _ck in _missing:
                    _pm = prev_can.get(_ck)
                    if not isinstance(_pm, dict):
                        continue
                    cand, used_inj, score, _sel_mode, _src_type = _select_for(_ck, _pm)
                    if isinstance(cand, dict):
                        _synth[_ck] = {
                            'name': _pm.get('name') or _pm.get('metric_name') or _ck,
                            'value_norm': cand.get('value_norm'),
                            'value': cand.get('value_norm'),
                            'raw': cand.get('raw'),
                            'unit_tag': cand.get('unit_tag') or _pm.get('unit_tag'),
                            'sources': ([{'url': cand.get('source_url')}] if cand.get('source_url') else []),
                            'source_url': cand.get('source_url'),
                            'selector_used': 'inference_bound_baseline_keyed',
                            'current_source_type_fix2d2q': _src_type,
                            'current_selection_mode_fix2d2q': _sel_mode,
                            'diag_fix2d2n': {
                                'parity_gap': True,
                                'missing_baseline_ckey': _ck,
                                'selected_from_injected_pass1': bool(used_inj),
                                'selected_score': score,
                                'selection_mode_fix2d2q': _sel_mode,
                                'selected_source_type_fix2d2q': _src_type,
                            },
                        }

                if _synth:
                    # copy-on-write: do not mutate original cur_can in case it is reused elsewhere
                    _cur_aug = dict(cur_can)
                    _cur_aug.update(_synth)
                    cur_can = _cur_aug

                    # attach a tiny debug marker for auditability
                    if isinstance(cur_response, dict):
                        _dbg = cur_response.setdefault('debug', {})
                        if isinstance(_dbg, dict):
                            _dbg.setdefault('fix2d2n_baseline_keyed_current', {})
                            if isinstance(_dbg.get('fix2d2n_baseline_keyed_current'), dict):
                                _dbg['fix2d2n_baseline_keyed_current'].update({
                                    'enabled': True,
                                    'missing_baseline_keys': list(_missing)[:25],
                                    'synthesized_count': int(len(_synth)),
                                })

                                # ============================================================
                                # PATCH FIX2D2O_PERSIST_BASELINE_KEYED_CURRENT (ADDITIVE)
                                # Persist the baseline-keyed augmented canonical map onto the
                                # current response so downstream renderers (and JSON inspection)
                                # can treat Analysis keys as the current authority for diffing.
                                # Safety: additive; does not remove Evolution-native keys.
                                # ============================================================
                                try:
                                    # Expose explicitly for diff consumers
                                    cur_response.setdefault('primary_metrics_canonical_for_diff', cur_can)
                                    if isinstance(cur_response.get('results'), dict):
                                        cur_response['results'].setdefault('primary_metrics_canonical_for_diff', cur_can)
                                    # For demo parity, also mirror into primary_metrics_canonical when parity gap exists
                                    # (keeps extras available via primary_metrics_canonical_extras if present).
                                    if isinstance(cur_response.get('primary_metrics_canonical'), dict):
                                        cur_response['primary_metrics_canonical'] = dict(cur_can)
                                    elif isinstance(cur_response.get('results'), dict) and isinstance(cur_response['results'].get('primary_metrics_canonical'), dict):
                                        cur_response['results']['primary_metrics_canonical'] = dict(cur_can)
                                except Exception:
                                    pass
                                # ============================================================
    except Exception:
        pass
    # =====================================================================

    rows = []
    summary = {
        # PATCH FIX2D2I_STAMP (ADD): unambiguous runtime stamp for which V2 builder produced rows
        "builder_id": "FIX2D2I__rows_core",
        "rows_total": 0,
        "joined_by_ckey": 0,
        "joined_by_anchor_hash": 0,
        "joined_by_inference": 0,
        "not_found": 0,
        "sample_anchor_joins": [],
        "sample_inference_joins": [],
    }

    if not isinstance(prev_can, dict):
        prev_can = {}
    if not isinstance(cur_can, dict):
        cur_can = {}

    prev_keys = sorted(prev_can.keys())
    summary["rows_total"] = len(prev_keys)

    # Sentinel row if prev empty
    if not prev_keys:
        rows.append({
            "canonical_key": "__no_prev_primary_metrics_canonical__",
            "name": "No previous canonical metrics",
            "previous_value": None,
            "current_value": None,
            "previous_unit": None,
            "current_unit": None,
            "change_type": "no_prev_metrics",
            "confidence": 0.0,
            "diag": {
                "diff_join_trace_v1": {
                    "prev_ckey": None,
                    "resolved_cur_ckey": None,
                    "method": "none",
                    "prev_anchor_hash": None,
                    "cur_anchor_hash": None,
                },
                "diff_current_source_trace_v1": {
                    "current_source_path_used": "none",
                    "current_value_norm": None,
                    "current_unit_tag": None,
                    "inference_disabled": False,
                    "inference_commit_v1": {
                        "inference_selected": bool(method_effective == "inference_bound"),
                        "inference_committed": bool(method_effective == "inference_bound" and cur_val_norm_effective is not None),
                        "committed_value_norm": (cur_val_norm_effective if method_effective == "inference_bound" else None),
                        "committed_raw": (cur_raw_effective if method_effective == "inference_bound" else None),
                        "committed_source_url": (cur_source_url_effective if method_effective == "inference_bound" else None),
                    },
                    "inference_commit_v2": {
                        "enabled": bool(_fix2d25_inference_enabled),
                        "attempted": bool(_fix2d25_inference_enabled and resolved_cur_ckey_effective is None and isinstance(prev_val_norm, (int, float))),
                        "selected": bool(_fix2d2e_inference_selected),
                        "selected_value_norm": (_fix2d2e_inference_selected.get('value_norm') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_raw": (_fix2d2e_inference_selected.get('raw') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_unit_tag": (_fix2d2e_inference_selected.get('unit_tag') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_source_url": (_fix2d2e_inference_selected.get('source_url') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_score": _fix2d2g_best_score,
                        "reason": _fix2d19_soft_reason,
                        "committed": bool(_fix2d2e_inference_committed),
                    },
                },
            },
        })
        summary["not_found"] = 1
        return rows, summary

    # Build cur index by anchor_hash
    cur_by_anchor = {}
    try:
        for ck in cur_can.keys():
            m = cur_can.get(ck)
            if not isinstance(m, dict):
                continue
            ah = _diffpanel_v2__extract_anchor_hash(m)
            if not ah:
                continue
            cur_by_anchor.setdefault(ah, [])
            cur_by_anchor[ah].append(ck)
        # deterministic tie-break
        for ah in list(cur_by_anchor.keys()):
            cur_by_anchor[ah] = sorted([c for c in cur_by_anchor[ah] if isinstance(c, str)])
    except Exception:
        pass
        cur_by_anchor = {}

    # ------------------------------
    # FIX2D25: inference config
    # ------------------------------
    _fix2d25_join_mode = None
    try:
        if "_fix2d6_get_diff_join_mode_v1" in globals() and callable(globals().get("_fix2d6_get_diff_join_mode_v1")):
            _fix2d25_join_mode = globals().get("_fix2d6_get_diff_join_mode_v1")()
    except Exception:
        pass
        _fix2d25_join_mode = None

    _fix2d25_inference_enabled = True
    try:
        # keep strict in non-union mode
        if str(_fix2d25_join_mode or "").lower() not in ("union", ""):
            _fix2d25_inference_enabled = False
    except Exception:
        pass
        _fix2d25_inference_enabled = True

    def _fix2d25_is_yearlike(_v, _unit_tag=None):
        try:
            if _unit_tag is not None and str(_unit_tag).strip():
                return False
            # prefer existing detector if present
            if "_fix2d24_is_yearlike_value" in globals() and callable(globals().get("_fix2d24_is_yearlike_value")):
                return bool(globals().get("_fix2d24_is_yearlike_value")(_v))
            fv = float(_v)
            if fv.is_integer():
                iv = int(fv)
                return 1900 <= iv <= 2100
        except Exception:
            return False

    def _fix2d25_expected_family(_ckey: str, _unit: str):
        c = str(_ckey or "").lower()
        u = str(_unit or "").lower()
        if "percent" in c or u in ("%", "percent", "percentage"):
            return "percent"
        if "currency" in c or any(x in u for x in ("usd", "$", "currency", "bn", "billion")):
            return "currency"
        if "unit_sales" in c or "sales" in c or "deliver" in c:
            return "unit_sales"
        if "unit_count" in c or "chargers" in c or "stations" in c or "count" in c:
            return "unit_count"
        return "unknown"

    def _fix2d25_collect_candidates(_cur_response: dict):
        out = []
        try:
            rr = (_cur_response or {}).get("results") if isinstance(_cur_response, dict) else None
            srcs = rr.get("source_results") if isinstance(rr, dict) else None
            if isinstance(srcs, list):
                for s in srcs:
                    if not isinstance(s, dict):
                        continue
                    su = s.get("source_url") or s.get("url")
                    nums = s.get("extracted_numbers")
                    if not isinstance(nums, list):
                        continue
                    for n in nums:
                        if not isinstance(n, dict):
                            continue
                        vn = n.get("value_norm")
                        ut = n.get("unit_tag") or n.get("unit")
                        if vn is None:
                            continue
                        if _fix2d25_is_yearlike(vn, ut):
                            continue
                        out.append({
                            "value_norm": vn,
                            "unit_tag": ut,
                            "raw": n.get("raw"),
                            "context": n.get("context_snippet") or n.get("context") or "",
                            "source_url": su,
                        })
        except Exception:
            return []
        return out

    _fix2d25_candidates = _fix2d25_collect_candidates(cur_response) if _fix2d25_inference_enabled else []

    # ------------------------------
    # FIX2D25: Guarded inference
    # ------------------------------
    def _fix2d25_get_join_mode():
        jm = None
        try:
            if "_fix2d6_get_diff_join_mode_v1" in globals():
                jm = globals().get("_fix2d6_get_diff_join_mode_v1")()
        except Exception:
            pass
            jm = None
        return str(jm or "").strip().lower()

    def _fix2d25_is_yearlike_value(v):
        try:
            fv = float(v)
            if not (1900.0 <= fv <= 2100.0):
                return False
            return abs(fv - round(fv)) < 1e-9
        except Exception:
            return False

    def _fix2d25_expected_family(prev_ckey: str, prev_unit: str):
        ck = str(prev_ckey or "").lower()
        pu = str(prev_unit or "").lower()
        if "percent" in ck or pu in ("%", "percent") or "%" in pu:
            return "percent"
        if "currency" in ck or "usd" in pu or "$" in pu:
            return "currency"
        if "unit_sales" in ck or " sales" in ck or ck.endswith("sales") or "ev_sales" in ck:
            return "unit_sales"
        if "unit_count" in ck or "charg" in ck or "station" in ck:
            return "unit_count"
        return "unknown"

    def _fix2d25_unit_family_match(expected: str, cand_unit: str, cand_ctx: str):
        u = str(cand_unit or "").lower()
        c = str(cand_ctx or "").lower()
        if expected == "percent":
            return ("%" in u) or ("percent" in u) or ("%" in c) or (" percent" in c)
        if expected == "currency":
            return ("$" in u) or ("usd" in u) or ("currency" in u) or ("$" in c) or (" usd" in c) or ("billion" in c) or ("bn" in c)
        if expected == "unit_sales":
            return ("unit" in u) or ("sales" in u) or ("million" in u) or ("m" == u) or (" units" in c) or (" million" in c)
        if expected == "unit_count":
            return ("unit" in u) or ("count" in u) or ("units" in c)
        return True

    # ------------------------------
    # FIX2D26: unit-first, context-bound eligibility
    # ------------------------------
    def _fix2d26_norm_ctx(s):
        try:
            return str(s or '').lower()
        except Exception:
            return ''

    def _fix2d26_required_tokens(expected: str):
        if expected == 'percent':
            return ['share', 'market']
        if expected == 'unit_sales':
            return ['sale']
        if expected == 'unit_count':
            return ['charg', 'station']
        if expected == 'currency':
            return ['invest', 'spend', 'capex', 'revenue', 'market']
        return []

    def _fix2d26_has_unit_cues(expected: str, unit_tag: str, ctx: str):
        u = _fix2d26_norm_ctx(unit_tag)
        c = _fix2d26_norm_ctx(ctx)
        if expected == 'percent':
            return ('%' in u) or ('%' in c) or (' percent' in c)
        if expected == 'unit_sales':
            return ('m' == u) or ('million' in u) or ('unit' in u) or (' million' in c) or (' units' in c) or (' million' in c)
        if expected == 'unit_count':
            return ('unit' in u) or ('count' in u) or (' units' in c) or (' station' in c) or (' charger' in c)
        if expected == 'currency':
            return ('$' in str(unit_tag or '')) or ('usd' in u) or ('$' in c) or (' usd' in c) or ('billion' in c) or (' bn' in c)
        return True

    def _fix2d26_is_eligible_candidate(expected: str, unit_tag: str, ctx: str, prev_ckey: str, prev_name: str):
        # Eligibility gate BEFORE scoring.
        c = _fix2d26_norm_ctx(ctx)
        # Unit-family must match strongly (do not rely on later score).
        if not _fix2d26_has_unit_cues(expected, unit_tag, c):
            return False, 'unit_family_miss'
        # Required domain tokens: at least one must appear for typed metrics.
        req = _fix2d26_required_tokens(expected)
        if req:
            if not any(t in c for t in req):
                # allow percent metrics if '%' present and the ckey/name already encodes share
                kw = _fix2d26_norm_ctx(prev_name or prev_ckey)
                if expected == 'percent' and ('share' in kw or 'market' in kw) and ('%' in c):
                    return True, None
                return False, 'domain_token_miss'
        return True, None


    def _fix2d25_infer_from_extracted_numbers(prev_ckey: str, prev_name: str, prev_v, prev_unit: str):
        """Return (cur_v, cur_unit, source_url, evidence_dict) or (None,...)."""
        expected = _fix2d25_expected_family(prev_ckey, prev_unit)
        year_hint = None
        try:
            m = re.search(r"\b(19\d{2}|20\d{2})\b", str(prev_ckey))
            if m:
                year_hint = m.group(1)
        except Exception:
            pass
            year_hint = None

        # Gather candidates
        candidates = []
        _rej_counts = {}  # FIX2D26: rejection reasons
        try:
            sr_list = (((cur_response or {}).get("results") or {}).get("source_results") or [])
            if isinstance(sr_list, dict):
                sr_list = [sr_list]
            for sr in sr_list:
                if not isinstance(sr, dict):
                    continue
                src_url = sr.get("source_url") or sr.get("url")
                en = sr.get("extracted_numbers") or []
                if isinstance(en, dict):
                    en = [en]
                for c in en:
                    if not isinstance(c, dict):
                        continue
                    v = c.get("value_norm")
                    if v is None:
                        continue
                    # never infer unitless years
                    cu = c.get("unit_tag")
                    if (not cu) and _fix2d25_is_yearlike_value(v):
                        continue
                    ctx = c.get("context_snippet") or c.get("context") or ""
                    ok, why = _fix2d26_is_eligible_candidate(expected, cu, ctx, prev_ckey, prev_name)
                    if not ok:
                        _rej_counts[why] = _rej_counts.get(why, 0) + 1
                        continue
                    candidates.append({
                        "value_norm": v,
                        "unit_tag": cu,
                        "context": ctx,
                        "source_url": src_url,
                        "raw": c.get("raw"),
                    })
        except Exception:
            pass
            candidates = []

        if not candidates:
            return None, None, None, None

        # Deterministic scoring
        best = None
        best_key = None
        prevv = None
        try:
            prevv = float(prev_v) if prev_v is not None else None
        except Exception:
            pass
            prevv = None
        kw = str(prev_name or prev_ckey or "").lower()
        for cand in candidates:
            v = cand.get("value_norm")
            cu = cand.get("unit_tag")
            ctx = cand.get("context") or ""
            score = 0
            # unit-family match already filtered; reward explicit unit
            if cu:
                score += 30
            # FIX2D26: keyword binding (required by expected type)
            _ctx_l = str(ctx).lower()
            _hits = 0
            if expected == 'percent':
                if ('%' in _ctx_l) or ('%' in str(cu or '').lower()):
                    _hits += 1
                if ('share' in _ctx_l) or ('market share' in _ctx_l) or ('ev share' in _ctx_l):
                    _hits += 1
            elif expected == 'unit_sales':
                if ('sale' in _ctx_l) or ('sold' in _ctx_l) or ('deliver' in _ctx_l):
                    _hits += 1
                if ('million' in _ctx_l) or (' units' in _ctx_l) or (str(cu or '').lower() in ('m','million','units','unit')):
                    _hits += 1
            elif expected == 'currency':
                if ('$' in _ctx_l) or ('usd' in _ctx_l) or ('billion' in _ctx_l) or ('bn' in _ctx_l):
                    _hits += 1
                if ('invest' in _ctx_l) or ('spend' in _ctx_l) or ('capex' in _ctx_l) or ('revenue' in _ctx_l):
                    _hits += 1
            elif expected == 'unit_count':
                if ('charg' in _ctx_l) or ('station' in _ctx_l) or ('infrastructure' in _ctx_l):
                    _hits += 1
                if ('count' in _ctx_l) or ('units' in _ctx_l) or ('number of' in _ctx_l):
                    _hits += 1
            if _hits > 0:
                score += 10 * _hits
            # year hint
            if year_hint and (year_hint in str(ctx)):
                score += 10
            # closeness to baseline (if available)
            try:
                fv = float(v)
                if prevv is not None and prevv > 0 and fv > 0:
                    ratio = fv / prevv
                    if 0.2 <= ratio <= 5.0:
                        score += 20
                    # prefer closer
                    score -= min(20.0, abs(np.log(ratio)) * 10.0)
            except Exception:
                pass

            # deterministic tie-break
            tie = (
                -score,
                0 if cand.get("source_url") else 1,
                str(cand.get("source_url") or ""),
                str(cand.get("raw") or ""),
                float(cand.get("value_norm") or 0.0),
            )
            if best is None or tie < best_key:
                best = cand
                best_key = tie

        if not best:
            return None, None, None, None
        try:
            if isinstance(best, dict):
                best.setdefault("diag", {})
                best["diag"]["fix2d26_candidate_count"] = int(len(candidates) or 0)
                best["diag"]["fix2d26_reject_counts"] = dict(_rej_counts or {})
                best["diag"]["fix2d26_policy"] = "unit_first_v1"
        except Exception:
            return best.get("value_norm"), best.get("unit_tag"), best.get("source_url"), best

    # =====================================================
    # PATCH FIX2D2A START: inference gate is now always-on (guarded)
    # Rationale: inference is already protected by FIX2D24/FIX2D26 guards;
    # disabling it in strict mode prevents binding current values entirely.
    # We keep an explicit kill-switch via EVO_DISABLE_DIFF_INFERENCE=1.
    # =====================================================
    _fix2d25_join_mode = _fix2d25_get_join_mode()
    _fix2d25_inference_enabled = True  # enabled by default (guarded)
    _fix2d2a_inference_gate_reason = 'enabled_default'
    try:
        import os as _os
        if str(_os.getenv('EVO_DISABLE_DIFF_INFERENCE','')).strip().lower() in ('1','true','yes','y'):
            _fix2d25_inference_enabled = False
            _fix2d2a_inference_gate_reason = 'disabled_by_env'
    except Exception:
        pass
    # PATCH FIX2D2A END

    for prev_ckey in prev_keys:
        pm = prev_can.get(prev_ckey)
        pm = pm if isinstance(pm, dict) else {}
        prev_name = str(pm.get("name") or pm.get("metric_name") or prev_ckey)
        prev_anchor = _diffpanel_v2__extract_anchor_hash(pm)

        resolved_cur_ckey = None
        method = "none"
        cur_anchor = None

        # Primary join: exact canonical key
        if prev_ckey in cur_can:
            resolved_cur_ckey = prev_ckey
            method = "ckey"
            summary["joined_by_ckey"] += 1
        else:
            # Secondary join: anchor hash
            if prev_anchor and prev_anchor in cur_by_anchor and cur_by_anchor.get(prev_anchor):
                resolved_cur_ckey = cur_by_anchor[prev_anchor][0]  # lexicographic min
                method = "anchor_hash"
                summary["joined_by_anchor_hash"] += 1
                if len(summary["sample_anchor_joins"]) < 5:
                    summary["sample_anchor_joins"].append({
                        "prev_ckey": prev_ckey,
                        "resolved_cur_ckey": resolved_cur_ckey,
                    })

        prev_v, prev_unit = _diffpanel_v2__extract_value_norm_and_unit(pm)

        cur_v = None
        cur_unit = None
        cur_source_url = None
        inference_used = False
        inference_evidence = None
        if resolved_cur_ckey and resolved_cur_ckey in cur_can and isinstance(cur_can.get(resolved_cur_ckey), dict):
            cm = cur_can.get(resolved_cur_ckey)
            cur_anchor = _diffpanel_v2__extract_anchor_hash(cm)
            cur_v, cur_unit = _diffpanel_v2__extract_value_norm_and_unit(cm)

        # FIX2D25: if join missed (or joined but no numeric), attempt safe inference for baseline keys
        if _fix2d25_inference_enabled and (resolved_cur_ckey is None or cur_v is None) and (prev_v is not None):
            iv, iu, isrc, ie = _fix2d25_infer_from_extracted_numbers(prev_ckey, prev_name, prev_v, prev_unit)
            if iv is not None:
                cur_v = iv
                cur_unit = iu
                cur_source_url = isrc
                inference_used = True
                inference_evidence = ie
                if resolved_cur_ckey is None:
                    # FIX2D27: promote inference from suggestive -> binding by
                    # binding the inferred current value to the baseline canonical key.
                    resolved_cur_ckey = prev_ckey
                    method = "inference_bound"
                    summary["joined_by_inference"] += 1
                    if len(summary.get("sample_inference_joins") or []) < 5:
                        summary["sample_inference_joins"].append({
                            "prev_ckey": prev_ckey,
                            "picked_value_norm": iv,
                            "picked_unit": iu,
                            "source_url": isrc,
                        })

        if resolved_cur_ckey is None:
            summary["not_found"] += 1

        # Basic change classification (no inference)
        change_type = "not_found" if resolved_cur_ckey is None else "unknown"
        try:
            if resolved_cur_ckey is not None:
                if prev_v is None or cur_v is None:
                    change_type = "unknown"
                else:
                    if float(cur_v) > float(prev_v):
                        change_type = "increased"
                    elif float(cur_v) < float(prev_v):
                        change_type = "decreased"
                    else:
                        change_type = "unchanged"
        except Exception:
            pass
            change_type = "unknown" if resolved_cur_ckey is not None else "not_found"



        # =====================================================
        # PATCH FIX2D29 START: binding inference -> commit Current fields
        # Goal: write-through inferred (or joined) current values into the exact
        #       fields the UI + diff engine render/read.
        #       This is the promotion step: suggestive inference -> binding inference.
        # =====================================================
        cur_display = None
        try:
            if inference_used and isinstance(inference_evidence, dict):
                cur_display = inference_evidence.get("display_value") or inference_evidence.get("raw")
        except Exception:
            pass
            cur_display = None
        if cur_display is None:
            cur_display = cur_v

        cur_source_final = cur_source_url
        try:
            if not cur_source_final and resolved_cur_ckey and isinstance(cur_can.get(resolved_cur_ckey), dict):
                _cm_src = cur_can.get(resolved_cur_ckey) or {}
                if isinstance(_cm_src, dict):
                    cur_source_final = _cm_src.get("source_url") or _cm_src.get("url") or None
                    if not cur_source_final and isinstance(_cm_src.get("sources"), list) and _cm_src.get("sources"):
                        _s0 = _cm_src.get("sources")[0]
                        if isinstance(_s0, dict):
                            cur_source_final = _s0.get("url") or _s0.get("source_url") or None
        except Exception:
            pass

        baseline_is_comparable = False
        try:
            baseline_is_comparable = (
                resolved_cur_ckey is not None
                and isinstance(prev_v, (int, float))
                and isinstance(cur_v, (int, float))
            )
        except Exception:
            pass
            baseline_is_comparable = False
        # PATCH FIX2D29 END
        rows.append({
            "canonical_key": prev_ckey,
            "name": prev_name,
            "previous_value": prev_v,
            "current_value": (cur_display if resolved_cur_ckey is not None else "N/A"),
            "current_value_norm": (cur_v if resolved_cur_ckey is not None else None),
            "current_source": cur_source_url_effective,
            "current_method": method_effective,
            "current_value_norm": cur_val_norm_effective,
            "change_pct": None,
            "change_type": "no_prev_metrics",
            "match_confidence": 0.0,
            "context_snippet": None,
            "source_url": None,
            "anchor_used": False,
            "prev_anchor_hash": None,
            "cur_anchor_hash": None,
            "prev_value_norm": None,
            "cur_value_norm": None,
            "diag": {
                "diff_join_trace_v1": {
                    "prev_ckey": None,
                    "resolved_cur_ckey": None,
                    "method": "none",
                    "prev_anchor_hash": None,
                    "cur_anchor_hash": None,
                },
                "diff_current_source_trace_v1": {
                    "current_source_path_used": "none",
                    "current_value_norm": None,
                    "current_unit_tag": None,
                    "inference_disabled": False,
                    "inference_commit_v1": {
                        "inference_selected": bool(method_effective == "inference_bound"),
                        "inference_committed": bool(method_effective == "inference_bound" and cur_val_norm_effective is not None),
                        "committed_value_norm": cur_val_norm_effective,
                        "committed_raw": cur_raw_effective,
                        "committed_source_url": cur_source_url_effective,
                    },
                    "inference_commit_v2": {
                        "enabled": bool(_fix2d25_inference_enabled),
                        "attempted": bool(_fix2d25_inference_enabled and resolved_cur_ckey_effective is None and isinstance(prev_val_norm, (int, float))),
                        "selected": bool(_fix2d2e_inference_selected is not None),
                        "selected_value_norm": (_fix2d2e_inference_selected.get('value_norm') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_raw": (_fix2d2e_inference_selected.get('raw') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_unit_tag": (_fix2d2e_inference_selected.get('unit_tag') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_source_url": (_fix2d2e_inference_selected.get('source_url') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_score": _fix2d2g_best_score,
                        "committed": bool(_fix2d2e_inference_committed),
                        "reason": _fix2d19_soft_reason,
                    },
                },
            },
        })
        summary = {
            "rows_total": 1,
            "joined_by_ckey": 0,
            "joined_by_anchor_hash": 0,
            "not_found": 0,
            "unit_mismatch_splits": 0,
            "sample_anchor_joins": [],
            "current_only_rows": 0,
            "current_only_injected_rows": 0,
            "current_only_from_extracted_pool": 0,
        }
        return rows, summary

    # Build core prev-anchored rows
    for prev_ckey, pm in (prev_metrics or {}).items():
        if not isinstance(prev_ckey, str) or not prev_ckey or not isinstance(pm, dict):
            continue

        prev_raw = _raw_display_value(pm) or "N/A"
        prev_val_norm = _canon_value_norm(pm)
        prev_unit = _canon_unit_tag(pm)
        prev_ah = _get_anchor_hash_for_ckey(prev_ckey, pm, prev_anchors)

        resolved_cur_ckey = None
        method = "none"

        # 1) exact canonical_key
        if isinstance(cur_metrics, dict) and prev_ckey in cur_metrics:
            resolved_cur_ckey = prev_ckey
            method = "ckey"

        # 2) anchor_hash
        if resolved_cur_ckey is None and prev_ah and prev_ah in cur_by_anchor:
            cands = cur_by_anchor.get(prev_ah) or []
            if cands:
                resolved_cur_ckey = cands[0]
                method = "anchor_hash"
                try:
                    if len(sample_anchor_joins) < 5:
                        sample_anchor_joins.append({
                            "anchor_hash": prev_ah,
                            "prev_ckey": prev_ckey,
                            "cur_ckey": resolved_cur_ckey,
                        })
                except Exception:
                    pass

        cm = None
        if resolved_cur_ckey is not None and isinstance(cur_metrics, dict):
            cm = cur_metrics.get(resolved_cur_ckey)
            if not isinstance(cm, dict):
                cm = None

        cur_raw = "N/A"
        cur_val_norm = None
        cur_unit = ""
        cur_source_url = None
        cur_ah = None

        if cm is not None:
            cur_raw = _raw_display_value(cm) or "N/A"
            cur_val_norm = _canon_value_norm(cm)
            cur_unit = _canon_unit_tag(cm)
            cur_ah = _get_anchor_hash_for_ckey(resolved_cur_ckey, cm, cur_anchors)
            try:
                _urls = _metric_source_urls(cm)
                if _urls:
                    cur_source_url = _urls[0]
            except Exception:
                pass
                cur_source_url = None

        # ----------------------------------------------------------
        # FIX2J: If we *would* join but the units differ, do not join.
        # Treat as not_found + allow the current metric to appear as
        # a separate current_only row later.
        # ----------------------------------------------------------
        unit_mismatch = False
        if resolved_cur_ckey is not None:
            pu = _unit_norm(prev_unit)
            cu = _unit_norm(cur_unit)
            if pu and cu and pu != cu:
                unit_mismatch = True
        # FIX2D2E: track inference selection/commit per-row
        _fix2d2e_inference_selected = None
        _fix2d2e_inference_committed = False

        if unit_mismatch:
            unit_mismatch_split += 1
            # undo match
            resolved_cur_ckey_effective = None
            method_effective = "none"
            cur_raw_effective = "N/A"
            cur_val_norm_effective = None
            cur_unit_effective = None
            cur_source_url_effective = None
            cur_ah_effective = None
        else:
            resolved_cur_ckey_effective = resolved_cur_ckey
            method_effective = method
            cur_raw_effective = cur_raw
            cur_val_norm_effective = cur_val_norm
            cur_unit_effective = cur_unit
            cur_source_url_effective = cur_source_url
            cur_ah_effective = cur_ah

            # -----------------------------------------------------------------
            # PATCH FIX2D2I (ADD): If the joined current value is unitless and
            # yearlike (e.g., 2024/2030), treat it as *blocked* for display and
            # enable inference fallback instead of leaving Current as N/A.
            # This preserves FIX2D24's invariant while allowing binding inference
            # to populate a valid current value from extracted_numbers pools.
            # -----------------------------------------------------------------
            _fix2d2h_joined_current_yearlike_blocked = False
            _fix2d2h_joined_current_yearlike_payload = None
            try:
                if resolved_cur_ckey_effective is not None:
                    _ut = str(cur_unit_effective or '').strip()
                    if not _ut and _fix2d25_is_yearlike_value(cur_val_norm_effective):
                        _fix2d2h_joined_current_yearlike_blocked = True
                        _fix2d2h_joined_current_yearlike_payload = {
                            'blocked_value_norm': cur_val_norm_effective,
                            'blocked_raw': cur_raw_effective,
                            'blocked_unit': _ut,
                            'blocked_source_url': cur_source_url_effective,
                            'blocked_method': method_effective,
                        }
                        # undo effective join so inference can run
                        resolved_cur_ckey_effective = None
                        method_effective = 'none'
                        cur_raw_effective = 'N/A'
                        cur_val_norm_effective = None
                        cur_unit_effective = None
                        cur_source_url_effective = None
                        cur_ah_effective = None
            except Exception:
                pass

            if resolved_cur_ckey_effective:
                matched_cur_ckeys.add(resolved_cur_ckey_effective)
                if method_effective == "ckey":
                    joined_by_ckey += 1
                elif method_effective == "anchor_hash":
                    joined_by_anchor += 1
                    if len(sample_anchor_joins) < 10:
                        sample_anchor_joins.append({
                            "anchor_hash": prev_ah,
                            "prev_ckey": prev_ckey,
                            "cur_ckey": resolved_cur_ckey_effective,
                        })

        if resolved_cur_ckey_effective is None:
            not_found += 1

        # ---------------------------------------------------------------------
        # PATCH FIX2D2G_INFERENCE_POOL (ADD): Deterministic extracted_numbers
        # pool unwrap for Diff Panel V2 __rows inference.
        # Why: previous code referenced an undefined pool helper on this path,
        # leaving inference with an empty candidate set.
        # Sources tried (in order):
        #   baseline_sources_cache_current -> baseline_sources_cache ->
        #   results.baseline_sources_cache_current -> results.baseline_sources_cache ->
        #   results.source_results
        # Safety: render-only; does not fetch or re-extract.
        # ---------------------------------------------------------------------
        def _fix2d2g_unwrap_extracted_numbers_pool(_resp: dict):
            pool = []
            if not isinstance(_resp, dict):
                return pool

            def _add_from_sources(_sources):
                if not isinstance(_sources, list):
                    return
                for s in _sources:
                    if not isinstance(s, dict):
                        continue
                    su = s.get('source_url') or s.get('url') or None
                    nums = s.get('extracted_numbers')
                    if not isinstance(nums, list):
                        continue
                    for n in nums:
                        if not isinstance(n, dict):
                            continue
                        vn = n.get('value_norm')
                        if vn is None:
                            vn = n.get('value')
                        try:
                            fvn = float(vn)
                        except Exception:
                            pass
                            continue
                        unit_tag = str(n.get('unit_tag') or n.get('unit') or n.get('base_unit') or '').strip()
                        raw = str(n.get('raw') or n.get('display') or n.get('value') or '').strip()
                        ctx = str(n.get('context_snippet') or n.get('context') or n.get('context_window') or '')

                        # PATCH FIX2D2I (ADD): Backfill unit_family for inference
                        unit_family = str(n.get('unit_family') or '').strip()
                        try:
                            if not unit_family:
                                ut_l = unit_tag.lower()
                                raw_l = raw.lower()
                                ctx_l = str(ctx or '').lower()
                                if '%' in raw_l or '%' in ut_l or 'percent' in raw_l or 'percent' in ut_l:
                                    unit_family = 'percent'
                                elif '$' in raw_l or '$' in ut_l or 'usd' in ut_l or 'usd' in ctx_l or 'billion' in ctx_l or 'bn' in ctx_l:
                                    unit_family = 'currency'
                                elif ut_l in ('m','mn') or 'million' in raw_l or 'million' in ctx_l or ' units' in ctx_l or 'units' in raw_l:
                                    unit_family = 'magnitude'
                        except Exception:
                            pass
                            unit_family = unit_family

                        pool.append({
                            'value_norm': fvn,
                            'unit_tag': unit_tag,
                            'raw': raw,
                            'context_snippet': ctx,
                            'unit_family': unit_family,
                            'measure_kind': n.get('measure_kind'),
                            'measure_assoc': n.get('measure_assoc'),
                            'source_url': str(su).strip() if su else None,
                        })

            # prefer direct keys
            _add_from_sources(_resp.get('baseline_sources_cache_current'))
            if not pool:
                _add_from_sources(_resp.get('baseline_sources_cache'))

            # nested under results
            res = _resp.get('results')
            if isinstance(res, dict):
                if not pool:
                    _add_from_sources(res.get('baseline_sources_cache_current'))
                if not pool:
                    _add_from_sources(res.get('baseline_sources_cache'))
                if not pool:
                    _add_from_sources(res.get('source_results'))

            # PATCH FIX2D2Z (ADD): also unwrap candidates from web_context.scraped_meta (injection lane)
            # Some evolution runs fetch injected delta via fetch_web_context() but do not fully materialise
            # baseline_sources_cache_current in the response payload. scraped_meta contains extracted_numbers.
            try:
                wc = _resp.get('web_context')
                if not isinstance(wc, dict) and isinstance(res, dict):
                    wc = res.get('web_context')
                if isinstance(wc, dict):
                    sm = wc.get('scraped_meta') or {}
                    if isinstance(sm, dict):
                        for u, m in sm.items():
                            if not isinstance(m, dict):
                                continue
                            su = m.get('url') or u
                            nums = m.get('extracted_numbers') or []
                            if not isinstance(nums, list):
                                continue
                            for n in nums:
                                if not isinstance(n, dict):
                                    continue
                                vn = n.get('value_norm')
                                if vn is None:
                                    vn = n.get('value')
                                try:
                                    fvn = float(vn)
                                except Exception:
                                    pass
                                    continue
                                unit_tag = str(n.get('unit_tag') or n.get('base_unit') or n.get('unit') or '').strip()
                                if _fix2d2g_is_yearlike_candidate({'value_norm': fvn, 'unit_tag': unit_tag}):
                                    continue
                                raw = str(n.get('raw') or n.get('display_value') or n.get('value') or '').strip()
                                ctx = str(n.get('context_snippet') or n.get('context') or n.get('context_window') or '').strip()
                                unit_family = str(n.get('unit_family') or '').strip()
                                try:
                                    if not unit_family:
                                        nf = globals().get('normalize_unit_family')
                                        if callable(nf):
                                            unit_family = str(nf(unit_tag, ctx=ctx, raw=raw) or '').strip()
                                except Exception:
                                    pass
                                pool.append({
                                    'value_norm': fvn,
                                    'unit_tag': unit_tag,
                                    'raw': raw,
                                    'context_snippet': ctx,
                                    'unit_family': unit_family,
                                    'measure_kind': n.get('measure_kind'),
                                    'measure_assoc': n.get('measure_assoc'),
                                    'source_url': str(su).strip() if su else None,
                                })
            except Exception:
                return pool

            def _add_from_sources(_sources):
                if not isinstance(_sources, list):
                    return
                for s in _sources:
                    if not isinstance(s, dict):
                        continue
                    su = s.get('source_url') or s.get('url') or None
                    nums = s.get('extracted_numbers')
                    if not isinstance(nums, list):
                        continue
                    for n in nums:
                        if not isinstance(n, dict):
                            continue
                        vn = n.get('value_norm')
                        if vn is None:
                            vn = n.get('value')
                        try:
                            fvn = float(vn)
                        except Exception:
                            pass
                            continue
                        unit_tag = str(n.get('unit_tag') or n.get('unit') or n.get('base_unit') or '').strip()
                        raw = str(n.get('raw') or n.get('display') or n.get('value') or '').strip()
                        ctx = str(n.get('context_snippet') or n.get('context') or n.get('context_window') or '')

                        # PATCH FIX2D2I (ADD): Backfill unit_family for inference
                        unit_family = str(n.get('unit_family') or '').strip()
                        try:
                            if not unit_family:
                                ut_l = unit_tag.lower()
                                raw_l = raw.lower()
                                ctx_l = str(ctx or '').lower()
                                if '%' in raw_l or '%' in ut_l or 'percent' in raw_l or 'percent' in ut_l:
                                    unit_family = 'percent'
                                elif '$' in raw_l or '$' in ut_l or 'usd' in ut_l or 'usd' in ctx_l or 'billion' in ctx_l or 'bn' in ctx_l:
                                    unit_family = 'currency'
                                elif ut_l in ('m','mn') or 'million' in raw_l or 'million' in ctx_l or ' units' in ctx_l or 'units' in raw_l:
                                    unit_family = 'magnitude'
                        except Exception:
                            pass
                            unit_family = unit_family
                        pool.append({
                            'value_norm': fvn,
                            'unit_tag': unit_tag,
                            'raw': raw,
                            'context_snippet': ctx,
                            'unit_family': unit_family,
                            'measure_kind': n.get('measure_kind'),
                            'measure_assoc': n.get('measure_assoc'),
                            'source_url': str(su).strip() if su else None,
                        })

            # prefer direct keys
            _add_from_sources(_resp.get('baseline_sources_cache_current'))
            if not pool:
                _add_from_sources(_resp.get('baseline_sources_cache'))

            # nested under results
            res = _resp.get('results')
            if isinstance(res, dict):
                if not pool:
                    _add_from_sources(res.get('baseline_sources_cache_current'))
                if not pool:
                    _add_from_sources(res.get('baseline_sources_cache'))
                if not pool:
                    _add_from_sources(res.get('source_results'))

            return pool

        def _fix2d2g_is_yearlike_candidate(n: dict) -> bool:
            try:
                if not isinstance(n, dict):
                    return True
                ut = str(n.get('unit_tag') or '').strip()
                if ut:
                    return False
                vn = n.get('value_norm')
                if vn is None:
                    vn = n.get('value')
                fv = float(vn)
                if not (1900.0 <= fv <= 2100.0):
                    return False
                return abs(fv - round(fv)) < 1e-9
            except Exception:
                return False

        # ---------------------------------------------------------------------
        # END PATCH FIX2D2G_INFERENCE_POOL
        # ---------------------------------------------------------------------

        # =====================================================================
        # PATCH FIX2D19 (ADD): Deterministic baseline soft-match fallback
        # When strict joins fail (ckey/anchor), attempt to find a plausible
        # current candidate from already-extracted_numbers pools so Analysis→
        # Evolution can produce comparable diffs without requiring Analysis to
        # include injected URLs. Render-only and fully auditable.
        # =====================================================================
        _fix2d19_soft = None
        _fix2d19_soft_reason = None
        _fix2d2g_best_score = None
        try:
            if resolved_cur_ckey_effective is None and isinstance(prev_val_norm, (int, float)):
                pool = _fix2d2g_unwrap_extracted_numbers_pool(cur_response)
                _fix2d2i_pool_size = len(pool) if isinstance(pool, list) else None
                _fix2d2i_scored = []  # [(score, n)] for trace
                _fix2d2i_pool_size = len(pool) if isinstance(pool, list) else None
                _fix2d2i_scored = []  # [(score, n)] for trace
                # derive expected type from canonical key / name
                ck_l = str(prev_ckey).lower()
                nm_l = str(pm.get('name') or '').lower()
                want_percent = ('share' in ck_l) or ('percent' in ck_l) or ('%' in (nm_l or ''))
                want_sales = ('sale' in ck_l) or ('sales' in ck_l) or ('unit_sales' in ck_l)
                # try to detect reference year in canonical key
                m = re.search(r'(19\d{2}|20\d{2})', ck_l)
                ref_year = m.group(1) if m else None
                # -----------------------------------------------------------------
                # PATCH FIX2D2Z_UNIT_FAMILY_HARD_REJECT (ADD)
                # Derive expected unit_family from frozen schema (authoritative) or
                # fall back to prev_unit_tag normalization. Used for hard rejection
                # to prevent currency≠percent leakage.
                # -----------------------------------------------------------------
                expected_family = ''
                try:
                    _msf = (cur_response.get('metric_schema_frozen') or (cur_response.get('results') or {}).get('metric_schema_frozen')) if isinstance(cur_response, dict) else None
                    if isinstance(_msf, dict):
                        _sch = _msf.get(prev_ckey)
                        if isinstance(_sch, dict):
                            expected_family = str(_sch.get('unit_family') or '').strip().lower()
                except Exception:
                    pass
                    expected_family = expected_family
                if not expected_family:
                    try:
                        nf = globals().get('normalize_unit_family')
                        if callable(nf):
                            expected_family = str(nf(str(prev_unit or ''), ctx=str(pm.get('name') or ''), raw=str(prev_raw or '')) or '').strip().lower()
                    except Exception:
                        pass
                        expected_family = ''

                def _has_currency_evidence_local(raw: str, ctx: str) -> bool:
                    r = (raw or '')
                    c = (ctx or '').lower()
                    if any(s in r for s in ['$', 'S$', '€', '£']):
                        return True
                    if any(code in c for code in [' usd', 'sgd', ' eur', ' gbp', ' aud', ' cad', ' jpy', ' cny', ' rmb']):
                        return True
                    if any(k in c for k in ['revenue','turnover','valuation','market value','market size','sales value','net profit','operating profit','gross profit','ebitda','earnings','income','capex','opex']):
                        return True
                    return False
                # -----------------------------------------------------------------
                # END PATCH FIX2D2Z_UNIT_FAMILY_HARD_REJECT
                # -----------------------------------------------------------------

                def _score(n):
                    try:
                        if not isinstance(n, dict):
                            return -1e9
                        if n.get('is_junk') is True:
                            return -1e9
                        if _fix2d2g_is_yearlike_candidate(n):
                            return -1e9
                        if _fix2d25_is_yearlike_value(n.get("value_norm") if isinstance(n, dict) else None):
                            return -1e9
                        raw = str(n.get('raw') or n.get('value') or '').lower()
                        ctx = str(n.get('context_snippet') or n.get('context') or n.get('context_window') or '').lower()
                        unit = str(n.get('unit_tag') or n.get('base_unit') or n.get('unit') or '').lower()
                        # PATCH FIX2D2Z (ADD): hard unit-family rejection (schema-aligned)
                        expected_family = ''
                        try:
                            _msf = (cur_response.get('metric_schema_frozen') if isinstance(cur_response, dict) else None)
                            if isinstance(_msf, dict):
                                _sch = _msf.get(prev_ckey)
                                if isinstance(_sch, dict):
                                    expected_family = str(_sch.get('unit_family') or '').lower().strip()
                        except Exception:
                            pass
                            expected_family = expected_family
                        if not expected_family:
                            try:
                                _nf = globals().get('normalize_unit_family')
                                if callable(_nf):
                                    expected_family = str(_nf(prev_unit or '', ctx=str(pm.get('name') or ''), raw=str(prev_raw or '')) or '').lower().strip()
                            except Exception:
                                pass
                                expected_family = ''

                        cand_family = str(n.get('unit_family') or '').lower().strip()
                        if not cand_family:
                            try:
                                _nf = globals().get('normalize_unit_family')
                                if callable(_nf):
                                    cand_family = str(_nf(unit or '', ctx=ctx, raw=raw) or '').lower().strip()
                            except Exception:
                                pass
                                cand_family = ''

                        def _has_currency_evidence_local(r: str, c: str) -> bool:
                            rr = (r or '')
                            cc = (c or '').lower()
                            if any(s in rr for s in ['$', 'S$', '€', '£']):
                                return True
                            if any(code in cc for code in [' usd', 'sgd', ' eur', ' gbp', ' aud', ' cad', ' jpy', ' cny', ' rmb']):
                                return True
                            if any(k in cc for k in ['revenue','turnover','valuation','market size','market value','sales value','net profit','operating profit','gross profit','ebitda','earnings','income']):
                                return True
                            return False

                        if expected_family and cand_family:
                            if expected_family == 'currency':
                                # allow magnitude only when currency evidence exists
                                if cand_family == 'magnitude' and not _has_currency_evidence_local(raw, ctx):
                                    return -1e9
                                if cand_family not in ('currency', 'magnitude'):
                                    return -1e9
                            else:
                                if cand_family != expected_family:
                                    return -1e9
                        s = raw + ' ' + unit
                        vn = n.get('value_norm')
                        try:
                            vn = float(vn) if vn is not None else None
                        except Exception:
                            pass
                            vn = None
                        if vn is None:
                            return -1e9

                        sc = 0.0
                # FIX2D2Z: hard unit-family incompatibility
                        try:
                            c_fam = str(n.get('unit_family') or '').strip().lower()
                            if not c_fam:
                                nf = globals().get('normalize_unit_family')
                                if callable(nf):
                                    c_fam = str(nf(unit, ctx=ctx, raw=raw) or '').strip().lower()
                        except Exception:
                            pass
                            c_fam = ''
                        if expected_family:
                            if expected_family == 'currency':
                                # allow magnitude only when currency evidence exists (mirrors Analysis selector)
                                if c_fam not in ('currency','magnitude'):
                                    return -1e9
                                if c_fam == 'magnitude' and not _has_currency_evidence_local(raw, ctx):
                                    return -1e9
                            else:
                                if c_fam and c_fam != expected_family:
                                    return -1e9
                        # unit-family gates
                        if want_percent:
                            if '%' not in raw and 'percent' not in s and 'pct' not in s:
                                return -1e9
                            sc += 5.0
                            if 'share' in ctx or 'market' in ctx:
                                sc += 2.0
                        if want_sales:
                            if ('million' not in s and 'units' not in s and ' m' not in s and unit in ('m','mn')):
                                # allow plain numbers if context says million/units
                                if 'million' not in ctx and 'units' not in ctx:
                                    return -1e9
                            sc += 4.0
                            if 'sales' in ctx or 'sold' in ctx:
                                sc += 2.0

                        # keyword binding from metric name
                        for kw in ('sales','share','market'):
                            if kw in nm_l and kw in ctx:
                                sc += 1.0

                        # prefer same-year sentence
                        if ref_year and ref_year in ctx:
                            sc += 3.0

                        # mild preference for closer numeric magnitude to baseline (avoid random huge/small)
                        try:
                            sc -= min(5.0, abs(float(vn) - float(prev_val_norm)) / max(1e-9, abs(float(prev_val_norm))))
                        except Exception:
                            return sc
                    except Exception:
                        return -1e9

                bestn = None
                bests = -1e9
                for n in pool:
                    sc = _score(n)
                    try:
                        _fix2d2i_scored.append((float(sc), n))
                    except Exception:
                        pass
                    if sc > bests:
                        bests = sc
                        bestn = n

                _fix2d2g_best_score = bests

                # PATCH FIX2D2I (TRACE): capture top candidates by score
                try:
                    _fix2d2i_top_candidates = []
                    if isinstance(_fix2d2i_scored, list) and _fix2d2i_scored:
                        _fix2d2i_scored_sorted = sorted(_fix2d2i_scored, key=lambda x: x[0], reverse=True)[:3]
                        for _sc, _n in _fix2d2i_scored_sorted:
                            if not isinstance(_n, dict):
                                continue
                            _fix2d2i_top_candidates.append({
                                "score": float(_sc),
                                "value_norm": _n.get("value_norm"),
                                "raw": _n.get("raw"),
                                "unit_tag": _n.get("unit_tag"),
                                "source_url": _n.get("source_url"),
                                "ctx_snip": (str(_n.get("context_snippet") or "")[:120] if isinstance(_n.get("context_snippet"), str) else None),
                            })
                except Exception:
                    pass
                    _fix2d2i_top_candidates = None

                if isinstance(bestn, dict) and bests > -1e8:
                    _fix2d19_soft = bestn
                    _fix2d19_soft_reason = 'soft_match_extracted_numbers'
        except Exception:
            pass
            _fix2d19_soft = None

        if _fix2d19_soft is not None:
            try:
                cur_raw_effective = str(_fix2d19_soft.get('raw') or _fix2d19_soft.get('value') or '').strip() or 'N/A'
                cur_val_norm_effective = float(_fix2d19_soft.get('value_norm')) if _fix2d19_soft.get('value_norm') is not None else None
                cur_unit_effective = str(_fix2d19_soft.get('unit_tag') or _fix2d19_soft.get('base_unit') or _fix2d19_soft.get('unit') or '').strip()
                cur_source_url_effective = str(_fix2d19_soft.get('source_url') or _fix2d19_soft.get('url') or '').strip() or None
                cur_ah_effective = str(_fix2d19_soft.get('anchor_hash') or '') or None
                method_effective = 'inference_bound'
                try:
                    summary["joined_by_inference"] += 1
                    if len(sample_inference_joins) < 10:
                        sample_inference_joins.append({
                            'prev_ckey': prev_ckey,
                            'value_norm': cur_val_norm_effective,
                            'raw': cur_raw_effective,
                            'source_url': cur_source_url_effective,
                        })
                except Exception:
                    pass
                _fix2d2e_inference_selected = dict(_fix2d19_soft)
                _fix2d2e_inference_committed = True
                resolved_cur_ckey_effective = prev_ckey  # keep row identity stable for baseline comparison
                # keep not_found counter as-is; we record separate summary below
            except Exception:
                pass
        # =====================================================================

        # FIX2D19: Recompute baseline semantics using the effective current value
        # (including soft_match) so comparable deltas can be emitted.
        try:
            baseline_prev_value = prev_raw
            baseline_cur_value = cur_raw_effective if (cur_raw_effective is not None and cur_raw_effective != 'N/A') else None
            baseline_delta_abs = None
            baseline_delta_pct = None
            baseline_change_type = None
            baseline_is_comparable = isinstance(prev_val_norm, (int, float)) and isinstance(cur_val_norm_effective, (int, float))
            if baseline_is_comparable:
                _d = float(cur_val_norm_effective) - float(prev_val_norm)
                baseline_delta_abs = _d
                if abs(float(prev_val_norm)) > 1e-12:
                    baseline_delta_pct = (_d / float(prev_val_norm)) * 100.0
                if abs(_d) < 1e-9:
                    baseline_change_type = 'unchanged'
                elif _d > 0:
                    baseline_change_type = 'increased'
                else:
                    baseline_change_type = 'decreased'
            else:
                if cur_raw_effective not in (None, 'N/A') and prev_raw not in (None, 'N/A'):
                    baseline_change_type = 'unknown'
                elif cur_raw_effective not in (None, 'N/A'):
                    baseline_change_type = 'added'
                else:
                    baseline_change_type = 'not_found'
        except Exception:
            pass

        # classify change only if both numeric (no inference)
        change_type = "unknown"
        change_pct = None
        match_conf = 0.0
        if method_effective == "ckey":
            match_conf = 95.0
        elif method_effective == "anchor_hash":
            match_conf = 85.0


        # =====================================================================
        # PATCH FIX2D32_ANCHOR_MISMATCH_DIFFABLE (ADDITIVE)
        # Treat anchor_hash mismatches as still diffable when canonical_key join
        # succeeds and unit-family guards pass. Stamp diagnostics for audit.
        # =====================================================================
        _fix2d32_anchor_mismatch = False
        try:
            if method_effective == "ckey" and prev_ah and cur_ah_effective and str(prev_ah) != str(cur_ah_effective):
                _fix2d32_anchor_mismatch = True
                try:
                    summary["joined_by_ckey_anchor_mismatch"] = int(summary.get("joined_by_ckey_anchor_mismatch") or 0) + 1
                except Exception:
                    pass
        except Exception:
            pass
            _fix2d32_anchor_mismatch = False
        # =====================================================================
        # END PATCH FIX2D32_ANCHOR_MISMATCH_DIFFABLE
        # =====================================================================



        display_name = pm.get("name") or pm.get("display_name") or pm.get("original_name") or prev_ckey

        row = {
            "name": display_name or "Unknown Metric",
            "canonical_key": prev_ckey,
            "previous_value": prev_raw,
            "current_value": cur_raw_effective,
            "current_value_norm": cur_val_norm_effective,
            "current_source": cur_source_url_effective,
            "current_method": method_effective,
            "change_pct": change_pct,
            "change_type": change_type,
            "match_confidence": match_conf,

            # PATCH FIX2D13 (ADDITIVE): baseline-focused diff fields
            "baseline_prev_value": baseline_prev_value,
            "baseline_cur_value": baseline_cur_value,
            "baseline_delta_abs": baseline_delta_abs,
            "baseline_delta_pct": baseline_delta_pct,
            "baseline_change_type": baseline_change_type,
            "baseline_is_comparable": baseline_is_comparable,

            # PATCH FIX2D35 (ADDITIVE): proxy baseline audit
            "baseline_proxy_used": bool(_fix2d35_baseline_proxy_used),
            "baseline_proxy_type": _fix2d35_baseline_proxy_type,

            "context_snippet": None,
            "source_url": None,
            "anchor_used": (method_effective == "anchor_hash"),
            "prev_anchor_hash": prev_ah,
            "cur_anchor_hash": cur_ah_effective,
            "anchor_mismatch_diffable_v1": bool(_fix2d32_anchor_mismatch),
            "prev_value_norm": prev_val_norm,
            "cur_value_norm": cur_val_norm_effective,
            "diag": {
                "diff_join_trace_v1": {
                    # PATCH FIX2D2I_STAMP (ADD): prove which builder path emitted this row
                    "builder_id": "FIX2D2I__rows_core",
                    "prev_ckey": prev_ckey,
                    "resolved_cur_ckey": resolved_cur_ckey_effective,
                    "method": method_effective,
                    "prev_anchor_hash": prev_ah,
                    "cur_anchor_hash": cur_ah_effective,
                    "anchor_mismatch_diffable_v1": bool(_fix2d32_anchor_mismatch),
                    "pool_size": (_fix2d2i_pool_size if '_fix2d2i_pool_size' in locals() else None),
                    "top_candidates": (_fix2d2i_top_candidates if '_fix2d2i_top_candidates' in locals() else None),
                },
                "diff_current_source_trace_v1": {
                    # PATCH FIX2D2I_STAMP (ADD): prove which builder path emitted this row
                    "builder_id": "FIX2D2I__rows_core",
                    "current_source_path_used": ("inference_bound" if method_effective == "inference_bound" else ("primary_metrics_canonical" if resolved_cur_ckey_effective else "none")),
                    "current_value_norm": cur_val_norm_effective,
                    "current_unit_tag": cur_unit_effective,
                    "joined_current_yearlike_blocked_v1": (_fix2d2h_joined_current_yearlike_payload if '_fix2d2h_joined_current_yearlike_payload' in locals() else None),
                    "inference_disabled": False,
                    "inference_commit_v1": {
                        "inference_selected": bool(method_effective == "inference_bound"),
                        "inference_committed": bool(method_effective == "inference_bound" and cur_val_norm_effective is not None),
                        "committed_value_norm": cur_val_norm_effective,
                        "committed_raw": cur_raw_effective,
                        "committed_source_url": cur_source_url_effective,
                    },
                    "inference_commit_v2": {
                        "enabled": bool(_fix2d25_inference_enabled),
                        "attempted": bool(_fix2d25_inference_enabled and resolved_cur_ckey_effective is None and isinstance(prev_val_norm, (int, float))),
                        "selected": bool(_fix2d2e_inference_selected is not None),
                        "selected_value_norm": (_fix2d2e_inference_selected.get('value_norm') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_raw": (_fix2d2e_inference_selected.get('raw') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_unit_tag": (_fix2d2e_inference_selected.get('unit_tag') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_source_url": (_fix2d2e_inference_selected.get('source_url') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_score": _fix2d2g_best_score,
                        "committed": bool(_fix2d2e_inference_committed),
                        "reason": _fix2d19_soft_reason,
                    },
                },
            },
        }

        # PATCH FIX2D2Q_ROW_PROVENANCE (ADDITIVE): stamp where Current came from
        # (injected vs base), and if available, the selection mode used when
        # synthesizing baseline-keyed currents.
        try:
            if cur_source_url_effective:
                row['current_source_type_fix2d2q'] = ('injected' if cur_source_url_effective in _fix2d2q_inj_set else 'base')
            else:
                row['current_source_type_fix2d2q'] = None
        except Exception:
            pass
            row['current_source_type_fix2d2q'] = None
        try:
            _cm_dbg = None
            if resolved_cur_ckey_effective and isinstance(cur_can, dict):
                _cm_dbg = cur_can.get(resolved_cur_ckey_effective)
            if isinstance(_cm_dbg, dict):
                row['current_selection_mode_fix2d2q'] = _cm_dbg.get('current_selection_mode_fix2d2q') or _cm_dbg.get('diag_fix2d2n', {}).get('selection_mode_fix2d2q')
            else:
                row['current_selection_mode_fix2d2q'] = ('injected_first' if row.get('current_source_type_fix2d2q') == 'injected' else 'base')
        except Exception:
            pass
            row['current_selection_mode_fix2d2q'] = None

        if unit_mismatch:
            row["diag"]["diff_unit_mismatch_v1"] = {
                "detected": True,
                "prev_unit_tag": prev_unit,
                "cur_unit_tag": cur_unit,
                "joined_ckey": resolved_cur_ckey,
                "action": "split_to_current_only",
            }

        rows.append(row)

    # Append current-only rows from canonical (if present)
    current_only_total = 0
    current_only_injected = 0
    try:
        if isinstance(cur_metrics, dict) and cur_metrics:
            for ck, cm in cur_metrics.items():
                if not isinstance(ck, str) or not ck:
                    continue
                if ck in matched_cur_ckeys:
                    continue
                if not isinstance(cm, dict):
                    continue

                cur_raw = _raw_display_value(cm) or "N/A"
                cur_val_norm = _canon_value_norm(cm)
                cur_unit = _canon_unit_tag(cm)
                cur_ah = _get_anchor_hash_for_ckey(ck, cm, cur_anchors)
                urls = _metric_source_urls(cm)
                from_inj = _is_from_injected_url_from_urls(urls)

                rows.append({
                    "name": cm.get("name") or cm.get("metric_name") or ck,
                    "canonical_key": ck,
                    "previous_value": "N/A",
                    "current_value": cur_raw,
                    "change_pct": None,
                    "change_type": "current_only",
                    "match_confidence": 0.0,

                    # PATCH FIX2D13 (ADDITIVE): baseline-focused diff fields
                    "baseline_prev_value": None,
                    "baseline_cur_value": cur_raw,
                    "baseline_delta_abs": None,
                    "baseline_delta_pct": None,
                    "baseline_change_type": "added",
                    "baseline_is_comparable": False,

                    "context_snippet": None,
                    "source_url": (urls or [None])[0],
                    "anchor_used": False,
                    "prev_anchor_hash": None,
                    "cur_anchor_hash": cur_ah,
                    "prev_value_norm": None,
                    "cur_value_norm": cur_val_norm,
                    "from_injected_url": from_inj,
                    "diag": {
                        "diff_join_trace_v1": {
                            "prev_ckey": None,
                            "resolved_cur_ckey": ck,
                            "method": "current_only",
                            "prev_anchor_hash": None,
                            "cur_anchor_hash": cur_ah,
                        },
                        "diff_current_source_trace_v1": {
                            "current_source_path_used": "primary_metrics_canonical",
                            "current_value_norm": cur_val_norm,
                            "current_unit_tag": cur_unit,
                            "inference_disabled": False,
                    "inference_commit_v1": {
                        "inference_selected": bool(method_effective == "inference_bound"),
                        "inference_committed": bool(method_effective == "inference_bound" and cur_val_norm_effective is not None),
                        "committed_value_norm": cur_val_norm_effective,
                        "committed_raw": cur_raw_effective,
                        "committed_source_url": cur_source_url_effective,
                    },
                    "inference_commit_v2": {
                        "enabled": bool(_fix2d25_inference_enabled),
                        "attempted": bool(_fix2d25_inference_enabled and resolved_cur_ckey_effective is None and isinstance(prev_val_norm, (int, float))),
                        "selected": bool(_fix2d2e_inference_selected is not None),
                        "selected_value_norm": (_fix2d2e_inference_selected.get('value_norm') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_raw": (_fix2d2e_inference_selected.get('raw') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_unit_tag": (_fix2d2e_inference_selected.get('unit_tag') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_source_url": (_fix2d2e_inference_selected.get('source_url') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_score": _fix2d2g_best_score,
                        "committed": bool(_fix2d2e_inference_committed),
                        "reason": _fix2d19_soft_reason,
                    },
                        },
                    },
                })
                current_only_total += 1
                if from_inj:
                    current_only_injected += 1
    except Exception:
        pass

    # Append current-only rows from extracted_numbers when canonical is absent/empty
    raw_current_only_total = 0
    raw_current_only_injected = 0
    try:
        if not (isinstance(cur_metrics, dict) and cur_metrics):
            pool = _fix2d2g_unwrap_extracted_numbers_pool(cur_response)
            seen_ah = set()
            for n in pool:
                if not isinstance(n, dict):
                    continue
                if n.get("is_junk") is True:
                    continue
                if _fix2d25_is_yearlike_value(n.get("value_norm") if isinstance(n, dict) else None):
                    continue

                ah = str(n.get("anchor_hash") or "")
                if ah:
                    if ah in seen_ah:
                        continue
                    seen_ah.add(ah)

                raw = str(n.get("raw") or n.get("value") or "").strip() or "N/A"
                unit = str(n.get("base_unit") or n.get("unit") or n.get("unit_tag") or "").strip()
                vn = None
                try:
                    if n.get("value_norm") is not None:
                        vn = float(n.get("value_norm"))
                except Exception:
                    pass
                    vn = None

                src = n.get("source_url")
                from_inj = (isinstance(src, str) and src in inj_set) if inj_set else False

                # deterministic pseudo key for UI
                ckey = f"__current_only_raw__{ah[:12]}" if ah else f"__current_only_raw__{raw_current_only_total+1}"

                rows.append({
                    "name": f"Current-only (unmapped) {raw}",
                    "canonical_key": ckey,
                    "previous_value": "N/A",
                    "current_value": raw,
                    "change_pct": None,
                    "change_type": "current_only_raw",
                    "match_confidence": 0.0,
                    "context_snippet": n.get("context_snippet") or n.get("context") or None,
                    "source_url": src if isinstance(src, str) else None,
                    "anchor_used": False,
                    "prev_anchor_hash": None,
                    "cur_anchor_hash": ah or None,
                    "prev_value_norm": None,
                    "cur_value_norm": vn,
                    "from_injected_url": from_inj,
                    "diag": {
                        "diff_join_trace_v1": {
                            "prev_ckey": None,
                            "resolved_cur_ckey": ckey,
                            "method": "current_only_raw",
                            "prev_anchor_hash": None,
                            "cur_anchor_hash": ah or None,
                        },
                        "diff_current_source_trace_v1": {
                            "current_source_path_used": "baseline_sources_cache_current.extracted_numbers",
                            "current_value_norm": vn,
                            "current_unit_tag": unit,
                            "inference_disabled": False,
                    "inference_commit_v1": {
                        "inference_selected": bool(method_effective == "inference_bound"),
                        "inference_committed": bool(method_effective == "inference_bound" and cur_val_norm_effective is not None),
                        "committed_value_norm": cur_val_norm_effective,
                        "committed_raw": cur_raw_effective,
                        "committed_source_url": cur_source_url_effective,
                    },
                    "inference_commit_v2": {
                        "enabled": bool(_fix2d25_inference_enabled),
                        "attempted": bool(_fix2d25_inference_enabled and resolved_cur_ckey_effective is None and isinstance(prev_val_norm, (int, float))),
                        "selected": bool(_fix2d2e_inference_selected is not None),
                        "selected_value_norm": (_fix2d2e_inference_selected.get('value_norm') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_raw": (_fix2d2e_inference_selected.get('raw') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_unit_tag": (_fix2d2e_inference_selected.get('unit_tag') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_source_url": (_fix2d2e_inference_selected.get('source_url') if isinstance(_fix2d2e_inference_selected, dict) else None),
                        "selected_score": _fix2d2g_best_score,
                        "committed": bool(_fix2d2e_inference_committed),
                        "reason": _fix2d19_soft_reason,
                    },
                        },
                    },
                })
                raw_current_only_total += 1
                if from_inj:
                    raw_current_only_injected += 1
                if raw_current_only_total >= 25:
                    break
    except Exception:
        pass

    summary = {
        "rows_total": int(len(rows)),
        "joined_by_ckey": int(joined_by_ckey),
        "joined_by_anchor_hash": int(joined_by_anchor),
        "not_found": int(not_found),
        "sample_anchor_joins": sample_anchor_joins,
        "current_only_rows": int(current_only_total),
        "current_only_injected_rows": int(current_only_injected),
        "current_only_raw_rows": int(raw_current_only_total),
        "current_only_raw_injected_rows": int(raw_current_only_injected),
        "unit_mismatch_split": int(unit_mismatch_split),
        "cur_can_n": int(len(cur_metrics) if isinstance(cur_metrics, dict) else 0),
        "fix2d19_soft_match_rows": int(sum(1 for r in rows if isinstance(r, dict) and r.get('diag', {}).get('diff_join_trace_v1', {}).get('method') == 'soft_match')),

    }

    return rows, summary


# Bump code version (best-effort, render-layer only)
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
    if "fix2j" not in CODE_VERSION.lower():
        CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass

# =====================================================================
# END PATCH FIX2J
# =====================================================================



# =====================================================================
# PATCH FIX2K (additive): Diff Panel V2 — stop "unit mismatch" when no join,
# broaden "current-only" emission from metric_anchors + injected raw pool.
#
# Goals:
# 1) If a row has no resolved_cur_ckey, it must NOT present as unit_mismatch.
#    Instead: change_type=not_found, unit_mismatch=False, current_value="N/A".
# 2) If current-only canonical metrics are absent, still surface injected
#    "new perspective" signals as separate appended rows derived from:
#    - cur.metric_anchors (identity + source context)
#    - injected baseline_sources_cache_current extracted_numbers (raw, render-only)
#
# Constraints:
# - No changes to extraction, hashing, snapshot attach, legacy diff join internals.
# - Purely render-layer additive behavior.
# =====================================================================

try:
    _CV = str(globals().get('CODE_VERSION') or '')
    if _CV and 'fix2k' not in _CV:
        CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
    elif not _CV:
        CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass


def build_diff_metrics_panel_v2_fix2k(prev_response: dict, cur_response: dict):
    """Wrap Fix2J with: (a) no unit_mismatch when no join, (b) broader current-only rows."""
    # Prefer Fix2J if present, else base V2.
    _impl = globals().get('build_diff_metrics_panel_v2_fix2j')
    if not callable(_impl):
        _impl = globals().get('build_diff_metrics_panel_v2')
    if not callable(_impl):
        return ([], {'rows_total': 0, 'joined_by_ckey': 0, 'joined_by_anchor_hash': 0, 'not_found': 0})

    # PATCH FIX2D1 START (part 2): harden _impl call so summary always defined
    rows = []
    summary = {'rows_total': 0, 'joined_by_ckey': 0, 'joined_by_anchor_hash': 0, 'not_found': 0, 'error': None}
    try:
        rows, summary = _impl(prev_response, cur_response)
    except Exception as _e:
        # Keep the panel renderable even if underlying builder fails.
        summary['error'] = "diff_panel_v2_impl_exception:" + str(type(_e).__name__)
        try:
            summary['error_detail'] = str(_e)[:300]
        except Exception:
            pass
    # PATCH FIX2D1 END (part 2)

    # -------------------------------
    # Helpers
    # -------------------------------
    def _safe_dict(x):
        return x if isinstance(x, dict) else {}

    def _unwrap_metric_anchors(resp: dict):
        if not isinstance(resp, dict):
            return {}
        ma = resp.get('metric_anchors')
        if isinstance(ma, dict) and ma:
            return ma
        pr = resp.get('primary_response')
        if isinstance(pr, dict) and isinstance(pr.get('metric_anchors'), dict) and pr.get('metric_anchors'):
            return pr.get('metric_anchors') or {}
        res = resp.get('results')
        if isinstance(res, dict) and isinstance(res.get('metric_anchors'), dict) and res.get('metric_anchors'):
            return res.get('metric_anchors') or {}
        return {}

    def _unwrap_primary_metrics_canonical(resp: dict):
        if not isinstance(resp, dict):
            return {}
        pmc = resp.get('primary_metrics_canonical')
        if isinstance(pmc, dict) and pmc:
            return pmc
        pr = resp.get('primary_response')
        if isinstance(pr, dict) and isinstance(pr.get('primary_metrics_canonical'), dict) and pr.get('primary_metrics_canonical'):
            return pr.get('primary_metrics_canonical') or {}
        res = resp.get('results')
        if isinstance(res, dict) and isinstance(res.get('primary_metrics_canonical'), dict) and res.get('primary_metrics_canonical'):
            return res.get('primary_metrics_canonical') or {}
        return {}

    def _unwrap_inj_admitted_norm(resp: dict):
        dbg = _safe_dict(_safe_dict(resp).get('debug'))
        it = _safe_dict(dbg.get('inj_trace_v1'))
        admitted = it.get('admitted_norm')
        if isinstance(admitted, list):
            return [str(u) for u in admitted if u]
        return []

    def _unwrap_baseline_sources_cache_current(resp: dict):
        if not isinstance(resp, dict):
            return []
        b = resp.get('baseline_sources_cache_current')
        if isinstance(b, list) and b:
            return b
        # sometimes under results
        res = resp.get('results')
        if isinstance(res, dict) and isinstance(res.get('baseline_sources_cache_current'), list):
            return res.get('baseline_sources_cache_current') or []
        return []

    def _norm_url(u: str):
        try:
            return str(u).strip()
        except Exception:
            return ''

    # -------------------------------
    # (1) No unit_mismatch when no join
    # -------------------------------
    nojoin_demoted = 0
    for r in (rows or []):
        if not isinstance(r, dict):
            continue
        diag = r.get('diag') if isinstance(r.get('diag'), dict) else {}
        jt = diag.get('diff_join_trace_v1') if isinstance(diag.get('diff_join_trace_v1'), dict) else {}
        resolved = jt.get('resolved_cur_ckey')
        if not resolved:
            # If upstream labeled as unit_mismatch without a join, demote to not_found.
            if r.get('change_type') == 'unit_mismatch' or r.get('unit_mismatch') is True:
                r['unit_mismatch'] = False
                r['change_type'] = 'not_found'
                r['current_value'] = 'N/A'
                r['cur_value_norm'] = None
                r['cur_anchor_hash'] = None
                r['change_pct'] = None
                nojoin_demoted += 1
                try:
                    diag['diff_nojoin_unit_mismatch_demote_v1'] = {
                        'reason': 'resolved_cur_ckey_null',
                        'action': 'demote_to_not_found',
                    }
                    r['diag'] = diag
                except Exception:
                    pass

    # -------------------------------
    # PATCH FIX2D16 (ADD): Soft-match fill for baseline rows when no join
    # Goal: enable baseline diffing (increased/decreased) even when anchor/ckey join fails,
    # by searching deterministic extracted_numbers pools for a plausible current candidate.
    # Scope: only rows where resolved_cur_ckey is null and prev_value_norm is numeric.
    # Safety: does NOT affect canonical metrics; render-only row enrichment.
    # -------------------------------
    def _fix2d16_tokens(s: str):
        try:
            s = str(s or "").lower()
            s = re.sub(r"[^a-z0-9]+", " ", s)
            toks = [t for t in s.split() if t and len(t) >= 4 and not re.fullmatch(r"\d{4}", t)]
            stop = set(["global","world","total","overall","metric","market","report","growth","forecast","electric","vehicle","vehicles","sales"])
            return [t for t in toks if t not in stop]
        except Exception:
            return []

    def _fix2d16_unit_family_from_row(r: dict) -> str:
        try:
            ut = str(r.get("prev_unit_tag") or r.get("unit_tag") or "").lower()
            name = str(r.get("name") or "").lower()
            if "%" in ut or "percent" in ut or "pct" in ut or "share" in name:
                return "percent"
            if "$" in ut or "usd" in ut or "us$" in ut or "currency" in ut or "revenue" in name or "investment" in name:
                return "currency"
            return ""
        except Exception:
            return ""

    def _fix2d16_candidate_unit_ok(c: dict, uf: str) -> bool:
        try:
            raw = str(c.get("raw") or c.get("display") or "")
            unit = str(c.get("unit") or c.get("unit_tag") or c.get("base_unit") or "").lower()
            s2 = (raw + " " + unit).lower()
            if uf == "percent":
                return ("%" in raw) or ("percent" in s2) or ("pct" in s2)
            if uf == "currency":
                return ("$" in raw) or ("us$" in s2) or ("usd" in s2) or ("eur" in s2) or ("gbp" in s2) or ("€" in raw) or ("£" in raw) or ("bn" in s2) or ("billion" in s2)
            return True
        except Exception:
            return True

    def _fix2d16_build_pool(resp: dict):
        pool = []
        bsc = _unwrap_baseline_sources_cache_current(resp)
        for s in (bsc or []):
            if not isinstance(s, dict):
                continue
            src = _norm_url(s.get("source_url") or s.get("url") or "")
            nums = s.get("extracted_numbers")
            if not isinstance(nums, list) or not nums:
                continue
            for n in nums:
                if not isinstance(n, dict):
                    continue
                vn = n.get("value_norm")
                if vn is None:
                    vn = n.get("value")
                try:
                    fv = float(vn)
                except Exception:
                    pass
                    continue
                unit_tag = str(n.get("unit_tag") or n.get("unit") or n.get("base_unit") or "").strip()
                raw = str(n.get("raw") or n.get("display") or n.get("value") or "")
                if (not unit_tag) and 1900 <= fv <= 2100 and re.fullmatch(r"\d{4}(?:\.0)?", raw.strip()):
                    continue
                pool.append({
                    "value_norm": fv,
                    "raw": raw,
                    "unit_tag": unit_tag,
                    "context": str(n.get("context") or n.get("context_snippet") or n.get("snippet") or ""),
                    "source_url": src,
                })
        pool.sort(key=lambda x: (x.get("source_url") or "", x.get("raw") or "", str(x.get("value_norm") or "")))
        return pool

    _fix2d16_pool = _fix2d16_build_pool(cur_response)

    _fix2d16_soft_filled = 0
    for r in (rows or []):
        if not isinstance(r, dict):
            continue
        diag = r.get("diag") if isinstance(r.get("diag"), dict) else {}
        jt = diag.get("diff_join_trace_v1") if isinstance(diag.get("diff_join_trace_v1"), dict) else {}
        resolved = jt.get("resolved_cur_ckey")
        if resolved:
            continue

        pv = r.get("prev_value_norm")
        try:
            pvf = float(pv)
        except Exception:
            pass
            continue

        name = str(r.get("name") or r.get("canonical_key") or "")
        toks = _fix2d16_tokens(name) + _fix2d16_tokens(str(r.get("canonical_key") or ""))
        uf = _fix2d16_unit_family_from_row(r)

        best = None
        best_key = None
        for c in _fix2d16_pool:
            if uf and not _fix2d16_candidate_unit_ok(c, uf):
                continue
            ctxn = ((c.get("context") or "") + " " + (c.get("raw") or "")).lower()
            hits = 0
            for t in toks:
                if t and t in ctxn:
                    hits += 1
            if toks and hits <= 0:
                continue
            key = (-hits, c.get("source_url") or "", c.get("raw") or "", float(c.get("value_norm") or 0.0))
            if best is None or key < best_key:
                best = c
                best_key = key

        if not best:
            continue

        cvf = float(best.get("value_norm"))
        r["current_value"] = best.get("raw") or cvf
        r["cur_value_norm"] = cvf

        try:
            if cvf > pvf:
                r["change_type"] = "increased"
            elif cvf < pvf:
                r["change_type"] = "decreased"
            else:
                r["change_type"] = "unchanged"
            if pvf != 0:
                r["change_pct"] = ((cvf - pvf) / pvf) * 100.0
            else:
                r["change_pct"] = None
        except Exception:
            pass

        try:
            diag["diff_soft_match_v1"] = {
                "applied": True,
                "unit_family": uf or None,
                "candidate_source_url": best.get("source_url"),
                "candidate_raw": (best.get("raw") or "")[:120],
                "candidate_value_norm": cvf,
            }
            cst = diag.get("diff_current_source_trace_v1") if isinstance(diag.get("diff_current_source_trace_v1"), dict) else {}
            cst["current_source_path_used"] = "baseline_sources_cache_current.soft_match"
            cst["current_value_norm"] = cvf
            cst["current_unit_tag"] = best.get("unit_tag") or None
            diag["diff_current_source_trace_v1"] = cst
            r["diag"] = diag
        except Exception:
            pass

        _fix2d16_soft_filled += 1

    try:
        if not isinstance(summary, dict):
            summary = {}
        summary["fix2d16_soft_match_filled_rows"] = int(_fix2d16_soft_filled)
    except Exception:
        pass
        if isinstance(rc, str) and rc:
            represented_cur_ckeys.add(rc)
        # prefer explicit cur_anchor_hash fields if present
        cah = r.get('cur_anchor_hash') or jt.get('cur_anchor_hash')
        if cah:
            represented_cur_anchors.add(str(cah))

    # If we already have current_only rows, don't duplicate.
    already_has_current_only = any(isinstance(r, dict) and str(r.get('change_type','')).startswith('current_only') for r in (rows or []))

    cur_can = _unwrap_primary_metrics_canonical(cur_response)
    cur_ma = _unwrap_metric_anchors(cur_response)
    inj_norm = set(_unwrap_inj_admitted_norm(cur_response))

    appended_from_metric_anchors = 0
    appended_from_injected_raw = 0
    appended_rows = []

    # 2a) Append from metric_anchors when they represent additional identities not in prev-anchored table.
    # We only append if:
    # - canonical_key isn't already represented, AND
    # - anchor_hash isn't already represented (strong identity), AND
    # - it has a source_url (for traceability)
    if isinstance(cur_ma, dict) and cur_ma and not already_has_current_only:
        for ck, a in cur_ma.items():
            if not ck or not isinstance(a, dict):
                continue
            if ck in represented_cur_ckeys:
                continue
            ah = a.get('anchor_hash') or a.get('anchor') or a.get('anchorHash')
            ah = str(ah) if ah else None
            if ah and ah in represented_cur_anchors:
                continue
            src = a.get('source_url')
            srcn = _norm_url(src)
            from_inj = bool(srcn and srcn in inj_norm)
            # Prefer a canonical metric object if present, else fall back to anchor record.
            cm = cur_can.get(ck) if isinstance(cur_can, dict) else None
            name = None
            cur_val = None
            cur_vn = None
            unit_tag = None
            if isinstance(cm, dict):
                name = cm.get('name') or cm.get('metric_name') or ck
                cur_val = cm.get('display_value') or cm.get('value') or cm.get('raw_value')
                cur_vn = cm.get('value_norm')
                unit_tag = cm.get('base_unit') or cm.get('unit_tag')
                srcn = _norm_url(cm.get('source_url') or srcn)
                from_inj = bool(srcn and srcn in inj_norm)
            else:
                name = ck
                cur_val = None

            appended_rows.append({
                'name': name or ck,
                'canonical_key': ck,
                'previous_value': 'N/A',
                'current_value': cur_val if cur_val is not None else 'N/A',
                'change_pct': None,
                'change_type': 'current_only_anchor',
                'match_confidence': 0.0,
                'context_snippet': a.get('context_snippet') if isinstance(a.get('context_snippet'), str) else None,
                'source_url': srcn or None,
                'anchor_used': False,
                'prev_anchor_hash': None,
                'cur_anchor_hash': ah,
                'prev_value_norm': None,
                'cur_value_norm': cur_vn,
                'unit_mismatch': False,
                'diag': {
                    'diff_join_trace_v1': {
                        'prev_ckey': None,
                        'resolved_cur_ckey': ck,
                        'method': 'current_only_from_metric_anchors',
                        'prev_anchor_hash': None,
                        'cur_anchor_hash': ah,
                    },
                    'diff_current_source_trace_v1': {
                        'current_source_path_used': 'metric_anchors' if not isinstance(cm, dict) else 'primary_metrics_canonical',
                        'current_value_norm': float(cur_vn) if cur_vn is not None and str(cur_vn).replace('.','',1).isdigit() else None,
                        'current_unit_tag': unit_tag,
                        'inference_disabled': True,
                    },
                    'diff_current_only_trace_v1': {
                        'from_injected_url': from_inj,
                        'source_url_norm': srcn or None,
                        'reason': 'cur_metric_anchor_not_represented_in_prev_anchored_rows',
                    }
                }
            })
            appended_from_metric_anchors += 1

    # 2b) If still no current-only rows, append injected raw rows from baseline_sources_cache_current extracted_numbers.
    if not already_has_current_only and appended_from_metric_anchors == 0:
        bsc = _unwrap_baseline_sources_cache_current(cur_response)
        # Deterministically scan injected sources first.
        for s in (bsc or []):
            if not isinstance(s, dict):
                continue
            srcn = _norm_url(s.get('source_url') or s.get('url') or '')
            if not srcn or srcn not in inj_norm:
                continue
            nums = s.get('extracted_numbers')
            if not isinstance(nums, list) or not nums:
                continue
            # Take up to 5 entries deterministically (given source order), skipping obvious years.
            taken = 0
            for n in nums:
                if taken >= 5:
                    break
                if not isinstance(n, dict):
                    continue
                v = n.get('value')
                try:
                    fv = float(v)
                except Exception:
                    pass
                    continue
                # skip year-ish
                if 1900 <= fv <= 2100:
                    continue
                disp = n.get('display') or n.get('raw') or str(v)
                appended_rows.append({
                    'name': n.get('label') or 'Injected source metric (raw)',
                    'canonical_key': None,
                    'previous_value': 'N/A',
                    'current_value': disp,
                    'change_pct': None,
                    'change_type': 'current_only_raw',
                    'match_confidence': 0.0,
                    'context_snippet': n.get('context') if isinstance(n.get('context'), str) else (s.get('context_snippet') if isinstance(s.get('context_snippet'), str) else None),
                    'source_url': srcn,
                    'anchor_used': False,
                    'prev_anchor_hash': None,
                    'cur_anchor_hash': None,
                    'prev_value_norm': None,
                    'cur_value_norm': fv,
                    'unit_mismatch': False,
                    'diag': {
                        'diff_join_trace_v1': {
                            'prev_ckey': None,
                            'resolved_cur_ckey': None,
                            'method': 'current_only_raw_injected',
                            'prev_anchor_hash': None,
                            'cur_anchor_hash': None,
                        },
                        'diff_current_source_trace_v1': {
                            'current_source_path_used': 'raw_extracted_numbers_pool',
                            'current_value_norm': fv,
                            'current_unit_tag': n.get('unit') or None,
                            'inference_disabled': True,
                        },
                        'diff_current_only_trace_v1': {
                            'from_injected_url': True,
                            'source_url_norm': srcn,
                            'reason': 'no_cur_primary_metrics_canonical_and_no_metric_anchor_extras',
                        }
                    }
                })
                taken += 1
                appended_from_injected_raw += 1

    # Append any new rows after base rows.
    if appended_rows:
        try:
            rows = list(rows or []) + appended_rows
        except Exception:
            pass

    # Update summary
    try:
        if not isinstance(summary, dict):
            summary = {}
        summary['nojoin_unit_mismatch_demoted_rows'] = int(nojoin_demoted)
        summary['current_only_anchor_rows'] = int(appended_from_metric_anchors)
        summary['current_only_raw_injected_rows'] = int(appended_from_injected_raw)
        summary['rows_total'] = int(len(rows or []))
    except Exception:
        return rows, summary


# --- Wiring: prefer Fix2K ---
try:
    globals()['build_diff_metrics_panel_v2_fix2k'] = build_diff_metrics_panel_v2_fix2k
except Exception:
    pass

try:
    # Override the main V2 builder with Fix2K, but keep Fix2J available.
    globals()['build_diff_metrics_panel_v2'] = build_diff_metrics_panel_v2_fix2k
except Exception:
    pass


# =====================================================================
# PATCH FIX2U_VERSION_BUMP (ADDITIVE)
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass
# END PATCH FIX2U_VERSION_BUMP
# =====================================================================



# =====================================================================
# PATCH FIX2Y_VERSION_BUMP (ADDITIVE)
# =====================================================================
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass
# =====================================================================
# END PATCH FIX2Y_VERSION_BUMP
# =====================================================================

# =====================================================================
# PATCH FIX2D2 (ADDITIVE)
# - Fill current_metrics using analysis anchors when schema_frozen is missing/misaligned
# - Broaden rebuild fn name fallbacks to avoid fn_missing masking usable rebuilds
# - Version bump + patch tracker entry
# =====================================================================

try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2",
        "date": "2026-01-15",
        "summary": "Anchor-fill current_metrics for diff/display when schema_frozen is misaligned; add rebuild-fn name fallbacks to prevent fn_missing.",
        "files": ["FIX2D2.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass





# =====================================================================
# PATCH TRACKER ENTRY (ADDITIVE)
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2I",
        "date": "2026-01-16",
        "summary": "Authoritative Diff Panel V2 wiring to __rows builder; adds pool/selection/commit trace and preserves guarded inference year-blocking. by defining deterministic extracted_numbers pool unwrapping for Diff Panel V2 __rows (previously undefined, silently disabling inference). Harden sentinel trace, add explicit per-row inference_commit trace fields, and bump CODE_VERSION with final override.",
        "files": ["FIX2D2I.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# =====================================================================
# PATCH FIX2D2I (AUTHORITATIVE): Diff Panel V2 — binding inference commit
# + trace + simplified wiring
#
# Problem: multiple legacy V2 builders/wrappers exist; some "attempt" inference
# but never commit into row[current_value/current_value_norm], leaving Current=N/A.
#
# Fix: make the active Diff Panel V2 entrypoint call the proven __rows builder
# (which already implements guarded soft-match inference and commits into the
# UI-read fields), and add explicit trace proving candidate pool / selection / commit.
#
# Also: treat "yearlike joined current" as a join failure (already handled in __rows)
# and ensure the trace captures that event.
#
# Obsolete patches: prior ad-hoc V2 wrappers (fix2k, etc.) remain defined for
# backward compatibility but are no longer used by default.
# =====================================================================

try:
    # ---- Enhance __rows trace with pool stats / top candidates (non-invasive) ----
    # We patch via wrapper to avoid risky edits in the core loop.
    # REFACTOR49: Make FIX2D2I-style wrapper idempotent and non-recursive.
    # Streamlit reruns execute this file multiple times in the same interpreter;
    # duplicate wrapper passes previously captured an already-wrapped __rows,
    # triggering RecursionError (maximum recursion depth exceeded).
    _cur_rows_fn = globals().get('build_diff_metrics_panel_v2__rows')
    # If already wrapped in this run/session, do not wrap again.
    if callable(_cur_rows_fn) and getattr(_cur_rows_fn, '__yureeka_fix2d2i_wrapped__', False):
        pass
    else:
        _base_rows_fn = globals().get('_YUREEKA_DIFFPANEL_V2_ROWS_BASE')
        # First time we see a callable __rows in this run, treat it as the base.
        if not callable(_base_rows_fn) and callable(_cur_rows_fn):
            _base_rows_fn = _cur_rows_fn
            globals()['_YUREEKA_DIFFPANEL_V2_ROWS_BASE'] = _base_rows_fn

        if callable(_base_rows_fn):
            def build_diff_metrics_panel_v2__rows_fix2d2i(prev_response: dict, cur_response: dict):
                rows, summary = _base_rows_fn(prev_response, cur_response)
                try:
                    # Add lightweight trace fields if inference_commit_v2 exists.
                    for r in rows:
                        if not isinstance(r, dict):
                            continue
                        diag = r.get('diag')
                        if not isinstance(diag, dict):
                            continue
                        dcs = diag.get('diff_current_source_trace_v1')
                        if not isinstance(dcs, dict):
                            continue
                        ic = dcs.get('inference_commit_v2')
                        if not isinstance(ic, dict):
                            continue
                        # pool stats may have been computed inside __rows; if not present, leave None
                        if 'pool_size' not in ic:
                            ic['pool_size'] = diag.get('diff_join_trace_v1', {}).get('pool_size') if isinstance(diag.get('diff_join_trace_v1'), dict) else None
                        if 'top_candidates' not in ic:
                            ic['top_candidates'] = diag.get('diff_join_trace_v1', {}).get('top_candidates') if isinstance(diag.get('diff_join_trace_v1'), dict) else None
                except Exception:
                    return rows, summary
                return rows, summary

            try:
                setattr(build_diff_metrics_panel_v2__rows_fix2d2i, '__yureeka_fix2d2i_wrapped__', True)
                setattr(build_diff_metrics_panel_v2__rows_fix2d2i, '__yureeka_fix2d2i_base_name__', getattr(_base_rows_fn, '__name__', ''))
            except Exception:
                pass

            globals()['build_diff_metrics_panel_v2__rows'] = build_diff_metrics_panel_v2__rows_fix2d2i

except Exception:
    pass

try:
    # ---- Authoritative wiring: Diff Panel V2 entrypoint ----
    _rows_impl = globals().get('build_diff_metrics_panel_v2__rows')
    if callable(_rows_impl):
        def build_diff_metrics_panel_v2_fix2d2i(prev_response: dict, cur_response: dict):
            return _rows_impl(prev_response, cur_response)
        # Make this the default V2 builder used by DIFF_PANEL_V2_WIRING.
        globals()['build_diff_metrics_panel_v2'] = build_diff_metrics_panel_v2_fix2d2i
except Exception:
    pass

# END PATCH FIX2D2I


# =====================================================================
# PATCH TRACKER ENTRY: FIX2D2M (ADDITIVE)
# - Injected-first current-value selection (two-pass: injected pool then global)
# - Trace fields: pass1_injected_pool_size, pass1_selected, fallback_used, selected_source_url
# - Final version bump
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2M",
        "date": "2026-01-16",
        "summary": "Injected-first current-value inference: two-pass selection (injected-only pool then global fallback) with explicit trace fields and authoritative commit into metric_changes.current_value(_norm).",
        "files": ["FIX2D2M.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass




# =====================================================================
# PATCH TRACKER ENTRY: FIX2D2N (ADDITIVE)
# - Baseline-keyed current mapping for Analysis→Evolution diffing
# - When current canonical keys do not match baseline keys, synthesize current
#   metric objects for each baseline ckey using injected-first, unit-family-guarded
#   inference from extracted_numbers pools.
# - This closes Analysis/Evolution key parity gaps for the diff join without
#   changing canonical key generation.
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2N",
        "date": "2026-01-16",
        "summary": "Baseline-keyed current mapping: for each Analysis baseline canonical key, synthesize a current metric via injected-first, unit-family-guarded inference from extracted_numbers pools when Evolution canonical keys diverge; enables deterministic Analysis→Evolution diff joins even under key parity gaps.",
        "files": ["FIX2D2N.py"],
        "supersedes": ["FIX2D2M"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# =====================================================================
# PATCH TRACKER ENTRY: FIX2D2O (ADDITIVE)
# - Persist baseline-keyed augmented current canonical map into the
#   Evolution/current response under primary_metrics_canonical_for_diff
#   and mirror into primary_metrics_canonical for demo parity.
# - This makes Evolution JSON inspection show Analysis keys as the
#   effective current authority for diffing, while keeping extras possible.
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2O",
        "date": "2026-01-16",
        "summary": "Persist baseline-keyed current mapping for diffing: when baseline keys are synthesized into cur_can, expose them as primary_metrics_canonical_for_diff and mirror into primary_metrics_canonical so Evolution output keys align with Analysis for the diff demo.",
        "files": ["FIX2D2O.py"],
        "supersedes": ["FIX2D2N"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# =====================================================================
# PATCH TRACKER ENTRIES (CLEAN): FIX2D2Q / FIX2D2R / FIX2D2S
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2Q",
        "date": "2026-01-16",
        "summary": "Baseline-aligned current selection for diffing: injected-first with optional base fallback; stamps provenance fields (source_type, selection_mode) to prevent confusion while preserving union pool behavior.",
        "files": ["FIX2D2Q.py"],
        "supersedes": ["FIX2D2O"],
    })

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2R",
        "date": "2026-01-16",
        "summary": "Rebuild parity guard: prevent schema-only rebuild paths from committing bare-year tokens when a unit-qualified sibling candidate exists in the same snippet; improves Analysis/Evolution parity for injected content.",
        "files": ["FIX2D2R.py"],
        "supersedes": ["FIX2D2Q"],
    })

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2S",
        "date": "2026-01-16",
        "summary": "Schema-only rebuild hardening: when non-year candidates exist for a schema key, skip bare-year tokens during winner selection (down-rank/skip) and record diagnostics; reduces year-token pollution before downstream year-blocking.",
        "files": ["FIX2D2S.py"],
        "supersedes": ["FIX2D2R"],
    })

    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# =========================
# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D42
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D42",
        "date": "2026-01-17",
        "summary": "Serialize/promote baseline_schema_metrics_v1 into Analysis primary_response/results so Evolution diff can consume it; extend nested results promotion to mirror baseline_schema_metrics_v1.",
        "files": ["FIX2D42.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# FINAL VERSION OVERRIDE
# =========================
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass

# =====================================================================
# PATCH FIX2D2T (ADDITIVE): Baseline->Current projection for Diff Panel V2
# Why:
# - We have proven Evolution can extract/commit injected values, but diffing
#   still shows 0 increased/decreased/unchanged when baseline rows never get
#   current_value_norm populated.
# - This patch makes the final "handoff" explicit: for each baseline ckey
#   (prev row), if cur_response contains a canonical metric object for that
#   same ckey (prefer primary_metrics_canonical_for_diff), project it into
#   metric_changes[*].current_value/_norm and mark comparable.
# Safety:
# - Render/diff-layer only. No changes to extraction, hashing, anchors,
#   canonical key generation, or snapshot pools.
# - Only fills CURRENT when the metric object exists under the SAME ckey.
# =====================================================================

try:
    diff_metrics_by_name_FIX2D2T_BASE = diff_metrics_by_name  # type: ignore
except Exception:
    pass
    diff_metrics_by_name_FIX2D2T_BASE = None  # type: ignore


def _fix2d2t_s(x):
    try:
        return "" if x is None else str(x)
    except Exception:
        return ""


def _fix2d2t_f(x):
    try:
        if x is None:
            return None
        if isinstance(x, (int, float)):
            return float(x)
        s = _fix2d2t_s(x).strip().replace(",", "")
        if not s:
            return None
        return float(s)
    except Exception:
        return None


def _fix2d2t_get_cur_maps(cur_response: dict):
    """Return (cur_for_diff, cur_primary) dicts."""
    cur_for_diff = {}
    cur_primary = {}
    try:
        if isinstance(cur_response, dict):
            cfd = cur_response.get("primary_metrics_canonical_for_diff")
            if not isinstance(cfd, dict) and isinstance(cur_response.get("results"), dict):
                cfd = cur_response["results"].get("primary_metrics_canonical_for_diff")
            if isinstance(cfd, dict):
                cur_for_diff = cfd
            pmc = cur_response.get("primary_metrics_canonical")
            if not isinstance(pmc, dict) and isinstance(cur_response.get("results"), dict):
                pmc = cur_response["results"].get("primary_metrics_canonical")
            if isinstance(pmc, dict):
                cur_primary = pmc
    except Exception:
        return cur_for_diff, cur_primary


def diff_metrics_by_name_FIX2D2T_PROJECT_BASELINE_CURRENT(prev_response: dict, cur_response: dict):
    """Wrapper over current diff that ensures baseline rows receive current values when available under same ckey."""
    if not callable(diff_metrics_by_name_FIX2D2T_BASE):
        return ([], 0, 0, 0, 0)

    metric_changes, unchanged, increased, decreased, found = diff_metrics_by_name_FIX2D2T_BASE(prev_response, cur_response)

    cur_for_diff, cur_primary = _fix2d2t_get_cur_maps(cur_response if isinstance(cur_response, dict) else {})

    proj_applied = 0
    proj_filled = 0
    proj_source_injected = 0
    samples = []

    def _is_missing(row: dict):
        try:
            cvn = row.get("current_value_norm")
            if cvn is None:
                cvn = row.get("cur_value_norm")
            if cvn is not None:
                return False
            cv = row.get("current_value")
            cvs = _fix2d2t_s(cv).strip().upper()
            return (not cvs) or (cvs == "N/A")
        except Exception:
            return True

    def _build_display(vn, unit):
        try:
            if vn is None:
                return None
            if unit:
                return f"{vn} {unit}".strip()
            return _fix2d2t_s(vn)
        except Exception:
            return _fix2d2t_s(vn)

    if isinstance(metric_changes, list):
        for row in metric_changes:
            if not isinstance(row, dict):
                continue
            prev_ckey = _fix2d2t_s(row.get("canonical_key") or row.get("ckey") or "").strip()
            if not prev_ckey:
                continue

            # We only project into baseline rows (i.e., rows with prev_value_norm present)
            pv = row.get("prev_value_norm")
            if pv is None:
                pv = row.get("previous_value_norm")
            if pv is None:
                continue

            if not _is_missing(row):
                continue

            proj_applied += 1

            # Prefer baseline-keyed current mapping, then fallback to primary_metrics_canonical.
            cm = cur_for_diff.get(prev_ckey) if isinstance(cur_for_diff, dict) else None
            used_map = "primary_metrics_canonical_for_diff"
            if not isinstance(cm, dict) or not cm:
                cm = cur_primary.get(prev_ckey) if isinstance(cur_primary, dict) else None
                used_map = "primary_metrics_canonical"

            if not isinstance(cm, dict) or not cm:
                continue

            vn = cm.get("value_norm") if cm.get("value_norm") is not None else cm.get("value")
            vn = _fix2d2t_f(vn)
            if vn is None:
                continue
            unit = _fix2d2t_s(cm.get("unit_tag") or cm.get("unit") or cm.get("unit_cmp") or "").strip() or None
            src = _fix2d2t_s(cm.get("source_url") or "").strip()

            # Fill row
            row["current_value_norm"] = vn
            row["cur_value_norm"] = vn
            row["current_value"] = _build_display(vn, unit) or row.get("current_value")
            if unit:
                row["current_unit"] = unit
                row["cur_unit_cmp"] = unit

            # Diagnostics
            row.setdefault("diag", {})
            if isinstance(row.get("diag"), dict):
                row["diag"].setdefault("fix2d2t_baseline_projection_v1", {})
                if isinstance(row["diag"].get("fix2d2t_baseline_projection_v1"), dict):
                    row["diag"]["fix2d2t_baseline_projection_v1"].update({
                        "applied": True,
                        "used_map": used_map,
                        "resolved_cur_ckey": prev_ckey,
                        "current_source_url": src or None,
                    })

            # Provenance labels (align with FIX2D2Q concept)
            if src:
                if "github.io" in src or "injection" in src:
                    row["current_source_type_fix2d2q"] = "injected"
                    row["current_selection_mode_fix2d2q"] = "baseline_projection_injected"
                    proj_source_injected += 1
                else:
                    row["current_source_type_fix2d2q"] = row.get("current_source_type_fix2d2q") or "base"
                    row["current_selection_mode_fix2d2q"] = row.get("current_selection_mode_fix2d2q") or "baseline_projection_base"

            # Recompute comparability and change_type
            try:
                pv_num = _fix2d2t_f(pv)
                if pv_num is not None and vn is not None:
                    row["baseline_is_comparable"] = True
                    if abs(vn - pv_num) < 1e-9:
                        row["baseline_change_type"] = "unchanged"
                    elif vn > pv_num:
                        row["baseline_change_type"] = "increased"
                    else:
                        row["baseline_change_type"] = "decreased"
            except Exception:
                pass

            proj_filled += 1
            if len(samples) < 8:
                samples.append({
                    "ckey": prev_ckey,
                    "used_map": used_map,
                    "pv": _fix2d2t_f(pv),
                    "cv": vn,
                    "src": src,
                })

    # Recompute counters from rows (authoritative)
    try:
        u = i = d = 0
        f = 0
        if isinstance(metric_changes, list):
            for r in metric_changes:
                if not isinstance(r, dict):
                    continue
                ct = _fix2d2t_s(r.get("baseline_change_type") or r.get("change_type") or "").strip().lower()
                if ct in ("unchanged",):
                    u += 1; f += 1
                elif ct in ("increased",):
                    i += 1; f += 1
                elif ct in ("decreased",):
                    d += 1; f += 1
        unchanged, increased, decreased, found = u, i, d, f
    except Exception:
        pass

    # Attach top-level debug
    try:
        if isinstance(cur_response, dict):
            cur_response.setdefault("debug", {})
            if isinstance(cur_response.get("debug"), dict):
                cur_response["debug"]["fix2d2t_baseline_projection_summary_v1"] = {
                    "applied_to_missing_rows": int(proj_applied),
                    "filled_rows": int(proj_filled),
                    "filled_from_injected": int(proj_source_injected),
                    "samples": samples,
                }
    except Exception:
        return metric_changes, unchanged, increased, decreased, found


# Wire wrapper
try:
    if callable(diff_metrics_by_name_FIX2D2T_PROJECT_BASELINE_CURRENT):
        diff_metrics_by_name = diff_metrics_by_name_FIX2D2T_PROJECT_BASELINE_CURRENT  # type: ignore
except Exception:
    pass

# Patch tracker + version bump
try:
    if isinstance(globals().get('PATCH_TRACKER_V1'), list):
        PATCH_TRACKER_V1.append({
            "patch_id": "FIX2D2T",
            "summary": "Add explicit baseline->current projection in diff layer: if baseline row is missing CURRENT but cur_response has same canonical_key in primary_metrics_canonical_for_diff/primary_metrics_canonical, project into metric_changes current_value/_norm and recompute diff counters; attach debug summary.",
            "ts": "2026-01-16",
        })

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2U",
        "date": "2026-01-17",
        "summary": "Introduce shared semantic eligibility gate (local-snippet required tokens) and apply it across Analysis selector and Evolution schema-only rebuild paths to prevent cross-metric pollution (e.g., China sales value mapping to chargers 2040).",
        "files": ["FIX2D2U.py"],
        "supersedes": ["FIX2D2T"],
    })
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2W",
        "date": "2026-01-17",
        "summary": "Fix parity leak: ensure schema_only_rebuild commit-time semantic gate is always active (avoid NameError when _FIX2D2U_ENABLE defined later) and fix year-token extraction regex for required-year checks; bump CODE_VERSION.",
        "files": ["FIX2D2W.py"],
        "supersedes": ["FIX2D2V"],
    })

except Exception:
    pass

try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass

# =====================================================================
# END PATCH FIX2D2T
# =====================================================================

# =====================================================================

# =====================================================================

# =====================================================================
# PATCH FIX2D2X (PARITY): Reuse Analysis selector for Evolution baseline-key current
# ---------------------------------------------------------------------
# Rationale:
#   - Analysis has the authoritative semantic eligibility gates.
#   - Evolution schema_only_rebuild is a legacy shortcut that can mis-assign values.
#   - For Analysis→Evolution diffing, Evolution must populate CURRENT for the Analysis
#     keyspace using the same selector/gates, differing only in source preference
#     (injected-first).
# Implementation (additive override):
#   - Provide a new rebuild_metrics_from_snapshots_schema_only_fix17 that:
#       * derives a schema spec per baseline key (adds keyword hints from key/name)
#       * builds candidates from baseline_sources_cache extracted_numbers
#       * runs a strict injected-first two-pass selection via _analysis_canonical_final_selector_v1
#         with preferred-source locking disabled
#       * commits the chosen candidate under the same canonical_key
#   - This makes Evolution and Analysis share the same semantic gates for this step.
# ---------------------------------------------------------------------
# Supersedes (functionally): FIX2D2U/FIX2D2V/FIX2D2W schema_only_rebuild gating patches.
# =====================================================================

try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2X",
        "date": "2026-01-17",
        "summary": "Parity patch: replace Evolution schema-only slot filling for baseline-key current with Analysis authoritative selector (_analysis_canonical_final_selector_v1) using injected-first two-pass selection and synthesized keyword hints from canonical_key/name; prevents cross-metric misassignment (e.g., China sales -> chargers 2040) and aligns gating with Analysis.",
        "files": ["FIX2D2X.py"],
        "supersedes": ["FIX2D2W", "FIX2D2V", "FIX2D2U"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


def _fix2d2x_parse_injected_urls(web_context: dict) -> list:
    """Best-effort extraction of injected/extra URLs from web_context."""
    urls = []
    try:
        wc = web_context if isinstance(web_context, dict) else {}
        for k in ("diag_extra_urls", "extra_urls", "injected_urls"):
            v = wc.get(k)
            if isinstance(v, list):
                for u in v:
                    if isinstance(u, str) and u.strip():
                        urls.append(u.strip())
        # raw UI fields (string) used in earlier patches
        for k in ("diag_extra_urls_ui_raw", "extra_urls_ui_raw"):
            raw = wc.get(k)
            if isinstance(raw, str) and raw.strip():
                # split on whitespace / commas
                for part in re.split(r"[\s,]+", raw.strip()):
                    if part.startswith("http"):
                        urls.append(part)
    except Exception:
        pass

    # Stable de-dupe
    out = []
    seen = set()
    for u in urls:
        uu = str(u or "").strip()
        if not uu or uu in seen:
            continue
        seen.add(uu)
        out.append(uu)
    return out


def _fix2d2x_keywords_from_key_and_name(canonical_key: str, name: str) -> list:
    """Synthesize keyword hints when schema_frozen came from baseline (often lacks keywords)."""
    toks = []
    try:
        ck = str(canonical_key or "")
        nm = str(name or "")
        # split key on underscores
        for t in ck.replace("__", "_").split("_"):
            t = t.strip().lower()
            if not t:
                continue
            # drop pure years and very short tokens
            if re.fullmatch(r"(19\d{2}|20\d{2})", t):
                continue
            if len(t) <= 2:
                continue
            toks.append(t)
        # add name tokens
        for t in re.split(r"[^a-zA-Z0-9]+", nm.lower()):
            if not t:
                continue
            if re.fullmatch(r"(19\d{2}|20\d{2})", t):
                continue
            if len(t) <= 2:
                continue
            toks.append(t)
    except Exception:
        pass
        toks = []

    # small normalization + de-dupe
    out = []
    seen = set()
    stop = {"global", "world", "worldwide", "ev", "electric", "vehicle", "vehicles", "unknown"}
    for t in toks:
        if t in stop:
            continue
        if t in seen:
            continue
        seen.add(t)
        out.append(t)
    return out[:24]


def _fix2d2x_required_years_from_key(canonical_key: str) -> list:
    ys = []
    try:
        for m in re.findall(r"\b(19\d{2}|20\d{2})\b", str(canonical_key or "")):
            ys.append(m)
    except Exception:
        pass
        ys = []
    # de-dupe preserve
    out = []
    seen = set()
    for y in ys:
        if y in seen:
            continue
        seen.add(y)
        out.append(y)
    return out


def _fix2d2x_local_text_for_candidate(c: dict) -> str:
    try:
        return " ".join([
            str(c.get("context_snippet") or ""),
            str(c.get("raw") or ""),
            str(c.get("unit") or c.get("unit_tag") or ""),
        ]).lower()
    except Exception:
        return ""



# PATCH FIX2D70: relaxed year/keyword gating helpers (deterministic)
_fix2d70_year_re = re.compile(r"\b(19|20)\d{2}\b")

def _fix2d70_year_tokens(s: str):
    if not isinstance(s, str) or not s:
        return []
    return [m.group(0) for m in _fix2d70_year_re.finditer(s)]

def _fix2d70_ok_year_relaxed(local_text: str, req_years: list) -> bool:
    """Return True if local_text is compatible with req_years under FIX2D70 tolerance.

    Policy:
    - If key encodes a single year Y: accept Y, Y-1, Y+1
    - If key encodes multiple years (range): accept if ANY encoded year appears
    - If local_text has no year tokens at all: accept (avoid over-pruning)
    """
    if not req_years:
        return True
    lt = local_text or ""
    years_in_lt = set(_fix2d70_year_tokens(lt))
    if not years_in_lt:
        return True
    try:
        req = [int(y) for y in req_years if isinstance(y, str) and y.isdigit()]
    except Exception:
        req = []
    if not req:
        return True
    if len(req) == 1:
        y=req[0]
        return any(str(v) in years_in_lt for v in (y-1,y,y+1))
    # multi-year: accept any overlap
    return any(str(y) in years_in_lt for y in req)

def _fix2d70_keyword_hit(local_text: str, kws: list) -> bool:
    if not kws:
        return True
    lt = local_text or ""
    for k in kws:
        if k and k in lt:
            return True
    return False

def _fix2d2x_filter_candidates_for_key(canonical_key: str, spec: dict, candidates: list) -> list:
    """Pre-filter candidates for a schema key.

    FIX2D70: if strict year/keyword gating would eliminate all candidates, apply a controlled relaxation:
    - year tolerance (Y±1 for single-year keys; any overlap for ranges)
    - keyword requirement is relaxed only if strict pass yields zero

    Diagnostics are recorded into spec["debug_meta"]["fix2d70_prefilter"].
    """
    out = []
    req_years = _fix2d2x_required_years_from_key(canonical_key)
    kws = spec.get("keywords") or []
    kws = [str(k).lower() for k in kws if isinstance(k, str) and k.strip()]

    rej_year = 0
    rej_kw = 0
    rej_empty = 0

    # strict pass (legacy semantics)
    for c in candidates or []:
        if not isinstance(c, dict):
            continue
        lt = _fix2d2x_local_text_for_candidate(c)
        if not lt:
            rej_empty += 1
            continue
        ok_year = True
        if req_years:
            ok_year = all((y in lt) for y in req_years)
        if not ok_year:
            rej_year += 1
            continue
        if kws:
            if not _fix2d70_keyword_hit(lt, kws):
                rej_kw += 1
                continue
        out.append(c)

    if out:
        try:
            spec.setdefault("debug_meta", {})["fix2d70_prefilter"] = {
                "mode": "strict",
                "req_years": req_years,
                "kws_n": len(kws),
                "in_n": int(len(candidates or [])),
                "out_n": int(len(out)),
                "rej_year": int(rej_year),
                "rej_kw": int(rej_kw),
                "rej_empty": int(rej_empty),
            }
        except Exception:
            pass
        return out

    # relaxed pass (only if strict eliminated everything)
    out2 = []
    rej_year2 = 0
    rej_empty2 = 0
    for c in candidates or []:
        if not isinstance(c, dict):
            continue
        lt = _fix2d2x_local_text_for_candidate(c)
        if not lt:
            rej_empty2 += 1
            continue
        if not _fix2d70_ok_year_relaxed(lt, req_years):
            rej_year2 += 1
            continue
        # keyword gate intentionally removed in relaxed mode
        out2.append(c)

    try:
        spec.setdefault("debug_meta", {})["fix2d70_prefilter"] = {
            "mode": "relaxed" if out2 else "relaxed_empty",
            "req_years": req_years,
            "kws_n": len(kws),
            "in_n": int(len(candidates or [])),
            "out_n": int(len(out2)),
            "rej_year": int(rej_year),
            "rej_kw": int(rej_kw),
            "rej_empty": int(rej_empty),
            "rej_year_relaxed": int(rej_year2),
            "rej_empty_relaxed": int(rej_empty2),
        }
    except Exception:
        pass

    return out2




# ---------------------------------------------------------------------
# OVERRIDE: schema-only rebuild FIX17
# ---------------------------------------------------------------------

def rebuild_metrics_from_snapshots_schema_only_fix17(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:  # noqa: F811
    """Parity rebuild: populate CURRENT for baseline keyspace using Analysis selector."""
    if not isinstance(prev_response, dict):
        return {}

    # Schema keyspace
    schema_frozen = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or {}
    )
    if not isinstance(schema_frozen, dict) or not schema_frozen:
        return {}

    # Candidate pool from baseline_sources_cache
    bsc = baseline_sources_cache
    if not isinstance(bsc, list):
        # try prev_response locations
        bsc = (prev_response.get("results") or {}).get("baseline_sources_cache")
    if not isinstance(bsc, list):
        bsc = []

    all_candidates = []
    for item in bsc:
        if not isinstance(item, dict):
            continue
        src = item.get("source_url") or item.get("url") or ""
        nums = item.get("extracted_numbers")
        if not isinstance(nums, list):
            # sometimes nested in scraped_meta
            sm = item.get("scraped_meta")
            if isinstance(sm, dict):
                nums = sm.get("extracted_numbers")
        if not isinstance(nums, list):
            continue
        for c in nums:
            if not isinstance(c, dict):
                continue
            cc = dict(c)
            if src and not cc.get("source_url"):
                cc["source_url"] = src
            all_candidates.append(cc)

    injected_urls = _fix2d2x_parse_injected_urls(web_context)

    rebuilt = {}
    debug = {}
    reject_counts = {}
    filled = 0

    for ck, spec0 in schema_frozen.items():
        if not isinstance(ck, str) or not ck:
            continue
        spec = spec0 if isinstance(spec0, dict) else {}
        best, meta = _fix2d2x_select_current_for_key(ck, spec, all_candidates, injected_urls)
        if not isinstance(best, dict):
            # track block reasons (selector meta)
            try:
                br = str((meta or {}).get("blocked_reason") or "")
                if br:
                    reject_counts[br] = int(reject_counts.get(br, 0)) + 1
            except Exception:
                pass
            continue

        # Commit minimal metric object
        try:
            m = {
                "canonical_key": ck,
                "value": best.get("value"),
                "value_norm": best.get("value_norm"),
                "unit": best.get("unit") or best.get("unit_tag") or best.get("unit_cmp"),
                "unit_tag": best.get("unit_tag") or best.get("unit") or best.get("unit_cmp"),
                "unit_family": best.get("unit_family") or spec.get("unit_family") or "",
                "source_url": best.get("source_url") or "",
                "raw": best.get("raw") or "",
                "context_snippet": best.get("context_snippet") or "",
                "method": "analysis_selector_shared_fix2d2x",
                "selection_meta": meta or {},
            }
            rebuilt[ck] = m
            filled += 1
        except Exception:
            pass
            continue

    # Attach debug to web_context if present
    try:
        debug["fix2d2x_selector_shared_summary_v1"] = {
            "filled": int(filled),
            "schema_keys": int(len(schema_frozen)),
            "candidates_total": int(len(all_candidates)),
            "injected_urls": injected_urls,
            "reject_counts": reject_counts,
        }
        if isinstance(web_context, dict):
            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].update(debug)
    except Exception:
        pass

    return rebuilt
# =====================================================================
# END PATCH FIX2D2X
# =====================================================================

# =====================================================================
# PATCH FIX2D2Y (PARITY HARDWIRE)
# Objective:
# - Ensure Evolution's current-metric rebuild (including fix41afc19 override path)
#   uses the SAME authoritative Analysis selector for baseline-keyed diffing.
# - This addresses the observed issue where fix41afc19 applied a different rebuild
#   function (rebuild_metrics_from_snapshots_analysis_canonical_v1) which produced
#   a disjoint keyset (overlap_count==0), preventing diff activation.
#
# What:
# - Override `rebuild_metrics_from_snapshots_analysis_canonical_v1` so it performs a
#   baseline-keyed rebuild using the shared selector helper from FIX2D2X.
# - This makes the fix41afc19 path call the parity-correct rebuild automatically.
#
# Safety:
# - Additive override only; does not modify hashing, snapshot attach, or fastpath.
# - Only affects display/diff "current" semantics.
# =====================================================================

# Version stamp (ensure last-wins in monolithic file)
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
# Patch tracker entry
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2Y",
        "date": "2026-01-17",
        "summary": "Hardwire Evolution rebuild_metrics_from_snapshots_analysis_canonical_v1 to use the shared Analysis final selector for baseline-keyed diff current metrics (fix41afc19 parity); eliminates disjoint keyset that blocks diff activation.",
        "files": ["FIX2D2Y.py"],
        "supersedes": ["FIX2D2X"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


def rebuild_metrics_from_snapshots_analysis_canonical_v1(prev_response, snapshot_pool, web_context=None):
    """FIX2D2Y override: baseline-keyed current rebuild using Analysis selector.

    Returns a dict keyed by baseline canonical keys (from prev_response.primary_metrics_canonical)
    so Analysis↔Evolution overlap can be non-zero and diffing can activate.
    """
    # Resolve baseline schema (keys + minimal specs) from Analysis prev_response
    schema_keys = []
    schema_specs = {}
    try:
        _pmc = None
        if isinstance(prev_response, dict):
            _pmc = prev_response.get("primary_metrics_canonical")
            if not isinstance(_pmc, dict):
                _pmc = prev_response.get("results", {}).get("primary_metrics_canonical")
        if isinstance(_pmc, dict) and _pmc:
            for ck, mv in _pmc.items():
                if not isinstance(ck, str):
                    continue
                schema_keys.append(ck)
                mv = mv if isinstance(mv, dict) else {}
                schema_specs[ck] = {
                    "canonical_key": ck,
                    "unit_family": mv.get("unit_family") or "",
                    "unit_tag": mv.get("unit_tag") or mv.get("unit") or mv.get("unit_cmp") or "",
                    "display_name": mv.get("display_name") or mv.get("metric_name") or ck,
                }
    except Exception:
        pass
        schema_keys = []
        schema_specs = {}

    if not schema_keys:
        return {}

    # Build candidate universe from snapshot_pool extracted_numbers
    all_candidates = []
    try:
        if isinstance(snapshot_pool, list):
            for src in snapshot_pool:
                if not isinstance(src, dict):
                    continue
                url = src.get("url") or src.get("source_url") or ""
                nums = src.get("extracted_numbers")
                if not isinstance(nums, list):
                    continue
                for n in nums:
                    if not isinstance(n, dict):
                        continue
                    cand = dict(n)
                    cand["source_url"] = cand.get("source_url") or url
                    all_candidates.append(cand)
    except Exception:
        pass

    # Determine injected URLs (if any)
    injected_urls = []
    try:
        if isinstance(web_context, dict):
            inj = (
                web_context.get("diag_extra_urls")
                or web_context.get("extra_urls")
                or web_context.get("diag_extra_urls_final")
                or []
            )
            if isinstance(inj, list):
                injected_urls = [u for u in inj if isinstance(u, str) and u.strip()]
    except Exception:
        pass
        injected_urls = []

    # Select best candidate per baseline key using the shared selector helper
    rebuilt = {}
    debug = {
        "fix2d2y_analysis_selector_rebuild_v1": {
            "baseline_keys": int(len(schema_keys)),
            "candidates_total": int(len(all_candidates)),
            "injected_urls": injected_urls,
            "filled": 0,
            "reject_counts": {},
        }
    }

    filled = 0
    reject_counts = {}

    for ck in schema_keys:
        spec = schema_specs.get(ck) or {"canonical_key": ck}

        best = None
        meta = None
        try:
            # Reuse FIX2D2X selector if present
            if callable(globals().get("_fix2d2x_select_best")):
                best, meta = globals()["_fix2d2x_select_best"](ck, spec, all_candidates, injected_urls=injected_urls)
            else:
                # Fallback: no selection helper available
                best, meta = None, {"error": "missing_fix2d2x_select_best"}
        except Exception as e:
            best, meta = None, {"error": str(e)}

        if not isinstance(best, dict):
            # Count rejects if available
            try:
                rsn = None
                if isinstance(meta, dict):
                    rsn = meta.get("reject_reason") or meta.get("reason")
                if isinstance(rsn, str) and rsn:
                    reject_counts[rsn] = int(reject_counts.get(rsn, 0)) + 1
            except Exception:
                pass
            continue

        try:
            m = {
                "canonical_key": ck,
                "value": best.get("value"),
                "value_norm": best.get("value_norm"),
                "unit": best.get("unit") or best.get("unit_tag") or best.get("unit_cmp"),
                "unit_tag": best.get("unit_tag") or best.get("unit") or best.get("unit_cmp"),
                "unit_family": best.get("unit_family") or spec.get("unit_family") or "",
                "source_url": best.get("source_url") or "",
                "raw": best.get("raw") or "",
                "context_snippet": best.get("context_snippet") or "",
                "method": "analysis_selector_shared_fix2d2y",
                "selection_meta": meta or {},
            }
            rebuilt[ck] = m
            filled += 1
        except Exception:
            pass
            continue

    debug["fix2d2y_analysis_selector_rebuild_v1"]["filled"] = int(filled)
    debug["fix2d2y_analysis_selector_rebuild_v1"]["reject_counts"] = reject_counts

    # Attach debug to web_context if provided
    try:
        if isinstance(web_context, dict):
            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].update(debug)
    except Exception:
        return rebuilt

# Ensure global override binding
try:
    globals()["rebuild_metrics_from_snapshots_analysis_canonical_v1"] = rebuild_metrics_from_snapshots_analysis_canonical_v1
except Exception:
    pass

# =====================================================================
# END PATCH FIX2D2Y
# =====================================================================


# ============================================================
# PATCH START: FIX2D34_PREV_KEY_DRIVEN_DIFF_UNIVERSE_V1
# Purpose:
#   - Force Diff Panel V2 universe to be PREV-key driven (baseline canonical keys)
#   - Source CURRENT strictly from cur_response.primary_metrics_canonical for the same ckey
#   - Do NOT emit cur-only "added" rows (those are not baseline diffs)
# Safety:
#   - Diff layer only. Does not affect extraction, hashing, fastpath, snapshots.
#   - Keeps existing unit-family gates in later layers.
# ============================================================

def diff_metrics_by_name_FIX2D34(prev_response: dict, cur_response: dict):
    """Prev-key driven diff: iterate prev canonical keys only; hydrate current from PMC."""
    import re

    ABS_EPS = 1e-9
    REL_EPS = 0.0005

    def _s(x):
        try:
            return str(x)
        except Exception:
            return ""

    def _norm_name(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def _parse_num(v, unit=""):
        fn = globals().get("parse_human_number")
        if callable(fn):
            try:
                return fn(_s(v), unit)
            except Exception:
                return None
        try:
            return float(_s(v).replace(",", "").strip())
        except Exception:
            return None

    def _get_val_unit(m: dict, is_current: bool=False):
        m = m if isinstance(m, dict) else {}
        # Prefer canonical numeric
        if m.get("value_norm") is not None:
            try:
                v = float(m.get("value_norm"))
                u = _s(m.get("base_unit") or m.get("unit") or m.get("unit_tag") or "").strip()
                return v, u
            except Exception:
                pass
        # For current side, respect v27 disable flag
        try:
            if is_current and isinstance(cur_response, dict) and cur_response.get("_disable_numeric_inference_v27"):
                u = _s(m.get("unit") or m.get("unit_tag") or "").strip()
                return None, u
        except Exception:
            pass
        u = _s(m.get("unit") or m.get("unit_tag") or "").strip()
        return _parse_num(m.get("value"), u), u

    def _get_schema(prev: dict):
        if not isinstance(prev, dict):
            return {}
        sch = prev.get("metric_schema_frozen")
        if isinstance(sch, dict) and sch:
            return sch
        pr = prev.get("primary_response")
        if isinstance(pr, dict):
            sch = pr.get("metric_schema_frozen")
            if isinstance(sch, dict) and sch:
                return sch
        return {}

    def _display_name(ckey: str) -> str:
        ckey = _s(ckey).strip()
        if not ckey:
            return "Unknown Metric"
        left, _, right = ckey.partition("__")
        left = " ".join(w.capitalize() for w in left.replace("_", " ").split())
        right = right.replace("_", " ").strip()
        return f"{left} ({right})" if right else left

    def _metric_def(schema: dict, ckey: str):
        md = schema.get(ckey) if isinstance(schema, dict) else None
        return md if isinstance(md, dict) else {}

    prev_can = None
    try:
        prev_can = (prev_response or {}).get("primary_metrics_canonical")
        if not isinstance(prev_can, dict) or not prev_can:
            pr = (prev_response or {}).get("primary_response")
            if isinstance(pr, dict):
                prev_can = pr.get("primary_metrics_canonical")
    except Exception:
        pass
        prev_can = None
    prev_can = prev_can if isinstance(prev_can, dict) else {}

    cur_pmc = None
    try:
        cur_pmc = (cur_response or {}).get("primary_metrics_canonical")
        if not isinstance(cur_pmc, dict) or not cur_pmc:
            pr = (cur_response or {}).get("primary_response")
            if isinstance(pr, dict):
                cur_pmc = pr.get("primary_metrics_canonical")
    except Exception:
        pass
        cur_pmc = None
    cur_pmc = cur_pmc if isinstance(cur_pmc, dict) else {}

    schema = _get_schema(prev_response)

    metric_changes = []
    unchanged = increased = decreased = found = 0

    # PREV-KEY DRIVEN UNIVERSE
    for ckey, pm in prev_can.items():
        if not isinstance(ckey, str):
            ckey = _s(ckey)
        ckey = ckey.strip()
        if not ckey:
            continue
        pm = pm if isinstance(pm, dict) else {}

        cm = cur_pmc.get(ckey)
        cm = cm if isinstance(cm, dict) else {}

        name = _display_name(ckey)
        definition = _metric_def(schema, ckey)

        prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

        if not cm:
            metric_changes.append({
                "name": name,
                "previous_value": prev_raw,
                "current_value": "N/A",
                "change_pct": None,
                "change_type": "not_found",
                "match_confidence": 0.0,
                "canonical_key": ckey,
                "metric_definition": definition,
            })
            continue

        found += 1
        cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

        pv, pu = _get_val_unit(pm, is_current=False)
        cv, cu = _get_val_unit(cm, is_current=True)

        change_type = "unknown"
        change_pct = None

        if pv is not None and cv is not None:
            if abs(pv - cv) <= max(ABS_EPS, abs(pv) * REL_EPS):
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            elif cv > pv:
                change_type = "increased"
                change_pct = ((cv - pv) / max(ABS_EPS, abs(pv))) * 100.0
                increased += 1
            else:
                change_type = "decreased"
                change_pct = ((cv - pv) / max(ABS_EPS, abs(pv))) * 100.0
                decreased += 1
        else:
            if pv is None and cv is not None:
                change_type = "invalid_previous"
            elif pv is not None and cv is None:
                change_type = "invalid_current"
            else:
                change_type = "unknown"

        # Build display current
        cur_unit_tag = _s(cm.get("unit") or cm.get("unit_tag") or "").strip()
        cur_disp = _s(cur_raw).strip() if cur_raw is not None else ""
        if cur_disp and cur_unit_tag and (cur_unit_tag not in cur_disp):
            cur_disp = f"{cur_disp} {cur_unit_tag}".strip()

        metric_changes.append({
            "name": name,
            "previous_value": prev_raw,
            "current_value": cur_disp or ("N/A" if cv is None else _s(cv)),
            "change_pct": change_pct,
            "change_type": change_type,
            "match_confidence": 1.0,
            "canonical_key": ckey,
            "metric_definition": definition,
            "previous_value_norm": pv,
            "current_value_norm": cv,
            "prev_unit_cmp": pu,
            "cur_unit_cmp": cu,
        })

    # Attach top-level summary diagnostics
    try:
        if isinstance(cur_response, dict):
            cur_response.setdefault("debug", {})
            if isinstance(cur_response.get("debug"), dict):
                cur_response["debug"].setdefault("fix2d34_prev_key_driven_diff_v1", {})
                cur_response["debug"]["fix2d34_prev_key_driven_diff_v1"] = {
                    "prev_keys": int(len(prev_can)) if isinstance(prev_can, dict) else 0,
                    "cur_pmc_keys": int(len(cur_pmc)) if isinstance(cur_pmc, dict) else 0,
                    "rows_emitted": int(len(metric_changes)),
                    "found_both": int(found),
                    "note": "Universe iterates prev canonical keys only; current sourced from primary_metrics_canonical.",
                }
    except Exception:
        return metric_changes, unchanged, increased, decreased, found

# Wire FIX2D34 as the active diff function (last-wins override)
try:
    diff_metrics_by_name = diff_metrics_by_name_FIX2D34  # type: ignore
except Exception:
    pass

try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass

# ============================================================
# PATCH END: FIX2D34_PREV_KEY_DRIVEN_DIFF_UNIVERSE_V1
# ============================================================


# =========================================================
# FIX2D45 — FORCE BASELINE SCHEMA MATERIALISATION (FINAL)
# =========================================================
# This patch removes all optional/gated baseline construction
# and unconditionally builds baseline_schema_metrics_v1 during
# Analysis finalisation when schema + canonical metrics exist.

CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
def _fix2d45_force_baseline_schema_materialisation(analysis: dict) -> None:
    if "results" not in analysis:
        analysis["results"] = {}

    schema = analysis.get("metric_schema_frozen")
    canonical = analysis.get("primary_metrics_canonical")
    anchors = analysis.get("metric_anchors", {})

    if not schema:
        raise RuntimeError("FIX2D45: metric_schema_frozen missing")

    if not canonical:
        raise RuntimeError("FIX2D45: primary_metrics_canonical missing")

    baseline_schema_metrics_v1 = {}

    for schema_key in schema.keys():
        metric = canonical.get(schema_key)
        if not metric:
            continue

        baseline_schema_metrics_v1[schema_key] = {
            "canonical_key": schema_key,
            "canonical_id": metric.get("canonical_id"),
            "value_norm": metric.get("value_norm"),
            "unit_family": metric.get("unit_family"),
            "dimension": metric.get("dimension"),
            "anchor_hash": anchors.get(schema_key, {}).get("anchor_hash"),
            "source_url": metric.get("source_url"),
        }

    analysis["results"]["baseline_schema_metrics_v1"] = baseline_schema_metrics_v1
    analysis["baseline_schema_metrics_v1"] = baseline_schema_metrics_v1

    analysis.setdefault("debug", {})
    analysis["debug"]["fix2d45_baseline_count"] = len(baseline_schema_metrics_v1)


# ---- invoke FIX2D45 in Analysis finalisation ----
if "_fix2d45_force_baseline_schema_materialisation" not in globals():
    pass


# FIX2D47 — Diff Panel V2: Schema-union row universe + cross-source current winner
# --------------------------------------------------------------------------------
# Objective:
#   Implement the behaviour you described:
#     - Evolution may discover new sources
#     - If a discovered metric binds to the SAME schema canonical key as a baseline metric,
#       the diff panel should treat it as "current" and compute deltas (even if source differs).
#
# What this patch does:
#   1) Builds the diff row universe as: schema_keys = prev_schema_keys ∪ cur_schema_keys
#   2) Uses Analysis baseline_schema_metrics_v1 (schema-keyed) as the prev map (when available)
#   3) Builds a schema-keyed current map from ALL current canonical metrics (any source),
#      selecting a deterministic "winner" per schema key
#   4) Joins strictly by schema key (not by source universe), computes change_type + pct.
#
# Safety:
#   - Additive: defines a new builder and (optionally) swaps it in behind a join_mode gate.
#   - Deterministic: stable tie-breaks for current winner selection.
#
# Versioning:
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
def _fix2d47_get_nested(d, path, default=None):
    try:
        x = d
        for k in path:
            if not isinstance(x, dict):
                return default
            x = x.get(k)
        return x if x is not None else default
    except Exception:
        return default

def _fix2d47_first_present(d, paths, default=None):
    for p in paths:
        v = _fix2d47_get_nested(d, p, None)
        if v is not None:
            return v
    return default

def _fix2d47_unwrap_baseline_schema_metrics(prev_response: dict) -> dict:
    # Where Analysis should serialize it (post FIX2D45):
    #   - results.baseline_schema_metrics_v1 (preferred)
    #   - baseline_schema_metrics_v1 (mirror)
    # plus a few legacy wrapper shapes.
    paths = [
        ["results","baseline_schema_metrics_v1"],
        ["baseline_schema_metrics_v1"],
        ["primary_response","results","baseline_schema_metrics_v1"],
        ["results","primary_response","results","baseline_schema_metrics_v1"],
        ["primary_response","baseline_schema_metrics_v1"],
    ]
    v = _fix2d47_first_present(prev_response or {}, paths, default={})
    return v if isinstance(v, dict) else {}

def _fix2d47_unwrap_primary_metrics_canonical(resp: dict) -> dict:
    if not isinstance(resp, dict):
        return {}
    if isinstance(resp.get("primary_metrics_canonical"), dict) and resp.get("primary_metrics_canonical"):
        return resp.get("primary_metrics_canonical") or {}
    pr = resp.get("primary_response")
    if isinstance(pr, dict):
        if isinstance(pr.get("primary_metrics_canonical"), dict) and pr.get("primary_metrics_canonical"):
            return pr.get("primary_metrics_canonical") or {}
        res = pr.get("results")
        if isinstance(res, dict) and isinstance(res.get("primary_metrics_canonical"), dict) and res.get("primary_metrics_canonical"):
            return res.get("primary_metrics_canonical") or {}
    res = resp.get("results")
    if isinstance(res, dict) and isinstance(res.get("primary_metrics_canonical"), dict) and res.get("primary_metrics_canonical"):
        return res.get("primary_metrics_canonical") or {}
    return {}

def _fix2d47_schema_key_for_metric(ckey: str, m: dict) -> str:
    # Prefer explicit schema/canonical_key fields; fall back to dict key.
    if isinstance(m, dict):
        for k in ("canonical_key", "schema_key", "schema_canonical_key", "schema_ckey"):
            v = m.get(k)
            if isinstance(v, str) and v.strip():
                return v.strip()
    if isinstance(ckey, str) and ckey.strip():
        return ckey.strip()
    return ""

def _fix2d47_metric_confidence(m: dict) -> float:
    if not isinstance(m, dict):
        return 0.0
    for k in ("confidence", "score", "match_confidence", "bind_confidence"):
        v = m.get(k)
        try:
            if v is not None:
                return float(v)
        except Exception:
            return 0.0

def _fix2d47_pick_cur_winner(existing: dict, challenger: dict) -> dict:
    # Deterministic winner selection:
    #   1) higher confidence
    #   2) higher anchor_confidence (if present)
    #   3) stable tie-break by source_url then raw then canonical_id
    if not isinstance(existing, dict):
        return challenger
    if not isinstance(challenger, dict):
        return existing

    ce = _fix2d47_metric_confidence(existing)
    cc = _fix2d47_metric_confidence(challenger)
    if cc != ce:
        return challenger if cc > ce else existing

    def _af(m):
        try:
            return float(m.get("anchor_confidence") or 0.0)
        except Exception:
            return 0.0

    ae = _af(existing)
    ac = _af(challenger)
    if ac != ae:
        return challenger if ac > ae else existing

    def _k(m):
        try:
            su = m.get("source_url") or ""
            raw = m.get("raw") or m.get("value") or ""
            cid = m.get("canonical_id") or ""
            return (str(su), str(raw), str(cid))
        except Exception:
            return ("", "", "")

    return challenger if _k(challenger) < _k(existing) else existing

def _fix2d47_build_cur_map_by_schema_key(cur_response: dict) -> dict:
    cur_metrics = _fix2d47_unwrap_primary_metrics_canonical(cur_response or {})
    out = {}
    if not isinstance(cur_metrics, dict):
        return out
    for ck, m in cur_metrics.items():
        if not isinstance(m, dict):
            continue
        sk = _fix2d47_schema_key_for_metric(ck, m)
        if not sk:
            continue
        out[sk] = _fix2d47_pick_cur_winner(out.get(sk), m)
    return out

def _fix2d47_raw_display_value(m: dict):
    if not isinstance(m, dict):
        return None
    if m.get("raw") is not None:
        return m.get("raw")
    if m.get("value") is not None:
        return m.get("value")
    # baseline_schema_metrics_v1 often stores display in prev_raw/current_raw fields elsewhere; keep None if absent
    return None

def _fix2d47_value_norm(m: dict):
    if not isinstance(m, dict):
        return None
    for k in ("value_norm", "valuenorm", "current_value_norm", "prev_value_norm", "cur_value_norm"):
        if m.get(k) is None:
            continue
        try:
            return float(m.get(k))
        except Exception:
            return None

def _fix2d47_unit_tag(m: dict) -> str:
    if not isinstance(m, dict):
        return ""
    for k in ("base_unit", "unit", "unit_tag", "unittag", "baseunit"):
        v = m.get(k)
        if v is not None:
            return str(v).strip()
    return ""

def _fix2d47_metric_name(schema_key: str, prev_m: dict, cur_m: dict) -> str:
    for m in (prev_m, cur_m):
        if isinstance(m, dict):
            for k in ("name", "metric_name", "label", "title"):
                v = m.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()
    return schema_key

def build_diff_metrics_panel_v2_FIX2D47(prev_response: dict, cur_response: dict):
    """Return (rows, summary) for Diff Metrics Panel V2 (schema-union + cross-source current)."""
    rows = []

    prev_map = _fix2d47_unwrap_baseline_schema_metrics(prev_response or {})
    cur_map = _fix2d47_build_cur_map_by_schema_key(cur_response or {})

    prev_keys = set([k for k in prev_map.keys() if isinstance(k, str) and k])
    cur_keys = set([k for k in cur_map.keys() if isinstance(k, str) and k])
    all_keys = sorted(prev_keys | cur_keys)

    both_count = 0
    prev_only_count = 0
    cur_only_count = 0

    inc = dec = unc = add = rem = not_found = 0

    for skey in all_keys:
        pm = prev_map.get(skey) if isinstance(prev_map, dict) else None
        pm = pm if isinstance(pm, dict) else {}
        cm = cur_map.get(skey) if isinstance(cur_map, dict) else None
        cm = cm if isinstance(cm, dict) else {}

        has_prev = skey in prev_keys
        has_cur = skey in cur_keys

        if has_prev and has_cur:
            both_count += 1
        elif has_prev and not has_cur:
            prev_only_count += 1
        elif has_cur and not has_prev:
            cur_only_count += 1

        prev_raw = pm.get("value_raw") if pm.get("value_raw") is not None else _fix2d47_raw_display_value(pm)
        cur_raw = cm.get("value_raw") if cm.get("value_raw") is not None else _fix2d47_raw_display_value(cm)

        prev_val_norm = _fix2d47_value_norm(pm)
        cur_val_norm = _fix2d47_value_norm(cm)

        prev_unit = pm.get("unit_tag") if pm.get("unit_tag") is not None else _fix2d47_unit_tag(pm)
        cur_unit = cm.get("unit_tag") if cm.get("unit_tag") is not None else _fix2d47_unit_tag(cm)

        name = _fix2d47_metric_name(skey, pm, cm)

        change_type = "unknown"
        change_pct = None
        delta_abs = None

        # Semantics:
        # - both present + numeric => increased/decreased/unchanged
        # - prev present, cur missing => removed (vs baseline)
        # - cur present, prev missing => added (new discovery)
        if has_prev and not has_cur:
            change_type = "removed"
            rem += 1
        elif has_cur and not has_prev:
            change_type = "added"
            add += 1
        else:
            # both
            if isinstance(prev_val_norm, (int, float)) and isinstance(cur_val_norm, (int, float)):
                delta_abs = float(cur_val_norm) - float(prev_val_norm)
                if abs(prev_val_norm) > 1e-12:
                    change_pct = (delta_abs / float(prev_val_norm)) * 100.0
                # tolerance for float jitter
                if abs(delta_abs) <= max(1e-9, abs(float(prev_val_norm)) * 0.0005):
                    change_type = "unchanged"
                    unc += 1
                elif delta_abs > 0:
                    change_type = "increased"
                    inc += 1
                else:
                    change_type = "decreased"
                    dec += 1
            else:
                # both present but not numerically comparable
                change_type = "unknown"
                not_found += 1

        rows.append({
            "canonical_key": skey,               # schema key
            "name": name,
            "previous_value": prev_raw if has_prev else None,
            "current_value": cur_raw if has_cur else None,
            "prev_value_norm": prev_val_norm if has_prev else None,
            "cur_value_norm": cur_val_norm if has_cur else None,
            "previous_unit": prev_unit if has_prev else None,
            "current_unit": cur_unit if has_cur else None,
            "change_type": change_type,
            "change_pct": change_pct,
            "delta_abs": delta_abs,
            # provenance helpers
            "prev_source_url": pm.get("source_url"),
            "cur_source_url": cm.get("source_url"),
            "cur_confidence": _fix2d47_metric_confidence(cm),
            "method": "schema_key",              # explicit: joined by schema key
        })

    summary = {
        "join_mode": "schema_union",
        "rows_total": len(rows),
        "both_count": both_count,
        "prev_only_count": prev_only_count,
        "cur_only_count": cur_only_count,
        "metrics_increased": inc,
        "metrics_decreased": dec,
        "metrics_unchanged": unc,
        "metrics_added": add,
        "metrics_removed": rem,
        "metrics_unknown": not_found,
    }
    return rows, summary


# -------------------------------
# OPTIONAL WIRING HOOK:
# If your code already calls build_diff_metrics_panel_v2(prev, cur),
# replace it with build_diff_metrics_panel_v2_FIX2D47 behind your join-mode flag.
#
# Example drop-in (inside compute_source_anchored_diff before calling the builder):
#   jm = _fix2d6_get_diff_join_mode_v1()
#   if jm in ("schema_union","schema","cross","schema_cross_source","x"):
#       rows, summary = build_diff_metrics_panel_v2_FIX2D47(prev_response, cur_response)
#   else:
#       rows, summary = build_diff_metrics_panel_v2(prev_response, cur_response)
#
# Patch tracker entry (manual):
#   - FIX2D47: Diff Panel V2 schema-union universe + cross-source current winner selection



# =========================================================
# FIX2D47 — FINAL VERSION STAMP OVERRIDE
# =========================================================
# Ensure the authoritative code version reflects this patch.
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
# =========================================================
# FIX2D48 — Canonical Key Grammar v1 (Builder + Validator)
# =========================================================
# Purpose:
#   Establish a single authoritative canonical key grammar and validator,
#   and route canonical-key minting through it.
#
# Key format:
#   <scope>_<entity>_<metric>_<time_qualifier>[_<qualifier>...]__<dimension>
#
# Determinism:
#   - No free-text hashing.
#   - Normalization + strict validation.
#
# Integration:
#   - Use build_canonical_key_v1(...) only in schema-binding / key minting.
#   - Validate keys at the point primary_metrics_canonical is finalized.

import re
from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional, Tuple

_FIX2D48_TOKEN_RE = re.compile(r"^[a-z0-9]+(?:_[a-z0-9]+)*$")

def _fix2d48_parse_canonical_key_v1(key: str) -> Tuple[str, str]:
    if not isinstance(key, str) or not key:
        raise ValueError("canonical_key: empty")
    if key.count("__") != 1:
        raise ValueError(f"canonical_key: expected exactly one '__' separator, got {key.count('__')}")
    subject, dimension = key.split("__", 1)
    if not subject or not dimension:
        raise ValueError("canonical_key: missing subject or dimension")
    return subject, dimension

def _fix2d48_norm_token(s: str) -> str:
    s = (s or "").strip().lower()
    s = re.sub(r"[^a-z0-9]+", "_", s)
    s = re.sub(r"_+", "_", s).strip("_")
    return s

def _fix2d48_validate_token(name: str, token: str) -> None:
    if not token:
        raise ValueError(f"{name}: missing")
    if "__" in token:
        raise ValueError(f"{name}: contains '__' (reserved)")
    if not _FIX2D48_TOKEN_RE.match(token):
        raise ValueError(f"{name}: invalid token '{token}' (must match {_FIX2D48_TOKEN_RE.pattern})")

def _fix2d48_validate_dimension(dimension: str, allowed_dimensions: Optional[Iterable[str]] = None) -> None:
    _fix2d48_validate_token("dimension", dimension)
    if allowed_dimensions is not None:
        allowed = set(_fix2d48_norm_token(x) for x in allowed_dimensions)
        if dimension not in allowed:
            raise ValueError(f"dimension: '{dimension}' not in allowed dimensions ({sorted(allowed)})")

def _fix2d48_validate_time_qualifier(time_q: str) -> None:
    _fix2d48_validate_token("time_qualifier", time_q)

    if re.fullmatch(r"\d{4}", time_q):
        return
    if re.fullmatch(r"ytd_\d{4}", time_q):
        return
    if re.fullmatch(r"\d{4}_\d{4}", time_q):
        a, b = time_q.split("_", 1)
        if int(b) < int(a):
            raise ValueError(f"time_qualifier: range out of order '{time_q}'")
        return
    if re.fullmatch(r"asof_\d{4}_\d{2}", time_q):
        parts = time_q.split("_")
        mm = int(parts[2])
        if not (1 <= mm <= 12):
            raise ValueError(f"time_qualifier: invalid month in '{time_q}'")
        return
    if re.fullmatch(r"asof_\d{4}_\d{2}_\d{2}", time_q):
        parts = time_q.split("_")
        mm, dd = int(parts[2]), int(parts[3])
        if not (1 <= mm <= 12):
            raise ValueError(f"time_qualifier: invalid month in '{time_q}'")
        if not (1 <= dd <= 31):
            raise ValueError(f"time_qualifier: invalid day in '{time_q}'")
        return

    raise ValueError(f"time_qualifier: '{time_q}' does not match allowed families")

@dataclass(frozen=True)
class CanonicalKeyFieldsV1:
    scope: str
    entity: str
    metric: str
    time_qualifier: str
    dimension: str
    qualifiers: Tuple[str, ...] = ()

def build_canonical_key_v1(
    fields: CanonicalKeyFieldsV1,
    *,
    allowed_dimensions: Optional[Iterable[str]] = None,
) -> str:
    scope = _fix2d48_norm_token(fields.scope)
    entity = _fix2d48_norm_token(fields.entity)
    metric = _fix2d48_norm_token(fields.metric)
    time_q = _fix2d48_norm_token(fields.time_qualifier)
    dimension = _fix2d48_norm_token(fields.dimension)
    qualifiers = tuple(_fix2d48_norm_token(q) for q in (fields.qualifiers or ()))

    _fix2d48_validate_token("scope", scope)
    _fix2d48_validate_token("entity", entity)
    _fix2d48_validate_token("metric", metric)
    _fix2d48_validate_time_qualifier(time_q)
    _fix2d48_validate_dimension(dimension, allowed_dimensions=allowed_dimensions)

    if qualifiers:
        for q in qualifiers:
            _fix2d48_validate_token("qualifier", q)

    subject_parts: List[str] = [scope, entity, metric, time_q] + list(qualifiers)
    subject = "_".join(subject_parts)
    key = f"{subject}__{dimension}"

    validate_canonical_key_v1(key, allowed_dimensions=allowed_dimensions)
    return key

def validate_canonical_key_v1(key: str, *, allowed_dimensions: Optional[Iterable[str]] = None) -> Dict[str, str]:
    subject, dimension = _fix2d48_parse_canonical_key_v1(key)

    _fix2d48_validate_token("subject", subject)
    _fix2d48_validate_dimension(_fix2d48_norm_token(dimension), allowed_dimensions=allowed_dimensions)

    parts = subject.split("_")
    if len(parts) < 4:
        raise ValueError(
            f"canonical_key: subject must have >=4 parts (scope_entity_metric_time), got {len(parts)}: '{subject}'"
        )

    scope, entity, metric = parts[0], parts[1], parts[2]

    if parts[3] == "ytd" and len(parts) >= 5:
        time_q = f"ytd_{parts[4]}"
        remaining = parts[5:]
    elif parts[3] == "asof" and len(parts) >= 6:
        if len(parts) >= 7 and re.fullmatch(r"\d{2}", parts[6]):
            time_q = f"asof_{parts[4]}_{parts[5]}_{parts[6]}"
            remaining = parts[7:]
        else:
            time_q = f"asof_{parts[4]}_{parts[5]}"
            remaining = parts[6:]
    elif re.fullmatch(r"\d{4}", parts[3]) and len(parts) >= 5 and re.fullmatch(r"\d{4}", parts[4]):
        time_q = f"{parts[3]}_{parts[4]}"
        remaining = parts[5:]
    else:
        time_q = parts[3]
        remaining = parts[4:]

    _fix2d48_validate_token("scope", scope)
    _fix2d48_validate_token("entity", entity)
    _fix2d48_validate_token("metric", metric)
    _fix2d48_validate_time_qualifier(time_q)

    for q in remaining:
        _fix2d48_validate_token("qualifier", q)

    return {
        "subject": subject,
        "dimension": _fix2d48_norm_token(dimension),
        "scope": scope,
        "entity": entity,
        "metric": metric,
        "time_qualifier": time_q,
    }

def _fix2d48_allowed_dimensions_from_schema(metric_schema_frozen: dict) -> Optional[set]:
    if not isinstance(metric_schema_frozen, dict) or not metric_schema_frozen:
        return None
    dims = set()
    for skey, spec in metric_schema_frozen.items():
        if isinstance(spec, dict):
            d = spec.get("dimension")
            if d:
                dims.add(_fix2d48_norm_token(str(d)))
    return dims or None

def validate_primary_metrics_canonical_keys_v1(primary_metrics_canonical: dict, metric_schema_frozen: Optional[dict] = None) -> None:
    allowed_dims = _fix2d48_allowed_dimensions_from_schema(metric_schema_frozen or {})
    if not isinstance(primary_metrics_canonical, dict):
        return
    for ckey in list(primary_metrics_canonical.keys()):
        validate_canonical_key_v1(str(ckey), allowed_dimensions=allowed_dims)

# =========================================================
# END FIX2D48 CANONICAL KEY MODULE
# =========================================================



# =========================================================
# FIX2D48 — Integration Hook (non-invasive)
# =========================================================
# Where to call this:
#   - Right before serializing Analysis/Evolution output (after primary_metrics_canonical is finalized),
#     call:
#       validate_primary_metrics_canonical_keys_v1(primary_metrics_canonical, metric_schema_frozen)
#
# This will crash early in dev runs if any minted key violates the grammar.
#
# NOTE:
#   This patch does not attempt to rewrite all minting sites automatically in this single-file
#   environment; it provides the authoritative builder+validator and a validator tripwire.
#   The next patch should audit and route minting through build_canonical_key_v1(...).
# =========================================================
def _fix2d48_try_validate_outputs(output_obj: dict) -> None:
    if not isinstance(output_obj, dict):
        return
    metric_schema_frozen = (
        output_obj.get("metric_schema_frozen") if isinstance(output_obj.get("metric_schema_frozen"), dict) else None
    ) or (
        output_obj.get("primary_response", {}).get("metric_schema_frozen") if isinstance(output_obj.get("primary_response"), dict) else None
    ) or (
        output_obj.get("results", {}).get("metric_schema_frozen") if isinstance(output_obj.get("results"), dict) else None
    )

    pmc = (
        output_obj.get("primary_metrics_canonical") if isinstance(output_obj.get("primary_metrics_canonical"), dict) else None
    ) or (
        output_obj.get("primary_response", {}).get("primary_metrics_canonical") if isinstance(output_obj.get("primary_response"), dict) else None
    ) or (
        output_obj.get("results", {}).get("primary_metrics_canonical") if isinstance(output_obj.get("results"), dict) else None
    )

    if isinstance(pmc, dict) and pmc:
        validate_primary_metrics_canonical_keys_v1(pmc, metric_schema_frozen=metric_schema_frozen)

def _fix2d48_should_validate_ckeys(web_context: Optional[dict]) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        return bool(web_context.get("validate_canonical_keys_v1") or web_context.get("diag_validate_ckeys_v1"))
    except Exception:
        return False



# =========================================================
# FIX2D48 — FINAL VERSION STAMP OVERRIDE
# =========================================================
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
# =========================================================
# FIX2D49 — Audit canonical-key minting + optional rekeying
# =========================================================
# Goal:
#   (1) Identify where canonical keys are being minted in a way that violates the v1 grammar,
#       or where a metric dict's own canonical_key disagrees with the dict key.
#   (2) Provide an optional, deterministic "rekey" pass that repairs obvious mismatches by:
#       - moving entries to metric["canonical_key"] if it validates
#       - or moving entries to metric["schema_key"/"schema_canonical_key"] if present + validates
#   (3) Emit a compact diagnostics ledger into output_obj["debug"] so you can see drift immediately.
#
# Safety:
#   Off by default. Enable via web_context:
#     - web_context["diag_fix2d49_audit"] = True
#     - web_context["diag_fix2d49_rekey"] = True   (optional)
#     - web_context["diag_fix2d49_strict"] = True  (raise on invalid keys)
#
# Note:
#   In a single-file patch environment we can’t reliably refactor every minting site.
#   This audit/rekey pass makes those sites visible and stabilizes downstream joins
#   while you convert minting sites to use build_canonical_key_v1(...) in subsequent cleanup.
# =========================================================

from typing import Any

def _fix2d49_get_schema_key_hint(m: dict) -> str:
    if not isinstance(m, dict):
        return ""
    for k in ("canonical_key", "schema_key", "schema_canonical_key", "schema_ckey"):
        v = m.get(k)
        if isinstance(v, str) and v.strip():
            return v.strip()
    return ""

def _fix2d49_is_suspicious_key(key: str) -> bool:
    # Heuristics (non-blocking): help surface "free-text" keys or hashed keys.
    if not isinstance(key, str):
        return True
    if len(key) > 120:
        return True
    if " " in key or "(" in key or ")" in key or "/" in key:
        return True
    # looks like a hash-heavy identifier
    if sum(ch.isdigit() for ch in key) > 50:
        return True
    return False

def _fix2d49_audit_primary_metrics_canonical(pmc: dict, metric_schema_frozen: dict | None = None) -> dict:
    """
    Returns an audit report dict:
      {
        "total": int,
        "invalid_keys": [..],
        "mismatch_key_vs_metric": [..],
        "suspicious_keys": [..],
      }
    """
    rep = {
        "total": 0,
        "invalid_keys": [],
        "mismatch_key_vs_metric": [],
        "suspicious_keys": [],
    }
    if not isinstance(pmc, dict):
        return rep

    allowed_dims = _fix2d48_allowed_dimensions_from_schema(metric_schema_frozen or {})

    for k, m in pmc.items():
        rep["total"] += 1
        sk_hint = _fix2d49_get_schema_key_hint(m)
        if sk_hint and isinstance(k, str) and sk_hint != k:
            rep["mismatch_key_vs_metric"].append({"dict_key": k, "metric_key": sk_hint})

        if isinstance(k, str) and _fix2d49_is_suspicious_key(k):
            rep["suspicious_keys"].append(k)

        try:
            validate_canonical_key_v1(str(k), allowed_dimensions=allowed_dims)
        except Exception as e:
            rep["invalid_keys"].append({"key": str(k), "error": str(e)})

    return rep

def _fix2d49_rekey_primary_metrics_canonical(pmc: dict, metric_schema_frozen: dict | None = None) -> tuple[dict, dict]:
    """
    Deterministically rekeys pmc by metric's own canonical_key/schema_key when valid.
    Returns: (new_pmc, rekey_report)
    """
    report = {
        "moved": [],
        "dropped": [],
        "kept": 0,
    }
    if not isinstance(pmc, dict) or not pmc:
        return pmc, report

    allowed_dims = _fix2d48_allowed_dimensions_from_schema(metric_schema_frozen or {})

    new_pmc = {}
    # stable iteration for determinism
    for old_key in sorted(pmc.keys(), key=lambda x: str(x)):
        m = pmc.get(old_key)
        if not isinstance(m, dict):
            continue

        # Candidate preferred: metric's explicit canonical_key
        candidate = m.get("canonical_key")
        if not (isinstance(candidate, str) and candidate.strip()):
            # next: schema hints
            candidate = m.get("schema_key") or m.get("schema_canonical_key") or m.get("schema_ckey")

        candidate = candidate.strip() if isinstance(candidate, str) else ""

        chosen_key = str(old_key) if old_key is not None else ""

        # If candidate exists and validates, use it; else keep old_key if it validates.
        def _valid(k: str) -> bool:
            try:
                validate_canonical_key_v1(k, allowed_dimensions=allowed_dims)
                return True
            except Exception:
                return False

        if candidate and _valid(candidate):
            chosen_key = candidate
        elif chosen_key and _valid(chosen_key):
            pass
        else:
            # can't validate either; keep old_key but mark dropped/invalid for downstream stability
            report["dropped"].append({"old_key": str(old_key), "candidate": candidate})
            continue

        # deterministic collision handling: prefer higher confidence, else stable tie-break
        if chosen_key in new_pmc:
            existing = new_pmc[chosen_key]
            winner = _fix2d47_pick_cur_winner(existing, m)  # reuse deterministic picker from FIX2D47
            new_pmc[chosen_key] = winner
        else:
            new_pmc[chosen_key] = m

        if str(old_key) != chosen_key:
            report["moved"].append({"from": str(old_key), "to": chosen_key})
        else:
            report["kept"] += 1

    return new_pmc, report

def _fix2d49_try_audit_and_rekey_outputs(output_obj: dict, web_context: dict | None = None) -> None:
    if not isinstance(output_obj, dict):
        return

    do_audit = bool(web_context and web_context.get("diag_fix2d49_audit"))
    do_rekey = bool(web_context and web_context.get("diag_fix2d49_rekey"))
    strict = bool(web_context and web_context.get("diag_fix2d49_strict"))

    if not (do_audit or do_rekey):
        return

    metric_schema_frozen = (
        output_obj.get("metric_schema_frozen") if isinstance(output_obj.get("metric_schema_frozen"), dict) else None
    ) or (
        output_obj.get("primary_response", {}).get("metric_schema_frozen") if isinstance(output_obj.get("primary_response"), dict) else None
    ) or (
        output_obj.get("results", {}).get("metric_schema_frozen") if isinstance(output_obj.get("results"), dict) else None
    ) or {}

    # Locate pmc in common shapes
    pmc_path = None
    pmc = None
    if isinstance(output_obj.get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_metrics_canonical",)
        pmc = output_obj["primary_metrics_canonical"]
    elif isinstance(output_obj.get("primary_response"), dict) and isinstance(output_obj["primary_response"].get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_response", "primary_metrics_canonical")
        pmc = output_obj["primary_response"]["primary_metrics_canonical"]
    elif isinstance(output_obj.get("results"), dict) and isinstance(output_obj["results"].get("primary_metrics_canonical"), dict):
        pmc_path = ("results", "primary_metrics_canonical")
        pmc = output_obj["results"]["primary_metrics_canonical"]
    else:
        pmc = None

    output_obj.setdefault("debug", {})

    if isinstance(pmc, dict) and pmc:
        audit = _fix2d49_audit_primary_metrics_canonical(pmc, metric_schema_frozen=metric_schema_frozen)
        output_obj["debug"]["fix2d49_pmc_audit"] = audit

        if strict and audit.get("invalid_keys"):
            raise RuntimeError(f"FIX2D49 strict: invalid canonical keys detected: {audit.get('invalid_keys')[:3]}")

        if do_rekey:
            new_pmc, rep = _fix2d49_rekey_primary_metrics_canonical(pmc, metric_schema_frozen=metric_schema_frozen)
            output_obj["debug"]["fix2d49_pmc_rekey"] = rep

            # Write back to the same location
            if pmc_path == ("primary_metrics_canonical",):
                output_obj["primary_metrics_canonical"] = new_pmc
            elif pmc_path == ("primary_response","primary_metrics_canonical"):
                output_obj["primary_response"]["primary_metrics_canonical"] = new_pmc
            elif pmc_path == ("results","primary_metrics_canonical"):
                output_obj["results"]["primary_metrics_canonical"] = new_pmc
    else:
        output_obj["debug"]["fix2d49_pmc_audit"] = {"total": 0, "note": "primary_metrics_canonical not found"}

# =========================================================
# END FIX2D49
# =========================================================

# =========================================================
# FIX2D49 — AUTO-HOOK FINAL OUTPUT (NO NEED TO FIND FINALIZER)
# =========================================================
# Problem:
#   You don't know which function "finalizes" primary_metrics_canonical.
#
# Solution:
#   Wrap the top-level run entrypoints (if present) and apply:
#     - FIX2D48 validation (optional flag)
#     - FIX2D49 audit/rekey (optional flags)
#   to the returned output object right before it is handed back to the UI / serializer.
#
# Enable via web_context flags (same as before):
#   web_context["validate_canonical_keys_v1"] = True
#   web_context["diag_fix2d49_audit"] = True
#   web_context["diag_fix2d49_rekey"] = True
#   web_context["diag_fix2d49_strict"] = True
#
# This is additive and does not change behaviour unless flags are enabled.
# =========================================================

def _fix2d49_extract_web_context_from_call(args, kwargs):
    # Try kwargs first
    wc = kwargs.get("web_context")
    if isinstance(wc, dict):
        return wc
    # Heuristic: scan args for a dict that looks like web_context
    for a in args:
        if isinstance(a, dict) and any(k in a for k in ("diag_fix2d49_audit","diag_fix2d49_rekey","validate_canonical_keys_v1","diag_validate_ckeys_v1")):
            return a
    return None

def _fix2d49_apply_postprocess_if_enabled(output_obj, web_context):
    if not isinstance(output_obj, dict):
        return output_obj

    # FIX2D48 validation (opt-in)
    if _fix2d48_should_validate_ckeys(web_context):
        _fix2d48_try_validate_outputs(output_obj)

    # FIX2D49 audit/rekey (opt-in via flags inside the function)
    _fix2d49_try_audit_and_rekey_outputs(output_obj, web_context)

    # FIX2D50 gatekeeper (opt-in)
    _fix2d50_try_gate_output_obj(output_obj, web_context)

    return output_obj

    # FIX2D48 validation (opt-in)
    try:
        if _fix2d48_should_validate_ckeys(web_context):
            _fix2d48_try_validate_outputs(output_obj)
    except Exception:
        pass
        # keep behaviour consistent: validation failures are surfaced only when flag enabled;
        # if enabled, allow exception to propagate.
        raise

    # FIX2D49 audit/rekey (opt-in via flags inside the function)
    try:
        _fix2d49_try_audit_and_rekey_outputs(output_obj, web_context)
    except Exception:
        pass
        raise

    return output_obj

def _fix2d49_wrap_entrypoint(fn_name: str):
    fn = globals().get(fn_name)
    if not callable(fn):
        return
    # Avoid double-wrapping
    if getattr(fn, "_fix2d49_wrapped", False):
        return

    def _wrapped(*args, **kwargs):
        web_context = _fix2d49_extract_web_context_from_call(args, kwargs)
        out = fn(*args, **kwargs)
        return _fix2d49_apply_postprocess_if_enabled(out, web_context)

    _wrapped._fix2d49_wrapped = True
    _wrapped.__name__ = getattr(fn, "__name__", fn_name)
    _wrapped.__doc__ = getattr(fn, "__doc__", None)
    globals()[fn_name] = _wrapped

def _fix2d49_install_autohooks():
    # Common entrypoints in your project (safe no-ops if absent)
    for name in (
        "run_source_anchored_analysis",
        "run_source_anchored_evolution",
        "run_source_anchored_evolution_previous_data",
        "run_source_anchored_evolution_previousdata",  # legacy spelling
        "run_analysis",
        "run_evolution",
    ):
        _fix2d49_wrap_entrypoint(name)

# Install at import time (additive)
try:
    _fix2d49_install_autohooks()
except Exception:
    pass

# =========================================================
# END AUTO-HOOK
# =========================================================

# =========================================================
# FIX2D50 — PMC Gatekeeper: Schema-bound canonical keys only
# =========================================================
# Objective:
#   Close the remaining gap by making schema binding authoritative at the PMC boundary.
#
# Rule enforced (when enabled):
#   - primary_metrics_canonical may only contain keys that:
#       (a) validate under FIX2D48 grammar
#       (b) exist in metric_schema_frozen (schema allowlist)
#       (c) have a non-"unknown" dimension compatible with schema
#   - dict key is the canonical key; metric["canonical_key"] is set to the same key.
#
# Enablement:
#   Gate runs when any of these is true:
#     - web_context["diag_fix2d50_gate"] = True
#     - web_context["enforce_schema_bound_pmc"] = True
#     - web_context["diag_fix2d49_rekey"] = True   (rekey implies intent to canonicalize)
#
# Strictness:
#   - If web_context["diag_fix2d50_strict"] or web_context["diag_fix2d49_strict"] is True,
#     the gate raises if it drops any PMC entries or finds invalid keys.
#
# Output:
#   - Writes debug.fix2d50_pmc_gate = {kept, dropped, dropped_examples, strict, enabled}
# =========================================================

def _fix2d50_should_gate(web_context: dict | None) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        return bool(
            web_context.get("diag_fix2d50_gate")
            or web_context.get("enforce_schema_bound_pmc")
            or web_context.get("diag_fix2d49_rekey")
        )
    except Exception:
        return False

def _fix2d50_is_strict(web_context: dict | None) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        return bool(web_context.get("diag_fix2d50_strict") or web_context.get("diag_fix2d49_strict"))
    except Exception:
        return False

def _fix2d50_get_dimension_from_metric(m: dict) -> str:
    if not isinstance(m, dict):
        return ""
    for k in ("dimension", "dim", "metric_dimension"):
        v = m.get(k)
        if isinstance(v, str) and v.strip():
            return _fix2d48_norm_token(v)
    return ""

def _fix2d50_get_schema_dimension(metric_schema_frozen: dict, ckey: str) -> str:
    try:
        spec = metric_schema_frozen.get(ckey)
        if isinstance(spec, dict):
            d = spec.get("dimension")
            if isinstance(d, str) and d.strip():
                return _fix2d48_norm_token(d)
    except Exception:
        return ""

def _fix2d50_gate_primary_metrics_canonical(pmc: dict, metric_schema_frozen: dict, web_context: dict | None) -> tuple[dict, dict]:
    report = {
        "enabled": True,
        "strict": _fix2d50_is_strict(web_context),
        "total_in": 0,
        "kept": 0,
        "dropped": 0,
        "dropped_examples": [],
        "reasons": {},  # reason -> count
    }
    if not isinstance(pmc, dict) or not pmc:
        report["enabled"] = True
        report["total_in"] = 0
        return pmc, report
    if not isinstance(metric_schema_frozen, dict) or not metric_schema_frozen:
        # Can't enforce allowlist if schema absent; keep but record.
        report["enabled"] = True
        report["note"] = "metric_schema_frozen missing/empty; gate skipped"
        report["total_in"] = len(pmc)
        report["kept"] = len(pmc)
        return pmc, report

    allowed_dims = _fix2d48_allowed_dimensions_from_schema(metric_schema_frozen)  # normalized set or None

    out = {}
    for k in sorted(pmc.keys(), key=lambda x: str(x)):
        report["total_in"] += 1
        key = str(k)

        # (a) grammar validation
        try:
            validate_canonical_key_v1(key, allowed_dimensions=allowed_dims)
        except Exception as e:
            reason = "invalid_grammar"
            report["reasons"][reason] = report["reasons"].get(reason, 0) + 1
            report["dropped"] += 1
            if len(report["dropped_examples"]) < 10:
                report["dropped_examples"].append({"key": key, "reason": reason, "error": str(e)})
            continue

        # (b) schema allowlist
        if key not in metric_schema_frozen:
            reason = "not_in_schema"
            report["reasons"][reason] = report["reasons"].get(reason, 0) + 1
            report["dropped"] += 1
            if len(report["dropped_examples"]) < 10:
                report["dropped_examples"].append({"key": key, "reason": reason})
            continue

        m = pmc.get(k)
        if not isinstance(m, dict):
            reason = "non_dict_metric"
            report["reasons"][reason] = report["reasons"].get(reason, 0) + 1
            report["dropped"] += 1
            if len(report["dropped_examples"]) < 10:
                report["dropped_examples"].append({"key": key, "reason": reason})
            continue

        # (c) dimension guard
        md = _fix2d50_get_dimension_from_metric(m)
        sd = _fix2d50_get_schema_dimension(metric_schema_frozen, key)
        # reject unknowns explicitly
        if md == "unknown" or sd == "unknown":
            reason = "unknown_dimension"
            report["reasons"][reason] = report["reasons"].get(reason, 0) + 1
            report["dropped"] += 1
            if len(report["dropped_examples"]) < 10:
                report["dropped_examples"].append({"key": key, "reason": reason, "metric_dim": md, "schema_dim": sd})
            continue
        # If both known and disagree, drop (prevents silent unit/dim leakage)
        if md and sd and md != sd:
            reason = "dimension_mismatch"
            report["reasons"][reason] = report["reasons"].get(reason, 0) + 1
            report["dropped"] += 1
            if len(report["dropped_examples"]) < 10:
                report["dropped_examples"].append({"key": key, "reason": reason, "metric_dim": md, "schema_dim": sd})
            continue

        # Make canonical_key authoritative at boundary
        m["canonical_key"] = key

        out[key] = m
        report["kept"] += 1

    # Strict raising if anything dropped
    if report["strict"] and report["dropped"] > 0:
        raise RuntimeError(f"FIX2D50 strict: dropped {report['dropped']} PMC entries; examples={report['dropped_examples'][:3]}")

    return out, report

def _fix2d50_try_gate_output_obj(output_obj: dict, web_context: dict | None = None) -> None:
    if not isinstance(output_obj, dict):
        return
    if not _fix2d50_should_gate(web_context):
        return

    metric_schema_frozen = (
        output_obj.get("metric_schema_frozen") if isinstance(output_obj.get("metric_schema_frozen"), dict) else None
    ) or (
        output_obj.get("primary_response", {}).get("metric_schema_frozen") if isinstance(output_obj.get("primary_response"), dict) else None
    ) or (
        output_obj.get("results", {}).get("metric_schema_frozen") if isinstance(output_obj.get("results"), dict) else None
    ) or {}

    # Locate PMC in common shapes
    pmc_path = None
    pmc = None
    if isinstance(output_obj.get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_metrics_canonical",)
        pmc = output_obj["primary_metrics_canonical"]
    elif isinstance(output_obj.get("primary_response"), dict) and isinstance(output_obj["primary_response"].get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_response", "primary_metrics_canonical")
        pmc = output_obj["primary_response"]["primary_metrics_canonical"]
    elif isinstance(output_obj.get("results"), dict) and isinstance(output_obj["results"].get("primary_metrics_canonical"), dict):
        pmc_path = ("results", "primary_metrics_canonical")
        pmc = output_obj["results"]["primary_metrics_canonical"]

    output_obj.setdefault("debug", {})

    if isinstance(pmc, dict):
        new_pmc, rep = _fix2d50_gate_primary_metrics_canonical(pmc, metric_schema_frozen, web_context)
        output_obj["debug"]["fix2d50_pmc_gate"] = rep

        if pmc_path == ("primary_metrics_canonical",):
            output_obj["primary_metrics_canonical"] = new_pmc
        elif pmc_path == ("primary_response","primary_metrics_canonical"):
            output_obj["primary_response"]["primary_metrics_canonical"] = new_pmc
        elif pmc_path == ("results","primary_metrics_canonical"):
            output_obj["results"]["primary_metrics_canonical"] = new_pmc
    else:
        output_obj["debug"]["fix2d50_pmc_gate"] = {"enabled": True, "note": "primary_metrics_canonical not found"}

# =========================================================
# END FIX2D50
# =========================================================







# =========================================================
# FIX2D49 — FINAL VERSION STAMP OVERRIDE
# =========================================================
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
# =========================================================
# FIX2D52 — Schema-first canonical key resolution (binder)
# =========================================================
# Objective:
#   Close the canonical-key convergence gap by deterministically binding PMC metrics
#   to existing schema keys (metric_schema_frozen) BEFORE the FIX2D50 gate runs.
#
# Mechanism:
#   - For each metric in primary_metrics_canonical, attempt to match a schema key using:
#       * schema keywords overlap (authoritative allowlist)
#       * dimension compatibility (prefer schema dimension; allow unknown->schema)
#       * time-qualifier hint extracted from current key/name (year/ytd/asof/range)
#   - If a high-confidence match is found, rekey the PMC entry to that schema key and
#     set metric["canonical_key"] = schema_key and metric["dimension"] = schema_dimension (if unknown).
#
# Enablement (recommended ON for now):
#   - web_context["diag_fix2d52_bind"] = True
#   - OR web_context["enforce_schema_bound_pmc"] = True
#
# Strictness:
#   - If web_context["diag_fix2d52_strict"] is True, raise if any metric cannot be bound
#     AND is not already a valid schema key.
#
# Output:
#   - debug.fix2d52_schema_bind = {total, bound, already_schema, unbound, examples}
# =========================================================

def _fix2d52_should_bind(web_context: dict | None) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        return bool(web_context.get("diag_fix2d52_bind") or web_context.get("enforce_schema_bound_pmc"))
    except Exception:
        return False

def _fix2d52_is_strict(web_context: dict | None) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        return bool(web_context.get("diag_fix2d52_strict"))
    except Exception:
        return False

def _fix2d52_norm_text(s: str) -> str:
    s = (s or "").lower()
    s = re.sub(r"[^a-z0-9]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _fix2d52_extract_time_hint(s: str) -> str:
    """
    Return a time qualifier hint in canonical form if found (best-effort):
      - ytd_YYYY
      - YYYY
      - YYYY_YYYY (range)
    """
    s2 = (s or "").lower()
    # ytd patterns
    m = re.search(r"\b(ytd|year to date|year-to-date)\s*(20\d{2})\b", s2)
    if m:
        return f"ytd_{m.group(2)}"
    # range patterns (2026-2040 etc)
    m = re.search(r"\b(20\d{2})\s*(?:-|–|to)\s*(20\d{2})\b", s2)
    if m:
        a, b = m.group(1), m.group(2)
        if int(b) >= int(a):
            return f"{a}_{b}"
    # single year
    m = re.search(r"\b(20\d{2})\b", s2)
    if m:
        return m.group(1)
    return ""

def _fix2d52_schema_candidates(metric_schema_frozen: dict) -> list:
    cands = []
    if not isinstance(metric_schema_frozen, dict):
        return cands
    for skey, spec in metric_schema_frozen.items():
        if not isinstance(skey, str) or "__" not in skey:
            continue
        if not isinstance(spec, dict):
            spec = {}
        kw = spec.get("keywords")
        if isinstance(kw, list):
            keywords = [_fix2d52_norm_text(str(x)) for x in kw if str(x).strip()]
        else:
            keywords = []
        dim = spec.get("dimension")
        dim = _fix2d48_norm_token(str(dim)) if dim else ""
        # also build a normalized name bag from metric_name/name and the key itself
        nm = spec.get("metric_name") or spec.get("name") or ""
        name_norm = _fix2d52_norm_text(str(nm))
        key_norm = _fix2d52_norm_text(skey.replace("__", " "))
        cands.append({
            "skey": skey,
            "dimension": dim,
            "keywords": keywords,
            "name_norm": name_norm,
            "key_norm": key_norm,
            "time_hint": _fix2d52_extract_time_hint(skey),
        })
    return cands

def _fix2d52_score_match(metric: dict, dict_key: str, schema_cand: dict) -> float:
    # Base: keyword overlap
    text_fields = []
    if isinstance(metric, dict):
        for k in ("name", "metric_name", "label", "title", "snippet", "raw_text"):
            v = metric.get(k)
            if isinstance(v, str) and v.strip():
                text_fields.append(v)
    text_fields.append(dict_key or "")
    blob = _fix2d52_norm_text(" ".join(text_fields))
    if not blob:
        return 0.0

    blob_set = set(blob.split(" "))
    kw = schema_cand.get("keywords") or []
    kw_hits = 0
    for k in kw:
        # keyword list may contain multi-word phrases; count a hit if all tokens present
        toks = k.split(" ")
        if toks and all(t in blob_set for t in toks):
            kw_hits += 1

    # Name/key soft overlap
    name_norm = schema_cand.get("name_norm") or ""
    key_norm = schema_cand.get("key_norm") or ""
    name_hits = 0
    for t in name_norm.split(" "):
        if t and t in blob_set:
            name_hits += 0.2  # soft
    for t in key_norm.split(" "):
        if t and t in blob_set:
            name_hits += 0.05  # very soft

    # Dimension compatibility
    sd = schema_cand.get("dimension") or ""
    md = _fix2d50_get_dimension_from_metric(metric) if isinstance(metric, dict) else ""
    dim_score = 0.0
    if sd and md:
        if md == "unknown":
            dim_score = 0.4
        elif md == sd:
            dim_score = 0.8
        else:
            dim_score = -1.0  # hard penalty
    elif sd and not md:
        dim_score = 0.2

    # Time qualifier hint
    mh = _fix2d52_extract_time_hint(blob)
    th = schema_cand.get("time_hint") or ""
    time_score = 0.0
    if mh and th and mh == th:
        time_score = 0.6
    elif mh and th and (mh in th or th in mh):
        time_score = 0.2

    score = (kw_hits * 1.0) + name_hits + dim_score + time_score
    return float(score)

def _fix2d52_bind_pmc_to_schema(pmc: dict, metric_schema_frozen: dict, web_context: dict | None) -> tuple[dict, dict]:
    report = {
        "enabled": True,
        "total": 0,
        "already_schema": 0,
        "bound": 0,
        "unbound": 0,
        "examples": [],
        "threshold": 2.0,
    }
    if not isinstance(pmc, dict) or not pmc:
        report["enabled"] = True
        return pmc, report
    if not isinstance(metric_schema_frozen, dict) or not metric_schema_frozen:
        report["enabled"] = True
        report["note"] = "metric_schema_frozen missing; bind skipped"
        return pmc, report

    # Precompute schema candidates once
    cands = _fix2d52_schema_candidates(metric_schema_frozen)
    thresh = float(web_context.get("diag_fix2d52_threshold") or report["threshold"]) if isinstance(web_context, dict) else report["threshold"]
    report["threshold"] = thresh

    allowed_dims = _fix2d48_allowed_dimensions_from_schema(metric_schema_frozen) or None

    out = {}
    strict = _fix2d52_is_strict(web_context)

    for old_key in sorted(pmc.keys(), key=lambda x: str(x)):
        report["total"] += 1
        key = str(old_key)
        m = pmc.get(old_key)
        if not isinstance(m, dict):
            continue

        # If already a valid schema key, keep.
        if key in metric_schema_frozen:
            out[key] = m
            m["canonical_key"] = key
            report["already_schema"] += 1
            continue

        # If key is grammatically invalid, we still attempt schema bind (that's the whole point).
        best = None
        best_score = -1e9
        for cand in cands:
            s = _fix2d52_score_match(m, key, cand)
            if s > best_score:
                best_score = s
                best = cand

        if best is not None and best_score >= thresh:
            skey = best["skey"]
            # Validate schema key (should always validate, but keep safe)
            try:
                validate_canonical_key_v1(skey, allowed_dimensions=allowed_dims)
            except Exception:
                pass
                # If somehow invalid, treat as unbound
                best_score = -1e9
                best = None
            else:
                # Apply binding
                m["canonical_key"] = skey
                # If dimension unknown, adopt schema dimension
                md = _fix2d50_get_dimension_from_metric(m)
                sd = best.get("dimension") or ""
                if (not md) or md == "unknown":
                    if sd:
                        m["dimension"] = sd
                out[skey] = _fix2d47_pick_cur_winner(out.get(skey), m)
                report["bound"] += 1
                if len(report["examples"]) < 10:
                    report["examples"].append({"from": key, "to": skey, "score": round(best_score, 3)})
                continue

        # Unbound: keep original in out (so downstream audit can see it), unless strict.
        report["unbound"] += 1
        if strict:
            raise RuntimeError(f"FIX2D52 strict: could not bind PMC key '{key}' to schema (best_score={best_score})")
        out[key] = m

    return out, report

def _fix2d52_try_bind_output_obj(output_obj: dict, web_context: dict | None = None) -> None:
    if not isinstance(output_obj, dict):
        return
    if not _fix2d52_should_bind(web_context):
        return

    metric_schema_frozen = (
        output_obj.get("metric_schema_frozen") if isinstance(output_obj.get("metric_schema_frozen"), dict) else None
    ) or (
        output_obj.get("primary_response", {}).get("metric_schema_frozen") if isinstance(output_obj.get("primary_response"), dict) else None
    ) or (
        output_obj.get("results", {}).get("metric_schema_frozen") if isinstance(output_obj.get("results"), dict) else None
    ) or {}

    # Locate PMC in common shapes
    pmc_path = None
    pmc = None
    if isinstance(output_obj.get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_metrics_canonical",)
        pmc = output_obj["primary_metrics_canonical"]
    elif isinstance(output_obj.get("primary_response"), dict) and isinstance(output_obj["primary_response"].get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_response", "primary_metrics_canonical")
        pmc = output_obj["primary_response"]["primary_metrics_canonical"]
    elif isinstance(output_obj.get("results"), dict) and isinstance(output_obj["results"].get("primary_metrics_canonical"), dict):
        pmc_path = ("results", "primary_metrics_canonical")
        pmc = output_obj["results"]["primary_metrics_canonical"]

    output_obj.setdefault("debug", {})

    if isinstance(pmc, dict):
        new_pmc, rep = _fix2d52_bind_pmc_to_schema(pmc, metric_schema_frozen, web_context)
        output_obj["debug"]["fix2d52_schema_bind"] = rep

        if pmc_path == ("primary_metrics_canonical",):
            output_obj["primary_metrics_canonical"] = new_pmc
        elif pmc_path == ("primary_response","primary_metrics_canonical"):
            output_obj["primary_response"]["primary_metrics_canonical"] = new_pmc
        elif pmc_path == ("results","primary_metrics_canonical"):
            output_obj["results"]["primary_metrics_canonical"] = new_pmc
    else:
        output_obj["debug"]["fix2d52_schema_bind"] = {"enabled": True, "note": "primary_metrics_canonical not found"}

# =========================================================
# END FIX2D52
# =========================================================

# =========================================================
# FIX2D53 — Legacy->Schema Baseline Remap (Option A)
# =========================================================
# Objective:
#   Allow schema-bound diffing to activate even when Analysis produced legacy/text-derived keys,
#   by deterministically remapping a small set of known legacy key shapes onto existing schema keys.
#
# Rationale:
#   - Analysis may emit keys like "2025_global_ev_sales__unknown"
#   - Schema key is "global_ev_sales_ytd_2025__unit_sales"
#   - Without remap, prev/cur overlap is zero -> both_count stays 0.
#
# Policy:
#   - Only remap when the destination schema key exists in metric_schema_frozen.
#   - Only remap when the source key fails schema allowlisting OR has dimension "unknown".
#   - Remap is deterministic and narrow (pattern-based); it does NOT invoke any LLM/NLP.
#
# Enablement:
#   - web_context["diag_fix2d53_remap"] = True
#   - OR web_context["enforce_schema_bound_pmc"] = True
#
# Output:
#   - debug.fix2d53_legacy_remap = {enabled,total,mapped,examples}
# =========================================================

def _fix2d53_should_remap(web_context: dict | None) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        return bool(web_context.get("diag_fix2d53_remap") or web_context.get("enforce_schema_bound_pmc"))
    except Exception:
        return False

def _fix2d53_extract_year_from_key(ckey: str) -> str:
    if not isinstance(ckey, str):
        return ""
    m = re.search(r"\b(20\d{2})\b", ckey)
    return m.group(1) if m else ""

def _fix2d53_schema_key_exists(metric_schema_frozen: dict, skey: str) -> bool:
    return isinstance(metric_schema_frozen, dict) and isinstance(skey, str) and skey in metric_schema_frozen

def _fix2d53_schema_dim(metric_schema_frozen: dict, skey: str) -> str:
    try:
        spec = metric_schema_frozen.get(skey)
        if isinstance(spec, dict):
            d = spec.get("dimension")
            if isinstance(d, str) and d.strip():
                return _fix2d48_norm_token(d)
    except Exception:
        return ""

def _fix2d53_apply_legacy_schema_remaps(pmc: dict, metric_schema_frozen: dict, web_context: dict | None) -> tuple[dict, dict]:
    rep = {"enabled": True, "total": 0, "mapped": 0, "examples": []}
    if not isinstance(pmc, dict) or not pmc:
        return pmc, rep
    if not isinstance(metric_schema_frozen, dict) or not metric_schema_frozen:
        rep["note"] = "metric_schema_frozen missing; remap skipped"
        return pmc, rep

    out = {}
    for k in sorted(pmc.keys(), key=lambda x: str(x)):
        rep["total"] += 1
        key = str(k)
        m = pmc.get(k)
        if not isinstance(m, dict):
            out[key] = m
            continue

        md = _fix2d50_get_dimension_from_metric(m)

        # Only consider remap for non-schema keys OR unknown dimension
        if key in metric_schema_frozen and md != "unknown":
            out[key] = m
            continue

        year = _fix2d53_extract_year_from_key(key) or _fix2d53_extract_year_from_key(m.get("name",""))

        mapped = False
        dest = None

        # --- Remap Rule 1: global EV sales legacy key -> schema YTD key (demo-enabler)
        # Examples:
        #   "2025_global_ev_sales__unknown" -> "global_ev_sales_ytd_2025__unit_sales"
        if year and re.fullmatch(rf"{year}_global_ev_sales__unknown", key):
            cand = f"global_ev_sales_ytd_{year}__unit_sales"
            if _fix2d53_schema_key_exists(metric_schema_frozen, cand):
                dest = cand
                mapped = True

        # --- Remap Rule 2: china EV sales legacy key -> schema YTD key if present
        #   "2025_china_ev_sales__unknown" -> "china_ev_sales_ytd_2025__unit_sales"
        if (not mapped) and year and re.fullmatch(rf"{year}_china_ev_sales__unknown", key):
            cand = f"china_ev_sales_ytd_{year}__unit_sales"
            if _fix2d53_schema_key_exists(metric_schema_frozen, cand):
                dest = cand
                mapped = True

        if mapped and dest:
            # adopt schema dimension if unknown/missing
            sd = _fix2d53_schema_dim(metric_schema_frozen, dest)
            if (not md) or md == "unknown":
                if sd:
                    m["dimension"] = sd
            m["canonical_key"] = dest

            # collision handling deterministic
            if dest in out and isinstance(out[dest], dict):
                out[dest] = _fix2d47_pick_cur_winner(out[dest], m)
            else:
                out[dest] = m

            rep["mapped"] += 1
            if len(rep["examples"]) < 10:
                rep["examples"].append({"from": key, "to": dest})
            continue

        out[key] = m

    return out, rep

def _fix2d53_try_remap_output_obj(output_obj: dict, web_context: dict | None = None) -> None:
    if not isinstance(output_obj, dict):
        return
    if not _fix2d53_should_remap(web_context):
        return

    metric_schema_frozen = (
        output_obj.get("metric_schema_frozen") if isinstance(output_obj.get("metric_schema_frozen"), dict) else None
    ) or (
        output_obj.get("primary_response", {}).get("metric_schema_frozen") if isinstance(output_obj.get("primary_response"), dict) else None
    ) or (
        output_obj.get("results", {}).get("metric_schema_frozen") if isinstance(output_obj.get("results"), dict) else None
    ) or {}

    pmc_path = None
    pmc = None
    if isinstance(output_obj.get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_metrics_canonical",)
        pmc = output_obj["primary_metrics_canonical"]
    elif isinstance(output_obj.get("primary_response"), dict) and isinstance(output_obj["primary_response"].get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_response", "primary_metrics_canonical")
        pmc = output_obj["primary_response"]["primary_metrics_canonical"]
    elif isinstance(output_obj.get("results"), dict) and isinstance(output_obj["results"].get("primary_metrics_canonical"), dict):
        pmc_path = ("results", "primary_metrics_canonical")
        pmc = output_obj["results"]["primary_metrics_canonical"]

    output_obj.setdefault("debug", {})

    if isinstance(pmc, dict):
        new_pmc, rep = _fix2d53_apply_legacy_schema_remaps(pmc, metric_schema_frozen, web_context)
        output_obj["debug"]["fix2d53_legacy_remap"] = rep

        if pmc_path == ("primary_metrics_canonical",):
            output_obj["primary_metrics_canonical"] = new_pmc
        elif pmc_path == ("primary_response","primary_metrics_canonical"):
            output_obj["primary_response"]["primary_metrics_canonical"] = new_pmc
        elif pmc_path == ("results","primary_metrics_canonical"):
            output_obj["results"]["primary_metrics_canonical"] = new_pmc
    else:
        output_obj["debug"]["fix2d53_legacy_remap"] = {"enabled": True, "note": "primary_metrics_canonical not found"}

# =========================================================
# END FIX2D53
# =========================================================




# =========================================================
# FIX2D52 — FINAL VERSION STAMP OVERRIDE
# =========================================================
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
# =========================================================
# FIX2D54 — Schema Baseline Materialisation (PMC lifting)
# =========================================================
# Objective:
#   Ensure Analysis baseline values are materialised under schema keys so that
#   Evolution can produce BOTH(prev+cur) rows for diffing.
#
# What it does (when enabled):
#   - Scans primary_metrics_canonical (PMC) for legacy keys and/or metrics that
#     imply a destination schema key.
#   - If a destination schema key exists in metric_schema_frozen, it creates/updates
#     PMC[dest_schema_key] using the legacy metric record (value_norm, unit, source, etc.).
#
# Enablement:
#   - web_context["diag_fix2d54_materialize"] = True
#   - OR web_context["enforce_schema_bound_pmc"] = True
#
# Safety:
#   - Does NOT invent values; only re-homes existing extracted metrics.
#   - Collision handling is deterministic (reuses FIX2D47 picker).
#
# Output:
#   - debug.fix2d54_materialize = {enabled,total,created,updated,skipped,examples}
# =========================================================

def _fix2d54_should_materialize(web_context: dict | None) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        return bool(web_context.get("diag_fix2d54_materialize") or web_context.get("enforce_schema_bound_pmc"))
    except Exception:
        return False

def _fix2d54_guess_dest_schema_key(legacy_key: str, metric: dict, metric_schema_frozen: dict) -> str:
    """
    Deterministic, narrow remap guesses:
      - <YYYY>_global_ev_sales__unknown -> global_ev_sales_ytd_<YYYY>__unit_sales
      - <YYYY>_china_ev_sales__unknown  -> china_ev_sales_ytd_<YYYY>__unit_sales (if present)
    Plus: if metric has canonical_key/schema_key that exists in schema, use it.
    """
    if isinstance(metric, dict):
        for kk in ("canonical_key", "schema_key", "schema_canonical_key", "schema_ckey"):
            v = metric.get(kk)
            if isinstance(v, str) and v in metric_schema_frozen:
                return v

    key = str(legacy_key or "")
    year = ""
    m = re.search(r"\b(20\d{2})\b", key)
    if m:
        year = m.group(1)

    if year and re.fullmatch(rf"{year}_global_ev_sales__unknown", key):
        cand = f"global_ev_sales_ytd_{year}__unit_sales"
        if cand in metric_schema_frozen:
            return cand

    if year and re.fullmatch(rf"{year}_china_ev_sales__unknown", key):
        cand = f"china_ev_sales_ytd_{year}__unit_sales"
        if cand in metric_schema_frozen:
            return cand

    return ""

def _fix2d54_materialize_schema_baseline(pmc: dict, metric_schema_frozen: dict, web_context: dict | None) -> tuple[dict, dict]:
    rep = {"enabled": True, "total": 0, "created": 0, "updated": 0, "skipped": 0, "examples": []}
    if not isinstance(pmc, dict) or not pmc:
        return pmc, rep
    if not isinstance(metric_schema_frozen, dict) or not metric_schema_frozen:
        rep["note"] = "metric_schema_frozen missing; materialize skipped"
        return pmc, rep

    out = dict(pmc)  # copy
    for k in sorted(pmc.keys(), key=lambda x: str(x)):
        rep["total"] += 1
        key = str(k)
        m = pmc.get(k)
        if not isinstance(m, dict):
            rep["skipped"] += 1
            continue

        dest = _fix2d54_guess_dest_schema_key(key, m, metric_schema_frozen)
        if not dest or dest == key:
            rep["skipped"] += 1
            continue

        # Adopt schema dimension if metric dimension unknown/missing
        md = _fix2d50_get_dimension_from_metric(m)
        sd = _fix2d53_schema_dim(metric_schema_frozen, dest) if " _fix2d53_schema_dim" else ""
        if (not md) or md == "unknown":
            if sd:
                m["dimension"] = sd

        m["canonical_key"] = dest

        if dest in out and isinstance(out[dest], dict):
            out[dest] = _fix2d47_pick_cur_winner(out[dest], m)
            rep["updated"] += 1
        else:
            out[dest] = m
            rep["created"] += 1

        if len(rep["examples"]) < 10:
            rep["examples"].append({"from": key, "to": dest})

    return out, rep

def _fix2d54_try_materialize_output_obj(output_obj: dict, web_context: dict | None = None) -> None:
    if not isinstance(output_obj, dict):
        return
    if not _fix2d54_should_materialize(web_context):
        return

    metric_schema_frozen = (
        output_obj.get("metric_schema_frozen") if isinstance(output_obj.get("metric_schema_frozen"), dict) else None
    ) or (
        output_obj.get("primary_response", {}).get("metric_schema_frozen") if isinstance(output_obj.get("primary_response"), dict) else None
    ) or (
        output_obj.get("results", {}).get("metric_schema_frozen") if isinstance(output_obj.get("results"), dict) else None
    ) or {}

    pmc_path = None
    pmc = None
    if isinstance(output_obj.get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_metrics_canonical",)
        pmc = output_obj["primary_metrics_canonical"]
    elif isinstance(output_obj.get("primary_response"), dict) and isinstance(output_obj["primary_response"].get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_response", "primary_metrics_canonical")
        pmc = output_obj["primary_response"]["primary_metrics_canonical"]
    elif isinstance(output_obj.get("results"), dict) and isinstance(output_obj["results"].get("primary_metrics_canonical"), dict):
        pmc_path = ("results", "primary_metrics_canonical")
        pmc = output_obj["results"]["primary_metrics_canonical"]

    output_obj.setdefault("debug", {})

    if isinstance(pmc, dict):
        new_pmc, rep = _fix2d54_materialize_schema_baseline(pmc, metric_schema_frozen, web_context)
        output_obj["debug"]["fix2d54_materialize"] = rep

        if pmc_path == ("primary_metrics_canonical",):
            output_obj["primary_metrics_canonical"] = new_pmc
        elif pmc_path == ("primary_response","primary_metrics_canonical"):
            output_obj["primary_response"]["primary_metrics_canonical"] = new_pmc
        elif pmc_path == ("results","primary_metrics_canonical"):
            output_obj["results"]["primary_metrics_canonical"] = new_pmc
    else:
        output_obj["debug"]["fix2d54_materialize"] = {"enabled": True, "note": "primary_metrics_canonical not found"}

# =========================================================
# END FIX2D54
# =========================================================
# =========================================================
# FIX2D55 — Pre-Diff Schema Lift for Prev Full Payload
# =========================================================
# Objective:
#   Ensure the rehydrated previous analysis payload (prev_full) has schema-keyed
#   baseline values materialised BEFORE compute_source_anchored_diff runs.
#
# Why:
#   Even if Analysis produced legacy keys, Evolution diff is prev-key driven; if the
#   schema key has no prev_value_norm, it becomes "added" and both_count stays 0.
#
# What it does:
#   - Applies FIX2D53 remap + FIX2D54 materialisation + FIX2D52 bind + FIX2D50 gate
#     to prev_full (in-place) just before diff computation.
#
# Enablement:
#   - web_context["diag_fix2d55_prev_lift"] = True
#   - OR web_context["diag_fix2d54_materialize"] / ["diag_fix2d53_remap"] / ["diag_fix2d52_bind"]
#     / ["diag_fix2d50_gate"] are enabled (any of them).
#
# Output:
#   - web_context.debug.fix2d55_prev_lift
# =========================================================

def _fix2d55_should_prev_lift(web_context: dict | None) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        if web_context.get("diag_fix2d55_prev_lift"):
            return True
        # FIX2D56: auto-enable prev-lift when injection present
        if _fix2d56_should_enable(web_context):
            return True
        # If any of the schema-bound pipeline flags are enabled, lift prev as well.
        for k in (
            "diag_fix2d54_materialize",
            "diag_fix2d53_remap",
            "diag_fix2d52_bind",
            "diag_fix2d50_gate",
            "enforce_schema_bound_pmc",
        ):
            if web_context.get(k):
                return True
        return False
    except Exception:
        return False

def _fix2d55_apply_prev_lift(prev_full: dict, web_context: dict | None) -> None:
    if not isinstance(prev_full, dict):
        return
    if not _fix2d55_should_prev_lift(web_context):
        return

    try:
        # Reuse the same postprocess logic you run on outputs, but target prev_full explicitly.
        # Ensure a debug bucket exists on web_context for quick visibility.
        if isinstance(web_context, dict):
            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].setdefault("fix2d55_prev_lift", {"enabled": True})

        # Apply in-place, in the same order as the output postprocess.
        # 1) FIX2D53 remap (if present)
        try:
            _fix2d53_try_remap_output_obj(prev_full, web_context)
        except Exception:
            pass
            # keep going; remap is best-effort
            if isinstance(web_context, dict) and isinstance(web_context.get("debug"), dict):
                web_context["debug"]["fix2d55_prev_lift"]["remap_error"] = True

        # 2) FIX2D52 bind (if present)
        try:
            _fix2d52_try_bind_output_obj(prev_full, web_context)
        except Exception:
            pass
            if isinstance(web_context, dict) and isinstance(web_context.get("debug"), dict):
                web_context["debug"]["fix2d55_prev_lift"]["bind_error"] = True

        # 3) FIX2D54 materialise (if present)
        try:
            _fix2d54_try_materialize_output_obj(prev_full, web_context)
        except Exception:
            pass
            if isinstance(web_context, dict) and isinstance(web_context.get("debug"), dict):
                web_context["debug"]["fix2d55_prev_lift"]["materialize_error"] = True

        # 4) FIX2D50 gate (if present)
        try:
            _fix2d50_try_gate_output_obj(prev_full, web_context)
        except Exception:
            pass
            if isinstance(web_context, dict) and isinstance(web_context.get("debug"), dict):
                web_context["debug"]["fix2d55_prev_lift"]["gate_error"] = True

        # Capture quick counters if available in prev_full.debug
        if isinstance(web_context, dict) and isinstance(web_context.get("debug"), dict):
            _rep = web_context["debug"].get("fix2d55_prev_lift") or {}
            if isinstance(_rep, dict):
                _rep["done"] = True
                # Pull embedded reports if they were attached onto prev_full.debug
                pd = prev_full.get("debug") if isinstance(prev_full.get("debug"), dict) else {}
                if isinstance(pd, dict):
                    for k in ("fix2d53_legacy_remap", "fix2d54_materialize", "fix2d50_pmc_gate", "fix2d52_schema_bind"):
                        if k in pd:
                            _rep[k] = pd.get(k)
                web_context["debug"]["fix2d55_prev_lift"] = _rep

    except Exception:
        pass
        # never break evolution; this is a best-effort lift
        if isinstance(web_context, dict) and isinstance(web_context.get("debug"), dict):
            web_context["debug"].setdefault("fix2d55_prev_lift", {})
            if isinstance(web_context["debug"].get("fix2d55_prev_lift"), dict):
                web_context["debug"]["fix2d55_prev_lift"]["crashed"] = True

# =========================================================
# END FIX2D55


# =========================================================
# FIX2D56 — Force-fetch injected URLs in Evolution + auto prev-lift
# =========================================================
# A) Force-fetch injected URLs:
#   Evolution previously recorded injected URLs as admitted but never fetched them,
#   leaving inj_trace_v1.attempted empty. When injection is present, we now force
#   a fetch_web_context(... identity_only=False, force_scrape_extra_urls=True,
#   force_admit_extra_urls=True) call.
#
# B) Auto-enable prev_full lifting:
#   When injection is present, automatically enable FIX2D55 prev_lift (schema remap/bind/materialise)
#   so Analysis baseline can participate under schema keys without requiring extra flags.
#
# Enablement:
#   - Always-on when injection is present.
#   - Optional explicit opt-in via web_context['diag_fix2d56'].


def _fix2d56_has_injection(web_context: dict | None) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        if isinstance(web_context.get('extra_urls'), (list, tuple)) and len(web_context.get('extra_urls') or []) > 0:
            return True
        raw = web_context.get('diag_extra_urls_ui_raw') or web_context.get('extra_urls_ui_raw')
        return isinstance(raw, str) and raw.strip() != ''
    except Exception:
        return False


def _fix2d56_should_enable(web_context: dict | None) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        if web_context.get('diag_fix2d56') or web_context.get('enforce_schema_bound_pmc'):
            return True
        return _fix2d56_has_injection(web_context)
    except Exception:
        return False

# =========================================================
# END FIX2D56
# =========================================================

# =========================================================



# =========================================================
# FIX2D54 — FINAL VERSION STAMP OVERRIDE
# =========================================================
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
# =========================================================
# FIX2D57 — Analysis-side Schema Baseline Materialisation
# =========================================================
# Objective:
#   Ensure the Analysis baseline (Run A) already contains schema-keyed PMC entries with
#   numeric values, so Evolution can diff against Analysis without requiring Analysis injection.
#
# Problem observed:
#   Analysis emits legacy/text keys (e.g., "2025_global_ev_sales__unknown") while Evolution emits
#   schema keys (e.g., "global_ev_sales_ytd_2025__unit_sales"). No overlap => both_count=0.
#
# Solution:
#   After run_source_anchored_analysis returns its output object, force-run the schema pipeline:
#     FIX2D53 remap -> FIX2D52 bind -> FIX2D54 materialise -> FIX2D50 gate
#   using a temporary web_context with enforce_schema_bound_pmc=True.
#
# Safety:
#   - Only runs when metric_schema_frozen exists and PMC appears to contain legacy/unknown keys.
#   - Does not invent values; only rehomes extracted metrics onto schema keys.
#
# Output:
#   - debug.fix2d57_analysis_lift
# =========================================================

def _fix2d57_pmc_looks_legacy(pmc):
    if not isinstance(pmc, dict) or not pmc:
        return False
    for k, m in list(pmc.items())[:200]:
        ks = str(k)
        if ks.endswith("__unknown"):
            return True
        if re.match(r"^20\d{2}_.+__unknown$", ks):
            return True
        if isinstance(m, dict):
            d = m.get("dimension")
            if isinstance(d, str) and d.strip().lower() == "unknown":
                return True
    return False

def _fix2d57_force_schema_pipeline(output_obj, web_context):
    if not isinstance(output_obj, dict):
        return

    metric_schema_frozen = (
        output_obj.get("metric_schema_frozen") if isinstance(output_obj.get("metric_schema_frozen"), dict) else None
    ) or (
        output_obj.get("primary_response", {}).get("metric_schema_frozen") if isinstance(output_obj.get("primary_response"), dict) else None
    ) or (
        output_obj.get("results", {}).get("metric_schema_frozen") if isinstance(output_obj.get("results"), dict) else None
    ) or {}

    pmc = None
    if isinstance(output_obj.get("primary_metrics_canonical"), dict):
        pmc = output_obj["primary_metrics_canonical"]
    elif isinstance(output_obj.get("primary_response"), dict) and isinstance(output_obj["primary_response"].get("primary_metrics_canonical"), dict):
        pmc = output_obj["primary_response"]["primary_metrics_canonical"]
    elif isinstance(output_obj.get("results"), dict) and isinstance(output_obj["results"].get("primary_metrics_canonical"), dict):
        pmc = output_obj["results"]["primary_metrics_canonical"]

    output_obj.setdefault("debug", {})

    if not isinstance(metric_schema_frozen, dict) or not metric_schema_frozen:
        output_obj["debug"]["fix2d57_analysis_lift"] = {"enabled": True, "ran": False, "reason": "metric_schema_frozen_missing"}
        return
    if not isinstance(pmc, dict) or not pmc:
        output_obj["debug"]["fix2d57_analysis_lift"] = {"enabled": True, "ran": False, "reason": "pmc_missing"}
        return
    if not _fix2d57_pmc_looks_legacy(pmc):
        output_obj["debug"]["fix2d57_analysis_lift"] = {"enabled": True, "ran": False, "reason": "pmc_not_legacy"}
        return

    wc2 = dict(web_context) if isinstance(web_context, dict) else {}
    wc2["enforce_schema_bound_pmc"] = True
    wc2["diag_fix2d53_remap"] = True
    wc2["diag_fix2d52_bind"] = True
    wc2["diag_fix2d54_materialize"] = True
    wc2["diag_fix2d50_gate"] = True

    # Apply in order (in-place on output_obj)
    _fix2d53_try_remap_output_obj(output_obj, wc2)
    _fix2d52_try_bind_output_obj(output_obj, wc2)
    _fix2d54_try_materialize_output_obj(output_obj, wc2)
    _fix2d50_try_gate_output_obj(output_obj, wc2)

    rep = {"enabled": True, "ran": True}
    dbg = output_obj.get("debug") if isinstance(output_obj.get("debug"), dict) else {}
    if isinstance(dbg, dict):
        for k in ("fix2d53_legacy_remap", "fix2d52_schema_bind", "fix2d54_materialize", "fix2d50_pmc_gate"):
            if k in dbg:
                rep[k] = dbg.get(k)
    output_obj["debug"]["fix2d57_analysis_lift"] = rep

# =========================================================
# END FIX2D57
# =========================================================


# =========================================================
# FIX2D57 — FINAL VERSION STAMP OVERRIDE
# =========================================================
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK

# =========================================================
# FIX2D57B — FINAL VERSION STAMP OVERRIDE
# =========================================================
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK


# =========================
# FINAL VERSION STAMP: FIX2D65 (last-wins)
# =========================
try:
    globals()["CODE_VERSION"] = "FIX2D65"
except Exception:
    pass

# =====================================================================
# PATCH FIX2D65 (AUTHORITY TAKEOVER): Canonical Identity Spine V1 becomes the only authority
# - Rewire schema-first identity resolution to use canonical_identity_spine.resolve_key_v1
# - Enforce "no canonical outside spine" at the Analysis schema-bound commit split
# - Harden Evolution current selection by pruning yearlike candidates even when unit evidence is WINDOW_BACKFILL
#   (prevents year headings from ever being selected for unit/count metrics)
# - Deterministic, auditable: stamp authority + trace blocks on every resolved metric
# =====================================================================

# Patch tracker entry
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D65",
        "date": "2026-01-19",
        "summary": "Authority takeover: route schema-first identity resolution through canonical_identity_spine (single authority), enforce no-canonical-outside-spine at Analysis commit split, and prune yearlike candidates (immune to WINDOW_BACKFILL) before Evolution selector selection.",
        "files": ["canonical_identity_spine.py", "FIX2D65_full_codebase.py"],
        "supersedes": ["FIX2D64"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# Ensure spine module is importable
try:
    import canonical_identity_spine as _cis
    _FIX2D65_SPINE_OK = True
except Exception:
    pass
    _cis = None
    _FIX2D65_SPINE_OK = False


def _fix2d65_build_schema_index_v1(metric_schema: dict) -> dict:
    """Build schema_index for spine resolver: maps <prefix>__<dim> -> canonical_key.

    Also builds a limited bare-token fallback map only when unique per (metric_token, dim).
    """
    idx = {}
    # track bare token collisions
    bare_counts = {}
    bare_last = {}

    if not isinstance(metric_schema, dict) or not metric_schema:
        return idx

    for skey, spec in metric_schema.items():
        if not isinstance(skey, str) or '__' not in skey:
            continue
        prefix, dim = skey.split('__', 1)
        dim = (dim or '').strip().lower()
        k = f"{prefix}__{dim}" if dim else skey
        idx[k] = skey

        # compute bare token (strip trailing time_scope if present)
        try:
            # Use spine's metric-token normalizer (removes time tokens) if available
            if _FIX2D65_SPINE_OK:
                tmp = _cis.build_identity_tuple_v1({"metric_token": prefix, "dimension": dim, "unit_family": (spec or {}).get("unit_family") or ""}, context={})
                bare = f"{tmp.metric_token}__{dim}" if dim else tmp.metric_token
            else:
                bare = f"{prefix.split('_')[0]}__{dim}" if dim else prefix
        except Exception:
            pass
            bare = ''

        if bare:
            bare_counts[bare] = int(bare_counts.get(bare, 0)) + 1
            bare_last[bare] = skey

    # Add bare fallbacks only if unique (prevents time-scope misbinding)
    for bare, cnt in bare_counts.items():
        if cnt == 1:
            idx.setdefault(bare, bare_last.get(bare))

    return idx








def _fix2d65_prune_yearlike_candidates_for_unit_metrics(candidates: list, canonical_key: str) -> tuple:
    """Remove yearlike candidates for unit/count metrics when unit evidence is NONE/WINDOW_BACKFILL.

    Returns (pruned_candidates, debug_info).
    """
    cands = [c for c in (candidates or []) if isinstance(c, dict)]
    dim = ''
    try:
        if isinstance(canonical_key, str) and '__' in canonical_key:
            dim = canonical_key.split('__', 1)[1].strip().lower()
    except Exception:
        pass
        dim = ''

    is_unit = dim in ('unit_sales', 'unit_count', 'count', 'units', 'units_sold')
    if not (_FIX2D65_SPINE_OK and is_unit):
        return cands, {"applied": False, "rejected": 0, "kept": len(cands), "dimension": dim}

    rejected = 0
    pruned = []
    for c in cands:
        v = c.get('value_norm')
        if v is None:
            v = c.get('value')
        if _cis.is_yearlike_value(v):
            strength = _cis.classify_unit_evidence_strength(c)
            if strength in ('NONE', 'WINDOW_BACKFILL'):
                rejected += 1
                continue
        pruned.append(c)

    return pruned, {"applied": True, "rejected": int(rejected), "kept": int(len(pruned)), "dimension": dim}




# Bind overrides into globals (last-wins)
try:
    globals()['resolve_canonical_identity_v1'] = resolve_canonical_identity_v1
    globals()['rekey_metrics_via_identity_resolver_v1'] = rekey_metrics_via_identity_resolver_v1
    globals()['_fix2d60_split_schema_bound_only'] = _fix2d60_split_schema_bound_only
except Exception:
    pass

# Final, authoritative version stamp (last-wins)
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK

# =====================================================================
# END PATCH FIX2D65
# =====================================================================


# =====================================================================
# PATCH FIX2D65B (FINAL OVERRIDE): version stamp + patch tracker
# =====================================================================
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass

try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D65B",
        "date": "2026-01-19",
        "summary": "Force canonical pipeline materialisation when injected URLs exist (seed schema via deterministic extensions so FIX2D31 schema-authority rebuild can run even for narrative queries).",
        "files": ["FIX2D65B_full_codebase.py"],
        "supersedes": ["FIX2D65A"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass
# =====================================================================


# =====================================================================
# PATCH FIX2D65C (FINAL OVERRIDE): contract restoration for analysis->evolution diff
# =====================================================================
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass

try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D65C",
        "date": "2026-01-19",
        "summary": "Restore analysis->evolution diff contract: broaden injected URL detection (ui_raw + legacy keys) so schema seeding and FIX2D31 schema-authority rebuild reliably run when injection is used; bump version.",
        "files": ["FIX2D65C_full_codebase.py"],
        "supersedes": ["FIX2D65B"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass
# =====================================================================


# =====================================================================
# PATCH FIX2D65D (FINAL OVERRIDE): version stamp + patch tracker
# =====================================================================
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass

try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D65D",
        "date": "2026-01-19",
        "summary": "Restore analysis->evolution diff contract by always serializing/seeding metric_schema_frozen (deterministic schema extensions), so Evolution schema-only rebuild has a stable keyspace even when LLM emits no primary_metrics; bump version.",
        "files": ["FIX2D65D_full_codebase.py"],
        "supersedes": ["FIX2D65C"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass
# =====================================================================


# =====================================================================
# PATCH FIX2D66 (FINAL OVERRIDE): version stamp + patch tracker
# =====================================================================
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass

try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D66",
        "date": "2026-01-19",
        "summary": "Deterministic injected-URL admission: promote UI raw/diag injection fields into web_context.extra_urls; synthesize diag_injected_urls when missing; ensure inj_trace_v1 and inj_diag reflect injected URLs in snapshot pool and hash inputs (auditable).",
        "files": ["FIX2D66_full_codebase.py"],
        "supersedes": ["FIX2D65D"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass
# =====================================================================

# =====================================================================
# PATCH TRACKER ENTRY (ADDITIVE): FIX2D66H
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D66H",
        "date": "2026-01-19",
        "summary": "Fix Google Sheets history save return semantics: add_to_history() now returns True on successful Sheets append and False on failure (previously fell through as None, triggering spurious 'Saved to session only' warning). Keeps session fallback and captures last Sheets error for diagnostics.",
        "files": ["FIX2D66H_full_codebase.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# =====================================================================
# PATCH TRACKER ENTRY (ADDITIVE): FIX2D67
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D67",
        "date": "2026-01-19",
        "summary": "Fix injected numeric extraction missing-link: fetch_web_context() now calls numeric extractor with correct parameter name (source_url vs url), preventing silent TypeError and empty extracted_numbers; restores injected HTML numbers into snapshot pools feeding schema-only rebuild.",
        "files": ["FIX2D67_full_codebase.py"],
        "supersedes": ["FIX2D66H"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass



# =====================================================================
# PATCH FIX2D68 PATCH TRACKER ENTRY (ADDITIVE)
# =====================================================================




try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2",
        "date": "2026-01-15",
        "summary": "Anchor-fill current_metrics for diff/display when schema_frozen is misaligned; add rebuild-fn name fallbacks to prevent fn_missing.",
        "files": ["FIX2D2.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass





# =====================================================================
# PATCH TRACKER ENTRY (ADDITIVE)
# =====================================================================
# =====================================================================
# PATCH TRACKER ENTRY: FIX2D2N (ADDITIVE)
# - Baseline-keyed current mapping for Analysis→Evolution diffing
# - When current canonical keys do not match baseline keys, synthesize current
#   metric objects for each baseline ckey using injected-first, unit-family-guarded
#   inference from extracted_numbers pools.
# - This closes Analysis/Evolution key parity gaps for the diff join without
#   changing canonical key generation.
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2N",
        "date": "2026-01-16",
        "summary": "Baseline-keyed current mapping: for each Analysis baseline canonical key, synthesize a current metric via injected-first, unit-family-guarded inference from extracted_numbers pools when Evolution canonical keys diverge; enables deterministic Analysis→Evolution diff joins even under key parity gaps.",
        "files": ["FIX2D2N.py"],
        "supersedes": ["FIX2D2M"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# =====================================================================
# PATCH TRACKER ENTRY: FIX2D2O (ADDITIVE)
# - Persist baseline-keyed augmented current canonical map into the
#   Evolution/current response under primary_metrics_canonical_for_diff
#   and mirror into primary_metrics_canonical for demo parity.
# - This makes Evolution JSON inspection show Analysis keys as the
#   effective current authority for diffing, while keeping extras possible.
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2O",
        "date": "2026-01-16",
        "summary": "Persist baseline-keyed current mapping for diffing: when baseline keys are synthesized into cur_can, expose them as primary_metrics_canonical_for_diff and mirror into primary_metrics_canonical so Evolution output keys align with Analysis for the diff demo.",
        "files": ["FIX2D2O.py"],
        "supersedes": ["FIX2D2N"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# =====================================================================
# PATCH TRACKER ENTRIES (CLEAN): FIX2D2Q / FIX2D2R / FIX2D2S
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2Q",
        "date": "2026-01-16",
        "summary": "Baseline-aligned current selection for diffing: injected-first with optional base fallback; stamps provenance fields (source_type, selection_mode) to prevent confusion while preserving union pool behavior.",
        "files": ["FIX2D2Q.py"],
        "supersedes": ["FIX2D2O"],
    })

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2R",
        "date": "2026-01-16",
        "summary": "Rebuild parity guard: prevent schema-only rebuild paths from committing bare-year tokens when a unit-qualified sibling candidate exists in the same snippet; improves Analysis/Evolution parity for injected content.",
        "files": ["FIX2D2R.py"],
        "supersedes": ["FIX2D2Q"],
    })

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2S",
        "date": "2026-01-16",
        "summary": "Schema-only rebuild hardening: when non-year candidates exist for a schema key, skip bare-year tokens during winner selection (down-rank/skip) and record diagnostics; reduces year-token pollution before downstream year-blocking.",
        "files": ["FIX2D2S.py"],
        "supersedes": ["FIX2D2R"],
    })

    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# =========================
# ============================================================
# PATCH TRACKER V1 (ADD): FIX2D42
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D42",
        "date": "2026-01-17",
        "summary": "Serialize/promote baseline_schema_metrics_v1 into Analysis primary_response/results so Evolution diff can consume it; extend nested results promotion to mirror baseline_schema_metrics_v1.",
        "files": ["FIX2D42.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# FINAL VERSION OVERRIDE
# =========================
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass

# =====================================================================
# PATCH FIX2D2T (ADDITIVE): Baseline->Current projection for Diff Panel V2
# Why:
# - We have proven Evolution can extract/commit injected values, but diffing
#   still shows 0 increased/decreased/unchanged when baseline rows never get
#   current_value_norm populated.
# - This patch makes the final "handoff" explicit: for each baseline ckey
#   (prev row), if cur_response contains a canonical metric object for that
#   same ckey (prefer primary_metrics_canonical_for_diff), project it into
#   metric_changes[*].current_value/_norm and mark comparable.
# Safety:
# - Render/diff-layer only. No changes to extraction, hashing, anchors,
#   canonical key generation, or snapshot pools.
# - Only fills CURRENT when the metric object exists under the SAME ckey.
# =====================================================================

try:
    diff_metrics_by_name_FIX2D2T_BASE = diff_metrics_by_name  # type: ignore
except Exception:
    pass
    diff_metrics_by_name_FIX2D2T_BASE = None  # type: ignore


def _fix2d2t_s(x):
    try:
        return "" if x is None else str(x)
    except Exception:
        return ""


def _fix2d2t_f(x):
    try:
        if x is None:
            return None
        if isinstance(x, (int, float)):
            return float(x)
        s = _fix2d2t_s(x).strip().replace(",", "")
        if not s:
            return None
        return float(s)
    except Exception:
        return None


def _fix2d2t_get_cur_maps(cur_response: dict):
    """Return (cur_for_diff, cur_primary) dicts."""
    cur_for_diff = {}
    cur_primary = {}
    try:
        if isinstance(cur_response, dict):
            cfd = cur_response.get("primary_metrics_canonical_for_diff")
            if not isinstance(cfd, dict) and isinstance(cur_response.get("results"), dict):
                cfd = cur_response["results"].get("primary_metrics_canonical_for_diff")
            if isinstance(cfd, dict):
                cur_for_diff = cfd
            pmc = cur_response.get("primary_metrics_canonical")
            if not isinstance(pmc, dict) and isinstance(cur_response.get("results"), dict):
                pmc = cur_response["results"].get("primary_metrics_canonical")
            if isinstance(pmc, dict):
                cur_primary = pmc
    except Exception:
        return cur_for_diff, cur_primary


def diff_metrics_by_name_FIX2D2T_PROJECT_BASELINE_CURRENT(prev_response: dict, cur_response: dict):
    """Wrapper over current diff that ensures baseline rows receive current values when available under same ckey."""
    if not callable(diff_metrics_by_name_FIX2D2T_BASE):
        return ([], 0, 0, 0, 0)

    metric_changes, unchanged, increased, decreased, found = diff_metrics_by_name_FIX2D2T_BASE(prev_response, cur_response)

    cur_for_diff, cur_primary = _fix2d2t_get_cur_maps(cur_response if isinstance(cur_response, dict) else {})

    proj_applied = 0
    proj_filled = 0
    proj_source_injected = 0
    samples = []

    def _is_missing(row: dict):
        try:
            cvn = row.get("current_value_norm")
            if cvn is None:
                cvn = row.get("cur_value_norm")
            if cvn is not None:
                return False
            cv = row.get("current_value")
            cvs = _fix2d2t_s(cv).strip().upper()
            return (not cvs) or (cvs == "N/A")
        except Exception:
            return True

    def _build_display(vn, unit):
        try:
            if vn is None:
                return None
            if unit:
                return f"{vn} {unit}".strip()
            return _fix2d2t_s(vn)
        except Exception:
            return _fix2d2t_s(vn)

    if isinstance(metric_changes, list):
        for row in metric_changes:
            if not isinstance(row, dict):
                continue
            prev_ckey = _fix2d2t_s(row.get("canonical_key") or row.get("ckey") or "").strip()
            if not prev_ckey:
                continue

            # We only project into baseline rows (i.e., rows with prev_value_norm present)
            pv = row.get("prev_value_norm")
            if pv is None:
                pv = row.get("previous_value_norm")
            if pv is None:
                continue

            if not _is_missing(row):
                continue

            proj_applied += 1

            # Prefer baseline-keyed current mapping, then fallback to primary_metrics_canonical.
            cm = cur_for_diff.get(prev_ckey) if isinstance(cur_for_diff, dict) else None
            used_map = "primary_metrics_canonical_for_diff"
            if not isinstance(cm, dict) or not cm:
                cm = cur_primary.get(prev_ckey) if isinstance(cur_primary, dict) else None
                used_map = "primary_metrics_canonical"

            if not isinstance(cm, dict) or not cm:
                continue

            vn = cm.get("value_norm") if cm.get("value_norm") is not None else cm.get("value")
            vn = _fix2d2t_f(vn)
            if vn is None:
                continue
            unit = _fix2d2t_s(cm.get("unit_tag") or cm.get("unit") or cm.get("unit_cmp") or "").strip() or None
            src = _fix2d2t_s(cm.get("source_url") or "").strip()

            # Fill row
            row["current_value_norm"] = vn
            row["cur_value_norm"] = vn
            row["current_value"] = _build_display(vn, unit) or row.get("current_value")
            if unit:
                row["current_unit"] = unit
                row["cur_unit_cmp"] = unit

            # Diagnostics
            row.setdefault("diag", {})
            if isinstance(row.get("diag"), dict):
                row["diag"].setdefault("fix2d2t_baseline_projection_v1", {})
                if isinstance(row["diag"].get("fix2d2t_baseline_projection_v1"), dict):
                    row["diag"]["fix2d2t_baseline_projection_v1"].update({
                        "applied": True,
                        "used_map": used_map,
                        "resolved_cur_ckey": prev_ckey,
                        "current_source_url": src or None,
                    })

            # Provenance labels (align with FIX2D2Q concept)
            if src:
                if "github.io" in src or "injection" in src:
                    row["current_source_type_fix2d2q"] = "injected"
                    row["current_selection_mode_fix2d2q"] = "baseline_projection_injected"
                    proj_source_injected += 1
                else:
                    row["current_source_type_fix2d2q"] = row.get("current_source_type_fix2d2q") or "base"
                    row["current_selection_mode_fix2d2q"] = row.get("current_selection_mode_fix2d2q") or "baseline_projection_base"

            # Recompute comparability and change_type
            try:
                pv_num = _fix2d2t_f(pv)
                if pv_num is not None and vn is not None:
                    row["baseline_is_comparable"] = True
                    if abs(vn - pv_num) < 1e-9:
                        row["baseline_change_type"] = "unchanged"
                    elif vn > pv_num:
                        row["baseline_change_type"] = "increased"
                    else:
                        row["baseline_change_type"] = "decreased"
            except Exception:
                pass

            proj_filled += 1
            if len(samples) < 8:
                samples.append({
                    "ckey": prev_ckey,
                    "used_map": used_map,
                    "pv": _fix2d2t_f(pv),
                    "cv": vn,
                    "src": src,
                })

    # Recompute counters from rows (authoritative)
    try:
        u = i = d = 0
        f = 0
        if isinstance(metric_changes, list):
            for r in metric_changes:
                if not isinstance(r, dict):
                    continue
                ct = _fix2d2t_s(r.get("baseline_change_type") or r.get("change_type") or "").strip().lower()
                if ct in ("unchanged",):
                    u += 1; f += 1
                elif ct in ("increased",):
                    i += 1; f += 1
                elif ct in ("decreased",):
                    d += 1; f += 1
        unchanged, increased, decreased, found = u, i, d, f
    except Exception:
        pass

    # Attach top-level debug
    try:
        if isinstance(cur_response, dict):
            cur_response.setdefault("debug", {})
            if isinstance(cur_response.get("debug"), dict):
                cur_response["debug"]["fix2d2t_baseline_projection_summary_v1"] = {
                    "applied_to_missing_rows": int(proj_applied),
                    "filled_rows": int(proj_filled),
                    "filled_from_injected": int(proj_source_injected),
                    "samples": samples,
                }
    except Exception:
        return metric_changes, unchanged, increased, decreased, found


# Wire wrapper
try:
    if callable(diff_metrics_by_name_FIX2D2T_PROJECT_BASELINE_CURRENT):
        diff_metrics_by_name = diff_metrics_by_name_FIX2D2T_PROJECT_BASELINE_CURRENT  # type: ignore
except Exception:
    pass

# Patch tracker + version bump
try:
    if isinstance(globals().get('PATCH_TRACKER_V1'), list):
        PATCH_TRACKER_V1.append({
            "patch_id": "FIX2D2T",
            "summary": "Add explicit baseline->current projection in diff layer: if baseline row is missing CURRENT but cur_response has same canonical_key in primary_metrics_canonical_for_diff/primary_metrics_canonical, project into metric_changes current_value/_norm and recompute diff counters; attach debug summary.",
            "ts": "2026-01-16",
        })

    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2U",
        "date": "2026-01-17",
        "summary": "Introduce shared semantic eligibility gate (local-snippet required tokens) and apply it across Analysis selector and Evolution schema-only rebuild paths to prevent cross-metric pollution (e.g., China sales value mapping to chargers 2040).",
        "files": ["FIX2D2U.py"],
        "supersedes": ["FIX2D2T"],
    })
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2W",
        "date": "2026-01-17",
        "summary": "Fix parity leak: ensure schema_only_rebuild commit-time semantic gate is always active (avoid NameError when _FIX2D2U_ENABLE defined later) and fix year-token extraction regex for required-year checks; bump CODE_VERSION.",
        "files": ["FIX2D2W.py"],
        "supersedes": ["FIX2D2V"],
    })

except Exception:
    pass

try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass

# =====================================================================
# END PATCH FIX2D2T
# =====================================================================

# =====================================================================

# =====================================================================

# =====================================================================
# PATCH FIX2D2X (PARITY): Reuse Analysis selector for Evolution baseline-key current
# ---------------------------------------------------------------------
# Rationale:
#   - Analysis has the authoritative semantic eligibility gates.
#   - Evolution schema_only_rebuild is a legacy shortcut that can mis-assign values.
#   - For Analysis→Evolution diffing, Evolution must populate CURRENT for the Analysis
#     keyspace using the same selector/gates, differing only in source preference
#     (injected-first).
# Implementation (additive override):
#   - Provide a new rebuild_metrics_from_snapshots_schema_only_fix17 that:
#       * derives a schema spec per baseline key (adds keyword hints from key/name)
#       * builds candidates from baseline_sources_cache extracted_numbers
#       * runs a strict injected-first two-pass selection via _analysis_canonical_final_selector_v1
#         with preferred-source locking disabled
#       * commits the chosen candidate under the same canonical_key
#   - This makes Evolution and Analysis share the same semantic gates for this step.
# ---------------------------------------------------------------------
# Supersedes (functionally): FIX2D2U/FIX2D2V/FIX2D2W schema_only_rebuild gating patches.
# =====================================================================

try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2X",
        "date": "2026-01-17",
        "summary": "Parity patch: replace Evolution schema-only slot filling for baseline-key current with Analysis authoritative selector (_analysis_canonical_final_selector_v1) using injected-first two-pass selection and synthesized keyword hints from canonical_key/name; prevents cross-metric misassignment (e.g., China sales -> chargers 2040) and aligns gating with Analysis.",
        "files": ["FIX2D2X.py"],
        "supersedes": ["FIX2D2W", "FIX2D2V", "FIX2D2U"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


def _fix2d2x_parse_injected_urls(web_context: dict) -> list:
    """Best-effort extraction of injected/extra URLs from web_context."""
    urls = []
    try:
        wc = web_context if isinstance(web_context, dict) else {}
        for k in ("diag_extra_urls", "extra_urls", "injected_urls"):
            v = wc.get(k)
            if isinstance(v, list):
                for u in v:
                    if isinstance(u, str) and u.strip():
                        urls.append(u.strip())
        # raw UI fields (string) used in earlier patches
        for k in ("diag_extra_urls_ui_raw", "extra_urls_ui_raw"):
            raw = wc.get(k)
            if isinstance(raw, str) and raw.strip():
                # split on whitespace / commas
                for part in re.split(r"[\s,]+", raw.strip()):
                    if part.startswith("http"):
                        urls.append(part)
    except Exception:
        pass

    # Stable de-dupe
    out = []
    seen = set()
    for u in urls:
        uu = str(u or "").strip()
        if not uu or uu in seen:
            continue
        seen.add(uu)
        out.append(uu)
    return out


def _fix2d2x_keywords_from_key_and_name(canonical_key: str, name: str) -> list:
    """Synthesize keyword hints when schema_frozen came from baseline (often lacks keywords)."""
    toks = []
    try:
        ck = str(canonical_key or "")
        nm = str(name or "")
        # split key on underscores
        for t in ck.replace("__", "_").split("_"):
            t = t.strip().lower()
            if not t:
                continue
            # drop pure years and very short tokens
            if re.fullmatch(r"(19\d{2}|20\d{2})", t):
                continue
            if len(t) <= 2:
                continue
            toks.append(t)
        # add name tokens
        for t in re.split(r"[^a-zA-Z0-9]+", nm.lower()):
            if not t:
                continue
            if re.fullmatch(r"(19\d{2}|20\d{2})", t):
                continue
            if len(t) <= 2:
                continue
            toks.append(t)
    except Exception:
        pass
        toks = []

    # small normalization + de-dupe
    out = []
    seen = set()
    stop = {"global", "world", "worldwide", "ev", "electric", "vehicle", "vehicles", "unknown"}
    for t in toks:
        if t in stop:
            continue
        if t in seen:
            continue
        seen.add(t)
        out.append(t)
    return out[:24]


def _fix2d2x_required_years_from_key(canonical_key: str) -> list:
    ys = []
    try:
        for m in re.findall(r"\b(19\d{2}|20\d{2})\b", str(canonical_key or "")):
            ys.append(m)
    except Exception:
        pass
        ys = []
    # de-dupe preserve
    out = []
    seen = set()
    for y in ys:
        if y in seen:
            continue
        seen.add(y)
        out.append(y)
    return out


def _fix2d2x_local_text_for_candidate(c: dict) -> str:
    try:
        return " ".join([
            str(c.get("context_snippet") or ""),
            str(c.get("raw") or ""),
            str(c.get("unit") or c.get("unit_tag") or ""),
        ]).lower()
    except Exception:
        return ""


def _fix2d2x_filter_candidates_for_key(canonical_key: str, spec: dict, candidates: list) -> list:
    """Pre-filter candidates for a schema key.

    FIX2D70: if strict year/keyword gating would eliminate all candidates, apply a controlled relaxation:
    - year tolerance (Y±1 for single-year keys; any overlap for ranges)
    - keyword requirement is relaxed only if strict pass yields zero

    Diagnostics are recorded into spec["debug_meta"]["fix2d70_prefilter"].
    """
    out = []
    req_years = _fix2d2x_required_years_from_key(canonical_key)
    kws = spec.get("keywords") or []
    kws = [str(k).lower() for k in kws if isinstance(k, str) and k.strip()]

    rej_year = 0
    rej_kw = 0
    rej_empty = 0

    # strict pass (legacy semantics)
    for c in candidates or []:
        if not isinstance(c, dict):
            continue
        lt = _fix2d2x_local_text_for_candidate(c)
        if not lt:
            rej_empty += 1
            continue
        ok_year = True
        if req_years:
            ok_year = all((y in lt) for y in req_years)
        if not ok_year:
            rej_year += 1
            continue
        if kws:
            if not _fix2d70_keyword_hit(lt, kws):
                rej_kw += 1
                continue
        out.append(c)

    if out:
        try:
            spec.setdefault("debug_meta", {})["fix2d70_prefilter"] = {
                "mode": "strict",
                "req_years": req_years,
                "kws_n": len(kws),
                "in_n": int(len(candidates or [])),
                "out_n": int(len(out)),
                "rej_year": int(rej_year),
                "rej_kw": int(rej_kw),
                "rej_empty": int(rej_empty),
            }
        except Exception:
            pass
        return out

    # relaxed pass (only if strict eliminated everything)
    out2 = []
    rej_year2 = 0
    rej_empty2 = 0
    for c in candidates or []:
        if not isinstance(c, dict):
            continue
        lt = _fix2d2x_local_text_for_candidate(c)
        if not lt:
            rej_empty2 += 1
            continue
        if not _fix2d70_ok_year_relaxed(lt, req_years):
            rej_year2 += 1
            continue
        # keyword gate intentionally removed in relaxed mode
        out2.append(c)

    try:
        spec.setdefault("debug_meta", {})["fix2d70_prefilter"] = {
            "mode": "relaxed" if out2 else "relaxed_empty",
            "req_years": req_years,
            "kws_n": len(kws),
            "in_n": int(len(candidates or [])),
            "out_n": int(len(out2)),
            "rej_year": int(rej_year),
            "rej_kw": int(rej_kw),
            "rej_empty": int(rej_empty),
            "rej_year_relaxed": int(rej_year2),
            "rej_empty_relaxed": int(rej_empty2),
        }
    except Exception:
        pass

    return out2




# ---------------------------------------------------------------------
# OVERRIDE: schema-only rebuild FIX17
# ---------------------------------------------------------------------

def rebuild_metrics_from_snapshots_schema_only_fix17(prev_response: dict, baseline_sources_cache, web_context=None) -> dict:  # noqa: F811
    """Parity rebuild: populate CURRENT for baseline keyspace using Analysis selector."""
    if not isinstance(prev_response, dict):
        return {}

    # Schema keyspace
    schema_frozen = (
        prev_response.get("metric_schema_frozen")
        or (prev_response.get("results") or {}).get("metric_schema_frozen")
        or (prev_response.get("primary_response") or {}).get("metric_schema_frozen")
        or {}
    )
    if not isinstance(schema_frozen, dict) or not schema_frozen:
        return {}

    # Candidate pool from baseline_sources_cache
    bsc = baseline_sources_cache
    if not isinstance(bsc, list):
        # try prev_response locations
        bsc = (prev_response.get("results") or {}).get("baseline_sources_cache")
    if not isinstance(bsc, list):
        bsc = []

    all_candidates = []
    for item in bsc:
        if not isinstance(item, dict):
            continue
        src = item.get("source_url") or item.get("url") or ""
        nums = item.get("extracted_numbers")
        if not isinstance(nums, list):
            # sometimes nested in scraped_meta
            sm = item.get("scraped_meta")
            if isinstance(sm, dict):
                nums = sm.get("extracted_numbers")
        if not isinstance(nums, list):
            continue
        for c in nums:
            if not isinstance(c, dict):
                continue
            cc = dict(c)
            if src and not cc.get("source_url"):
                cc["source_url"] = src
            all_candidates.append(cc)

    injected_urls = _fix2d2x_parse_injected_urls(web_context)

    rebuilt = {}
    debug = {}
    reject_counts = {}
    filled = 0

    for ck, spec0 in schema_frozen.items():
        if not isinstance(ck, str) or not ck:
            continue
        spec = spec0 if isinstance(spec0, dict) else {}
        best, meta = _fix2d2x_select_current_for_key(ck, spec, all_candidates, injected_urls)
        if not isinstance(best, dict):
            # track block reasons (selector meta)
            try:
                br = str((meta or {}).get("blocked_reason") or "")
                if br:
                    reject_counts[br] = int(reject_counts.get(br, 0)) + 1
            except Exception:
                pass
            continue

        # Commit minimal metric object
        try:
            m = {
                "canonical_key": ck,
                "value": best.get("value"),
                "value_norm": best.get("value_norm"),
                "unit": best.get("unit") or best.get("unit_tag") or best.get("unit_cmp"),
                "unit_tag": best.get("unit_tag") or best.get("unit") or best.get("unit_cmp"),
                "unit_family": best.get("unit_family") or spec.get("unit_family") or "",
                "source_url": best.get("source_url") or "",
                "raw": best.get("raw") or "",
                "context_snippet": best.get("context_snippet") or "",
                "method": "analysis_selector_shared_fix2d2x",
                "selection_meta": meta or {},
            }
            rebuilt[ck] = m
            filled += 1
        except Exception:
            pass
            continue

    # Attach debug to web_context if present
    try:
        debug["fix2d2x_selector_shared_summary_v1"] = {
            "filled": int(filled),
            "schema_keys": int(len(schema_frozen)),
            "candidates_total": int(len(all_candidates)),
            "injected_urls": injected_urls,
            "reject_counts": reject_counts,
        }
        if isinstance(web_context, dict):
            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].update(debug)
    except Exception:
        return rebuilt

# =====================================================================
# END PATCH FIX2D2X
# =====================================================================

# =====================================================================
# PATCH FIX2D2Y (PARITY HARDWIRE)
# Objective:
# - Ensure Evolution's current-metric rebuild (including fix41afc19 override path)
#   uses the SAME authoritative Analysis selector for baseline-keyed diffing.
# - This addresses the observed issue where fix41afc19 applied a different rebuild
#   function (rebuild_metrics_from_snapshots_analysis_canonical_v1) which produced
#   a disjoint keyset (overlap_count==0), preventing diff activation.
#
# What:
# - Override `rebuild_metrics_from_snapshots_analysis_canonical_v1` so it performs a
#   baseline-keyed rebuild using the shared selector helper from FIX2D2X.
# - This makes the fix41afc19 path call the parity-correct rebuild automatically.
#
# Safety:
# - Additive override only; does not modify hashing, snapshot attach, or fastpath.
# - Only affects display/diff "current" semantics.
# =====================================================================

# Version stamp (ensure last-wins in monolithic file)
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
# Patch tracker entry
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D2Y",
        "date": "2026-01-17",
        "summary": "Hardwire Evolution rebuild_metrics_from_snapshots_analysis_canonical_v1 to use the shared Analysis final selector for baseline-keyed diff current metrics (fix41afc19 parity); eliminates disjoint keyset that blocks diff activation.",
        "files": ["FIX2D2Y.py"],
        "supersedes": ["FIX2D2X"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


def rebuild_metrics_from_snapshots_analysis_canonical_v1(prev_response, snapshot_pool, web_context=None):
    """FIX2D2Y override: baseline-keyed current rebuild using Analysis selector.

    Returns a dict keyed by baseline canonical keys (from prev_response.primary_metrics_canonical)
    so Analysis↔Evolution overlap can be non-zero and diffing can activate.
    """
    # Resolve baseline schema (keys + minimal specs) from Analysis prev_response
    schema_keys = []
    schema_specs = {}
    try:
        _pmc = None
        if isinstance(prev_response, dict):
            _pmc = prev_response.get("primary_metrics_canonical")
            if not isinstance(_pmc, dict):
                _pmc = prev_response.get("results", {}).get("primary_metrics_canonical")
        if isinstance(_pmc, dict) and _pmc:
            for ck, mv in _pmc.items():
                if not isinstance(ck, str):
                    continue
                schema_keys.append(ck)
                mv = mv if isinstance(mv, dict) else {}
                schema_specs[ck] = {
                    "canonical_key": ck,
                    "unit_family": mv.get("unit_family") or "",
                    "unit_tag": mv.get("unit_tag") or mv.get("unit") or mv.get("unit_cmp") or "",
                    "display_name": mv.get("display_name") or mv.get("metric_name") or ck,
                }
    except Exception:
        pass
        schema_keys = []
        schema_specs = {}

    if not schema_keys:
        return {}

    # Build candidate universe from snapshot_pool extracted_numbers
    all_candidates = []
    try:
        if isinstance(snapshot_pool, list):
            for src in snapshot_pool:
                if not isinstance(src, dict):
                    continue
                url = src.get("url") or src.get("source_url") or ""
                nums = src.get("extracted_numbers")
                if not isinstance(nums, list):
                    continue
                for n in nums:
                    if not isinstance(n, dict):
                        continue
                    cand = dict(n)
                    cand["source_url"] = cand.get("source_url") or url
                    all_candidates.append(cand)
    except Exception:
        pass

    # Determine injected URLs (if any)
    injected_urls = []
    try:
        if isinstance(web_context, dict):
            inj = (
                web_context.get("diag_extra_urls")
                or web_context.get("extra_urls")
                or web_context.get("diag_extra_urls_final")
                or []
            )
            if isinstance(inj, list):
                injected_urls = [u for u in inj if isinstance(u, str) and u.strip()]
    except Exception:
        pass
        injected_urls = []

    # Select best candidate per baseline key using the shared selector helper
    rebuilt = {}
    debug = {
        "fix2d2y_analysis_selector_rebuild_v1": {
            "baseline_keys": int(len(schema_keys)),
            "candidates_total": int(len(all_candidates)),
            "injected_urls": injected_urls,
            "filled": 0,
            "reject_counts": {},
        }
    }

    filled = 0
    reject_counts = {}

    for ck in schema_keys:
        spec = schema_specs.get(ck) or {"canonical_key": ck}

        best = None
        meta = None
        try:
            # Reuse FIX2D2X selector if present
            if callable(globals().get("_fix2d2x_select_best")):
                best, meta = globals()["_fix2d2x_select_best"](ck, spec, all_candidates, injected_urls=injected_urls)
            else:
                # Fallback: no selection helper available
                best, meta = None, {"error": "missing_fix2d2x_select_best"}
        except Exception as e:
            best, meta = None, {"error": str(e)}

        if not isinstance(best, dict):
            # Count rejects if available
            try:
                rsn = None
                if isinstance(meta, dict):
                    rsn = meta.get("reject_reason") or meta.get("reason")
                if isinstance(rsn, str) and rsn:
                    reject_counts[rsn] = int(reject_counts.get(rsn, 0)) + 1
            except Exception:
                pass
            continue

        try:
            m = {
                "canonical_key": ck,
                "value": best.get("value"),
                "value_norm": best.get("value_norm"),
                "unit": best.get("unit") or best.get("unit_tag") or best.get("unit_cmp"),
                "unit_tag": best.get("unit_tag") or best.get("unit") or best.get("unit_cmp"),
                "unit_family": best.get("unit_family") or spec.get("unit_family") or "",
                "source_url": best.get("source_url") or "",
                "raw": best.get("raw") or "",
                "context_snippet": best.get("context_snippet") or "",
                "method": "analysis_selector_shared_fix2d2y",
                "selection_meta": meta or {},
            }
            rebuilt[ck] = m
            filled += 1
        except Exception:
            pass
            continue

    debug["fix2d2y_analysis_selector_rebuild_v1"]["filled"] = int(filled)
    debug["fix2d2y_analysis_selector_rebuild_v1"]["reject_counts"] = reject_counts

    # Attach debug to web_context if provided
    try:
        if isinstance(web_context, dict):
            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].update(debug)
    except Exception:
        return rebuilt

# Ensure global override binding
try:
    globals()["rebuild_metrics_from_snapshots_analysis_canonical_v1"] = rebuild_metrics_from_snapshots_analysis_canonical_v1
except Exception:
    pass

# =====================================================================
# END PATCH FIX2D2Y
# =====================================================================


# ============================================================
# PATCH START: FIX2D34_PREV_KEY_DRIVEN_DIFF_UNIVERSE_V1
# Purpose:
#   - Force Diff Panel V2 universe to be PREV-key driven (baseline canonical keys)
#   - Source CURRENT strictly from cur_response.primary_metrics_canonical for the same ckey
#   - Do NOT emit cur-only "added" rows (those are not baseline diffs)
# Safety:
#   - Diff layer only. Does not affect extraction, hashing, fastpath, snapshots.
#   - Keeps existing unit-family gates in later layers.
# ============================================================

def diff_metrics_by_name_FIX2D34(prev_response: dict, cur_response: dict):
    """Prev-key driven diff: iterate prev canonical keys only; hydrate current from PMC."""
    import re

    ABS_EPS = 1e-9
    REL_EPS = 0.0005

    def _s(x):
        try:
            return str(x)
        except Exception:
            return ""

    def _norm_name(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", " ", (s or "").lower()).strip()

    def _parse_num(v, unit=""):
        fn = globals().get("parse_human_number")
        if callable(fn):
            try:
                return fn(_s(v), unit)
            except Exception:
                return None
        try:
            return float(_s(v).replace(",", "").strip())
        except Exception:
            return None

    def _get_val_unit(m: dict, is_current: bool=False):
        m = m if isinstance(m, dict) else {}
        # Prefer canonical numeric
        if m.get("value_norm") is not None:
            try:
                v = float(m.get("value_norm"))
                u = _s(m.get("base_unit") or m.get("unit") or m.get("unit_tag") or "").strip()
                return v, u
            except Exception:
                pass
        # For current side, respect v27 disable flag
        try:
            if is_current and isinstance(cur_response, dict) and cur_response.get("_disable_numeric_inference_v27"):
                u = _s(m.get("unit") or m.get("unit_tag") or "").strip()
                return None, u
        except Exception:
            pass
        u = _s(m.get("unit") or m.get("unit_tag") or "").strip()
        return _parse_num(m.get("value"), u), u

    def _get_schema(prev: dict):
        if not isinstance(prev, dict):
            return {}
        sch = prev.get("metric_schema_frozen")
        if isinstance(sch, dict) and sch:
            return sch
        pr = prev.get("primary_response")
        if isinstance(pr, dict):
            sch = pr.get("metric_schema_frozen")
            if isinstance(sch, dict) and sch:
                return sch
        return {}

    def _display_name(ckey: str) -> str:
        ckey = _s(ckey).strip()
        if not ckey:
            return "Unknown Metric"
        left, _, right = ckey.partition("__")
        left = " ".join(w.capitalize() for w in left.replace("_", " ").split())
        right = right.replace("_", " ").strip()
        return f"{left} ({right})" if right else left

    def _metric_def(schema: dict, ckey: str):
        md = schema.get(ckey) if isinstance(schema, dict) else None
        return md if isinstance(md, dict) else {}

    prev_can = None
    try:
        prev_can = (prev_response or {}).get("primary_metrics_canonical")
        if not isinstance(prev_can, dict) or not prev_can:
            pr = (prev_response or {}).get("primary_response")
            if isinstance(pr, dict):
                prev_can = pr.get("primary_metrics_canonical")
    except Exception:
        pass
        prev_can = None
    prev_can = prev_can if isinstance(prev_can, dict) else {}

    cur_pmc = None
    try:
        cur_pmc = (cur_response or {}).get("primary_metrics_canonical")
        if not isinstance(cur_pmc, dict) or not cur_pmc:
            pr = (cur_response or {}).get("primary_response")
            if isinstance(pr, dict):
                cur_pmc = pr.get("primary_metrics_canonical")
    except Exception:
        pass
        cur_pmc = None
    cur_pmc = cur_pmc if isinstance(cur_pmc, dict) else {}

    schema = _get_schema(prev_response)

    metric_changes = []
    unchanged = increased = decreased = found = 0

    # PREV-KEY DRIVEN UNIVERSE
    for ckey, pm in prev_can.items():
        if not isinstance(ckey, str):
            ckey = _s(ckey)
        ckey = ckey.strip()
        if not ckey:
            continue
        pm = pm if isinstance(pm, dict) else {}

        cm = cur_pmc.get(ckey)
        cm = cm if isinstance(cm, dict) else {}

        name = _display_name(ckey)
        definition = _metric_def(schema, ckey)

        prev_raw = pm.get("raw") if pm.get("raw") is not None else pm.get("value")

        if not cm:
            metric_changes.append({
                "name": name,
                "previous_value": prev_raw,
                "current_value": "N/A",
                "change_pct": None,
                "change_type": "not_found",
                "match_confidence": 0.0,
                "canonical_key": ckey,
                "metric_definition": definition,
            })
            continue

        found += 1
        cur_raw = cm.get("raw") if cm.get("raw") is not None else cm.get("value")

        pv, pu = _get_val_unit(pm, is_current=False)
        cv, cu = _get_val_unit(cm, is_current=True)

        change_type = "unknown"
        change_pct = None

        if pv is not None and cv is not None:
            if abs(pv - cv) <= max(ABS_EPS, abs(pv) * REL_EPS):
                change_type = "unchanged"
                change_pct = 0.0
                unchanged += 1
            elif cv > pv:
                change_type = "increased"
                change_pct = ((cv - pv) / max(ABS_EPS, abs(pv))) * 100.0
                increased += 1
            else:
                change_type = "decreased"
                change_pct = ((cv - pv) / max(ABS_EPS, abs(pv))) * 100.0
                decreased += 1
        else:
            if pv is None and cv is not None:
                change_type = "invalid_previous"
            elif pv is not None and cv is None:
                change_type = "invalid_current"
            else:
                change_type = "unknown"

        # Build display current
        cur_unit_tag = _s(cm.get("unit") or cm.get("unit_tag") or "").strip()
        cur_disp = _s(cur_raw).strip() if cur_raw is not None else ""
        if cur_disp and cur_unit_tag and (cur_unit_tag not in cur_disp):
            cur_disp = f"{cur_disp} {cur_unit_tag}".strip()

        metric_changes.append({
            "name": name,
            "previous_value": prev_raw,
            "current_value": cur_disp or ("N/A" if cv is None else _s(cv)),
            "change_pct": change_pct,
            "change_type": change_type,
            "match_confidence": 1.0,
            "canonical_key": ckey,
            "metric_definition": definition,
            "previous_value_norm": pv,
            "current_value_norm": cv,
            "prev_unit_cmp": pu,
            "cur_unit_cmp": cu,
        })

    # Attach top-level summary diagnostics
    try:
        if isinstance(cur_response, dict):
            cur_response.setdefault("debug", {})
            if isinstance(cur_response.get("debug"), dict):
                cur_response["debug"].setdefault("fix2d34_prev_key_driven_diff_v1", {})
                cur_response["debug"]["fix2d34_prev_key_driven_diff_v1"] = {
                    "prev_keys": int(len(prev_can)) if isinstance(prev_can, dict) else 0,
                    "cur_pmc_keys": int(len(cur_pmc)) if isinstance(cur_pmc, dict) else 0,
                    "rows_emitted": int(len(metric_changes)),
                    "found_both": int(found),
                    "note": "Universe iterates prev canonical keys only; current sourced from primary_metrics_canonical.",
                }
    except Exception:
        return metric_changes, unchanged, increased, decreased, found

# Wire FIX2D34 as the active diff function (last-wins override)
try:
    diff_metrics_by_name = diff_metrics_by_name_FIX2D34  # type: ignore
except Exception:
    pass

try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass

# ============================================================
# PATCH END: FIX2D34_PREV_KEY_DRIVEN_DIFF_UNIVERSE_V1
# ============================================================


# =========================================================
# FIX2D45 — FORCE BASELINE SCHEMA MATERIALISATION (FINAL)
# =========================================================
# This patch removes all optional/gated baseline construction
# and unconditionally builds baseline_schema_metrics_v1 during
# Analysis finalisation when schema + canonical metrics exist.

CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
def _fix2d45_force_baseline_schema_materialisation(analysis: dict) -> None:
    if "results" not in analysis:
        analysis["results"] = {}

    schema = analysis.get("metric_schema_frozen")
    canonical = analysis.get("primary_metrics_canonical")
    anchors = analysis.get("metric_anchors", {})

    if not schema:
        raise RuntimeError("FIX2D45: metric_schema_frozen missing")

    if not canonical:
        raise RuntimeError("FIX2D45: primary_metrics_canonical missing")

    baseline_schema_metrics_v1 = {}

    for schema_key in schema.keys():
        metric = canonical.get(schema_key)
        if not metric:
            continue

        baseline_schema_metrics_v1[schema_key] = {
            "canonical_key": schema_key,
            "canonical_id": metric.get("canonical_id"),
            "value_norm": metric.get("value_norm"),
            "unit_family": metric.get("unit_family"),
            "dimension": metric.get("dimension"),
            "anchor_hash": anchors.get(schema_key, {}).get("anchor_hash"),
            "source_url": metric.get("source_url"),
        }

    analysis["results"]["baseline_schema_metrics_v1"] = baseline_schema_metrics_v1
    analysis["baseline_schema_metrics_v1"] = baseline_schema_metrics_v1

    analysis.setdefault("debug", {})
    analysis["debug"]["fix2d45_baseline_count"] = len(baseline_schema_metrics_v1)


# ---- invoke FIX2D45 in Analysis finalisation ----
if "_fix2d45_force_baseline_schema_materialisation" not in globals():
    pass


# FIX2D47 — Diff Panel V2: Schema-union row universe + cross-source current winner
# --------------------------------------------------------------------------------
# Objective:
#   Implement the behaviour you described:
#     - Evolution may discover new sources
#     - If a discovered metric binds to the SAME schema canonical key as a baseline metric,
#       the diff panel should treat it as "current" and compute deltas (even if source differs).
#
# What this patch does:
#   1) Builds the diff row universe as: schema_keys = prev_schema_keys ∪ cur_schema_keys
#   2) Uses Analysis baseline_schema_metrics_v1 (schema-keyed) as the prev map (when available)
#   3) Builds a schema-keyed current map from ALL current canonical metrics (any source),
#      selecting a deterministic "winner" per schema key
#   4) Joins strictly by schema key (not by source universe), computes change_type + pct.
#
# Safety:
#   - Additive: defines a new builder and (optionally) swaps it in behind a join_mode gate.
#   - Deterministic: stable tie-breaks for current winner selection.
#
# Versioning:
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
def _fix2d47_get_nested(d, path, default=None):
    try:
        x = d
        for k in path:
            if not isinstance(x, dict):
                return default
            x = x.get(k)
        return x if x is not None else default
    except Exception:
        return default

def _fix2d47_first_present(d, paths, default=None):
    for p in paths:
        v = _fix2d47_get_nested(d, p, None)
        if v is not None:
            return v
    return default

def _fix2d47_unwrap_baseline_schema_metrics(prev_response: dict) -> dict:
    # Where Analysis should serialize it (post FIX2D45):
    #   - results.baseline_schema_metrics_v1 (preferred)
    #   - baseline_schema_metrics_v1 (mirror)
    # plus a few legacy wrapper shapes.
    paths = [
        ["results","baseline_schema_metrics_v1"],
        ["baseline_schema_metrics_v1"],
        ["primary_response","results","baseline_schema_metrics_v1"],
        ["results","primary_response","results","baseline_schema_metrics_v1"],
        ["primary_response","baseline_schema_metrics_v1"],
    ]
    v = _fix2d47_first_present(prev_response or {}, paths, default={})
    return v if isinstance(v, dict) else {}

def _fix2d47_unwrap_primary_metrics_canonical(resp: dict) -> dict:
    if not isinstance(resp, dict):
        return {}
    if isinstance(resp.get("primary_metrics_canonical"), dict) and resp.get("primary_metrics_canonical"):
        return resp.get("primary_metrics_canonical") or {}
    pr = resp.get("primary_response")
    if isinstance(pr, dict):
        if isinstance(pr.get("primary_metrics_canonical"), dict) and pr.get("primary_metrics_canonical"):
            return pr.get("primary_metrics_canonical") or {}
        res = pr.get("results")
        if isinstance(res, dict) and isinstance(res.get("primary_metrics_canonical"), dict) and res.get("primary_metrics_canonical"):
            return res.get("primary_metrics_canonical") or {}
    res = resp.get("results")
    if isinstance(res, dict) and isinstance(res.get("primary_metrics_canonical"), dict) and res.get("primary_metrics_canonical"):
        return res.get("primary_metrics_canonical") or {}
    return {}

def _fix2d47_schema_key_for_metric(ckey: str, m: dict) -> str:
    # Prefer explicit schema/canonical_key fields; fall back to dict key.
    if isinstance(m, dict):
        for k in ("canonical_key", "schema_key", "schema_canonical_key", "schema_ckey"):
            v = m.get(k)
            if isinstance(v, str) and v.strip():
                return v.strip()
    if isinstance(ckey, str) and ckey.strip():
        return ckey.strip()
    return ""

def _fix2d47_metric_confidence(m: dict) -> float:
    if not isinstance(m, dict):
        return 0.0
    for k in ("confidence", "score", "match_confidence", "bind_confidence"):
        v = m.get(k)
        try:
            if v is not None:
                return float(v)
        except Exception:
            return 0.0

def _fix2d47_pick_cur_winner(existing: dict, challenger: dict) -> dict:
    # Deterministic winner selection:
    #   1) higher confidence
    #   2) higher anchor_confidence (if present)
    #   3) stable tie-break by source_url then raw then canonical_id
    if not isinstance(existing, dict):
        return challenger
    if not isinstance(challenger, dict):
        return existing

    ce = _fix2d47_metric_confidence(existing)
    cc = _fix2d47_metric_confidence(challenger)
    if cc != ce:
        return challenger if cc > ce else existing

    def _af(m):
        try:
            return float(m.get("anchor_confidence") or 0.0)
        except Exception:
            return 0.0

    ae = _af(existing)
    ac = _af(challenger)
    if ac != ae:
        return challenger if ac > ae else existing

    def _k(m):
        try:
            su = m.get("source_url") or ""
            raw = m.get("raw") or m.get("value") or ""
            cid = m.get("canonical_id") or ""
            return (str(su), str(raw), str(cid))
        except Exception:
            return ("", "", "")

    return challenger if _k(challenger) < _k(existing) else existing

def _fix2d47_build_cur_map_by_schema_key(cur_response: dict) -> dict:
    cur_metrics = _fix2d47_unwrap_primary_metrics_canonical(cur_response or {})
    out = {}
    if not isinstance(cur_metrics, dict):
        return out
    for ck, m in cur_metrics.items():
        if not isinstance(m, dict):
            continue
        sk = _fix2d47_schema_key_for_metric(ck, m)
        if not sk:
            continue
        out[sk] = _fix2d47_pick_cur_winner(out.get(sk), m)
    return out

def _fix2d47_raw_display_value(m: dict):
    if not isinstance(m, dict):
        return None
    if m.get("raw") is not None:
        return m.get("raw")
    if m.get("value") is not None:
        return m.get("value")
    # baseline_schema_metrics_v1 often stores display in prev_raw/current_raw fields elsewhere; keep None if absent
    return None

def _fix2d47_value_norm(m: dict):
    if not isinstance(m, dict):
        return None
    for k in ("value_norm", "valuenorm", "current_value_norm", "prev_value_norm", "cur_value_norm"):
        if m.get(k) is None:
            continue
        try:
            return float(m.get(k))
        except Exception:
            return None

def _fix2d47_unit_tag(m: dict) -> str:
    if not isinstance(m, dict):
        return ""
    for k in ("base_unit", "unit", "unit_tag", "unittag", "baseunit"):
        v = m.get(k)
        if v is not None:
            return str(v).strip()
    return ""

def _fix2d47_metric_name(schema_key: str, prev_m: dict, cur_m: dict) -> str:
    for m in (prev_m, cur_m):
        if isinstance(m, dict):
            for k in ("name", "metric_name", "label", "title"):
                v = m.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip()
    return schema_key

def build_diff_metrics_panel_v2_FIX2D47(prev_response: dict, cur_response: dict):
    """Return (rows, summary) for Diff Metrics Panel V2 (schema-union + cross-source current)."""
    rows = []

    prev_map = _fix2d47_unwrap_baseline_schema_metrics(prev_response or {})
    cur_map = _fix2d47_build_cur_map_by_schema_key(cur_response or {})

    prev_keys = set([k for k in prev_map.keys() if isinstance(k, str) and k])
    cur_keys = set([k for k in cur_map.keys() if isinstance(k, str) and k])
    all_keys = sorted(prev_keys | cur_keys)

    both_count = 0
    prev_only_count = 0
    cur_only_count = 0

    inc = dec = unc = add = rem = not_found = 0

    for skey in all_keys:
        pm = prev_map.get(skey) if isinstance(prev_map, dict) else None
        pm = pm if isinstance(pm, dict) else {}
        cm = cur_map.get(skey) if isinstance(cur_map, dict) else None
        cm = cm if isinstance(cm, dict) else {}

        has_prev = skey in prev_keys
        has_cur = skey in cur_keys

        if has_prev and has_cur:
            both_count += 1
        elif has_prev and not has_cur:
            prev_only_count += 1
        elif has_cur and not has_prev:
            cur_only_count += 1

        prev_raw = pm.get("value_raw") if pm.get("value_raw") is not None else _fix2d47_raw_display_value(pm)
        cur_raw = cm.get("value_raw") if cm.get("value_raw") is not None else _fix2d47_raw_display_value(cm)

        prev_val_norm = _fix2d47_value_norm(pm)
        cur_val_norm = _fix2d47_value_norm(cm)

        prev_unit = pm.get("unit_tag") if pm.get("unit_tag") is not None else _fix2d47_unit_tag(pm)
        cur_unit = cm.get("unit_tag") if cm.get("unit_tag") is not None else _fix2d47_unit_tag(cm)

        name = _fix2d47_metric_name(skey, pm, cm)

        change_type = "unknown"
        change_pct = None
        delta_abs = None

        # Semantics:
        # - both present + numeric => increased/decreased/unchanged
        # - prev present, cur missing => removed (vs baseline)
        # - cur present, prev missing => added (new discovery)
        if has_prev and not has_cur:
            change_type = "removed"
            rem += 1
        elif has_cur and not has_prev:
            change_type = "added"
            add += 1
        else:
            # both
            if isinstance(prev_val_norm, (int, float)) and isinstance(cur_val_norm, (int, float)):
                delta_abs = float(cur_val_norm) - float(prev_val_norm)
                if abs(prev_val_norm) > 1e-12:
                    change_pct = (delta_abs / float(prev_val_norm)) * 100.0
                # tolerance for float jitter
                if abs(delta_abs) <= max(1e-9, abs(float(prev_val_norm)) * 0.0005):
                    change_type = "unchanged"
                    unc += 1
                elif delta_abs > 0:
                    change_type = "increased"
                    inc += 1
                else:
                    change_type = "decreased"
                    dec += 1
            else:
                # both present but not numerically comparable
                change_type = "unknown"
                not_found += 1

        rows.append({
            "canonical_key": skey,               # schema key
            "name": name,
            "previous_value": prev_raw if has_prev else None,
            "current_value": cur_raw if has_cur else None,
            "prev_value_norm": prev_val_norm if has_prev else None,
            "cur_value_norm": cur_val_norm if has_cur else None,
            "previous_unit": prev_unit if has_prev else None,
            "current_unit": cur_unit if has_cur else None,
            "change_type": change_type,
            "change_pct": change_pct,
            "delta_abs": delta_abs,
            # provenance helpers
            "prev_source_url": pm.get("source_url"),
            "cur_source_url": cm.get("source_url"),
            "cur_confidence": _fix2d47_metric_confidence(cm),
            "method": "schema_key",              # explicit: joined by schema key
        })

    summary = {
        "join_mode": "schema_union",
        "rows_total": len(rows),
        "both_count": both_count,
        "prev_only_count": prev_only_count,
        "cur_only_count": cur_only_count,
        "metrics_increased": inc,
        "metrics_decreased": dec,
        "metrics_unchanged": unc,
        "metrics_added": add,
        "metrics_removed": rem,
        "metrics_unknown": not_found,
    }
    return rows, summary


# -------------------------------
# OPTIONAL WIRING HOOK:
# If your code already calls build_diff_metrics_panel_v2(prev, cur),
# replace it with build_diff_metrics_panel_v2_FIX2D47 behind your join-mode flag.
#
# Example drop-in (inside compute_source_anchored_diff before calling the builder):
#   jm = _fix2d6_get_diff_join_mode_v1()
#   if jm in ("schema_union","schema","cross","schema_cross_source","x"):
#       rows, summary = build_diff_metrics_panel_v2_FIX2D47(prev_response, cur_response)
#   else:
#       rows, summary = build_diff_metrics_panel_v2(prev_response, cur_response)
#
# Patch tracker entry (manual):
#   - FIX2D47: Diff Panel V2 schema-union universe + cross-source current winner selection



# =========================================================
# FIX2D47 — FINAL VERSION STAMP OVERRIDE
# =========================================================
# Ensure the authoritative code version reflects this patch.
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
# =========================================================
# FIX2D48 — Canonical Key Grammar v1 (Builder + Validator)
# =========================================================
# Purpose:
#   Establish a single authoritative canonical key grammar and validator,
#   and route canonical-key minting through it.
#
# Key format:
#   <scope>_<entity>_<metric>_<time_qualifier>[_<qualifier>...]__<dimension>
#
# Determinism:
#   - No free-text hashing.
#   - Normalization + strict validation.
#
# Integration:
#   - Use build_canonical_key_v1(...) only in schema-binding / key minting.
#   - Validate keys at the point primary_metrics_canonical is finalized.

import re
from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional, Tuple

_FIX2D48_TOKEN_RE = re.compile(r"^[a-z0-9]+(?:_[a-z0-9]+)*$")

def _fix2d48_parse_canonical_key_v1(key: str) -> Tuple[str, str]:
    if not isinstance(key, str) or not key:
        raise ValueError("canonical_key: empty")
    if key.count("__") != 1:
        raise ValueError(f"canonical_key: expected exactly one '__' separator, got {key.count('__')}")
    subject, dimension = key.split("__", 1)
    if not subject or not dimension:
        raise ValueError("canonical_key: missing subject or dimension")
    return subject, dimension

def _fix2d48_norm_token(s: str) -> str:
    s = (s or "").strip().lower()
    s = re.sub(r"[^a-z0-9]+", "_", s)
    s = re.sub(r"_+", "_", s).strip("_")
    return s

def _fix2d48_validate_token(name: str, token: str) -> None:
    if not token:
        raise ValueError(f"{name}: missing")
    if "__" in token:
        raise ValueError(f"{name}: contains '__' (reserved)")
    if not _FIX2D48_TOKEN_RE.match(token):
        raise ValueError(f"{name}: invalid token '{token}' (must match {_FIX2D48_TOKEN_RE.pattern})")

def _fix2d48_validate_dimension(dimension: str, allowed_dimensions: Optional[Iterable[str]] = None) -> None:
    _fix2d48_validate_token("dimension", dimension)
    if allowed_dimensions is not None:
        allowed = set(_fix2d48_norm_token(x) for x in allowed_dimensions)
        if dimension not in allowed:
            raise ValueError(f"dimension: '{dimension}' not in allowed dimensions ({sorted(allowed)})")

def _fix2d48_validate_time_qualifier(time_q: str) -> None:
    _fix2d48_validate_token("time_qualifier", time_q)

    if re.fullmatch(r"\d{4}", time_q):
        return
    if re.fullmatch(r"ytd_\d{4}", time_q):
        return
    if re.fullmatch(r"\d{4}_\d{4}", time_q):
        a, b = time_q.split("_", 1)
        if int(b) < int(a):
            raise ValueError(f"time_qualifier: range out of order '{time_q}'")
        return
    if re.fullmatch(r"asof_\d{4}_\d{2}", time_q):
        parts = time_q.split("_")
        mm = int(parts[2])
        if not (1 <= mm <= 12):
            raise ValueError(f"time_qualifier: invalid month in '{time_q}'")
        return
    if re.fullmatch(r"asof_\d{4}_\d{2}_\d{2}", time_q):
        parts = time_q.split("_")
        mm, dd = int(parts[2]), int(parts[3])
        if not (1 <= mm <= 12):
            raise ValueError(f"time_qualifier: invalid month in '{time_q}'")
        if not (1 <= dd <= 31):
            raise ValueError(f"time_qualifier: invalid day in '{time_q}'")
        return

    raise ValueError(f"time_qualifier: '{time_q}' does not match allowed families")

@dataclass(frozen=True)
class CanonicalKeyFieldsV1:
    scope: str
    entity: str
    metric: str
    time_qualifier: str
    dimension: str
    qualifiers: Tuple[str, ...] = ()

def build_canonical_key_v1(
    fields: CanonicalKeyFieldsV1,
    *,
    allowed_dimensions: Optional[Iterable[str]] = None,
) -> str:
    scope = _fix2d48_norm_token(fields.scope)
    entity = _fix2d48_norm_token(fields.entity)
    metric = _fix2d48_norm_token(fields.metric)
    time_q = _fix2d48_norm_token(fields.time_qualifier)
    dimension = _fix2d48_norm_token(fields.dimension)
    qualifiers = tuple(_fix2d48_norm_token(q) for q in (fields.qualifiers or ()))

    _fix2d48_validate_token("scope", scope)
    _fix2d48_validate_token("entity", entity)
    _fix2d48_validate_token("metric", metric)
    _fix2d48_validate_time_qualifier(time_q)
    _fix2d48_validate_dimension(dimension, allowed_dimensions=allowed_dimensions)

    if qualifiers:
        for q in qualifiers:
            _fix2d48_validate_token("qualifier", q)

    subject_parts: List[str] = [scope, entity, metric, time_q] + list(qualifiers)
    subject = "_".join(subject_parts)
    key = f"{subject}__{dimension}"

    validate_canonical_key_v1(key, allowed_dimensions=allowed_dimensions)
    return key

def validate_canonical_key_v1(key: str, *, allowed_dimensions: Optional[Iterable[str]] = None) -> Dict[str, str]:
    subject, dimension = _fix2d48_parse_canonical_key_v1(key)

    _fix2d48_validate_token("subject", subject)
    _fix2d48_validate_dimension(_fix2d48_norm_token(dimension), allowed_dimensions=allowed_dimensions)

    parts = subject.split("_")
    if len(parts) < 4:
        raise ValueError(
            f"canonical_key: subject must have >=4 parts (scope_entity_metric_time), got {len(parts)}: '{subject}'"
        )

    scope, entity, metric = parts[0], parts[1], parts[2]

    if parts[3] == "ytd" and len(parts) >= 5:
        time_q = f"ytd_{parts[4]}"
        remaining = parts[5:]
    elif parts[3] == "asof" and len(parts) >= 6:
        if len(parts) >= 7 and re.fullmatch(r"\d{2}", parts[6]):
            time_q = f"asof_{parts[4]}_{parts[5]}_{parts[6]}"
            remaining = parts[7:]
        else:
            time_q = f"asof_{parts[4]}_{parts[5]}"
            remaining = parts[6:]
    elif re.fullmatch(r"\d{4}", parts[3]) and len(parts) >= 5 and re.fullmatch(r"\d{4}", parts[4]):
        time_q = f"{parts[3]}_{parts[4]}"
        remaining = parts[5:]
    else:
        time_q = parts[3]
        remaining = parts[4:]

    _fix2d48_validate_token("scope", scope)
    _fix2d48_validate_token("entity", entity)
    _fix2d48_validate_token("metric", metric)
    _fix2d48_validate_time_qualifier(time_q)

    for q in remaining:
        _fix2d48_validate_token("qualifier", q)

    return {
        "subject": subject,
        "dimension": _fix2d48_norm_token(dimension),
        "scope": scope,
        "entity": entity,
        "metric": metric,
        "time_qualifier": time_q,
    }

def _fix2d48_allowed_dimensions_from_schema(metric_schema_frozen: dict) -> Optional[set]:
    if not isinstance(metric_schema_frozen, dict) or not metric_schema_frozen:
        return None
    dims = set()
    for skey, spec in metric_schema_frozen.items():
        if isinstance(spec, dict):
            d = spec.get("dimension")
            if d:
                dims.add(_fix2d48_norm_token(str(d)))
    return dims or None

def validate_primary_metrics_canonical_keys_v1(primary_metrics_canonical: dict, metric_schema_frozen: Optional[dict] = None) -> None:
    allowed_dims = _fix2d48_allowed_dimensions_from_schema(metric_schema_frozen or {})
    if not isinstance(primary_metrics_canonical, dict):
        return
    for ckey in list(primary_metrics_canonical.keys()):
        validate_canonical_key_v1(str(ckey), allowed_dimensions=allowed_dims)

# =========================================================
# END FIX2D48 CANONICAL KEY MODULE
# =========================================================



# =========================================================
# FIX2D48 — Integration Hook (non-invasive)
# =========================================================
# Where to call this:
#   - Right before serializing Analysis/Evolution output (after primary_metrics_canonical is finalized),
#     call:
#       validate_primary_metrics_canonical_keys_v1(primary_metrics_canonical, metric_schema_frozen)
#
# This will crash early in dev runs if any minted key violates the grammar.
#
# NOTE:
#   This patch does not attempt to rewrite all minting sites automatically in this single-file
#   environment; it provides the authoritative builder+validator and a validator tripwire.
#   The next patch should audit and route minting through build_canonical_key_v1(...).
# =========================================================
def _fix2d48_try_validate_outputs(output_obj: dict) -> None:
    if not isinstance(output_obj, dict):
        return
    metric_schema_frozen = (
        output_obj.get("metric_schema_frozen") if isinstance(output_obj.get("metric_schema_frozen"), dict) else None
    ) or (
        output_obj.get("primary_response", {}).get("metric_schema_frozen") if isinstance(output_obj.get("primary_response"), dict) else None
    ) or (
        output_obj.get("results", {}).get("metric_schema_frozen") if isinstance(output_obj.get("results"), dict) else None
    )

    pmc = (
        output_obj.get("primary_metrics_canonical") if isinstance(output_obj.get("primary_metrics_canonical"), dict) else None
    ) or (
        output_obj.get("primary_response", {}).get("primary_metrics_canonical") if isinstance(output_obj.get("primary_response"), dict) else None
    ) or (
        output_obj.get("results", {}).get("primary_metrics_canonical") if isinstance(output_obj.get("results"), dict) else None
    )

    if isinstance(pmc, dict) and pmc:
        validate_primary_metrics_canonical_keys_v1(pmc, metric_schema_frozen=metric_schema_frozen)

def _fix2d48_should_validate_ckeys(web_context: Optional[dict]) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        return bool(web_context.get("validate_canonical_keys_v1") or web_context.get("diag_validate_ckeys_v1"))
    except Exception:
        return False



# =========================================================
# FIX2D48 — FINAL VERSION STAMP OVERRIDE
# =========================================================
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
# =========================================================
# FIX2D49 — Audit canonical-key minting + optional rekeying
# =========================================================
# Goal:
#   (1) Identify where canonical keys are being minted in a way that violates the v1 grammar,
#       or where a metric dict's own canonical_key disagrees with the dict key.
#   (2) Provide an optional, deterministic "rekey" pass that repairs obvious mismatches by:
#       - moving entries to metric["canonical_key"] if it validates
#       - or moving entries to metric["schema_key"/"schema_canonical_key"] if present + validates
#   (3) Emit a compact diagnostics ledger into output_obj["debug"] so you can see drift immediately.
#
# Safety:
#   Off by default. Enable via web_context:
#     - web_context["diag_fix2d49_audit"] = True
#     - web_context["diag_fix2d49_rekey"] = True   (optional)
#     - web_context["diag_fix2d49_strict"] = True  (raise on invalid keys)
#
# Note:
#   In a single-file patch environment we can’t reliably refactor every minting site.
#   This audit/rekey pass makes those sites visible and stabilizes downstream joins
#   while you convert minting sites to use build_canonical_key_v1(...) in subsequent cleanup.
# =========================================================

from typing import Any

def _fix2d49_get_schema_key_hint(m: dict) -> str:
    if not isinstance(m, dict):
        return ""
    for k in ("canonical_key", "schema_key", "schema_canonical_key", "schema_ckey"):
        v = m.get(k)
        if isinstance(v, str) and v.strip():
            return v.strip()
    return ""

def _fix2d49_is_suspicious_key(key: str) -> bool:
    # Heuristics (non-blocking): help surface "free-text" keys or hashed keys.
    if not isinstance(key, str):
        return True
    if len(key) > 120:
        return True
    if " " in key or "(" in key or ")" in key or "/" in key:
        return True
    # looks like a hash-heavy identifier
    if sum(ch.isdigit() for ch in key) > 50:
        return True
    return False

def _fix2d49_audit_primary_metrics_canonical(pmc: dict, metric_schema_frozen: dict | None = None) -> dict:
    """
    Returns an audit report dict:
      {
        "total": int,
        "invalid_keys": [..],
        "mismatch_key_vs_metric": [..],
        "suspicious_keys": [..],
      }
    """
    rep = {
        "total": 0,
        "invalid_keys": [],
        "mismatch_key_vs_metric": [],
        "suspicious_keys": [],
    }
    if not isinstance(pmc, dict):
        return rep

    allowed_dims = _fix2d48_allowed_dimensions_from_schema(metric_schema_frozen or {})

    for k, m in pmc.items():
        rep["total"] += 1
        sk_hint = _fix2d49_get_schema_key_hint(m)
        if sk_hint and isinstance(k, str) and sk_hint != k:
            rep["mismatch_key_vs_metric"].append({"dict_key": k, "metric_key": sk_hint})

        if isinstance(k, str) and _fix2d49_is_suspicious_key(k):
            rep["suspicious_keys"].append(k)

        try:
            validate_canonical_key_v1(str(k), allowed_dimensions=allowed_dims)
        except Exception as e:
            rep["invalid_keys"].append({"key": str(k), "error": str(e)})

    return rep

def _fix2d49_rekey_primary_metrics_canonical(pmc: dict, metric_schema_frozen: dict | None = None) -> tuple[dict, dict]:
    """
    Deterministically rekeys pmc by metric's own canonical_key/schema_key when valid.
    Returns: (new_pmc, rekey_report)
    """
    report = {
        "moved": [],
        "dropped": [],
        "kept": 0,
    }
    if not isinstance(pmc, dict) or not pmc:
        return pmc, report

    allowed_dims = _fix2d48_allowed_dimensions_from_schema(metric_schema_frozen or {})

    new_pmc = {}
    # stable iteration for determinism
    for old_key in sorted(pmc.keys(), key=lambda x: str(x)):
        m = pmc.get(old_key)
        if not isinstance(m, dict):
            continue

        # Candidate preferred: metric's explicit canonical_key
        candidate = m.get("canonical_key")
        if not (isinstance(candidate, str) and candidate.strip()):
            # next: schema hints
            candidate = m.get("schema_key") or m.get("schema_canonical_key") or m.get("schema_ckey")

        candidate = candidate.strip() if isinstance(candidate, str) else ""

        chosen_key = str(old_key) if old_key is not None else ""

        # If candidate exists and validates, use it; else keep old_key if it validates.
        def _valid(k: str) -> bool:
            try:
                validate_canonical_key_v1(k, allowed_dimensions=allowed_dims)
                return True
            except Exception:
                return False

        if candidate and _valid(candidate):
            chosen_key = candidate
        elif chosen_key and _valid(chosen_key):
            pass
        else:
            # can't validate either; keep old_key but mark dropped/invalid for downstream stability
            report["dropped"].append({"old_key": str(old_key), "candidate": candidate})
            continue

        # deterministic collision handling: prefer higher confidence, else stable tie-break
        if chosen_key in new_pmc:
            existing = new_pmc[chosen_key]
            winner = _fix2d47_pick_cur_winner(existing, m)  # reuse deterministic picker from FIX2D47
            new_pmc[chosen_key] = winner
        else:
            new_pmc[chosen_key] = m

        if str(old_key) != chosen_key:
            report["moved"].append({"from": str(old_key), "to": chosen_key})
        else:
            report["kept"] += 1

    return new_pmc, report

def _fix2d49_try_audit_and_rekey_outputs(output_obj: dict, web_context: dict | None = None) -> None:
    if not isinstance(output_obj, dict):
        return

    do_audit = bool(web_context and web_context.get("diag_fix2d49_audit"))
    do_rekey = bool(web_context and web_context.get("diag_fix2d49_rekey"))
    strict = bool(web_context and web_context.get("diag_fix2d49_strict"))

    if not (do_audit or do_rekey):
        return

    metric_schema_frozen = (
        output_obj.get("metric_schema_frozen") if isinstance(output_obj.get("metric_schema_frozen"), dict) else None
    ) or (
        output_obj.get("primary_response", {}).get("metric_schema_frozen") if isinstance(output_obj.get("primary_response"), dict) else None
    ) or (
        output_obj.get("results", {}).get("metric_schema_frozen") if isinstance(output_obj.get("results"), dict) else None
    ) or {}

    # Locate pmc in common shapes
    pmc_path = None
    pmc = None
    if isinstance(output_obj.get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_metrics_canonical",)
        pmc = output_obj["primary_metrics_canonical"]
    elif isinstance(output_obj.get("primary_response"), dict) and isinstance(output_obj["primary_response"].get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_response", "primary_metrics_canonical")
        pmc = output_obj["primary_response"]["primary_metrics_canonical"]
    elif isinstance(output_obj.get("results"), dict) and isinstance(output_obj["results"].get("primary_metrics_canonical"), dict):
        pmc_path = ("results", "primary_metrics_canonical")
        pmc = output_obj["results"]["primary_metrics_canonical"]
    else:
        pmc = None

    output_obj.setdefault("debug", {})

    if isinstance(pmc, dict) and pmc:
        audit = _fix2d49_audit_primary_metrics_canonical(pmc, metric_schema_frozen=metric_schema_frozen)
        output_obj["debug"]["fix2d49_pmc_audit"] = audit

        if strict and audit.get("invalid_keys"):
            raise RuntimeError(f"FIX2D49 strict: invalid canonical keys detected: {audit.get('invalid_keys')[:3]}")

        if do_rekey:
            new_pmc, rep = _fix2d49_rekey_primary_metrics_canonical(pmc, metric_schema_frozen=metric_schema_frozen)
            output_obj["debug"]["fix2d49_pmc_rekey"] = rep

            # Write back to the same location
            if pmc_path == ("primary_metrics_canonical",):
                output_obj["primary_metrics_canonical"] = new_pmc
            elif pmc_path == ("primary_response","primary_metrics_canonical"):
                output_obj["primary_response"]["primary_metrics_canonical"] = new_pmc
            elif pmc_path == ("results","primary_metrics_canonical"):
                output_obj["results"]["primary_metrics_canonical"] = new_pmc
    else:
        output_obj["debug"]["fix2d49_pmc_audit"] = {"total": 0, "note": "primary_metrics_canonical not found"}

# =========================================================
# END FIX2D49
# =========================================================

# =========================================================
# FIX2D49 — AUTO-HOOK FINAL OUTPUT (NO NEED TO FIND FINALIZER)
# =========================================================
# Problem:
#   You don't know which function "finalizes" primary_metrics_canonical.
#
# Solution:
#   Wrap the top-level run entrypoints (if present) and apply:
#     - FIX2D48 validation (optional flag)
#     - FIX2D49 audit/rekey (optional flags)
#   to the returned output object right before it is handed back to the UI / serializer.
#
# Enable via web_context flags (same as before):
#   web_context["validate_canonical_keys_v1"] = True
#   web_context["diag_fix2d49_audit"] = True
#   web_context["diag_fix2d49_rekey"] = True
#   web_context["diag_fix2d49_strict"] = True
#
# This is additive and does not change behaviour unless flags are enabled.
# =========================================================

def _fix2d49_extract_web_context_from_call(args, kwargs):
    # Try kwargs first
    wc = kwargs.get("web_context")
    if isinstance(wc, dict):
        return wc
    # Heuristic: scan args for a dict that looks like web_context
    for a in args:
        if isinstance(a, dict) and any(k in a for k in ("diag_fix2d49_audit","diag_fix2d49_rekey","validate_canonical_keys_v1","diag_validate_ckeys_v1")):
            return a
    return None

def _fix2d49_apply_postprocess_if_enabled(output_obj, web_context):
    if not isinstance(output_obj, dict):
        return output_obj

    # FIX2D48 validation (opt-in)
    if _fix2d48_should_validate_ckeys(web_context):
        _fix2d48_try_validate_outputs(output_obj)

    # FIX2D49 audit/rekey (opt-in via flags inside the function)
    _fix2d49_try_audit_and_rekey_outputs(output_obj, web_context)

    # FIX2D50 gatekeeper (opt-in)
    _fix2d50_try_gate_output_obj(output_obj, web_context)

    return output_obj

    # FIX2D48 validation (opt-in)
    try:
        if _fix2d48_should_validate_ckeys(web_context):
            _fix2d48_try_validate_outputs(output_obj)
    except Exception:
        pass
        # keep behaviour consistent: validation failures are surfaced only when flag enabled;
        # if enabled, allow exception to propagate.
        raise

    # FIX2D49 audit/rekey (opt-in via flags inside the function)
    try:
        _fix2d49_try_audit_and_rekey_outputs(output_obj, web_context)
    except Exception:
        pass
        raise

    return output_obj

def _fix2d49_wrap_entrypoint(fn_name: str):
    fn = globals().get(fn_name)
    if not callable(fn):
        return
    # Avoid double-wrapping
    if getattr(fn, "_fix2d49_wrapped", False):
        return

    def _wrapped(*args, **kwargs):
        web_context = _fix2d49_extract_web_context_from_call(args, kwargs)
        out = fn(*args, **kwargs)
        return _fix2d49_apply_postprocess_if_enabled(out, web_context)

    _wrapped._fix2d49_wrapped = True
    _wrapped.__name__ = getattr(fn, "__name__", fn_name)
    _wrapped.__doc__ = getattr(fn, "__doc__", None)
    globals()[fn_name] = _wrapped

def _fix2d49_install_autohooks():
    # Common entrypoints in your project (safe no-ops if absent)
    for name in (
        "run_source_anchored_analysis",
        "run_source_anchored_evolution",
        "run_source_anchored_evolution_previous_data",
        "run_source_anchored_evolution_previousdata",  # legacy spelling
        "run_analysis",
        "run_evolution",
    ):
        _fix2d49_wrap_entrypoint(name)

# Install at import time (additive)
try:
    _fix2d49_install_autohooks()
except Exception:
    pass

# =========================================================
# END AUTO-HOOK
# =========================================================

# =========================================================
# FIX2D50 — PMC Gatekeeper: Schema-bound canonical keys only
# =========================================================
# Objective:
#   Close the remaining gap by making schema binding authoritative at the PMC boundary.
#
# Rule enforced (when enabled):
#   - primary_metrics_canonical may only contain keys that:
#       (a) validate under FIX2D48 grammar
#       (b) exist in metric_schema_frozen (schema allowlist)
#       (c) have a non-"unknown" dimension compatible with schema
#   - dict key is the canonical key; metric["canonical_key"] is set to the same key.
#
# Enablement:
#   Gate runs when any of these is true:
#     - web_context["diag_fix2d50_gate"] = True
#     - web_context["enforce_schema_bound_pmc"] = True
#     - web_context["diag_fix2d49_rekey"] = True   (rekey implies intent to canonicalize)
#
# Strictness:
#   - If web_context["diag_fix2d50_strict"] or web_context["diag_fix2d49_strict"] is True,
#     the gate raises if it drops any PMC entries or finds invalid keys.
#
# Output:
#   - Writes debug.fix2d50_pmc_gate = {kept, dropped, dropped_examples, strict, enabled}
# =========================================================

def _fix2d50_should_gate(web_context: dict | None) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        return bool(
            web_context.get("diag_fix2d50_gate")
            or web_context.get("enforce_schema_bound_pmc")
            or web_context.get("diag_fix2d49_rekey")
        )
    except Exception:
        return False

def _fix2d50_is_strict(web_context: dict | None) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        return bool(web_context.get("diag_fix2d50_strict") or web_context.get("diag_fix2d49_strict"))
    except Exception:
        return False

def _fix2d50_get_dimension_from_metric(m: dict) -> str:
    if not isinstance(m, dict):
        return ""
    for k in ("dimension", "dim", "metric_dimension"):
        v = m.get(k)
        if isinstance(v, str) and v.strip():
            return _fix2d48_norm_token(v)
    return ""

def _fix2d50_get_schema_dimension(metric_schema_frozen: dict, ckey: str) -> str:
    try:
        spec = metric_schema_frozen.get(ckey)
        if isinstance(spec, dict):
            d = spec.get("dimension")
            if isinstance(d, str) and d.strip():
                return _fix2d48_norm_token(d)
    except Exception:
        return ""

def _fix2d50_gate_primary_metrics_canonical(pmc: dict, metric_schema_frozen: dict, web_context: dict | None) -> tuple[dict, dict]:
    report = {
        "enabled": True,
        "strict": _fix2d50_is_strict(web_context),
        "total_in": 0,
        "kept": 0,
        "dropped": 0,
        "dropped_examples": [],
        "reasons": {},  # reason -> count
    }
    if not isinstance(pmc, dict) or not pmc:
        report["enabled"] = True
        report["total_in"] = 0
        return pmc, report
    if not isinstance(metric_schema_frozen, dict) or not metric_schema_frozen:
        # Can't enforce allowlist if schema absent; keep but record.
        report["enabled"] = True
        report["note"] = "metric_schema_frozen missing/empty; gate skipped"
        report["total_in"] = len(pmc)
        report["kept"] = len(pmc)
        return pmc, report

    allowed_dims = _fix2d48_allowed_dimensions_from_schema(metric_schema_frozen)  # normalized set or None

    out = {}
    for k in sorted(pmc.keys(), key=lambda x: str(x)):
        report["total_in"] += 1
        key = str(k)

        # (a) grammar validation
        try:
            validate_canonical_key_v1(key, allowed_dimensions=allowed_dims)
        except Exception as e:
            reason = "invalid_grammar"
            report["reasons"][reason] = report["reasons"].get(reason, 0) + 1
            report["dropped"] += 1
            if len(report["dropped_examples"]) < 10:
                report["dropped_examples"].append({"key": key, "reason": reason, "error": str(e)})
            continue

        # (b) schema allowlist
        if key not in metric_schema_frozen:
            reason = "not_in_schema"
            report["reasons"][reason] = report["reasons"].get(reason, 0) + 1
            report["dropped"] += 1
            if len(report["dropped_examples"]) < 10:
                report["dropped_examples"].append({"key": key, "reason": reason})
            continue

        m = pmc.get(k)
        if not isinstance(m, dict):
            reason = "non_dict_metric"
            report["reasons"][reason] = report["reasons"].get(reason, 0) + 1
            report["dropped"] += 1
            if len(report["dropped_examples"]) < 10:
                report["dropped_examples"].append({"key": key, "reason": reason})
            continue

        # (c) dimension guard
        md = _fix2d50_get_dimension_from_metric(m)
        sd = _fix2d50_get_schema_dimension(metric_schema_frozen, key)
        # reject unknowns explicitly
        if md == "unknown" or sd == "unknown":
            reason = "unknown_dimension"
            report["reasons"][reason] = report["reasons"].get(reason, 0) + 1
            report["dropped"] += 1
            if len(report["dropped_examples"]) < 10:
                report["dropped_examples"].append({"key": key, "reason": reason, "metric_dim": md, "schema_dim": sd})
            continue
        # If both known and disagree, drop (prevents silent unit/dim leakage)
        if md and sd and md != sd:
            reason = "dimension_mismatch"
            report["reasons"][reason] = report["reasons"].get(reason, 0) + 1
            report["dropped"] += 1
            if len(report["dropped_examples"]) < 10:
                report["dropped_examples"].append({"key": key, "reason": reason, "metric_dim": md, "schema_dim": sd})
            continue

        # Make canonical_key authoritative at boundary
        m["canonical_key"] = key

        out[key] = m
        report["kept"] += 1

    # Strict raising if anything dropped
    if report["strict"] and report["dropped"] > 0:
        raise RuntimeError(f"FIX2D50 strict: dropped {report['dropped']} PMC entries; examples={report['dropped_examples'][:3]}")

    return out, report

def _fix2d50_try_gate_output_obj(output_obj: dict, web_context: dict | None = None) -> None:
    if not isinstance(output_obj, dict):
        return
    if not _fix2d50_should_gate(web_context):
        return

    metric_schema_frozen = (
        output_obj.get("metric_schema_frozen") if isinstance(output_obj.get("metric_schema_frozen"), dict) else None
    ) or (
        output_obj.get("primary_response", {}).get("metric_schema_frozen") if isinstance(output_obj.get("primary_response"), dict) else None
    ) or (
        output_obj.get("results", {}).get("metric_schema_frozen") if isinstance(output_obj.get("results"), dict) else None
    ) or {}

    # Locate PMC in common shapes
    pmc_path = None
    pmc = None
    if isinstance(output_obj.get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_metrics_canonical",)
        pmc = output_obj["primary_metrics_canonical"]
    elif isinstance(output_obj.get("primary_response"), dict) and isinstance(output_obj["primary_response"].get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_response", "primary_metrics_canonical")
        pmc = output_obj["primary_response"]["primary_metrics_canonical"]
    elif isinstance(output_obj.get("results"), dict) and isinstance(output_obj["results"].get("primary_metrics_canonical"), dict):
        pmc_path = ("results", "primary_metrics_canonical")
        pmc = output_obj["results"]["primary_metrics_canonical"]

    output_obj.setdefault("debug", {})

    if isinstance(pmc, dict):
        new_pmc, rep = _fix2d50_gate_primary_metrics_canonical(pmc, metric_schema_frozen, web_context)
        output_obj["debug"]["fix2d50_pmc_gate"] = rep

        if pmc_path == ("primary_metrics_canonical",):
            output_obj["primary_metrics_canonical"] = new_pmc
        elif pmc_path == ("primary_response","primary_metrics_canonical"):
            output_obj["primary_response"]["primary_metrics_canonical"] = new_pmc
        elif pmc_path == ("results","primary_metrics_canonical"):
            output_obj["results"]["primary_metrics_canonical"] = new_pmc
    else:
        output_obj["debug"]["fix2d50_pmc_gate"] = {"enabled": True, "note": "primary_metrics_canonical not found"}

# =========================================================
# END FIX2D50
# =========================================================







# =========================================================
# FIX2D49 — FINAL VERSION STAMP OVERRIDE
# =========================================================
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
# =========================================================
# FIX2D52 — Schema-first canonical key resolution (binder)
# =========================================================
# Objective:
#   Close the canonical-key convergence gap by deterministically binding PMC metrics
#   to existing schema keys (metric_schema_frozen) BEFORE the FIX2D50 gate runs.
#
# Mechanism:
#   - For each metric in primary_metrics_canonical, attempt to match a schema key using:
#       * schema keywords overlap (authoritative allowlist)
#       * dimension compatibility (prefer schema dimension; allow unknown->schema)
#       * time-qualifier hint extracted from current key/name (year/ytd/asof/range)
#   - If a high-confidence match is found, rekey the PMC entry to that schema key and
#     set metric["canonical_key"] = schema_key and metric["dimension"] = schema_dimension (if unknown).
#
# Enablement (recommended ON for now):
#   - web_context["diag_fix2d52_bind"] = True
#   - OR web_context["enforce_schema_bound_pmc"] = True
#
# Strictness:
#   - If web_context["diag_fix2d52_strict"] is True, raise if any metric cannot be bound
#     AND is not already a valid schema key.
#
# Output:
#   - debug.fix2d52_schema_bind = {total, bound, already_schema, unbound, examples}
# =========================================================

def _fix2d52_should_bind(web_context: dict | None) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        return bool(web_context.get("diag_fix2d52_bind") or web_context.get("enforce_schema_bound_pmc"))
    except Exception:
        return False

def _fix2d52_is_strict(web_context: dict | None) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        return bool(web_context.get("diag_fix2d52_strict"))
    except Exception:
        return False

def _fix2d52_norm_text(s: str) -> str:
    s = (s or "").lower()
    s = re.sub(r"[^a-z0-9]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _fix2d52_extract_time_hint(s: str) -> str:
    """
    Return a time qualifier hint in canonical form if found (best-effort):
      - ytd_YYYY
      - YYYY
      - YYYY_YYYY (range)
    """
    s2 = (s or "").lower()
    # ytd patterns
    m = re.search(r"\b(ytd|year to date|year-to-date)\s*(20\d{2})\b", s2)
    if m:
        return f"ytd_{m.group(2)}"
    # range patterns (2026-2040 etc)
    m = re.search(r"\b(20\d{2})\s*(?:-|–|to)\s*(20\d{2})\b", s2)
    if m:
        a, b = m.group(1), m.group(2)
        if int(b) >= int(a):
            return f"{a}_{b}"
    # single year
    m = re.search(r"\b(20\d{2})\b", s2)
    if m:
        return m.group(1)
    return ""

def _fix2d52_schema_candidates(metric_schema_frozen: dict) -> list:
    cands = []
    if not isinstance(metric_schema_frozen, dict):
        return cands
    for skey, spec in metric_schema_frozen.items():
        if not isinstance(skey, str) or "__" not in skey:
            continue
        if not isinstance(spec, dict):
            spec = {}
        kw = spec.get("keywords")
        if isinstance(kw, list):
            keywords = [_fix2d52_norm_text(str(x)) for x in kw if str(x).strip()]
        else:
            keywords = []
        dim = spec.get("dimension")
        dim = _fix2d48_norm_token(str(dim)) if dim else ""
        # also build a normalized name bag from metric_name/name and the key itself
        nm = spec.get("metric_name") or spec.get("name") or ""
        name_norm = _fix2d52_norm_text(str(nm))
        key_norm = _fix2d52_norm_text(skey.replace("__", " "))
        cands.append({
            "skey": skey,
            "dimension": dim,
            "keywords": keywords,
            "name_norm": name_norm,
            "key_norm": key_norm,
            "time_hint": _fix2d52_extract_time_hint(skey),
        })
    return cands

def _fix2d52_score_match(metric: dict, dict_key: str, schema_cand: dict) -> float:
    # Base: keyword overlap
    text_fields = []
    if isinstance(metric, dict):
        for k in ("name", "metric_name", "label", "title", "snippet", "raw_text"):
            v = metric.get(k)
            if isinstance(v, str) and v.strip():
                text_fields.append(v)
    text_fields.append(dict_key or "")
    blob = _fix2d52_norm_text(" ".join(text_fields))
    if not blob:
        return 0.0

    blob_set = set(blob.split(" "))
    kw = schema_cand.get("keywords") or []
    kw_hits = 0
    for k in kw:
        # keyword list may contain multi-word phrases; count a hit if all tokens present
        toks = k.split(" ")
        if toks and all(t in blob_set for t in toks):
            kw_hits += 1

    # Name/key soft overlap
    name_norm = schema_cand.get("name_norm") or ""
    key_norm = schema_cand.get("key_norm") or ""
    name_hits = 0
    for t in name_norm.split(" "):
        if t and t in blob_set:
            name_hits += 0.2  # soft
    for t in key_norm.split(" "):
        if t and t in blob_set:
            name_hits += 0.05  # very soft

    # Dimension compatibility
    sd = schema_cand.get("dimension") or ""
    md = _fix2d50_get_dimension_from_metric(metric) if isinstance(metric, dict) else ""
    dim_score = 0.0
    if sd and md:
        if md == "unknown":
            dim_score = 0.4
        elif md == sd:
            dim_score = 0.8
        else:
            dim_score = -1.0  # hard penalty
    elif sd and not md:
        dim_score = 0.2

    # Time qualifier hint
    mh = _fix2d52_extract_time_hint(blob)
    th = schema_cand.get("time_hint") or ""
    time_score = 0.0
    if mh and th and mh == th:
        time_score = 0.6
    elif mh and th and (mh in th or th in mh):
        time_score = 0.2

    score = (kw_hits * 1.0) + name_hits + dim_score + time_score
    return float(score)

def _fix2d52_bind_pmc_to_schema(pmc: dict, metric_schema_frozen: dict, web_context: dict | None) -> tuple[dict, dict]:
    report = {
        "enabled": True,
        "total": 0,
        "already_schema": 0,
        "bound": 0,
        "unbound": 0,
        "examples": [],
        "threshold": 2.0,
    }
    if not isinstance(pmc, dict) or not pmc:
        report["enabled"] = True
        return pmc, report
    if not isinstance(metric_schema_frozen, dict) or not metric_schema_frozen:
        report["enabled"] = True
        report["note"] = "metric_schema_frozen missing; bind skipped"
        return pmc, report

    # Precompute schema candidates once
    cands = _fix2d52_schema_candidates(metric_schema_frozen)
    thresh = float(web_context.get("diag_fix2d52_threshold") or report["threshold"]) if isinstance(web_context, dict) else report["threshold"]
    report["threshold"] = thresh

    allowed_dims = _fix2d48_allowed_dimensions_from_schema(metric_schema_frozen) or None

    out = {}
    strict = _fix2d52_is_strict(web_context)

    for old_key in sorted(pmc.keys(), key=lambda x: str(x)):
        report["total"] += 1
        key = str(old_key)
        m = pmc.get(old_key)
        if not isinstance(m, dict):
            continue

        # If already a valid schema key, keep.
        if key in metric_schema_frozen:
            out[key] = m
            m["canonical_key"] = key
            report["already_schema"] += 1
            continue

        # If key is grammatically invalid, we still attempt schema bind (that's the whole point).
        best = None
        best_score = -1e9
        for cand in cands:
            s = _fix2d52_score_match(m, key, cand)
            if s > best_score:
                best_score = s
                best = cand

        if best is not None and best_score >= thresh:
            skey = best["skey"]
            # Validate schema key (should always validate, but keep safe)
            try:
                validate_canonical_key_v1(skey, allowed_dimensions=allowed_dims)
            except Exception:
                pass
                # If somehow invalid, treat as unbound
                best_score = -1e9
                best = None
            else:
                # Apply binding
                m["canonical_key"] = skey
                # If dimension unknown, adopt schema dimension
                md = _fix2d50_get_dimension_from_metric(m)
                sd = best.get("dimension") or ""
                if (not md) or md == "unknown":
                    if sd:
                        m["dimension"] = sd
                out[skey] = _fix2d47_pick_cur_winner(out.get(skey), m)
                report["bound"] += 1
                if len(report["examples"]) < 10:
                    report["examples"].append({"from": key, "to": skey, "score": round(best_score, 3)})
                continue

        # Unbound: keep original in out (so downstream audit can see it), unless strict.
        report["unbound"] += 1
        if strict:
            raise RuntimeError(f"FIX2D52 strict: could not bind PMC key '{key}' to schema (best_score={best_score})")
        out[key] = m

    return out, report

def _fix2d52_try_bind_output_obj(output_obj: dict, web_context: dict | None = None) -> None:
    if not isinstance(output_obj, dict):
        return
    if not _fix2d52_should_bind(web_context):
        return

    metric_schema_frozen = (
        output_obj.get("metric_schema_frozen") if isinstance(output_obj.get("metric_schema_frozen"), dict) else None
    ) or (
        output_obj.get("primary_response", {}).get("metric_schema_frozen") if isinstance(output_obj.get("primary_response"), dict) else None
    ) or (
        output_obj.get("results", {}).get("metric_schema_frozen") if isinstance(output_obj.get("results"), dict) else None
    ) or {}

    # Locate PMC in common shapes
    pmc_path = None
    pmc = None
    if isinstance(output_obj.get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_metrics_canonical",)
        pmc = output_obj["primary_metrics_canonical"]
    elif isinstance(output_obj.get("primary_response"), dict) and isinstance(output_obj["primary_response"].get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_response", "primary_metrics_canonical")
        pmc = output_obj["primary_response"]["primary_metrics_canonical"]
    elif isinstance(output_obj.get("results"), dict) and isinstance(output_obj["results"].get("primary_metrics_canonical"), dict):
        pmc_path = ("results", "primary_metrics_canonical")
        pmc = output_obj["results"]["primary_metrics_canonical"]

    output_obj.setdefault("debug", {})

    if isinstance(pmc, dict):
        new_pmc, rep = _fix2d52_bind_pmc_to_schema(pmc, metric_schema_frozen, web_context)
        output_obj["debug"]["fix2d52_schema_bind"] = rep

        if pmc_path == ("primary_metrics_canonical",):
            output_obj["primary_metrics_canonical"] = new_pmc
        elif pmc_path == ("primary_response","primary_metrics_canonical"):
            output_obj["primary_response"]["primary_metrics_canonical"] = new_pmc
        elif pmc_path == ("results","primary_metrics_canonical"):
            output_obj["results"]["primary_metrics_canonical"] = new_pmc
    else:
        output_obj["debug"]["fix2d52_schema_bind"] = {"enabled": True, "note": "primary_metrics_canonical not found"}

# =========================================================
# END FIX2D52
# =========================================================

# =========================================================
# FIX2D53 — Legacy->Schema Baseline Remap (Option A)
# =========================================================
# Objective:
#   Allow schema-bound diffing to activate even when Analysis produced legacy/text-derived keys,
#   by deterministically remapping a small set of known legacy key shapes onto existing schema keys.
#
# Rationale:
#   - Analysis may emit keys like "2025_global_ev_sales__unknown"
#   - Schema key is "global_ev_sales_ytd_2025__unit_sales"
#   - Without remap, prev/cur overlap is zero -> both_count stays 0.
#
# Policy:
#   - Only remap when the destination schema key exists in metric_schema_frozen.
#   - Only remap when the source key fails schema allowlisting OR has dimension "unknown".
#   - Remap is deterministic and narrow (pattern-based); it does NOT invoke any LLM/NLP.
#
# Enablement:
#   - web_context["diag_fix2d53_remap"] = True
#   - OR web_context["enforce_schema_bound_pmc"] = True
#
# Output:
#   - debug.fix2d53_legacy_remap = {enabled,total,mapped,examples}
# =========================================================

def _fix2d53_should_remap(web_context: dict | None) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        return bool(web_context.get("diag_fix2d53_remap") or web_context.get("enforce_schema_bound_pmc"))
    except Exception:
        return False

def _fix2d53_extract_year_from_key(ckey: str) -> str:
    if not isinstance(ckey, str):
        return ""
    m = re.search(r"\b(20\d{2})\b", ckey)
    return m.group(1) if m else ""

def _fix2d53_schema_key_exists(metric_schema_frozen: dict, skey: str) -> bool:
    return isinstance(metric_schema_frozen, dict) and isinstance(skey, str) and skey in metric_schema_frozen

def _fix2d53_schema_dim(metric_schema_frozen: dict, skey: str) -> str:
    try:
        spec = metric_schema_frozen.get(skey)
        if isinstance(spec, dict):
            d = spec.get("dimension")
            if isinstance(d, str) and d.strip():
                return _fix2d48_norm_token(d)
    except Exception:
        return ""

def _fix2d53_apply_legacy_schema_remaps(pmc: dict, metric_schema_frozen: dict, web_context: dict | None) -> tuple[dict, dict]:
    rep = {"enabled": True, "total": 0, "mapped": 0, "examples": []}
    if not isinstance(pmc, dict) or not pmc:
        return pmc, rep
    if not isinstance(metric_schema_frozen, dict) or not metric_schema_frozen:
        rep["note"] = "metric_schema_frozen missing; remap skipped"
        return pmc, rep

    out = {}
    for k in sorted(pmc.keys(), key=lambda x: str(x)):
        rep["total"] += 1
        key = str(k)
        m = pmc.get(k)
        if not isinstance(m, dict):
            out[key] = m
            continue

        md = _fix2d50_get_dimension_from_metric(m)

        # Only consider remap for non-schema keys OR unknown dimension
        if key in metric_schema_frozen and md != "unknown":
            out[key] = m
            continue

        year = _fix2d53_extract_year_from_key(key) or _fix2d53_extract_year_from_key(m.get("name",""))

        mapped = False
        dest = None

        # --- Remap Rule 1: global EV sales legacy key -> schema YTD key (demo-enabler)
        # Examples:
        #   "2025_global_ev_sales__unknown" -> "global_ev_sales_ytd_2025__unit_sales"
        if year and re.fullmatch(rf"{year}_global_ev_sales__unknown", key):
            cand = f"global_ev_sales_ytd_{year}__unit_sales"
            if _fix2d53_schema_key_exists(metric_schema_frozen, cand):
                dest = cand
                mapped = True

        # --- Remap Rule 2: china EV sales legacy key -> schema YTD key if present
        #   "2025_china_ev_sales__unknown" -> "china_ev_sales_ytd_2025__unit_sales"
        if (not mapped) and year and re.fullmatch(rf"{year}_china_ev_sales__unknown", key):
            cand = f"china_ev_sales_ytd_{year}__unit_sales"
            if _fix2d53_schema_key_exists(metric_schema_frozen, cand):
                dest = cand
                mapped = True

        if mapped and dest:
            # adopt schema dimension if unknown/missing
            sd = _fix2d53_schema_dim(metric_schema_frozen, dest)
            if (not md) or md == "unknown":
                if sd:
                    m["dimension"] = sd
            m["canonical_key"] = dest

            # collision handling deterministic
            if dest in out and isinstance(out[dest], dict):
                out[dest] = _fix2d47_pick_cur_winner(out[dest], m)
            else:
                out[dest] = m

            rep["mapped"] += 1
            if len(rep["examples"]) < 10:
                rep["examples"].append({"from": key, "to": dest})
            continue

        out[key] = m

    return out, rep

def _fix2d53_try_remap_output_obj(output_obj: dict, web_context: dict | None = None) -> None:
    if not isinstance(output_obj, dict):
        return
    if not _fix2d53_should_remap(web_context):
        return

    metric_schema_frozen = (
        output_obj.get("metric_schema_frozen") if isinstance(output_obj.get("metric_schema_frozen"), dict) else None
    ) or (
        output_obj.get("primary_response", {}).get("metric_schema_frozen") if isinstance(output_obj.get("primary_response"), dict) else None
    ) or (
        output_obj.get("results", {}).get("metric_schema_frozen") if isinstance(output_obj.get("results"), dict) else None
    ) or {}

    pmc_path = None
    pmc = None
    if isinstance(output_obj.get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_metrics_canonical",)
        pmc = output_obj["primary_metrics_canonical"]
    elif isinstance(output_obj.get("primary_response"), dict) and isinstance(output_obj["primary_response"].get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_response", "primary_metrics_canonical")
        pmc = output_obj["primary_response"]["primary_metrics_canonical"]
    elif isinstance(output_obj.get("results"), dict) and isinstance(output_obj["results"].get("primary_metrics_canonical"), dict):
        pmc_path = ("results", "primary_metrics_canonical")
        pmc = output_obj["results"]["primary_metrics_canonical"]

    output_obj.setdefault("debug", {})

    if isinstance(pmc, dict):
        new_pmc, rep = _fix2d53_apply_legacy_schema_remaps(pmc, metric_schema_frozen, web_context)
        output_obj["debug"]["fix2d53_legacy_remap"] = rep

        if pmc_path == ("primary_metrics_canonical",):
            output_obj["primary_metrics_canonical"] = new_pmc
        elif pmc_path == ("primary_response","primary_metrics_canonical"):
            output_obj["primary_response"]["primary_metrics_canonical"] = new_pmc
        elif pmc_path == ("results","primary_metrics_canonical"):
            output_obj["results"]["primary_metrics_canonical"] = new_pmc
    else:
        output_obj["debug"]["fix2d53_legacy_remap"] = {"enabled": True, "note": "primary_metrics_canonical not found"}

# =========================================================
# END FIX2D53
# =========================================================




# =========================================================
# FIX2D52 — FINAL VERSION STAMP OVERRIDE
# =========================================================
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
# =========================================================
# FIX2D54 — Schema Baseline Materialisation (PMC lifting)
# =========================================================
# Objective:
#   Ensure Analysis baseline values are materialised under schema keys so that
#   Evolution can produce BOTH(prev+cur) rows for diffing.
#
# What it does (when enabled):
#   - Scans primary_metrics_canonical (PMC) for legacy keys and/or metrics that
#     imply a destination schema key.
#   - If a destination schema key exists in metric_schema_frozen, it creates/updates
#     PMC[dest_schema_key] using the legacy metric record (value_norm, unit, source, etc.).
#
# Enablement:
#   - web_context["diag_fix2d54_materialize"] = True
#   - OR web_context["enforce_schema_bound_pmc"] = True
#
# Safety:
#   - Does NOT invent values; only re-homes existing extracted metrics.
#   - Collision handling is deterministic (reuses FIX2D47 picker).
#
# Output:
#   - debug.fix2d54_materialize = {enabled,total,created,updated,skipped,examples}
# =========================================================

def _fix2d54_should_materialize(web_context: dict | None) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        return bool(web_context.get("diag_fix2d54_materialize") or web_context.get("enforce_schema_bound_pmc"))
    except Exception:
        return False

def _fix2d54_guess_dest_schema_key(legacy_key: str, metric: dict, metric_schema_frozen: dict) -> str:
    """
    Deterministic, narrow remap guesses:
      - <YYYY>_global_ev_sales__unknown -> global_ev_sales_ytd_<YYYY>__unit_sales
      - <YYYY>_china_ev_sales__unknown  -> china_ev_sales_ytd_<YYYY>__unit_sales (if present)
    Plus: if metric has canonical_key/schema_key that exists in schema, use it.
    """
    if isinstance(metric, dict):
        for kk in ("canonical_key", "schema_key", "schema_canonical_key", "schema_ckey"):
            v = metric.get(kk)
            if isinstance(v, str) and v in metric_schema_frozen:
                return v

    key = str(legacy_key or "")
    year = ""
    m = re.search(r"\b(20\d{2})\b", key)
    if m:
        year = m.group(1)

    if year and re.fullmatch(rf"{year}_global_ev_sales__unknown", key):
        cand = f"global_ev_sales_ytd_{year}__unit_sales"
        if cand in metric_schema_frozen:
            return cand

    if year and re.fullmatch(rf"{year}_china_ev_sales__unknown", key):
        cand = f"china_ev_sales_ytd_{year}__unit_sales"
        if cand in metric_schema_frozen:
            return cand

    return ""

def _fix2d54_materialize_schema_baseline(pmc: dict, metric_schema_frozen: dict, web_context: dict | None) -> tuple[dict, dict]:
    rep = {"enabled": True, "total": 0, "created": 0, "updated": 0, "skipped": 0, "examples": []}
    if not isinstance(pmc, dict) or not pmc:
        return pmc, rep
    if not isinstance(metric_schema_frozen, dict) or not metric_schema_frozen:
        rep["note"] = "metric_schema_frozen missing; materialize skipped"
        return pmc, rep

    out = dict(pmc)  # copy
    for k in sorted(pmc.keys(), key=lambda x: str(x)):
        rep["total"] += 1
        key = str(k)
        m = pmc.get(k)
        if not isinstance(m, dict):
            rep["skipped"] += 1
            continue

        dest = _fix2d54_guess_dest_schema_key(key, m, metric_schema_frozen)
        if not dest or dest == key:
            rep["skipped"] += 1
            continue

        # Adopt schema dimension if metric dimension unknown/missing
        md = _fix2d50_get_dimension_from_metric(m)
        sd = _fix2d53_schema_dim(metric_schema_frozen, dest) if " _fix2d53_schema_dim" else ""
        if (not md) or md == "unknown":
            if sd:
                m["dimension"] = sd

        m["canonical_key"] = dest

        if dest in out and isinstance(out[dest], dict):
            out[dest] = _fix2d47_pick_cur_winner(out[dest], m)
            rep["updated"] += 1
        else:
            out[dest] = m
            rep["created"] += 1

        if len(rep["examples"]) < 10:
            rep["examples"].append({"from": key, "to": dest})

    return out, rep

def _fix2d54_try_materialize_output_obj(output_obj: dict, web_context: dict | None = None) -> None:
    if not isinstance(output_obj, dict):
        return
    if not _fix2d54_should_materialize(web_context):
        return

    metric_schema_frozen = (
        output_obj.get("metric_schema_frozen") if isinstance(output_obj.get("metric_schema_frozen"), dict) else None
    ) or (
        output_obj.get("primary_response", {}).get("metric_schema_frozen") if isinstance(output_obj.get("primary_response"), dict) else None
    ) or (
        output_obj.get("results", {}).get("metric_schema_frozen") if isinstance(output_obj.get("results"), dict) else None
    ) or {}

    pmc_path = None
    pmc = None
    if isinstance(output_obj.get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_metrics_canonical",)
        pmc = output_obj["primary_metrics_canonical"]
    elif isinstance(output_obj.get("primary_response"), dict) and isinstance(output_obj["primary_response"].get("primary_metrics_canonical"), dict):
        pmc_path = ("primary_response", "primary_metrics_canonical")
        pmc = output_obj["primary_response"]["primary_metrics_canonical"]
    elif isinstance(output_obj.get("results"), dict) and isinstance(output_obj["results"].get("primary_metrics_canonical"), dict):
        pmc_path = ("results", "primary_metrics_canonical")
        pmc = output_obj["results"]["primary_metrics_canonical"]

    output_obj.setdefault("debug", {})

    if isinstance(pmc, dict):
        new_pmc, rep = _fix2d54_materialize_schema_baseline(pmc, metric_schema_frozen, web_context)
        output_obj["debug"]["fix2d54_materialize"] = rep

        if pmc_path == ("primary_metrics_canonical",):
            output_obj["primary_metrics_canonical"] = new_pmc
        elif pmc_path == ("primary_response","primary_metrics_canonical"):
            output_obj["primary_response"]["primary_metrics_canonical"] = new_pmc
        elif pmc_path == ("results","primary_metrics_canonical"):
            output_obj["results"]["primary_metrics_canonical"] = new_pmc
    else:
        output_obj["debug"]["fix2d54_materialize"] = {"enabled": True, "note": "primary_metrics_canonical not found"}

# =========================================================
# END FIX2D54
# =========================================================
# =========================================================
# FIX2D55 — Pre-Diff Schema Lift for Prev Full Payload
# =========================================================
# Objective:
#   Ensure the rehydrated previous analysis payload (prev_full) has schema-keyed
#   baseline values materialised BEFORE compute_source_anchored_diff runs.
#
# Why:
#   Even if Analysis produced legacy keys, Evolution diff is prev-key driven; if the
#   schema key has no prev_value_norm, it becomes "added" and both_count stays 0.
#
# What it does:
#   - Applies FIX2D53 remap + FIX2D54 materialisation + FIX2D52 bind + FIX2D50 gate
#     to prev_full (in-place) just before diff computation.
#
# Enablement:
#   - web_context["diag_fix2d55_prev_lift"] = True
#   - OR web_context["diag_fix2d54_materialize"] / ["diag_fix2d53_remap"] / ["diag_fix2d52_bind"]
#     / ["diag_fix2d50_gate"] are enabled (any of them).
#
# Output:
#   - web_context.debug.fix2d55_prev_lift
# =========================================================

def _fix2d55_should_prev_lift(web_context: dict | None) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        if web_context.get("diag_fix2d55_prev_lift"):
            return True
        # FIX2D56: auto-enable prev-lift when injection present
        if _fix2d56_should_enable(web_context):
            return True
        # If any of the schema-bound pipeline flags are enabled, lift prev as well.
        for k in (
            "diag_fix2d54_materialize",
            "diag_fix2d53_remap",
            "diag_fix2d52_bind",
            "diag_fix2d50_gate",
            "enforce_schema_bound_pmc",
        ):
            if web_context.get(k):
                return True
        return False
    except Exception:
        return False

def _fix2d55_apply_prev_lift(prev_full: dict, web_context: dict | None) -> None:
    if not isinstance(prev_full, dict):
        return
    if not _fix2d55_should_prev_lift(web_context):
        return

    try:
        # Reuse the same postprocess logic you run on outputs, but target prev_full explicitly.
        # Ensure a debug bucket exists on web_context for quick visibility.
        if isinstance(web_context, dict):
            web_context.setdefault("debug", {})
            if isinstance(web_context.get("debug"), dict):
                web_context["debug"].setdefault("fix2d55_prev_lift", {"enabled": True})

        # Apply in-place, in the same order as the output postprocess.
        # 1) FIX2D53 remap (if present)
        try:
            _fix2d53_try_remap_output_obj(prev_full, web_context)
        except Exception:
            pass
            # keep going; remap is best-effort
            if isinstance(web_context, dict) and isinstance(web_context.get("debug"), dict):
                web_context["debug"]["fix2d55_prev_lift"]["remap_error"] = True

        # 2) FIX2D52 bind (if present)
        try:
            _fix2d52_try_bind_output_obj(prev_full, web_context)
        except Exception:
            pass
            if isinstance(web_context, dict) and isinstance(web_context.get("debug"), dict):
                web_context["debug"]["fix2d55_prev_lift"]["bind_error"] = True

        # 3) FIX2D54 materialise (if present)
        try:
            _fix2d54_try_materialize_output_obj(prev_full, web_context)
        except Exception:
            pass
            if isinstance(web_context, dict) and isinstance(web_context.get("debug"), dict):
                web_context["debug"]["fix2d55_prev_lift"]["materialize_error"] = True

        # 4) FIX2D50 gate (if present)
        try:
            _fix2d50_try_gate_output_obj(prev_full, web_context)
        except Exception:
            pass
            if isinstance(web_context, dict) and isinstance(web_context.get("debug"), dict):
                web_context["debug"]["fix2d55_prev_lift"]["gate_error"] = True

        # Capture quick counters if available in prev_full.debug
        if isinstance(web_context, dict) and isinstance(web_context.get("debug"), dict):
            _rep = web_context["debug"].get("fix2d55_prev_lift") or {}
            if isinstance(_rep, dict):
                _rep["done"] = True
                # Pull embedded reports if they were attached onto prev_full.debug
                pd = prev_full.get("debug") if isinstance(prev_full.get("debug"), dict) else {}
                if isinstance(pd, dict):
                    for k in ("fix2d53_legacy_remap", "fix2d54_materialize", "fix2d50_pmc_gate", "fix2d52_schema_bind"):
                        if k in pd:
                            _rep[k] = pd.get(k)
                web_context["debug"]["fix2d55_prev_lift"] = _rep

    except Exception:
        pass
        # never break evolution; this is a best-effort lift
        if isinstance(web_context, dict) and isinstance(web_context.get("debug"), dict):
            web_context["debug"].setdefault("fix2d55_prev_lift", {})
            if isinstance(web_context["debug"].get("fix2d55_prev_lift"), dict):
                web_context["debug"]["fix2d55_prev_lift"]["crashed"] = True

# =========================================================
# END FIX2D55


# =========================================================
# FIX2D56 — Force-fetch injected URLs in Evolution + auto prev-lift
# =========================================================
# A) Force-fetch injected URLs:
#   Evolution previously recorded injected URLs as admitted but never fetched them,
#   leaving inj_trace_v1.attempted empty. When injection is present, we now force
#   a fetch_web_context(... identity_only=False, force_scrape_extra_urls=True,
#   force_admit_extra_urls=True) call.
#
# B) Auto-enable prev_full lifting:
#   When injection is present, automatically enable FIX2D55 prev_lift (schema remap/bind/materialise)
#   so Analysis baseline can participate under schema keys without requiring extra flags.
#
# Enablement:
#   - Always-on when injection is present.
#   - Optional explicit opt-in via web_context['diag_fix2d56'].


def _fix2d56_has_injection(web_context: dict | None) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        if isinstance(web_context.get('extra_urls'), (list, tuple)) and len(web_context.get('extra_urls') or []) > 0:
            return True
        raw = web_context.get('diag_extra_urls_ui_raw') or web_context.get('extra_urls_ui_raw')
        return isinstance(raw, str) and raw.strip() != ''
    except Exception:
        return False


def _fix2d56_should_enable(web_context: dict | None) -> bool:
    try:
        if not isinstance(web_context, dict):
            return False
        if web_context.get('diag_fix2d56') or web_context.get('enforce_schema_bound_pmc'):
            return True
        return _fix2d56_has_injection(web_context)
    except Exception:
        return False

# =========================================================
# END FIX2D56
# =========================================================

# =========================================================



# =========================================================
# FIX2D54 — FINAL VERSION STAMP OVERRIDE
# =========================================================
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
# =========================================================
# FIX2D57 — Analysis-side Schema Baseline Materialisation
# =========================================================
# Objective:
#   Ensure the Analysis baseline (Run A) already contains schema-keyed PMC entries with
#   numeric values, so Evolution can diff against Analysis without requiring Analysis injection.
#
# Problem observed:
#   Analysis emits legacy/text keys (e.g., "2025_global_ev_sales__unknown") while Evolution emits
#   schema keys (e.g., "global_ev_sales_ytd_2025__unit_sales"). No overlap => both_count=0.
#
# Solution:
#   After run_source_anchored_analysis returns its output object, force-run the schema pipeline:
#     FIX2D53 remap -> FIX2D52 bind -> FIX2D54 materialise -> FIX2D50 gate
#   using a temporary web_context with enforce_schema_bound_pmc=True.
#
# Safety:
#   - Only runs when metric_schema_frozen exists and PMC appears to contain legacy/unknown keys.
#   - Does not invent values; only rehomes extracted metrics onto schema keys.
#
# Output:
#   - debug.fix2d57_analysis_lift
# =========================================================

def _fix2d57_pmc_looks_legacy(pmc):
    if not isinstance(pmc, dict) or not pmc:
        return False
    for k, m in list(pmc.items())[:200]:
        ks = str(k)
        if ks.endswith("__unknown"):
            return True
        if re.match(r"^20\d{2}_.+__unknown$", ks):
            return True
        if isinstance(m, dict):
            d = m.get("dimension")
            if isinstance(d, str) and d.strip().lower() == "unknown":
                return True
    return False

def _fix2d57_force_schema_pipeline(output_obj, web_context):
    if not isinstance(output_obj, dict):
        return

    metric_schema_frozen = (
        output_obj.get("metric_schema_frozen") if isinstance(output_obj.get("metric_schema_frozen"), dict) else None
    ) or (
        output_obj.get("primary_response", {}).get("metric_schema_frozen") if isinstance(output_obj.get("primary_response"), dict) else None
    ) or (
        output_obj.get("results", {}).get("metric_schema_frozen") if isinstance(output_obj.get("results"), dict) else None
    ) or {}

    pmc = None
    if isinstance(output_obj.get("primary_metrics_canonical"), dict):
        pmc = output_obj["primary_metrics_canonical"]
    elif isinstance(output_obj.get("primary_response"), dict) and isinstance(output_obj["primary_response"].get("primary_metrics_canonical"), dict):
        pmc = output_obj["primary_response"]["primary_metrics_canonical"]
    elif isinstance(output_obj.get("results"), dict) and isinstance(output_obj["results"].get("primary_metrics_canonical"), dict):
        pmc = output_obj["results"]["primary_metrics_canonical"]

    output_obj.setdefault("debug", {})

    if not isinstance(metric_schema_frozen, dict) or not metric_schema_frozen:
        output_obj["debug"]["fix2d57_analysis_lift"] = {"enabled": True, "ran": False, "reason": "metric_schema_frozen_missing"}
        return
    if not isinstance(pmc, dict) or not pmc:
        output_obj["debug"]["fix2d57_analysis_lift"] = {"enabled": True, "ran": False, "reason": "pmc_missing"}
        return
    if not _fix2d57_pmc_looks_legacy(pmc):
        output_obj["debug"]["fix2d57_analysis_lift"] = {"enabled": True, "ran": False, "reason": "pmc_not_legacy"}
        return

    wc2 = dict(web_context) if isinstance(web_context, dict) else {}
    wc2["enforce_schema_bound_pmc"] = True
    wc2["diag_fix2d53_remap"] = True
    wc2["diag_fix2d52_bind"] = True
    wc2["diag_fix2d54_materialize"] = True
    wc2["diag_fix2d50_gate"] = True

    # Apply in order (in-place on output_obj)
    _fix2d53_try_remap_output_obj(output_obj, wc2)
    _fix2d52_try_bind_output_obj(output_obj, wc2)
    _fix2d54_try_materialize_output_obj(output_obj, wc2)
    _fix2d50_try_gate_output_obj(output_obj, wc2)

    rep = {"enabled": True, "ran": True}
    dbg = output_obj.get("debug") if isinstance(output_obj.get("debug"), dict) else {}
    if isinstance(dbg, dict):
        for k in ("fix2d53_legacy_remap", "fix2d52_schema_bind", "fix2d54_materialize", "fix2d50_pmc_gate"):
            if k in dbg:
                rep[k] = dbg.get(k)
    output_obj["debug"]["fix2d57_analysis_lift"] = rep

# =========================================================
# END FIX2D57
# =========================================================


# =========================================================
# FIX2D57 — FINAL VERSION STAMP OVERRIDE
# =========================================================
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK

# =========================================================
# FIX2D57B — FINAL VERSION STAMP OVERRIDE
# =========================================================
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK


# =========================
# FINAL VERSION STAMP: FIX2D65 (last-wins)
# =========================
try:
    globals()["CODE_VERSION"] = "FIX2D65"
except Exception:
    pass

# =====================================================================
# PATCH FIX2D65 (AUTHORITY TAKEOVER): Canonical Identity Spine V1 becomes the only authority
# - Rewire schema-first identity resolution to use canonical_identity_spine.resolve_key_v1
# - Enforce "no canonical outside spine" at the Analysis schema-bound commit split
# - Harden Evolution current selection by pruning yearlike candidates even when unit evidence is WINDOW_BACKFILL
#   (prevents year headings from ever being selected for unit/count metrics)
# - Deterministic, auditable: stamp authority + trace blocks on every resolved metric
# =====================================================================

# Patch tracker entry
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D65",
        "date": "2026-01-19",
        "summary": "Authority takeover: route schema-first identity resolution through canonical_identity_spine (single authority), enforce no-canonical-outside-spine at Analysis commit split, and prune yearlike candidates (immune to WINDOW_BACKFILL) before Evolution selector selection.",
        "files": ["canonical_identity_spine.py", "FIX2D65_full_codebase.py"],
        "supersedes": ["FIX2D64"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# Ensure spine module is importable
try:
    import canonical_identity_spine as _cis
    _FIX2D65_SPINE_OK = True
except Exception:
    pass
    _cis = None
    _FIX2D65_SPINE_OK = False


def _fix2d65_build_schema_index_v1(metric_schema: dict) -> dict:
    """Build schema_index for spine resolver: maps <prefix>__<dim> -> canonical_key.

    Also builds a limited bare-token fallback map only when unique per (metric_token, dim).
    """
    idx = {}
    # track bare token collisions
    bare_counts = {}
    bare_last = {}

    if not isinstance(metric_schema, dict) or not metric_schema:
        return idx

    for skey, spec in metric_schema.items():
        if not isinstance(skey, str) or '__' not in skey:
            continue
        prefix, dim = skey.split('__', 1)
        dim = (dim or '').strip().lower()
        k = f"{prefix}__{dim}" if dim else skey
        idx[k] = skey

        # compute bare token (strip trailing time_scope if present)
        try:
            # Use spine's metric-token normalizer (removes time tokens) if available
            if _FIX2D65_SPINE_OK:
                tmp = _cis.build_identity_tuple_v1({"metric_token": prefix, "dimension": dim, "unit_family": (spec or {}).get("unit_family") or ""}, context={})
                bare = f"{tmp.metric_token}__{dim}" if dim else tmp.metric_token
            else:
                bare = f"{prefix.split('_')[0]}__{dim}" if dim else prefix
        except Exception:
            pass
            bare = ''

        if bare:
            bare_counts[bare] = int(bare_counts.get(bare, 0)) + 1
            bare_last[bare] = skey

    # Add bare fallbacks only if unique (prevents time-scope misbinding)
    for bare, cnt in bare_counts.items():
        if cnt == 1:
            idx.setdefault(bare, bare_last.get(bare))

    return idx


def resolve_canonical_identity_v1(identity: dict, metric_schema: dict = None) -> dict:  # noqa: F811
    """FIX2D65 override: schema-first resolver delegates to canonical_identity_spine (single authority)."""
    identity = identity if isinstance(identity, dict) else {}
    metric_schema = metric_schema if isinstance(metric_schema, dict) else {}

    # Under-specified identities remain provisional.
    mt = str(identity.get('metric_token') or '').strip().lower()
    dim = str(identity.get('dimension') or '').strip().lower()
    uf = str(identity.get('unit_family') or '').strip().lower()
    ut = str(identity.get('unit_tag') or '').strip()
    ts = str(identity.get('time_scope') or '').strip().lower()

    if (not mt) or (not dim) or dim == 'unknown' or (not uf and bool(ut)):
        mt_eff = f"{mt}_{ts}" if (mt and ts) else mt
        provisional_key = f"{mt_eff}__{dim or 'unknown'}" if mt_eff else f"__provisional__{dim or 'unknown'}"
        return {
            'canonical_key': provisional_key,
            'bound': False,
            'status': 'PROVISIONAL',
            'matched_schema_key': '',
            'authority': 'spine_v1' if _FIX2D65_SPINE_OK else 'legacy',
            'reason': 'underspecified',
        }

    if not _FIX2D65_SPINE_OK:
        # fallback to legacy behavior (should be rare; keeps runtime safe)
        mt_eff = f"{mt}_{ts}" if (mt and ts) else mt
        direct = f"{mt_eff}__{dim}"
        if direct in metric_schema:
            return {
                'canonical_key': direct,
                'bound': True,
                'status': 'CANONICAL_SCHEMA',
                'matched_schema_key': direct,
                'authority': 'legacy',
                'reason': 'direct',
            }
        return {
            'canonical_key': direct,
            'bound': False,
            'status': 'PROVISIONAL',
            'matched_schema_key': '',
            'authority': 'legacy',
            'reason': 'not_schema_bound',
        }

    schema_index = _fix2d65_build_schema_index_v1(metric_schema)

    # Build spine tuple
    raw = {
        'metric_token': mt,
        'time_scope': ts,
        'dimension': dim,
        'unit_family': uf,
        'unit_tag': ut,
        'geo': identity.get('geo_scope') or '',
    }
    try:
        tup = _cis.normalize_identity_v1(_cis.build_identity_tuple_v1(raw, context={'time_scope': ts}))
        res = _cis.resolve_key_v1(tup, schema_index)
    except Exception as e:
        # Safe fail to provisional
        mt_eff = f"{mt}_{ts}" if (mt and ts) else mt
        direct = f"{mt_eff}__{dim}"
        return {
            'canonical_key': direct,
            'bound': False,
            'status': 'PROVISIONAL',
            'matched_schema_key': '',
            'authority': 'spine_v1',
            'reason': f'spine_error:{e}',
        }

    if res.bound and res.canonical_key:
        return {
            'canonical_key': res.canonical_key,
            'bound': True,
            'status': 'CANONICAL_SCHEMA',
            'matched_schema_key': res.canonical_key,
            'authority': 'spine_v1',
            'reason': res.reason,
            'considered': list(res.considered),
        }

    # Not schema-bound => provisional (but deterministic)
    mt_eff = f"{mt}_{ts}" if (mt and ts) else mt
    direct = f"{mt_eff}__{dim}"
    return {
        'canonical_key': direct,
        'bound': False,
        'status': 'PROVISIONAL',
        'matched_schema_key': '',
        'authority': 'spine_v1',
        'reason': res.reason,
        'considered': list(res.considered),
    }


def rekey_metrics_via_identity_resolver_v1(pmc: dict, metric_schema: dict) -> dict:  # noqa: F811
    """FIX2D65 override: ensure every canonical_key assignment is produced by spine resolver."""
    if not isinstance(pmc, dict):
        return pmc
    metric_schema = metric_schema if isinstance(metric_schema, dict) else {}

    out = {}
    for k, v in pmc.items():
        if not isinstance(v, dict):
            out[k] = v
            continue
        mt = str(v.get('canonical_id') or '').strip().lower() or str(k).split('__')[0].strip().lower()
        _ts = str(v.get('time_scope') or '')
        try:
            fn_norm = globals().get('normalize_metric_token_time_scope_v1')
            if callable(fn_norm):
                mt, _ts = fn_norm(mt, _ts)
        except Exception:
            pass

        dim = str(v.get('dimension') or '').strip().lower() or str(k).split('__')[-1].strip().lower()
        ident = {
            'metric_token': mt,
            'time_scope': str(_ts or ''),
            'geo_scope': str(v.get('geo_scope') or ''),
            'dims': tuple(v.get('dims') or ()),
            'dimension': dim,
            'unit_family': str(v.get('unit_family') or ''),
            'unit_tag': str(v.get('unit_tag') or ''),
            'statistic': str(v.get('statistic') or ''),
            'aggregation': str(v.get('aggregation') or ''),
        }
        res = resolve_canonical_identity_v1(ident, metric_schema)
        new_k = res.get('canonical_key') or k

        vv = dict(v)
        vv.setdefault('debug', {})
        if isinstance(vv.get('debug'), dict):
            vv['debug']['identity_tuple_v1'] = ident
            vv['debug']['identity_resolve_v1'] = res
        vv['canonical_key'] = new_k
        out[new_k] = vv

    return out


def _fix2d60_split_schema_bound_only(pmc: dict):  # noqa: F811
    """FIX2D65 override: enforce 'no canonical outside spine'.

    Only rows with status==CANONICAL_SCHEMA AND authority==spine_v1 may enter primary_metrics_canonical.
    Everything else is quarantined to provisional.
    """
    try:
        if not isinstance(pmc, dict):
            return {}, {}
        bound = {}
        prov = {}
        for k, v in pmc.items():
            if not isinstance(v, dict):
                prov[k] = v
                continue
            dbg = v.get('debug') if isinstance(v.get('debug'), dict) else {}
            res = dbg.get('identity_resolve_v1') if isinstance(dbg.get('identity_resolve_v1'), dict) else {}
            status = str(res.get('status') or '').strip().upper()
            auth = str(res.get('authority') or '').strip().lower()

            if status == 'CANONICAL_SCHEMA' and auth == 'spine_v1':
                bound[k] = v
            else:
                vv = dict(v)
                vv.setdefault('debug', {})
                if isinstance(vv.get('debug'), dict):
                    vv['debug']['quarantined_v1'] = True
                    vv['debug']['quarantine_reason_v1'] = vv['debug'].get('quarantine_reason_v1') or 'not_schema_bound_or_not_spine_authority'
                    vv['debug']['quarantine_status_v1'] = status
                    vv['debug']['quarantine_authority_v1'] = auth
                prov[k] = vv
        return bound, prov
    except Exception:
        return pmc if isinstance(pmc, dict) else {}, {}


def _fix2d65_prune_yearlike_candidates_for_unit_metrics(candidates: list, canonical_key: str) -> tuple:
    """Remove yearlike candidates for unit/count metrics when unit evidence is NONE/WINDOW_BACKFILL.

    Returns (pruned_candidates, debug_info).
    """
    cands = [c for c in (candidates or []) if isinstance(c, dict)]
    dim = ''
    try:
        if isinstance(canonical_key, str) and '__' in canonical_key:
            dim = canonical_key.split('__', 1)[1].strip().lower()
    except Exception:
        pass
        dim = ''

    is_unit = dim in ('unit_sales', 'unit_count', 'count', 'units', 'units_sold')
    if not (_FIX2D65_SPINE_OK and is_unit):
        return cands, {"applied": False, "rejected": 0, "kept": len(cands), "dimension": dim}

    rejected = 0
    pruned = []
    for c in cands:
        v = c.get('value_norm')
        if v is None:
            v = c.get('value')
        if _cis.is_yearlike_value(v):
            strength = _cis.classify_unit_evidence_strength(c)
            if strength in ('NONE', 'WINDOW_BACKFILL'):
                rejected += 1
                continue
        pruned.append(c)

    return pruned, {"applied": True, "rejected": int(rejected), "kept": int(len(pruned)), "dimension": dim}


def _fix2d2x_select_current_for_key(  # noqa: F811
    canonical_key: str,
    spec_in: dict,
    candidates_all: list,
    injected_urls: list,
) -> tuple:
    """FIX2D77 override: percent-schema guardrail.

    Extends the FIX2D65 override by adding a strict eligibility filter for __percent
    schema keys so unitless year-like tokens (e.g., "2040") cannot win selection.

    Rules (for percent schema keys):
      - Candidate must have percent evidence (unit tag contains %/percent OR raw/context indicates percent)
      - If candidate value is year-like (1900-2100 integer) and raw does not contain %/percent, reject.

    Returns (best_candidate_dict_or_None, meta_dict).
    """
    spec = dict(spec_in or {})

    def _n(x):
        try:
            return str(x or "").strip().lower()
        except Exception:
            return ""

    def _requires_percent() -> bool:
        try:
            if isinstance(canonical_key, str) and canonical_key.endswith('__percent'):
                return True
            dim = _n(spec.get('dimension') or '')
            uf = _n(spec.get('unit_family') or '')
            ut = _n(spec.get('unit_tag') or '')
            blob = " ".join([dim, uf, ut])
            return ('percent' in blob) or ('%' in blob)
        except Exception:
            return False

    def _is_yearlike_candidate(c: dict) -> bool:
        try:
            v = c.get('value_norm')
            if v is None:
                v = c.get('value')
            fv = float(v)
            if not fv.is_integer():
                return False
            iv = int(fv)
            return 1900 <= iv <= 2100
        except Exception:
            return False

    def _has_percent_evidence(c: dict) -> bool:
        try:
            raw = _n(c.get('raw'))
            if '%' in raw or 'percent' in raw:
                return True
            for k in ('unit_tag', 'unit', 'base_unit', 'unit_cmp', 'unit_norm'):
                v = _n(c.get(k))
                if '%' in v or 'percent' in v:
                    return True
            # weaker signal
            uf = _n(c.get('unit_family'))
            if uf == 'percent':
                return True
        except Exception:
            return False
        return False

    # Disable preferred source locking for Evolution
    for k in ("preferred_url", "source_url"):
        if k in spec:
            spec.pop(k, None)

    if not spec.get("keywords"):
        nm = str(spec.get("name") or "")
        spec["keywords"] = globals().get('_fix2d2x_keywords_from_key_and_name', lambda ck, nm: [])(canonical_key, nm)

    # prefilter
    fn_filter = globals().get('_fix2d2x_filter_candidates_for_key')
    if callable(fn_filter):
        candidates_all = fn_filter(canonical_key, spec, candidates_all)

    # FIX2D65 prune step
    candidates_all, prune_dbg = _fix2d65_prune_yearlike_candidates_for_unit_metrics(candidates_all, canonical_key)

    # FIX2D77 percent eligibility filter
    req_pct = _requires_percent()
    fix2d77_dbg = {
        'applied': False,
        'requires_percent': bool(req_pct),
        'before': int(len(candidates_all or [])) if isinstance(candidates_all, list) else 0,
        'kept': int(len(candidates_all or [])) if isinstance(candidates_all, list) else 0,
        'rejected_missing_percent_evidence': 0,
        'rejected_yearlike_no_pct_raw': 0,
    }
    if req_pct and isinstance(candidates_all, list) and candidates_all:
        kept = []
        for c in candidates_all:
            if not isinstance(c, dict):
                continue
            raw = _n(c.get('raw'))
            raw_has_pct = ('%' in raw) or ('percent' in raw)
            if _is_yearlike_candidate(c) and not raw_has_pct:
                fix2d77_dbg['rejected_yearlike_no_pct_raw'] += 1
                continue
            if not _has_percent_evidence(c):
                fix2d77_dbg['rejected_missing_percent_evidence'] += 1
                continue
            kept.append(c)
        candidates_all = kept
        fix2d77_dbg['applied'] = True
        fix2d77_dbg['kept'] = int(len(candidates_all))

    # Build injected subset after filtering
    injected_norm = set()
    try:
        injected_norm = set(globals().get('_ph2b_norm_url', lambda u: u)(u) for u in (injected_urls or []) if isinstance(u, str))
    except Exception:
        injected_norm = set()

    cands_inj = []
    if injected_norm and isinstance(candidates_all, list):
        for c in candidates_all:
            if not isinstance(c, dict):
                continue
            cu = globals().get('_ph2b_norm_url', lambda u: u)(c.get('source_url') or '')
            if cu and cu in injected_norm:
                cands_inj.append(c)

    fn_sel = globals().get('_analysis_canonical_final_selector_v1')
    if not callable(fn_sel):
        return None, {
            'blocked_reason': 'missing_analysis_selector',
            'fix2d65_prune': prune_dbg,
            'fix2d77_percent_filter': fix2d77_dbg,
        }

    # Pass 1: injected-only
    if cands_inj:
        best, meta = fn_sel(canonical_key, spec, cands_inj, anchors=None, prev_metric=None, web_context=None)
        if isinstance(best, dict):
            meta = dict(meta or {})
            meta['fix2d2x_pass'] = 'injected_only'
            meta['fix2d65_prune'] = prune_dbg
            meta['fix2d77_percent_filter'] = fix2d77_dbg
            return best, meta

    # Pass 2: global
    best, meta = fn_sel(canonical_key, spec, candidates_all, anchors=None, prev_metric=None, web_context=None)
    meta = dict(meta or {})
    meta['fix2d2x_pass'] = 'global'
    meta['fix2d65_prune'] = prune_dbg
    meta['fix2d77_percent_filter'] = fix2d77_dbg
    return best, meta

# Bind overrides into globals (last-wins)
try:
    globals()['resolve_canonical_identity_v1'] = resolve_canonical_identity_v1
    globals()['rekey_metrics_via_identity_resolver_v1'] = rekey_metrics_via_identity_resolver_v1
    globals()['_fix2d60_split_schema_bound_only'] = _fix2d60_split_schema_bound_only
    globals()['_fix2d2x_select_current_for_key'] = _fix2d2x_select_current_for_key
except Exception:
    pass

# Final, authoritative version stamp (last-wins)
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK

# =====================================================================
# END PATCH FIX2D65
# =====================================================================


# =====================================================================
# PATCH FIX2D65B (FINAL OVERRIDE): version stamp + patch tracker
# =====================================================================
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass

try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D65B",
        "date": "2026-01-19",
        "summary": "Force canonical pipeline materialisation when injected URLs exist (seed schema via deterministic extensions so FIX2D31 schema-authority rebuild can run even for narrative queries).",
        "files": ["FIX2D65B_full_codebase.py"],
        "supersedes": ["FIX2D65A"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass
# =====================================================================


# =====================================================================
# PATCH FIX2D65C (FINAL OVERRIDE): contract restoration for analysis->evolution diff
# =====================================================================
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass

try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D65C",
        "date": "2026-01-19",
        "summary": "Restore analysis->evolution diff contract: broaden injected URL detection (ui_raw + legacy keys) so schema seeding and FIX2D31 schema-authority rebuild reliably run when injection is used; bump version.",
        "files": ["FIX2D65C_full_codebase.py"],
        "supersedes": ["FIX2D65B"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass
# =====================================================================


# =====================================================================
# PATCH FIX2D65D (FINAL OVERRIDE): version stamp + patch tracker
# =====================================================================
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass

try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D65D",
        "date": "2026-01-19",
        "summary": "Restore analysis->evolution diff contract by always serializing/seeding metric_schema_frozen (deterministic schema extensions), so Evolution schema-only rebuild has a stable keyspace even when LLM emits no primary_metrics; bump version.",
        "files": ["FIX2D65D_full_codebase.py"],
        "supersedes": ["FIX2D65C"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass
# =====================================================================


# =====================================================================
# PATCH FIX2D66 (FINAL OVERRIDE): version stamp + patch tracker
# =====================================================================
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass

try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D66",
        "date": "2026-01-19",
        "summary": "Deterministic injected-URL admission: promote UI raw/diag injection fields into web_context.extra_urls; synthesize diag_injected_urls when missing; ensure inj_trace_v1 and inj_diag reflect injected URLs in snapshot pool and hash inputs (auditable).",
        "files": ["FIX2D66_full_codebase.py"],
        "supersedes": ["FIX2D65D"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass
# =====================================================================

# =====================================================================
# PATCH TRACKER ENTRY (ADDITIVE): FIX2D66H
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D66H",
        "date": "2026-01-19",
        "summary": "Fix Google Sheets history save return semantics: add_to_history() now returns True on successful Sheets append and False on failure (previously fell through as None, triggering spurious 'Saved to session only' warning). Keeps session fallback and captures last Sheets error for diagnostics.",
        "files": ["FIX2D66H_full_codebase.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# =====================================================================
# PATCH TRACKER ENTRY (ADDITIVE): FIX2D67
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D67",
        "date": "2026-01-19",
        "summary": "Fix injected numeric extraction missing-link: fetch_web_context() now calls numeric extractor with correct parameter name (source_url vs url), preventing silent TypeError and empty extracted_numbers; restores injected HTML numbers into snapshot pools feeding schema-only rebuild.",
        "files": ["FIX2D67_full_codebase.py"],
        "supersedes": ["FIX2D66H"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass



# =====================================================================
# PATCH FIX2D68 PATCH TRACKER ENTRY (ADDITIVE)
# =====================================================================


try:
    PATCH_TRACKER_V1 = globals().get('PATCH_TRACKER_V1')
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D68",
        "date": "2026-01-19",
        "summary": "Make numeric extraction failures auditable and robust: fetch_web_context now attempts extractor calls in modes (source_url/url/plain) without silent swallowing; records fix2d68_extract_* diagnostics including input length/head and exception list so injected HTML extraction gaps become visible.",
        "files": ["FIX2D68_full_codebase.py"],
        "supersedes": ["FIX2D67"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# =====================================================================


# =====================================================================
# PATCH FIX2D69A PATCH TRACKER ENTRY (ADDITIVE)
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get('PATCH_TRACKER_V1')
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "FIX2D69A",
        "date": "2026-01-19",
        "summary": "Normalize numeric extractor return contracts (list/tuple/None) to prevent silent empty extracted_numbers and TypeError unpack failures; keep auditable fix2d68/69 diagnostics; bump version.",
        "files": ["FIX2D69A_full_codebase.py"],
        "supersedes": ["FIX2D69"],
    })
    globals()['PATCH_TRACKER_V1'] = PATCH_TRACKER_V1
except Exception:
    pass


# =====================================================================
# PATCH FIX2D69A PATCH TRACKER ENTRY (ADDITIVE)
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get('PATCH_TRACKER_V1')
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        'patch_id': 'FIX2D69A',
        'date': '2026-01-19',
        'summary': 'Fix numeric extraction wrapper compatibility: tolerate extractor return styles (list, (list,meta), dict, None) without tuple-unpack errors; keep auditable fix2d68 diagnostics; bump version stamp.',
        'files': ['FIX2D69A_full_codebase.py'],
        'supersedes': ['FIX2D69'],
    })
    globals()['PATCH_TRACKER_V1'] = PATCH_TRACKER_V1
except Exception:
    pass


# =====================================================================
# PATCH FIX2D69B PATCH TRACKER ENTRY (ADDITIVE)
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get('PATCH_TRACKER_V1')
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        'patch_id': 'FIX2D69B',
        'date': '2026-01-19',
        'summary': 'Fix crash inside extract_numbers_with_context by defensively normalizing helper returns (_junk_tag/_classify_measure) to prevent tuple-unpack of None; keeps schema-first rebuild honest and restores metric changes population.',
        'files': ['FIX2D69B_full_codebase.py'],
        'supersedes': ['FIX2D69A'],
    })
    globals()['PATCH_TRACKER_V1'] = PATCH_TRACKER_V1
except Exception:
    pass


# =====================================================================
# PATCH FIX2D76 PATCH TRACKER ENTRY (ADDITIVE)
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get('PATCH_TRACKER_V1')
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        'patch_id': 'FIX2D76',
        'date': '2026-01-20',
        'summary': 'Prevent false unit_mismatch blanking on metric_changes_v2: add explicit unit evidence fields (current_unit/current_unit_tag/cur_unit_cmp/prev_unit_tag) to V2 rows and teach FIX39 sanitizer to fall back to diag.diff_current_source_trace_v1 for unit tags before invalidating.',
        'files': ['FIX2D76_full_codebase.py'],
        'supersedes': ['FIX2D75'],
    })
    globals()['PATCH_TRACKER_V1'] = PATCH_TRACKER_V1
except Exception:
    pass
# PATCH FIX2D69B FINAL VERSION OVERRIDE (ADDITIVE)
# =====================================================================
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
    globals()["CODE_VERSION"] = CODE_VERSION
except Exception:
    pass


# PATCH FIX2D71: FINAL_OVERRIDE
CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK


# PATCH FIX2D71: final end-of-file version override (last-wins)
globals()["CODE_VERSION"] = "FIX2D71"


# =====================
# FINAL LAST-WINS CODE_VERSION OVERRIDE
# =====================
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass


# FIX2D73_VERSION_FINAL_OVERRIDE (REQUIRED): ensure patch id is authoritative
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
except Exception:
    pass


# FIX2D75_VERSION_FINAL_OVERRIDE (REQUIRED): option B fork
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
    globals()["CODE_VERSION"] = CODE_VERSION
except Exception:
    pass


# FIX2D76_VERSION_FINAL_OVERRIDE (REQUIRED): fix39 unit evidence for v2 rows; prevent false unit_mismatch blanking
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
    globals()["CODE_VERSION"] = CODE_VERSION
except Exception:
    pass


# =====================================================================
# PATCH FIX2D77 PATCH TRACKER ENTRY (ADDITIVE)
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get('PATCH_TRACKER_V1')
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        'patch_id': 'FIX2D77',
        'date': '2026-01-20',
        'summary': 'Percent-schema guardrail: prevent schema-only rebuild from binding year-like tokens (e.g., 2040) to __percent keys by requiring percent evidence and rejecting yearlike-without-percent raw; fixes incorrect prev value for CAGR percent metrics.',
        'files': ['FIX2D77_full_codebase.py'],
        'supersedes': ['FIX2D76'],
    })
    globals()['PATCH_TRACKER_V1'] = PATCH_TRACKER_V1
except Exception:
    pass

# FIX2D77_VERSION_FINAL_OVERRIDE (REQUIRED): ensure patch id is authoritative
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
    globals()['CODE_VERSION'] = CODE_VERSION
except Exception:
    pass


# =====================================================================
# PATCH FIX2D82 (ADDITIVE): definitive percent year-token rejection +
#                           force-apply at schema_only rebuild + prev_data
# ---------------------------------------------------------------------
# Why FIX2D80 can still leak 2040->__percent:
#   - The schema_only rebuild may select the year token as the "best" match.
#   - Percent evidence checks can be fooled by unrelated % signs elsewhere.
#   - In some Streamlit exec() reload patterns, earlier wrappers may not
#     (re-)install as expected, so sanitation doesn't run.
#
# FIX2D82 makes the guardrail unconditional:
#   1) Override percent sanitizer: if value_norm is yearlike AND the token
#      itself is that year (raw=="YYYY"), drop regardless of context.
#   2) Wrap rebuild_metrics_from_snapshots_schema_only_fix16 directly so that
#      any caller (incl. FIX2D75 materialize) gets the sanitized output.
#   3) Wrap compute_source_anchored_diff directly to sanitize previous_data
#      before diff join (cleans old HistoryFull snapshots too).
# =====================================================================

try:

    def _fix2d82__extract_raw_token_v1(rec: dict) -> str:
        """Extract the best *token* string for this record (not context)."""
        try:
            if not isinstance(rec, dict):
                return ''
            ev = rec.get('evidence')
            if isinstance(ev, list) and ev:
                e0 = ev[0]
                if isinstance(e0, dict):
                    r = e0.get('raw')
                    if isinstance(r, str) and r.strip():
                        return r.strip()
            r2 = rec.get('raw')
            if isinstance(r2, str) and r2.strip():
                return r2.strip()
            v = rec.get('value')
            if v is None:
                v = rec.get('value_norm')
            return str(v).strip() if v is not None else ''
        except Exception:
            return ''


    def _fix2d82__is_year_token_v1(vnorm, raw_token: str) -> bool:
        """True when (vnorm is yearlike 1900-2100 int) AND raw_token is that YYYY."""
        try:
            if not callable(globals().get('_fix2d80__is_yearlike_number_v1')):
                # fallback
                try:
                    fv = float(vnorm)
                    if not fv.is_integer():
                        return False
                    iv = int(fv)
                    yearlike = 1900 <= iv <= 2100
                except Exception:
                    return False
            else:
                yearlike = bool(_fix2d80__is_yearlike_number_v1(vnorm))
            if not yearlike:
                return False
            tok = str(raw_token or '').strip()
            if len(tok) != 4 or (not tok.isdigit()):
                return False
            return int(tok) == int(float(vnorm))
        except Exception:
            return False


    def _fix2d82__is_percent_key_v1(k: str, rec: dict, schema: dict) -> bool:
        try:
            if isinstance(k, str) and k.endswith('__percent'):
                return True
            if isinstance(schema, dict) and isinstance(k, str):
                spec = schema.get(k)
                if isinstance(spec, dict):
                    dim = str(spec.get('dimension') or '').strip().lower()
                    uf = str(spec.get('unit_family') or '').strip().lower()
                    ut = str(spec.get('unit_tag') or '').strip().lower()
                    blob = ' '.join([dim, uf, ut])
                    return ('percent' in blob) or ('%' in blob)
            if isinstance(rec, dict):
                dim = str(rec.get('dimension') or '').strip().lower()
                uf = str(rec.get('unit_family') or '').strip().lower()
                ut = str(rec.get('unit_tag') or '').strip().lower()
                blob = ' '.join([dim, uf, ut])
                return ('percent' in blob) or ('%' in blob)
        except Exception:
            return False
        return False


    def _fix2d82__sanitize_pmc_percent_keys_v1(pmc: dict, metric_schema_frozen: dict = None) -> tuple:
        dbg = {
            "applied": False,
            "input_count": int(len(pmc) if isinstance(pmc, dict) else 0),
            "output_count": 0,
            "checked_percent_keys": 0,
            "dropped_count": 0,
            "dropped_keys_sample": [],
            "reasons_sample": [],
            "fix": "FIX2D82",
        }
        try:
            if not isinstance(pmc, dict) or not pmc:
                dbg["applied"] = True
                dbg["output_count"] = 0
                return pmc if isinstance(pmc, dict) else {}, dbg

            schema = metric_schema_frozen if isinstance(metric_schema_frozen, dict) else {}
            out = dict(pmc)

            for k in list(out.keys()):
                rec = out.get(k)
                if not _fix2d82__is_percent_key_v1(k, rec if isinstance(rec, dict) else {}, schema):
                    continue

                dbg["checked_percent_keys"] += 1
                if not isinstance(rec, dict):
                    out.pop(k, None)
                    dbg["dropped_count"] += 1
                    if len(dbg["dropped_keys_sample"]) < 8:
                        dbg["dropped_keys_sample"].append(str(k))
                        dbg["reasons_sample"].append({"key": str(k), "reason": "non_dict_record"})
                    continue

                vnorm = rec.get('value_norm')
                raw_token = _fix2d82__extract_raw_token_v1(rec)

                # FIX2D82: hard drop year-token values for percent keys.
                if _fix2d82__is_year_token_v1(vnorm, raw_token):
                    out.pop(k, None)
                    dbg["dropped_count"] += 1
                    if len(dbg["dropped_keys_sample"]) < 8:
                        dbg["dropped_keys_sample"].append(str(k))
                        dbg["reasons_sample"].append({
                            "key": str(k),
                            "reason": "year_token_for_percent",
                            "raw_token": str(raw_token)[:60],
                            "value_norm": vnorm,
                        })
                    continue

                # Strong percent evidence must be tied to the token/unit, not context.
                unit = str(rec.get('unit') or rec.get('unit_tag') or '')
                rt = str(raw_token or '').lower()
                token_has_pct = ('%' in rt) or ('percent' in rt) or ('pct' in rt)
                unit_has_pct = ('%' in unit) or ('percent' in unit.lower())

                if (not token_has_pct) and (not unit_has_pct):
                    out.pop(k, None)
                    dbg["dropped_count"] += 1
                    if len(dbg["dropped_keys_sample"]) < 8:
                        dbg["dropped_keys_sample"].append(str(k))
                        dbg["reasons_sample"].append({
                            "key": str(k),
                            "reason": "no_strong_percent_evidence_token",
                            "unit": unit,
                            "raw_token": str(raw_token)[:80],
                            "value_norm": vnorm,
                        })
                    continue

            dbg["output_count"] = int(len(out))
            dbg["applied"] = True
            return out, dbg
        except Exception as e:
            dbg["applied"] = False
            dbg["error"] = str(type(e).__name__)
            return pmc, dbg


    def _fix2d82__get_schema_from_prev_response_v1(prev_response: dict) -> dict:
        try:
            if not isinstance(prev_response, dict):
                return None
            schema = prev_response.get('metric_schema_frozen')
            if isinstance(schema, dict):
                return schema
            pr = prev_response.get('primary_response') if isinstance(prev_response.get('primary_response'), dict) else None
            if isinstance(pr, dict) and isinstance(pr.get('metric_schema_frozen'), dict):
                return pr.get('metric_schema_frozen')
            return None
        except Exception:
            return None


    # -----------------------------------------------------------------
    # (1) Force-apply sanitizer to schema_only rebuild (Analysis baseline)
    # -----------------------------------------------------------------
    try:
        _fix2d82__orig_schema_only = globals().get('rebuild_metrics_from_snapshots_schema_only_fix16')
        if callable(_fix2d82__orig_schema_only) and (not getattr(_fix2d82__orig_schema_only, '_fix2d82_wrapped', False)):

            def rebuild_metrics_from_snapshots_schema_only_fix16(prev_response, snapshot_pool, web_context=None):  # noqa: F811
                rebuilt = _fix2d82__orig_schema_only(prev_response, snapshot_pool, web_context=web_context)
                schema = _fix2d82__get_schema_from_prev_response_v1(prev_response)
                if isinstance(rebuilt, dict) and rebuilt:
                    rebuilt2, sdbg = _fix2d82__sanitize_pmc_percent_keys_v1(rebuilt, metric_schema_frozen=schema)
                    rebuilt = rebuilt2
                    try:
                        if isinstance(prev_response, dict):
                            prev_response.setdefault('debug', {})
                            if isinstance(prev_response.get('debug'), dict):
                                prev_response['debug']['fix2d82_percent_sanitize_schema_only'] = sdbg
                    except Exception:
                        pass
                return rebuilt

            try:
                rebuild_metrics_from_snapshots_schema_only_fix16._fix2d82_wrapped = True
            except Exception:
                pass
            globals()['rebuild_metrics_from_snapshots_schema_only_fix16'] = rebuild_metrics_from_snapshots_schema_only_fix16
    except Exception:
        pass


    # -----------------------------------------------------------------
    # (2) Force-apply sanitizer to previous_data before diff join
    # -----------------------------------------------------------------
    try:
        _fix2d82__orig_compute = globals().get('compute_source_anchored_diff')
        if callable(_fix2d82__orig_compute) and (not getattr(_fix2d82__orig_compute, '_fix2d82_wrapped', False)):

            def compute_source_anchored_diff(previous_data: dict, web_context: dict = None) -> dict:  # noqa: F811
                try:
                    if isinstance(previous_data, dict):
                        schema = _fix2d82__get_schema_from_prev_response_v1(previous_data)
                        # locate pmc
                        pmc = previous_data.get('primary_metrics_canonical')
                        if not isinstance(pmc, dict) or not pmc:
                            pr = previous_data.get('primary_response') if isinstance(previous_data.get('primary_response'), dict) else None
                            if isinstance(pr, dict) and isinstance(pr.get('primary_metrics_canonical'), dict):
                                pmc = pr.get('primary_metrics_canonical')
                        if isinstance(pmc, dict) and pmc:
                            pmc2, sdbg = _fix2d82__sanitize_pmc_percent_keys_v1(pmc, metric_schema_frozen=schema)
                            try:
                                previous_data['primary_metrics_canonical'] = pmc2
                            except Exception:
                                pass
                            try:
                                previous_data.setdefault('primary_response', {})
                                if isinstance(previous_data.get('primary_response'), dict):
                                    previous_data['primary_response']['primary_metrics_canonical'] = pmc2
                            except Exception:
                                pass
                            try:
                                previous_data.setdefault('debug', {})
                                if isinstance(previous_data.get('debug'), dict):
                                    previous_data['debug']['fix2d82_prev_percent_sanitize'] = sdbg
                            except Exception:
                                pass
                except Exception:
                    pass

                return _fix2d82__orig_compute(previous_data, web_context=web_context)

            try:
                compute_source_anchored_diff._fix2d82_wrapped = True
            except Exception:
                pass
            globals()['compute_source_anchored_diff'] = compute_source_anchored_diff
    except Exception:
        pass

except Exception:
    pass


# =====================================================================
# PATCH FIX2D82 PATCH TRACKER ENTRY (ADDITIVE)
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get('PATCH_TRACKER_V1')
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        'patch_id': 'FIX2D82',
        'date': '2026-01-20',
        'summary': 'Definitive fix for year-token leakage into __percent metrics: drop year-token values when raw token is YYYY regardless of context % signs; force-apply at schema_only rebuild (Analysis baseline materialize) and sanitize previous_data before diff join (cleans old HistoryFull snapshots). Adds debug: fix2d82_percent_sanitize_schema_only / fix2d82_prev_percent_sanitize.',
        'files': ['FIX2D82_full_codebase.py'],
        'supersedes': ['FIX2D80'],
    })
    globals()['PATCH_TRACKER_V1'] = PATCH_TRACKER_V1
except Exception:
    pass

# FIX2D82_VERSION_FINAL_OVERRIDE (REQUIRED): ensure patch id is authoritative
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
    globals()['CODE_VERSION'] = CODE_VERSION
except Exception:
    pass


# =====================================================================
# PATCH FIX2D83 (CLEANUP): remove obsolete percent-guard wrappers and stabilize versioning
#
# - Removes FIX2D78/FIX2D79 percent-guard wrappers (superseded by definitive FIX2D82 sanitizer)
# - Ensures a single authoritative CODE_VERSION at end-of-file
# - Adds patch tracker entry
# =====================================================================
try:
    PATCH_TRACKER_V1 = globals().get('PATCH_TRACKER_V1')
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        'patch_id': 'FIX2D83',
        'date': '2026-01-20',
        'summary': 'Cleanup consolidation: remove obsolete percent-key guard wrappers (FIX2D78/FIX2D79) now superseded by definitive FIX2D82 sanitizer; stabilize CODE_VERSION with final override to avoid stale late assignments; reduce patch clutter without changing diff behavior.',
        'files': ['FIX2D83_full_codebase.py'],
        'supersedes': ['FIX2D78', 'FIX2D79', 'FIX2D80'],
    })
    PATCH_TRACKER_V1.append({
        'patch_id': 'REFACTOR45',
        'date': '2026-01-25',
        'summary': 'Evolution: fix Diff Panel V2 empty rows by hardening the V2 builder call against RecursionError (minimal wrappers, traceback capture, preserve existing rows, strict canonical-join fallback).',
        'files': ['REFACTOR45_full_codebase_streamlit_safe.py'],
        'supersedes': ['REFACTOR44'],
    })

    globals()['PATCH_TRACKER_V1'] = PATCH_TRACKER_V1
except Exception:
    pass

# FIX2D86_VERSION_FINAL_OVERRIDE (REQUIRED): keep patch id authoritative
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
    globals()['CODE_VERSION'] = CODE_VERSION
except Exception:
    pass

# --- Streamlit exec display guard: avoid dumping long function docs in UI ---
try:
    _fn = globals().get("diff_metrics_by_name")
    if callable(_fn):
        _fn.__doc__ = ""
except Exception:
    pass

# FIX2D86: final wrapper — sanitize schema-only rebuild output for percent keys
try:
    _fix2d86__orig_schema_only = globals().get("rebuild_metrics_from_snapshots_schema_only_fix16")
    if callable(_fix2d86__orig_schema_only) and (not getattr(_fix2d86__orig_schema_only, "_fix2d86_wrapped", False)):

        def rebuild_metrics_from_snapshots_schema_only_fix16(prev_response, snapshot_pool, web_context=None):  # noqa: F811
            rebuilt = _fix2d86__orig_schema_only(prev_response, snapshot_pool, web_context=web_context)
            try:
                schema = {}
                if isinstance(prev_response, dict):
                    schema = prev_response.get("metric_schema_frozen") or {}
                if isinstance(rebuilt, dict) and rebuilt:
                    rebuilt2, sdbg = _fix2d86_sanitize_pmc_percent_year_tokens_v1(
                        pmc=rebuilt,
                        metric_schema_frozen=schema if isinstance(schema, dict) else {},
                        label="schema_only_rebuild_fix16_final_wrap",
                    )
                    rebuilt = rebuilt2
                    try:
                        if isinstance(prev_response, dict):
                            prev_response.setdefault("debug", {})
                            if isinstance(prev_response.get("debug"), dict):
                                prev_response["debug"]["fix2d86_percent_year_token_sanitize_schema_only"] = sdbg
                    except Exception:
                        pass
            except Exception:
                pass
            return rebuilt

        try:
            rebuild_metrics_from_snapshots_schema_only_fix16._fix2d86_wrapped = True
        except Exception:
            pass

        globals()["rebuild_metrics_from_snapshots_schema_only_fix16"] = rebuild_metrics_from_snapshots_schema_only_fix16
except Exception:
    pass


# =====================
# REFACTOR04: VERSION FINAL OVERRIDE (LAST-WINS)
# - This file contains legacy CODE_VERSION bumps from earlier phases.
# - Ensure the refactor patch id remains authoritative.
# =====================
try:
    CODE_VERSION = _YUREEKA_CODE_VERSION_LOCK
    globals()["CODE_VERSION"] = CODE_VERSION
except Exception:
    pass

# ============================================================
# REFACTOR04: REGRESSION HARNESS V2 (ADDITIVE, STREAMLIT-SAFE)
# Goal:
#   - Provide a stable gate for refactor/consolidation work.
#   - Executes: Analysis (headless) -> Evolution (source-anchored) and asserts invariants.
#
# Invocation:
#   python REFACTOR04_full_codebase_streamlit_safe.py --run_refactor_harness
#   or RUN_REFACTOR_HARNESS=1
#
# Optional env overrides:
#   REFACTOR_HARNESS_QUERY                      - analysis question text
#   REFACTOR_HARNESS_NUM_SOURCES               - int (default: 3)
#   REFACTOR_HARNESS_EXTRA_URLS                - newline-separated URLs (applied to both analysis+evolution)
#   REFACTOR_HARNESS_EXTRA_URLS_ANALYSIS       - newline-separated URLs (analysis only)
#   REFACTOR_HARNESS_EXTRA_URLS_EVOLUTION      - newline-separated URLs (evolution only)
#   REFACTOR_HARNESS_FORCE_REBUILD             - 1/0 (default: 1)
#   REFACTOR_HARNESS_REPORT_PATH               - directory path for JSON report (default: cwd)
# ============================================================

def _refactor01__bool(v, default=False):
    try:
        s = str(v).strip().lower()
        if s in ("1", "true", "yes", "y", "on"):
            return True
        if s in ("0", "false", "no", "n", "off"):
            return False
    except Exception:
        pass
    return bool(default)

def _refactor01__parse_urls(raw):
    urls = []
    try:
        for line in str(raw or "").splitlines():
            u = line.strip()
            if not u:
                continue
            if u.startswith("http://") or u.startswith("https://"):
                urls.append(u)
    except Exception:
        pass
    # de-dupe while preserving order
    out = []
    seen = set()
    for u in urls:
        if u in seen:
            continue
        seen.add(u)
        out.append(u)
    return out

def _refactor01__safe_now_iso():
    try:
        if callable(globals().get("now_utc")):
            return now_utc().isoformat()
    except Exception:
        pass
    try:
        from datetime import datetime, timezone
        return datetime.now(timezone.utc).isoformat()
    except Exception:
        return ""

def _refactor01__safe_get_schema_and_pmc(primary_data: dict):
    """Mirror the minimal canonicalization steps used by the Analysis UI."""
    if not isinstance(primary_data, dict):
        return {}, {}, {}

    # Ensure primary_metrics_canonical is split into ok/provisional deterministically
    try:
        _pmc_raw = primary_data.get("primary_metrics_canonical") or {}
        _split = globals().get("_fix2d58b_split_primary_metrics_canonical")
        if callable(_split):
            _pmc_ok, _pmc_prov = _split(_pmc_raw)
            if isinstance(_pmc_ok, dict):
                primary_data["primary_metrics_canonical"] = _pmc_ok
            if isinstance(_pmc_prov, dict) and _pmc_prov:
                primary_data["primary_metrics_provisional"] = _pmc_prov
    except Exception:
        pass

    # Freeze schema if missing
    try:
        if (not isinstance(primary_data.get("metric_schema_frozen"), dict)) and isinstance(primary_data.get("primary_metrics_canonical"), dict) and primary_data.get("primary_metrics_canonical"):
            _freeze = globals().get("freeze_metric_schema")
            if callable(_freeze):
                primary_data["metric_schema_frozen"] = _freeze(primary_data.get("primary_metrics_canonical") or {})
    except Exception:
        pass

    schema = primary_data.get("metric_schema_frozen") if isinstance(primary_data.get("metric_schema_frozen"), dict) else {}
    pmc = primary_data.get("primary_metrics_canonical") if isinstance(primary_data.get("primary_metrics_canonical"), dict) else {}
    prov = primary_data.get("primary_metrics_provisional") if isinstance(primary_data.get("primary_metrics_provisional"), dict) else {}
    return schema, pmc, prov

def _refactor02_run_harness_v2():
    import os, sys, json, traceback

    # REFACTOR12: ensure version lock + final bindings are applied before harness assertions
    try:
        _yureeka_lock_version_globals_v1()
        _yureeka_ensure_final_bindings_v1()
    except Exception:
        pass

    # ---- config
    query = str(os.getenv("REFACTOR_HARNESS_QUERY") or "").strip()
    if not query:
        # Safe default (user can override via env)
        query = "Global EV sales 2024 and global EV market share 2025"

    try:
        num_sources = int(str(os.getenv("REFACTOR_HARNESS_NUM_SOURCES") or "3").strip())
    except Exception:
        num_sources = 3

    force_rebuild = _refactor01__bool(os.getenv("REFACTOR_HARNESS_FORCE_REBUILD", "1"), default=True)

    extra_urls_common = _refactor01__parse_urls(os.getenv("REFACTOR_HARNESS_EXTRA_URLS"))
    extra_urls_analysis = _refactor01__parse_urls(os.getenv("REFACTOR_HARNESS_EXTRA_URLS_ANALYSIS"))
    extra_urls_evolution = _refactor01__parse_urls(os.getenv("REFACTOR_HARNESS_EXTRA_URLS_EVOLUTION"))

    # common applies to both (unless already present)
    for u in extra_urls_common:
        if u not in extra_urls_analysis:
            extra_urls_analysis.append(u)
        if u not in extra_urls_evolution:
            extra_urls_evolution.append(u)

    analysis_run_id = ""
    evo_run_id = ""
    try:
        mk = globals().get("_inj_diag_make_run_id")
        if callable(mk):
            analysis_run_id = mk("analysis_harness")
            evo_run_id = mk("evolution_harness")
    except Exception:
        pass

    report = {
        "patch_id": "REFACTOR18",
        "code_version": _yureeka_get_code_version(),
            "authority_manifest_v1": _yureeka_authority_manifest_v1(),
        "run_ts_utc": _refactor01__safe_now_iso(),
        "config": {
            "query": query,
            "num_sources": int(num_sources),
            "force_rebuild": bool(force_rebuild),
            "extra_urls_analysis": list(extra_urls_analysis),
            "extra_urls_evolution": list(extra_urls_evolution),
        },
        "analysis": {},
        "evolution": {},
        "assertions": [],
        "status": "unknown",
    }

    def _assert(name, ok, detail=""):
        report["assertions"].append({
            "name": str(name),
            "pass": bool(ok),
            "detail": (str(detail)[:2000] if detail is not None else ""),
        })
        return bool(ok)

    ok_all = True


    # 0) Binding sanity: ensure we are running against the authoritative diff binding.
    try:
        _auth = _yureeka_get_authoritative_binding_tag_v1(globals().get("diff_metrics_by_name"))
    except Exception:
        _auth = None
    expected_auth = str(_yureeka_get_code_version() or "")
    ok_all = _assert("binding.diff_metrics_by_name_authoritative", (_auth == expected_auth), f"auth={_auth} expected={expected_auth}") and ok_all
    try:
        _obj = globals().get("diff_metrics_by_name")
        _auth_obj = globals().get("_YUREEKA_DIFF_METRICS_BY_NAME_AUTHORITATIVE")
        ok_all = _assert("binding.diff_fn_object_match", (_auth_obj is None) or (_obj is _auth_obj), f"obj={type(_obj)}") and ok_all
    except Exception:
        ok_all = _assert("binding.diff_fn_object_match", False, "exception") and ok_all

    # ---- run analysis (headless)
    try:
        fwc = globals().get("fetch_web_context")
        qp = globals().get("query_perplexity")
        if not callable(fwc):
            ok_all = _assert("analysis.fetch_web_context_defined", False, "fetch_web_context is not callable") and ok_all
            raise RuntimeError("fetch_web_context missing")
        if not callable(qp):
            ok_all = _assert("analysis.query_perplexity_defined", False, "query_perplexity is not callable") and ok_all
            raise RuntimeError("query_perplexity missing")

        web_context = fwc(
            query,
            num_sources=num_sources,
            extra_urls=extra_urls_analysis,
            diag_run_id=str(analysis_run_id or ""),
            diag_extra_urls_ui_raw="\n".join(extra_urls_analysis),
        )

        if not isinstance(web_context, dict) or not web_context.get("search_results"):
            # mirror UI fallback
            web_context = {
                "search_results": [],
                "scraped_content": {},
                "summary": "",
                "sources": [],
                "source_reliability": [],
            }

        primary_response = qp(query, web_context, query_structure=None)
        ok_all = _assert("analysis.primary_response_nonempty", bool(primary_response), "Primary model returned empty response") and ok_all
        if not primary_response:
            raise RuntimeError("primary_response empty")

        try:
            primary_data = json.loads(primary_response)
        except Exception as e:
            ok_all = _assert("analysis.primary_response_json_parse", False, f"JSON parse failed: {e}") and ok_all
            raise

        schema, pmc, prov = _refactor01__safe_get_schema_and_pmc(primary_data)

        # optional veracity scoring (non-fatal)
        veracity_scores = {}
        try:
            ev = globals().get("evidence_based_veracity")
            if callable(ev):
                veracity_scores = ev(primary_data, web_context) or {}
        except Exception:
            veracity_scores = {}

        analysis_out = {
            "question": query,
            "timestamp": _refactor01__safe_now_iso(),
            "primary_response": primary_data,
            "veracity_scores": veracity_scores,
            "web_sources": (web_context.get("sources", []) if isinstance(web_context, dict) else []),
            "code_version": _yureeka_get_code_version(),
            "authority_manifest_v1": _yureeka_authority_manifest_v1(),
            # ensure evolution can find these top-level as well
            "metric_schema_frozen": schema,
            "primary_metrics_canonical": pmc,
        }
        try:
            if isinstance(analysis_out.get("primary_response"), dict):
                analysis_out["primary_response"]["code_version"] = _yureeka_get_code_version()
        except Exception:
            pass

        # attach analysis-aligned snapshots (stable cache evolution should reuse)
        try:
            attach = globals().get("attach_source_snapshots_to_analysis")
            if callable(attach):
                analysis_out = attach(analysis_out, web_context)
        except Exception:
            pass

        report["analysis"] = {
            "diag_run_id": analysis_run_id,
            "schema_key_count": int(len(schema or {})),
            "pmc_key_count": int(len(pmc or {})),
            "baseline_sources_cache_count": int(len((analysis_out or {}).get("baseline_sources_cache") or [])),
        }

        # ---- REFACTOR07 invariant: version + binding manifest must match runtime lock
        ok_all = _assert("analysis.code_version_matches_lock", str((analysis_out or {}).get("code_version") or "") == _yureeka_get_code_version(), f"analysis.code_version={(analysis_out or {}).get('code_version')} lock={_yureeka_get_code_version()}") and ok_all
        try:
            _pr = (analysis_out or {}).get("primary_response") if isinstance(analysis_out, dict) else None
            ok_all = _assert("analysis.primary_response_code_version_matches_lock", str((_pr or {}).get("code_version") or "") == _yureeka_get_code_version(), f"analysis.primary_response.code_version={(_pr or {}).get('code_version')} lock={_yureeka_get_code_version()}") and ok_all
        except Exception:
            pass
        try:
            _fn = globals().get("diff_metrics_by_name")
            _tag = str(getattr(_fn, "__YUREEKA_AUTHORITATIVE_BINDING__", "") or "")
            ok_all = _assert("binding.diff_metrics_by_name_authoritative_tag", _tag == str(globals().get("_YUREEKA_FINAL_BINDINGS_VERSION") or ""), f"tag={_tag} final={globals().get('_YUREEKA_FINAL_BINDINGS_VERSION')}") and ok_all
        except Exception:
            pass

        ok_all = _assert("analysis.schema_nonempty", int(len(schema or {})) > 0, f"schema_key_count={len(schema or {})}") and ok_all
        ok_all = _assert("analysis.pmc_nonempty", int(len(pmc or {})) > 0, f"pmc_key_count={len(pmc or {})}") and ok_all


        # ---- REFACTOR04 invariants: baseline PMC dimensional sanity
        def _h_has_currency(_s):
            try:
                s = str(_s or "").lower()
                return any(x in s for x in ("us$", "usd", "sgd", "eur", "gbp", "aud", "cny", "jpy", "$", "€", "£", "¥"))
            except Exception:
                return False

        def _h_has_percent(_s):
            try:
                s = str(_s or "").lower()
                return ("%" in s) or ("percent" in s)
            except Exception:
                return False

        bad_mag = []
        bad_cur = []
        bad_pct = []

        for _ckey, _mobj in (pmc or {}).items():
            if not isinstance(_mobj, dict):
                continue

            _raw = _mobj.get("raw")
            if not _raw:
                _ev = _mobj.get("evidence")
                if isinstance(_ev, list) and _ev and isinstance(_ev[0], dict):
                    _raw = _ev[0].get("raw") or _ev[0].get("snippet") or ""
                elif isinstance(_ev, dict):
                    _raw = _ev.get("raw") or _ev.get("snippet") or ""
            if not _raw:
                try:
                    _v = _mobj.get("value_norm") if _mobj.get("value_norm") is not None else _mobj.get("value")
                    _ut = (_mobj.get("unit") or _mobj.get("unit_tag") or "").strip()
                    if _v is not None and _ut:
                        _raw = f"{_v} {_ut}"
                    elif _v is not None:
                        _raw = str(_v)
                except Exception:
                    _raw = ""

            _unit_tag = (_mobj.get("unit") or _mobj.get("unit_tag") or _mobj.get("base_unit") or "").strip().lower()
            _ck = str(_ckey or "").lower()

            _is_pct = ("__percent" in _ck) or (_unit_tag == "percent")
            _is_cur = ("__currency" in _ck)
            _is_mag = ("__unit_" in _ck) or (not _is_pct and not _is_cur)

            if _is_mag:
                # magnitude/count-like must not carry currency/percent markers (cross-dimension leakage guard)
                if _h_has_currency(_raw) or _h_has_percent(_raw):
                    bad_mag.append({"canonical_key": str(_ckey), "raw": str(_raw)[:120]})

            if _is_cur:
                # currency keys should carry a currency marker in raw/unit
                if not _h_has_currency(_raw):
                    bad_cur.append({"canonical_key": str(_ckey), "raw": str(_raw)[:120]})

            if _is_pct:
                # percent keys must carry percent marker and must not bind bare years
                _bad_here = False
                if not _h_has_percent(_raw):
                    _bad_here = True
                try:
                    _v = _mobj.get("value_norm") if _mobj.get("value_norm") is not None else _mobj.get("value")
                    if isinstance(_v, (int, float)):
                        _iv = int(_v)
                        if 1900 <= _iv <= 2100 and abs(float(_v) - float(_iv)) < 1e-9:
                            _bad_here = True
                except Exception:
                    pass
                if _bad_here:
                    bad_pct.append({"canonical_key": str(_ckey), "raw": str(_raw)[:120], "value_norm": _mobj.get("value_norm")})

            if (len(bad_mag) >= 5) and (len(bad_cur) >= 5) and (len(bad_pct) >= 5):
                break

        ok_all = _assert("pmc.magnitude_keys_no_currency_or_percent_markers", (len(bad_mag) == 0), f"samples={bad_mag}") and ok_all
        ok_all = _assert("pmc.currency_keys_have_currency_marker", (len(bad_cur) == 0), f"samples={bad_cur}") and ok_all
        ok_all = _assert("pmc.percent_keys_have_percent_marker_and_not_yearlike", (len(bad_pct) == 0), f"samples={bad_pct}") and ok_all

    except Exception as e:
        report["analysis"]["error"] = f"{type(e).__name__}: {e}"
        report["analysis"]["traceback"] = traceback.format_exc()[:8000]
        report["status"] = "fail"
        # write report
        _dir = str(os.getenv("REFACTOR_HARNESS_REPORT_PATH") or os.getcwd())
        try:
            os.makedirs(_dir, exist_ok=True)
        except Exception:
            _dir = os.getcwd()
        fname = f"refactor_harness_report_REFACTOR19_{analysis_run_id or 'analysis'}_{evo_run_id or 'evo'}.json"
        fpath = os.path.join(_dir, fname)
        try:
            with open(fpath, "w", encoding="utf-8") as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
        except Exception:
            pass
        print("[REFACTOR10] Harness FAILED during analysis stage. Report:", fpath)
        return False

    # ---- run evolution
    evo_out = None
    try:
        evo_fn = globals().get("run_source_anchored_evolution")
        if not callable(evo_fn):
            ok_all = _assert("evolution.run_source_anchored_evolution_defined", False, "run_source_anchored_evolution is not callable") and ok_all
            raise RuntimeError("run_source_anchored_evolution missing")

        evo_out = evo_fn(
            analysis_out,
            web_context={
                "force_rebuild": bool(force_rebuild),
                "extra_urls": list(extra_urls_evolution),
                "diag_run_id": str(evo_run_id or ""),
                "diag_extra_urls_ui_raw": "\n".join(extra_urls_evolution),
            },
        )
        ok_all = _assert("evolution.output_dict", isinstance(evo_out, dict), f"type={type(evo_out)}") and ok_all
        if not isinstance(evo_out, dict):
            raise RuntimeError("evolution output not a dict")

        rows = evo_out.get("metric_changes_v2")
        if not isinstance(rows, list):
            rows = []

        dbg = evo_out.get("debug") if isinstance(evo_out.get("debug"), dict) else {}
        summary = dbg.get("diff_panel_v2_summary") if isinstance(dbg.get("diff_panel_v2_summary"), dict) else {}

        both_count = summary.get("both_count")
        prev_only = summary.get("prev_only_count")
        cur_only = summary.get("cur_only_count")
        rows_total = summary.get("rows_total")
        join_mode = summary.get("join_mode")

        report["evolution"] = {
            "diag_run_id": evo_run_id,
            "metric_changes_v2_rows": int(len(rows)),
            "diff_panel_v2_summary": summary,
        }


        # ---- REFACTOR08 invariants: diff_panel_v2_summary consistency (if present)
        try:
            if rows_total is not None:
                ok_all = _assert("diff.summary_rows_total_matches_len", int(rows_total) == int(len(rows)), f"rows_total={rows_total} len(rows)={len(rows)}") and ok_all
            if (both_count is not None) and (prev_only is not None) and (cur_only is not None) and (rows_total is not None):
                ok_all = _assert("diff.summary_partition_counts_match_total", (int(both_count) + int(prev_only) + int(cur_only)) == int(rows_total), f"both={both_count} prev_only={prev_only} cur_only={cur_only} rows_total={rows_total}") and ok_all

            _found = summary.get("found")
            _not_found = summary.get("not_found")
            if _found is not None:
                ok_all = _assert("diff.summary_found_gt_0", int(_found) > 0, f"found={_found}") and ok_all
            if _not_found is not None:
                ok_all = _assert("diff.summary_not_found_eq_0", int(_not_found) == 0, f"not_found={_not_found}") and ok_all

            kov = dbg.get("key_overlap_v1")
            if isinstance(kov, dict):
                _pc = kov.get("prev_count")
                _cc = kov.get("cur_count")
                _oc = kov.get("overlap_count")
                if _pc is not None:
                    ok_all = _assert("diff.key_overlap_prev_count_gt_0", int(_pc) > 0, f"prev_count={_pc}") and ok_all
                if _cc is not None:
                    ok_all = _assert("diff.key_overlap_cur_count_gt_0", int(_cc) > 0, f"cur_count={_cc}") and ok_all
                if (_pc is not None) and (_cc is not None) and (_oc is not None):
                    ok_all = _assert("diff.key_overlap_overlap_leq_min", int(_oc) <= min(int(_pc), int(_cc)), f"prev={_pc} cur={_cc} overlap={_oc}") and ok_all
        except Exception:
            ok_all = _assert("diff.summary_consistency_checks", False, "exception") and ok_all

        # ---- REFACTOR07 invariant: evolution code_version matches runtime lock
        try:
            ok_all = _assert("evolution.code_version_matches_lock", str((evo_out or {}).get("code_version") or "") == _yureeka_get_code_version(), f"evolution.code_version={(evo_out or {}).get('code_version')} lock={_yureeka_get_code_version()}") and ok_all
        except Exception:
            pass

        # ---- invariants
        # 1) diff rows exist and include both prev+cur
        any_both = False
        for r in rows:
            try:
                if (r.get("previous_value_norm") is not None) and (r.get("current_value_norm") is not None):
                    any_both = True
                    break
            except Exception:
                continue

        ok_all = _assert("diff.any_both_row_exists", bool(any_both), f"rows={len(rows)}") and ok_all

        # 2) summary both_count > 0 (if present)
        if both_count is not None:
            ok_all = _assert("diff.summary_both_count_gt_0", int(both_count) > 0, f"both_count={both_count}") and ok_all

        # 3) no global 'no_prev_metrics' failure mode (heuristic: not all rows are no_prev_metrics)
        if rows:
            no_prev_all = True
            for r in rows:
                ct = str(r.get("change_type") or "").strip().lower()
                if ct != "no_prev_metrics":
                    no_prev_all = False
                    break
            ok_all = _assert("diff.not_all_no_prev_metrics", (not no_prev_all), f"rows_total={len(rows)}") and ok_all

        # 4) percent-year token rule: percent-key prev/current must not look like a bare year
        bad_percent_year = []
        for r in rows:
            try:
                ckey = str(r.get("canonical_key") or "")
                unit_tag = str(r.get("current_unit_tag") or r.get("previous_unit_tag") or "")
                is_pct = ("__percent" in ckey) or (unit_tag == "percent") or ("percent" in ckey.lower())
                if not is_pct:
                    continue

                for fld in ("previous_value_norm", "current_value_norm"):
                    v = r.get(fld)
                    if v is None:
                        continue
                    try:
                        fv = float(v)
                    except Exception:
                        continue
                    if 1900.0 <= fv <= 2100.0 and abs(fv - round(fv)) < 1e-9:
                        bad_percent_year.append({"canonical_key": ckey, fld: v})
                        if len(bad_percent_year) >= 5:
                            break
                if len(bad_percent_year) >= 5:
                    break
            except Exception:
                continue

        ok_all = _assert("percent.prev_or_cur_not_yearlike_1900_2100", (len(bad_percent_year) == 0), f"samples={bad_percent_year}") and ok_all

        # 4b) REFACTOR04: guard against false unit_mismatch when both sides share the same scale tag on magnitude keys.
        bad_unit_mismatch_same_scale = []
        for r in rows:
            try:
                ckey = str(r.get("canonical_key") or "")
                if "__unit_" not in ckey:
                    continue
                if str(r.get("change_type") or "").strip().lower() != "unit_mismatch":
                    continue
                pu = str(r.get("previous_unit_tag") or "").upper().strip()
                cu = str(r.get("current_unit_tag") or "").upper().strip()
                if pu and cu and (pu == cu) and (pu in ("K", "M", "B", "T")):
                    bad_unit_mismatch_same_scale.append({
                        "canonical_key": ckey,
                        "previous_unit_tag": pu,
                        "current_unit_tag": cu,
                        "prev": r.get("previous_value_norm"),
                        "cur": r.get("current_value_norm"),
                    })
                    if len(bad_unit_mismatch_same_scale) >= 5:
                        break
            except Exception:
                continue
        ok_all = _assert(
            "unit_mismatch.not_triggered_when_same_scale_magnitude",
            (len(bad_unit_mismatch_same_scale) == 0),
            f"samples={bad_unit_mismatch_same_scale}",
        ) and ok_all


        # 5) internal count sanity (summary counts should align where possible)
        if isinstance(summary, dict) and both_count is not None and prev_only is not None and rows_total is not None:
            try:
                prev_key_count_summary = int(both_count) + int(prev_only)
                pmc_keys = list((analysis_out.get("primary_metrics_canonical") or {}).keys()) if isinstance(analysis_out.get("primary_metrics_canonical"), dict) else []
                ok_all = _assert("counts.prev_key_count_matches_baseline_pmc", prev_key_count_summary == len(pmc_keys), f"summary_prev={prev_key_count_summary} baseline_pmc={len(pmc_keys)}") and ok_all
                if str(join_mode or "") != "union":
                    ok_all = _assert("counts.rows_total_matches_prev_key_count_nonunion", int(rows_total) == prev_key_count_summary, f"rows_total={rows_total} prev_key_count={prev_key_count_summary} join_mode={join_mode}") and ok_all
            except Exception:
                pass

    except Exception as e:
        report["evolution"]["error"] = f"{type(e).__name__}: {e}"
        report["evolution"]["traceback"] = traceback.format_exc()[:8000]
        report["status"] = "fail"
        ok_all = False

    # ---- write report
    try:
        report["status"] = "pass" if ok_all else "fail"
        _dir = str(os.getenv("REFACTOR_HARNESS_REPORT_PATH") or os.getcwd())
        try:
            os.makedirs(_dir, exist_ok=True)
        except Exception:
            _dir = os.getcwd()
        fname = f"refactor_harness_report_REFACTOR08_{analysis_run_id or 'analysis'}_{evo_run_id or 'evo'}.json"
        fpath = os.path.join(_dir, fname)
        with open(fpath, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"[REFACTOR19] Harness {'PASSED' if ok_all else 'FAILED'}. Report: {fpath}")
    except Exception:
        pass

    return bool(ok_all)


# ============================================================
#


# ============================================================


# PATCH TRACKER V1 (ADD): REFACTOR11
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    try:
        for _e in PATCH_TRACKER_V1:
            if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR11":
                _already = True
                break
    except Exception:
        _already = False
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR11",
            "date": "2026-01-23",
            "summary": "Fix evolution JSON/UI counters: recompute summary (increased/decreased/unchanged/added) from final metric_changes rows; recompute stability_score accordingly; bump final binding/version locks to REFACTOR11 for hygiene.",
            "files": ["REFACTOR11_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR10"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass



# PATCH TRACKER V1 (ADD): REFACTOR13
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    try:
        for _e in PATCH_TRACKER_V1:
            if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR13":
                _already = True
                break
    except Exception:
        _already = False
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR13",
            "date": "2026-01-23",
            "summary": "Summary/stability correctness: recompute evolution results.summary and stability_score from canonical-first diff rows (metric_changes_v2/metric_changes). Add graded stability fallback (100 - mean abs % change) when discrete unchanged/small-change scoring would yield 0, and mirror counts into diff_panel_v2_summary for auditability.",
            "files": ["REFACTOR13_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR12"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# PATCH TRACKER V1 (ADD): REFACTOR14
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    try:
        for _e in PATCH_TRACKER_V1:
            if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR14":
                _already = True
                break
    except Exception:
        _already = False
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR14",
            "date": "2026-01-23",
            "summary": "Diff engine consolidation: eliminate diff_metrics_by_name override chain by renaming legacy impls and introducing a single public wrapper entrypoint. Preserve legacy/fix31/v24 impls under stable names; update base capture vars and keep binding manifest stable/streamlit-safe.",
            "files": ["REFACTOR14_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR13"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR15
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    try:
        for _e in PATCH_TRACKER_V1:
            if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR15":
                _already = True
                break
    except Exception:
        _already = False
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR15",
            "date": "2026-01-23",
            "summary": "Restore diffing after REFACTOR14: harden diff_metrics_by_name resolution in FINAL BINDINGS, eliminate V2 UnboundLocalError (cur_resp_for_diff), and add safe fallback extraction for canonical_for_render/current PMC when nested under output['results'].",
            "files": ["REFACTOR15_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR14"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass



# PATCH TRACKER V1 (ADD): REFACTOR12
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    try:
        for _e in PATCH_TRACKER_V1:
            if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR12":
                _already = True
                break
    except Exception:
        _already = False
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR12",
            "date": "2026-01-23",
            "summary": "Truth-lock version stamping + binding manifest hygiene: freeze _yureeka_get_code_version() via locked default arg; re-assert globals for observability; ensure FINAL BINDINGS tag + diff function authoritative tag always present; standardize canonical_for_render_v1 debug block to avoid stale missing diagnostics.",
            "files": ["REFACTOR12_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR11"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass
# PATCH TRACKER V1 (ADD): REFACTOR10
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    try:
        for _e in PATCH_TRACKER_V1:
            if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR10":
                _already = True
                break
    except Exception:
        _already = False
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR10",
            "date": "2026-01-22",
            "summary": "Fix refactor versioning + binding hygiene: set CODE_VERSION/_YUREEKA_CODE_VERSION_LOCK to REFACTOR10; make binding_manifest_v1 report diff function name/qualname; tighten harness binding expectation to _yureeka_get_code_version(); update final bindings tag to REFACTOR10.",
            "files": ["REFACTOR10_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR09"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# PATCH TRACKER V1 (ADD): REFACTOR09
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    try:
        for _e in PATCH_TRACKER_V1:
            if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR09":
                _already = True
                break
    except Exception:
        _already = False
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR09",
            "date": "2026-01-22",
            "summary": "Introduce a single authoritative diff engine wrapper (_refactor09_diff_metrics_by_name) and bind diff_metrics_by_name to it in final bindings; prepare for safe removal of legacy duplicate diff definitions.",
            "files": ["REFACTOR09_full_codebase_streamlit_safe.py"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ============================================================
# REFACTOR09: DIFF ENGINE CONSOLIDATION (WRAPPER)
#
# Purpose:
#   - Establish one stable, explicit implementation target for future extraction.
#   - Preserve current working behavior by delegating to the legacy implementation
#     captured at end-of-file time.
#   - Final bindings will point to this wrapper (single edit point).
# ============================================================
try:
    _YUREEKA_DIFF_METRICS_BY_NAME_LEGACY = globals().get("diff_metrics_by_name")
    globals()["_YUREEKA_DIFF_METRICS_BY_NAME_LEGACY"] = _YUREEKA_DIFF_METRICS_BY_NAME_LEGACY
except Exception:
    _YUREEKA_DIFF_METRICS_BY_NAME_LEGACY = None

def _refactor09_diff_metrics_by_name(prev_response: dict, cur_response: dict):
    """Authoritative diff entrypoint (REFACTOR10).

    Delegates to the legacy diff implementation captured at load time, so behavior
    remains unchanged while we consolidate/remove duplicates safely.
    """
    try:
        fn = globals().get("_YUREEKA_DIFF_METRICS_BY_NAME_LEGACY")
        if callable(fn):
            return fn(prev_response, cur_response)
    except Exception:
        pass
    # Fallback (should be unreachable)
    try:
        fn2 = globals().get("diff_metrics_by_name")
        if callable(fn2) and fn2 is not _refactor09_diff_metrics_by_name:
            return fn2(prev_response, cur_response)
    except Exception:
        pass
    # Safe empty result
    return ([], [], [], [], False)


# REFACTOR11: FINAL BINDINGS (AUTHORITATIVE)
#
# Purpose:
#   - Eliminate "edited the wrong function" risk by ensuring a single,
#     explicit, end-of-file binding wins.
#   - Bind diff_metrics_by_name to the REFACTOR09 wrapper (single edit point).
#   - Tag the authoritative binding so the refactor harness can verify it.
#
# Notes:
#   - Legacy diff/binding blocks earlier in the file remain for archaeology,
#     but are overridden here.
# ============================================================
try:
    _YUREEKA_FINAL_BINDINGS_VERSION = _yureeka_get_code_version()
    globals()["_YUREEKA_FINAL_BINDINGS_VERSION"] = _YUREEKA_FINAL_BINDINGS_VERSION
except Exception:
    pass

# --- Authoritative diff binding (REFACTOR09 wrapper)
try:
    _YUREEKA_DIFF_METRICS_BY_NAME_AUTHORITATIVE = globals().get("_refactor09_diff_metrics_by_name")
    if callable(_YUREEKA_DIFF_METRICS_BY_NAME_AUTHORITATIVE):
        try:
            setattr(_YUREEKA_DIFF_METRICS_BY_NAME_AUTHORITATIVE, "__YUREEKA_AUTHORITATIVE_BINDING__", _yureeka_get_code_version())
        except Exception:
            pass
        diff_metrics_by_name = _YUREEKA_DIFF_METRICS_BY_NAME_AUTHORITATIVE  # type: ignore
        globals()["diff_metrics_by_name"] = diff_metrics_by_name
    globals()["_YUREEKA_DIFF_METRICS_BY_NAME_AUTHORITATIVE"] = _YUREEKA_DIFF_METRICS_BY_NAME_AUTHORITATIVE
except Exception:
    pass

# --- Final CODE_VERSION override (refactor sequence) [REFACTOR12 truth lock]
try:
    _yureeka_lock_version_globals_v1()
except Exception:
    pass

# ============================================================
# REFACTOR11: END-OF-FILE HARNESS DISPATCH (ADDITIVE)
# - HARD SAFETY: Never terminate the Streamlit server.
# - Only runs when explicitly invoked AND we are not under Streamlit runtime.
# ============================================================
try:
    if bool(globals().get("_REFACTOR01_HARNESS_REQUESTED")):
        import sys as _rf01_sys
        _is_st = False
        try:
            argv = _rf01_sys.argv or []
            if any("streamlit" in str(a).lower() for a in argv[:5]):
                _is_st = True
        except Exception:
            pass
        try:
            if ("streamlit" in _rf01_sys.modules) or ("streamlit.runtime.scriptrunner" in _rf01_sys.modules):
                _is_st = True
        except Exception:
            pass

        if not _is_st:
            _ok = _refactor02_run_harness_v2()
            _rf01_sys.exit(0 if _ok else 1)
except SystemExit:
    raise
except Exception:
    try:
        import traceback as _rf01_tb
        _rf01_tb.print_exc()
    except Exception:
        pass
    try:
        import sys as _rf01_sys
        _rf01_sys.exit(1)
    except Exception:
        pass


# ============================================================
# REFACTOR28 FINAL OVERRIDE: Schema-only rebuild authority
# - Eliminates stale wrapper-capture chains (FIX2D86/FIX16)
# - Ensures REFACTOR27 candidate filters are actually active at runtime
# ============================================================

def _refactor28_schema_only_rebuild_authoritative_v1(
    prev_response: dict,
    baseline_sources_cache: list,
    web_context: dict = None
) -> dict:
    """Schema-driven deterministic rebuild from cached snapshots only.

    This is intentionally minimal:
      - It does NOT attempt free-form metric discovery.
      - It ONLY populates metrics declared in the frozen schema.
      - Candidate selection is driven by schema fields (keywords + unit family/tag).
      - Deterministic sorting ensures stable output ordering.

    Returns:
      Dict[str, Dict] shaped like primary_metrics_canonical.
    """
    import re

# =====================================================================
    # =====================================================================
    # PATCH FIX33 (ADDITIVE): enforce unit-required eligibility in schema-only rebuild
    # Why:
    #   - When anchors are not used (anchor_used:false), schema-only rebuild can still
    #     select unit-less year tokens (e.g., 2024/2025) for currency/percent metrics.
    #   - This patch hard-rejects candidates with no token-level unit evidence when
    #     the schema (or canonical_key suffix) implies a unit is required.
    #   - Also optionally emits compact debug metadata for top candidates/rejections.
    # Determinism:
    #   - Pure filtering + stable ordering; no refetch; no randomness.
    # =====================================================================

    def _fix33_schema_unit_required(spec_unit_family: str, spec_unit_tag: str, canonical_key: str) -> bool:
        uf = str(spec_unit_family or "").strip().lower()
        ut = str(spec_unit_tag or "").strip().lower()
        ck = str(canonical_key or "").strip().lower()
        # Explicit unit families
        if uf in {"currency", "percent", "rate", "ratio"}:
            return True
        if ut in {"%", "percent"}:
            return True
        # Unit-sales metrics require a unit (prevents bare-year selection)
        if ck.endswith("__unit_sales") or ck.endswith("__units") or ck.endswith("__unit"):
            return True
        # Canonical-key suffix conventions (backstop)
        if ck.endswith("__currency") or ck.endswith("__percent") or ck.endswith("__rate") or ck.endswith("__ratio"):
            return True
        return False

    def _fix33_candidate_has_unit_evidence(c: dict) -> bool:
        if not isinstance(c, dict):
            return False
        # Any explicit unit/currency/% evidence is enough to qualify as "has unit".
        if str(c.get("unit_tag") or "").strip():
            return True
        if str(c.get("unit_family") or "").strip():
            return True
        if str(c.get("base_unit") or "").strip():
            return True
        if str(c.get("unit") or "").strip():
            return True
        if str(c.get("currency_symbol") or c.get("currency") or "").strip():
            return True
        if bool(c.get("is_percent") or c.get("has_percent")):
            return True
        mk = str(c.get("measure_kind") or "").strip().lower()
        if mk in {"money", "percent", "percentage", "rate", "ratio"}:
            return True
        toks = c.get("unit_tokens") or c.get("unit_evidence_tokens") or []
        if isinstance(toks, (list, tuple)) and len(toks) > 0:
            return True
        return False

    _fix33_dbg = False
    try:
        _fix33_dbg = bool((web_context or {}).get("debug_evolution") or ((prev_response or {}).get("debug") or {}).get("debug_evolution"))
    except Exception:
        pass
        _fix33_dbg = False


    # -------------------------
    # Resolve frozen schema (supports multiple storage locations)
    # -------------------------
    schema = None
    try:
        if isinstance(prev_response, dict):
            if isinstance(prev_response.get("metric_schema_frozen"), dict):
                schema = prev_response.get("metric_schema_frozen")
            elif isinstance(prev_response.get("primary_response"), dict) and isinstance(prev_response["primary_response"].get("metric_schema_frozen"), dict):
                schema = prev_response["primary_response"].get("metric_schema_frozen")
            elif isinstance(prev_response.get("results"), dict) and isinstance(prev_response["results"].get("metric_schema_frozen"), dict):
                schema = prev_response["results"].get("metric_schema_frozen")
    except Exception:
        pass
        schema = None

    if not isinstance(schema, dict) or not schema:
        return {}

    # -------------------------
    # Collect candidates from snapshots (no re-fetch)
    # -------------------------
    candidates = []
    if isinstance(baseline_sources_cache, list):
        for src in baseline_sources_cache:
            if not isinstance(src, dict):
                continue
            nums = src.get("extracted_numbers")
            if not isinstance(nums, list) or not nums:
                continue
            for n in nums:
                if not isinstance(n, dict):
                    continue
                # Filter junk deterministically (strict rebuild exclusion)
                if _candidate_disallowed_for_metric(n, None):
                    continue
                # Normalize a few fields to ensure stable downstream access
                c = dict(n)
                if not c.get("source_url"):
                    c["source_url"] = src.get("url", "") or src.get("source_url", "") or ""
                candidates.append(c)

    # Deterministic candidate ordering (no set/dict iteration surprises)
    def _cand_sort_key(c: dict):
        return (
            str(c.get("source_url") or ""),
            str(c.get("anchor_hash") or ""),
            int(c.get("start_idx") or 0),
            str(c.get("raw") or ""),
            str(c.get("unit_tag") or ""),
            str(c.get("unit_family") or ""),
            float(c.get("value_norm") or c.get("value") or 0.0),
        )

    candidates.sort(key=_cand_sort_key)

    if not candidates:
        return {}

    # -------------------------
    # Deterministic schema-driven selection
    # -------------------------
    def _norm_text(s: str) -> str:
        return re.sub(r"\s+", " ", (s or "").lower()).strip()

    out = {}

    for canonical_key in sorted(schema.keys()):
        spec = schema.get(canonical_key) or {}
        if not isinstance(spec, dict):
            continue

        spec_keywords = spec.get("keywords") or []
        if not isinstance(spec_keywords, list):
            spec_keywords = []
        spec_keywords_norm = [str(k).lower().strip() for k in spec_keywords if str(k).strip()]

        spec_unit_tag = str(spec.get("unit_tag") or spec.get("unit") or "").strip()
        spec_unit_family = str(spec.get("unit_family") or "").strip()

        # Score candidates by schema keyword hits, then filter by unit constraints if present.
        best = None
        best_key = None

        # ============================================================
        # PATCH FIX33 (ADDITIVE): per-metric debug collectors
        # ============================================================
        _fix33_top = []
        _fix33_rej = {}

        for c in candidates:
            # PATCH F: strict candidate exclusion at scoring time
            if _candidate_disallowed_for_metric(c, spec):
                continue
            # REFACTOR03 (ADDITIVE): enforce unit family + unit-tag eligibility
            if _refactor03_candidate_rejected_by_unit_family_v1(c, spec):
                continue
            # REFACTOR27 (ADDITIVE): reject currency date-fragment candidates (e.g., 'July 01, 2025')
            try:
                if _refactor27_candidate_rejected_currency_date_fragment_v1(c, spec):
                    continue
            except Exception:
                pass

            # =====================================================================
            # PATCH AI2 (ADDITIVE): guard against year-only candidates on currency-like metrics
            # Why:
            # - Some sources contain many years (e.g., 2023, 2024) that can outscore true values.
            # - For currency-ish metrics, suppress candidates that look like bare years unless context clearly indicates money.
            # Determinism:
            # - Pure filter; does not invent candidates or refetch content.
            # =====================================================================
            try:
                def _ai2_is_year_only(c: dict):
                    """Return True if candidate is a likely standalone year (1900-2100) with no unit."""
                    try:
                        c = c if isinstance(c, dict) else {}
                        # Prefer canonical numeric
                        v = c.get("value_norm")
                        if v is None:
                            v = c.get("value")
                        try:
                            iv = int(float(v))
                        except Exception:
                            return False
                        if iv < 1900 or iv > 2100:
                            return False
                        # Must be truly 4-digit (avoid 2023.5 etc)
                        try:
                            if abs(float(v) - float(iv)) > 1e-9:
                                return False
                        except Exception:
                            pass

                        # If the candidate itself signals time/year, do not treat as "junk year".
                        u = str(c.get("base_unit") or c.get("unit") or "").strip().lower()
                        ut = str(c.get("unit_tag") or "").strip().lower()
                        uf = str(c.get("unit_family") or "").strip().lower()
                        if "year" in u or "year" in ut or "year" in uf or "time" in uf:
                            return False

                        raw = str(c.get("raw") or "").strip()
                        sval = str(iv)

                        # -------------------------------------------------------------
                        # PATCH E (ADDITIVE): strict handling when raw contains context
                        # - Some extractors store a wider raw window (e.g. includes '$721m ... in 2023')
                        # - Currency symbols elsewhere in raw should NOT make a year candidate non-year.
                        # - Only treat as non-year if the currency symbol is directly attached to the year.
                        # -------------------------------------------------------------
                        try:
                            if re.search(r"(\$|usd|eur|gbp|aud|cad|sgd)\s*"+re.escape(sval)+r"\b", raw.lower()):
                                return False
                        except Exception:
                            pass

                        # If raw is basically just the year token (allow brackets/punctuation), it's year-only.
                        try:
                            raw2 = re.sub(r"[\s,.;:()\[\]{}<>]", "", raw)
                            if raw2 == sval:
                                return True
                        except Exception:
                            pass

                        # If raw contains multiple numbers, it's likely context; still treat as year-only
                        # when this candidate has no unit.
                        try:
                            nums = re.findall(r"\d{2,}", raw)
                            if len(nums) >= 2:
                                return True
                        except Exception:
                            pass

                        # If raw contains month names, likely a date; treat as year-only (we suppress dates too).
                        try:
                            if re.search(r"\b(jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)\b", raw.lower()):
                                return True
                        except Exception:
                            return True
                    except Exception:
                        return False
                def _ai2_schema_currencyish(sd: dict) -> bool:
                    try:
                        if not isinstance(sd, dict):
                            return False
                        u = str(sd.get('unit') or sd.get('base_unit') or '').lower()
                        if any(x in u for x in ('usd','sgd','eur','gbp','jpy','$','€','£')):
                            return True
                        # heuristic keywords on definition (safe, schema-driven-ish)
                        nm = str(sd.get('name') or '').lower()
                        if any(x in nm for x in ('revenue','sales','cost','price','capex','opex','investment','spend','spending','expenditure','value')):
                            return True
                        return False
                    except Exception:
                        return False

                _sd = locals().get('schema_def')
                if _ai2_schema_currencyish(_sd) and _ai2_is_year_only(c):
                    continue

                # =====================================================================
                # PATCH YEAR3 (ADDITIVE): suppress year-only candidates for percent/CAGR-like metrics too
                # Why: year tokens (e.g., 2025) can outrank true percent values when unit evidence is weak.
                # Safe: only suppress when candidate has no explicit unit and looks like a bare year.
                # =====================================================================
                try:
                    if _ai2_is_year_only(c):
                        _sd_name = str((_sd or {}).get('name') or '').lower()
                        _sd_ckey = str((_sd or {}).get('canonical_key') or ckey or '').lower()
                        _sd_unit_tag = str((_sd or {}).get('unit_tag') or '').lower()
                        _sd_unit_family = str((_sd or {}).get('unit_family') or '').lower()
                        if ('cagr' in _sd_name) or ('cagr' in _sd_ckey) or (_sd_unit_tag in ('percent','pct')) or (_sd_unit_family in ('percent','ratio','rate')):
                            continue
                except Exception:
                    pass
            except Exception:
                pass

            ctx = _norm_text(c.get("context_snippet") or "")
            if not ctx:
                continue

            # Keyword hits: schema-driven (no external heuristics)
            hits = 0
            if spec_keywords_norm:
                for kw in spec_keywords_norm:
                    if kw and kw in ctx:
                        hits += 1

            if spec_keywords_norm and hits == 0:
                continue

            # Unit constraints (only if schema declares them)
            if spec_unit_family:
                if str(c.get("unit_family") or "").strip() != spec_unit_family:
                    # allow a unit_tag-only match when family is missing in candidate
                    if not (spec_unit_tag and str(c.get("unit_tag") or "").strip() == spec_unit_tag):
                        continue

            if spec_unit_tag:
                # if a tag is specified, prefer exact tag matches
                if str(c.get("unit_tag") or "").strip() != spec_unit_tag:
                    # allow family match when tag differs
                    if not (spec_unit_family and str(c.get("unit_family") or "").strip() == spec_unit_family):
                        continue

            # =====================================================================
            # PATCH FIX41AFC5 (ADDITIVE): reject year-only candidates early (schema-only rebuild parity)
            # =====================================================================
            try:
                _vnorm = c.get("value_norm", None)
                if _vnorm is None:
                    _vnorm = c.get("value", None)
                _is_year = _is_yearish_value(_vnorm)
                _mk0 = str(c.get("measure_kind") or "").strip().lower()
                _cand_ut0 = str(c.get("unit_tag") or "").strip()
                _cand_fam0 = str(c.get("unit_family") or "").strip().lower()
                _is_pct0 = bool(c.get("is_percent") or c.get("has_percent") or (_cand_ut0 == "%") or (_cand_fam0 == "percent"))
                _has_curr0 = bool(str(c.get("currency_symbol") or c.get("currency") or "").strip())
                _has_unit_ev0 = bool(_cand_ut0 or _cand_fam0 or _is_pct0 or _has_curr0 or str(c.get("base_unit") or c.get("unit") or "").strip())
                if _is_year and (not _has_unit_ev0) and (not _is_pct0) and (not _has_curr0) and (_mk0 in ("magnitude_other", "count_units", "count", "number", "")):
                    try:
                        _fix41afc5_dbg2["rejected_year_only"] = int(_fix41afc5_dbg2.get("rejected_year_only", 0) or 0) + 1
                    except Exception:
                        pass
                    continue
            except Exception:
                pass

            # =====================================================================
            # PATCH FIX33 (ADDITIVE): hard-reject unit-less candidates when unit is required
            # =====================================================================
            try:
                _req = _fix33_schema_unit_required(spec_unit_family, spec_unit_tag, canonical_key)
                _has_unit_ev = _fix33_candidate_has_unit_evidence(c)
                # PATCH FIX2D58G (ADDITIVE): reject year-like candidates for unit_sales metrics
                # unit_sales keys represent quantities; they must never take a bare year token as the value.
                try:
                    if str(canonical_key or '').strip().lower().endswith('__unit_sales'):
                        _v = c.get('value_norm', None)
                        if _v is None:
                            _v = c.get('value', None)
                        if _is_yearish_value(_v):
                            if _fix33_dbg:
                                try:
                                    _fix33_rej['rejected_year_for_unit_sales'] = int(_fix33_rej.get('rejected_year_for_unit_sales', 0) or 0) + 1
                                except Exception:
                                    pass
                            continue
                except Exception:
                    pass

                if _req and not _has_unit_ev:
                    # Track rejection (debug)
                    if _fix33_dbg:
                        try:
                            _fix33_rej["missing_unit_required"] = int(_fix33_rej.get("missing_unit_required", 0) or 0) + 1
                        except Exception:
                            pass
                    continue


                # Track top candidates (debug)
                if _fix33_dbg:
                    try:
                        _fix33_top.append({
                            "raw": c.get("raw"),
                            "value_norm": c.get("value_norm"),
                            "unit_tag": c.get("unit_tag"),
                            "unit_family": c.get("unit_family"),
                            "base_unit": c.get("base_unit") or c.get("unit"),
                            "measure_kind": c.get("measure_kind"),
                            "hits": hits,
                            "has_unit_ev": bool(_has_unit_ev),
                            "source_url": c.get("source_url"),
                            "anchor_hash": c.get("anchor_hash"),
                        })
                    except Exception:
                        pass
            except Exception:
                pass

            # Deterministic tie-break:
            #   (-hits, then stable candidate identity tuple)
            tie = (-hits,) + _cand_sort_key(c)
            if best is None or tie < best_key:
                best = c
                best_key = tie

        if not isinstance(best, dict):
            continue

        # Emit a minimal canonical metric row (schema-driven, deterministic)
        metric = {
            "name": spec.get("name") or spec.get("canonical_id") or canonical_key,
            "value": best.get("value"),
            "unit": best.get("unit") or spec.get("unit") or "",
            "unit_tag": best.get("unit_tag") or spec.get("unit_tag") or "",
            "unit_family": best.get("unit_family") or spec.get("unit_family") or "",
            "currency_code": best.get("currency_code") or "",
            "base_unit": best.get("base_unit") or best.get("unit_tag") or spec.get("unit_tag") or "",
            "multiplier_to_base": best.get("multiplier_to_base") if best.get("multiplier_to_base") is not None else 1.0,
            "value_norm": best.get("value_norm") if best.get("value_norm") is not None else best.get("value"),
            "canonical_id": spec.get("canonical_id") or spec.get("canonical_key") or canonical_key,
            "canonical_key": canonical_key,
            "dimension": spec.get("dimension") or "",
            "original_name": spec.get("name") or "",
            "geo_scope": "unknown",
            "geo_name": "",
            "is_proxy": False,
            "proxy_type": "",
            "provenance": {
                "method": "schema_keyword_match",
                "best_candidate": {
                    "raw": best.get("raw"),
                    "source_url": best.get("source_url"),
                    "context_snippet": best.get("context_snippet"),
                    "anchor_hash": best.get("anchor_hash"),
                    "start_idx": best.get("start_idx"),
                    "end_idx": best.get("end_idx"),
                },
            },
        }

# ============================================================
        # ============================================================
        # PATCH FIX33 (ADDITIVE): selection debug (top candidates + rejection counts)
        # ============================================================
        try:
            if _fix33_dbg and isinstance(metric, dict):
                try:
                    _fix33_top_sorted = sorted(
                        _fix33_top,
                        key=lambda d: (-(int(d.get("hits") or 0)), str(d.get("value_norm") or ""), str(d.get("raw") or "")),
                    )
                except Exception:
                    pass
                    _fix33_top_sorted = _fix33_top
                metric.setdefault("provenance", {})
                metric["provenance"]["fix33_top_candidates"] = list(_fix33_top_sorted[:10])
                metric["provenance"]["fix33_rejected_reason_counts"] = dict(_fix33_rej or {})
        except Exception:
            pass

        out[canonical_key] = metric

    return out



# ===================== PATCH RMS_AWARE1 (ADDITIVE) =====================


# REFACTOR28: define the authoritative FIX16 schema-only wrapper directly on top of the authoritative base.
try:
    _refactor28__schema_only_wrapped = globals().get("_refactor28__schema_only_wrapped", False)
except Exception:
    _refactor28__schema_only_wrapped = False

if not _refactor28__schema_only_wrapped:
    def rebuild_metrics_from_snapshots_schema_only_fix16(prev_response, snapshot_pool, web_context=None):  # noqa: F811
        """Authoritative schema-only rebuild wrapper (REFACTOR28).

        Contract:
          - Deterministic selection from baseline snapshots (no re-fetch).
          - Preserves percent-year poisoning sanitization for percent keys.
          - Ensures currency date-fragment candidates (e.g., 'July 01, 2025') are not eligible.
        """
        rebuilt = {}
        try:
            rebuilt = _refactor28_schema_only_rebuild_authoritative_v1(prev_response, snapshot_pool, web_context=web_context)
        except Exception:
            rebuilt = {}

        # Preserve FIX2D86 sanitization: percent keys must not bind to bare year tokens.
        try:
            schema = {}
            if isinstance(prev_response, dict):
                schema = prev_response.get("metric_schema_frozen") or (prev_response.get("results") or {}).get("metric_schema_frozen") or {}
            if isinstance(rebuilt, dict) and rebuilt:
                rebuilt2, _sdbg = _fix2d86_sanitize_pmc_percent_year_tokens_v1(
                    pmc=rebuilt,
                    metric_schema_frozen=schema if isinstance(schema, dict) else {},
                    label="schema_only_rebuild_refactor28_final",
                )
                rebuilt = rebuilt2
        except Exception:
            pass

        return rebuilt

    try:
        rebuild_metrics_from_snapshots_schema_only_fix16._fix2d86_wrapped = True  # type: ignore[attr-defined]
        rebuild_metrics_from_snapshots_schema_only_fix16._refactor28_authoritative = True  # type: ignore[attr-defined]
    except Exception:
        pass

    # Rebind canonical symbol names to the authoritative wrapper.
    try:
        globals()["rebuild_metrics_from_snapshots_schema_only_fix16"] = rebuild_metrics_from_snapshots_schema_only_fix16
    except Exception:
        pass
    try:
        globals()["rebuild_metrics_from_snapshots_schema_only"] = rebuild_metrics_from_snapshots_schema_only_fix16
    except Exception:
        pass

    try:
        globals()["_refactor28__schema_only_wrapped"] = True
    except Exception:
        pass

# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR35
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    PATCH_TRACKER_V1.append({
        "patch_id": "REFACTOR35",
        "summary": "Move main() invocation to EOF so late refactor defs/overrides are active during runs; add schema-key filter to baseline PMC materialization to prevent debug-key leakage.",
        "files": ["REFACTOR35_full_codebase_streamlit_safe.py"],
    })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR36
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR36":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR36",
            "summary": "Fix Evolution fatal 'NoneType has no attribute get' by hardening add_to_history() against None callers and coercing run_source_anchored_evolution inputs (previous_data/web_context) to dicts.",
            "files": ["REFACTOR36_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR35"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass



# REFACTOR37 patch tracker
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR37":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR37",
            "summary": "Add a final crash-proof wrapper for run_source_anchored_evolution to prevent fatal NoneType.get exceptions from aborting Streamlit Evolution; coerce inputs to dict and return renderer-safe failed payload with traceback.",
            "files": ["REFACTOR37_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR36"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# REFACTOR38 patch tracker
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR38":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR38",
            "summary": "Fix FIX24 helper regressions causing None.get crashes in Evolution: ensure _fix24_get_prev_full_payload/_fix24_get_prev_hashes/_fix24_compute_current_hashes always return dicts; enhance REFACTOR37 evolution wrapper to persist full traceback under debug.error_traceback, surface a callsite hint in message/debug, and guard against non-dict impl returns.",
            "files": ["REFACTOR38_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR37"],
        })

        # =====================================================================
        # PATCH TRACKER: REFACTOR39
        # =====================================================================
        _already = False
        for _e in PATCH_TRACKER_V1:
            if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR39":
                _already = True
                break
        if not _already:
            PATCH_TRACKER_V1.append({
                "patch_id": "REFACTOR39",
                "summary": "Fix source-anchored evolution snapshot gating regression: when baseline_sources_cache is omitted from HistoryFull payload (Sheets cell limit), rehydrate snapshots deterministically via snapshot_store_ref/snapshot_store_ref_v2 and source_snapshot_hash_v2/source_snapshot_hash (Snapshots worksheet and local snapshot store). Attach snapshot_store_debug into output.debug on failure for fast diagnosis. No heuristic matching added; remains strict snapshot-gated without valid cached source text.",
                "files": ["REFACTOR39_full_codebase_streamlit_safe.py"],
                "supersedes": ["REFACTOR38"],
            })

    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ============================================================
# REFACTOR35: EOF entrypoint (Streamlit-safe)
# - We intentionally call main() only after ALL patch blocks and helper defs have executed,
#   so late overrides (diff engine, schema-only rebuild, etc.) are active during runs.
# ============================================================
# REFACTOR37: Crash-proof wrapper for run_source_anchored_evolution()
#
# Context:
# - This codebase historically had multiple definitions of run_source_anchored_evolution
#   (FIX24 wrapper + earlier entrypoints).
# - A deep NoneType.get() crash inside the FIX24 wrapper can bubble up to the Streamlit UI
#   and abort the run.
#
# Behavior:
# - Preserve the existing implementation as _REFACTOR37_RUN_SOURCE_ANCHORED_EVOLUTION_IMPL
# - Provide a final, Streamlit-safe wrapper that:
#     * coerces inputs to dict
#     * catches all exceptions and returns a renderer-safe failed payload with traceback
# ============================================================
try:
    _REFACTOR37_RUN_SOURCE_ANCHORED_EVOLUTION_IMPL = run_source_anchored_evolution
except Exception:
    _REFACTOR37_RUN_SOURCE_ANCHORED_EVOLUTION_IMPL = None


def run_source_anchored_evolution(previous_data: dict, web_context: dict = None) -> dict:
    # Input coercion (avoid None.get)
    if not isinstance(previous_data, dict):
        previous_data = {}
    if web_context is None or not isinstance(web_context, dict):
        web_context = {}

    def _fail(msg: str, tb: str = "") -> dict:
        out = {
            "status": "failed",
            "message": msg,
            "sources_checked": 0,
            "sources_fetched": 0,
            "numbers_extracted_total": 0,
            "stability_score": 0.0,
            "summary": {
                "total_metrics": 0,
                "metrics_found": 0,
                "metrics_increased": 0,
                "metrics_decreased": 0,
                "metrics_unchanged": 0,
            },
            "metric_changes": [],
            "source_results": [],
            "interpretation": "Evolution failed.",
            "code_version": str(globals().get("_YUREEKA_CODE_VERSION_LOCK") or ""),
            "debug": {
                "refactor37": {
                    "error": msg,
                }
            },
        }
        if tb:
            try:
                out["debug"]["refactor37"]["traceback"] = tb
            except Exception:
                pass
            try:
                _cs = ""
                for _ln in str(tb).splitlines():
                    _lns = _ln.strip()
                    if _lns.startswith("File "):
                        _cs = _lns
                if _cs:
                    try:
                        out["debug"]["refactor37"]["callsite"] = _cs
                    except Exception:
                        pass
                    try:
                        out["message"] = f"{msg} | {_cs}"
                    except Exception:
                        pass
            except Exception:
                pass
        return out

    impl = _REFACTOR37_RUN_SOURCE_ANCHORED_EVOLUTION_IMPL
    if not callable(impl):
        return _fail("run_source_anchored_evolution implementation is not callable.")

    try:
        _res = impl(previous_data, web_context=web_context)
        if not isinstance(_res, dict):
            import traceback as _tb
            return _fail("run_source_anchored_evolution returned non-dict result (impl)", tb=_tb.format_stack())
        return _res
    except TypeError:
        # Backward-compat: some historical defs accept only previous_data
        try:
            _res = impl(previous_data)
            if not isinstance(_res, dict):
                import traceback as _tb
                return _fail("run_source_anchored_evolution returned non-dict result (impl fallback)", tb=_tb.format_stack())
            return _res
        except Exception as e:
            import traceback as _tb
            return _fail(f"run_source_anchored_evolution crashed: {e}", tb=_tb.format_exc())
    except Exception as e:
        import traceback as _tb
        return _fail(f"run_source_anchored_evolution crashed: {e}", tb=_tb.format_exc())

# ============================================================
try:
    if __name__ == "__main__":
        if not bool(globals().get("_REFACTOR01_HARNESS_REQUESTED")):
            main()
except Exception:
    # Streamlit-safe: surface the exception if possible without crashing hard.
    try:
        import streamlit as st
        st.exception(Exception("Yureeka app crashed during main() execution (REFACTOR35)."))
    except Exception:
        pass


# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR40
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR40":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR40",
            "summary": "Fix recent snapshot retrievability and partial snapshot corruption in Snapshots sheet store. load_full_snapshots_from_sheet now bypasses stale cache on hash miss, and selects the most recent *complete* write batch (grouped by created_at) to avoid mixed/partial merges. store_full_snapshots_to_sheet no longer treats any existing rows as 'complete'; it validates loadability first and attempts repair writes when prior batch is incomplete, and invalidates snapshot worksheet cache after successful writes.",
            "files": ["REFACTOR40_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR39"],
        })

    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR41
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR41":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR41",
            "date": "2026-01-25",
            "summary": "Fix recent snapshot rehydration failures by (1) preventing fake snapshot_store_ref_v2 (gsheet:Snapshots:<hash>) from being emitted unless the mirror-write actually succeeded, (2) emitting snapshot_store_ref_stable pointing to a verified store (v2 sheet > v1 sheet > local) plus a compact snapshot_store_write_v1 debug manifest, and (3) making store_full_snapshots_to_sheet more reliable via smaller default chunk size and batched append_rows to reduce API payload size / rate-limit failures.",
            "files": ["REFACTOR41_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR40"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR42
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR42":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR42",
            "date": "2026-01-25",
            "summary": "Fix snapshot-gate failures caused by large baseline_sources_cache writes silently failing under Sheets rate limits. Snapshots sheet store now compresses very large payloads (zlib+base64 with 'zlib64:' prefix) to drastically reduce chunk count and API calls, adds a small throttle between batch appends for very large writes, and the loader transparently detects/decompresses compressed payloads while remaining backward-compatible with existing uncompressed snapshots.",
            "files": ["REFACTOR42_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR41"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ===========================================================
# PATCH TRACKER V1 (ADD): REFACTOR43
# ===========================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR43":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR43",
            "date": "2026-01-25",
            "summary": "BUGFIX: make Snapshots sheet loader actually decode REFACTOR42 compressed payloads ('zlib64:' prefix). Previously store_full_snapshots_to_sheet could write compressed snapshots but load_full_snapshots_from_sheet attempted json.loads() on the compressed string and returned empty, causing Evolution to be snapshot-gated for recent runs while older (uncompressed) snapshots still loaded.",
            "files": ["REFACTOR43_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR42"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ===================== PATCH TRACKER ENTRY: REFACTOR44 =====================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR44":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR44",
            "date": "2026-01-25",
            "summary": "BUGFIX: Fix local snapshot persistence path creation. _snapshot_store_dir() previously omitted a return on the success path, returning None and causing local snapshot store/load to fail (os.path.join(None,...)). Wrapped local snapshot write call in add_to_history with guards to prevent snapshot persistence block from aborting. Also hardened store_full_snapshots_local to compute path safely. This restores reliable snapshot persistence for recent runs when Sheets snapshot store is unavailable/partial, eliminating Evolution snapshot-gate failures caused by missing baseline_sources_cache.",
            "files": ["REFACTOR44_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR43"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass



# =============================================================================
# REFACTOR47 (ADDITIVE): Fix Diff Panel V2 recursion by rebinding __rows entrypoint
# =============================================================================
# Problem:
# - build_diff_metrics_panel_v2__rows was repeatedly wrapped by FIX2D2I-style
#   closures that captured an already-wrapped implementation, causing
#   RecursionError: maximum recursion depth exceeded.
# - The engine then fell back to REFACTOR45_STRICT_FALLBACK and set
#   debug.diff_panel_v2_error, even though rows were produced.
#
# Fix:
# - Provide a deterministic, non-recursive canonical-first join implementation.
# - Rebind build_diff_metrics_panel_v2__rows (and FIX2D2I alias names) to this
#   implementation as the last-wins definition at module import.
# - Preserve strict unit comparability + percent/year poisoning containment.
# - Schema/key grammar untouched.
# =============================================================================
try:
    def build_diff_metrics_panel_v2__rows_refactor47(prev_response: dict, cur_response: dict):
        # ---- unwrap canonical maps (robust to packaging variants)
        def _unwrap_pmc(resp: dict) -> dict:
            try:
                fn = globals().get("_diffpanel_v2__unwrap_primary_metrics_canonical")
                if callable(fn):
                    m = fn(resp)
                    return m if isinstance(m, dict) else {}
            except Exception:
                pass
            try:
                if isinstance(resp, dict):
                    m = resp.get("primary_metrics_canonical")
                    if isinstance(m, dict):
                        return m
                    r = resp.get("results")
                    if isinstance(r, dict):
                        m2 = r.get("primary_metrics_canonical")
                        if isinstance(m2, dict):
                            return m2
            except Exception:
                pass
            return {}

        prev_can = _unwrap_pmc(prev_response if isinstance(prev_response, dict) else {})
        cur_can = _unwrap_pmc(cur_response if isinstance(cur_response, dict) else {})

        # ---- helpers
        def _as_float(v):
            try:
                if v is None:
                    return None
                if isinstance(v, (int, float)):
                    return float(v)
                if isinstance(v, str):
                    s = v.strip().replace(",", "")
                    if not s:
                        return None
                    return float(s)
            except Exception:
                return None
            return None

        def _get_value_unit(m):
            try:
                if not isinstance(m, dict):
                    return (None, None)
                v = m.get("value_norm")
                if v is None:
                    v = m.get("value")
                unit = m.get("base_unit") or m.get("unit_tag") or m.get("unit")
                return (_as_float(v), unit)
            except Exception:
                return (None, None)

        def _is_percent_key(ck: str) -> bool:
            try:
                if not isinstance(ck, str):
                    return False
                ck_l = ck.lower()
                return ("__percent" in ck_l) or ck_l.endswith("_percent") or ("percent" in ck_l and "__" in ck_l)
            except Exception:
                return False

        def _is_yearlike_number(x) -> bool:
            try:
                if x is None:
                    return False
                if not isinstance(x, (int, float)):
                    return False
                xi = int(x)
                # Conservative yearlike band
                return (1900 <= xi <= 2500) and abs(float(x) - float(xi)) < 1e-9
            except Exception:
                return False

        def _looks_like_percent_unit(u) -> bool:
            try:
                s = str(u or "").lower()
                return ("%" in s) or ("percent" in s) or ("pct" in s)
            except Exception:
                return False

        rows = []

        prev_keys = sorted([k for k in prev_can.keys() if isinstance(k, str) and k])
        cur_keys = set([k for k in cur_can.keys() if isinstance(k, str) and k])

        both_count = 0
        prev_only = 0
        cur_only = 0

        for ck in prev_keys:
            pm = prev_can.get(ck) if isinstance(prev_can, dict) else None
            cm = cur_can.get(ck) if isinstance(cur_can, dict) else None

            pv, pu = _get_value_unit(pm)
            cv, cu = _get_value_unit(cm)

            # Percent/year poisoning containment (defensive, even without inference):
            # If the key is percent-like but unit doesn't look like percent and the
            # numeric looks yearlike, treat as invalid (non-comparable).
            if _is_percent_key(ck):
                if (cv is not None) and _is_yearlike_number(cv) and (not _looks_like_percent_unit(cu)):
                    cv = None
                if (pv is not None) and _is_yearlike_number(pv) and (not _looks_like_percent_unit(pu)):
                    pv = None

            # strict unit comparability
            is_comp = (pv is not None and cv is not None and str(pu or "").strip() == str(cu or "").strip())

            d = None
            pct = None
            ctype = "unknown"
            if cm is None:
                ctype = "not_found"
            elif pm is None:
                ctype = "added"
            elif is_comp:
                d = float(cv) - float(pv)
                if abs(d) < 1e-12:
                    d = 0.0
                if pv != 0:
                    pct = (d / float(pv)) * 100.0
                else:
                    pct = 0.0 if d == 0.0 else None
                if d > 0:
                    ctype = "increased"
                elif d < 0:
                    ctype = "decreased"
                else:
                    ctype = "unchanged"
            else:
                # present but not comparable
                ctype = "incomparable"

            # counts for summary
            if ck in cur_keys:
                both_count += 1
            else:
                prev_only += 1

            name = None
            try:
                name = (pm.get("name") if isinstance(pm, dict) else None) or (cm.get("name") if isinstance(cm, dict) else None) or ck
            except Exception:
                name = ck

            # attempt to surface source_url (if present)
            src_url = None
            try:
                if isinstance(cm, dict):
                    src_url = cm.get("source_url") or cm.get("url") or None

                    # Common nested winner shape: provenance.best_candidate.source_url
                    if (not src_url) and isinstance(cm.get("provenance"), dict):
                        prov = cm.get("provenance")
                        bc = prov.get("best_candidate") or prov.get("best_candidate_v1") or prov.get("winner") or prov.get("best")
                        if isinstance(bc, dict):
                            src_url = bc.get("source_url") or bc.get("url") or None

                    if (not src_url) and isinstance(cm.get("provenance_v1"), dict):
                        prov = cm.get("provenance_v1")
                        bc = prov.get("best_candidate") or prov.get("best_candidate_v1") or prov.get("winner") or prov.get("best")
                        if isinstance(bc, dict):
                            src_url = bc.get("source_url") or bc.get("url") or None

                    if not src_url and isinstance(cm.get("sources"), list) and cm.get("sources"):
                        s0 = cm.get("sources")[0]
                        if isinstance(s0, dict):
                            src_url = s0.get("url") or s0.get("source_url") or None
            except Exception:
                src_url = None

            # Last resort: centralized extractor (schema-preserving)
            if not src_url:
                try:
                    fn = globals().get("_refactor26_extract_metric_source_url_v1")
                    if callable(fn):
                        su2 = fn(cm)
                        if isinstance(su2, str) and su2.strip():
                            src_url = su2.strip()
                except Exception:
                    pass

            try:
                src_url = (src_url.strip() if isinstance(src_url, str) else "")
            except Exception:
                src_url = ""

            rows.append({
                "canonical_key": ck,
                "name": name,
                "previous_value": pv,
                "current_value": (cv if cm is not None else "N/A"),
                "previous_unit": pu,
                "current_unit": (cu if cm is not None else None),
                "prev_value_norm": pv,
                "cur_value_norm": (cv if cm is not None else None),
                "delta_abs": d,
                "delta_pct": pct,
                "change_type": ctype,
                "baseline_is_comparable": bool(is_comp),
                "current_method": "refactor47_canonical_join",
                "source_url": src_url,
                "cur_source_url": src_url,
                "current_source_url": src_url,
            })

        # cur-only keys (optional accounting; we do not emit rows by default)
        try:
            cur_only = int(max(0, len(cur_keys) - len(set(prev_keys) & cur_keys)))
        except Exception:
            cur_only = 0

        summary = {
            "rows_total": int(len(rows)),
            "builder_id": "REFACTOR47_CANONICAL_JOIN",
            "join_mode": "canonical_key",
            "both_count": int(both_count),
            "prev_only_count": int(prev_only),
            "cur_only_count": int(cur_only),
            "found": int(both_count),
            "not_found": int(prev_only),
        }
        return rows, summary

    # ---- last-wins rebind (eliminate recursion wrappers)
    try:
        globals()["build_diff_metrics_panel_v2__rows"] = build_diff_metrics_panel_v2__rows_refactor47
    except Exception:
        pass
    # neutralize FIX2D2I alias names if present
    try:
        globals()["build_diff_metrics_panel_v2__rows_fix2d2i"] = build_diff_metrics_panel_v2__rows_refactor47
    except Exception:
        pass
    try:
        globals()["build_diff_metrics_panel_v2"] = build_diff_metrics_panel_v2__rows_refactor47
    except Exception:
        pass
except Exception:
    pass



# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR47
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR47":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR47",
            "date": "2026-01-25",
            "summary": "Fix Diff Panel V2 recursion (maximum recursion depth exceeded) caused by FIX2D2I wrapper chains capturing already-wrapped __rows implementations. Provide a deterministic, non-recursive canonical-first join builder (strict unit comparability + percent/year poisoning containment) and rebind build_diff_metrics_panel_v2__rows (and FIX2D2I aliases) as last-wins entrypoint so Evolution no longer sets diff_panel_v2_error or falls back to strict_fallback_v2.",
            "files": ["REFACTOR47_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR46"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR48
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR48":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR48",
            "date": "2026-01-25",
            "summary": "Fix source-anchored Metric Changes table to render metric_changes_v2 fields (delta_abs/delta_pct/comparability/method) while keeping legacy fallback; bump version lock to REFACTOR48.",
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR49
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR49":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR49",
            "date": "2026-01-25",
            "summary": "Eliminate Diff Panel V2 RecursionError by making FIX2D2I-style __rows wrapper idempotent and non-recursive across duplicate wrapper blocks and Streamlit reruns. Store a stable base __rows implementation, avoid re-wrapping an already wrapped function, and keep trace augmentation without affecting schema/key grammar or diff semantics.",
            "files": ["REFACTOR49_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR48"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR50
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR50":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR50",
            "date": "2026-01-25",
            "summary": "Fix Evolution stability calculation: prevent unchanged rows from being double-counted as 'small change' (<10%), clamp discrete stability to 0–100, and compute stable/small counts from comparable rows only. Removes impossible 150% stability when all rows are unchanged; no schema/key-grammar changes.",
            "files": ["REFACTOR50_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR49"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass

# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR51
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR51":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR51",
            "date": "2026-01-25",
            "summary": "Fix evolution stability graded fallback: cap per-row abs % change at 100 before averaging so injected/outlier deltas don't force 0% stability. Add debug fields mean_abs_pct_raw/capped.",
            "files": ["REFACTOR51_full_codebase_streamlit_safe.py"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR52
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR52":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR52",
            "date": "2026-01-25",
            "summary": "Add authority_manifest_v1 (runtime last-wins introspection) into binding_manifest_v1 to make refactor deletions safer. No schema/key-grammar changes; no behavior changes.",
            "files": ["REFACTOR52_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR51"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR53
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR53":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR53",
            "date": "2026-01-25",
            "summary": "Make metric_changes rows self-attributing for injected-vs-production gating. Extend source_url extraction to include provenance.best_candidate.source_url; stamp rows with cur/current/source_url fields; change Δt gating to suppress only when row source URL matches injected URL set (missing attribution no longer treated as injected). Add debug counters rows_with_source_url/rows_missing_source_url/rows_suppressed_by_injection.",
            "files": ["REFACTOR53_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR52"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass
# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR54
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR54":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR54",
            "date": "2026-01-25",
            "summary": "Safe downsizing + durability diagnostics: remove duplicated FIX2D2I Diff Panel V2 wrapper block (redundant after recursion hardening); add snapshot_roundtrip_v1 (best-effort readback of snapshot_store_ref_stable) into Analysis persistence debug to catch recent snapshot save/retrieve issues early. No schema/key-grammar changes.",
            "files": ["REFACTOR54_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR53"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass
# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR55
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR55":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR55",
            "date": "2026-01-25",
            "summary": "Consolidate metric changes outputs to a single canonical feed: output['metric_changes'] is authoritative; output['metric_changes_v2'] mirrors the same list for backward/UI compatibility; drop output['metric_changes_legacy'] (no longer maintained). Add a late-stage consolidation block in compute_source_anchored_diff_BASE to enforce invariants. No schema/key-grammar changes.",
            "files": ["REFACTOR55_full_codebase_streamlit_safe.py"],
            "supersedes": ["REFACTOR54"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR56
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR56":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR56",
            "date": "2026-01-26",
            "summary": "Controlled downsizing: stop emitting metric_changes_legacy entirely (remove late-stage re-add), add end-of-function safety rail to pop it before returning. No schema/key-grammar changes; Diff Panel V2 remains authoritative and metric_changes_v2 continues to mirror metric_changes.",
            "files": ["REFACTOR56.py"],
            "supersedes": ["REFACTOR55"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass


# ============================================================
# PATCH TRACKER V1 (ADD): REFACTOR57
# ============================================================
try:
    PATCH_TRACKER_V1 = globals().get("PATCH_TRACKER_V1")
    if not isinstance(PATCH_TRACKER_V1, list):
        PATCH_TRACKER_V1 = []
    _already = False
    for _e in PATCH_TRACKER_V1:
        if isinstance(_e, dict) and _e.get("patch_id") == "REFACTOR57":
            _already = True
            break
    if not _already:
        PATCH_TRACKER_V1.append({
            "patch_id": "REFACTOR57",
            "date": "2026-01-26",
            "summary": "Stabilize diff harness prior to further downsizing: prefer REFACTOR47 V2 row builder when present; suppress emission of debug.diff_panel_v2_error/traceback (legacy V2 builder can fail before late rebinds). Canonical-first strict fallback remains authoritative; no schema/key-grammar changes.",
            "files": ["REFACTOR57.py"],
            "supersedes": ["REFACTOR56"],
        })
    globals()["PATCH_TRACKER_V1"] = PATCH_TRACKER_V1
except Exception:
    pass
